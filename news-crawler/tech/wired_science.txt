
This story originally appeared on Project Earth and is part of the Climate Desk collaboration.
A controversial study of the electric grid, requested by Energy Secretary Rick Perry in April, is finally expected to be released this month. And to some experts, its exact purpose remains, well, questionable at best.
In an April memo to energy department staff, Perry called for a study investigating whether certain federal energy policies, such as subsidies for renewables like wind and solar, were prematurely forcing coal, nuclear and other baseload power plants into retirement—and whether this is a problem for the grid’s performance. These issues, he writes, are “central to protecting the long-term reliability of the electric grid.”
But the request has sparked alarm among renewable energy advocates, some of whom are bracing themselves for a report they fear will become a vehicle for the Trump administration to attack wind and solar energy. These fears are compounded by the fact that the person Perry has appointed to lead the study is Energy Department political appointee Travis Fisher, a former economist from the right-leaning Institute for Energy Research, who has previously criticized the existence of federal tax credits for renewable energy.
And while the report’s findings are still to be seen, other energy experts say the motivation for the study—and what it will actually accomplish once released—remains murky. The study seems to rely on several inaccurate assumptions, they say, and its purported goal of protecting grid reliability may actually be threatened by the Trump administration itself.
In its most recent 2018 budget proposal, the White House has proposed significant budget reductions for the Department of Energy, including cuts to the Office of Energy Efficiency and Renewable Energy, the Office of Nuclear Energy and the Office of Electricity Delivery and Energy Reliability. The latter two programs, in particular, would seem to support research that’s directly related to Perry’s interest in supporting baseload power plants and safeguarding grid performance.
“...This is a report being created by the political leadership of an agency that doesn’t have a vision for what they’re going to do, and they don’t have control over their budget, it seems.”
“There are probably multiple motivations [for the study],” said David Victor, an energy policy expert at the University of California San Diego. “The most uncharitable one is that this is a report being created by the political leadership of an agency that doesn’t have a vision for what they’re going to do, and they don’t have control over their budget, it seems — and so they’re kind of hunching around for something to do.”
A more optimistic perspective, he suggested, is that the forthcoming report is intended to be used as a kind of “road map” for the federal government and interested states to explore and prepare for the challenges that may face the grid as the US energy landscape continues to evolve. But its contents, once they’re revealed, will speak volumes about whether this is actually the case.
Perry’s request is based around the idea that “baseload power is necessary to a well-functioning grid,” as he writes in his memo—and that recent closures of baseload power plants are threatening the grid’s reliability.
By “baseload,” he’s referring to power plants, such as coal and nuclear plants—both energy sources favored by the Trump administration—that are able to produce a constant supply of electricity to satisfy minimum demand. Wind and solar, on the other hand, are what’s known as “intermittent” energy sources because they can’t be harnessed in the same location all the time—they only provide energy when the sun is shining or the wind is blowing.
While small communities with their own microgrids and battery storage systems may be able to power themselves entirely on renewable energy, for the US as a whole it’s true that intermittent sources alone would be unable to meet energy demands around the clock without significant improvements in energy storage technology, which would enable utilities to save up energy as it’s generated and deploy it later as needed.
But much of the controversy surrounding the grid study stems from Perry’s implication that federal policies, including subsidies for renewables, are responsible for edging baseload power plants out of the picture—and that they’re doing so to an extent that they “create acute and chronic problems for maintaining adequate baseload generation and have impacted reliable generators of all types,” as Perry writes.
These suggestions rely on inaccurate assumptions about both the energy market and the grid, said Ryan Fitzpatrick, deputy director of the clean energy program at centrist think tank Third Way.
For one thing, the challenges facing baseload power plants are hardly being driven by federal policies alone. In fact, the greatest single factor in the decline of coal has actually been the rise of cheap natural gas, Victor noted — not renewables. Coal has just been unable to compete.
That’s not to say that the continued expansion of renewables like wind and solar are not helping to edge coal out of the picture, but they’re hardly the primary culprit. According to Victor, it seems unusual that the new report doesn’t call for a greater focus on natural gas — although, he noted, this could be another indication of the administration’s interest in simply “bashing renewables.”
Renewable industry trade groups have already expressed their worries about these potential biases in an open letter to Perry, submitted in May, which called for a transparent review that’s open to public input and voiced concern about what they see as the study’s “faulty premise.”
“What we’re actually seeing is that the markets are doing exactly what they’re programmed to do, which is provide reliable electricity at low prices.”
Another problem is the idea “that we have this supposed reliability crisis that we need fixed, and that the markets aren’t solving that on their own,” Fitzpatrick told Project Earth. “What we’re actually seeing is that the markets are doing exactly what they’re programmed to do, which is provide reliable electricity at low prices.”
That’s not to say there are never any problems with the US grid system. Just last weekend, a blackout in California’s San Fernando Valley, which officials say was likely caused by an intense heat wave and excessive energy demand, left 140,000 people without power.
For the time being, Fitzpatrick said, it’s unlikely that any reliability issues have much to do with the growth of renewables, which currently only account for about 15 percent of US electricity generation. The San Fernando Valley incident actually speaks both to the need to replace aging infrastructure around the country—the outage was reportedly sparked by an explosion at a transformer more than 40 years old—and the strains that future climate change may place on the grid in the form of increased energy demand.
Fitzpatrick also suggested that other issues with grid reliability are more likely to stem from the nation’s increasing reliance on natural gas, which recently surpassed coal as the country’s leading source of electricity generation.
“The more you rely on any one fuel, the more challenging it can become to manage unexpected or unanticipated or rare obstructions,” he said.
Indeed, some experts suggest that expanding renewable energy sources and investing in the increased flexibility of the grid, which was originally designed to be supported mainly by baseload power sources, is the way to go. A report recently commissioned by the Natural Resources Defense Council, for instance, suggests that grid planners should be “technology neutral” instead of focusing on designing electricity systems around baseload power sources.
As market forces continue to phase out more expensive power sources like coal and support the expansion of alternatives, the report suggests a variety of strategies to deliver lower-cost mixes of energy, including taking advantage of resources like energy storage technology, utilizing hydro or natural gas-fired plants that can more easily ramp up and down in response to demand, and providing incentives for consumers to reduce their energy consumption at certain times to reduce pressure on the grid.
That said, as intermittent energy sources continue to expand, there are some legitimate questions about their effect on grid performance, Victor pointed out. Continued research on energy storage technology and more efficient electric power transmission will be key in the continued integration of renewables on the grid.
“There’s a really big technical debate about how much renewables, at what cost, you can bring onto the grid and still keep reliability high,” he noted.
However, these questions have already been addressed by a number of recent studies, including a federal report released just last year by the National Renewable Energy Laboratory. It found that renewable technologies have the potential to supply up to 80 percent of US electricity generation in the year 2050 “while meeting electricity demand on an hourly basis in every region of the country.”
There may still be some room for debate about this exact value, and several academic studies in the past few years have come to different conclusions. The point, though, is that these questions have been asked over and over already—and research from both the federal government and independent scientists suggests that renewables can expand a lot further before they begin to threaten the grid’s reliability.
Despite so much controversy surrounding the new report, though, experts suggest it’s unlikely to have any major policy significance. Indeed, Victor noted that he’s “very skeptical that this report is going to have any relevance at all.”
“People are very suspicious of what this administration is doing, and of what Rick Perry in particular is doing.”
These days, energy markets are influenced by state-level policies, such as renewable energy portfolios, far more than federal ones—and the report will almost certainly have no authority to change them. It’s true that federal tax credits for wind and solar have provided significant benefits for the renewables industry, but these have already been extended by Congress through the year 2021 and are unlikely to change before that time.
Fitzpatrick noted that this doesn’t mean the report’s contents couldn’t still create difficulties for the renewable energy industry in certain ways.
“A lot of the states that have renewable portfolio standards, they’re run by Republican legislatures,” he pointed out. Depending on its contents, he said, the report “could make it kind of difficult if supporting an increase for renewables on a given grid puts you in direct contradiction—even if it is completely rhetorical contradiction—with what the White House has says it cares about and believes. Not to say that that’s the deciding factor, but it doesn’t make it any easier to support or expand things like renewables.”
According to Victor, there are many legitimate questions that remain about the future of grid reliability, the integration of renewables and the influence of power markets on these issues—and a reasonable, unbiased report from the Department of Energy could help to further some of these discussions.
Should the final report be viewed as an unfair attack on renewable energy, though, the hope of any productive discussions coming out of it may be lost.
“People are very suspicious of what this administration is doing, and of what Rick Perry in particular is doing,” he said. “And they are going to look for evidence that their suspicions are right in things like this report.”
Take a look inside the first commercial-scale solar energy plant to use nothing more than the sun, molten salt, and a whole lot of mirrors to send power to the people. If the Crescent Dunes Solar Energy facility works as promised, it could be a model for the future of renewable energy.
Huntington’s disease is brutal in its simplicity. The disorder, which slowly bulldozes your ability to control your body, starts with just a single mutation, in the gene for huntingtin protein. That tweak tacks an unwelcome glob of glutamines—extra amino acids—onto the protein, turning it into a destroyer that attacks neurons.
Huntington’s simplicity is exciting, because theoretically, it means you could treat it with a single drug targeted at that errant protein. But in the 24 years since scientists discovered it the gene for huntingtin, the search for suitable drugs has come up empty. This century’s riches of genetic and chemical data seem like it should have sped up research, but so far, the drug pipeline is more faucet than fire hydrant.
Part of the problem is simply that drug design is hard. But many researchers point to the systems of paywalls and patents that lock up data, slowing the flow of information. So a nonprofit called the Structural Genomics Consortium is countering with a strategy of extreme openness. They’re partnering with nine pharmaceutical companies and labs at six universities, including Oxford, the University of Toronto, and UNC Chapel Hill. They’re pledging to share everything with each other—drug wish lists, results in open access journals, and experimental samples—hoping to speed up the long, expensive drug design process for tough diseases like Huntington’s.
Rachel Harding, a postdoc at the University of Toronto arm of the collaboration, joined up to study the Huntington’s protein after she finished her PhD at Oxford. In a recent round of experiments, her lab grew insect cells in stacks of lab flasks fed with pink media. After slipping the cells a DNA vector that directed them to produce huntingtin, Rachel purified and stabilized the protein—and once it hangs out in a deep freezer for a while, she’ll map it with an electron microscope at Oxford.
Harding’s approach deviates from the norm in one major way: She doesn’t wait to publish a paper before sharing her results. After each of her experiments, “we’ll just put that into the public domain so that more people can use our stuff for free,” she says: protocols, the genetic sequences that worked for making proteins, experimental data. She’d even like to share protein samples with interested researchers, as she’s offered on Twitter. All this work is to create a map of huntingtin, “how all the atoms are connected to each other in three-dimensional space,” Harding says, including potential binding sites for drugs.
The next step is to ping that protein structure with thousands of molecules–chemical probes–to see if any bind in a helpful way. That’s what Kilian Huber, a medicinal chemistry researcher at Oxford University’s arm of the Structural Genomics Consortium, spends his days working on. Given a certain protein, he develops a way to measure its activity in cells, and then tests it against chemicals from pharmaceutical companies’ compound libraries, full of thousands of potential drug molecules.
If they score a hit, Huber and his consortium collaborators have pledged not to patent any of these chemicals. To the contrary, they want to share any chemical probe that works so it can quickly get more replication and testing. Many times, at other researchers’ requests, he has “put these compounds in an envelope, and sent them over,” he says. Recipient researchers generally cover shipping costs, and the organization as a whole has shipped off more than 10,000 samples since it started in 2004.
Under the umbrella of the SGC, about 200 scientists like Kilian and Rachel have agreed to never file any patents, and to publish only open access papers. CEO Aled Edwards beams when he talks about the group’s “metastatic openness.” Asking researchers to agree to share their work hasn’t been a problem. “There’s a willingness to be open,” he says, “you just have to show the way.”
There are a few challenges to such a high degree of openness. The academic labs are involved in which projects they tackle first—but it’s their funders that ultimately decide which tricky proteins everyone will work on. Each government, pharmaceutical company, or nonprofit that gifts $8 million to the organization can nominate proteins to a master to-do list, which researchers at these companies and affiliate universities tackle together.
That list could be a risk for the pharma companies at the table: While it doesn’t specify which company nominated which protein, the entire group can see that somebody is interested in a Huntington’s strategy, for example. But they’re hedging their bets on a selective reveal of their priorities. For several million dollars—a fraction of most of these companies’ R&D budgets—companies including Pfizer, Novartis, and Bayer buy into the scientific expertise of this group and stand to get results a bit faster. And since no one is patenting any of the genes, protein structures, or experimental chemicals they produce, the companies can still file their own patents for whatever drugs they create as a result of this research.
That might seem like a bum deal for the scientists doing all the work of discovery. But mostly, scientists at the SGC seem thrilled that collaborating can accelerate their research.
Why Pharma Wants to Put Sensors in This Blockbuster Drug
Fixing a Broken Drug Business by Spreading the Wealth
Drug Test Cowboys: The Secret World of Pharmaceutical Trial Subjects
“Rather than trying to do everything yourself, I can just share whatever I'm generating, and give it to the people that I think are experts in that area,” says Huber. “Then they will share the information back with us, and that, to me, is the key, from a personal point of view, on top of hopefully being able to support the development of new medicines,” says Huber. Because all the work is published open access, technically anyone in the world could benefit.
Edwards has pushed the SGC to slowly open up new steps of the drug discovery process. They started out working on genes, which is why they’re named a ‘genomics consortium’, then eked their way to sharing protein structures like the ones Harding works on. Creating and sharing tool compounds like Huber’s is their latest advance. “We’re trying to create a parallel universe where we can invent medicines in the open, where we can share our data,” Edwards says.
He hopes their approach will expand into a wider movement, so that other life science researchers get on board with data sharing, and open-source science improves repeatability and speeds up research findings. The Montreal Neurological Institute stopped filing patents on any of its discoveries last year. And there are other groups, like the Open Source Malaria Project, that have made a point of keeping all of their science in the open.
Sharing data won’t necessarily solve the inflating price of certain drugs. But it could certainly speed up understanding of new compounds, and shore up their chances of getting through clinical trials. The drug-making process is so complicated that if data sharing shaved just a bit of time off each step, it could save people years of waiting. The Huntington’s patients are waiting.
Medicine has an expiration stamp—but Is it actually, you know, serious? Or are those sell-by dates just a Big Pharma racket? Mr. Know-It-All gives you a healthy dose of the truth.
In a cramped meeting room Wednesday on Capitol Hill, House Democrats hosted a roundtable to discuss climate change with several national security experts. In attendance were two former admirals, a retired general, a once-ambassador to Nigeria, and the former undersecretary to the Secretary of Defense.
Over several hours of questioning, they described how climate change would escalate instability across the globe and make it harder for the US military to conduct its operations. Nothing they said, however, was all that new. In fact, the Department of Defense has known about, and sometimes planned for, the security threats created by climate change for well over a decade. Congressional Democrats—minority members of the House Science Committee—called the roundtable as a plea to the Republican-led Congress to stop standing in the way of the military's preparations for the heightened dangers of a warming world.
One of the key phrases here is "threat multiplier." Coined about a decade ago by panelist Sherri Goodman, a former Deputy Under Secretary of Defense, it means climate change will raise the stakes for existing conflicts, and push unstable communities toward catastrophe. Case study: the Syrian Civil War, rise of ISIS, and Syrian refugee crisis began in part because of a climate change-linked drought that began in 2006. "Droughts affected the Syrian harvests, compounded by historically poor governance and water management," says Marcus King, a professor of international affairs at George Washington University. This caused migrations of farmers into the cities, where they had neither jobs nor food. The violent protests for both became rallying cry against repressive president Bashar Assad. The protests became riots, then insurgency, and eventually full-blown chaos.
The threat multiplier paradigm is appearing in other places. Guatemala already has problems with food security, and many regions are still left ungoverned after that country's not-so-distant civil war. Rising seas are bringing saltwater incursion to Egypt's Nile Delta, adding food insecurity to that country's already tense political situation. And in Nigeria's capital city of Lagos, nearly half of the 22 million residents live below sea level and will eventually have to relocate—unlikely to be easy or conflict-free. "This isn’t a political issue for the defense community," says Ann Phillips, a retired admiral and an advisor for the Center for Climate and Security. "We in this community are pragmatic and mission-focused."
Without preparation, responding to those threats will stretch both the US military's defense and humanitarian capabilities. "If you take Yemen as an example, what if we sent forces there, how do we deal with their famine situation?" asks Phillips. She says coming in with that dual capability leads to mission creep, the military doing jobs better suited for other agencies—and encroaching on its primary job of defense.
How Global Warming Helped Cause the Syrian War
United Nations Says Climate Change Threatens Global Security
Climate Change Is So Bad That the US and China Agree on It
This worrisome trend is ever more likely, given President Trump and the Republican majority's climate-cutting spree throughout the federal government. This doesn't just affect future threats. Climate change is already hindering the military's operational readiness. A 2015 Rolling Stone article illustrated how higher tides—driven by sea level rise—regularly flood the world's largest naval base in Hampton Roads, VA. President Obama had directed multiple agencies to attack the problem together. The Department of Transportation would elevate the roads, Department of Energy would shore up the grid, etc. "Current Secretary of Defense General James Mattis has testified that he understands climate change should be addressed where it impacts his mission," says Phillips. "But some of other agencies have directors who are less ... interested, you could say?" That could mean the DoD is left to mop up these problems all on its own.
There's an even broader danger to leaving the military to address the threats from climate change. "The problem with leaving it to military, is that they are trained to deal with threats. So their solutions to climate crises tend to frame the victims of climate change as threats," says Nick Buxton, co-editor of The Secure and the Dispossessed, a collection of academic articles exploring how the military and corporations are responding to climate change. Just look at how the US, EU, and other western democracies have responded to Syria's refugee crisis. And refugees don't always live overseas—Buxton points to how many New Orleans residents experienced violations of their personal liberties in the chaos following Hurricane Katrina.
Buxton says continuing to ignore these climate catastrophes until they boil over could lead to ecological apartheid. Societies with resources will protect themselves from those without. "The US will not be most vulnerable, it has resources, but how other nations are defended and protected will have implications for us," he says. Again, look at Syrian refugee crisis: It was one of the major issues that drove UK voters to vote for Brexit, and threatens to further fracture the EU.
In the backdrop of the climate security roundtable, the US House of Representatives was debating the National Defense Appropriations Act, which authorizes the military's annual budget. In it is an amendment that recognizes climate change as a threat to national security. Every panelist mentioned this amendment as a favorable step towards greater security. Buxton, however, points out that this is a short sighted, and inconsistent goal, as it only addresses the threats, and not the causes of climate change. "I can understand, given nature of Republican party, why Democrats believe this is only avenue for advancing the conversation on climate change in Washington, but it has a lot of dangers," he says. "If you’ve only got a hammer as your tool, then everything in your shed starts to look like a nail."
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
Space could be capitalism's next frontier, but not without the government's help.
Luckily, the government is all ears. Last Thursday, a bunch of space capitalists sat across from a bunch of senators to talk policy. The panel included everyone from SpaceX's senior VP to the CEO of a small launch startup. Topics ranged from removing orbital debris to enabling deep space commerce; from eliminating bureaucratic speed bumps to dealing with space pirates. Yes, the discussion ranged wide, and at times weird—again, space pirates?—but overall one point was clear: The riches of space are America's to lose.
Congressional hearings like these often reveal their organizers' political motives. Texas Republican Ted Cruz, chair of the Senate Science, Space, and Commerce committee, convened this meeting, and it's not unreasonable to think the themes these speakers touched upon were his, and its attendees chosen by his staff to give voice to his vision. "Ted Cruz is a very smart man, no matter what anyone thinks of his politics, and he's been on track lately to do commerce the old fashioned way," says Keith Cowing, editor of NASAWatch. If this meeting is really an indication of what's to come, the future of space will continue in that good ol' fashioned American spirit of free markets with a heaping side of government help.
The US has always looked at space through the Lewis and Clark paradigm—exploration leads to commerce. It's even written into NASA's 1958 founding charter. (The same document requires NASA to study the Earth's atmosphere.) And space commerce has exploded. Globally, it's a $330 billion a year industry, with commercial activities making up more than three quarters of that total value. Every starry-eyed space entrepreneur owes some measure of their success to technology, property, or expertise that NASA has given away at cut-rate prices, or occasionally, for free. The rest of the federal government is generally just as accommodating. Hence, this meeting.
But still, the government is by definition a bureaucracy, and its concerns for things like safety, security, and process can slow down the flow of space bucks. Hence, this meeting.
Rocket launches get more publicity than satellite operations, but the latter are a bigger business, representing more than 60 percent of the space economy. And like in any big business, pollution can become a problem. Moriba Jah, an astrodynamicist and professor at the University of Texas, Austin, compares Earth's orbital environment to the Wild West: NASA's work providing infrastructure and expertise to commercial interests was like the Transcontinental Railroad, which brought Eastern American businesses to everything left of the Mississippi. "The environmental impact of runaway mining and prospecting was harsh and detrimental in many instances," says Jah. Except instead of mercury poisoning, the proliferation of commercial space operations has left Earth's orbital environment littered with junk.
And as anyone who saw Gravity knows, even small pieces of space junk can cause catastrophic damage to expensive operational pieces of equipment like the International Space Station. Jah points out how woefully behind the US's space junk tracking (let alone clean-up) efforts are. The Air Force's US Strategic Command tracks over 24,000 objects in its space situational awareness database. Jah says the actual number of potential flight hazards—any orbital object bigger than a centimeter in diameter—is at least 100 times that.
There's another good reason for keeping an eye on all the objects in the sky. "Space piracy has likely already happened, is happening, and will happen so long as we lack the ability to comprehensively monitor all space activities," he says. "This unfortunate human behavior has happened in all other domains, and to expect the space domain to be an exception is naïve at best." Yargh! Jah recommends that the Air Force let civilian operators take over monitoring orbital space, modeled after civil air traffic management.
Rockets may be economic small potatoes, but their obvious importance—can't get to space without 'em—explains the presence of two rocket company reps at Thursday's meeting. SpaceX wants to go to Mars, and its VP of global business Tim Hughes made clear how the US government could support that goal: by contracting with commercial launch companies for its deep space missions.
Beyond subsidizing Elon Musk's plans to colonize the Red Planet, deep space launches are necessary to grow the space economy overall. Remember, satellites are the space economy's biggest payout. But that can't last forever. Earth can only hold so many humans, which means there's a finite limit to this species'  telecommunications needs. If space capitalism is going to grow, it needs to open up new markets.
And yet, Earth's crowded orbital space still has some niches, particularly for smaller payloads. Relativity Space, Inc. is a space launch startup that wants to cater to small satellite operators, those that don't require massive rockets to reach low Earth orbit. In explaining his company's needs, Relativity's CEO Tim Ellis extended Jah's historical manifest destiny analogy by a hundred years. "We firmly believe that opening and strategically building up specialized government infrastructure could act as an “accelerator” of space startups, in much the same way that President Eisenhower created the highway system and catalyzed the automobile industry," he says. This would free up small companies like his from using their limited startup capital on infrastructure like launchpads and engine stands, and instead letting them build the rockets they need to survive until their next round of funding.
Somebody Just Buy the ISS Already
SpaceX's Plan to Reach Mars by 2018 Is ... Actually Not That Crazy
Congress Says Yes to Space Mining, No to Rocket Regulations
The US government provides a third, more specialized space market: The International Space Station. And while the ISS needs no introduction, it might soon require a eulogy. At present, Congress plans to stop funding the $3 to $4 billion a year orbital behemoth in 2024. This is mostly so NASA can focus on other big money projects, like sending humans to Mars. Many smart, hopeful experts in and outside of the government have suggested extending this lifespan by courting commercial activity.
Notably, Casis, the nonprofit that is ostensibly in charge of commercializing the ISS, wasn't at the meeting. Jeffrey Manber, CEO of NanoRacks—the only really successful business operating on board the ISS—was. His request was simple: Be clear about when the ISS will close down. "No matter what the end of operations date, the private sector needs to hear what that date is, rather than keeping it ambiguous," he says. For him, this isn't about getting an extended government handout—Manber is outspoken about the fact that he doesn't ask for NASA funding. He says that time will be necessary for him and other space capitalists to plan ahead so foreign governments don't encroach on all the robust services the ISS offered.
In a nutshell, that is space capitalism, the American way.
The private American companies battling it out for a $3.5 billion NASA contract have one last chance to successfully launch their spacecraft before a decision is made in January.
For a species whose numbers show no signs of collapsing, humans have a shockingly high mutation rate. Each of us is born with about 70 new genetic errors that our parents did not have. That’s much more than a slime mold, say, or a bacterium. Mutations are likely to decrease an organism’s fitness, and an avalanche like this every generation could be deadly to our species. The fact that we haven’t gone extinct suggests that over the long term, we have some way of taking out our genetic garbage. And a new paper, recently published in Science, provides evidence that the answer may be linked to another fascinating procedure: sex.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
For about three decades, one of the senior authors of that paper, Alexey Kondrashov, a biologist at University of Michigan, has explored how populations might shed such mutations. The question poses more of a conundrum than you might think. One model of natural selection is that it acts on mutations one by one: letting this one stay, forcing that one out. Another, though, is that the fates of mutations can be linked—an effect that population geneticists call synergistic, or narrowing, epistasis. This might happen if having one mutation can compound the effects of another: for instance, a system that’s able to limp along with one defective piece will fail with the loss of a second or a third. In this way of thinking, for an individual, having more mutations is not just additively worse, but closer to exponentially worse.
To Kondrashov and others, that prediction suggests an escape route from the trap of rapidly accumulating mistakes, both for humans and other multicellular organisms prone to mutations: As the number of nasty genetic errors in a population rises, natural selection will sweep large rafts of them out of the genome together. And in sexual organisms, because of the ways that mutations from each parent can recombine randomly onto the same chromosomes, the synergistic elimination of bad mutations can happen even faster.
Kondrashov has investigated the implications of synergistic epistasis with theoretical studies. Other researchers have taken the experimental route, trying to detect whether, in real life, mutations can interact with each other this way. Those tests yielded mixed results, though, perhaps because the effect would not have to be very large to keep a population from succumbing.
Now, however, Kondrashov and his co-authors have put together a statistical case, pulled from the genomes of about 2,000 people and about 300 wild fruit flies, that the effect has been quietly acting on us and other organisms all along. Drawing on knowledge of the species’ mutation rates and other factors, the scientists began by calculating what the distribution of mutations in populations of humans and flies ought to be in the absence of this purging effect. Certain numbers of individuals in the group, for example, ought to show 100, 50 or 30 mutations. Then the group of researchers turned to the genomic data, looking for the distribution of mutations in real-world populations.
What they found was that significantly fewer individuals than expected had large numbers of dangerous mutations. They are missing from the population, “suggesting that at the high end, at the end where people have many deleterious mutations, there’s stronger selection against these people,” said Arjan de Visser, an evolutionary geneticist at University of Wageningen who was not involved in the work. This observation fits well with what should happen if mutations are not acting independently.
That finding comes with some caveats. There does not seem to be any shrinkage in the number of individuals with less-than-devastating mutations, cautioned both Kondrashov and Shamil Sunyaev, a computational geneticist at Harvard Medical School and another senior author of the paper. “We don’t see it for the whole genome,” Sunyaev said, although the decrease is there “at least for mutations that are undoubtedly deleterious in effect.” The team would also like to get better data on the consequences of mutations in parts of the genome that don’t make proteins. That would let them run their statistical tests again with more confidence that the interactions are occurring more broadly.
Still, the evidence is provocative, and the idea elegant. “I always found it quite attractive, biologically,” said Brian Charlesworth, an evolutionary geneticist at University of Edinburgh who was not involved in the study. “If you think about someone getting hit on the head with a hammer, the first few blows might not do you too much harm, but after a while it will finish you off.” Of the new work, he said, “It’s really the first study which comes up with evidence from what’s going on actually out there in natural populations.”
Perhaps the most interesting corollary of this finding, however, is that it might help explain the persistence of sex. Among population geneticists, sexual reproduction is notoriously difficult to justify as an evolutionary strategy. As a sexual organism, even if everything goes well—if you manage to find a mate who accepts you, if you manage to conceive—you will still be passing on only half of your genes. An asexually reproducing organism, having daughters by making perfect copies of itself, gets double the benefit, none of the hassle. Yet clearly, sex continues.
The redeeming feature of sex, when it comes to evolution, seems to be that it shuffles the parents’ genes together in endlessly new combinations. Unless you have an identical twin, none of your siblings are just like you. And each of your sperm or egg cells carries a mish-mash of your own genes, so none of your children will get the same thing. Sex leads to greater variety for natural selection to work with, a wider palate of quirks, abilities, shapes and sizes that might be fitted to the situation at hand.
The benefits of this arrangement may exceed the costs, though, when there is some efficient way to get rid of the real genetic disasters. And that’s where this new work comes in. Dangerous mutations can be wiped out from the population en masse only if they happen to get shuffled together, thanks to sex, into the same individual. That unlucky “individual” loaded with bad mutations could be a sperm cell that’s not fit enough to ever reach an egg, or an organism that is not healthy enough to ever reproduce. Either way, that combination of mutations would drop out of the population, never to be passed on.
At one stroke, then, a large mass of worrisome problems—brought together by sex, then doomed by their associations with one another—would be culled from the gene pool.
Nearly 30 years ago, Kondrashov, then a scientist in the Soviet Union, wrote a paper for Nature that pointed out this process, now called the deterministic mutation hypothesis, could help to justify sex. “The [genetic profiles] that are eliminated can contain many mutations, which may give a sexual population an enormous advantage,” he mused in the paper. In an asexual population, because the members are genetically identical, natural selection can’t purge bad mutations rapidly without killing everyone.
Speaking from his summer research base near Moscow, Kondrashov said he hopes to see more experimental verification of the interactions between mutations. “Before it’s replicated on a number of species, I’m reluctant to say that we made a discovery,” he said dryly. “But I can’t think of any other explanation.” Next he plans to raise a carefully controlled population of fruit flies in which the genetic variation among individuals is known from the beginning, and then to run selection experiments to see in more precise detail exactly how it changes over time.
Helpful Mutations Didn't Sweep Through Early Humans
Signals of Natural Selection Found in Recent Human Evolution
Human Genome Still Chock-Full of Mysteries
Furthermore, the statistical test the group uses should be applicable to any population where researchers have some basic information to plug in, de Visser noted. It would be relatively straightforward for other scientists to apply it and see if they can uncover similar interactions in other human or animal populations.
It is easy to assume that, in an era with modern medicine and agriculture, we humans have somehow escaped the grasp of natural selection. But this glimpse into the mutational landscape of the human genome shows selection may still be acting on us without our noticing it, even as our numbers boom. These absences in the population, these empty places at the high end of the mutational distribution—these may be selection’s fingerprints on our DNA.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Suppose someone builds a wall. A great and tall wall that is both impenetrable and beautiful. Who knows—maybe it's even solar powered. This wall stands 10 meters tall and goes on and on and on.
Now suppose someone wants to toss a bag of stuff over that wall. A big bag with a mass of, oh, 60 pounds. (I will say 27 kilograms, because kilograms are better.) How much force must be applied to get this bag over that wall? And what happens if the bag bonks someone on the other side?
Yes, this is a two-part question. We do that a lot in physics.
I find that the best way to start just about any physics problem is to diagram it. Visualizing the problem helps determine just what you need to solve for and what you already know. Plus, I like making diagrams.
Yes, I know the diagram is not to scale. Don't worry about that. The key thing to consider is that the guy (or hombre—good or bad, you decide) must push this bag of stuff some distance s to get it up to speed. The sack then moves up to a height h so that it goes over that tall, glorious wall. Right here we should be able to see a path to the solution. We can solve this problem using the work-energy principle because we don't care about time, only distance. This principle says that the work done on a system is equal to the change in energy of that system.
If I choose a system consisting of the sack and the Earth, we will see two types of energy in this system: kinetic energy and gravitational potential energy.
Here g represents the gravitational field with a value of 9.8 N/kg. But what performs the work on this system? The man (or hombre), of course. As the fellow pushes on the sack of stuff, he applies a force (F) over some distance (s). The angle between this force and displacement would be zero degrees such that the cosine would be 1. And so the total work is:
Now for the change in energy. If I consider the sack at the moment just before it's thrown the starting point, it starts with a kinetic energy of zero joules. For the second point, I will use the highest point in the path at the sack clears the great and awesome wall. Here, too, the sack has a kinetic energy of zero joules. Therefore, the change in kinetic energy is zero joules.
For the gravitational potential energy, let's make the starting point y = 0 meters such that the initial potential is zero joules. At the highest point, this gives the sack a potential of mgh.  Making this work equal to the change in energy, I can solve for the require throwing force:
Now we plug and chug. Using a sack mass of 27 kg and a beautiful bodacious wall height of 10 meters, I just need a value for the throw distance. Let's be generous and give it a value of 1 meter—using both arms and legs to increase the distance of the throw. This would require an average throwing force of 2,646 newtons, or almost 600 pounds. That makes this one tough sack-throwing hombre, someone not to be messed with.
But wait! But what if that hombre hurls that sack over that mighty wall and bonks someone on the head? How do we find out what happens. Well, the physics is exactly the same, but backward. If you caught this sack on the other side with a catching distance of 1 meter, you would need an average force of 2,646 Newtons.  If it hit you in the head, it might stop over a much shorter distance of about 0.25 meters.  In this case, there would be an impact force of 10,584 newtons. Bam. That would hurt. Hopefully someone gives this massive magnificent wall some windows so you can see the sacks being tossed over.
Among the many reasons humans are bizarre among mammals (the dearth of body hair, the bipedalism, the fact that someone invented the turducken) is a sad shortcoming: You and I don’t have sensory whiskers. Cats, dogs, raccoons, sea lions—you name a mammal and it’s probably got special hairs sprouting out of its face. After all, whiskers are immensely useful. Rats use them to navigate the darkness, for instance, while a seal's whiskers detect the movements of fishy prey.
Whiskers are all the rage in nature, so why not give them to robots? Mechanical engineer Mitra Hartmann of Northwestern University is doing just that. In a new paper published in the journal Soft Robotics, Hartmann and her team detail how they’ve pulled one step closer to a rat-like machine that can feel an object and pinpoint it in 3-D space. Meaning robots of all kinds could soon get a powerful new sense.
In animals, the whisker itself doesn’t have sensors running along its length—they're packed into the follicle instead. When the whisker hits an object, those sensors trip and relay information to the brain, acting as a supplement to the critter’s other senses.
If you wanted to give a robot whiskers you could use something called a six-axis load cell. Connect a whisker to this device and it can relay lots of information when the appendage hits an object: lateral force, how much the whisker twists in the follicle, how much it pushes into the follicle, in what direction the whisker bent, and how much it bent. That's a whole lot of information, and accordingly that kind of cell is bulky and expensive.
Working in a simulation, Hartmann and her team discovered that you don't actually need all this information to pinpoint objects with a whisker. “You just need how much the whisker bent, what direction the whisker bent, and how much it got pushed into the follicle,” says Hartmann. She's currently working on two versions of a sensor that can measure those three signals. At .8 mm cubed, the first is about half the size of a macro-scale six-axis load cell. The other is 1 mm long and 0.5 mm in diameter—about the same size of the follicle of a real-life rat.
Meet Salto, the One-Legged Robot With an Incredible Leap
Finally, the Robot Bat We Deserve and the Robot Bat We Need
Soft Sensors Might Make Wearables Actually Wearable
The team also looked at the effectiveness of two different whisker designs: tapered and plain old cylindrical. Tapered is what you'd find in nature—thicker at the base and ending with a pointy tip—and indeed the team found that they could pinpoint objects with tapered whiskers but not cylindrical ones. Why, exactly, Hartmann can't say. But she ventures a guess. "If the whisker were cylindrical, then at every point along the whisker's length the stiffness would be the same," Hartmann says. "The tapered whisker is more flexible in some regions than other regions—it's more flexible at the tip."
So, now Hartmann knows what signals and what whisker shape she needs to build a rat-like robot. The question then becomes: Why whiskers? Robots can already map their surroundings in incredible detail using lasers. Why complicate matters with funny-looking face hairs?
The idea is that whiskers aren’t a replacement for machine vision—they’re a complement. Once night falls, traditional vision is no longer an option. And when a dust storm moves in, lasers are right out. So in these trying times, the robot could feel its way around instead. Whiskers could even detect currents like a seal, making for a powerful underwater robot.
Another upside: Whiskers are a sneaky way to map your environment. “What if you wanted to avoid detection?” Hartmann asks. “You wouldn't want to go blazing light all over the place, right? You'd want to be sneakier than that.”
So does that mean one day you'll get a whiskered robot cat instead of a real one, à la Blade Runner? Maybe. If it comes without the attitude problem, count me in.
Robots can learn to do tasks just fine. Getting different kinds of robots to share knowledge, though, is another challenge entirely.
Early this morning, a white Mercedes Sprinter van began a delivery route along the streets of Fancher Creek, a residential neighborhood on the southeastern edge of Fresno, California. Its cargo? 100,000 live mosquitoes, all male, all incapable of producing offspring. As it crisscrossed Fancher Creek’s 200 acres, it released its payload, piping out swarms of sterile Aedes aegypti into the air. It’ll do the same thing tomorrow, and the next day, from now until the end of December.
Though counterintuitive, the goal of this daily mosquito dump is actually to get …  _fewer_mosquitoes. Specifically, fewer female Aedes aegypti, the ones that bite and lay eggs and transmit diseases, including the United States’ newest scourge: Zika.
That mission, to “Debug Fresno,” is emblazoned across the van’s driver side. Behind its wheel is an employee from Verily—the secretive Alphabet spinout formerly known as Google Life Sciences. Verily partnered with MosquitoMate, a Kentucky-based sterile mosquito breeder, and Fresno’s local authority, the Consolidated Mosquito Abatement District, to release a million insects a week through the end of 2017. It’s the largest sterile mosquito trial in US history.
The scale of the project isn’t the only thing Googlian about it. Lest you think pest control is a step down for the company behind the ambitious Baseline Study and operating rooms staffed by surgical robots, Verily has turned one of the buildings on its South San Francisco campus into an autonomous skeeter factory capable of producing 150,000 bugs daily. Raising mosquitoes isn’t easy. You need precise conditions—the right levels of heat, humidity, and light—and because they grow so fast they need to be watched 24/7. It’s a laborious process for humans. But perfectly suited to robots.
Verily’s larval-rearing robots live inside a self-contained factory system a few thousand square feet in size. The system mostly runs on its own, syncing up feeding cycles and monitoring growth. Then, every day, humans take out a batch of pupae that are ready to be sorted into males and females. This is the most critical step: In order for eradication to work, Verily has to be 100 percent sure to release males and only males.
That’s because every mosquito here was bred to carry a bizarre bacterium called Wolbachia. When a Wolbachia-carrying male mates with a wild, local female, none of their offspring will be able to hatch. If the sterile males can outcompete the wild ones long enough, eventually, no more mosquitoes. But—and it’s a big but—if a bacterially-infected male mates with a bacterially-infected female, the two wrongs make a right, and the eggs will hatch into healthy mosquitoes. So, you gotta get the sex-sorting right.
Historically, the sex-sorting has fallen to humans. University of Kentucky entomologist Stephen Dobson, who patented how to make Wolbachia-carrying mosquitoes back in 2005, has gotten really good at spotting the differences between male and female. But it’s still a total pain in the ass. Dobson founded MosquitoMate in 2013, and the company ran a much smaller trial here in Fresno County last summer. His team used a sieve-like device to sort the sexes by size in their pupal stage, and then gave each mosquito another look under a microscope before shipping them from Kentucky to California, where district officials released the bugs by hand, shaking them out of cardboard tubes. He knew that scaling up was going to take some serious engineering, so when he met Verily’s Jacob Crawford at an entomology symposium, it seemed like a natural partnership.
A California City Is Fending Off Zika by Releasing 40,000 Mosquitoes Every Week
These Scientists Saw Zika Coming. Now They're Fighting Back
Florida Votes to Release Millions of Zika-Fighting Mosquitos
While Crawford was very light on details about its custom-designed autonomous sex-sorting set-up, he did say that it’s a two-step process that leverages the company’s computer vision technologies. “Achieving sex sorting at any scale has been an enormous hurdle,” says Crawford. “This is the stuff entomologists dream about.”
Dobson agrees. This year’s trial will put 25 times more mosquitoes on the streets of Fresno than MosquitoMate was able to do on its own last year. That level of coverage could actually kill mosquitoes in a wide enough range to protect people as they move around between their homes and work and school. “At this scale you’re starting to get into the level where you can actually have an epidemiological impact on disease transmission,” says Dobson. And while Fresno doesn’t currently have any of the diseases that Aedes aegypti carries around with it—Zika, dengue, yellow fever, chikunguya—all it takes is one infected visitor for the disease to establish itself in the local mosquito population.
Aedes aegypti arrived in California in 2013, and since then, it’s already spread from Fresno to places like Los Angeles and San Francisco counties. “This invasive species has really changed everything about mosquitoes in California,” says abatement district director Steve Mulligan. “They’re expanding ranges, and they don’t respond well to conventional control methods. When you add that along with emerging diseases it’s a real challenge.” If this summer’s trial proves effective, Mulligan and Dobson hope they could expand to other places in California and wipe out the existing pockets of Aedes aegypti before they become a permanent feature of the landscape.
Verily, on the other hand, is already thinking beyond California, and even the US. Its ultimate goal is to be able to make more mosquito factories, ready to ship all over the world whenever a new mosquito-borne disease strikes. They would provide a steady supply of sterile males to local public health officials fighting the next big outbreak. Verily’s Debug the World Tour might start in Fresno, but there’s no saying where it will lead next.
Scientists in California are breeding and releasing mosquitos into Zika hotspots. While it may seem like they're making matters worse, they are actually releasing a kind of biological trojan horse.
Update: Google's Transparency Project has posted an addendum to its Academics Inc. report.
Earlier this week the Wall Street Journal published a detailed investigation showing that Google has been systematically paying academics to publish research favorable to the company’s policy and business positions—often without disclosure of the financial relationship. Concurrently, an organization called the Campaign for Accountability published a report from its Google Transparency Project showing the same thing, and naming many more as the recipients of direct or indirect Google dollars.
It’s the kind of thing that would seem to contravene Google’s one-time exhortation “don’t be evil,” and the Journal article seems to have the company nailed. But in the case of the Google Transparency Project, things aren’t that simple. Several academics named in GTP’s database say that they’ve never received money from Google—that they’re innocent dolphins caught up in what was, best case, a methodologically sloppy tuna net.
In at least one instance, the GTP database captured a scholar with no financial ties to Google. Annemarie Bridy, a professor of law at the University of Idaho, got named because of her status as an affiliate scholar at Stanford’s Center for Internet and Society. Stanford CIS does list Google as one of its funders, but the center’s director, law professor Barbara van Schewick, confirms that it has no financial relationship with its affiliate scholars. “The idea that because I have those affiliations I’m somehow tainted by a relationship to the donors to those centers, I think is ludicrous,” Bridy says. “They said I receive indirect funding. That is a verifiably false factual claim harmful to my reputation, which is pretty much the definition of libel.”
Bridy isn't the only one. Van Schewick herself shows up in the database, owing to the Google contribution to CIS. Like Bridy, she has requested the Google Transparency Project remove her name—CIS doesn’t pay her salary. Stanford Law does.
Aaron Perzanowski, a law professor at Case Western Reserve University, seems to have been slurped into the database only by (dubiously-interpreted) association. Perzanowski says he has never taken money from Google, but his frequent co-author Jason Schultz once worked for the Electronic Frontier Foundation and then for the Samuelson Law, Technology, and Public Policy Clinic at UC Berkeley—both of which have indeed gotten Google dollars. But it was in the form of cy pres payments, basically a judge taking money from a class action settlement and directing it to some other recipient. “I suppose in some sense you could say that Google funded that center, perhaps unwillingly, right?” says Perzanowski. “That’s where this idea of indirect funding gets really messy. I’m not suggesting there’s an easy answer one way or the other, but this is saying individuals who at one point two jobs ago worked for an organization like the University of California that received funding from Google are forever tainted.”
It’s not clear how far back and how deep such connections should be to register as compromising. What warrants disclosure? Casey Fiesler, of the Department of Information Science at the University of Colorado Boulder, is in the database because of a Google Policy Fellowship she had in 2011, which paid some of her living expenses while she was working for Creative Commons (another recipient of Google funds) after law school. Fiesler says she doesn’t even remember if Google or CC determined she’d get the money. “I never worked for Google, and as far as I know they had nothing to do with the work I was doing at Creative Commons,” Fiesler says. “And the work I did that summer had nothing to do with the research referenced in the database.”
Here’s where “indirect” and the lack of clarity on what constitutes a disclosure-worthy association become complicated. Professionals are terrible at judging what kind of influence even a small gift or exchange can have on later behavior. And disclosure is a poor metric for potentially unethical behavior.
Still, though, a grad school fellowship is a long way from prospective pay-for-play. “The only way I can think of that would in any way relate to me is that Google is finding impressionable graduate students and giving them money so that maybe when they do policy work later they’ll favor Google,” Fiesler says. It does sound unlikely.
The Campaign for Accountability buys some but not all of that. “A lot of the emails I’ve received sort of don’t change my mind. I think we’ll add what they say about it or their defense,” says Dan Stevens, the Campaign’s executive director. “We’re working on a post or something we might add to allow these folks to voice what they say.” Stevens says that should happen in the next few days.
Bridy, he says, received what might be described as an indirect benefit through her association with the Stanford Center for Internet and Society, but “given that she’s not receiving any payments, it seems like something we can update in the database.” Perzanowski’s connection to Schultz, he says, is harder to figure out. “I think this is one where we’re going to have to think about how we disclose accurately.” And as for Fiesler? “Yeah, I think that’s fine,” Stevens says. “There’s a specific relationship between her and Google. She received direct funding from them.”
The Wall Street Journal article didn’t include Bridy, van Schewick, Perzanowski, or Fiesler. A person familiar with the article’s reporting (who wasn’t authorized to speak for the paper and so asked to go unnamed) says the writers, Brody Mullins and Jack Nicas, went through GTP’s list and retained entries, adding their own academics and throwing out many of GTP’s. Mullins and Nicas did their own investigation and analysis out of due diligence, but also because they were aware that Campaign for Accountability had a history of going after Google specifically.
Stevens says as much. “Our point is to say, look at Google. They’re funding people to extract this result,” he says. “Building this database to show their influence campaign, that’s the point. We don’t want to impugn anybody specifically. The larger issue is Google’s attempt to influence all these academics.”
Even the people who don’t think they should have been in the database agree that the pay-for-play the Journal uncovered and GTP wants to talk about is a pernicious, serious problem. “I think the issue is so important that I don’t take any money and I wouldn’t take any money that could be seen as compromising my independence,” Bridy says.
Perzanowski says he had the same reaction to the initial reports as any casual reader. “I saw the headlines and thought, ‘This is bullshit, people shouldn’t be out there taking money from Google and not disclosing. Who are these bad colleagues of mine?’” he says. “And I pull up the database and, like, that’s my name.” He adds: “Because of the sloppy way this was done, it’s distracting from a really important issue.”
“Sloppy” is, in fact, a word that came up again and again in my reporting. But so did a question: Who funds the Campaign for Accountability? I should have checked before I wrote my initial story, and didn’t—which I'm embarrassed about, because it turns out the organization, dedicated to transparency in financial affiliations, does not disclose its financial affiliations. “It’s just always been our policy since the beginning not to disclose our funding sources,” Stevens says.
Isn’t it ... ironic? “Of course I’ve heard that before. I would just say that we’re not a major company trying to extract things from government and policymakers. We have a different purpose,” he says. “Our statement from the beginning has been, let the work speak for itself.”
As a 501(c)(3) nonprofit, the Campaign for Accountability should have an Internal Revenue Service form 990 detailing its funding. It doesn’t. “We started in May of 2015 as a project of the New Venture Fund. They spun off some of their projects into the Hopewell Fund, which we were part of through 2016,” Stevens says. “We’ve been a standalone project through 2017, so because we are still new and getting spun up, we don’t have a 990 yet.”
Prior reporting has said that at least one funder of the organization is Oracle, which—perhaps unrelatedly—has been locked in a bitter legal battle with Google, the outcome of which could bring one side or the other billions of dollars.
That all makes it hard to draw conclusions about whether the Campaign for Accountability made the best-possible decisions with messy data, made bad decisions with messy data, or had some other motive. “Maybe they don’t care about the academics. I think what they probably care about is making Google look bad by whatever means they can,” Perzanowski says. “We’re not the target. We’re just sort of the ammunition.”
The first semester of an undergraduate physics course invariably spends a lot of time on two big ideas: The momentum principle and the work energy principle. Both deal with forces acting on an object, which often leads students to think they are similar. In a way, they are, and they play a huge role in almost everything you learn during an introduction to physics.
Before I give you a great physics question that uses these ideas, I will go over them in a super-brief physics lesson. First, the momentum principle says that a net force changes the momentum of an object where the momentum is the product of mass and velocity. Working in one dimension to avoid dealing with vectors, I can write it like this:
If you consult your introductory physics textbook, you'll see that this is essentially the same as Newton's Second Law, which states that the net force is equal to the product of mass and acceleration (where acceleration represents the change in velocity). You can rewrite the momentum principle to solve for the change in momentum (which is useful).  It looks like this:
Trust me, you'll find this equation useful in just a little bit.
OK, now for the second big idea, the work energy principle. It states that, for a single particle, the work done on an object is equal to the change in kinetic energy. Work is defined as the product of a force in the direction of a displacement. I can write this as:
Just to be clear, Δr represents the displacement (how far the force pushes something) and θ represents the angle between the force and the direction the object moves. As with the momentum principle, I can rewrite this so it looks a bit more useful:
Let's take a second and look at these two ideas. Two things differentiate the momentum principle from the work energy. First, it is technically a vector equation because the momentum of an object depends upon its direction of movement. Second, the momentum principle depends upon the change in time (this is important). The work energy principle depends only on displacement, not time.
OK. Now to my great physics question. Suppose a heavy truck and a light car start with the same momentum (if it makes you happy, we can say the truck has a mass three times that of the car). Both vehicles have the same force acting on them to bring them to a stop. Which one stops first?
If you want to take a moment to think about this, I'll wait.
I'm still waiting.
OK, hopefully you have an answer by now. If you like, you can check with friends to see what they think. However, since I'm not there and you aren't here, I will just share two common answers people provide.
Answer number 1: The light car stops first. Since it has lower mass, the force acting on it results in larger acceleration. This, in turn, causes the car to slow down more quickly because the truck has a large mass and a small acceleration.
Answer number 2: They stop in the same amount of time. Yes, it's true that the car has a lower mass and a higher acceleration. However, it starts with a much larger velocity since the two vehicles have the same starting momentum. In the end, both vehicles will have the same force with the same change in momentum. According to the momentum principle, they must have the same change in time.
Clearly, answer number 2 is correct. The cars stop at the same time because they start with the same momentum. Just for fun, let's create a numerical calculation for this. Of course, that requires some actual values for the mass of the two vehicles, the starting momentums, and the stopping force. We'll say the car has a mass of 10 kg (it's a really small car) and the truck has a mass of 30 kg (three times the mass of the tiny car). The initial momentum is 20 kg*m/s and the stopping force is 2 newtons.
A plot of the x-velocity for the car and the truck looks like this:
You can see that the car does indeed start with a higher velocity, but both cars stop at the same time. Yes, this is a plot of velocity vs. time instead of distance vs. time for a very particular reason.
Now for the next (and more interesting) question. Using the same situation we examined above, which vehicle stops in the shortest distance and why? Figure it out and explain your answer. I'll wait.
Really, you should answer this one.  Take your time.
I'll enjoy this picture of a horse while I wait.
Do you have an answer? Are you sure about it? I really ought to just stop here, but I can't leave this question unanswered. I enjoy talking about it too much to do that.
Instead of explaining the answer, I will show you the answer. Here is a numerical calculation of the two vehicles stopping. It's basically the graph above, except you can see the motion of the two objects.
Just press play to run it and the pencil to see (and edit) the code. The big red box represents the truck and the small blue box is the car. You'll notice the two vehicles leave a trail of dots. I did that so you see how fast they are moving. An arrow represents the velocity of the car.
Clearly, the red truck stops first. Let me explain why. When determining the time required to stop an object, it makes sense to use the momentum principle since it deals with time. To find the distance it takes an object to stop, I must use the work energy principle. Since the two vehicles will have the same acting force on them, I can compare stopping distances by looking at the change in kinetic energy. If the vehicles started with the same kinetic energy, it would take the same amount of work to stop them. With the same force, this would be the same stopping distance.
The fact the two vehicles have the same starting momentum doesn't mean they have the same starting kinetic energy. The car has a lower mass, so it must have a higher velocity in order to have the same momentum as the truck.  But since kinetic energy depends upon the square of the velocity, the higher car velocity matters much more than the lower mass. The car starts with a higher kinetic energy and thus requires more work to stop it. With a greater work, the force has to be applied over a larger distance.  That's the explanation.
But wait! I have one more question for you as homework. What kind of starting velocity would the car require to stop in the same distance as the truck? No, I am not revealing the answer. You're on your own. If you change the starting speed of the car, which vehicle stops in the shortest time? It's your turn to do the physics.
Picture a rectangle of fabric cut from a standard grey t-shirt. It’s stretchier than most tees, because it’s made from a mix of nylon and spandex, not cotton. And it stands out in another way, too: If you flip back a corner of the cloth, one side has an unexpected metallic sheen.
This textile isn’t the creation of a sci-fi costume director. It’s called shieldex, and it was exactly what textile engineer Asli Atalay and her team at Harvard needed to develop a soft, stretchable, motion-measuring sensor. The metallic shine comes from silver coating the flexible fibers, so the fabric can stretch and conduct electrons at the same time. Rather than slapping silicon chips into bracelets, these electronics could give wearables more of the stretchability and comfort of the best sweatpants.
While the roboticist’s arsenal of metal components and silicon chips accomplishes a lot, softer robotic wearables could be friendlier for injuries, or older users, driving down the risks to humans while still providing help with, say, opening a jar. Think gloves that boost grip, or sleeves that act as assistive exoskeletons. “You put on a t-shirt, a sweater, a pair of socks—you could have these types of sensors embedded in them,” says bioengineer Conor Walsh, a co-author of the paper.
To make the sensors, Atalay first sandwiches two layers of souped-up fabric around a film of soft, electrically insulating silicone. Then, a trusty laser cutter slices the sandwich into whatever shape she wants. She runs a hot iron over an adhesive to attach the electrical leads—like attaching an iron-on patch to your jean jacket, except she’s sticking a tiny wire to each layer of silver spandex.
Technically, what she's building is a parallel plate capacitor—each side of the metal-plated fabric is an electrode, holding equal but opposite charges. As the fabric stretches, the insulating silicone between the electrodes thins out and the electrodes get bigger and closer together, changing the sensor's capacitance (that’s the the charge on each conducting plate divided by the voltage difference between them). That capacitance change is used to measure how far the fabric stretches. And voila: a batch of stretchy, flexible motion sensors.
When Atalay and her collaborators attached these sensors to the fingers of a glove, they registered capacitance changes between different hand positions. Walsh imagines that a sensor integrated into a t-shirt would measure heart rate. Though it's not something you should expect to see on shelves soon: “We’re not quite at the put-it-in-the-machine and wash it for 20 cycles stage yet,” Walsh says.
The Robots Are Coming for Your Heart
Soft Robot Exosuits Will Give You Springier Steps
MIT Prof Invents a Squishy Material for Shape-Shifting Robots
Full-on roboclothes will also need other infrastructure to support these stretch-tracking sensors. A gripper would need actuators to provide oomph (Walsh’s lab has some in the works), and then chips for “wireless communication, data storage, and power, so that your glove is truly a fully integrated wearable system,” says Sheng Xu, a soft electronics researcher at UC San Diego. Xu has worked on stretchable lithium ion batteries, and other groups continue to make new types of optical fibers, Bluetooth antennas, and processing chips that are smaller and more flexible.
Other groups have made stabs at stretchy sensors before: They've tried carbon nanotubes, graphene, and liquid metals as the conducting electrodes in similar devices. But Walsh is excited that their process is capable of forming many sensors at once, rather than building just one sensor at a time.
Mass production is exciting, because stretchable electronics are geared to alter other human-machine interfaces, too. In Xu’s view, “the virtual world is also basically electronics,” so more sensors like these could crop up in VR gear. And inflatable robots, or the inflatable space dwellings that NASA is testing, would benefit from neatly integrated sensors in their fabric structures. Now that's metal.
A robotic heart points the way to a future where soft robots help us heal.
I went to Antarctica 20 years ago, and I didn’t care about ice shelves. I noticed one at last when the blinding white of the ice, struck up against an abidingly black ocean, made me understand at last why the penguins all around me and the orcas occasionally surfacing a few dozen feet away had the same basic color scheme. Evolution ain’t stupid.
But evolution isn’t smart, either, or we humans would be much better at perceiving patterns without such obvious visual clues. Like, when a 1.1 trillion-ton, 2,200-square-mile piece of ice breaks off of the Antarctic Peninsula—the fiddly spit-curl in the upper left1 of most maps of the continent—we might be able to see it not just as megasized glaciological action but as yet another piece of the global weirdness, increasing in magnitude and frequency, that tells us Earth is getting hotter, the seas are rising, and we are all in trouble.
Alas, no.
On its own, a massive iceberg unconsciously uncoupling from the Larsen C ice shelf won’t raise sea levels along the world’s coastlines—the newly-calved iceberg was already floating. Researchers from the UK-based Project Midas have been watching the region for decades and expected the break-up; they were there to be supportive just as they were when Larsen A split in 1995 and Larsen B collapsed in 2002. (Some peninsulas just fear commitment.)
I don’t think people are stupid. I think they recognize there’s a broad, systemic change happening. What used to be an esoteric concept is now something that hits home.
Michael Brune, Sierra Club
Hey, the break-up might not have even been due to climate change. “Although this is a natural event, and we’re not aware of any link to human-induced climate change, this puts the ice shelf in a very vulnerable position,” said Martin O’Leary, a glaciologist at Swansea University, in a Project Midas statement. But even if climate change didn’t make Larsen C fall off, it will make the potential consequences that much worse. “This is the furthest back that the ice front has been in recorded history,” O’Leary continued. “We’re going to be watching very carefully for signs that the rest of the shelf is becoming unstable.”
In 1997, over two weeks in Antarctica—a few days at McMurdo Station, a couple days at the old South Pole Base, and a few more days in the McMurdo Dry Valleys—I was much more interested in reporting on the place as an analog for an alien landscape. I went looking for microbes that could live without water for months at a time. I watched technicians bore into ice to install strings of glass balls like Christmas ornaments that could detect tiny blue flashes of Cherenkov radiation caused by subatomic neutrinos passing through the continent. Talk of the Western Antarctic Ice Sheet and various glacial rivers bored the hell out of me, to be honest. As usual I chased things that sounded like science fiction—only true.
So it’s appropriate, I guess, that it took science fiction to explain why I was being a dope back then. Kim Stanley Robinson’s latest novel New York 2140 is set in a flooded, Venetian Manhattan—catastrophic sea level rise having been induced by the failure of all the various ice shelves and sheets in Antarctica, which in turn allowed all that other ice that carapaces the continent to slide into the warming seas. Oh, says me! That’s why we’re supposed to care about ice shelves.
Yet we still kind of don’t. Even if climate change didn’t send Larsen C packing, the air and oceans on Earth are incontrovertibly warmer than they used to be. That makes it less likely that the Larsen ice will ever bulk up again, and the newly exposed shelf even more vulnerable to the lapping sea. Might this calving galvanize action to fight climate change? “The only appropriate answer is, who knows? It’s been less than a day,” says Michael Brune, executive director of the Sierra Club. “I don’t think people are stupid. I think they recognize there’s a broad, systemic change happening. What used to be an esoteric concept is now something that hits home, literally.”
The question is, can an event like Larsen C move a policy needle? What makes something into a focusing event that opens a policy window, or even just a teachable moment that might shift the positions of the 25 percent of Americans whose opinion on climate change ranges from “meh” to “conspiracy?” “This particular event is very important from a climate science perspective,” says Tony Leiserowitz, director of the Yale Project on Climate Change. "But it’s happening within a very complicated political-economic-cultural landscape where there are already well-entrenched positions, where different audiences exist, and where people will either hear about or not hear about this because of their different media sources or networks.”
Scientists, activists, and journalists all tend to race for the existentially dreadful bottom at times like this. It’s a little ironic, considering that just a few days ago a New York magazine article charting Earth’s impending climatic doomsday took heavy fire from climateers themselves, who, like apologetic wingmen and -women for a drunken friend at a party, quickly tried to minimize the damage. It probably won’t be that bad, it might never be that bad, there’s still time to fix this. In this field, tradition demands a certain restraint when you’re pitching doomsdays—as Elizabeth Kolbert named it in the New Yorker, erring on the side of least drama.
Map Shows Where Sea Level Rise Will Drown American Cities
Britain's Antarctic Research Station Looks Like a Spaceship
The Break in the Larsen Ice Shelf Is Bad for the Planet, But Huge for Science
Is banging a climatic-disaster drum about Larsen C’s viking funeral “dramatic?” Maybe. I can handle the cognitive dissonance of simultaneous apocalyptic despair and hope because of upward spikes in hybrid automobile sales, wind and solar energy, and lots of civilized countries agreeing to cut greenhouse gas emissions. “All responsible scientists are saying, look, there are some very serious impacts coming our way, but we do still have choices, and let’s act on those,” says Rachel Cleetus, lead economist and climate policy manager at the Union of Concerned Scientists. “This is not just gloom and doom. This is a moment that should galvanize us to action, and we should push our policymakers to take those actions.”
Inland from the ice shelves, in the McMurdo Dry Valleys of Antarctica, the Austral summer turns bits of glaciers into transient meltwater ponds. In these rocky, shallow pools, tiny bubbles of microbes lurch, temporarily, back to life. No matter how frozen some Earthly biome may seem—a pond near Seuss Glacier or a Capitol—a little sunshine always has a chance to spark a miracle.
1 UPDATE 7/13/17 3:40 PM Corrected to reflect the correct map placement
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.
Last month, Phoenix enduring a blistering heat wave, with temperatures so high that airport officials had to cancel dozens of flights. The reason was two-fold. First off, some jet engines risk catching on fire in extreme heat. And when air gets hot, it expands and becomes less dense—so an airplane’s wings can’t generate enough lift to get off the ground. Planes either need to speed up during take-off or use a longer runway.
But Phoenix’s flight delays aren’t a one-off event. As the Earth’s climate undergoes a 1 to 3 degree Celsius warming over the next half-century, extreme heat waves will hit more frequently. Some of these heat waves will hit airports with short runways. Forget about rain delays or missing flight crews. At places like Washington’s Reagan National Airport, New York’s LaGuardia Airport, and Dubai International, the real trouble will come during heat waves.
During the hottest part of the day, 10 to 30 percent of planes will have to offload cargo or people, according to a new study by graduate student Ethan Coffell and climate scientist Radley Horton at Columbia University. “This study shined a light on a potential vulnerability,” Horton says. “A lot of airplanes at full capacity are ill-equipped to take off on some of the world’s runways when temperatures get really high.”
The scientists looked at five common commercial airplane models—the Boeing 737-800, Airbus A320, Boeing 787-8, Boeing 777-300, and Airbus A380—and calculated how their takeoffs would be affected at 19 airports around the world, based on projected temperatures from 27 different global climate models. The runways represented the most common climates, elevations, and runway conditions at busy airports around the world—including US airports in Denver, Phoenix, Chicago, Atlanta, New York, Washington, DC, Los Angeles, Houston and Miami.
The good news for air travelers is that London, Paris, and JFK in New York will be able to shrug off the worst of future heat waves. But a Boeing 777-300 departing from Dubai at the time of the daily highest temperature may be weight-restricted about 55 percent of the time according to the study, which appears today in the journal Climactic Science. Because of the short runways, airplanes at Washington’s Reagan-National (7,170 feet) and New York’s LaGuardia (7,000 feet) will also have to lighten their load to get off the ground. Expanding the runways probably won’t work, given that they are either sandwiched along the Potomac River or Jamaica Bay.
Why Phoenix's Airplanes Can't Take Off in Extreme Heat
It's Cheaper for Airlines to Cut Emissions Than You Think
Oh Great. Climate Change Will Make Flying Worse, Too
Heat waves aren’t the only problem facing the aviation industry as the Earth’s climate changes rapidly. Other scientists have calculated that severe turbulence—the kind that sends drink carts flying and sometimes even unbuckled passengers—will increase over certain transatlantic routes that follow the meandering jet streams.
Paul D. Williams, an atmospheric scientist at the University of Reading who published on the connection between climate change and turbulence this past year, thinks the industry needs to start dealing with climate change more aggressively. They could develop engines that produce fewer greenhouse gases, for one, as well as adapt its planes to the future world. “I’ve yet to see a benefit of climate change to aviation,” Williams says. “All the published studies have been about thing getting worse.”
There are plenty of solutions out there, some more complicated than others. Flights may have to leave earlier in the day when its cooler, Horton suggests, or aircraft manufacturers may have to make planes lighter. Or engineers could step in, building some kind of special new wings that generate additional lift. There’s one other option that probably won’t happen: leaving three or four paying customers back at the terminal in order to make the takeoff weight.
Last year was the hottest year since scientists started keeping records in the 19th century. It's no fluke---because it's humanity's fault.
Picture Jupiter. Even if you're a total space junkie, your mental image is probably an orange and white-striped planet with a big red dot in the southern hemisphere. Jupiter's red spot—a storm with a diameter larger than Earth's—has been the planet's most conspicuous feature for centuries, and was definitely the answer to a question on your fifth grade astronomy test.
But the spot itself has always been kind of mysterious. Scientists don't fully understand what created the storm, or how it's been swirling around for so long. And while they haven't figured that part out yet, NASA's Juno spacecraft has brought them closer than they've ever been before—literally. On Monday, Juno skimmed just 5,600 miles above the storm clouds, and snapped some pictures as it went. It's taken the data a few days to get back to Juno's Earthbound science team, but the images are finally here.
The Juno team posted these raw, unprocessed images on the webpage dedicated to images taken by the mission's onboard camera, JunoCam. NASA encourages Jupiter fans to edit the images themselves as a kind of audience participation gimmick, but that doesn't mean these images are the final product.
Still, the snapshots reveal hints of the curiosities to come. The big red spot seems to cause pockets of turbulence in other bands of Jupiter's atmosphere as they pass by the behemoth, though scientists are no closer to knowing how the storm maintains its energy and cohesion. Future images will include data from Juno's other instruments, which should tell scientists more about the atmospheric processes boiling away underneath the storm's brick red surface. For now, though, that surface is still a pretty good view.
Find out how NASA’s Juno Mission will help unlock the mysteries of our planet and our solar system.
E. coli might best be known for giving street food connoisseurs occasional bouts of gastric regret. But the humble microbial workhorse, with its easy-to-edit genome, has given humankind so much more—insulin, antibiotics, cancer drugs, biofuels, synthetic rubber, and now: a place to keep your selfies safe for the next millennium.
Scientists have already used plain old DNA to encode and store all 587,287 words of War and Peace, a list of all the plant material archived in the Svalbard Seed Vault, and an OK Go music video. But now, researchers have created for the first time a living library, embedded within, you guessed it: E. coli. In a paper published today in Nature, Harvard researchers1 describe using a Crispr system to insert bits of DNA encoded with photos and a GIF of a galloping horse into live bacteria. When the scientists retrieved and reconstructed the images by sequencing the bacterial genomes, they got back the same images they put in with about 90 percent accuracy.
The study is an interesting—if slightly gimmicky—way to show off Crispr's power to turn living cells into digital data warehouses. (As if E. coli didn’t already have enough on its plate, what with securing global insulin supplies and weaning the world off fossil fuels.) But the real question is, why would anyone want to do this?
To the left are a series of frames from Eadweard Muybridge’s Human and Animal Locomotion. To the right are the frames after multiple generations of bacterial growth, recovered by sequencing bacterial genomes.
If you’re Jeff Nivala, it’s not to preserve visual messages for people in the far-off future. It’s so he can turn human cells like neurons into biological recording devices. “The E. coli is just a proof of concept to show what cool things you can do with this Crispr system,” says Nivala, a co-author on the paper and geneticist at Harvard. “Our real goal is to enable cells to gather information about themselves and to store it in their genome for us to look at later.” That concept is called the “molecular ticker tape.” It’s something George Church thought up before Nivala, a post-doc, arrived in his lab. But it’s a challenge Nivala thinks is uniquely suited to Crispr.
In case you’ve been living in a bunker, Crispr-Cas9 is a revolutionary molecular tool that combines special proteins and RNA molecules to precisely cut and edit DNA. It was discovered in bacteria, which use it as a sort of ancient immune system to fend off viral attackers. Cas9 is the protein that does all the cutting, i.e. gene editing’s heavy lifting. Lesser known are Cas1 and Cas2. They’re the ones that tell Cas9 where to do the cutting.
Church's lab plans to leverage that system to get human brain cells to show how exactly they develop into neurons. Nivala thinks they’ll be able to do that because of how Cas1 and Cas2 work. During a viral invasion, the proteins go out and grab a piece of the attacker’s DNA, which they slip into the bacterial genome for another enzyme to turn into a matching guide RNA. That’s what helps Cas9 find (and then chop up) copies of the virus in the cell. The really cool bit is that Cas1 and Cas2 don’t just insert viral DNA into the genome at random. As they encounter new threats, they add DNA in the order in which it arrives. That turns a cell’s genome into a temporal record—think ice cores for molecular history—of whatever the cell encounters.
To the left is an image of a human hand, which was encoded into nucleotides and captured by the CRISPR-Cas adaptation system in living bacteria. To the right is the image after multiple generations of bacterial growth, recovered by sequencing bacterial genomes.
One day, Nivala thinks scientists will be able to use that system to record synaptic activity. Like a guest book at a wedding, embedded signals in the genome could tell researchers exactly which neurons were talking to each other at different times, in response to different stimuli.
“If you think of a cell as a processor, this adds a thumb drive, which stores information for later processing,” says Karin Strauss, lead researcher on Microsoft's own DNA storage project. Last year the company set a new record—200 megabytes—and has plans to get a DNA storage system up and running by the end of this decade. “As for DNA data storage in the IT industry, it is more well served by standard DNA synthesis and sequencing at the moment, because they are easier to control and a lot denser than whole cells,” says Strauss, who is unconnected to the Harvard research.
Companies that make custom DNA, like Twist Biosciences, are already selling to customers using it for storage purposes. But it’s still only a small piece of their business—about 5 percent. Costs have to come down by a factor of about 10,000 before DNA becomes competitive with traditional storage methods. But the long-term benefits will be huge; properly stored in a cold, dry place, DNA can keep data intact for at least 100,000 years.
Crispr Creator Jennifer Doudna on the Promises—and Pitfalls—of Easy Genetic Modification
Scientists Capture Crispr's Gene-Cutting in Action
Easy DNA Editing Will Remake the World. Buckle Up.
That’s why scientists like Ewan Birney, director of the European Bioinformatics Institute, are working on better tools and methods to make DNA storage truly scalable. In that endeavor he doesn’t see a place for live cells, which start out at less than 100 percent accuracy and are susceptible to mutations over time that could further degrade data integrity. “It’s cute, and I wish I’d done it,” said Birney of the Nature paper. “But it doesn’t add much on the DNA storage side of things. What did impress me was the amount of edits they achieved with high fidelity. It’s a real tour-de-force of Crispr.”
So, at least for now, there’s no reason to think your family photo albums will one day be backed up on an E. coli drive. More likely, the memories cells store will be their own.
1Disclosure: One of these researchers is married to a WIRED editor.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
When ferrets get a rabies shot in a neurobiology lab, they don't get infected with the virus—or even inoculated against it. They get a brain hack that might just explain how your brain handles vision, and maybe even your other senses, too.
In a lab at Dartmouth, scientists are experimenting with targeted injections of a modified rabies virus into the brains of ferrets—essentially allowing them to control how the animal responds to simple visual patterns. The goal is to understand the brain's enormously complex visual processing system. But really? Rabies? Ferrets? Are these guys just screwing around?
Lots of visual research depends on lab mice—the most popular of model organisms in biology. But Dartmouth neuroscientist and lead author Farran Briggs wanted to study an animal that uses its vision the same way humans do, in an evolutionary sense: to prey on tasty snacks. Mice aren’t predators, and their vision falls solidly in the ‘legally blind’ range. So these vision researchers turned to the notoriously vicious ferret and its front-facing eyes. They're color blind, but at the neural level, ferrets’ visual systems have “remarkable similarities to a primate, and a human,” says Briggs. (Ferrets also help avoid the ethical issues of experimenting on primates.)
The classic approach to understanding visual circuits would be to find a way to switch off part of the system, and see what changes. That’s one reason why mouse models are great: Scientists have figured out how to breed mice with light-sensitive instructions inserted next to specific genes—a relatively new field called optogenetics—so that neurons with that specific gene will turn on when exposed to light. Biologists don't have the same systems for genetic control in ferrets, though, so Briggs and her team figured out a different way to engineer light-controlled brain bits.
In this case, they used rabies—not so much a a virulent pathogen here as an on-off switch for neurons. When an animal gets rabies, the virus sneaks into nerve cells thanks to special protein coating that acts like a secret, neuronal password, and Briggs and her team capitalized on that entry system. With help from the Salk Institute scientists who invented this technique, they plunked a light-sensitivity gene into the rabies virus and stripped off its coat-protein genes. Then they covered their modified virus in just enough protein to enter one cell, and injected it into the part of the ferret’s brain they were interested in. “Once those viruses get into those neurons, they’re stuck,” says Briggs. The virus makes the neurons light-sensitive, but otherwise keeps to itself.
The Unexpected Science of Manipulating Neurons With Light
A Cure for Blindness Just Might Come From Algae
‘Brain Balls’ Grown From Skin Cells Spark With Electricity
Specifically, Briggs and her team wanted to look at the place in the brain where the cortex whispers to another brain region called the thalamus. One of the first steps of visual processing is a feedback loop between these two regions, which starts when light bounces into your eye. Your retina signals the thalamus, which calls out to the cortex, which in turn sends a message to the thalamus via long, neuronal tendrils. “For decades, basically, we haven't really had a great handle on the role of these kind of projections,” says Ben Scholl, a researcher at the Max Planck Florida Institute for Neuroscience.
Once those projections were inoculated with rabies, Briggs and Co. could switch them on by flashing a blue LED. Tracking the cortex and thalamus' activity with electrodes, they showed anesthetized ferrets statistically controlled TV noise and watched how visual processing changed. They saw that cortex feedback made the thalamus respond more quickly and precisely than when those feedback neurons were disengaged.
Briggs had expected an amplification in the signal, but instead saw a change in the speed and quality of signaling. And although this is just one circuit in the pathway of processing sensory information, it’s possible that interactions like this one might point to design rules that apply to other feedback loops, as well, says Scholl.
So, no, these guys aren't just messing around. Understand the way the brain works is so complicated that researchers have had to come up with complicated, highly specialized methods like these to isolate pieces of the puzzle. Maybe one day, armed with more rules for these types of circuits, researchers will be able to nail down an idea of how your brain lets some information fade, while other signals get cranked to 11. Until then, you can expect lots more weird neuroscience dispatches.
With optogenetics, the ability to restore and enhance brain function is becoming a reality.  In this World Economic Forum discussion, Nature magazine neuroscience editor I-han Chou explains how the radical method works and the ethical issues it could cause.
Officially, the online search giant Google’s mission is to “organize the world’s information and make it universally accessible and useful.” According to two new reports—one from The Wall Street Journal and one from the nonprofit, nonpartisan Campaign for Accountability’s Google Transparency Project, the company doesn’t just organize. When Google wishes it had information that’d maybe help further its policy and regulatory goals, it just pays academics under the table to gin it up.
That’s pretty evil, y’all.
The assertions in both—the Journal reporters had access to an early draft of the Google Transparency Project’s report and did even more reporting—are astounding. The Journal article contends that Google financed hundreds of papers at anywhere from $5,000 to $400,000 a pop, even at times participating in the editing process. And the researchers the company worked with often didn’t disclose the relationship.
Guys. Guys. Do computer science departments not mention the thing about not subverting academic freedom with bribes in an attempt to influence legal and regulatory frameworks?
Oh, but you’re all like, “come on, don’t be so uptight! Google funds research! It’s practically an R&D institution. Machine-learning cars that search their own balloon-powered books database at gmail dot com!”
Google has a tremendous amount of power and wealth, and they really try to leverage that to get what they want from policymakers.
Dan Stevens, Campaign for Accountability
The Google Transparency Project says no. The papers Google funded expressly supported the business, covering “a wide range of policy and legal issues of critical importance to Google’s bottom line, including antitrust, privacy, net neutrality, search neutrality, patents and copyright.”
When European and US regulators started looking hard at Google for potential antitrust violations between 2011 and 2013, the number of Google-funded papers with titles like “Google? A Monopoly? Don’t Make Me, a Credentialed Academic, Laugh” spiked. The same thing happened again in 2015. In 2013, when regulators and media companies wanted to know if Google could be held responsible for linking to pirated materials, it was all papers like “Just Because You Found Something Copyrighted on Google, That’s Just, Like, Your Opinion, Man.” OK, I made those specific titles up, but still.
Then those articles and papers themselves got linked or referenced elsewhere, further muddying the trail of money, and those entire networks of pseudo-knowledge became fodder to lobby regulators and elected officials.
You know what company is very, very good at understanding network effects?
“Google is a company with a tremendous amount of power and wealth,” says Dan Stevens, executive director of the Campaign for Accountability. “They really try to leverage that to get what they want from policymakers.” At one point, his organization’s report says, Google CEO Eric Schmidt even cited to Congress a paper saying his company wasn’t a monopoly—without disclosing that Google had paid for the paper.
Here’s the Journal again:
Google has paid professors whose papers, for instance, declared that
the collection of consumer data was a fair exchange for its free
services; that the company didn’t use its market dominance to
improperly steer users to Google’s commercial sites or its
advertisers; and that it hasn’t unfairly quashed competitors. Several
papers argued that Google’s search engine should be allowed to link to
books and other intellectual property that authors and publishers say
should be paid for.
You might remember this tactic from such betrayals of the public trust as Big Tobacco covering up the link between cigarettes and cancer, or oil companies obfuscating the link between greenhouse gas emissions and climate change. And, to be fair, Google gave up on the injunction “don’t be evil” a couple years ago.
The Google website posted a response to the Campaign for Accountability report. The company took issue with the idea that any amount of funding at any time represented an ongoing influence on a person or organization. “Our support for the principles underlying an open internet is shared by many academics and institutions who have a long history of undertaking research on these topics—across important areas like copyright, patents, and free expression. We provide support to help them undertake further research, and to raise awareness of their ideas,” the statement says. Furthermore, the company says the researchers it funds have complete editorial and intellectual independence, and they’re supposed to disclose their own financial relationships to journals, conferences, and whoever else is reading their work.
That’s tricksy. First of all, while Google both funded people who already supported the company’s positions and solicited favorable research, people who take money or gifts—of almost any size—almost always end up showing favoritism to the giver’s positions. (Physicians who take gifts from pharmaceutical companies are a great example. Members of presidents’ families who meet with Russian government lawyers might be, too.)
But the second thing—the part about disclosing financial relationships—is even sketchier. In the world of banking and finance, forcing people to admit if they’re being paid to make certain claims, or getting money from the people whom those claims support, applies a coat of teflon to a transaction. Everyone having information about everyone’s priors removes some friction from the transaction.
Outside that world, in let’s say science or public policy, it’s less clear what good a disclosure does. “In an ideal world, you’d have a lot of academics studying this stuff, being funded by a university,” Stevens says. “You want untainted research. But if that’s impossible, at least disclose.” Many academic and trade associations require disclosures of financial connections from their members. Government usually does, too, unless the heads of ethics agencies deployed to watchdog such stuff quit in a fog of mindboggle.
Google's Big EU Fine Isn't Just About the Money
Digital Privacy Is Making Antitrust Exciting Again
Google Takes on Rare Fight Against National Security Letters
But even if the researchers and academics who took Google money did disclose, nothing about that disclosure guarantees fair work, ethically conducted. If a disclosure is supposed to make a reader regard work with more skepticism, how much more? And however high a disclosure of a financial interest makes you raise an eyebrow, does it stay that high for a subsequent paper that cites the first one?
It’s hard to get academic research funded. Google has a lot of money, and funds a lot of good research. It’s a company with a lot of goodwill in the world—but imagine how this would look if, as my colleague Megan Molteni suggests, this same story global-replaced “Google” with “Monsanto.”
Google owns massive amounts of data on you and all of its other users that it does not share. And if capitalism’s answer to potential corporate malfeasance is that customers are free to take their business elsewhere, well, it’s hard to find a viable option to the world’s biggest internet search engine. If The Wall Street Journal and the Google Transparency Project are right, the company’s doing everything it can to make sure things stay that way.
Publicly traded U.S. tech companies have stashed as much as $530 billion in offshore tax havens. How’d they do it? Find out what magic tricks companies like Google, Apple, and Microsoft have up their sleeves.
Every year, 5 million people die from causes associated with one of the most mundane scourges of the modern era: sitting around. That’s like losing one Norway-sized country every 365 days to the likes of heart disease, diabetes, and bowel cancer—illnesses linked to a lack of exercise. Norwegians though, aren’t falling victim to inactivity nearly as much as elsewhere in the world. At least according to the largest human movement study ever undertaken, brought to science by the ubiquitous smartphone.
In a paper published Monday in Nature, researchers at Stanford analyzed the minute-by-minute habits of 717,527 people from 111 countries to understand how things like activity levels, gender, and location impact their weight. By dissecting data from a physical activity-tracking app, the researchers found that in countries with low obesity rates, people walked a similar amount each day. The bigger the gap between those who took steps and those who didn’t, the fatter the country—a phenomenon they call "activity inequality."
“Up until now we’ve had a very limited picture of how active people are,” said Tim Althoff, a doctoral candidate in computer science and first author on the paper. “Smartphones give us this unprecedented opportunity to better understand what people are doing all day and how that relates to their health and wellbeing.” That’s the same promise digital health devotees at Stanford and elsewhere have been making ever since the iPhone debuted. But using smartphones to study public health requires reliable data—and researchers, even at well-connected universities like Stanford, still have a hard time getting their hands on the truly good stuff.
Yesterday's study came out of Stanford’s Mobilize Center—an institution dedicated to translating America's oodles of smartphone and wearable data. It was made possible by a $12 million grant from the National Institutes of Health, as part of a 2014 initiative to form 12 top data-crunching centers around the country. Althoff and his collaborators there started with data donated by Palo Alto-based Azumio, makers of the Argus app. The company anonymized the step-counting data but provided a few key demographics: age, gender, height, and weight. The last two enabled the researchers to calculate each user’s body mass index, and from there they correlated activity levels with obesity rates.
They found some interesting results. Take the US and Mexico, for example. Americans and Mexicans take roughly the same number of steps each day—about 4,500. But in the US, those steps are distributed much more widely across the population. And that gap between the activity-rich and the activity-poor corresponds with a much higher rate of obesity. “It’s not just about individuals," says Abby King, a public health researcher at Stanford who contributed to the study. "It’s about where they live.”
King leads the center’s efforts to help people manage weight via mobile health apps, and she sees a huge opportunity to use that kind of continuous data to provide more targeted, dynamic interventions to people who are headed down a wellness dead end. “We can catch people on their way toward obesity, and provide them feedback through smartphone apps, so they can actually do something about it in the moment.”
For now, though, using smartphone-based data to build public health research and guidance is still problematic. Reason number one: Step-tracking data is actually pretty unreliable.
Wearables Could Soon Know You're Sick Before You Do
Social Networks May One Day Diagnose Disease—But at a Cost
Can Apple's ResearchKit Really Change Medical Research?
“In particular, steps that come out of commercial devices like the Apple built-in step counters are not very accurate,” says Bruce Schatz, head of Medical Information Science at the University of Illinois-Urbana Champaign. “They’re tuned for making physically active people feel good.” The issue, he says, isn’t with the measurement device. Smartphones are equipped with accelerometers that measure tiny variations in location, and they do it well.
But the handful of algorithms that Apple and other phone manufacturers and app developers employ to package that raw data into easy-to-use step counts can't accurately capture the huge variety in people's walking mechanics. They don’t have enough flexibility to account for, say, old people who shuffle instead of stride. And not all steps are created equal. Strolling in the park burns fewer calories than sprinting up stairs. Which matters for people trying to manage their weight (though not as much as what people eat). Detecting those distinctions requires raw, not pre-packaged accelerometer data. That's why Schatz, who has worked with the NIH and NSF on their population-scale mobile health initiatives, says raw is the way to go if data is going to be used for health interventions.
The downside is it’s a lot harder to work with. Most app developers don’t keep raw data themselves because the storage costs would be huge. And constantly pulling that data from your phone (think 60 times every second instead of 60 times every hour) would knock out its battery in about an hour or two. Algorithms that store inferences about what you’re doing—walking, biking, sitting—cut down all that data and save battery power. That’s the kind of information Althoff and his Stanford collaborators got from Azumio: 1,440 data points per person per day as opposed to 5 million.
That data was constrained in a less technical way, too. By only looking at the steps of people who bought iPhones and downloaded Azumio's app, the researchers limited themselves to a self-selected group—more likely to be wealthier and healthier than average. Azumio doesn't collect data on things like income and race, and while some app users do keep track of daily food logs and calorie intakes, the company didn't share those for this study. So researchers couldn't test any other hypotheses about lifestyle variations that could impact obesity other than steps. Building accurate models with which to detect, monitor, and predict obesity will require more information than most smartphones readily give up.
Getting population-scale raw accelerometer data from phone manufacturers like Apple and Google isn't impossible. It's just wildly impractical. Researchers who wanted to do it would need to either partner with a developer or build an app themselves, then get loads of people to download it despite the battery drain. Neither Apple nor Google are just giving away data pulls on the billions of phones they have circulating the globe because of its value to paying customers, like online advertisers. And that makes the best information for building accurate predictive models for public health issues like obesity, for all intents and purposes, beyond the reach of most scientists.
“Mobile data really is good enough now to be actionable,” says Schatz. “But nobody has done it except for targeted ads.” Which means that for smartphone data to be able to tackle public health problems, it may first have to become a public good.
When you're trying to meet a Fitbit threshold, but you just can't cut the mustard, we've got several cheats that will bring your count up to 10,000 steps with minimal effort.
Elon Musk is the closest thing this world has to a real-life Tony Stark. Think about it. He builds cool cars. He builds cool rockets. He builds cool tunneling machines. He wants to fire people through pneumatic tubes. He built a ginormous battery factory in the desert, and now he's building the world's largest battery.
OK, technically, Elon Musk isn't building it. Tesla is. But same difference, because Tesla is his company. And Tesla plans to build a lithium-ion battery array capable of storing 129 megawatt-hours of energy.
Wait... 129 mega-whats? What is a megawatt hour, and just what could you do with all that energy?
The most common unit for energy is the joule. If you pick a textbook up off the floor and place it on a table, you've expended about 10 joules of energy. Yes, that's an approximation. The precise figure would depend upon the mass of the book and the height you raised it.
OK, so what about a megawatt-hour? That's also a unit of energy. To understand it, let's first look at power. We define power as the rate at which you use energy.
Measuring the change in energy (ΔE) in Joules and the time interval (Δt) in seconds yields a power measured in watts. That means that power-time is a unit of energy and a watt-second is equivalent to a joule. And a watt-hour? Just do a simple unit conversion. Remember, the key to converting units is to multiply by the number one.
So a 129 megawatt-hour battery represents 4.6 x 1011 joules (where a megawatt equals one million watts). Whoa. What can you do with 460 billion joules?
Let me return to my example of lifting a book. What could I lift with 460 billion joules? Lifting something, increases its gravitational potential energy. This change in potential energy is equal to the product of the mass, the height, and the gravitational field (9.8 N/kg here on Earth).
With that in mind, how much mass could you lift to a height of 10 meters using the energy in Musk's super-battery? With a change in height of 10 meters and a total energy of 460 billion joules, I can solve for the mass—4.7 billion kilograms. Or roughly the mass of the pyramid of Giza. Imagine lifting that 10 meters. Pretty cool.
Want to work with a known mass and solve for lifting height? OK, how about a Nimitz class aircraft carrier like the Carl Vinson? Such ships have a mass of about 108 kg. Using the same energy but solving for the change in height, I get a value of 469 meters. Now, I should note that this assumes 100 percent efficiency in the battery and lifting mechanism. Reduce the efficiency to, say, 50 percent and you reduce these values by half.
Another common form of energy is the energy of motion, something we call kinetic energy.  The faster something moves, the more kinetic energy it possesses. And the greater the mass, the greater the kinetic energy. The equation looks like this:
Once again, I can pick the mass of an object and calculate the speed that I could get that object up to using the energy in the battery. How about a locomotive? They're pretty heavy, right? Let's use a locomotive mass of 100,000 kg. Given 460 billion joules, this locomotive could achieve 3,033 m/s—way faster than a bullet (assuming no air resistance). Want something a little more typical? How about a baseball with a mass of 145 g? That same amount of energy would propel it to 2.5 x 106 m/s. Whoa. Crazy fast. But nothing compared to the speed of light (3 x 108 m/s). The real question is: Why would you want to get a baseball going this fast? Who knows. Maybe you're Iron Man.
Well, not stuff. Ice. How much ice could you melt with that much energy? First, let's assume  the ice is at 0oC so we don't consume any energy heating the ice, only melting it. You need 334 joules to melt 1 gram of ice. Scientists call this the latent heat of fusion for water. So I we must do is divide the battery energy by the latent heat of fusion. That gives us an ice mass of 1.4 million kilograms. Can't picture that? Imagine a cube that measures 459 meters on each side. That is one giant ice cube.
By now, you know how to calculate this stuff, which means it's time for some homework questions. But first check out a short python program with my calculations. It will help.
Determine the mass of this giant battery. Don't just Google it. Use the density of lithium-ion batteries to calculate it.
How high could a fully charged super-battery lift itself?
How fast could the battery move itself?
You want to make coffee. Using the super-battery to boil room-temperature water, determine how many cups you can make for your friends.
What is the biggest object you could place in orbit using 460 billion joules?
How long could you use this battery to power your house, assuming your house uses 1,000 watts daily.
A human can pedal a bicycle at about 100 watts of power output. How many humans on bicycles would it take to charge the super-battery 24 hours?
How long would your phone last using this battery? How many times could you watch Iron Man on your phone?
Tesla's Gigafactory, under construction in Sparks, Nevada, will be the largest building in the world, by footprint, when it's finished. The batteries it produces are crucial to Tesla's plan to make affordable electric vehicles.
As I walk through life, I often look down at people’s shoes. No disrespect intended. I’m not trying to avoid eye contact with you. I’m paying attention to the imperfections in the world and looking for ways to help fix a few of them.
You see; shoelaces are not just shoelaces when you view them through the filter of everything all at once. They are the raw material of knots, and knots are the embodiment of mathematical beauty; mathematical beauty is a fabulously useful tool for rational problem solving; and rational problem solving is, of course, the most powerful tool for changing the world. In my Nye’s-eye view of the world, tying a well-crafted knot is like a personal promise to engage in that whole glorious process. I often have three such knots with me: two on my shoes and one around my neck in the form of my beloved bow tie.
But when I look at the knots all around me—well, it’s troubling. There’s a lot of work to be done.
Try looking down yourself, and what do you see? Around half of the people I meet tie their shoes with bow knots that are prone to coming untied from the day-to-day flexing inherent in walking. These bowknotters often compensate by tying their laces with doubled knots, piling one asymmetrical knot upon the other in a desperate bid to keep it together—or worse, they repeatedly walk with loose laces dragging. It doesn’t have to be this way. With a little more thought and attention, you can bring inspirational order to what may seem like one of the most mundane objects in your daily life. Plus, your shoes will fit better and stay tied.
Let’s start with a simple experiment we can do together, right here and right now, using only the loosened laces on your shoe. Begin by tying one of the most useful of all knots, the square knot. It’s also called a “reef knot,” as it was and is, from time to time, used to reduce the sail area of a sail on a boat, to reef the sail in a storm or strong wind.
Wrap one lace over the other, then the second lace over the first one. You may have heard the expression “right over left, left over right.” Look at that knot. It’s beautiful, symmetrical; it’s the marriage of two curves. This square or reef knot is square; I mean it’s symmetrical. It’s the basis for the knot we call a “bow.” Now, untie the second of the two wraps. You might go, “right over right, right over right” again. Please examine this knot. I hope you notice it’s not as good looking as the reef knot described above.
If you’re like me, you might at this point exclaim, “Oh, the asymmetry!” This lack of balance found in about half of all conventional shoelace knots is heartbreaking. What we want in a square or a reef knot is symmetry. Here, mathematical beauty is a means to an end. It’s more than beauty for beauty’s sake, although that ain’t bad. It’s a matter of function: A shoe tied with a reef knot will stay tied long after other, sloppier knots have come unraveled. In shoelaces, as in so much of physics, symmetry is the key to balance and stability.
When you tie a conventional bow on your shoe, check to see if its two loops, or bunny ears, lie perpendicular across your foot, left to right, or lengthwise along your foot, toe to heel. If the loops or ears come to rest in a neat left-to-right position (“athwart,” as we say at sea), that’s the way we want it. That’s symmetrical, and that arrangement will seldom come untied. This is what I call a “square bow.” If one gently pulls the loops so that the loose ends of the laces pop free, the knot that is left there underneath is the beautiful square knot. Even if you perceive your laces to be woven from slippery stuff, the squarebow knot will hold its own once it is gently but snuggly tightened. Or as the saying goes, any knot has to be properly “dressed.” (For you crossword puzzlers out there, the loop of the lace is called a “bight.” It’s pronounced just like our word “bite,” and it works wonderfully in Scrabble.) The unsymmetrical knot, on the other hand, will slip with each step. It will start to lose its shape, its integrity, and its stability the moment you start walking and put stress on it. Oh, the trauma; oh, the suffering.
As you may have inferred, I tie my bows by forming a single bight and wrapping the other end of the lace around the base of the bight. If you are among those who tie laces by finishing the knot with two loops, or “bunny ears,” it all works the same way. The bunny ears are your knot-ty-er bights. Allow me to reassure you bunny-ear, double-bight people: You can create a square bow just fine. If you tie the base overhand knot, then form your two bunny ear bights, and tie them in the opposite direction from your base overhand knot, you will produce a lovely square bow.
Bill Nye Saves the World, the Anti-Anti-Science Show, Hits Netflix in April
Bill Nye Says Climate Change Deniers Have a Bad Case of Cognitive Dissonance
Why Bill Nye Makes the Perfect Leslie Knope
Now, I loved my grandmothers. They were both remarkable people. They raised my parents, after all, and I believe anyone who met either of them would say, “That girl has plenty of common sense.” Nevertheless, the asymmetrical, not-quite-a-proper reef knot is, by long tradition, called a “granny knot.” Sorry, Nana. Sorry, Mini. We seek a square bow rather than a “granny bow.” If you have suffered lo the many years of your life with asymmetrical granny bows, you’ll find it’s a hard habit to break. But it can be done. Try this: Reverse the first wrap of your laces. Instead of going right over left, reverse that and go left over right. Then let your muscle memory take over for finishing the bow, either by wrapping individual laces or by wrapping bunny ear loops.
All this talk of shoe laces may seem like an unimportant detail of everyday life, but it is always underfoot—or literally atop foot. A shoelace knot is a metaphor for the scientific approach to problem-solving. Too many people learned to tie bow knots in their shoes and accepted that imperfect, unsymmetrical, time-consuming route rather than dig deeper for a better long-term approach. So when I wax poetic about the beauty of a square knot, it’s not only because I like showing off my sailor skills; it’s because good design should be good all the way down to the details, even when we’re talking about something fairly straightforward like tying knots. I think we should all make a habit out of expecting the best problem-solving from ourselves, and there’s no better place to start than with design problems we encounter every day. That’s where things like shoelaces work well or . . . not. (Get it? Or knot? Uh . . . sorry.)
There is another big idea in here, masquerading as a small one. Even if you have tied your laces the other way, in granny-bow fashion, for years on end, you still have a chance to change. This ongoing potential for improvement is at the heart of the scientific way of looking at the world. In politics or religion, changing your ideas can be risky or even heretical. In science, abandoning a decades-old habit in response to new information reflects a vital quality of open-mindedness. Such open-mindedness is essential for making a fundamental discovery . . . or for keeping your shoes tied.
Excerpted from Everything All At Once: How to Unleash Your Inner Nerd, Tap into Radical Curiosity and Solve Any Problem by Bill Nye. Copyright © 2017 by Bill Nye. With permission of the publisher, Rodale Books. All rights reserved.
Bill Nye uses the power of Twitter to answer some common science questions.

Check out Bill's new show on Netflix "Bill Nye Saves The World" premiering April 21st!
This story originally appeared on CityLab and is part of the Climate Desk collaboration.
Urban Canadians are feeling the impact of climate change. Flooding in Quebec this spring damaged nearly 1,900 homes in 126 municipalities, causing widespread psychological distress. Summer heatwaves are predicted to become more frequent and severe each year, putting more people at risk of injury and death. Vancouver and Toronto are working to manage these risks. Most Canadian cities need to work harder to include climate change in public health planning.
The Climate Change Adaptation Research Group at McGill University looks at how climate change is impacting human society, and what solutions we can design to protect ourselves. Drawing on evidence from our research into cities in Canada and around the world, we propose that cities will need to integrate climate change concerns into public health and the health-care sector more seriously.
Cities must also focus on the most vulnerable groups (such as low-income households and older adults) and emphasize the participation of citizens and the community in planning for climate change impacts.
Heavy rains causing floods and mudslides are already frequent across Canada, as we’ve seen in Quebec and eastern Ontario this year, and in Calgary and Toronto in previous years. These events are predicted to increase in frequency and intensity. Violent thunderstorms and rising sea levels in coastal cities such as Vancouver and Halifax are also expected to get worse. Floods and violent weather cause injury, illness and death, as well as mental health effects of distress.
Heatwaves are expected to become more frequent and severe over the next few decades, causing heat stroke and even deaths, as well as respiratory and cardiovascular disorders. Increased air pollution in cities will also come from automobile exhausts, exacerbated by projected warmer temperatures. Urban air pollution is linked with eye, nose and throat irritation, respiratory conditions, and chronic pulmonary disease and asthma.
These climate change events will affect some groups more than others. Flooding is devastating to households lacking financial resources. Low-income families have reduced access to air-conditioned places. Older adults are more vulnerable to heat because of reduced thirst sensation, challenges of moving, visual or hearing impairments and often social isolation.
Children are also at risk during heatwaves. They depend on a caregiver to recognize the symptoms of heat stroke and have less ability to sweat than adults.
Toronto and Vancouver are leading health adaptation to climate change both in Canada and globally. Most of Toronto’s initiatives address extreme heat, along with flooding and air quality. Vancouver’s health adaptation initiatives also focus on heat-related risks. Vancouver also places an importance on vulnerable groups, namely homeless residents and low-income households.
Montreal only released its first climate change plan in 2015, but the city has been a pioneer in protecting residents from extreme heatwaves since 1994. The heatwave plan involves monitoring signs of heat-related illness, frequent visits to home-care patients, opening air-conditioned shelters, extending pool hours and mass media communication campaigns. This has reduced mortality by 2.52 deaths per day during hot days.
Smaller cities face tougher challenges. Most Canadian municipalities simply do not have the resources and expertise to plan for the health impacts of climate change. Health adaptation competes with other important health priorities, such as smoking, obesity and poverty.
Some argue that climate change needs to be integrated deeper into city plans across all sectors. Vancouver and Toronto are already experimenting with this. Vancouver has updated its building code bylaw to raise flood construction levels. Toronto now requires all new buildings over 2,000 square metres to include roofs with vegetation on them—to slow down the urban heat island effect and reduce the incidence of heatwaves.
Cities also need to place the voices of people closest to impacts at the centre of decisions. Low income and older residents, for example, are at the highest risk for heat-related illnesses or death. Many of these residents already suffer from health conditions and are more likely to experience social isolation and lack of support.
Another way to make adaptation easier is through collaboration and coordination. Municipalities can learn from each other, rather than reinventing the wheel. For example, it’s important to make sure there is a strong link and coordination between local public health authorities and municipal governments; in most Canadian provinces, these two are separate.
International networks of mayors’ offices such as C40 and Resilient Cities already work toward sharing knowledge and best practices.
Finally, cities should seek out adaptation options that have other health co-benefits. An example would be urban parks that provide shading from the sun but also serve as social amenities for recreation and socializing.
Preparing cities for the health impacts of climate change, then, needs to integrate climate risks into public health and the health-care sector. It needs to consider the risks for vulnerable people such as the elderly. It also needs to emphasize collaboration among cities and among government agencies.
With the federal government committing $125 billion to infrastructure from 2015 to 2025, now is the time to build health protections into how we climate-proof our cities.
This story originally appeared on The Conversation.
In 2003, straight out of college, Janet Campbell started her first real job as a software developer at Epic, one of the country's biggest producers of electronic health care records. At a desk in the company's Verona, Wisconsin headquarters, she worked on a straightforward project: Building a feature that could restrict diagnostic codes to patients of a specific sex. That way, a clinic could get an alert if a provider tried to bill for a cervical exam, for example, in a patient marked "male."
Tinkering with a section of code about a year into the job, Campbell found herself fixated on the field doctors used to document patients’ sex. "This is weird," she thought. It had just three values: male, female, and unknown.
If they wanted to, clinics could work with Epic to add more choices to the list. But Campbell, thanks to the gender studies classes she took alongside her comp sci coursework, saw a deeper problem: That one field was doing too much. What if, instead of containing the patient's legal sex or sex assigned at birth, it also contained their gender identity? All of those data points were important in a health care setting. And for transgender people and other gender non-conforming patients, she realized, they often don’t match.
Many clinical settings fail in caring for transgender people. According to a 2015 report, 33 percent of transgender people surveyed had at least one negative health care experience in the past year related to their gender identity. Part of the problem is the reluctance of clinicians to simply ask—many don't know how to talk about gender identity, or fear offending patients. And that one-size-fits-all question in Epic’s electronic health care record—the type of system used in nearly 80 percent of outpatient clinics—certainly wasn’t helping matters. So Campbell, now vice president of patient experience at Epic, set out to change it.
Not long after she noticed the patient sex field, Campbell offered to give her team at Epic an educational presentation on gender variation—part of a regular series of workplace talks. She was one of the most junior people in the room, surrounded by other developers, mid-level managers, and a division manager. She still has the slides she presented that day. "Epic is almost completely unable to deal with this level of complexity," reads one, "and neither is the medical profession, for that matter."
The first customer to reach out for help correctly documenting gender identity was probably a user from a University of Wisconsin-affiliated clinic who contacted customer support in 2005, about a year after Campbell's presentation. "I’m sure other customers have had the same question,” reads the support log entry. They had.
In particular, health care workers at LGBTQ-centric organizations like Boston’s Fenway Health had started agitating for more accurate sexual orientation and gender identity data collection. Adding them would help provide better care to LGBTQ patients, including getting them the right preventive screenings, risk assessments, and behavioral health care. In 2014, more than 150 health care and advocacy organizations signed a letter requesting a multi-part question for gender identity in the electronic health care records that providers must use if they want federal funding.
Two years earlier, Campbell had come to a similar conclusion. In January 2012, she posted an internal wiki page containing her imaginings on the design changes needed for a two- or three-step sex and gender question. Meanwhile, a handful of other employees, several themselves transgender or gender non-binary, formed a workgroup to help customers collect gender identity information.
But external pressure to actually build the new features didn’t come until 2015. That’s when the Office of the National Coordinator of Health Information Technology released new regulations: Any outpatient clinic receiving federal incentive payments for using a government-certified electronic health care record—as 78 percent of them do—would have to use software that collects sexual orientation and gender identity information by 2018. Federally Qualified Health Centers, which receive more federal funding, had an even earlier deadline, in 2016. Suddenly, companies like Epic had to make big changes.
At Epic, the small workgroup exploded into a 25-person troupe—they called themselves the Volunteer Army. Changing their product to collect new data was not as simple as it sounds. "It's like the Y2K of the health record," says Campbell.
The code referenced the original patient sex field in hundreds of places. It was in code creating the patient header, a section with each person’s name and demographics. But it also popped up in less obvious places, like the chunks of code calculating normal ranges of blood tests that vary between genetic males and females. It even appeared when generating genetic pedigrees, with males depicted as squares and females as circles. Starting in early 2016, the Epic team had to comb through each appearance, figuring out what information was actually relevant: sex assigned at birth, legal sex, or gender identity.
Other electronic medical record manufacturers were similarly overwhelmed by the task. Rachel Miller, who oversees customer experience at the mental-health-focused Foothold Technology, says the code base used to write their sex data collection tool is older and less flexible than newer areas of the program. Huge amounts of data are attached to the field—and losing any of them while remapping to new values is a developer's worst nightmare. "Having to clean it up is a bigger bite than adding something new," says Miller.
Even with the Volunteer Army on the case, Epic is still cleaning its code to ensure gender identity is displayed consistently and correctly. But in June 2016, it went ahead and released its two-item gender identity question as a special update—sort of like a Windows patch—to its clinical customers.
That doesn't mean everyone is ready to use the new feature, though.
When the company started teaching its customers how to use the new fields, a new set of problems bubbled up. Several members of the workgroup found that customers were using wildly different practices to collect patients' gender identity. "The registrar would look at the patient and fill out whichever value they thought was correct," says Campbell. "Or maybe they'd put what was on the driver's license, or if it was a children's organization, the birth certificate."
That had potential to result in patients being called the wrong name or pronoun—not an inconsequential event. For transgender patients, "being misgendered is very distressing," says Adrian Daul, an emergency medicine physician in Atlanta, "and can be a deterrent from getting care in the first place." Furthermore, electronic health care records are often used as a medical census of sorts. Not documenting transgender status results in being undercounted—a problem when it comes to setting research, policy, and funding priorities.
Epic is working to change those practices. Back in 2013, the workgroup released a strategy handbook to educate clients on the best ways to capture and use gender identity information. And as the software has changed, so has the handbook. It's more than a technical guide to structuring workflows: In a subtle way, it's something of a primer on transgender cultural competence. After an introduction highlighting health disparities in sexual minorities, sections gently explain the difference between gender identity and sex, and suggest that health care workers consider which providers need to know about a patient.
How One Startup Built Better Health Insurance With the Magic of Data
Moving Patient Data Is Messy, But Blockchain Is Here to Help
How Technology Led a Hospital To Give a Patient 38 Times His Dosage
Campbell hasn't heard of any health care organizations rejecting the changes. But she has heard, "We're not sure we're ready for this yet." She suspects resistance to turning on the functionality stems mostly from a lack of confidence. Alex Keuroghlian, assistant professor at Harvard Medical School and director of the National LGBT Health Education Center at The Fenway Institute, is somewhat less sanguine. He’s encountered "clinicians saying, 'I treat everybody with respect—I treat everybody the same way.'" That's not appropriate, he says: "The reality is, to treat this particular subpopulation with respect, you can't treat them in exactly the same way."
With the exception of Federally Qualified Health Centers, collecting the new data is still optional. Which isn’t all bad: "If you try to have the technology force something an organization is not ready for, it's going to result in a poorer patient experience," says Campbell. If a patient is misgendered during a visit despite communicating preferred pronouns, for example, the negative impact may be worse than it would've been.
So institutions are turning on the new questions—slowly. In Atlanta, Adrian Daul has been asking his Atlanta hospital to start sexual orientation and gender identity data collection in its version of Epic for about a year. He says the decision has been delayed by disagreement over who should collect the data from the patient: Some feel it should be the doctor's responsibility, while others feel it should be done by registration personnel. "Everyone has to be on board at the hospital in order to be collecting this stuff and then using it in a sensitive way," he says.
As for the rest of Epic’s customers? Campbell estimates only about 10 to 20 percent have switched on the feature, and another 20 percent or so have expressed interest.
Turning on gender identity functionality isn't hard, she says. "Becoming an organization that can both ask that responsibly and then deal with the information respectfully and responsibly," though, "is a much harder challenge."
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?
By now, “Here Are Some Stupid Things on the Internet of Things” has become a full-on article genre. There’s even a Tumblr dedicated to the idea: “We Put a Chip in It,” it’s called.
In some visions of the future, smart devices capture, quantify, and control most aspects of daily life. The oven knows you forgot about your cookies and cools them off for you at peak crisped-edginess. The fan knows you have entered the room and desire a breeze. The pillow knows when you start snoring and vibrates so you shift in your sleep. Alexa can order you one! OK, Google?
Here’s the thing, though: For those chips in those devices to do any good, they have to communicate with the outside world, and the outside world has to talk back. And—like most communications magic—that often happens via radio waves.
The increasing number of smart objects on Earth (in addition to higher-power and longer-range WiFi-beaming satellites, car radars, and ubiquitous cell coverage) causes problems for scientists who want to look beyond our planet: Astronomers are finding it harder and harder to detect faint radio signals from space, which sometimes come in on the same frequencies as human technology. Scientists, industry, and the government are trying to share a spectrum so crowded many call it a crisis.
Right now, the FCC regulates the use of the radio spectrum. And it saves some “bands”, or ranges of frequencies, mostly for radio astronomy. Around 1,400 megahertz, for example, astronomers can fairly safely look for neutral hydrogen. A bit higher, near 1,600 megahertz, the FCC has protections for hydroxyl observations. In fully protected bands, like hydrogen's, no one else—not a smart toothbrush maker or a cell phone provider—can broadcast at those frequencies.
The rest of the FCC-allocated spectrum is split among 29 other services, like “broadcasting,” “amateur,” “mobile,” and “meteorological aids.” Not all technologies require licenses to use specific frequencies (including many Internet of Things things). But within some of the FCC's slices, companies do vie for specific sections. Cell providers, for instance, paid more than $19 billion earlier this year for 84 megahertz of bandwidth that television broadcasters used to use.
And that b$g number should tell you something: Those slices are precious. It's simple supply and demand. Which means those wedges reserved exclusively for radio astronomy? Someone would really like to use them to make money.
Because this is academia, there’s a committee for that: the National Academy of Sciences’ Committee on Radio Frequencies (CORF!). And on July 1, astronomer Liese vanZee will become its new head, leading the group of scientists who (try to) help guide the government’s—and the world’s—allocation of radio resources so scientists can study galaxies without confiscating your Samsung Galaxy.
VanZee’s research mostly uses one of the ultra-protected bands—around frequencies of 1,420 megahertz, where cosmic hydrogen beams out its emissions. So she’s got a lot less to worry about, personally, than some radio astronomers who study the complex organic molecules that send emit at the same frequency as anticollision radar. Still, even in vanZee’s supposedly science-only section of spectrum, problems pop up. “It doesn't prevent people from deciding to broadcast there,” she says. That often happens unintentionally, in the form of “harmonics,” or accidental overtones with frequencies exactly 2, 3, 4, etc. times higher than intended one.
Inside the New Arms Race to Control Bandwidth on the Battlefield
Next-Gen Wi-Fi Will Actually Connect the Internet of Things
How Attackers Can Use Radio Signals and Mobile Phones to Steal Protected Data
In preparation for an upcoming meeting of the World Radiocommunication Conference, vanZee’s committee will provide input to attending leaders on some “new” spectrum between 275 and 450 gigahertz. With the lower frequencies so crowded, people are pushing higher (even though the technology to do that isn't mature), and moving into previously un-allocated spectrum.
But there’s a big problem: A brand-new, billion-dollar telescope in Chile—the Atacama Large Millimeter/submillimeter Array, or ALMA—just opened its eyes a few years ago, staring into space in that radio range. “If you want to study molecules in our atmosphere or other parts of our galaxy or other galaxies, that's a part of the spectrum you want to be using,” says vanZee. If a bunch of communications types start broadcasting all up in there, that billion-dollar instrument won't be able to do its job.
Now, vanZee isn’t saying everyone except astronomers should become luddites to save astronomy. “It's really tempting for the science community to put their foot down and say, ‘No no no,’” she says. “But, in fact, we do want to work with industry.”
Both sides can work to minimize head butting: Astronomers can keep building their radio telescopes in the world’s wilds, far from the hordes of Blueteeth and cell towers and Teslas. And they can build “interferometers”—sets of smaller telescopes that work together as one, which help astronomers distinguish between terrestrial and celestial signals—instead of standalone dishes.
For industry’s part, it can say “sorry” when it creates harmonics, and then fix them. That's good for everyone. “You're wasting energy if you’re transmitting outside of your band,” vanZee says. And the FCC could give both sides more leeway: Put some blank space between astronomy’s sacred bands and the communication bands, so industry can be a little sloppy without obscuring the universe.
That's still an old way of thinking about things, though, says Darpa—the defense research agency that brought you this crawling jellyfish donut robot. "Allocating" the spectrum? So rigid, so passé. The way forward is not to tell radio-emitters exactly what to do, but to liberate them, let them decide for themselves.
The old model worked relatively well for more than a century. But it’s no longer practical, in Darpa's opinion, to have devices that operate at a set, static frequency. This is the basis of the agency's new Spectrum Collaboration Challenge (similar to a challenge from a few years ago): Outsiders create devices that can choose, on the fly, what frequency range will work best at that moment, based on the broadcasting characteristics of other nearby devices—including those that are also flipping between frequencies.
“If we want to eliminate the inefficiencies that exist today," says Paul Tilghman, head of the challenge, "we want to manage the spectrum at machine speeds, not people speeds.” Thirty teams, selected in January, are now preparing for the first “tournament” in December, where their radio-broadcasters will battle-of-the-bots it out.
The military, and so Darpa, is interested in this because its many “unmanned platforms”—drones in the water and the air, satellites in orbit—need consistent, uninterrupted communications. But whatever comes out of the competition can make its way into industry, too. Into your toothbrush that tells you if your teeth are clean!
Super-smart broadcasters like that could be both good and bad news for radio astronomy. The good news: The algorithms that help the machines figure out which frequencies to use can easily include things like a "never use 1,420 megahertz."
The bad news: When astronomers want to know if a signal comes from space, they sometimes depend on knowing what a given source of human-made radio waves looks like. "Yes, that's definitely the neighbors' iRobot," they may be able to say. But not if iRobot is always changing.
The important thing, however radio use evolves, is to share, smartly, and to talk it all through first. Because as cool as it is to communicate at home, doing so irresponsibly could cut humans off from space. “If you fill the spectrum with man-made emissions, you will never be able to understand certain parts of the universe,” says vanZee.
The Smart Remote is an intuitive and simple solution to controlling all the smart devices in the modern home.
On the night of May 5, 2017, Eliud Kipchoge, the world’s best marathon runner, lay awake with his eyes open and his mind racing.
Under ordinary circumstances he is amiable and serene, with his furrowed, leonine features often lit with an ice-white smile. But that night, in his room in the Hotel de la Ville in Monza, Italy, he was more nervous than at any other time in his professional life. In the morning, on the 63rd anniversary of Roger Bannister’s historic sub-four-minute mile and at the culmination of Nike’s three-year-long Breaking2 project, he would attempt to do something nobody had ever come close to doing: run a marathon in less than two hours.
Nike’s Controversial New Shoes Made Me Run Faster
What Happens When You Train Like Nike's Two-Hour-Marathon Runners
Why You'll Never Run a Sub 2 Hour Marathon—But the Pros Might
Kipchoge had asked Valentijn Trouw, one of his managers, to wake him at 2:45 am, exactly three hours before the start of the race. But when Trouw checked Kipchoge’s WhatsApp profile at 2:29, he found him online and awake. The pair decided to go to breakfast. In the hotel restaurant, Kipchoge betrayed no hint of tiredness as he greeted his two Breaking2 competitors—Zersenay Tadese, the current world-record holder in the half marathon, and Lelisa Desisa, a two-time winner of the Boston Marathon, neither of whom could sleep, either—or to the 30 pacemakers who had been recruited to guide these three contenders around the course. As Kipchoge ate his oatmeal, he smiled and shook hands with the battery of scientists and designers from Nike who circled the hotel, sleepless themselves.
At 4:15, Kipchoge was driven to the Autodromo Nazionale Monza, the Formula 1 racetrack whose 1 1/2-mile junior circuit had been chosen by Nike to host the two-hour attempt. It was starless and overcast: 53 degrees Fahrenheit and a little humid. In the home straightaway, where the race would end, giant screens showed gauzy highlight reels of the athletes in training, and the tarmac was lit in lurid pinks and blues as a crowd of 800 people waited for the show to begin.
On the back straightaway, where the race would start, there were no crowds. The atmosphere was tense and quiet. After changing into a red-orange singlet and arm sleeves, black half-tights, and the controversial racing shoes that Nike had engineered for him and his competitors, Kipchoge began a 30-minute warm-up: some easy jogging followed by a few “strides”—sprints to wake up the body. He spoke very little.
Kipchoge’s anxiety came not from the mere prospect of having to race, which he always welcomes, or from the expectations of Nike, which had spent millions of dollars applying the most advanced technology and sports science to get a marathon runner across the finish line in under two hours. Kipchoge was nervous because he simply didn’t know how his body would react to the stress of running so fast for so long. The fastest anyone, ever, had run a marathon was 2:02:57. Kipchoge wanted to run nearly three minutes faster, a 2.4 percent improvement, which might sound small but represents a giant leap in human performance. And when the body fails in the marathon, it can fail dramatically and painfully. Millions of people across the world were tuning in to watch livestreams of the event. Kipchoge, the marathon’s reigning Olympic champion, faced the real prospect of not just failure but mortification.
He was also aware of the skepticism, if not venom, that many obsessive running fans felt toward the Breaking2 project. Since Nike announced its effort to break the two-hour mark last December, many have called it a barely veiled marketing exercise for the shoe behemoth and a derogation of the sport’s spirit. Some decried Breaking2’s emphasis on record-breaking in a sport recently beset by a myriad of doping scandals, particularly from East Africa. A typical post on the influential LetsRun message board read simply, “What a stupid publicity stunt. I hate Nike even more after this.” Kipchoge believed all the complaints would dissolve if he could achieve what so many had thought impossible. But first, he had to do it.
At 5:45 am, the starter’s air horn bleated and the three athletes shot off into the darkness.
A black Tesla Model S, with a digital clock mounted on its roof, was parked at the starting line in Monza. The electric car would lead the athletes around the track, driving at a constant 1:59:59 pace and showing them their split times on the display. The Tesla also shot out a green laser onto the ground, which would help the pacemakers know exactly how fast they needed to run to maintain the two-hour pace. The sight of the lead athletes warming up in their vividly colored uniforms among the black-clad pacers, and the green laser beams spilling across the tarmac, was strange and eerie—like a silent, illicit rave on a deserted freeway.
As the seconds counted down to the start of the race, Tadese bounced on his toes like a boxer and Desisa worried at his arm sleeves. Kipchoge, at 5'6" and 125 pounds, was strikingly composed. His eyes looked straight ahead. With 15 seconds to go, he dipped his torso and braced his body to run.
At 5:45 am, the starter’s air horn bleated and the three athletes shot off, behind the car and a group of six pacemakers, into the darkness.
Desisa runs in a temperature-simulation chamber.
The Nike Sports Research Laboratory is a secretive area within the company’s sprawling headquarters in Beaverton, Oregon, where scientists and designers work on new products, many of which never see the inside of a store. The whole purpose of this space, with its clandestine projects and NDA requirements, is to create interesting new things through repeated and creative failure.
Early in the summer of 2014, a handful of NSRL employees escaped from Beaverton to a resort in the town of Sisters to drum up new ideas. Matt Nurse, a tall, broad-shouldered former high school swimmer who leads the NSRL, was feeling bored and belligerent, and he told his colleagues that they weren’t taking enough risks. To inspire them, he played one of his favorite YouTube videos—a monologue by former Black Eyed Pea will.i.am: “Sometimes,” will.i.am says in the clip, “you have to put yourself in the situation to fuck up … Anybody else who’s not going out there and risking messing up … they never get any bigger.”
Throughout the previous year, shoe designers within the NSRL had been working on a project intended to help distance runners of all abilities improve their performance by up to 3 percent. To Nurse, this felt worthy but safe; he wanted his designers to devote themselves to something at which they could either succeed or fail definitively. Something like a sub-two-hour marathon.
An American physician named Michael Joyner argued that the perfect time for the perfect athlete in perfect conditions was 1:57:58.
It was a science fiction idea, long dismissed by aficionados of the sport as a waste of time and energy. In 1896, in the first Olympic marathon race, only one man broke three hours—the winner, Spiridon Louis, of Greece—and that race was less than 25 miles long, rather than the 26.2 that became standard in 1921. Over the past century, the world’s best time for the marathon dropped at an average rate of about five minutes per decade, driven by the professionalization of the sport as well as advances in shoe technology.
In 1991, when the world best for the marathon was 2:06:50, an American physician named Michael Joyner wrote a now famous paper, published in the Journal of Applied Physiology, estimating the best possible time for a marathon runner. By analyzing three main factors that limit a runner’s performance—VO2 max (the maximum oxygen an athlete can consume while running), lactate threshold (the running speed above which lactic acid in the muscles accumulates prohibitively), and running economy (the efficiency with which a runner moves down the road)—Joyner argued that the perfect time for the perfect athlete in perfect conditions was 1:57:58. In other words, the sub-two was possible, but only just, and only in theory.
In the past 20 years, as the world record continued to inch downward, the debate over whether a sub-two might actually happen became more and more contentious. A parlor game arose among physiologists and statisticians, arguing when we might see the first sub-two: in 10 years, 25 years, 70 years, never. The game found its way to the retreat in Sisters, where Nurse tasked the NSRL team to imagine how they could make a two-hour marathon a reality. “We keep talking about the sub-two,” Nurse remembers saying. It was time to stop talking and actually do it.
Sandy Bodecker, a Nike employee for nearly four decades and the vice president of special projects, heard the call. He lobbied executives for funding and started a secret two-hour-marathon task force that was dubbed Project Able after one of the first monkeys to survive being sent into space. (In this context—a group of mostly white men aiming to propel an East African to a sub-two-hour marathon—that image has an unintentional but appalling resonance.) The name was changed to Breaking2 shortly before the project went public last December.
Bodecker and others on the senior innovation staff were adamant that the company not only design new shoes and performance clothing but a whole new kind of race, from the choice of venue to the selection of athletes to the training programs. Bodecker signaled his personal commitment to Breaking2 by having 1:59:59 tattooed across his wrist. The same number was also emblazoned, in digits 2 feet tall, at the entrance to the NSRL, even before the rest of the company was aware of the project’s existence.
These displays were a reminder not only of Nike’s mission but of its competition. In 2011 a young Scottish shoe designer at Adidas named Andy Barr had stood up in a meeting at the company’s German headquarters and declared that it should be thinking about a sub-two project. (Adidas brought out an Adizero Sub2 shoe this spring but has not announced when or if it will make an actual attempt.) Meanwhile, at roughly the time of Breaking2’s launch, in 2014, a physiologist from England’s University of Brighton, Yannis Pitsiladis, announced that he was seeking $30 million in sponsorship for a sub-two-hour marathon project of his own.
Tony Bignell, a jocular English designer who leads Nike’s footwear innovation team and works inside the NSRL, told me that Nike was never in any kind of “arms race” with Adidas or anyone else. Nurse, however, admitted to feeling some time pressure early in the project. “We believed that at some point within a few years, someone would get to a sub-two,” he says, and he wanted the team that succeeded to have a swoosh on its chest. To do that, Nike had to build its own perfect marathon from the ground up—starting, of course, with a shoe.
In the Nike lab, scientists measure Eliud Kipchoge’s oxygen uptake, a key indicator of how fast a runner can go over long distances.
Though nobody at the company will admit it outright, it seems naive to think that selling sneakers was not a significant driver behind Breaking2. Geng Luo, a biomechanist and a lifelong sneakerhead from the outskirts of Beijing, was among the core group in the NSRL charged with designing a shoe specifically for Breaking2.
Starting out, Luo and his fellow NSRL designers had been thinking about creating a “track spike for the marathon”—an idea born of the prevailing wisdom that weight should be the primary concern in long-distance shoes. Weight costs runners. For every 100 grams on a sneaker, a runner’s energy expenditure increases by 1 percent. With this in mind, Luo and his team created a number of prototypes of stiff, light flats, shorn of any extraneous materials, based on a strong carbon-fiber plate. In one prototype, they even chopped the heel off the shoe, because many elite runners land on their midfoot or forefoot. The theory had some merit, but Nike’s test runners hated it. The ride was much too unforgiving.
Then, in early 2015, Luo says, they made the critical decision to sacrifice “lightweight for right weight”—to incorporate foam into their prototypes, despite the added weight, if it meant they could cushion and protect their runners’ feet over the length of a marathon. They began using a huge stack of what they called ZoomX, a superlight but responsive foam of the type used in aircraft insulation, underneath the foot. They then embedded a spoon-shaped carbon-fiber plate within the foam, which stiffened the shoe and rocked runners forward, as if they were running downhill.
An athlete wearing Nike's new footwear should, in theory, be able to run a marathon 4 percent faster.
The test runners embraced the new prototype, and the laboratory results were spectacular. In trials conducted by Rodger Kram, a biomechanist at the University of Colorado who consulted with Nike on the shoe, the meta­bolic cost to runners using the new footwear was found to be 4 percent less on average than for those using Nike’s previous best racing flat, the Streak 6. That means an athlete wearing the new footwear should, in theory, be able to run a marathon 4 percent faster (or, for an elite athlete with less room for improvement, 3.4 percent faster) than an athlete wearing a Streak 6.
The real-world results were promising, if slightly less astounding. Beginning in 2016, some of Nike’s athletes began wearing prototypes of the new shoes, the uppers disguised to make them look like existing Nike models. Not only did Kipchoge cruise to victory wearing a pair at the 2016 London Marathon, but all three medals in the men’s Olympic marathon, including Kipchoge’s gold, were won by athletes secretly wearing the shoe. As impressive as the results are, it’s worth noting that none of the runners made giant leaps of the type promised in the lab tests.
Nike finally unveiled the shoes, dubbed Vaporfly, in both elite and consumer versions in March of this year. The announcement caused controversy. Ross Tucker, an influential sports scientist and blogger in South Africa, called for the shoe to be prohibited, saying it gave runners who wore it an unfair advantage. “Any device inserted into the shoe, and which purports to add to energy return or elastic recoil or stiffness, should be banned,” he wrote. A ban was unlikely, though, because none of the basic components in the Vaporfly are new—all modern shoes use foam, and racing flats have incorporated plates in the past. (Haile Gebrselassie wore an Adidas Pro Plate when he broke the world record in 2007.) What was new about Nike’s shoe was how the plate sat within the foam. “The magic,” said Stefan Guest, an Englishman who finessed the design, “is in the geometries.”
Officials at the International Association of Athletics Federations, the running world’s governing body, have been mostly silent about the Vaporfly, suggesting that the shoes conform to their vague rules about fairness. Not that Nike executives would have minded a little controversy over a new product. “We always joked, ‘Wouldn’t it be great if [a shoe] were banned?’” Nurse says. “Just by opening an orange box you have an advantage? That’s the holy grail.”
Kipchoge laces up his Vaporfly prototypes.
As the Tesla entered the bend at the top of the track at Monza for the first time, Kipchoge, Desisa, and Tadese were already settled into a steady, rapid rhythm, each of them about 12 meters behind the car and right behind the pacers.
Nike’s scientists had chosen the junior circuit of Monza’s historic racetrack because it lacked hills and corners, both of which hurt marathon times. It’s also looped, which meant a support team could regularly hand sports drinks to the Breaking2 runners to keep them hydrated and slow the depletion of glycogen in their muscles. The only drawback was the city’s slightly imperfect weather profile—Monza rarely gets much colder than 52 degrees Fahrenheit in early May, and studies suggest the optimum temperature for a world record is 50 degrees or colder.
The course at Monza also allowed the team to try a radical new approach to pacing. Most world records are set when runners spread their energy over the course of a whole marathon and move at an even, steady pace. But the most important function of pacers in Breaking2 wasn’t setting the speed—there was the Tesla, after all, which was driven with great skill at a near-constant 13.1 miles per hour—but rather blocking the wind. A number of studies have shown that the energy savings for a runner who is shielded from a headwind over the whole course of a fast marathon can be significant: enough to shave more than 60 seconds off the athlete’s final time.
Thirty minutes into the race, Desisa, wearing a white singlet, began to look profoundly uncomfortable.
This, however, is where science and the rules collide. In traditional marathons, the IAAF states that all pacers must start the race. Moreover, pacers typically drop out long before the race is completed, leaving the remaining competitors without any wind protection for the final 6 to 8 miles.
The Nike team began experimenting with having teams of pacers running in perfect formation, swapping in and out of the course, for the entire race. Breaking2’s lead physiologists, Brad Wilkins and Brett Kirby, who both have the laconic air of surfers, traveled to the University of New Hampshire’s Flow Physics Facility, the largest wind-tunnel testing site for runners and cyclists in the world.
In the middle of the Flow Physics lab, with twin 400-horsepower fans, Wilkins and Kirby placed an athlete on a treadmill and hooked him up to instruments measuring VO2 max and heart rate. They then placed other runners in front of him in a simple wall formation, and measured the benefits. With that data, they then used computational flow dynamics software to test as many other formations as they could think of: two stacked walls of three pacers, three rows of two pacers, and many others1.
Of all the potential formations, they found that a tight six-man arrowhead shape offered the most benefits. The savings the runners could make by hunkering behind the six-man arrowhead would be akin, Kirby said, to running the whole marathon downhill on a gradient of 2.5 percent. (As for the impact of the Tesla pace car and the large clock atop it, Nike ran tests both with CFD software and with test runners and determined that the car had no effect on air pressure 12 meters back, where the elite runners would be.)
In the end, a squad of 30 pacemakers was divided into six teams of three, with reserves ready in case of injury. On every circuit, the front three pacers would peel off and the rear three would take their place, while a new team of three pacers would fill in at the back row of the triangle. Many of the pacers were world-class athletes in their own right, mostly at shorter distances. (Speed, rather than endurance, was the most important prerequisite to pace Breaking2.)
Thirty minutes into the race, Desisa, wearing a white singlet, began to look profoundly uncomfortable. Even under optimal circumstances he has a ragged style, reminiscent of one journalist’s description of the great Czech runner of the 1950s, Emil Zátopek: “like a man wrestling an octopus on a conveyor belt.” But even so, it became obvious by the roll of Desisa’s shoulders that he was struggling. Near him, Tadese looked tough and busy—he is the Joe Frazier of running. He had twice run much faster than a two-hour-­marathon pace while breaking the world record for the half marathon; his problem has always been holding a fast pace for a whole marathon.
Kipchoge, meanwhile, ran with utter ease, his legs flicking along rhythmically and his torso as still as a rifle target.
“The magic,” says Stefan Guest, an Englishman who finessed Nike's Vaporfly design, “is in the geometries.”
To select the athletes who would compete in Breaking2, Wilkins and Kirby spent months analyzing the performances of professional runners, looking for key markers that reflected the necessary combination of speed and stamina. (Whether someone had run a sub-60-minute half marathon, for instance.) They were limited in their choices, because Nike does not have an exclusive hold over the world’s best distance athletes; indeed, most of the fastest marathoners of recent times, including the last four world-record holders, have been sponsored by Adidas. They also needed athletes who would forgo other marathons (and the attendant paychecks) to compete in Breaking2. Nike would not confirm what it paid its Breaking2 athletes, but one of Kipchoge’s managers told me that Nike offered two or three times the London Marathon rate. (London has offered star athletes appearance fees of around $250,000.) There were also time bonuses for the Breaking2 runners, including a rumored $1 million for a runner who broke two hours.
“It’s not your legs that run,” Kipchoge explained. “It’s your heart and your mind.”
Wilkins and Kirby began bringing in athletes to test each runner’s “critical velocity”—the maximum speed they could maintain for a long period. (The very best marathoners can sustain up to 95 percent of their critical velocity for 26.2 miles.) Despite never having run a marathon in less than two hours and 10 minutes, Tadese had run the fastest-ever half marathon in history: 58:23. What’s more, his numbers from the tests, particularly his running economy, were extraordinary. Desisa too showed that despite not having the fastest of marathon personal records—a 2:04:45—he had a huge engine. Both his numbers and his relative youth told the team he had potential to improve his marathon time dramatically. Nike booked him for the project too.
When Kipchoge came in for testing in September 2016, his results were not as uniformly eye-popping as the others’ had been. (One reason may have been his utter loathing of treadmills. That trip to Beaverton was the first time he had used one, and he ran on the machine like a newborn foal.) But by then, he had won four major marathons and the Olympic gold medal, and what he lacked in raw physical potential he made up for with … something.
Kipchoge was born just outside the village of Kapsisiywa, in a verdant, peaceful spot in Nandi County, the youngest of four children. If you believe his passport, he was born in 1984, but both his family and fellow runners believe he is a few years older than that document suggests. (This discrepancy is common among Kenyan runners, most of whom are born in rural areas and lack birth certificates.)
Eliud Kipchoge.
As a boy, Kipchoge ran 2 or 3 miles to school in his bare feet—often reversing and repeating the journey at lunchtime to eat at home. Even though he had won intramural running races at school, he never considered running as a career. About a year after he graduated from high school, he fell into conversation with a neighbor, Patrick Sang, a University of Texas graduate and Olympic silver medalist in the steeple­chase who had returned to his native Kenya to cultivate the next generation of runners. Sang wondered whether Kipchoge might have some aptitude for athletics. He gave Kipchoge running shoes and a training program, a combination of intense interval work, intermittent long, fast runs, and lots of easy running. Sang lives by the Kenyan running mantra: slowly by slowly. If an athlete has talent, he believes, it will blossom in its own time. Kipchoge’s flowered fast. Within three years of Sang’s guidance, Kipchoge was the 5,000-meter world champion.
Kipchoge spent the next few years winning a handful of medals at major championships at 3,000 and 5,000 meters, but he was never dominant in the way he is at the marathon. Since he started his marathon career at the 2013 Hamburg Marathon, he has won every race except his second marathon, in Berlin. Kipchoge’s two imperious wins in 2015 and 2016 at the London Marathon, which typically invites the strongest field, marked him as the best in the world, even before he won Olympic gold in Rio last year. Unlike such dominant athletes as Michael Phelps and Usain Bolt, Kipchoge has no real physiological point of difference from many other small, slight, super-fit Kenyans. His domination appears to result from his adamantine commitment, both in training and in races, and his self-belief. “It’s not your legs that run,” he explained to me one sunny morning at his training camp in the forest of Kaptagat, in Kenya. “It’s your heart and your mind.”
The Nike team encouraged Desisa’s coach to shorten and intensify some of his workouts in an effort to increase his speed.
Nike booked Kipchoge for Breaking2—they would have been crazy not to—and with their three runners set, they assembled the squad for the first time in late November at what they called Camp 12. The three-day event in Beaverton was an opportunity for the three athletes to meet with Breaking2’s growing sports science team, which now included two more contractors: Phil Skiba, a burly, garrulous physician from Chicago whose data-driven coaching has led triathletes to world championships, and Andy Jones, a dapper professor from the University of Exeter in England who had formerly advised the women’s world record holder, Paula Radcliffe. In Beaverton, this team looked on as the athletes recorded new data on the treadmill. The runners wore masks to measure their oxygen uptake and were tested while running in climate-­controlled chambers.
These tests did not touch on performance-­enhancing drugs. Although all three athletes received frequent blood and urine tests because of their elite status, they did not undergo any additional testing because of Breaking2. This omission dismayed many in the international running community who felt that Breaking2 didn’t just need to be clean—it needed to be seen as clean. The optics should have been especially important to Nike, whose star running coach Alberto Salazar is at the center of a US Anti-Doping Agency investigation into the use of controlled supplements in his program. When I spoke to Travis Tygart, head of the USADA, he said that Nike could easily have asked the IAAF to institute a tougher antidoping regimen for the three athletes involved in the project. Nike declined to do so, for reasons the company never satisfactorily explained to me.
The three principals were sent home from Camp 1 with GPS watches and heart-rate monitors that allowed the scientists in Beaverton to track every run they did. In January, the scientists then traveled to Kenya, Ethiopia, and Spain, where Kipchoge, Desisa, and Tadese lived and where the Nike team could observe the training in person. Tadese, for one, was fast but had trouble with stamina. After the science team watched him run a 9 x 1,200-meter session in Madrid, Skiba said, “We saw a monster out there today!” The science team asked Tadese’s coach, a Spaniard named, unimprovably, Jerónimo Bravo, to introduce more runs in the “heavy zone,” just below his lactate threshold, as well as longer runs of between 20 and 25 miles.
Desisa, meanwhile, appeared to log extraordinary mileage in his training in Ethiopia, sometimes as many as 200 miles a week, but did very little fast running. The Nike team encouraged Desisa’s coach to shorten and intensify some of his workouts in an effort to increase his speed.
With Kipchoge, they found very little they wanted to change. Skiba churned Kipchoge’s numbers to find areas of potential weakness and thought that he had found one: that the Kenyan “tapered,” or slowed down his training, a little too late before a big race. Months later, with more data in Skiba’s algorithms, it turned out that Kipchoge’s taper time was more or less perfect. His training continued unchanged.
Nike used a rotating group of six pacers to deflect headwinds, a strategy that wouldn’t be allowed in an official marathon.
In March, two months before before the attempt, the three athletes reunited on a bright and windy day in Monza for a “rehearsal” half marathon. The idea of the rehearsal was that Nike would test some of the techniques it hoped to apply in the race itself. They recruited pacers to help them test the interchanging teams and asked the principal athletes to try to run an evenly paced 60-minute half marathon. (It was so casual, this request: 60 minutes dead would win almost every half marathon in the world.) Despite unusually high winds and some shambolic organization from the pacers, Kipchoge and Tadese both looked comfortable as they ran half marathons below 60 minutes. Desisa’s form, however, was worrisome—he finished in more than 62 minutes.
Lingering over the test event, and over the whole project, was a question the team never satisfactorily answered among themselves: How far are we willing to go to break two hours?
“After I break two hours in May,” Kipchoge said, “the whole world will break.”
They could have run the race with a fan blowing the athletes, just as the sprinter Justin Gatlin ran a stunt 100 meters in Japan3. They could have run the race downhill all the way. There were any number of cheats Nike could have used. Matt Nurse, head of the NSRL, felt that Gatlin-esque stunts would be going too far. He was against anything “actively pushing or pulling” the athletes to their goal. And although the Nike sports science team had been working on methods to help the athletes run faster that breached the rules of the sport—such as having interchanging teams of pacemakers in front of them, or feeding them water bottles from moving bikes rather than stationary tables—the company felt those didn’t violate the spirit of the marathon as much as other potential transgressions.
This debate came to a head as Nike executives gathered at the March rehearsal. Some senior figures at the company, Nurse says, had become concerned that “certain parts of the media” had accused them of “cheating the sport,” and they were anxious about the negative publicity. And some wondered whether it would be better to scrap the two-hour attempt and simply go for an official world record, ratified by the IAAF, which would have precluded the use of the interchanging pacing groups. When word of these discussions reached Jos Hermens, who founded Global Sports, Kipchoge’s management company, he emailed Nike CEO Mark Parker to argue against watering down Breaking2.
“Nike should not be afraid,” said Hermens, paraphrasing his email. “Nike is meant to be … a rebel company! To make an unofficial record is fantastic! For the next five or 10 years everyone talks about Monza and Eliud and the shoes. You couldn’t have it any better.”
In the end, Nike listened to its athletes, and whatever anxiety the company was experiencing was overwhelmed by the enthusiasm of the runners themselves. Kipchoge’s explanation for why he wanted to attempt a sub-two-hour marathon would sound goofy were it not delivered so earnestly. He told me that he wanted to show ordinary people that they can overcome barriers in their own lives. It was a message he intended to carry not just to fans of the marathon, or to fans of sports in general, but to “8 billion”: every single person on the planet.
“After I break two hours in May,” he said, “the whole world will break.”
At Kipchoge's training camp in Kenya, runners set out at dawn.
The night in Monza had given way to a milky dawn, and at around 6:35 am on race day, with 11 miles down and 50 minutes on the clock, Desisa could bear no more. As the pacers swapped in and out in the transition zone at the top of the home straightaway, he began to trail Kipchoge and Tadese. So much of distance running is psychological. It’s mentally exhausting to run a pace you think you cannot maintain. Once Desisa knew he could not hang on to the two-hour pace, his chance of success was over and he separated from the pack like he’d been thrown off the back of a speedboat.
By the end of the next lap, just before the halfway mark, Tadese too began to fall behind Kipchoge, albeit less dramatically than Desisa. Out of the three elite runners chosen by Nike to attempt the sub-two marathon, only one was going to make it to halfway under 60 minutes.
Thirteen miles became 15 miles became 17 miles. Kipchoge was on schedule to break two hours.
As Kipchoge entered the second half of the race with no discernible change in his rhythm, he looked increasingly assured. When Kipchoge is running fast and feeling good, there is a tell in his form: As his hands pass his chest, it looks as if he’s using his thumbs to brush lint from the lapels of a dinner jacket. Thirteen miles became 15 miles became 17 miles. Still, he was on schedule to break two hours, and still he was brushing lint.
With three laps and roughly 4.5 miles to go, Kipchoge had been alone for 40 minutes. Trouw and Kirby cycled alongside him, offering words of encouragement and information about his progress. For a while, the display on the pace car had failed, and the two cyclists had informed Kipchoge of his splits themselves. Meanwhile, the pacers had interchanged in the transfer zone at the top of the home straight seamlessly. He seemed almost at peace, his face relaxed and his breathing steady.
But now his split times began to drift, almost imperceptibly at first, by a second or so a mile. The sub-two was still on, but the prospect was on a knife edge. The Tesla car, driven mercilessly at the 1:59:59 pace, began to edge away from him. The turnover of Kipchoge’s legs appeared to slow, just a fraction. At times, he appeared to smile broadly—a conscious effort, he later said, to relax and work through the pain.
Lelisa Desisa.
Although Kipchoge had not worn any monitors during the race, Nike scientists knew roughly what was going on within his body at this moment: His core temperature had risen by at least a couple of degrees, the glycogen stores in his muscles were nearly depleted, and those same muscles were suffering mechanical damage from the repeated impact on the road.
With two laps to go, Kipchoge was a few seconds off the target pace. There were many—including his managers, Trouw and Hermens—who thought he could somehow still break two hours. In his track career, Kipchoge showed electrifying speed to close races. If he could summon the strength for two rapid final laps, he might yet reach 1:59:59. As he rounded the bend into the home straight for the penultimate time, Hermens danced beside Kipchoge and bellowed encouragement.
On the final lap, Kipchoge needed to close with an impossibly fast 4:17 mile. While simple math told you he could not do it, still there was belief around the track. Trouw, on the bicycle, shouted, “Come on Eliud, history!”
As he rounded the final bend, urged on by an emotional team of pacers, Kipchoge knew the sub-two was beyond him. He drove onward, breaking the tape in two hours and 25 seconds—more than two and a half minutes faster than the world record. Having walked a few paces beyond the finish line, he found some hidden store of energy to run to his coach, Patrick Sang, who embraced him like a son. Kipchoge then lay down on the tarmac for a few seconds, and a salty smile spread across his face.
Afternoon tea at Kipchoge's camp in Kenya.
By the terms of its mission, Breaking2 was a failure. Nike tried everything to help a runner break two hours, and they couldn’t do it—not quite. But nobody who knows the sport, and who was at Monza to witness the attempt, could have seen Kipchoge’s run in such terms. Kipchoge’s 2:00:25 was one of the most impressive displays of distance running in history.
What’s more, Breaking2 has proved that a sub-two-hour marathon is not only possible but within reach. It’s hard for anyone, even Nike, to isolate the factors that contributed most to Kipchoge’s final time. Many believe the innovation that made the biggest difference was the arrowhead formation of pacers and the resulting wind deflection. The shoes probably shaved off some time too; not 3 or 4 percent, but something. The most significant factor may have been Nike’s audacity in making a sub-two push in the first place. “I’m sure there are people who think we’re a big, greedy, corporate monster that only wants to sell shoes,” Bignell says. “But this really was about human potential. I hope people will be inspired.”
Kipchoge could not summon the speed for the final laps. His effort in the first 24 miles had cost him too much.
If there was a grander narrative to extract from Breaking2, it was not about the possibilities of science but rather its limits. Data can only get you so far. What good, ultimately, came from having all of Kipchoge’s data? Kipchoge didn’t change his approach to training, even if—as both he and his coach told me—they were gratified to have their methods validated by scientists. As for the information Nike gathered on Desisa and Tadese, which did lead to real changes in training for both men, those insights could and should have been made already, by a coach. (Nike will not release any of its Breaking2 data, presumably so it can use it to enhance the company’s understanding of the hows and whys of fast running.)
There may also be lessons about the limits of the human body. Kipchoge ran at a two-hour pace for nearly a whole marathon but could not summon the speed for the final laps, because his effort in the first 24 miles had cost him too much. Andy Jones believes that Kipchoge simply ran out of fuel. “That might have been the limiting factor,” Jones says. “If we could find some way to have more glycogen on the start line or to consume more carbohydrates during the event … he might sustain that speed a bit longer.”
The single biggest scientific misjudgment in Breaking2 might have been running the race in Italy, in May. The bodies of elite marathoners get extraordinarily hot. To maintain a steady state of exertion while running fast, it’s necessary to dissipate that heat. The hotter it is, the harder that is to do. Literature suggests that it was at least a couple of degrees too hot for the very fastest times on May 6 in Monza. (The Nike scientists disputed this with me after the race. They thought the temperature about perfect.) There was also another, less scientific problem with how the race was staged at Monza: For long stretches of the course, it was deserted. Kipchoge told me afterward that he “missed the crowd” he gets at normal marathons.
Kipchoge admits no disappointment at having missed his goal. He is now back at his training camp in the forest of Kaptagat, where he stays for six days of every week away from his wife and three young children, who live in a well-appointed modern house in the nearby town of Eldoret. He trains twice a day, once in the morning and once in the afternoon, along with 30 other athletes. Between sessions, the runners do household chores; when it’s his turn, Kipchoge, a millionaire in a country where the average income hovers round $1,300 a year, pitches in and cleans the “long-drop” toilets.
After the race, Kipchoge sent me a text message, which contained a series of questions about my long-standing interest in the two-hour marathon: “Do you feel your dreams have come true? Do you feel complete now?”
I didn’t have a good reply, but in the minutes after he crossed the finish line, Kipchoge had shown his own answer to the same questions. He embraced Sandy Bodecker, whose early enthusiasm had launched the project, and pointed to the 1:59:59 tattoo on his wrist. The gesture was gracious, but there was also a pinch of yearning in it, and it swelled the heart to see.
Ed Caesar (@edcaesar) is the author of Two Hours: The Quest to Run the Impossible Marathon.
This article appears in the July issue. Subscribe now.
1Correction appended 3:15 PM ET 6/30/17: Updated to clarify the methods used to test Breaking2's pacer formations.
2Correction appended 3:15 PM ET 6/30/17: Updated to clarify the name of Nike's Camp 1 training event.
3Correction appended 2:45 PM ET 6/29/17: A previous version of this story misidentified the location of Justin Gatlin's 100-meter stunt run. It took place in Japan, not China.
I'm not sure when your last spacewalk was, but you probably remember it being a bit difficult to manipulate objects, what with the gloves and the microgravity and all. As such, your tools were tethered, though that doesn't necessarily mean your tool bag can't float away. Now imagine a robot arm trying to manage the same.
Even here on Earth, robots struggle with even the most mundane manipulations. Something as simple as picking up a bottle of water is tough for a robot, especially if it isn’t expecting the bottle to crumple under the force. Up in space, gripping objects takes on a whole new absurdity. Suction cups are right out, given they don’t work in a vacuum. And extreme temperature fluctuations rule out any sort of sticky adhesive.
But who needs glue when you’ve got geckos? Well, not actual geckos, but a grabber inspired by their remarkable, gravity-snubbing feet. A clever new kind of robotic gripper for space does just that, Stanford University and NASA JPL researchers report today in Science Robotics. It could not only help robots get a good grip on things like space debris, but supercharge robots right here on Earth.
Geckos can walk on walls because their feet are covered in tiny hairs, which themselves split into even smaller hairs known as spatulae. Each of these is so small that it makes extremely close contact with the surface, forming a minute attraction on a molecular level. Minute on its own, sure, but we're talking millions upon millions of spatulae. So those attractions, known as van der Waals forces, really add up, to the point where a gecko can stick to a wall.
This new gripper works on the same principle. It consists of pads covered with not hairs, but microscale wedges made of silicone rubber—the same stuff that those fancy spatulas are made of. So silicone rubber can withstand the heat of a stove, the chill of a freezer, and the wild temperature extremes of space.
Squares of gecko-inspired adhesive glow as they grip a piece of lit glass.
The handheld gripper consists of pairs of adhesive pads, whose microscale wedges point in opposite directions. The pairs are connected by cables—that's the robotic bit. “We pull them together and it creates an internal force," says Stanford University mechanical engineer Mark Cutkosky, co-author of the paper. "And that's what produces the adhesion." The wedges lie flat, making super close contact with the object, and boom, adhesion.
The question then becomes: How to scale this up to grip large objects? The trick is to make sure the load is the same on each pad. Otherwise, you risk one taking on too much and losing its grip. "If you don't do something to prevent it, it will just cascade across the whole area like an avalanche and the thing will come off," says Cutkosky. But because the pads are working together instead of independently, they evenly distribute the load.
Now, if you want to clean up space junk, your problems are two-fold. For one, you’d be working with flat or curved surfaces—think the shape of a rocket—without a good point to grasp with a traditional robotic hand. And two, this would be an extremely delicate operation. If you were to try to harpoon a piece of space junk or launch a net at it, you’d be throwing around some unwieldy forces.
This gripper could solve both problems. Because it lies flat, it could stick to a wayward solar panel like a suction cup, only without the suction. And it can tackle curved objects thanks to adhesive arms that fold out and embrace the surface. In their tests in zero gravity flights, the researchers would grab objects like cylinders and release their gripper’s grip and then … nothing. “Nothing,” Cutkosky says. “Absolutely nothing. It doesn't impart any bit of bobble or disturbance or extra momentum to it, and that's the effect you want in space.”
And like a lot of other tech developed for or by NASA, the gripper could well make its way into terrestrial robotics. Think of gripping not just space debris, but objects on an assembly line. After all, if it’s good enough for geckos and it’s good enough NASA, it’s good enough for the rest of us.
The Astrobee is a brilliant little robot that'll use puffs of air to autonomously float around the ISS.
Scientists and the military have often tussled over who calls the shots in space. The first astronauts were military test pilots. NASA made the space shuttle extra big to accommodate the spy satellites Pentagon planners wanted to launch. And it took 15 years for the Defense Department to release topographical maps gleaned during a classified shuttle mission so scientists could use them.
Now, two budget fights in Washington reveal how this uneasy relationship is tilting, once again, toward the needs of the military.
Last week, a House Armed Services subcommittee approved legislation calling for the creation of a “Space Corps” within the Air Force. This branch within a branch would operate independently of NASA, the nation’s civilian space agency. The idea is to prevent the Air Force from diverting space funding to other areas—say, Afghanistan or new fighter jets—and consolidate talent and expertise within in one office, Rep. John Rogers, R-Ala., who chairs the subcommittee on strategic services, said during the hearing. No one included a price tag on the proposal, but it would lead to a reorganization of  salaries and budgets.
Former defense secretary Donald Rumsfeld proposed the idea of a Space Corps in 2001. The Rumsfeld Commission suggested making it a branch of the military just like the Army or Air Force.  But the report came just a few months before  9/11, and the Air Force shelved the proposal.
Today, China and Russia wield significant power in space, where they can bring down Earth-launched missiles. China went so far as to shoot down one of its own satellites to test its capabilities in a space war, an exercise detailed in a 2015 report by the US-China Economic and Security Review Commission. “It used to be that space was a sanctuary,” says Victoria Samson, Washington director of the Secure World Foundation, a non-profit space policy think tank. “Once you got your asset (i.e. satellite) into orbit, you could do what you wanted to do. Now there’s a real concern that the backbone of our national security capabilities could be interfered with at a critical point.”
The plan, pushed by Rogers but opposed by Air Force Secretary Heather Wilson, lands on Capitol Hill at a time when military space technology consumes an ever-larger chunk of Pentagon spending. “Defending space assets is critical,” says Jeffrey Hoffman, a former shuttle astronaut who now leads the Man Vehicle Laboratory at the Massachusetts Institute of Technology. “When many people talk about militarization of space, they think of offensive weapons. I think the critical issue is being able to defend our communication and navigation assets.”
That requires money. Air Force leaders have requested a 20 percent increase for space systems: $7.7 billion in the 2018 federal budget the Trump administration sent to Congress in May. The request includes $4.3 billion for research and development and $3.4 billion for procurement. (Just what the Pentagon allocates for space in its classified "black budget" remains anyone's guess.) Meanwhile, Trump's budget sets aside $19.1 billion for NASA, a budget that funds more than 17,000 employees and grants to 10,000 academic scientists.
The idea of creating a new military space command even as the White House takes an axe to peaceful Earth-observing systems devoted to science. The Trump administration wants to cancel five NASA earth science missions and slash NOAA’s budget for studying the Earth, weather, and oceans—including ground and space-bound sensors.
Samson and other policy watchers say cuts to NASA’s and NOAA’s satellite monitoring programs are driven by the Trump administration’s hostility toward (and denial of) climate change. In fact, NOAA’s climate and weather programs observing satellites are also vital to keeping the United States safe, according to Anthony Busalacchi, president of the University Corporation for Atmospheric Research, a non-profit consortium of 100 universities focused on research and training in the climate and environmental sciences.
“These [Earth-observing satellite] assets that we have are central to protection to life, property and economic development and national security,” says Busalacchi. At the same time, he sees a future in which bright lines between military, commercial, and scientific missions into space blur. Already, the Pentagon’s National Geospatial Intelligence Agency buys Earth images from DigitalGlobe and other commercial satellite firms, and NASA hires SpaceX to make resupply missions and (purportedly) launch spy satellites for the National Reconnaisance Office.
Flight Lab: NASA's Global Hawk Drones Make Science, Not War
Pentagon Warns: 'Pervasive' Industrial Spying Targets U.S. Space Tech
Chinese Satellite Relays a Quantum Signal Between Cities
Given the political realities of science funding these days, Busalacchi thinks some creativity is in order. He argues for networks of smaller, cheaper satellites and other private sector solutions to replace the big, expensive government satellites that provide  easy targets for Congressional budget cutters. “The present path we are on is not sustainable,” he says. “We can't continue to put up these Battlestar Galacticas on a fixed budget.”
There also might be a way for the military and scientists to actually cooperate. "There’s no reason why the two different communities can’t figure out a way to work with each other,” says Samson. “There is a benefit to having people who understand how space works. It would be helpful to have people in the military who know that space is different than the ground. That might not be a bad thing.”
For now, the Space Corps proposal is just that. Members of the Senate Armed Services Committee are scheduled to mark up their version of a defense bill June 28. The full House will vote on Rogers’ plan when it returns from its recess after July 4.
NASA takes retired Global Hawk military drones and sets them up to fly dangerous missions monitoring some of the most extreme storms for better weather data.
You might think putting a helipad on Trump Tower would give the president's Manhattan residence an added veneer of affluence. After all, nothing conveys wealth and power quite like arriving at your own skyscraper aboard Marine One, right?
Nope. Not according to Penny, an artificial intelligence that uses satellite imagery to predict income levels in the Big Apple and how they change as you tinker with the urban landscape.
When I called up the president's Manhattan residence via Penny's clean, intuitive interface, it saw nothing but wealth. “PENNY is 100% confident that this is a HIGH median income area,” it reported. No surprise there. But when I selected a helipad icon from a toolbar at the bottom of the screen and dragged it, SimCity style, onto the roof, Penny changed its mind.
"Your adjustments have caused PENNY to reclassify this area as a MEDIUM-LOW median income area," the AI said.
Wait a sec. A helipad is an unambiguous symbol of wealth, isn't it? Does Penny know something I don’t, or has it misread the data? And why would anyone want a tool like this, anyway?
To answer those questions, it helps to understand how Penny came to be. Aman Tiwari, a computer scientist at Carnegie Mellon University, trained the AI by overlaying census data on high-resolution satellite imagery of New York and feeding it through a neural network. (He did the same thing with census data and satellite imagery of St. Louis, but each model can only predict household incomes in its respective city.) The AI started to associate visual patterns in the urban landscape with income, and different objects and shapes seemed to be highly correlated with different income levels—parking lots with low income, green spaces with high income, that sort of thing. Tiwari worked with data visualization studio Stamen to create an interface to probe those correlations. The UI lets you drag and drop baseball diamonds, solar panels, buildings, and other things all over town. The point isn't to design a city, but to learn more about what AI can, and can't, do.
Often, Penny performs intuitively. Plop a freeway or parking lot onto the Upper East Side and the AI predicts lower median income. Add some brownstones and parks to East New York and suddenly median incomes rise.
But every once in a while, Penny surprises you. Dropping the Plaza Hotel into Harlem makes Penny even more sure that it's a low-income area. Adding trees doesn’t help, either. Scenarios in which the AI defies intuition highlight both the power and the limitations of any system based on machine learning. “We don’t know whether it knows something that we haven’t noticed, or if it’s just plain wrong,” Tiwari says.
So which is it? Hard to say. "Sometimes an AI does amazing things, or locks onto some very intelligent solution to a problem, but that solution is inscrutable to us, so we don’t understand why it’s behaving in counterintuitive ways," says Jeff Clune, a University of Wyoming computer scientist who studies the opaque inner workings of neural networks. "But it’s simultaneously true that these networks don’t know as much as we think they know, and they often fail in bizarre or baffling ways—which is to say they make predictions that are wildly inaccurate when it’s obvious they shouldn’t be doing so."
This tension underpins a growing number of technologies people already interact with each day. Things like Facebook’s News Feed, which uses algorithms to tinker with the makeup of your social stream. Or Google’s new computer vision platform, Lens, which turns your phone's camera into a search box. Or the accident-avoidance protocols in Tesla’s cars. Not even the engineers who create the AI underpinning these products fully understand the decisions those sophisticated systems make.
Penny provides a glimpse at how AI and machine learning make sense of a city. "It’s not for deciding whether to put a hedgerow in your yard, it’s to help us understand how machines make sense of our world," says Jordan Winkler, the product manager for DigitalGlobe, the company that provided the imagery Penny uses. But he says Penny is mostly about getting people to think about how AI and machine learning actually work—or don’t.
Artificial Intelligence Finally Entered Our Everyday World
The Best Way To Transmit Satellite Data? In Trucks. Really
AI Helps Facebook's Internet Drones Find Where the People Are
Penny handles this task admirably, provided users take their time exploring. If Penny's early predictions match users' expectations, they won't probe further. They'll simply figure the AI is, well, intelligent. "It suggests all is well in the kingdom of AI, when in fact things are much more complicated," Clune says. Only after spending time with the tool and seeing it defy your expectations a few time do you begin to question how the model works.
Which brings me back to Trump Tower. Did adding a helipad decrease the predicted median income because helipads are bad, or because adding one altered some other feature the model correlates with wealth? Can you even assume that Penny is basing its decisions on trees, helipads, or buildings in isolation, or collectively?
To the extent that Penny causes people to ponder such things, it’s a valuable teaching tool. But it could be better. In its current incarnation, the model provokes questions more than it provides answers. One solution, Clune says, would be to have the model generate low-, middle-, and high-income neighborhoods. For the AI, the task would be more akin to an essay test than a multiple choice exam, and it would give people interacting with Penny a fuller understanding of what it sees, knows, and cares about.
Winkler and Tiwari say a generative version of Penny is in the works. Until then, give it a spin for yourself—and let me know if you find a good spot for that helipad.
What do other cities have that NYC doesn't? Well, Europe's great food halls, Sydney's robust ferry system, and Moscow's elegant train stations for starters. New York city-based architects Vishaan Chakrabarti and Gregg Pasquarelli analyze how urban metropolis from around the world stack up against the city that never sleeps.
Smartphones are pretty amazing when you think about it. Besides letting you watch Star Wars whenever you want, answer any question instantly, and play Super Mario Run during staff meetings, smartphones make it super-easy to learn about physics.
I've already shown you how to use your camera to figure out how fast your airplane is going, and last week I explained how to use a phone's barometer to measure the height of a building. Now I will show you how to measure distance using the accelerometer, just like I said I would at the end of that post.
An accelerometer measures acceleration, of course. But what is acceleration? Let me start with a definition in one dimension (so you don't have to worry about those annoying, but oh-so-useful, vectors):
This equation states that acceleration is equal to the change in velocity divided by the change in time. Looking at it differently, if I know that an object has a certain acceleration for a certain amount of time, I can determine the change in velocity for that object. To do this, I need to know the object's velocity before it accelerated. In general, if I know the velocity at one time (I will call this v1) then I can find the velocity at the next (v2):
Now imagine that your phone records the value of the acceleration 100 times every second. If the acceleration remained constant during each of interval of 0.01 seconds, you could calculate the velocity after each of these tiny 0.01 second intervals. If you keep doing this, you would have a plot of velocity vs. time (assuming you knew that very first velocity to start with).
OK, let's do this. I know you've got your phone handy, and you might even have an elevator nearby. (I will use a small car on a track so I can accurately measure its position.) You need some way of recording the acceleration. Many iOS apps do this, but I like NCSU MyTech.
Got everything? Good. Summon the elevator. When it arrives, put your phone on the floor, press record on your app, and ride all the way to the top. If there are people in the elevator, act like you're doing something important and they'll leave you alone. Once you reach the top floor, stop recording and grab your phone. Save the file and email it to yourself.
Before proceeding, we must deal with a small problem. If you place your phone on the floor of a stationary elevator, you'll get an acceleration reading of around 9.8 m/s2.  Why? Because of Einstein's equivalence principle. It states that the phone can't tell a difference between acceleration and a gravitational field. To fix this you must measure the initial acceleration and subtract it from every reading. This should give you a starting acceleration of 0 m/s2.
Now for a calculation. Load your data into a spreadsheet and subtract the stationary acceleration. We'll use this to determine the velocity from the acceleration and time values.  It should look something like this:
After loading the file into a spreadsheet, I deleted the acceleration columns I didn't need. My phone accelerated in the y-direction, but if you put your phone face-up in the elevator it probably accelerated in the z-direction. To calculate the velocity, I entered a value of zero in the first cell since it started from rest. For every other cell in that column, you simply enter the previous value, plus the acceleration multiplied by the change in time.
OK, let's plot it and see the velocity. You can create a plot in your spreadsheet, but I will use an online plotter. And I will cheat. Since I am using a cart on a track, I can determine the velocity with this Vernier tracking system.  Here is my calculated and measured velocity together.
You will notice that the two velocity plots are close, but not identical. In particular, my calculated velocity continues increasing after the cart should have stopped. I suspect this is my fault. I used a rubber band to affix my phone to the cart, and it might have shifted a bit, creating acceleration in the y-direction because of the gravitational field. Still, it's pretty close.
But I don't want to find the velocity of the elevator (or in my case, a cart). I want to find the position. We can use the velocity data to do this. Using the definition of average velocity, I can again solve for the new position based on the old position and the average velocity:
Yes, this leads to a small problem. Strictly speaking, I should use the average velocity, not the velocity at some particular time. However, with a small enough time interval, this should be approximately true. Let's try it and see what happens. Here is the position plot for both the calculation from the iPhone data and the motion encoder:
Again, the iPhone data suggests the cart keeps moving after it should have stopped. But overall, the method worked well.
Sometimes, a little snot goes a long way. And not just in the physical, stretchy sense. Today, some decade-old snot collected from the sinuses of cancer patients revealed a new technique to forecast how flu evolves.
Every winter, scientists try to predict which flus will be prominent during the following season. While the infection is simply a pesky use of sick days for some, it also causes millions of hospitalizations and hundreds of thousands of deaths annually. So to shore up prevention for the next year, the World Health Organization releases its predictions—based on global flu monitoring and animal studies to test the limits of herd immunity—to vaccine-makers. (Next year’s flu, FYI, is supposed to be an A/Michigan/45/2015 (H1N1)pdm09-like virus, an A/Hong Kong/4801/2014 (H3N2)-like virus, and a B/Brisbane/60/2008-like virus.)
But the methods aren’t perfect, because the flu is constantly evolving, and fast. So scientists have been trying to find better ways to predict flu strains by studying their evolution. Enter that really old snot. Or, more precisely, sinus fluid collected from four cancer patients back in 2006 and 2007. It’s the main player in a paper published today in Elife.
Normally, a person has the flu for just a week, which isn’t much time to study viral evolution. But these cancer patients were on medications that suppressed their immune systems—so when they caught the flu, their infections lasted for more than two months. In these patients, virus mutants were born, with roughly one random mutation arising in each virus every six hours. Some of those mutations made the virus survive better, so more and more of those showed up over time. And different mutants would battle for viral dominance—what evolutionary biologists call clonal interference, and we might as well call “going viral.”
Scientists were able to track that evolution thanks to the patients’ weekly nasal washes—preserved for almost a decade at Fred Hutchinson Cancer Research Center in Seattle. Starting in 2016, collaborators at the center sequenced the RNA of all the viruses, using a relatively new, high-resolution method called deep sequencing.1 Unlike your average genome sequencing, which gets an average estimate of the DNA or RNA sequence in question, deep sequencing checks and rechecks the sequence thousands of times, reading out all the different variants of flu living in that snot and recording how much of each mutant is present.
The researchers were curious about how flu evolution worked, and thought that cancer patients might have some weird biological forces acting inside them. So they deep sequenced for all the different mutants of one strain of flu called H3N2. “Going into the project, I thought the study wouldn't really tell us that much,” says Jesse Bloom, a biochemist at Fred Hutch and the senior author on the study. “I thought that the type of evolution that flu undergoes in any given individual—particularly cancer patients who are going to have unique, odd medical histories—might end up being very idiosyncratic.”
But that's not what they saw at all. Rather than observing unique dynamics in the four patients, the scientists found that the same mutations would eventually go viral in multiple patients: The virus populations inside each patient trended toward the same mutations. Some of these people weren’t even sick at the same time as one another.
AI Could Help Predict Which Flu Virus Will Cause the Next Deadly Human Outbreak
What's Inside a Flu Shot? Formaldehyde and Chicken Eggs
Darpa's Flu Fighters Ramp Up Veggie-Based Vaccines
Crazier still, some of those mutations ended up spreading across the globe a few years later. “It was incredibly surprising to see these kinds of similarities,” says Katherine Xue, the genetics grad student in Bloom’s lab who led the project. Looking inside cancer patients, the researchers discovered trends in flu evolution that could help scientists predict and monitor flu variants all over the place.
While the authors point out that the study is still just a proof-of-principle, being able to narrow down the possibilities for flu mutations would be exciting, says Katia Koelle, an infectious disease biologist at Duke who was not involved in the study. It could serve as an additional piece of the flu prediction machine. “If this was done on a large scale, looking across immunocompromised patients who have the flu, that might bound the set of mutations that might be interesting to follow up with surveillance,” Koelle says. And that type of study seems more feasible, says Xue, as high resolution gene sequencing gets cheaper.
This technique of following disease in patients with long infections could also become handy as researchers create new flu drugs. Unlike flu vaccines, which simply prime the immune system to kill a virus, flu drugs would stop an infection once it’s already started, hitting the virus directly. That’s great for high-risk individuals like cancer patients—but also risky, since putting pressure on the virus with a drug could create a drug-resistant super-flu. By deep sequencing the virus population before and after treatment with flu drugs, scientists could monitor drug resistance and determine how much of a problem resistance could be, says Bloom.
The study also let Bloom and Xue nerd out on the cool evolutionary battle going on inside patients. For decades, evolutionary biologists have just grown lab organisms like yeast and E. coli for long stretches to see how they evolve. Some of the dynamics, like the battles between rival mutants they call “clonal interference,” seem to be working exactly the same way in the lab and in the natural environment of a flu-infected human. So the study also showed that “evolutionary biologists basically had it right,” says Bloom—adding confidence to lab studies of viruses from flu to Zika. Not bad for a little snot.
1UPDATE 12:30 pm EST, 6/28/17: This story has been updated to correct the genetic material in the flu virus. It is RNA, not DNA.
Each year the World Health Organization determines which of the thousands of influenza variants are most likely to circulate. Flu vaccines then use these variants at the starting point when developing the annual flu shot. Find out what’s inside the influenza-warding shot.
Scientists and drug developers are always looking for new ways to hit cancer where it hurts. Recently, they’ve been focusing on metabolic pathways—how cancer cells hijack cells to support their own growth. One of the most promising new treatments for leukemia, for example, targets a single metabolic gene. And much of its therapeutic promise is built on the results of a 7-year-old study, published in Cancer Cell, that has been cited over 1,000 times.
Which is how it wound up in the Center for Open Science’s latest reproducibility project.
The Virginia-based non-profit has tasked itself with increasing the integrity of scientific research by re-running notable experiments—first in psychology, then in biology . For oncology research, the stakes are especially high when researchers can’t replicate a study’s results. So backed by millions of dollars from John Arnold’s philanthropic foundation, the group is replicating the 29 most important cancer papers of the last few years. Today, it published its latest findings on two papers, including the landmark Cancer Cell study. And, unlike their first batch of re-dos from January, things looked pretty good.
Back in 2010, scientists at the University of Pennsylvania published the original paper about a single mutation to a gene called IDH. Alterations to that gene change an enzyme that plays a crucial role in energy production—turning normal cells into tumorous ones. One of those new molecules was detectable in the blood of leukemia patients, so researchers and drug developers got interested in it as a potential biomarker for the cancer. Turns out about 20 percent of leukemia patients have the mutation. Now, the FDA is evaluating the first IDH-targeted cancer drug, with many more in clinical trials.
Science Needs to Learn How to Fail So It Can Succeed
Fighting Cancer's Crisis of Confidence, One Study at a Time
John Arnold Made a Fortune at Enron. Now He's Declared War on Bad Science
Science is an additive process; each result lets you ask and evaluate new questions. So if that first IDH paper was flawed, the whole field could be called into question. This spring, then, scientists at the West Coast Metabolomics Center in Davis redid three of their experiments, examining the relationship between IDH and and that biomarker in cell lines and patient samples. For all three, they were able to reproduce results statistically similar to the original paper. While it’s not entirely surprising—dozens of papers in the last few years have looked at this mechanism—it still provides some relief for an emerging field.
“Metabolomics is really grappling with realizing its full potential right now,” says Tim Errington, who is leading the cancer reproducibility project for the Center for Open Science. “By replicating the exact same experiments we help inform techniques and standards so that sharing this data is faster and easier for everyone going forward.” The results also help solidify IDH as a valuable pathway for drug developers to mine. Including, says Errington, those based on Crispr. “This is a single point mutation at the site of an enzyme,” says Errington. “We think the byproduct it makes is actually a driver of tumorigenesis, so it would be great to go back and just snip it out.”
Now, Crispr-based oncology therapeutics are a long way out. But the robust results are a heartening sign that current IDH-based drug candidates are on the right track—especially because 49 out of 50 cancer drugs that start in the clinic never make it out. To combat the high failure rate, pharma companies try to throw more and more compounds at the wall, hoping that some will stick. But Errington says one of his hopes for the reproducibility project is to show that it’s more efficient to take a harder look at biological pathways earlier on, and whittle down candidates to only the most promising few.
“It’s a balancing act between validation and innovation,” he says. “But we need to be a lot more happy with failing early, in cell culture and animals, where we can do more work more quickly and affordably.” Reproducibility can help build a solid foundation for a drug before a company invests in long, expensive human trials.
Of course, it’s not always that simple.
Take the second study the Center for Open Science replicated—a 2011 paper for a different kind of potential leukemia treatment, a class of molecules called BET inhibitors. This time, scientists were able to replicate the first few experiments in cells, but when it came to testing the treatment in mice, the results fell flat. They didn’t see the same increased survival rates with the treatment. That could be because they had a bigger sample size than the original study, or it could be because they used chemicals to create cancer in the mice instead of radiation. Or it could be because the effect just isn’t as robust as initially observed.
That’s the messy thing about reproducibility. Each paper is a story, a sequential layering of evidence. And one unreplicated result doesn’t invalidate the whole thing. So where do you draw the line?
ELife, the journal that published today’s results, called both papers “substantially replicated.” But Errington says those kinds of labels shut down the conversation instead of fostering it. “The devil’s in the details,” he says. “That’s where you’ve got to embrace nuance if we’re ever going to increase the integrity and efficiency of scientific work.”
The reproducibility project will be continuing to roll out individual results over the next 12 months. They’re hoping to complete all 29 replications by next year, at which point they’ll also publish an analysis of any trends they find. It’s too early to say what those might be. But it’s safe to assume they won't end cancer research’s crisis of confidence just yet.
There are 3 billion ways for something to go wrong with your DNA. But diseases caused by an error to a single gene—what geneticists call "big ticket" mutations—are quite rare. That’s why doctors don’t routinely recommend whole genome sequencing. But as the cost of sequencing continues to plummet and companies offer more and more ways for consumers to peer into their DNA, physicians are trying to figure out how genetic data might work into your next check-up. Can these tests really help people improve their health? Or will they confuse doctors, scare patients, and drive up unnecessary costs?
To find out, primary care doc Jason Vassy recruited a handful of colleagues from around Boston to sequence the full genomes of 50 patients—the first randomized trial of whole genome sequencing in primary care. They expected to find maybe one person with a marker for one of those rare, monogenic diseases. Instead, they found 11. “That’s a shockingly high number,” says Vassy. “If you look at the list of the conditions we found, most primary care physicians have never heard of them. It would would be crazy to think that 20 percent of people have a disease like that.”
It’s also crazy because almost none of those patients showed symptoms of their supposed genetic diseases. Except for one woman with a history of skin rashes that turned out to be something called variegate porphyria, all the patients were otherwise healthy-looking 40 and 50 year-olds. They didn’t have the symptoms their genes suggested they should, for things like chondrodysplasia punctata (shortened limbs and seizures) and Romano-Ward syndrome (when the heart to take longer than usual to recharge between beats).
Vassy found one of these rare pathogenic variants in the very first full genome his lab got back from the Illumina sequencing facility in San Diego. “It was like, OK, that’s pretty surprising, but it could also just be chance,” he says. The next few that came in all looked pretty normal. But as all 50 patients had their genomes sequenced, a clear pattern emerged: Markers of genetic disease were both more prevalent and less visible than they had ever expected. Vassy and his co-authors at Brigham and Women’s Hospital, Baylor College of Medicine, and Harvard Medical School published those results in a paper out today in The Annals of Internal Medicine.
The study wasn’t just looking to see what mutations showed up; they were also following how that information shaped patient care. They found that most doctors did the right things with the genetic data they were given (as judged by an outside panel of genetic counselors). Most patients didn’t experience undue anxiety over the new genetic information. And compared to a control group that just got a standard family history, the sequenced group made more changes for their health; exercising more, eating better, taking vitamins.
But the genome sequencing added an average of $350 to each patient’s overall health care costs—mostly in the form of extra tests, imaging, and visits to specialists. And with the exception of one patient, the tests didn’t have any drastic impact on health outcomes, at least over the 6 months they followed patients.
In the grand scheme of things, $350 and a few extra doctor’s visits might seem like a small price to pay for early detection. But think of those added costs on the scale of millions of patient genome sequences every year. It’s possible that over a longer timeframe, those discoveries could drive costs down—by stopping surgery for slow-mounting diseases, or avoiding certain medications—but the researchers say they’ll need more time and a bigger cohort to know for sure.
A Chinese Genome Giant Sets Its Sights on the UItimate Sequencer
Genos Will Sequence Your Genes—And Help You Sell Them to Science
The Go-To Gene Sequencing Machine With Very Strange Results
The study is also too small too to suggest that all primary care doctors are ready to be put on the front lines of genetic medicine. Boston is a well-connected community with plenty of genetic specialists. It’s not obvious that physicians in less well-resourced places of the country are equally well-prepared to deliver quality genetic care. But their patients may soon need them to be, whether they're ready or not.
Today there are only 4,000 certified genetic counselors in the US—or one for every 80,000 Americans—to serve the needs of a populace with ever more access to genetic testing. Who will interpret those results, especially in rural areas where there might be one counselor every 1,000 miles? Even with the rise of telemedicine, which the office of Veterans Affairs has employed to stretch its handful of counselors over the thousands of veterans it serves, the US is headed toward a severe bottleneck in genetic services.
For now at least, it seems like high prices and minimal insurance coverage will keep genetic testing from becoming part of the pricking and prodding routine of primary care. At a dinner to thank all the physicians who participated in the study, Vassy asked them all if they would order up genome sequencing for all their patients if they cost $1,000. Nobody raised a hand. He kept asking lower amounts. The hands stayed put. It wasn’t until he hit the $100 mark that he started to get a few takers. “With few exceptions whole genome sequencing is still really in the realm of research,” says Vassy. “But these doctors participated because they know genomics is coming, and they want to see evidence it works before they change anything about how they practice medicine.”
Panelists from the White House Frontiers Conference discuss the new frontiers of space, science, medicine, transportation, and cities. And their must haves for frontier expeditions.
The world is becoming one big clinical trial. Humanity is generating streams of data from different sources every second. And this information, continuously flowing from social media, mobile GPS and wifi locations, search history, drugstore rewards cards, wearable devices, and much more, can provide insights into a person's health and well-being.
Dr. Sam Volchenboum (@SamVolchenboum) is the director of the Center for Research Informatics at the University of Chicago, a board-certified pediatric hematologist and oncologist, and the cofounder of Litmus Health, a data science platform for early-stage clinical trials.
It’s now entirely conceivable that Facebook or Google—two of the biggest data platforms and predictive engines of our behavior—could tell someone they might have cancer before they even suspect it. Someone complaining about night sweats and weight loss on social media might not know these can be signs of lymphoma, or that their morning joint stiffness and propensity to sunburn could herald lupus. But it’s entirely feasible that bots trolling social network posts could pick up on these clues.
Sharing these insights and predictions could save lives and improve health, but there are good reasons why data platforms aren’t doing this today. The question is, then, do the risks outweigh the benefits?
Although social media platforms get press for being useful in predicting, and possibly preventing, suicide, the possibility that those platforms could see into the future before a patient has even visited the doctor is, for now, hypothetical. But it’s not far-fetched.
Let’s say Facebook released a large set of de-identified data, such as members’ location, travel, likes and dislikes, post frequency, sentiment, browsing, and search habits. Based on these data, a researcher could build models that predict physical and emotional states.
For instance, a data set consisting of social media posts from tens of thousands of people will likely chronicle the journey that some had on their way to a diagnosis of cancer, depression, or inflammatory bowel disease. Using machine-learning techniques, a researcher could take those data and study the language, style, and content of those posts both before and after the diagnosis. They could devise models that, when fed new sets of users’ data, could predict who will likely go on to develop similar conditions.
And such a system would not need to look only for hard and fast symptoms like fevers or weight loss. Seemingly unimportant and unrelated data—like purchasing anti-nausea medicine or watching a documentary on insomnia—could end up fueling a set of predictive rules that indicate that a user might have a certain medical condition. The point is that our digital trail leaves many clues, both subtle and overt, to our overall health and well-being. How we use those data for good is another issue.
As a clinician, I support integrating data and putting the troves of information to use for society’s benefit. One of the reasons I cofounded Litmus Health, a data science company, was to help researchers better collect, organize, and analyze data from clinical trials, and in turn, use those data to improve health outcomes for society writ large. However, significant regulatory, ethical, technical, and societal considerations require caution.
From a regulatory perspective, all companies bear some responsibility to care for their users’ data, as defined in their terms of service. Unfortunately, what has been exposed in cases like a 2014 Facebook study and in research from Carnegie Mellon is that terms of service and/or privacy policies are overly complicated, no one reads them anyway, and users just blindly sign them.
Companies can demonstrate an ethical “do no harm” obligation to their users by having a straightforward and easy-to-understand data policy, and by not using personal data in inappropriate ways. An ethical framework for big data must consider identity, privacy, data ownership, and reputation. For most firms today, releasing users’ data to build predictive models without their consent would go against their established value systems. But obtaining consent may be as trivial as someone mindlessly clicking through an exorbitantly long terms-of-service agreement.
AI Could Target Autism Before It Even Emerges—But It's No Cure-All
A Campus Murder Tests Facebook Clicks as Evidence of Hate
Medical Devices Are the Next Security Nightmare
If companies are going to ask users to share their data and participate in an experiment, they should be more transparent about how the data are collected, used, and shared.
Let’s say a social network has an algorithm that analyzes a user’s activities— things they complain about, articles they share, friends’ posts they like, among other things. The AI could potentially identify a pattern suggesting the presence of a medical condition.
Now imagine being able to link across social networks and also to other available data streams from wearables, sensors, and mobile devices. All of a sudden, the predictive value of these disparate data streams could become very high. For example, posts about headaches and nausea, combined with a gradually decreasing step count on a Fitbit, cell phone GPS data indicating trips to the pharmacy, and typing accuracy demonstrating a slow, almost imperceptible loss of coordination could all portend an ominous condition.
A perfect predictive system might be heralded as a medical breakthrough, but sometimes a typo is just a typo, and most people with headaches and nausea do not have brain tumors.
Using social media cues to help someone recognize that they may have the flu could prompt users to seek testing or treatment, both relatively benign and inexpensive interventions. But a cancer scare suggested under similar circumstances could carry more serious consequences, ranging from emotional trauma to expensive and potentially harmful tests and treatments. When amortized over millions of users, the potential logistical and financial implications for the healthcare system could be enormous. While algorithm-based predictions can be useful and are widely applied in many areas of our lives now, these examples show why these same predictions carry more weight in the realm of health and health care, and therefore their use should be closely governed and monitored for potential benefits and risks
As a clinician, I believe that consumers should be able to freely access the health data they generate across all streams. The benefits far outweigh the risks, and physicians are seeing more and more patients request access to their complete medical records. Patients are taking an active role in their treatment plans; it ought to be medical professionals' jobs to facilitate their ability to do so.
Individuals should be able to opt in to allow providers to collect and track their data for health predictions. Companies would need to carefully determine tracking criteria for specific diseases, and at what point they would notify the user that they are at risk. Once notified, the user would have the option to receive more information or send their data directly to their healthcare provider. For this to work, new data governance and stewardship models will be required, and legal protections for people and their data will become increasingly important.
The people, companies, and organizations that hold private data have a big responsibility. If they're going to use these data to make better predictions about health and disease, then everyone needs to work together to better understand the expectations and responsibilities of all parties. The technical, legal, and social barriers are significant, but the potential for improving people’s health is tremendous.
Dr. Sam Volchenboum (@SamVolchenboum) is the director of the Center for Research Informatics at the University of Chicago, a board-certified pediatric hematologist and oncologist, and the co-founder of Litmus Health, a data science platform for early-stage clinical trials. WIRED Opinion publishes pieces written by outside contributors and represents a wide range of viewpoints. Read more opinions here.
Of all the cancers in the world, lung cancer is the deadliest. This year, the National Cancer Institute estimates it will kill 150,000 people, despite a growing number of more targeted therapies. Part of the problem is there are a lot of different mutations that can cause the most common form of it: non-small cell lung cancer. And while the genetic profile of a tumor can tell doctors which treatment to prescribe, finding the mutation is an exercise in trial and error. Screen for the most common mutation, then try that drug. If it doesn’t work, screen again and try something else. It can take weeks or even months to find a treatment that clicks, taking time patients don’t have.
But that could all soon change. On Thursday, the Food and Drug Administration approved the first next-generation-sequencing-based test, from Thermo Fisher Scientific, that can tell you how different drugs will work for you, based on the genetic makeup of your tumor. And it only takes four days to get back results. In many ways, it represents the leading edge of precision medicine’s maturation from a buzzword in grant applications and investor pitch decks to a real, workable product that can actually improve patient outcomes.
Getting the FDA’s approval took nearly two years and 220,000 pages of data. (That’s like reading Karl Ove Knausgaard’s 6-book autobiographical memoir front to back 61 times in a row. Talk about My Struggle.) But the process has helped clarify the agency’s thinking about how to regulate personalized treatments going forward, opening up doors for tech that's still in the pipeline.
Oncomine Dx Target Test and the PGM Dx System it runs on. The technology can tie multiple genetic markers to approved cancer drugs using just 10 nanograms of DNA.
The panel, called Oncomine Dx Target Test, takes a tiny amount of tumor tissue and reports on alterations to 23 different genes. All that information is useful for physicians, but three in particular—ROS1, EGFR, and BRAF—are the the most crucial. That’s because those mutations have drugs to match: Precision medicine chemotherapies from Pfizer, Novartis, and AstraZeneca. The test can be performed at any CLIA-certified lab, and it’s already being offered by two of the largest oncology-focused ones.
Getting the FDA to approve that amalgam of tests wasn’t easy. “Putting multiple genes and multiple drugs on the same test; all of these are firsts,” says Joydeep Goswami, Thermo Fisher’s president of clinical next generation sequencing. “That put the technology under extraordinary scrutiny.” The FDA usually approves one diagnostic for one product or drug—that’s it. But the whole point of precision medicine is to tailor treatments for patients based on their genes, and a bunch of one-off genetic tests aren’t going to deliver on that promise. So a multi-gene, multi-drug panel is kind of a big deal.
For one, it allows Thermo Fisher to add new genetic markers and new drugs almost as quickly as they become available—turning it into a one-stop shop for cancer treatment recommendations. But it also opens the door for things like liquid cancer biopsies, which sequence tumor DNA floating in a patient’s blood to make earlier diagnoses, and personalized immunotherapy treatments, which boost the body’s natural defenses to fight cancer cells. These promising fronts in the war on cancer will only make it to the oncology ward if the US adopts a regulatory framework flexible enough to put the individual—genes, environment, history and all—at the center of the conversation.
Luckily, that’s exactly what the FDA is starting to do. Last February, the agency held a day-long public workshop to get feedback on its approach to gene-based cancer tests (a document that hasn’t been updated since 2014). A big question that came up was how the FDA would validate different genes and all their clinically relevant mutations. The agency gave an idea of how it might work last summer, describing a seal of approval it could put on genetic databases that already exist and meet high standards of clinical criteria. These would potentially include both commercial and academic databases as well as government repositories like the National Center for Biotechnology’s ClinGen Dosage Sensitivity Map, which curates more than 1,000 druggable genes. Those databases would be publicly available, so device and drug developers can all work to build treatments from the same set of information.
How Doctors Could One Day Use Your DNA to Cure You
Obama's Anti-Cancer Moonshot Will Need More Than Research
Medicine Is Going Digital. The FDA Is Racing to Catch Up
The FDA is currently working through feedback to develop its final guidance. Once that’s published, the agency will start taking requests for database stamps. It also plans to take what it’s learned from those discussions and put it into updating guidance for companion diagnostics like Thermo Fisher’s. But those processes move at the speed of federal bureaucracy. And until that happens, FDA officials say that individual approvals like Thermo Fisher’s are perhaps the best place for developers to look for guidance. “Then you have the whole summary of its performance, which gives a roadmap to the next manufacturer,” says Zivana Tezak, an associate director of personalized medicine at FDA’s Center for Devices and Radiological Health.
To keep up with more 200,000+-page submissions, the agency is also hiring more statisticians, bioinformaticians, and data scientists. It even has a new dedicated unit to recruit those kinds of staffers, the ones most equipped to discern and evaluate approvals in the age of big data. Genomes, of course, being some of the biggest data out there. And while scientists and drug developers are optimistic about what this all means for the future of precision medicine, doctors are excited for how they can now use it today.
“Time is of the essence with lung cancer patients,” says Collin Blakely, a thoracic oncologist at UCSF who is unconnected to Thermo Fisher. “Being able to profile multiple genes all at once and quickly hone in on the key driver is a game changer for how we treat the disease.” It prevents physicians from choosing treatments with incomplete information—oftentimes they opt to just put people on chemotherapy, even though it doesn’t work as well as targeted treatments, because patients can’t afford to wait six to eight weeks to get all the necessary tests.
And while the multi-gene panel probably costs more upfront, Blakely says most hospitals would actually see savings in the long run, by preventing patients from getting sicker and showing up in emergency rooms. “It more than makes up for it to get a patient on the right drug right away through precision medicine,” he says. For the 225,000 people who will get diagnosed with lung cancer this year, at least some will get a chance to find out.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
If Friday’s rocket livestream wasn’t enough for you, you’re in luck—this Sunday, SpaceX is set to launch its second Falcon 9 of the week. This time, the company is firing a shiny new rocket from California's Vandenberg Air Force Base. It’s the fastest turnaround yet for two SpaceX launches, but if it's going to launch as many satellites as it says, there are more rapid-fire liftoffs to come.
These two payloads weren’t originally planned as a double-whammy. A pneumatic valve pushed the BulgariaSat launch back from Monday, June 19. And after initially being delayed from October—then December, then April—today’s liftoff is actually a bit ahead of schedule. This launch delivers 10 more satellites to the fleet that telecommunications company Iridium is building in low Earth orbit. To get the new satellites situated just-so, the launch window is exact, scheduled for 1:25:14 pm Pacific time.
Roughly an hour after it lifts off from Space Launch Complex 4E, the Falcon 9 will dispense one satellite every 90 seconds. These newcomers will be tested for a few weeks before joining the rest of their brethren to beam voice and data information. After dispensing the satellites into orbit, the first stage of the Falcon 9, like a few before it, will land vertically on a drone barge in the Pacific Ocean, to be reused in later launches.
So far, Iridium has only contracted new, unused rockets from SpaceX to place its constellation of satellites. But they may soon get on board with Musk’s rocket reusability plan, if older rockets mean faster launches. Their 2010 agreement with SpaceX originally aimed to send around 70 satellites up by the end of 2017, and that the endpoint has now been delayed to 2018.
Despite the delays, the car-size satellites being launched today have come a long way since they were first trucked in pairs from Phoenix to Vandenberg. Today’s satellite delivery brings Iridium’s total up to 20, with six more SpaceX launches scheduled to deliver the remaining 55 satellites in the next year or so. If all goes well, the end of the day will mark two down, six to go, with precedent set for rapid launches to come.
Physicists have wondered for decades whether infinitely dense points known as singularities can ever exist outside black holes, which would expose the mysteries of quantum gravity for all to see. Singularities—snags in the otherwise smooth fabric of space and time where Albert Einstein’s classical gravity theory breaks down and the unknown quantum theory of gravity is needed—seem to always come cloaked in darkness, hiding from view behind the event horizons of black holes. The British physicist and mathematician Sir Roger Penrose conjectured in 1969 that visible or “naked” singularities are actually forbidden from forming in nature, in a kind of cosmic censorship. But why should quantum gravity censor itself?
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Now, new theoretical calculations provide a possible explanation for why naked singularities do not exist—in a particular model universe, at least. The findings indicate that a second, newer conjecture about gravity, if it is true, reinforces Penrose’s cosmic censorship conjecture by preventing naked singularities from forming in this model universe. Some experts say the mutually supportive relationship between the two conjectures increases the chances that both are correct. And while this would mean singularities do stay frustratingly hidden, it would also reveal an important feature of the quantum gravity theory that eludes us.
“It’s pleasing that there’s a connection” between the two conjectures, said John Preskill of the California Institute of Technology, who in 1991 bet Stephen Hawking that the cosmic censorship conjecture would fail (though he actually thinks it’s probably true).
The new work, reported in May in Physical Review Letters by Jorge Santos and his student Toby Crisford at the University of Cambridge and relying on a key insight by Cumrun Vafa of Harvard University, unexpectedly ties cosmic censorship to the 2006 weak gravity conjecture, which asserts that gravity must always be the weakest force in any viable universe, as it is in ours. (Gravity is by far the weakest of the four fundamental forces; two electrons electrically repel each other 1 million trillion trillion trillion times more strongly than they gravitationally attract each other.) Santos and Crisford were able to simulate the formation of a naked singularity in a four-dimensional universe with a different space-time geometry than ours. But they found that if another force exists in that universe that affects particles more strongly than gravity, the singularity becomes cloaked in a black hole. In other words, where a perverse pinprick would otherwise form in the space-time fabric, naked for all the world to see, the relative weakness of gravity prevents it.
Roger Penrose in Berkeley, California, in 1978, nine years after proposing the cosmic censorship conjecture.
Santos and Crisford are running simulations now to test whether cosmic censorship is saved at exactly the limit where gravity becomes the weakest force in the model universe, as initial calculations suggest. Such an alliance with the better-established cosmic censorship conjecture would reflect very well on the weak gravity conjecture. And if weak gravity is right, it points to a deep relationship between gravity and the other quantum forces, potentially lending support to string theory over a rival theory called loop quantum gravity. The “unification” of the forces happens naturally in string theory, where gravity is one vibrational mode of strings and forces like electromagnetism are other modes. But unification is less obvious in loop quantum gravity, where space-time is quantized in tiny volumetric packets that bear no direct connection to the other particles and forces. “If the weak gravity conjecture is right, loop quantum gravity is definitely wrong,” said Nima Arkani-Hamed, a professor at the Institute for Advanced Study who co-discovered the weak gravity conjecture.
The new work “does tell us about quantum gravity,” said Gary Horowitz, a theoretical physicist at the University of California, Santa Barbara.
In 1991, Preskill and Kip Thorne, both theoretical physicists at Caltech, visited Stephen Hawking at Cambridge. Hawking had spent decades exploring the possibilities packed into the Einstein equation, which defines how space-time bends in the presence of matter, giving rise to gravity. Like Penrose and everyone else, he had yet to find a mechanism by which a naked singularity could form in a universe like ours. Always, singularities lay at the centers of black holes—sinkholes in space-time that are so steep that no light can climb out. He told his visitors that he believed in cosmic censorship. Preskill and Thorne, both experts in quantum gravity and black holes (Thorne was one of three physicists who founded the black-hole-detecting LIGO experiment), said they felt it might be possible to detect naked singularities and quantum gravity effects. “There was a long pause,” Preskill recalled. “Then Stephen said, ‘You want to bet?’”
The bet had to be settled on a technicality and renegotiated in 1997, after the first ambiguous exception cropped up. Matt Choptuik, a physicist at the University of British Columbia who uses numerical simulations to study Einstein’s theory, showed that a naked singularity can form in a four-dimensional universe like ours when you perfectly fine-tune its initial conditions. Nudge the initial data by any amount, and you lose it—a black hole forms around the singularity, censoring the scene. This exceptional case doesn’t disprove cosmic censorship as Penrose meant it, because it doesn’t suggest naked singularities might actually form. Nonetheless, Hawking conceded the original bet and paid his debt per the stipulations, “with clothing to cover the winner’s nakedness.” He embarrassed Preskill by making him wear a T-shirt featuring a nearly-naked lady while giving a talk to 1,000 people at Caltech. The clothing was supposed to be “embroidered with a suitable concessionary message,” but Hawking’s read like a challenge: “Nature Abhors a Naked Singularity.”
The physicists posted a new bet online, with language to clarify that only non-exceptional counterexamples to cosmic censorship would count. And this time, they agreed, “The clothing is to be embroidered with a suitable, truly concessionary message.”
The wager still stands 20 years later, but not without coming under threat. In 2010, the physicists Frans Pretorius and Luis Lehner discovered a mechanism for producing naked singularities in hypothetical universes with five or more dimensions. And in their May paper, Santos and Crisford reported a naked singularity in a classical universe with four space-time dimensions, like our own, but with a radically different geometry. This latest one is “in between the ‘technical’ counterexample of the 1990s and a true counterexample,” Horowitz said. Preskill agrees that it doesn’t settle the bet. But it does change the story.
The new discovery began to unfold in 2014, when Horowitz, Santos and Benson Way found that naked singularities could exist in a pretend 4-D universe called “anti-de Sitter” (AdS) space whose space-time geometry is shaped like a tin can. This universe has a boundary—the can’s side—which makes it a convenient testing ground for ideas about quantum gravity: Physicists can treat bendy space-time in the can’s interior like a hologram that projects off of the can’s surface, where there is no gravity. In universes like our own, which is closer to a “de Sitter” (dS) geometry, the only boundary is the infinite future, essentially the end of time. Timeless infinity doesn’t make a very good surface for projecting a hologram of a living, breathing universe.
Despite their differences, the interiors of both AdS and dS universes obey Einstein’s classical gravity theory—everywhere outside singularities, that is. If cosmic censorship holds in one of the two arenas, some experts say you might expect it to hold up in both.
Horowitz, Santos and Way were studying what happens when an electric field and a gravitational field coexist in an AdS universe. Their calculations suggested that cranking up the energy of the electric field on the surface of the tin can universe will cause space-time to curve more and more sharply around a corresponding point inside, eventually forming a naked singularity. In their recent paper, Santos and Crisford verified the earlier calculations with numerical simulations.
But why would naked singularities exist in 5-D and in 4-D when you change the geometry, but never in a flat 4-D universe like ours? “It’s like, what the heck!” Santos said. “It’s so weird you should work on it, right? There has to be something here.”
In 2015, Horowitz mentioned the evidence for a naked singularity in 4-D AdS space to Cumrun Vafa, a Harvard string theorist and quantum gravity theorist who stopped by Horowitz’s office. Vafa had been working to rule out large swaths of the 10^^500 different possible universes that string theory naively allows. He did this by identifying “swamplands”: failed universes that are too logically inconsistent to exist. By understanding patterns of land and swamp, he hoped to get an overall picture of quantum gravity.
Working with Arkani-Hamed, Luboš Motl and Alberto Nicolis in 2006, Vafa proposed the weak gravity conjecture as a swamplands test. The researchers found that universes only seemed to make sense when particles were affected by gravity less than they were by at least one other force. Dial down the other forces of nature too much, and violations of causality and other problems arise. “Things were going wrong just when you started violating gravity as the weakest force,” Arkani-Hamed said. The weak-gravity requirement drowns huge regions of the quantum gravity landscape in swamplands.
Jorge Santos (left) and Toby Crisford of the University of Cambridge have found an unexpected link between two conjectures about gravity.
Weak gravity and cosmic censorship seem to describe different things, but in chatting with Horowitz that day in 2015, Vafa realized that they might be linked. Horowitz had explained Santos and Crisford’s simulated naked singularity: When the researchers cranked up the strength of the electric field on the boundary of their tin-can universe, they assumed that the interior was classical—perfectly smooth, with no particles quantum mechanically fluctuating in and out of existence. But Vafa reasoned that, if such particles existed, and if, in accordance with the weak gravity conjecture, they were more strongly coupled to the electric field than to gravity, then cranking up the electric field on the AdS boundary would cause sufficient numbers of particles to arise in the corresponding region in the interior to gravitationally collapse the region into a black hole, preventing the naked singularity.
Subsequent calculations by Santos and Crisford supported Vafa’s hunch; the simulations they’re running now could verify that naked singularities become cloaked in black holes right at the point where gravity becomes the weakest force. “We don’t know exactly why, but it seems to be true,” Vafa said. “These two reinforce each other.”
The full implications of the new work, and of the two conjectures, will take time to sink in. Cosmic censorship imposes an odd disconnect between quantum gravity at the centers of black holes and classical gravity throughout the rest of the universe. Weak gravity appears to bridge the gap, linking quantum gravity to the other quantum forces that govern particles in the universe, and possibly favoring a stringy approach over a loopy one. Preskill said, “I think it’s something you would put on your list of arguments or reasons for believing in unification of the forces.”
A Master of Umbral Moonshine Toys With String Theory
A Cosmic-Ray Hunter Closes in on Super-Energetic Particles
Virtuoso Experiment Reveals the Quantum Secret to Superconductivity
However, Lee Smolin of the Perimeter Institute, one of the developers of loop quantum gravity, has pushed back, arguing that if weak gravity is true, there might be a loopy reason for it. And he contends that there is a path to unification of the forces within his theory—a path that would need to be pursued all the more vigorously if the weak gravity conjecture holds.
Given the apparent absence of naked singularities in our universe, physicists will take hints about quantum gravity wherever they can find them. They’re as lost now in the endless landscape of possible quantum gravity theories as they were in the 1990s, with no prospects for determining through experiments which underlying theory describes our world. “It is thus paramount to find generic properties that such quantum gravity theories must have in order to be viable,” Santos said, echoing the swamplands philosophy.
Weak gravity might be one such property—a necessary condition for quantum gravity’s consistency that spills out and affects the world beyond black holes. These may be some of the only clues available to help researchers feel their way into the darkness.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
You've heard the PSA: Recycle that plastic water bottle, or else archaeologists will be digging it up thousands of years from now. What you probably haven't heard is that archaeologists are already digging up plastic water bottles that are thousands of years old.
This is not evidence of time travel. These bottles aren't clear, and they don't have labels. They're pitch black—made by indigenous tribes who coated large, woven bulbs with a tar-like substance called bitumen. Scientists have known about these bottles for years. But what they hadn't considered was whether these plastic bottles contributed to the declining health in some old societies, like the Native American tribes that once lived off the coast of California. Skeletons dating back thousands of years evidence a mysterious physical decline. A new study, published today in the journal Environmental Health, measured the toxicity of making plastic from oily bitumen, and of storing liquid in the bottles.
Modern water bottles aren't that different, really. But frozen, reused, even microwaved, there's not much risk of the liquid in them leaching enough harmful molecules—BPAs, DEHA, PET—to cause health problems. These ancient plastics are a different story, however. Bitumen is basically asphalt. Yes, basically the same stuff (when mixed with rocks, sand, and aggregates) that is used to pave roads. It's dense, viscous or semi-solid when cool, but turns into a malleable slop when heated up. It also releases chemicals known as polycyclic aromatic hydrocarbons, or PAHs, known to cause cancer (cigarettes, burning wood, and other smoky sources produce PAHs) and other health problems.
California's Channel Islands sit a few miles off the coast of Los Angeles. Like the mainland, they are dry. "They are also one of few places in North America where you find a more or less continuous population in the Americas, at least until the Industrial Age," says Sabrina Sholts, an anthropologist at the Smithsonian’s National Museum of Natural History in Washington, DC. "The earliest evidence we have of of people on these islands comes from about 13,000 years ago." One of the outstanding mysteries about these island-dwelling tribes—collectively called the Chumash—is why their overall health began to decline, beginning around 5,000 years ago.
Skeletal remains dating back to that time start to exhibit poor bone quality, reduced stature, smaller skulls, and bad teeth. Now, lots of things can cause these issues. Some researchers posit that malnutrition, poor sanitation, infectious disease, and lack of resources brought about by increased population on the islands might be the culprits. But Sholts developed a different hypothesis.
On certain beaches in Southern California, you have to watch your step to avoid stumbling on a nasty little ball of tar. Some come from the oil rigs offshore, but these balls have actually been washing ashore for thousands of years, the result of submarine seepage. This is bitumen, and for thousands of years Native Americans in this region had used it to build boats, make weapons, and craft water bottles. Sholts went to graduate school at UC Santa Barbara, and she recalls that one of her colleagues working with bitumen advised her early on to wear gloves and a mask to handle the stuff. She had recently learned about how Native Americans in this region had stored water in bitumen bottles. "I became uncomfortably curious, and not sure how strongly I should consider this as a factor in some of the changes I had seen in the skeletal record," she says.
After getting her PhD and a job at the Smithsonian, she took the opportunity to explore that curiosity. She conscripted a colleague, Kevin Smith, to recreate the bottle-making process. Smith is an archaeologist at UC Davis who has permits to do work on the Channel Islands—much of which are protected.
To make a Chumash-style plastic bottle, you start by weaving a bottle-shaped basket. Then you combine bitumen and pine pitch in an abalone shell. You have to melt them together, but you don't place the abalone directly onto the fire. Instead, you roast some pebbles in a fire until they are piping hot. Remove the pebbles, place them in the abalone, and stir them around until the the mixture is wet, hot, and bubbly. Finally, use a stick to paint the molten bitumen over the bottle-shaped basketry.
A bitumen-coated water bottle.
For scientific accuracy, Smith collected his materials from the islands—the plants for the basket, the pitch, the bitumen, even the pebbles. The only modern interlopers—besides Smith himself—were a cardboard windscreen and a mass spectrometer to measure that ominous white smoke.
Next came the environmental assessment. As everyone in this Dr. Oz-ified society knows, plastic bottles pollute whatever is put in them. So after the bottles cooled (the team made two of them, each with a different bitumen-to-pitch ratio), Smith and Sholts shipped them off to colleagues in Sweden. They filled the bottles with water, let them sit for two months, and analyzed the results. The results: accumulations of naphthalene, phenanthrene, and acenaphthalene, all of which are compounds with known toxicity.
The Chumash also likely ate food off bitumen-coated objects. So the Swedish cohort also filled the bottles with olive oil to test whether toxins would leech into lipids. (Of course, the Chumash didn't have olive oil, but it's a serviceable proxy for the fatty fish and marine mammal meat that comprised the Chumash diet.) "If you want to measure uptake directly, you need to have soft tissue," says Sholts. "We were looking to get a baseline measurement of what the fat could do."
Fracking's Problems Go Deeper Than Water Pollution
Chemical From Plastic Water Bottles Found Throughout Oceans
Toxic Soup: Plastics Could Be Leaching Chemicals Into Ocean
The air sampling showed that the smoke produced while making one of these proto-plastic bottles had a much higher concentration of toxins than cigarettes. The water had very low concentrations of the toxic compounds. The olive oil took up way more PAHs, but the researchers noted that the olive oil they bought had detectable PAHs in it before they put it in the bottle.
Transporting water—in bottles, through pipes—has always been tricky business. It's not just a matter of using a material that doesn't leak. Water is the universal solvent. Given enough time, and the right pH, it will dissolve just about anything. Sometimes this means leaching toxins from substances strong enough to contain the leaks, like the lead from Flint's pipes.
According to Sholts' study, the bitumen those Chumash islanders used to make their bottles didn't leak enough chemicals into their water to account for their skeletal problems. The ones who made the bottles did, but Sholts points out that they probably weren't making the bottles frequently enough to accumulate dangerous levels of the toxins in their bodies. However, she says this study was limited by the fact that they had to do everything by proxy—they only had Chumash skeletons to go by. "It’s hard to say how much of any chemical exposure would induce health problems," she says. "It's dependent on dose, duration, and when in the person's life they were exposed." She says the field also needs more research on how to detect toxic organic compounds in bone. Toxicologists mostly concerned themselves with the recently dead, so much of the published research only looks at toxin uptake in soft flesh. "Bones are all I have," she says.
You can't see it, but there's a lot going on inside a glass of tap water. The faucet aqua is essentially an invisible cocktail of sulfates, resins, varying levels of lead, and so much more. Find out what else is inside.
You have no real way of knowing if your town, your family, or your children face the kind of water contamination that exposed everyone in Flint, Michigan, to lead poisoning. Not because Flint is an outlier–it may, in fact, be the norm—but because no one has enough data to say for sure.
Five state and local officials in Flint face involuntary manslaughter charges for failing to alert the public to the looming health crisis there. Yet a recent Reuters report found 3,000 geographic areas in the US with lead poisoning rates twice that of Flint. But you would be hard-pressed to determine whether you lived in one of them because the United States lacks the data—and data collection requirements—needed to know for sure whether people are being poisoned by their drinking water. President Trump’s proposed cuts to the Environmental Protection Agency’s could make it even harder to know.
“The data gaps are so huge. It is abominable. We have a huge number of people in this country living completely in the dark,” says Eric Feigl-Ding, an epidemiologist at Harvard University’s Chan School of Public health and founder of the public health website ToxinAlert.org.
Some 170,000 public water systems provide water to Americans. The federal government regulates that water under laws like the Safe Water Drinking Act and the Lead and Copper Rule, but leaves it to states, utilities, and property owners to test that water and enforce the laws.
Yet the number of taps that must be tested remains woefully small. The rules require water systems serving at least 100,000 people, for instance, to test 100 taps every six months. The requirements decrease from there. Systems that serve, say, 90,000 people must test just 60 taps. Smaller systems, only five. And certain systems qualify for reduced testing. In some cases, that means testing once every nine years.
Erin Brockovich Thinks This California City Is the Next Flint
How One Company Contaminated Pittsburgh’s Drinking Water
Here's How Hard It Will Be to Unpoison Flint's Water
“Would you really rely on a sample of 100 people in New York or Boston?” says Feigl-Ding. “In no universe is that going to give you a statistically significant result. That’s just ludicrous.”
Lead is measured in parts per billion, or ppb. If more than 10 percent of a given system’s taps exceed 15 ppb (referred to as the “action level”) the system operator must inform the public of the risk and report the violation to the state, which reports it to the EPA. But some researchers worry that even that threshold is too high and creates a cycle in which water systems worry more about compliance than keeping people safe.
“The system detects violations. It’s not set up to be useful,”  says Jeffrey Griffiths, professor of public health at Tufts University. “If you had nothing but lead going straight to your house, nobody would know that, because all that gets captured is there was a violation.”
Griffiths says something as simple as GPS technology could vastly improve the ability of ordinary citizens to monitor their own risk levels. The hurdle to establishing such a system at the national level is each state has the authority to address the problem—or not—as it sees fit. “There’s a common understanding around what water contamination is,” he says, “but the degree to which they are enforced or there’s real help from the state is completely variable.”
Private property owners have no obligation to test their taps, a situation that includes privately owned wells serving small towns across the country. That said, if private property owners do detect lead in their water systems, they must address it. That can quickly get expensive. For that reason, most property owners skip testing entirely, says Angel Hsu, director of the Data-driven Environmental Group at Yale University. (The same problem applies to lead paint, another common cause of childhood poisoning.)
“This problem demonstrates the need for a federal program to underwrite lead clean-up,” Hsu says. “Cash-strapped people and municipal governments do not have the resources necessary to remedy such a broad and persistent hazard.”
The EPA stores much of this data on water contamination, but the Center for Disease Control measures the damage already inflicted on those living with lead in their water. The data there are even patchier. The CDC compiles data on blood lead levels, collected from children 1 through 5 by pediatricians nationwide because lead is most damaging in children. But nothing requires states to report that data, which explains why so much of it is outdated. Several states report no data at all.
“You’re talking about only 25 to 30 states that consistently report blood lead levels. And poor and rural people who don’t go to the doctor are less likely to be reported,” says Feigl-Ding. “By the time kids have elevated lead levels, gosh, it’s almost too late.”
Feigl-Deng created ToxinAlert.org to be a central repository of crowdsourced data about lead and other water contaminants. It aggregates data from the EPA and the US Geological Survey, which measures toxins in groundwater, and adds data from states and independent researchers. The portal allows anyone to order a test and have the results logged on its national risk map.
“It’s a public alert warning system,” Feigl-Ding says. “People can type in your address or zip code and it gives you all the alerts around the area.”
The goal is to give ordinary citizens the ability to hold local leaders accountable, because often, as was the case in Flint, the problem goes beyond inadequate information to a lack of political will to address the problem. A recent USA Today investigation found seven water systems in Ohio failed last year to notify the public of heightened lead levels within 60 days last as mandated by the EPA. Several more in Arizona that reported unsafe water levels to the government years before only alerted the public after USA Today began its investigation.
Michigan Attorney General Bill Schruette determined last week that local health officials' failure to alert the public to a Legionnaires' disease outbreak warrants charging them with involuntary manslaughter. He alleges that negligence led to the death of Robert Skidmore in December, 2015. “Involuntary manslaughter is a very serious crime and a very serious charge,” Schruette said during a news conference. “It holds significant gravity and weight for all involved, and I don’t take this lightly, not one bit.”
In most cases, that negligence usually comes down to a lack of resources and concerns over the cost of addressing the problems, problems that could be exacerbated by President Trump desire to cut the EPA's funding by 30 percent because federal funding helps defray the cost of testing. “If state agencies are struggling right now, what are they going to do when their budgets continue to go down?” says Lynn Thorp, national campaign director for the non-profit Clean Water Action. “That’s the gap I’m most freaked out about.”
Testing water and mitigating contamination is expensive, Griffiths says, but states must consider the repercussions of poisoning an entire generation of people. Lead poisoning can cause developmental delay in children, reduced IQ levels, anemia, and hypertension in adults. “These are kids who are not going to be as smart as they would have been,” Griffiths says.
Given that, you can consider spending the money for more and better testing an investment in the future.
Update: SpaceX successfully launched BulgariaSat-1into orbit and re-landed its Falcon 9 booster.
CAPE CANAVERAL, FL––It’s been almost three months since SpaceX sent a Falcon 9 roaring on its second flight through the skies off Florida’s space coast. On a Thursday in March, the spaceflight company became the first to recover, refurbish, and relaunch an orbital-class rocket. The event marked a watershed moment for an aerospace industry that has been disposing these complex and insanely expensive launch vehicles for over 60 years. Friday, SpaceX will attempt the feat again.
SpaceX plans to launch another recovered booster from the history-worn Pad 39A at Kennedy Space Center—what CEO Elon Musk calls the ‘Times Square’ of launch pads, thanks to its service during the Apollo and Space Shuttle era. Liftoff is scheduled for a two-hour launch window that opens at 2:10 PM Eastern. (Update: New liftoff time is set for 3:10 pm.) The mission was originally scheduled for Monday but delayed by SpaceX so engineers can replace a pneumatic valve in the rocket's nose cone—another component they're hoping to recover after the launch.
The Falcon 9 booster being used for SpaceX’s second-ever reflight was launched earlier this year on January 14th. The rocket flew 10 satellites for the Virginia-based communications company Iridium from Vandenberg Air Force base in California—part of a long-term contract to deliver Iridium’s 75-strong constellation of satellites to low-Earth orbit.
On its next flight, the reusable Falcon 9 will carry Bulgaria’s first geostationary satellite to orbit, where it can deliver television and data service to customers in the region. With every reusable rocket launch, SpaceX hopes to inch itself closer toward its ultimate goal of perfecting rapidly reusable flight hardware and sending humans (cost-effectively) to Mars.
SpaceX had plenty riding on that earlier Iridium mission. Just four months before, a fire that ignited in the upper-stage of a Falcon 9 soon turned into a fireball that destroyed the rocket, the launch pad, and an expensive Facebook-operated satellite. (Making for not one, but two, very sad billionaires that day.) Mark Zuckerberg didn’t have it worse than Musk though, who called the incident “to be the most difficult and complex failure we have ever had in 14 years.”
The Air Force and the FAA gave SpaceX the green-light to resume launches that began with Iridium-1 from California. After a successful launch and recovery on the Just Read the Instructions robotic drone ship, SpaceX cleaned and tested the Iridium-1 booster, then shipped it to Florida. SpaceX successfully test-fired the refurbished Falcon 9’s engines last Thursday at Pad 39A.
After the Falcon 9 launches the 8,000 lb BulgariaSat-1, the rocket’s booster will come flying home for a touchdown on autonomous drone ship number two, Of Course I Still Love You. If it lands upright, it will tally SpaceX’s successful booster landings to 12—five on ground at Cape Canaveral's Landing Zone 1 and seven at sea. SpaceX isn’t switching between land and sea just to keep its social media fresh: A returning rocket booster needs a lot more fuel to maneuver back to land, and heavier payloads like the BulgariaSat-1 satellite, bound for deployment in higher orbits, require more thrust and leave less fuel in the booster’s tank.
SpaceX Wants to Launch Thousands of Satellites. What on Earth For?
Watch SpaceX Relaunch a Commercial Cargo Capsule for the First Time
The SpaceX Explosion: What You Need to Know
SpaceX is also testing recoveries of the rocket’s payload fairing (aka the pointy nose cone) that houses commercial and military satellites (Dragon doesn’t require one). Musk says this “will certainly help, because each of these costs several million.” While recovering more parts of the Falcon 9 is something SpaceX is interested in, the next goal is to narrow the launch to relaunch window of a single flight-proven booster to 24 hours.
On its last cargo run for NASA, SpaceX launched the first refurbished Dragon capsule to the International Space Station. Following liftoff of that milestone mission, SpaceX brought home the booster to ground at Cape Canaveral. “It's starting to feel kinda normal to reuse rockets,” Musk tweeted. “That's how it is for cars & airplanes and how it should be for rockets.”
So how much is SpaceX actually saving by not sending a rocket to its doom after a single launch? A lot. The current advertised price for a SpaceX commercial launch on a brand new Falcon 9 rocket is around $62 million. SpaceX President Gwynne Shotwell said the company expects almost a 30 percent cost reduction when reusing a Falcon 9 booster. While it’s unclear how much, SpaceX intends to pass some of those savings onto their customers after breaking even on the $1 billion spent developing reusable technology.
The rescheduling of the BulgariaSat-1 launch to Friday has added some drama to the mission and the mission SpaceX has slated next. Why? Because the launches could lift off within 48 hours of each other. SpaceX hopes to deliver a second set of Iridium satellites on Sunday at 1:24pm Eastern. If they can pull it off, that means bi-coastal booster landings and tons of rocket GIFs to watch this weekend.
After seven years of promising to repeal and replace the Affordable Care Act, Senate Republicans are now closer to achieving that goal than ever before. Thursday morning, they finally unveiled their secretly drafted healthcare bill. It is not, as some had hoped, a drastic departure from the House's version, which was passed last month. While being slightly less "mean," in that it provides more financial support to some lower-income groups, the Senate bill still lands punches to Obamacare in all the same places.
It still ends the healthcare mandate that every American be insured. It still gives power to the states to drop many of the essential benefits required by the ACA, including maternity care, emergency services, substance abuse, and mental healthcare treatments. It still ends the Medicaid expansion that helped 20 million people get insured (although one year later than the House proposed). And it places a cap on Medicaid, while simultaneously slashing about $840 billion from the entitlement program over the next 10 years to pay for enormous tax cuts for the wealthy. All of which adds up to very bad news for patients—but especially the 2.5 million Americans currently struggling with an opioid addiction.
In closed-door sessions this week, Republican senators from states hardest hit by the current drug crisis tried to soften those deep Medicaid cuts by advocating for a separate funding stream of $45 billion over 10 years for substance abuse treatment and prevention, costs currently covered Obamacare's Medicaid expansion. According to data compiled by the Associated Press, that expansion accounted for 61 percent of total Medicaid spending on substance abuse treatment in Kentucky, 56 percent in Michigan, and 43 percent in Ohio.
Instead, what they got was a bill that promises to deliver $2 billion for 2018. That's it.
"This bill takes away the number one tool we have in the fight against opioids—Medicaid treatment," said Ohio's Democratic senator Sherrod Brown in a statement. Medicaid is the largest national payer for addiction and mental health treatment. Last year, Ohio alone spent $939 million on the drug epidemic, 70 percent of which was covered by Medicaid. If you evenly divided up the Senate's $2 billion earmark between all 50 states, Ohio's portion would cover just over 4 percent of their opioid-related expenses.
Addiction advocates like Gary Mendell say the provision is an insult to the people fighting to end the epidemic. "We need much more than that," says Mendell, who founded the nonprofit Shatterproof after his son Brian's opioid addiction sent him into a depression and he took his own life. "Right now we spend $5 billion a year on substance abuse in this country, and only 1 out of 10 people are actually in treatment," says Mendell. If you do the math, he says it would take more like $600 to $700 billion over the next decade to get 10 out of 10 people the help they need.
That, of course, is not about to happen. The latest Health and Human Services budget for the opioid crisis is more than three times as large as it was two years ago, but it's still only $811 million. At a recent budget hearing, HHS Secretary Tom Price defended the Trump administration's allocations and questioned how much difference Medicaid actually makes. But according to the President's own newly appointed Commission on Combating Drug Addiction and the Opioid Crisis, it makes all the difference.
At their first meeting last Friday, the commission's members spent the bulk of their time criticizing the House's new healthcare bill and pressing for more Medicaid spending in the Senate's version. Mendell, who sits on the commission, says the legislation announced this morning is a complete disappointment, undermining any progress the panel hoped to make toward crafting effective anti-addiction policies. "It's a death sentence," he says.
This Sniff Tech Could Protect Cops From Synthetic Opioids
How America Is Battling Its Horrific Opioid Epidemic
Obamacare's Demise Is a Looming Disaster for Mental Health
Not the least because of the bill's promise to end essential benefits for people covered under Medicaid expansion by 2020. Under the ACA, insurers are required to cover contraception, preventative care, emergency services, and mental health and substance abuse services. But in a Republican-led healthcare future, states would have the option to not require insurers to cover substance abuse treatments. “Insurers can stop providing coverage for mental health and addiction services," Baltimore health commissioner Leana Wen said in a statement responding to the Senate bill. "At a time of a public health emergency around opioid overdose, this will cost millions more lives.”
And don't forget that without the ACA's pre-existing conditions protections, states could also allow insurers to charge higher premiums to all the people who fall under a broad umbrella of mental health conditions. Which is important because people struggling with depression are more likely to be prescribed addiction-forming painkillers like hydrocodone and oxycodone, according to a new nationwide study published on Tuesday. "Depressed patients are at a greater risk for misuse and overdose of opioids," says study leader John Markman, director of the University of Rochester Medical Center's Translational Pain Research Program. Which is why treating addiction is about more than just titrating down people's prescriptions. "The research shows that patients who have access to multiple forms of treatment—cognitive behavioral therapy, physical therapy, as well as drugs—have the best outcomes," he says.
But in order to benefit from those kinds of anti-addiction interventions, people have to be in the system in the first place. And if there's one thing the Republican healthcare bills are poised to do, it's take people out.
In August the DEA announced plans to ban Kratom, a herbal substance used to treat pain, anxiety and in some cases opioid addiction. A group of tenacious users got the agency to back down and extend public comment until December 1st. Now a tough decision lies before the DEA.
If humans walked like robots, engineers already would have perfected zero-effort, mechanically-assisted walking. But what about people who bounce on their toes, power walkers, those who sashay? Habits, diseases, and disabilities can affect someone’s gait in unique ways. An idealized exoskeleton needs to be both easily accessible and personalized.
The Chipotle of exoskeletons doesn’t quite exist yet. Computers still struggle to anticipate how people will move—they're literally a moving target. From a data standpoint, humans are noisy, says Katherine Poggensee, a biomechatronics researcher at Carnegie Mellon. Plus, “they have brains, so they adapt over time.” And although humans generally find the easiest way to do any motion, very few people have the physical and spatial awareness to explain why one stride feels easier than another. That's why researchers are turning to algorithms to make exoskeletons more efficient.
So far, automatically tuning an exoskeleton’s force, and the timing of that oomf, is faster and better than hand-tuning. Thursday, in a paper published in Science, Poggensee and her fellow researchers outline an algorithm that calibrates an exoskeleton to best assist its user. To do that, they use a type of optimization that’s also helped govern how animated characters interact with their environments in CGI.
Instead of supplying users with standardized assistance, these control algorithms set themselves up like an eye doctor who flips through lenses while asking “better, or worse?” But instead of actually asking users, the algorithms rely on sensor feedback. To minimize the energy required to walk, for example, they track respiration to calculate metabolic rate, then optimize to minimize the calorie burn.
This algorithmic tuning can only happen in a lab, on a treadmill, where there are machines to perform and analyze these extra measurements. The idea is that eventually, you could get fitted for your exoskeleton or robotic prosthetic limb in a clinic, then transfer your personalized profile to the outside world. And in this study as well as others, automatically tuned exoskeletons do successfully lower the energy it takes to walk.
This is an improvement over previous versions of exoskeletal tuning, which were slower, and in some cases, demanded more effort than normal non-assisted walking. For simpler approaches that relied on a brute-force sweep through many different options, “the numbers get really hard to deal with,” says Daniel Ferris, who has developed similar algorithms to calibrate exoskeletons. There are different mathematical approaches to automating this tuning, but the most effective ones all start by guessing how a human will respond, then monitoring their actual response while offering up different calibrations.
Because the algorithms also incorporate stochasticity, or randomness, into their structure, the exoskeletal controllers evolve differently for each walker. In the method published this week, the controller starts off by trying eight different tuning profiles. Based on which of those work well, it generates eight new profiles to try, with a few wildcards thrown in. Sometimes the wildcards are better, and other times worse, but they all force the controller to evolve. As the wearer inevitably adapts to the exoskeleton's assistance, the control loop also adapts to the wearer.
For Poggensee’s proof-of-concept tests, 11 human guinea pigs donned an ankle exoskeleton over one of their shoes and took a stroll on a treadmill. As they walked, a respiratory mask measured the oxygen they inhaled and the carbon dioxide they exhaled, calculating the energy cost of walking. Meanwhile, the tuning algorithm cycled through four sets of eight different patterns of assistive torque, varied in timing and amount of force.
Soft Robot Exosuits Will Give You Springier Steps
We Take Hyundai’s Iron Man-Inspired Exoskeletons for a Spin
We Try a New Exoskeleton for Construction Workers
After about an hour of this strolling, the algorithm pinned down the optimal timing and torque to minimize the energy cost of each walker’s gait. Each participant’s ideal pattern was different—a little more help at toe-off, less force at the middle of the stride–so that when you look at the torque profiles of all the walkers, you see “a bunch of different shapes,” says Poggensee.
Energy expenditure, of course, is only only one way to assess the effectiveness of an exoskeleton. Studies like this one can also quantify activity by monitoring voltage across local muscles, using a method called electromyography. But there are plenty of other metrics to optimize, like heart rate, limb speed, and balance. Or, if you’re willing to delve into the wild west of subjectivity, comfort and perceived effort.
Taking those additional factors into account—and expanding those factors to address a wider range of needs—could be more of a challenge, says Ferris. He points out that these optimization methods do well with a handful of parameters in the lab, but the real world ultimately demands control of many knobs at nearly infinite settings. Navigating a crowded subway car, for example, requires attention to more than just energy. There’s also minimization of exposure to armpits, and additional calibration for manspreading. Before those factors can be optimized, they’d need to be measured—which might be work for another algorithm entirely.
Technology – from steel to server farm – has always changed what it means to be human. But what happens as we meld with ever more capable machines?
One morning, a few months ago, I didn’t wake up. My 29-year-old heart had suffered from a sudden and complete cardiac arrest as I slept.
Sudden cardiac arrest kills nine out of 10 people who experience it, and it is a leading cause of death in the United States. But it isn’t supposed to afflict a healthy young person. Fortunately, my wife is a light sleeper, and unflappable under pressure. She came to my rescue, and saved my life by performing CPR until the medics responded to her 911 call.
Lee Cooper (@leecoo4) works in corporate development for a private biotech company in Cambridge, Massachusetts. He has previously published articles on healthcare markets and innovation.
Genetic testing would soon reveal that I suffer from a rare genetic disease called Long QT Syndrome, or LQTS. Of the 3 billion DNA base pairs that make up a human’s genetic code, I have a single mutation, passed down from my mother, that can elongate the rhythm of my heartbeat by a few milliseconds. And a few milliseconds are all it takes to produce a sudden cardiac arrest.
Experts believe that LQTS accounts for many previously unexplainable deaths in babies, children, and adults. Thankfully, genetics can now put a name to those deaths, and a name gives us something to target. In fact, I believe that we can use a combination of prevention and treatment to eradicate LQTS and many other rare genetic diseases like it.
LQTS is called a monogenic disease because it's caused by a single gene mutation. Some of the other thousands of rare monogenic diseases include cystic fibrosis, Tay-Sachs, and hemophilia. In the case of hemophilia, patients cannot form blood clots, and they can die when a simple cut or bruise turns into uncontrolled bleeding. There are treatments for hemophilia, but they are expensive. What's more, no treatment cures the underlying and hereditary disease, leaving future generations vulnerable.
Rare genetic diseases are inherited, pre-existing conditions, and they aren’t so rare when we add them all up. Any given disease may have just a few thousand patients, but with thousands of rare genetic diseases, there are as many as 30 million people in the US—roughly one out of 10 people—adversely affected by this form of bad luck in a genetic lottery.
I know about many of these diseases because my career has been focused on building biotechnology companies. My job includes raising money from investors and forming collaborations with pharmaceutical companies to accelerate drug development, including for rare diseases. Through a twist of fate, I am now one of those rare disease patients.
As a patient, my greatest emotional challenge has been worrying about passing a deadly gene to unborn children. LQTS is autosomal dominant, which means that my child has a 50 percent chance of inheriting LQTS: a coin flip. This is an intense burden to bear, and so I needed to better understand how LQTS would affect the future health of my family.
When my wife and I began our family planning in the wake of my diagnosis, we discovered that in vitro fertilization could allow us to remove LQTS from our family tree. IVF is typically associated with fertility problems. Instead, for us, if we created embryos using IVF, doctors could employ a technique called preimplantation genetic diagnosis to examine embryos and determine whether they test positive for LQTS. IVF is certainly less enjoyable (and more expensive) than the old-fashioned way to pregnancy, but it may be a small price to pay to prevent a sudden cardiac death. In this case, prevention is a cure.
A treat-and-prevent approach is the only way to truly wipe out any disease, as exemplified by vaccines. Vaccines cost-effectively immunize us against some of the worst killers in history. They have saved 122 million children in the last 10 years alone, according to the Gates Foundation. Similarly, combining IVF with genetic testing could save countless lives by limiting the dissemination of deadly mutations that lurk in our gene pools.
Disease prevention makes medical sense, and it makes economic sense. Just as a $50 vaccine can prevent hospitalizations and tragic social costs, a comprehensive IVF process for around $25,000 could save more than $300,000 in annual drug expenditures alone. In my case, drug costs are minimal, but I have a surgically implanted defibrillator that will need to be replaced every six to 10 years for the rest of my life. Ultimately, by using tools that already exist, we can reduce the suffering of millions, and save hundreds of billions of dollars for the healthcare system.
How Crispr Could Snip Away Some of Humanity’s Worst Diseases
The House Health Plan Makes Your Genes a Preexisting Condition
Why I Won’t Get the Genetic Test for Breast Cancer
And yet, my doctors have seemed reticent to discuss IVF head-on. They have been bashful about the idea of removing this disease from my lineage. Similarly, patient advocacy groups spend little time on alternative family planning, and I have never heard biotech leaders suggest that we can cure rare diseases through prevention. But we can, and we must be able to speak clearly about the best ways to prevent disease if we are serious about eliminating it.
The tepid enthusiasm for using preimplantation genetic diagnosis to screen for genetic diseases is unfortunate, and it is time for a more open dialogue. From a bioethical perspective, screening for morbid monogenic diseases is widely accepted, and easily differentiated from screening for non-disease-related traits. Further, selecting away deadly genes would be more palatable than high-risk, emerging alternatives such as gene-editing embryos.
Imagine a world where a family finds a genetic disease in its blood, and then two things happen: First, the individuals who suffer from the disease begin taking the available treatments. Second, anyone who is a carrier for the gene can select away the disease-causing gene. Current patients are managed, while unborn children are spared future suffering. Family-by-family, life-by-life, a terrible disease is defeated.
My wife and I haven’t yet decided how we want to start our family, but we have seen that the medical system’s views on disease prevention are lagging its ability to prevent. I will love my children no matter the diseases they carry, but I am beginning to think that they shouldn’t have to carry mine.
Lee Cooper (@leecoo4) works in corporate development for a private biotech company in Cambridge, Massachusetts. He has previously published articles on healthcare markets and innovation. WIRED Opinion publishes pieces written by outside contributors and represents a wide range of viewpoints. Read more opinions here.
As you walk through a city, intersections regularly interrupt your path. While you wait to cross the street, trucks and cars are waiting too, putt-putting gases and particles into the atmosphere. There might be an industrial plant nearby that’s not up to code, plumes fanning out with the breeze. Or a city-spanning highway exuding an invisible, dastardly fog. You breathe it all in.
Until recently, pollution scientists had to use models to figure out where those emissions originated and localized—which neighborhood, which block—using weather models and guesses about hotspots. Sensors that agencies like the EPA use to measure pollution are expensive, so a city might have just a couple of sampling spots.
But now, scientists in the Bay Area have worked out two completely different approaches to monitoring pollution at high resolution. One group installed new, cheaper sensors on top of schools and museums, creating a grid that keeps track of seven different pollutants as they drift around Oakland and San Francisco. The other group, publishing today in Environmental Science & Technology, used Google Street View cars equipped with pollution sensors to make a detailed map of pollution in Oakland, block by block. Now both groups are scaling up and getting ready to go global.
Aclima, a San Francisco-based company, got its start making sensors for the insides of buildings. But after a pilot study of air quality in Colorado using Street View cars, they set to driving around California—Los Angeles, Oakland, and up and down the Central Valley. While those cars photographed the peeling paint of West Oakland homes, they also took in air through tubing installed into the back passenger-side window. Once a second, the system would sample the air, looking for carbon and nitrogen gases.
“Mobile monitoring is nothing new, but by putting the sensor on a vehicle that's driving every day with a professional staff of drivers, it makes it far, far easier than it's ever been,” says Joshua Apte, an environmental scientist at UT Austin who analyzed the data from the Street View cars in collaboration with Google, Aclima, and the Environmental Defense Fund.
To get an interactive map of pollution, the Street View cars passed through each road of East, West and Downtown Oakland tens of times. In fact, they went way overboard. “Our study design for this one year was, to put it informally, to drive a crazy, stupid amount,” Apte says. By driving more than they thought they needed to, the scientists could figure out how many days would be enough to get accurate measures. (It turns out measuring a certain street 20 times throughout a year is enough.)
All this crazy, stupid driving paid off in another way. The maps were so detailed that they displayed little hot spots of high pollution, specific intersections with five to eight times more pollution than their surrounding neighborhoods. “When we see a hotspot, we actually can investigate why, because we've got an incredible camera on the roof of the car,” Apte says. For an environmental scientist, having all this data is like being a kid in a candy store.
While Google’s cars drove down Grand Ave over and over, another bevy of sensors has been watching over an even wider array of gases and particles from the roofs of Bay Area schools, museums, and hospitals. Aclima can get an incredibly detailed view of every street they drive, but they're only measuring three types of pollutants—and they only have a sense of the median amount throughout the year. The rooftop sensors are sampling seven pollutants in the same network of locations around the clock, so while they might not catch a hotspot at an intersection, they can reveal how the pollution changes throughout the day and as the seasons change.
Called BEACO2N (BErkeley Atmospheric CO2 Observation Network), these sensors are the baby of Ron Cohen, an atmospheric chemist at UC Berkeley. His project started out as a way to measure whether California’s strict regulations around emissions—meant to help combat climate change—were paying off. But it soon morphed into a public health endeavor, too.
After measuring pollution in Oakland for the last five years, Cohen and his lab recently expanded into San Francisco, scrambling onto the roofs of the Exploratorium and other institutions to install the briefcase-sized sensors. They plan to add sensors to two cities north of Oakland in June. All told, they’ll have 60 sensors, which cost about $6,000 each, including the labor to assemble them. That’s a fraction of the cost of regulatory sensors—more or less the kind that Aclima used in the Street View cars.
Using low-cost equipment has always been part of Cohen’s plan. Each sensor on its own paints a slightly messy picture of pollution levels—cheaper sensors mean way less precision than the units used by regulatory groups. But with a network of cheap sensors, Cohen can triangulate pollution sources and compare measures from neighboring sensors to get a much more detailed picture. It's still not as high resolution as the Aclima measurements, but using the measures of the seven different pollutants, Cohen thinks he can tell how much comes from trucks, and how much is from other sources in these neighborhoods.
Google Street View Cars Now Sniff Pollution Instead of Wi-Fi
Dire Glimpses of What Pollution Is Doing in Bangladesh
How the EPA Puts a Price Tag on Pollution
Using cheaper sensors is also on Aclima's agenda. Since a Street View car only catches a given location every once in a while, having good measurements seemed like it might be necessary to get a good picture of pollution. But Aclima threw in lower-cost packages of sensors alongside their fancier equipment to test whether the cheaper sensors are precise enough for the driving approach. Their lower-cost package of sensors—100 to 1,000 times less expensive than the regulatory-grade equipment—seem to work almost as well, and could soon be installed in other kinds of vehicles, like public transit and taxis. “We can now let them out into the wild to walk on their own, if you will, and really start to scale,” says Melissa Lunden, Aclima’s chief scientist.
The scientists involved in both projects hope they’ll eventually use a whole ecosystem of pollution sensors—high-end, low-cost, and mobile sensors—to create the best picture of pollution possible. “I think that one of the things that our field is going to be working on in the next few years is learning how to integrate these different types of data,” says Apte.
These multiple maps of Oakland’s pollution are great for scientific models of how particles and gases move through a city, but both groups are hoping they’ll ultimately be tools for regulators, city planners, and health care providers to fight climate change and improve health. Cohen is working with public health scientists to start following people with asthma, while expanding BEACO2N to see how attacks correlate with daily pollution. Aclima is partnering with epidemiologists to learn how emissions levels relate to health near pollution hotspots.
The question isn’t whether or not air pollution is bad for health. “There's quite a strong consensus that air pollution exposures are quite bad for you,” says Darby Jack, an environmental health scientist at Columbia’s Mailman School of Public Health. Jack is conducting his own pollution and health monitoring project in New York City on the backs of cyclists. Bike commuters wear biometric shirts and pollution sensors; using the movement of their chests, the researchers can measure how often and how deeply the cyclists breathe while they measure how polluted the air is.
The real question is exactly how much pollution is too much. If you pass through a dirty intersection once a day, is that enough to harm your health? Or do you need to live there? Past epidemiology studies have relied mostly on pollution models rather than real maps—they just can't provide that kind of information. “That’s a pretty crude way of measuring exposure,” Jack says.
Just having a detailed map of the pollution levels will make a huge difference in the kinds of questions scientists can ask, but there’s still a lot of work to do. “We've shown that the tools work,"Cohen says. "Now the challenge for all people trying these different sensing strategies is to go past the tool and actually use it to learn something we couldn't before."
In some parts of the world, where pollution is likely even worse, there isn’t any data at all—not even the crudest of measurements. Cohen, for one, has started to hear interest from Asia in creating networks like BEACO2N. He predicts that there will be an explosion of sensor networks, whether on the backs of moving vehicles or affixed to buildings, in the next few years. The Street View team agrees. “We did this in Oakland," says Apte. "But we saw Oakland as a laboratory for the world."
Not long after the Big Bang, all went dark. The hydrogen gas that pervaded the early universe would have snuffed out the light of the universe’s first stars and galaxies. For hundreds of millions of years, even a galaxy’s worth of stars—or unthinkably bright beacons such as those created by supermassive black holes—would have been rendered all but invisible.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Eventually this fog burned off as high-energy ultraviolet light broke the atoms apart in a process called reionization. But the questions of exactly how this happened—which celestial objects powered the process and how many of them were needed—have consumed astronomers for decades.
Now, in a series of studies, researchers have looked further into the early universe than ever before. They’ve used galaxies and dark matter as a giant cosmic lens to see some of the earliest galaxies known, illuminating how these galaxies could have dissipated the cosmic fog. In addition, an international team of astronomers has found dozens of supermassive black holes—each with the mass of millions of suns—lighting up the early universe. Another team has found evidence that supermassive black holes existed hundreds of millions of years before anyone thought possible. The new discoveries should make clear just how much black holes contributed to the reionization of the universe, even as they’ve opened up questions as to how such supermassive black holes were able to form so early in the universe’s history.
In the first years after the Big Bang, the universe was too hot to allow atoms to form. Protons and electrons flew about, scattering any light. Then after about 380,000 years, these protons and electrons cooled enough to form hydrogen atoms, which coalesced into stars and galaxies over the next few hundreds of millions of years.
Starlight from these galaxies would have been bright and energetic, with lots of it falling in the ultraviolet part of the spectrum. As this light flew out into the universe, it ran into more hydrogen gas. These photons of light would break apart the hydrogen gas, contributing to reionization, but as they did so, the gas snuffed out the light.
To find these stars, astronomers have to look for the non-ultraviolet part of their light and extrapolate from there. But this non-ultraviolet light is relatively dim and hard to see without help.
A team led by Rachael Livermore, an astrophysicist at the University of Texas at Austin, found just the help needed in the form of a giant cosmic lens. These so-called gravitational lenses form when a galaxy cluster, filled with massive dark matter, bends space-time to focus and magnify any object on the other side of it. Livermore used this technique with images from the Hubble Space Telescope to spot extremely faint galaxies from as far back as 600 million years after the Big Bang—right in the thick of reionization.
In a recent paper that appeared in The Astrophysical Journal, Livermore and colleagues also calculated that if you add galaxies like these to the previously known galaxies, then stars should be able to generate enough intense ultraviolet light to reionize the universe.
Yet there’s a catch. Astronomers doing this work have to estimate how much of a star’s ultraviolet light escaped its home galaxy (which is full of light-blocking hydrogen gas) to go out into the wider universe and contribute to reionization writ large. That estimate—called the escape fraction—creates a huge uncertainty that Livermore is quick to acknowledge.
In addition, not everyone believes Livermore’s results. Rychard Bouwens, an astrophysicist at Leiden University in the Netherlands, argues in a paper submitted to The Astrophysical Journal that Livermore didn’t properly subtract the light from the galaxy clusters that make up the gravitational lens. As a result, he said, the distant galaxies aren’t as faint as Livermore and colleagues claim, and astronomers have not found enough galaxies to conclude that stars ionized the universe.
Inside the Hunt for the Source of a Mysterious Cosmic Burst
The Man Who’s Trying to Kill Dark Matter
After Two Black Holes Collide, a Puzzling Flash
If stars couldn’t get the job done, perhaps supermassive black holes could. Beastly in size, up to a billion times the mass of the sun, supermassive black holes devour matter. They tug it toward them and heat it up, a process that emits lots of light and creates luminous objects that we call quasars. Because quasars emit way more ionizing radiation than stars do, they could in theory reionize the universe.
The trick is finding enough quasars to do it. In a paper posted to the scientific preprint site arxiv.org last month, astronomers working with the Subaru Telescope announced the discovery of 33 quasars that are about a 10th as bright as ones identified before. With such faint quasars, the astronomers should be able to calculate just how much ultraviolet light these supermassive black holes emit, said Michael Strauss, an astrophysicist at Princeton University and a member of the team. The researchers haven’t done the analysis yet, but they expect to publish the results in the coming months.
The Subaru Telescope (center) on the summit of Mauna Kea in Hawaii.
The oldest of these quasars dates back to around a billion years after the Big Bang, which seems about how long it would take ordinary black holes to devour enough matter to bulk up to supermassive status.
This is why another recent discovery is so puzzling. A team of researchers led by Richard Ellis, an astronomer at the European Southern Observatory, was observing a bright, star-forming galaxy seen as it was just 600 million years after the Big Bang. The galaxy’s spectrum—a catalog of light by wavelength—appeared to contain a signature of ionized nitrogen. It’s hard to ionize ordinary hydrogen, and even harder to ionize nitrogen. It requires more higher-energy ultraviolet light than stars emit. So another strong source of ionizing radiation, possibly a supermassive black hole, had to exist at this time, Ellis said.
One supermassive black hole at the center of an early star-forming galaxy might be an outlier. It doesn’t mean there were enough of them around to reionize the universe. So Ellis has started to look at other early galaxies. His team now has tentative evidence that supermassive black holes sat at the centers of other massive, star-forming galaxies in the early universe. Studying these objects could help clarify what reionized the universe and illuminate how supermassive black holes formed at all. “That is a very exciting possibility,” Ellis said.
The James Webb Space Telescope, seen here inside a clean room at NASA’s Goddard Space Flight Center, has been designed to capture light from the first galaxies that formed in the early universe.
All this work is beginning to converge on a relatively straightforward explanation for what reionized the universe. The first population of young, hot stars probably started the process, then drove it forward for hundreds of millions of years. Over time, these stars died; the stars that replaced them weren’t quite so bright and hot. But by this point in cosmic history, supermassive black holes had enough time to grow and could start to take over. Researchers such as Steve Finkelstein, an astrophysicist at the University of Texas at Austin, are using the latest observational data and simulations of early galactic activity to test out the details of this scenario, such as how much stars and black holes contribute to the process at different times.
His work—and all work involving the universe’s first billion years—will get a boost in the coming years after the 2018 launch of the James Webb Space Telescope, Hubble’s successor, which has been explicitly designed to find the first objects in the universe. Its findings will probably provoke many more questions, too.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Update: On June 1, President Donald Trump announced he would pull out of the 2015 Paris climate agreement.
After months of delays, many expect President Donald Trump to follow through with his campaign promise to pull the US out of the 2015 Paris climate agreement (the imminent announcement is practically a done deal, according to reported leaks). While many establishment Republicans have pushed for the ... let's say, Parexit, a surprising number of US businesses opposed the decision, on grounds that it will weaken the US's global competitiveness.
Among the most surprising combatants are oil giants like Exxon and Shell. Exxon's new CEO, Darren Woods, even wrote Trump a personal letter urging him to stay in the agreement. It's not that Woods and the other big oil magnates suddenly saw some polar bears and had their hearts grow three sizes; they are worried about a change in the global economic climate. See, most of the world has promised to transition to low carbon, and eventually carbon-free, energy sources. Many economists and industry insiders say the US will get left behind as the rest of the world transitions to a clean energy economy.
Since last April, almost every country on the planet has signed the Paris climate agreement, a collective effort to limit global warming to 2˚C and try to keep it as close to 1.5˚C as possible. Exceptions include Syria (mired in a civil war) and Nicaragua (didn't think the Paris agreement was strong enough!). Each country submitted a commitment to reduce its overall greenhouse gas emissions, forging a sort of collective promise to share the economic burden of transitioning away from fossil fuels. The agreement is globally appealing because no single country wants to sacrifice prosperity or see its rivals gain an unfair advantage. "In order to reduce greenhouse gas emissions, you have to restrict certain types of economic activities," says Rod Godby, the director of the University of Wyoming's Center for Energy Economics & Public Policy.
Take the US. America promised a 26 to 28 percent reduction of 2005 levels by 2025. Part of getting to those numbers was enacting the Clean Power Plan. It puts heavy restrictions on emissions from coal power plants in an attempt to make coal-generated electricity more expensive, and therefore incentivize power companies to invest in cleaner energy sources. Obama's Climate Action Plan detailed numerous other efforts—better fuel standards, home insulation requirements, forest protections—to tip the scales in green energy's favor.
Los Angeles Says It’ll Stay In the Paris Climate Agreement It Isn’t In
President of Gabon: The Paris Agreement Is Just the Start of Africa’s Climate Quest
Nations Be Damned, the World’s Cities Can Take a Big Bite Out of Emissions
Trump has promised to kill the Clean Power Plan, and he signed an executive order that rolls back much of the Climate Action Plan. He seems to want to get rid of all the regulations, regardless of how he swings on the Paris agreement.
But all this regulatory reversalism might be for naught. Photovoltaic and wind energy are now both cheaper than coal, so long as the sun is up and the wind is blowing (and new large-scale battery technology could make erase these intermittency issues). Given enough time, renewables could outscale fossil fuels.
And the real change agent has been natural gas. Fracking has made gas so cheap that both coal and oil have lost market share to it. "Wholesale changes in economy happen over time naturally; look at historic evolution of different fuel sources," says Godby. "But in the past, these changes have been market driven, and this agreement seeks to drive and accelerate them by social choice."
Natural gas is a fossil fuel, yes, but it emits way less carbon dioxide than coal or oil—though it emits a lot of methane, another potent greenhouse gas. Still, natural gas is an important bridge for countries transitioning fully to renewables. Obama's regulations—and the Paris agreement—were all about giving these maybe-already-happening market changes a little extra oomph.
Trump is not about all that. His "America First Energy Plan" has few details but a very clear message: Save coal, save oil, fossil fuels rule! But there is a paradox at play here. Trump's fossil fueled deregulatory agenda will actually crimp oil and coal domestically, and ditching the Paris agreement could damage US natural gas producers' ability to sell their volatiles abroad.
Oil has been on the mend lately—back to nearly $50 a barrel—but is still recovering from a massive slump. Again, blame natural gas, not regulations. "The collapse of the oil price which occurred early last year was really a function of market forces and technological innovation in the fracking sector," says Tom Sanzillo, the director of finance for the Institute for Energy Economics and Financial Analysis. "The problem isn't regulations; it's that the market is oversupplied," says Sanzillo.
Not to say that the Paris agreement isn't putting a hurt on oil. "If you are a Texas oil company with lots of energy exports, you have to consider what will happen in the rest of the world," says Godby. Two weeks ago, shareholders for Occidental Petroleum made history by voting that the company research and report how global climate regulations would hurt its business. Today, while everyone was waiting to see if Trump would pull the trigger on Paris, shareholders at Exxon did the same thing. Exxon!
Natural gas and fracking—not regulations—also usurped King Coal. Coal was never a really big mover on the international stage (except during China's boom years), and US production peaked back in 2007. In February, this trend—along with both China and India's major commitments to renewables—prompted Goldman Sachs to publish a report on coal's irreversible decline.
As for renewables, pulling out of Paris could cut the US out of an explosively growing market. China, India, and other growing economies have pledged billions towards renewables. The competition may have already begun. In April, Atlanta-based solar panel company Suniva filed for bankruptcy, citing an unfair advantage by Asian competitors. Adding to the drama, a Chinese wind company recently offered to teach US coal workers how to be turbine technicians. That's the kind of thing that'll make you want to yell out Covfefe!
US Secretary of State John Kerry discusses the ways that recently signed UN climate agreement will spur innovations in renewable energy across the globe, including terror hotspots.
This story originally appeared on Grist and is part of the Climate Desk collaboration.
Just a few decades ago, California’s Inland Empire billed itself as “the Orange Empire” for the citrus orchards that fueled its primary industry. Today, many of those groves are gone, and so is the nickname. The landlocked region of 4 million people an hour east of Los Angeles now sprouts more enormous warehouses (a billion square feet of them) than fruit trees.
Forty percent of the nation’s consumer goods—iPhones, sneakers, and everything available from Amazon—spend time sitting on those warehouse shelves after coming off ships at nearby ports, awaiting delivery to stores and homes. What was once a mostly rural region finds itself struggling with a high poverty rate and growing population. Residents are plagued by tremendous traffic and air pollution, which recently earned the region an “F” from the American Lung Association.
The Arctic Doomsday Seed Vault Flooded. Thanks, Global Warming
Veggies Grown With Toilet Water Could Be Headed to Your Table
Silicon Valley’s Mission to Save California Ag From Dying of Thirst
Those environmental and health concerns will get much worse, advocates say, if the city of Moreno Valley—a town of 200,000 located in the heart of the Inland Empire—builds the largest warehouse project anywhere in the country.
Tom Thornsley is a 60-year-old urban planner who moved to Moreno Valley in 1998, just as the rural-to-warehouse transformation was beginning. He thought he had chosen wisely, settling in a gray, ranch-style home that sat near a wide-open space zoned for more homes, not warehouses. “I know better than to look at dirt and not check what it would be,” he says.
But after a developer proposed a project in 2012, city officials rezoned that dirt patch next to Thornsley’s house to make it home to one of the world’s largest warehouse complexes.
The World Logistics Center, planned by a company called Highland Fairview, would be the largest such facility in the country, covering 2,610 acres—the size of 700 football fields. It would be more than 25 times bigger than the largest warehouse in the United States, a 98-acre hangar operated in Washington by the airplane manufacturer Boeing.
As a planner, Thornsley doesn’t have a problem with industrial development. He’s worked on commercial buildings since 1989. But the environmental costs of the World Logistics Center are too much for his community, he says, so he’s become a leader in the effort to stop it—an effort that might hinge on next month’s special city council election.
The World Logistics Center, which is now known locally by the acronym “WLC,” has turned Moreno Valley politics into a bloodsport. Community organizers and environmental groups have fought—in both city hall and the courtroom—to protect residents from the pollution it would cause and save protected species like peregrine falcons and California golden eagles that live in the nearby San Jacinto Wildlife Area.
Once built, warehouses don’t pollute the way that factories and power plants do. But a project the size of the WLC would be a magnet for truck traffic, spewing exhaust on 69,000 estimated daily trips in and out of the complex. In a struggling region, though, the lure of jobs has proven difficult to overcome, despite the public health and quality of life concerns.
“That’s why people are pressing so hard now,” Thornsley says, “to get somebody elected who’s not going to be, in essence, another developer’s puppet.”
Southern California’s two ports are among the deepest on the West Coast, allowing massive ships to dock at Los Angeles and Long Beach. More than $360 billion worth of goods from production centers in the Asian Pacific were offloaded there in 2014. Warehouses originally crowded around the ports, until Los Angeles could no longer contain the growth.
Demand for more space at cheaper rates pushed development farther east, and the Inland Empire became the hidden purgatory between production and consumption. Only the Philadelphia area currently has more warehouse space, but projects like the WLC would leave that East Coast hub in the dust. Over the past five years, the logistics industry has delivered a quarter of the new jobs in the region.
But the economic boom carries a heavy environmental toll: Diesel trucks zip along the Inland Empire’s roads, carrying cargo to customers and piping particulates into the air. Winds rushing in from the ocean blow added pollution from LA and Orange County, which accumulates in the basin bounded to the north and east by mountains.
That makes the Inland Empire one of the unhealthiest places to live in the country. Air pollution leads to higher risk of heart disease, asthma, bronchitis, cancer, and more. The South Coast Air Basin—which encompasses parts of Orange, Riverside, San Bernadino, and Los Angeles counties—exceeds federal and state requirements for lead and small particulate matter, which can lodge in the lungs. San Bernardino and Riverside counties, which make up the Inland Empire, ranked first and second, respectively, among the top 25 most ozone-polluted counties in the American Lung Association’s 2016 air quality report.
Low-income neighborhoods and communities of color bear the brunt of this pollution, because they’re often situated near freeways or become sites for warehouses. Moreno Valley’s population is 18 percent African-American and about 54 percent Latino.
In a community where nearly 20 percent of people live in poverty, it’s easy for a big developer to gain support for a project like the World Logistics Center—especially with the promise of 20,000 permanent jobs and $2.5 billion a year added to the local economy. But the downside includes 14,000 added diesel truck trips per day and a 44 percent increase in the city’s yearly greenhouse gas emissions.
Many warehouse jobs are also low wage, temporary, and unsafe. The facilities rack up a plethora of safety violations, according to California health and safety inspectors, and workers report high levels of injury and illness.
Many residents believe “this is the best I can get,” explains Sheheryar Kaaosji, coexecutive director of the Warehouse Workers Resource Center, which advocates for employee rights. “That’s what it comes down to.”
Tom Thornsley, who has decades of experience in urban planning, says development decisions are all about weighing the pros and cons. When a project is proposed, environmental risk assessments let you know how harmful the development could be, which tells you whether it’s worth it and if you can do anything to mitigate the harm.
Ultimately, he says, if a project’s benefits outweigh its problems, “You just decide you can live with it.” In the case of the World Logistics Center, he can’t. An analysis of health impacts prepared for the developer acknowledges the potential for increased incidences of asthma, heart disease, and premature deaths.
Thornsley’s dismay over another warehouse project—for the shoe brand Skechers, which opened in 2011 and brought a net job loss—pushed him to join up with Residents for a Livable Moreno Valley, a community group created to combat rampant warehouse development.
The group has found itself battling a formidable nemesis in the person of Highland Fairview’s charismatic CEO, Iddo Benzeevi. During the protracted WLC fight, Benzeevi has become a folk hero to some Moreno Valley residents. When the city council narrowly approved the project in August 2015, residents swayed by the promise of jobs chanted “Iddo, Iddo, Iddo.”
“They think he walks on water,” Thornsley says. “I honestly couldn’t tell you why they find him to be so adorable.”
In public council meetings ahead of the vote, many residents lauded the project as a game changer for the city. “These are not jobs that are just back and hands, they’re technical jobs, they’re jobs that are going to be able to support a family,” said one woman who has lived in Moreno Valley for more than 30 years.
“Most people recognize Moreno Valley as one of three things: crime, corruption, or unemployment,” says resident Leo Gonzalez in a series of 2015 YouTube videos supporting the WLC. “I believe we have the ability to change all that.”
In addition to currying favor with residents, Benzeevi skillfully navigated California’s arcane political and environmental laws. Thanks to a 2014 California Supreme Court decision involving Walmart, ballot initiatives that garner enough residents’ signatures can escape the state’s Environmental Quality Act review process. Benzeevi took full advantage of that for the WLC.
Meanwhile, Thornsley’s group pushed back. Residents for a Livable Moreno Valley filed lawsuits against the project, which has also faced separate legal challenges from the regional South Coast Air Quality Management District, Riverside County and its transportation commission, and a roster of environmental groups including EarthJustice, the Center for Biological Diversity, and the Coalition for Clean Air.
“We already have among the dirtiest air in the nation,” says George Hague, a volunteer for the local Sierra Club chapter. “We keep trying to clean it up, but projects like this put us back.”
Benzeevi disputes this, arguing that the benefits of job creation would outweigh the environmental impacts. “There is no better solution to reducing impacts to the environment than creating jobs where people live while utilizing the best available and viable clean technologies,” Benzeevi told Grist via an email from his public relations department. “This is precisely what the WLC does.”
Benzeevi notes that Moreno Valley is requiring Highland Fairview to ensure that diesel trucks servicing the WLC meet a 2010 federal government standard for clean diesel. To which Hague counters: “They’re cleaner, but they’re not clean.”
With options running out for WLC opponents, the special June 6 city council election to replace a single vacated seat represents a longshot chance to block the megawarehouse project, by putting an opponent on the board who can help overturn previous approvals. The council is now deadlocked 2-2.
The open seat was previously held by Moreno Valley’s new mayor, Yxstian Gutierrez. A supporter of the World Logistics Center, Gutierrez won just under a third of the mayoral vote, but that was enough in a multi-candidate race. Benzeevi spent more than $160,000 to help his campaign.
Highland Fairview is now backing a preferred candidate for Gutierrez’s open council seat, pouring tens of thousands into canvassing for his campaign through the Benzeevi-sponsored Committee for Ethics and Accountability in Government. WLC opponents are pinning their hopes on the three remaining candidates—one in particular has been an outspoken WLC opponent—who face long odds of winning.
Whatever the election outcome, development in the Inland Empire shows no signs of slowing in the short term. But the environmental and health impacts of the logistics industry are starting to spur more awareness of the costs of warehouse growth.
The California Air Resources Board voted in March to relocate its research facility to the Inland Empire’s Riverside County, which could potentially offer a more comprehensive understanding of the region’s poor environmental quality. And a bill in the state assembly is challenging the lack of environmental review in the initiative process—the very rule that allowed Benzeevi to push his megawarehouse project through local government.
“We have such a disengaged citizenry—most of the people don’t understand the connection of the trucks to city hall,” says Kathleen Dale, a retired urban planner with the Residents for a Livable Moreno Valley group. “When you talk to people, this is a concern of theirs, but they’re not expressing it by going to the polls.”

Astronomer Meredith Rawls was in an astronomy master's program at San Diego State University in 2008 when her professor threw a curveball. “We’re going to need to do some coding," he said to her class. "Do you know how to do that?”
Not really, the students said.
And so he taught them—at lunch, working around their regular class schedule. But what he meant by “coding” was Fortran, a language IBM developed in the 1950s. Later, working on her PhD at New Mexico State, Rawls decided her official training wasn’t going to cut it. She set out to learn a more modern language called Python, which she saw other astronomers switching to. “It's going to suck,” she remembers telling herself, “but I'm just going to do it.”
And so she started teaching herself, and signed up for a workshop called SciCoder. “I basically lost the better part of a year of standard research productivity time largely due to that choice, to switch my tools,” she says, “but I don't think I could have succeeded without that, either.”
That’s probably true. Rawls’s educational experience is still typical: Fledgling astronomers take maybe one course in coding and then informally learn whatever language their leaders happen to use, because those are the ones the leaders know how to teach. They usually don't take meaningful courses in modern coding, data science, or their best practices.
But today’s astronomers don't just need to know how stars form and black holes burst. They also need knowledge of how to pry that information from the many terabytes of data that will stream from next-generation telescopes like the Large Synoptic Survey Telescope and the Square Kilometer Array. So they're largely teaching themselves—using a suite of open-source training tools, focused workshops, and fellowship programs aims to help and actually prepare astronomers for the universe they’re entering.
Back when telescopes produced less data, astronomers could get by on teaching themselves. “The old model was you go to your telescope—or you log in remotely because you're fancy—you get your data, you download it on your computer, you make a plot, you write a paper, and you’re a scientist,” says Rawls, who is now a postdoc at the University of Washington. “Now, it's not practical to download all the data.” And “a plot” is laughable. You just try using graph paper to nail down the correlation function that shows the distribution of millions of galaxies (go ahead; I'll wait).
There are social costs to that inadequate education. First, it gives a booster to people who knew, early, both that they wanted to be astronomers and that astronomy meant typing into your computer all day. You know, the kinds of kids who sat in Algebra I “hacking” their TI-83s—ones with access to autodidactic materials and the free time to do that didacting. That kind of favoring is a good way to, on average, keep astronomy’s usual suspects—white guys!—on top.
Want to Make It as a Biologist? Better Learn to Code
Science Has Its Problems, But the Web Could Be the Fix
You Should Be Coding in Your Physics Course
Beyond the social costs, though, lie scientific ones. Let’s say a scientist writes a program that analyzes quakes inside the sun (that happens!). But there’s no documentation on how the program works, and its kludgy, coagulated subroutines are opaque. No second scientist, then, can run that code see if they get the same result, or if the program actually does what Scientist 1 claims. “Reproducibility is held up as the gold standard for what is real or not,” says Lucianne Walkowicz, an astrophysicist at the Adler Planetarium. “You need the materials upon which the experiment was performed, and you need the tools. Code is the equivalent of our beakers and Bunsen burners.”
Plus, the way astrophysics programming has historically worked is inefficient. Out on overheating desktops across Earth’s universities are dozens of programs that do the same thing—catch those quakes, comb for exoplanets—different research groups having made their own. Instead of applying increasingly refined algorithms to their research problems, ill-trained astronomer-coders sometimes spend their time reinventing the wheel.
Walkowicz wants to help fix these problems before they get worse—which they’re about to. She is the science collaboration coordinator for the Large Synoptic Survey Telescope, which will essentially make a 10-year-long HD movie of the sky, so astronomers can see—and, ideally, understand—what changes from diurn to diurn. “Part of the reason we could all get by on being self-taught is that datasets, even when they're on the fairly big side, are pretty small,” says Walkowicz. “They're not as large and complex as the data from LSST will be. Problems will be amplified.”
Knowing this, and knowing that astronomer apprentices are getting essentially the same training astronomers have gotten since always, she and LSST colleagues decided to help prepare those apprentices. The LSST Corporation (LSSTC) Data Science Fellowship program was born, bringing cohorts of students to six weeklong workshops over two years. To select fellows, they use a program called Entrofy, which optimizes diversity among each class.
The idea doesn’t always go over well with professors. “Reactions that I’ve gotten run the gamut from ‘That's a good point, but our students don't have time’ to ‘Stop trying to turn our astronomers into computer scientists,’” says Walkowicz.
Reactions that I’ve gotten run the gamut from ‘That's a good point, but our students don't have time’ to ‘Stop trying to turn our astronomers into computer scientists.'
Lucianne Walkowicz
But for their part, the students—perhaps more aware of the future of their field than the more senior researchers—feel more like astronomers. “Before being in this program, I already knew my thesis and my thesis hasn't changed,” says Charee Peters, a grad student at the University of Wisconsin, “but I feel more comfortable now being able to approach it. I feel more like a scientist.”
Grad student Bela Abolfathi of UC Irvine has similar feelings, and thinks it makes sense that education be driven by data. “I had been trying to learn a lot of these techniques on my own, and my progress was glacial,” she says. “It really helps to learn these skills in a formal way, where you can ask questions from experts in the field, just as you would any other subject.”
You can often spot a formally untrained astronomer’s code a light-year away—with its lack of documentation, its serpentine subroutines. But you can also spot a computer scientist’s astronomy code. It’s high and tight, but it doesn’t display the same depth of knowledge about what the program is doing, and what those actions mean for, say, supernovae. “The key thing is combining the two approaches,” says Joachim Moeyens, an LSSTC data fellow from the University of Washington. “My goal is to keep everyone guessing about whether I'm an astronomer or a software engineer.” (My guess: a new kind of hybrid.)
The LSSTC’s fellowship only admits 15 students at a time—hardly the whole field. But the curriculum is online, and it has company. The Banneker & Aztlán Institute preps undergrads from all over in Unix, Python, computational astronomy, and data visualization. There are general boot camps, astro-specific modules, and continent-centric workshops. NASA and the SETI Institute recently teamed up to start the Frontier Development Lab, which brings planetary researchers and data scientists into contact with the private sector. And the University of Washington has a whole organization—the E-Science Institute—dedicated to the cause.
Astronomers have also given each other actual tools. The open-source AstroPy is “a community effort to develop a single core package for Astronomy in Python and foster interoperability between Python astronomy packages.” AstroML has a similar goal for the machine learning and data mining side. Scientists, here, can use the same code to do the same things on different data, fixing both that whole redundant wheel thing and the reproducibility problem.
Still, there’s some resistance in The Academy, reluctance to integrate all of this into curricula instead of requiring students to (or just tolerating students who) boot themselves off to camp. Alexandra Amon, an LSSTC Data Science fellow and a grad student at the University of Edinburgh, feels this acutely, in thinking about how, in the view of some, her hours spent learning to deal with data detract from her science—essentially the same sentiment Rawls expressed, despite the difference in their years. “Traditionally, from a job application point of view, time spent doing data analysis is detracting from delivering science results and paper-producing,” Almon says, “and therefore a hindrance.”
But “doing science” means—and has meant, for a while now—doing the kind of analysis that demands data and computer science expertise. Without that, the gap between knowledge and scientists' ability to get that knowledge will only grow, like, you know, the universe itself.
Tiny Bop makes beautiful apps like Everything Machine which uses a smartphone's camera, gyroscope, light, speakers and microphone to teach coding through games.
Carbon dioxide is one hell of a molecule. Perhaps you only know it as the stuff humans exhale and plants inhale, or the primary culprit for climate change. But CO2 is capable of so much more. For instance, some engineers think it could help make the power industry a little greener.
Now, you're probably thinking this is a twist on carbon capture and storage. Nope. It's about turbine generators—the enormous machines that convert heat into electricity. Most power plants use steam turbines. But turning water into a gas (steam) requires a lot of energy. Carbon dioxide exists as a gas at room temperature, saving you that trouble. Plus, it compresses far more easily, meaning you can cram a lot more of it through a turbine. A paper published in Science says extremely hot and extremely compressed—a state called supercritical—CO2 could generate more power with smaller turbines.
More than two thirds of all the electricity in the US is generated using steam generators operating on what engineers call the Rankine cycle. You start with water, pressurized using a pump. Then apply heat—by burning coal, letting radioactive material decay, or focusing sunlight reflected off thousands of mirrors onto a single point. This boils the water, creating steam. Add more heat. And still more heat. You want that steam as hot as possible before sending it through the turbine: More heat means more energy means more electricity. The turbine blades spin, and the generator attached to them creates electricity. Then steam goes through a condenser, becomes water, and returns to the pump. The cycle begins anew.
The Rankine cycle has worked quite nicely for well over a century. No one had any reason to change things because, until recently, generating electricity was pretty cheap and the consequences of using coal (read: climate change) to do it weren't so readily apparent. But the Rankine cycle is inefficient, mostly because it uses water. "It's an interesting accident of physics, that in order to get anything to change phase, like, from ice to water, or water to steam, you need to add lots of energy," says Avi Shultz, a program manager at the DOE’s SunShot Initiative. In other words, a steam generator going through the Rankine cycle wastes a whole lot of energy boiling water.
This is especially rankling when you remember that the hotter the steam going through a turbine, the more electricity that turbine generates. All that heat energy wasted boiling the water could have been used to generate more power.
A CO2-driven turbine like those described in the Science paper skips the liquid phase entirely with what's called the Brayton cycle. "It uses a gas phase throughout, so you really end up with a better use of energy," says Levi Irwin, a DOE contractor, and author of the paper. Carbon dioxide also compresses more easily than water. That means you can pack more energized (heated) CO2 in a smaller volume. Irwin's paper proposes heating and compressing CO2 until it enters a supercritical state in which it is a bit like a liquid and a bit like a gas. "This lets you push energy through a turbine at 10 times the rate of steam," Irwin says. Vrooooom!
This makes a supercritical CO2 generator 30 percent more efficient at converting energy into electricity, Irwin writes in his paper. Those generators are smaller and simpler—because they deal with only a single phase (gas) and therefore have fewer parts. The only thing that might make them better is if they somehow harvested CO2 from the atmosphere. Instead, they rely upon industrial grade carbon dioxide that remains in a closed system.
So what's the hangup? Well, the intense heat can play hell on turbines. "When you talk about high energy, you have a lot of large temperature gradients that are going to put mechanical stresses on the turbine," says Irwin. That means building CO~2 ~turbines with metals that won't crack, distend, or deform, and making them big enough to take the abuse. Also, there are a few engineering problems to work out. Like, the turbine blades, which need to be designed to work efficiently with the not quite liquid, not quite vapor consistency of supercritical CO2.
The DOE announced in October that it is building a prototype power plant that uses supercritical CO2 turbines. When the $80 million project goes online in about six years, it will generate 10 megawatts of energy—about enough to operate a few thousand homes. That explains why Shultz doesn't expect supercritical CO2 turbines to start replacing traditional steam turbines en masse for at least a decade. And if coal happens to be obsolete by then, no problem. This technology works with any power plant that converts heat into electricity, including solar thermal and nuclear energy. That's one hell of a machine.
NASA's Juno mission has already made mincemeat of precedents and expectations. When it arrived at Jupiter last July after a five year journey, it was further from Earth than any solar-powered craft had ever been, and traveling faster than any other human-crafted object had before. Its flight path skims closer to the storm-torn gas giant than any orbiter preceding it. And it's the first spacecraft to pass over Jupiter's mysterious poles—finding, counter to most assumptions, that they're blue, and lack the planet's characteristic stripes.
Juno isn't done with firsts, or with sending scientists back to their whiteboards. Scientists have been poring over the data Juno collected in its first cloud-grazingly close pass over Jupiter last August, and today published two papers on what they've discovered about Jupiter's auroras, atmosphere, and magnetic and gravity fields. And not only are Jupiter's atmospheric dynamics less Earth-like than scientists thought, they're also far more complex and variable. That means if scientists want to fully understand planets, a single probe might give incomplete, misleading information. Luckily for Jupiter scientists, Juno—with its many, closely spaced orbits designed to map the whole planet—is the right tool for the job.
NASA’s Juno Spacecraft Discovers Curiosities at Jupiter’s Poles
Watch Live as Juno Enters Jupiter’s Orbit
Some Things You Need to Know About NASA’s Juno Mission
Let's start at the top, in the upper atmosphere with Jupiter's auroras. Scientists already knew that Jupiter's auroras make the Northern Lights look like a flicker: They're hundreds of times more energetic, and cover more area than the entire planet Earth. Juno uses several instruments to look at the auroras' energized particles and the physics controlling their dynamics, and if the data from this first close pass is any indication, they'll only continue to diverge from Earth's light shows. "It's very tempting to interpret what you see on another planet based on Earth," says Jack Connerney, an astrophysicist at NASA's Goddard Space Flight Center and author of one of the papers. "Up until last week, our models of Jupiter's auroras had the electrons going in the wrong direction." On Earth, electrons in the planet's magnetic field get excited by solar wind and then funneled toward the poles, where they bang into other atoms and molecules and emit light. On Jupiter, Juno's instruments have found that electrons actually get excited when pulled out of polar regions.
On top of that, it seems like planetary scientists had Jupiter's atmospheric dynamics wrong in general. "Scientists thought the main energy source in the atmosphere would be the sun," says Scott Bolton, Juno's principal investigator and lead author of the other paper. "So they assumed that once we dropped below the sunlight that the particles would be simple and well mixed." That turns out not to be the case: The particles in Jupiter's atmosphere are just as diverse and banded as the planet's famously stripey exterior. Particularly interesting to Juno's team is a massive equatorial band of ammonia that extends hundreds of kilometers down toward the planet's core—as far as Juno's instruments can see. According to even the most up-to-date models of Jupiter's atmosphere, there's no reason it should do that.
Another area showing surprising amounts of activity? The deeps of Jupiter's atmosphere: the magnetic and gravity fields Juno intends to map. "If Jupiter is just a big, rotating gas ball, it should not have any odd harmonics in its gravity field," says Connerney. But Jupiter's gravity isn't uniform, which might suggest deep convection—density differentials deep inside Jupiter might drive gravity fluctuations the same way atmospheric pressure differentials drive weather on Earth. Juno's readings of the planet's magnetic field were also much more geographically variable than scientists expected.
Juno's team is still a long ways from understanding why Jupiter's atmosphere is so all over the place, though Connerney ventures that the fluctuations may all be connected, with the deep convection expressed in the gravity field also driving the uneven magnetic field strength. "In hindsight, it's hard to imagine why would we have ever thought it would be simple and boring," Bolton says.
Understanding Jupiter's atmosphere in more detail may help scientists come to grips with some of Earth's weirder traits. Bolton compares Jupiter's equatorial ammonia to the tropical band around Earth's own equator. "The concept we have on Earth is that that band gets developed because the air has an ocean to bounce off," Bolton says. "Jupiter doesn't, so why would it look the same? We may be learning something fundamental about atmospheres. Maybe our assumptions about Earth are wrong."
The same information transfer could apply to Earth's magnetic field—which is tough to study because it's generated deep under the crust and somewhat occluded by random iron deposits. Jupiter has no crust, and no extra magnets to gunk up sensor data. "This will be our first time looking down on a real functioning magnetic dynamo," Connerney says. "Maybe we should have started with Jupiter."
All of these discoveries are challenging conventional space wisdom—and not just because of their results. Typically, scientists send a probe to a planet first, and follow it up with an orbiter equipped with all the doodads the probe data suggests they'll need. "Our concept of how Jupiter and giant planets work that developed over the last few decades was probably oversimplified," Bolton says. "Maybe we need to stop sticking in a probe and thinking we’re going to accurately sample a whole planet."
Bolton's answer to this problem—develop more Juno-style missions with lots of orbits designed to map a planet in its entirety—befits his role as, well, the leader of a Juno-style mission. But he's probably right. The good news is that Juno has Jupiter covered. If scientists learned this much from the mission's first close brush with the planet, imagine what will come of the next 36.
Find out how NASA’s Juno Mission will help unlock the mysteries of our planet and our solar system.
President Trump’s proposed 2018 budget will never actually determine how the government spends your money: Potus proposes and Congress disposes. But that's no reason for relief. In fact, it makes this document even more of a nightmare. It doesn't direct funding, but it does put the Trump administration's underlying philosophy of governance on display. And it's a harsh one.
The science cuts make this most visible. Now, stipulated, governments don’t have to fund science. Since World War II, though, America has; the rationale comes from a 1945 report written by Vannevar Bush, then-director of the US Office of Scientific Research and Development. In Science: The Endless Frontier, he wrote: "Since health, well-being, and security are proper concerns of Government, scientific progress is, and must be, of vital interest to Government." Basic research would drive innovation, and that would be the engine of the American economy.
Built into Bush's idea was the notion that if the free-enterprise, free-thinking citizens of the United States created any problems in pursuit of that progress, well, progress could solve them, too. Industrial revolution polluting all your rivers and air? Keep going, and we’ll eventually build power sources that don’t cough out poison. Cigarettes giving you cancer? We’ll figure out a cure. Pouring antibiotics on every damn thing until even the most basic infection can resist the drugs? We’ll make new ones. Scientific advances would surf the wave of calamities both natural and synthetic. Problems lead to solutions lead to problems lead to … you get it.
It was a good deal. Policymakers wouldn’t have to quash the rugged individualism that made Americans want to out-do their peers, or the rugged collectivism that made corporations want to do the same. In fact, government could encourage all that and rely on ongoing scientific research to hedge against the risks.
In other words, Bush was selling insurance.
Pay the money in, and if nothing happens, great! You get to know more about the universe. But if something does, you’re ready. Which is why President Trump’s proposed budget is, from a philosophical perspective, nuts. It either fails to account for the future, or holds it in escrow for the rich.
I’m not just talking about the cuts to both basic and applied research. These are egregious—“unsafe at any level of enactment,” as Tom Frieden, head of the Centers for Disease Control and Prevention under President Obama, tweeted. The AIDS treatment cuts alone could kill a million people, according to The New York Times. A cap on a specific kind of National Institutes of Health grant would nuke academic research. It could make weather forecasting worse. And so on.
Pay the money in, and if nothing happens, great! You get to know more about the universe. And if something does? You’re ready. Which is why President Trump’s budget is nuts.
The more telling cuts are in the programs that enforce regulations or the law, or that protect citizens against abuses and dangers. Eliminating the Federal Emergency Management Agency’s Flood Hazard Mapping and Risk Analysis program, for example, would save $190 million … and cost the knowledge of where climate change-powered sea level rise will inundate cities. Reducing the Department of State’s Global Health budget by almost a quarter would allow HIV, tuberculosis, and malaria to spread further and faster. Cutting $129 million out of the Environmental Protection Agency’s enforcement budget would allow polluters to keep polluting. You can’t pull $126 million out of public health preparedness and response programs and not realize what that looks like. The philosophy of the Trump budget is that the United States doesn’t need insurance anymore.
Here’s one possible explanation: The budget doesn't understand risk.
“When you have disasters, whether it’s a financial meltdown or national catastrophe or terrorist attack, right afterwards there’s a tendency to think it’s a big deal and take all measures,” says Robert Meyer, co-director of the Center for Risk Management and Decision Processes at the Wharton School at Penn. “The problem is that when we go ahead and take a preparatory action, most of the time the feedback we get is that the preparation is not worthwhile.” After the big hurricanes of 2005, for example, flood insurance purchases went way up. But two years later, people couldn’t recall why they were still cutting those checks.
Now, this explanation is actually terrible. Some premium is always paying a dividend for the feds. In a sense, the science budget is an aggregated risk pool. Maybe this year it won’t be a hurricane or sea level rise, but it might be Zika, or an oil spill, or a terror attack. The government gets all the positive feedback it could possibly want. “Given the large degrees of aggregation of risk they’re dealing with, you do get higher payoff,” Meyer says. “Which makes a decision to look away from these things even more shortsighted.”
So here’s another explanation: The budget doesn’t give a crap.
Sure, a government doesn’t have to buy science insurance. This is where the philosophy part comes in. Maybe society decides it's not worth spending a gazillion dollars on cancer or diabetes; maybe public health outcomes would be better with more money spent on prevention, on primary care, on emergency rooms. Maybe an agricultural policy that didn’t massively favor the production of high-fructose corn syrup would have a bigger impact on diabetes than trying to figure out how to get embryonic stem cells to turn into islets of Langerhans inside a pancreas. Maybe making tobacco prohibitively expensive and sunblock as cheap as water would dent cancer numbers more than drugs targeted at someone’s genome. Maybe deep space exploration is, while super-cool, not where the government wants to spend money at this time.
Congress Saved the Science Budget—And That’s the Problem
Trump’s Budget Would Break American Science, Today and Tomorrow
Scientists Prepare to Fight for Their Funding Under Trump
Science insurance does, on the other hand, allow a government to avoid becoming proactively hyperprotective. (Hey, check it out—a libertarian argument for science funding, almost by accident.) All of those broad policy changes I just halfway pitched would push money away from entrenched corporate interests and toward … well, other corporate interests, probably. Whatever; how about, do it all! Fund the science. Change the tax code. Embrace the "and."
Research says facts and statistics don’t convince people to buy insurance. One approach that does work, though, is to build it into their costs. “Like, make it part of their mortgage each year. Automatically, you get flood insurance,” Meyer says. “But you can opt out. Then the weight of thinking is, am I really sure I don’t want flood insurance? You’re not infringing on freedom of choice. You just kind of make investments in protection and safety the default.”
Again, I can imagine what that might look like—a sort of “science safety net” akin to the social safety net, built into the federal budget’s mandatory spending just like Medicare and Social Security. I don’t know what the priorities would be, or how they’d flex according to new problems (and solutions and problems).
But that’s not what’s happening in the Trump budget. It doesn’t find new priorities; it just abandons the old ones. So it’s hard to avoid concluding that President Trump’s budget doesn’t care about risk—because his administration doesn’t think it has to. The brunt of that risk will be born by the poor, by people of color, by women, by urbanites. Rich people can afford to pay for the things this budget would not—medicine, clean water, dry land. A metaphoric dome of money protects those people from inbound killer asteroids, metaphoric and literal. They don’t need insurance; they are paying for life out-of-pocket.
At 4 pm local time on May 25, Rocket Lab’s Electron stood on the company’s private launch pad on the Mahia Peninsula in New Zealand. Perched on the edge of an eroding cliff, pointing toward the sky from the southern tip of the world, the little rocket—just 56 feet tall and 4 feet wide, meant to carry similarly small satellites—looked ready for its first trip to space.
It was.
Around 30 minutes later, the engines ignited, and water poured over the launch site to protect the pad and quiet the noise. Steam billowed up as the rocket steeled itself to rise. The engines ran for a few seconds while technicians did their checks. And then they released the Electron. It went up, in that slow-at-first way of rockets, taking three seconds to get itself above the four-story launch tower. A minute after launch, it was as high as an Air New Zealand jet, and heading higher.
While the Electron did make the trip to space, it didn’t exactly arrive at its destination. “We didn’t quite reach orbit, and we’ll be investigating why,” said Rocket Lab founder Peter Beck, in a statement later that evening.1
With this test flight, and two more to follow, the Electron is set to become the first launcher made for, and for sale to, small satellite startups. Historically, little orbiters have had to hitch costly rides on big rockets, which, as the adjectives imply, are meant for big satellites. But having dedicated providers that tailor to their shrunken-down needs means more smallsat companies can get to space better, cheaper, and quicker. If you build a rocket, the payloads will come.
Beck, a New Zealander, founded Rocket Lab in 2006. He wanted to bring down the cost of spacefaring by using small, lower-cost rockets. “Right from Day 1, we were all about how we can make space more accessible,” says Beck, “whether that be through sounding rockets or orbital vehicles.”
The market Rocket Lab is now going for didn’t really exist in its beginnings. In the company’s first year, fewer than 20 CubeSats went to space. The same was true in 2009, when Rocket Lab tested its first suborbital rocket, the Atea-1. A year later, the CubeSat count stayed sub-20, but Rocket Lab won a government contract to study the viability of a little launch vehicle. By 2014, the world was readier: More than 120 CubeSats went into space, and soon Rocket Lab had a $6.9 million NASA contract to develop its rocket and do a demo flight.
That number now seems small. The company just raised $75 million of venture capital, achieving one-horned status: Someone thinks they are worth more than $1 billion.
Rocket Lab has been putting its cash to good use, ginning up the Electron rocket and a private launch site in a place that—let’s be honest—is a lot prettier than Cape Canaveral. From the Mahia Peninsula in New Zealand, the company can (legally) launch 100 rockets to space a year, and it’s allowed to blast off once every three days. With a 3-D printed engine and a small stature (really, it’s just 8.5 NBA players stacked on top of each other and filled with fuel), Rocket Lab can make and spark up its Electrons much faster and more often than anyone can an Atlas V.
And with this successful test rocket—named “It’s A Test,” following in the grand space tradition of straightforward nomenclature (black hole, Very Large Array)—the company is nearly ready to do so. “This is the beginning of the flight test program,” says Beck. “It’s the end of four years of R&D and testing on the ground. From a more personal level, its a really significant milestone to actually get a vehicle on the pad. Not many people make it there.”
Still, Rocket Lab’s legal launch rate sounds crazy. In the whole of 2016, all countries and companies together managed just 85 orbital launches. And one company thinks it might need a permit for 100? OK. Dream on, dreamweaver.
But the world might actually have room for that many miniature rockets. And Rocket Lab has company in its quest to create them. Vector, whose first public customer is the orbital radar-maker Iceye, is a similar rocket start-up. Others come with big backing: Virgin Galactic just spun off Virgin Orbit, whose LauncherOne will make at least 39 trips to space for would-be internet provider OneWeb. The industry already has fallen soldiers, too: Firefly Space Systems, which got the same NASA grant as Rocket Lab, filed for bankruptcy earlier this year. And SpaceX meant its very first vehicle, the diminutive Falcon 1, to rip open the smallsat industry. But coming as it did in 2008, the entry was premature. SpaceX retired its early bird and focused, later, on selling rideshares.
The smallsat makers—whose creations will mostly take pictures of Earth and provide space-based internet, making up a projected $22 billion industry in a decade—are ready for their special rockets. They have cut their own costs by shrinking their satellites, but right now they have to pay for expensive tickets on outsized rockets. And they share the ride as second-class citizens, stuffed in among the more substantial payloads those rockets are actually meant for. The substantial stuff determines where the rockets go, and those orbits are often suboptimal for smallsats. But they must follow the big satellites’ money. Either that or a company can buy its own behemoth rocket—at a cost of $30 to $60 million—and then try to get other people to ride along and chip in for gas money.
Frankly, everyone is sick of it. “This geospatial revolution is happening, but access to space is critically important,” Jason Andrews, CEO of Spaceflight Industries, told me at the Space Symposium, a yearly gathering of companies and governments with business off-Earth. “The technology has changed, the size of the spacecraft has shrunk, but the size of the launch vehicles hasn’t.”
Until, of course, now. That’s why Spaceflight bought its own Electron rocket, which costs around $5 million, on May 17. Spaceflight books launches for satellite makers and sets them up with rideshares if they so desire. They already own one of SpaceX’s Falcon 9 rockets, and now they have this. “We purchased the Electron to accommodate customers with payloads who need to reach less common orbits that are not routinely served today,” says company president Curt Blake.
That’s an option the satellite companies themselves—like Planet, which currently operates the solar system’s biggest group of Earth-observing orbiters—also like the sound of. “Small-size payloads will be able to purchase the entire Electron rocket and have complete control over when it launches and to what orbit,” says Mike Safyan, Planet’s director of launch. “This further lowers the barrier to entry for small satellite companies, allowing many more new ideas to be deployed into space on software startup budgets.”
Others who have jumped over the barrier and signed on as Electron customers in one way or another include NASA itself; Spire, which will track ships and talk about the weather; and Moon Express, which wants to go to the moon, expressly.
All of those organizations were likely watching as the very first Electron heaved itself from the launch pad, jumped up an energy level, and became not just a little rocket that could but one that understood what they needed and gave it to them.
Tiny DIY Satellites from Stanford
1Update 12:55 am EDT 5/25/2017: This story has been updated to add information Rocket Lab provided after launch.
When astronaut Peggy Whitson pushed out of the International Space Station’s airlock on Tuesday morning, she was floating into history. Stipulated, Whitson was already a badass. But this extra-vehicular activity—an EVA, NASAspeak for a spacewalk—was Whitson’s 10th. That ties her for the American record. A PhD biochemist before she became an astronaut, Whitson has now spent more time in space outside a spacecraft than all but two other human beings.
Whitson was also floating into the future, though, and it seems sure to be filled with more urgent repairs like this one. The spacewalk was a “contingency EVA,” which—NASAspeak again, etymologically derived from Testpilot High Laconic and Scientific Detach-ese—means “serious emergency.” No one knows yet why a box full of computer boards called a Multiplexer-Demultiplexer failed, but NASA started building the ISS in 1998. The station is entering its third decade of life in orbit. More and more pieces are going to start breaking.
That’s not to say the astronauts can’t handle it. “Today’s EVA—they’re never routine, but it was among the more routine,” says Michael Lopez-Alegria, a retired astronaut and private space consultant, (He shares the 10-EVA American record, too, but has more total time in space than Whitson—for now.) “They’ve practiced it before, and the ground team has put together similar procedures on a similarly short fuse.”
For Tuesday’s spacewalk, Whitson and fellow astronaut Jack Fischer had to first build a new MDM from spare parts on board. And they’d just replaced this same MDM and its partner last month … after a previous contingency EVA in 2014 had replaced one of them, too. The station has 48 altogether; these two problematic ones regulate radiators, solar arrays, and cooling loops—temperature and power. “If one of them fails and we’re down to the backup for that box, we are one more failure away from something near catastrophic,” says Terry Soich, a project engineering manager for human space at Honeywell Aerospace, which manufactured the MDMs. “They jump on that pretty quick.”
The reality is, they’re going to continue to have problems. As all that equipment gets older, you’re going to have a higher incidence of failures.
Daryl Shuck, Honeywell Aerospace
To do the job, Whitson had to make her way out to the Starboard Zero truss, unbolt the MDM with basically a space power wrench, clean out the bolt-holes with a cold-gas gun (with Fischer’s help), and bolt in the new one. They had to hold onto the broken MDM, because NASA and Honeywell want to know what went wrong, and if they put it down it would have just, like, floated away.
Planning and preparing for the 2014 MDM replacement spacewalk took a couple weeks; this crew did it in slightly more than two days. Whitson and Fischer—who also installed a couple antennas on the EVA—started early and finished ahead of schedule. “It’s just a great testament to the machine that this EVA team, on the ground and the crews on orbit, have become,” Lopez-Alegria says. “They made it look easy.”
But, gosh, it sure would be nice to know what was wrong with the MDMs the astronauts keep having to fix. “It showed similar, but not identical, symptoms to the event in 2014 in that the failure was not preceded by any signs of bad telemetry,” says Daniel Huot, a NASA spokesperson. The box they swapped out back then had lasted longer; it had been in continuous service since 2002.
So what happened this time? “The short answer is, they’re getting old,” Soich says. “Until we get it back inside and test it, we’re not going to know. We requested that data from the crew.”
The MDMs were supposed to last a decade; they’ve nearly doubled their operational lifespan. But that’s true for a lot of the pieces that comprise the ISS. And that could be a problem. The station was supposed to have a 15-year mission, but NASA keeps extending its time in orbit. The clock just got reset to 2024, maybe longer. “NASA has anticipated these failures. They know they’re on somewhat borrowed time when they’re that far over the designed life,” says Daryl Schuck, a customer business manager at Honeywell Aerospace. “The reality is, they’re going to continue to have problems. As all that equipment gets older, you’re going to have a higher incidence of failures.”
The Future of the International Space Station Is Up to a Weird Little Florida Nonprofit
SpaceX Will Give Its ISS Re-Supply Another Go, After Scrapping Yesterday’s Launch Attempt
NASA’s Power Supply Mistake on the ISS Was Totally Avoidable
Not to knock as amazing a piece of technology as the ISS—it’s still safe and functioning nominally, as the NASA people say. But at this point it’s a little bit like a 20-year-old car. Even if it’s a classic, you’re probably not going to put in a new sound system. It’s a hub for space research, albeit an expensive one. NASA has committed to furthering commercial space businesses; right now, the ISS is a destination for SpaceX and, perhaps someday, its competitors. If it goes away, what happens to them? “We’d love to see more upgrades. NASA is, I think, trying to fly as economically as possible,” Schuck says. “You could preempt those failures with preventative maintenance, but you’d spend all your time doing these preemptive changes and not doing a whole lot of science.”
As good at handling contingency spacewalks as the astronauts and ground crews are, they’re not supposed to become the new normal. Then again, more frequent maintenance spacewalks would have an upside: The cosmonaut Anatoly Solovyev still holds the world record, with 16 EVAs and over 82 hours in space. But Commander Whitson’s still up there, space power wrench at the ready. Just saying.
On April 8, SpaceX will launch an inflatable, inhabitable bouncy castle to the International Space Station and it may be the start of the first hotel chain in Space.
In April 2015, researchers in Brazil reported the first case of Zika virus—finally putting a name to the mysterious rash, fever, and joint pain-causing illness that had been swarming the northeast corner of the country. By the time the World Health Organization declared Zika a global health emergency nearly a year later, the outbreak had spread to 26 countries and territories in the Americas, infecting hundreds of thousands of people and leaving many babies with an incurable developmental defect called microcephaly.
Since then, researchers have been racing to develop treatments and vaccines, the first of which entered mid-stage human trials at the end of March. But according to new genetic evidence published today, public health efforts to contain and fight the disease could have—and should have—gotten underway much sooner.
Zika, it turns out, had established itself in Brazil as early as 2013.
The revelation comes from the same group of seasoned virus sleuths who used genetics to help stop Ebola's spread through Sierra Leone in 2014. This time, they sequenced more than 100 new Zika genomes, taken from patients and mosquitoes throughout the Americas. They traced the virus’s spread from Brazil to the nations next door, into the Caribbean and then the US. The reconstructed genetic history, published in three separate Nature papers, could help drug developers look for a cure, and public health officials develop containment strategies. But mostly, they make a strong case for developing a genetics-based global surveillance system so that the next outbreak—whether it’s Zika or something else—doesn’t shake the world quite so hard.
Throughout the summer and fall of 2015, as Zika cases poured into public health databases, photos of babies with under-sized skulls made their way to the front pages of newspapers. The whole time, the world’s most famous virus hunterwasn’t lifting a finger to help. She could barely lift a finger at all.
Pardis Sabeti had lost weeks of sleep and five teammates helping crack Ebola's code in 2014. When Zika hit the following summer, the Harvard computational biologist was recovering from something more devastatingly prosaic. In July, while a passenger on all-terrain vehicle in Montana, the vehicle went over a cliff, catapulting Pardis onto boulders, shattering her knees and pelvis and causing a traumatic brain injury. “I was in a wheelchair, the lab was running without me, it didn’t seem like the right time to step in and do something,” she says. “And it was outside of our domain and our region of the world; we didn’t want to just parachute in.”
Sabeti watched the Zika crisis from her bed, as her body and brain continued to heal. Emails poured in, asking for her help. Many of the same people she had worked with on Ebola were trying to tackle this new outbreak the same way they had in Africa—with portable sequencing machines in mobile laboratories that could spit out full viral genomes just a few minutes after diagnosis. But Zika was sneaky. There just wasn’t very much of it in the blood samples. The sequencers couldn’t reliably capture all of Zika’s 10,000-plus base pairs. So as soon as Sabeti was able, she brought her Broad Institute lab into the fray.
Creating Zika-Proof Mosquitoes Means Rigging Natural Selection
The Scientists Trying to Keep Zika Out of Your Placenta
A Clue to the Mystery of Colombia’s Missing Zika Cases
Using a new method they developed with partner labs at Oxford University and the University of Birmingham, they eventually were able to sequence whole Zika genomes directly from clinical samples without having to first isolate the virus and culture it. The new approach involved making lots of copies of the genome fragments by carrying out dozens of reactions in just one tube—to cut down on time and opportunities to mess something up. Now they could collect samples from Brazil, Colombia, Honduras, and all over the Caribbean—even Florida. Sabeti’s lab sequenced them all.
As soon as they validated the genomes, they sent them back to their respective health departments, then released the genomes to the public through databases like GenBank and Virological. They uploaded the first 33 sequences last October, and continued to release more as fast as they could process them. They didn’t want to wait for months for other researchers to be able to start using them. “Genomic data is central to creating better diagnostics, treatments, and vaccines,” she says. “This is frightening for people, and with a very vulnerable population we wanted to make sure we’re capturing it as soon as possible.”
Once all the genomes were sequenced, Sabeti and her collaborators still had to figure out how they all fit together. To do that, they used a technique called a "molecular clock." When cells divide, or viruses replicate, the proteins that make copies of the DNA introduce mutations into the genome at specific, predictable rates. Zika, like all RNA viruses, is a pretty fast mutation machine. (Not Ebola fast, but still fast.) You can think of each mutation like the tick of a clock: The more mutations accumulate, the more time has passed. Using that technique, the researchers were able to estimate when Zika actually began showing up in different countries—often many months before the first official reports.
These massive, genomic-based surveillance efforts could potentially foretell an outbreak within days or weeks, rather than waiting months or years for symptoms to start showing up. “This is a big red alert to all countries,” says Sabeti. “We have the tools to monitor outbreaks in as close to real time as possible, but we have to have these systems in place before they hit.”
In the US, only Miami, Florida and Brownsville, Texas would need year-round surveillance—they are they only place where Aedes aegypti mosquitos survive for all 12 months. Kristian Andersen, an infectious disease geneticist at Scripps Research Institute and author of one of the three *Nature *papers, discovered this by layering epidemiological data on the molecular clock-derived evolutionary histories. His team discovered that it was the mosquito's year-round presence that made Zika establish so firmly in those cities. The major airports helped too.
Andersen’s work also turned up some interesting “bonus” tidbits, as he calls them. The first is that Zika cases are perfectly correlated with the number of mosquitoes in the area. If you knock down mosquito populations by half, the number of Zika diagnoses drops by half. “That tells us that vector control is an effective way of preventing human cases from occurring,” Andersen says.
The second has to do with the seasonality of the outbreaks. “We think there’s something special happening in the early spring, when mosquitoes are bouncing back from the winter,” Andersen says. As Aedes aegypti numbers explode, that’s when they’re spreading Zika into humans. Not later, when the symptoms start showing up. “When mosquito populations are expanding dramatically, before you see the first human cases, that’s when you should focus the majority of your control efforts,” says Andersen.
Last week, two years after its first reported Zika case, and more than three years after it first showed up, Brazil officially declared an end to its Zika national emergency. The virus has hit so much of the population that herd immunity seems to have finally caught up. But that kind of protection is fleeting. People still need reliable tests and reliable vaccines, tailored to whatever strains they’re most likely to encounter. Sequencing projects like this one provide a road map for scientists to develop them. But it will take much wider deployment to catch the next outbreak before it happens. In the US alone, more than a dozen new human diseases have cropped up in the last two decades. It's only a matter of time before the next one arrives.
Scientists in California are breeding and releasing mosquitos into Zika hotspots. While it may seem like they're making matters worse, they are actually releasing a kind of biological trojan horse.
Anna Devane lies in a hospital bed surrounded by doctors, one of whom just told her she has cancer. "It's a scary word, right? We can all admit that," says a second doctor, Robin Scorpio, who also happens to be Devane's daughter. This cancer is rare, says the first doctor, and somewhat different. People with the condition, called polycythemia vera, make too many blood cells. This, adds a third doctor, explains Devane's recent migraines.
"So how do we treat it? Is it ... radiation? Chemo?" Devane's voice cracks. Remarkably, no. The first doctor, a husky-voiced practitioner named Hamilton Finn, tells Devane she needs only a prescription blood thinner and regular phlebotomy—blood letting. Anna shows relief, then irritation. "OK, fine, I can do that," she says. "But this protocol sounds like you're treating the symptoms of this cancer. How do we beat it?"
"I'm sorry Anna," says the third doctor, Griffin Monro. "There's no cure for this disease."
This dramatic scene—complete with music and close ups—played out on General Hospital in February. The plot twist grew from a partnership between the show's producers and the Incyte Pharmaceuticals. Strictly speaking, it is not an advertisement, because the FDA allows companies to fund disease awareness programs. But Incyte makes exactly one drug, and it targets the genetic mutation associated with polycythemia vera. Medical professionals worry that General Hospital's plotline blurs the lines between awareness and advertisement.
Vinay Prasad is a real life doctor who works on rare blood diseases at the Oregon Health and Science University in Portland. "I heard about this through the grapevine," he says. "One of our nurses was home one day watching General Hospital, and told an oncology fellow in my department that there was a character with polycythemia vera." That stunned him. No more than 100,000 Americans live with PV, making it an ultra-rare blood condition. "PV is this very indolent, mild cancer where you just make a little bit too many red blood cells," says Prasad. The most common treatment, he says, is exactly what those TV doctors prescribed: a combination of bloodletting and blood-thinning medicines like hydroxyurea or aspirin.
But that's not the only treatment. About 96 percent of PV patients share a mutation in a gene called JAK2. As it happens, the only FDA-approved drug Incyte Pharmaceuticals sells is a JAK2 inhibitor called ruxolitinib that targets PV and a few similar diseases. Although no one on General Hospital characters mentions ruxolitinib, Prasad says the dialogue during Devane's diagnosis contains subtle language that might lead viewers to believe they have PV symptoms, and seek out unnecessary treatment.
Gene Therapy Emerges From Disgrace to Be the Next Big Thing, Again
The Weird Business Behind a Trendy “Anti-Aging” Pill
How Prescription Drugs Get So Wildly Expensive
The FDA has rules about disease awareness, but they far more relaxed than those regulating direct-to-consumer advertising. You know these ads: In the first act, an actor mentions that his herpes/toe fungus/erectile disfunction doesn't keep him from kayaking/rock climbing/enjoying a fulfilling love life because he takes a drug with a name that sounds like a Lovecraftian Elder God. In the second act, the actor quickly explains that the drug might also cause nausea, vomiting, sweating, fainting, jaundice, bleeding, light-headedness, stroke, blindness ... and in a very small number of cases, death. "Talk to your doctor before using [blank]."
Non-branded disease awareness campaigns follow looser rules: They can talk about a medical condition as long as they don't promote specific treatment. General Hospital followed those rules. However, disease awareness can obliquely prompt people to seek unnecessary treatments. Take the awareness culture surrounding breast cancer. On one hand, many women now regularly check for lumps, and go for screenings. But not all breast cancers are deadly. Some may never become malignant.
But, to quote Robin Scorpio: Cancer is a scary word. So when women find a lump, they are likely to seek treatment. Those treatments come from pharmaceutical companies, which—surprise!—fund many breast cancer awareness efforts. A study published in October in the New England Journal of Medicine found that only about 19 percent of the small tumors women find during early screening are likely to become large.
Drug companies defend their awareness efforts. "Patients suffering from rare diseases often face a dearth of information about their disease and related support," says Catalina Loveman, an Incyte spokesperson. "As such, it is critical that those who have a voice—companies, advocates and media—do all they can to raise awareness and provide resources to these often overlooked and underserved communities." She adds that Incyte did not hide its partnership with General Hospital. It even issued a press release. ABC, General Hospital's network, did not reply to a request for comment.
Prasad says General Hospital left out crucial context. In fact, in a recent letter to the Journal of the American Medical Association, he and Sham Mailonkody (an oncologist at Memorial Sloan Kettering in New York) argue that Incyte and General Hospital were doing marketing in disguise. For instance, the show's dialogue did not emphasize the subtleties doctors look for in diagnosing PV. Viewers might identify with symptoms Devane exhibited, and seek testing from a doctor. So what—If most PV patients carry the JAK2 mutation, shouldn't their diagnosis be a slam dunk? Not so fast. JAK2 is common in many people without PV. Not everyone needs to start popping aspirin and draining blood.
But most people with PV probably should be doing those things. And that's where the General Hospital subplot is most insidious. Prasad says Devane's complains that those treatments only attack the *symptoms *might prime any viewers who receive a positive PV diagnosis to make the same complaint to their doctors. Then they might hear about ruxolitinib—a treatment that targets the proteins the JAK2 mutation expresses— not the symptoms that come from having too much red blood.
Ruxolitinib isn't a cure, though, because scientists still haven't figured out if JAK2 causes PV. All they know for sure is that most people with the disease have the same mutation. "We don’t know when these mutations are passengers, drivers, when they should be targeted, and when they shouldn't be," says Prasad. That makes ruxolitinib a gamble—an expensive one. The drug can cost upward of $1,000 a month.
The logical counterpoint to all of this anti-pharma hysteria is that General Hospital and Incyte are guilty of nothing more than disseminating information. "I would say you want to have the right information," says Robert Klitzman, a bioethicist and psychiatrist at Columbia University. He said the soap opera could have laid out exactly how the symptoms led to the diagnosis, and then mentioned the company's drug as an option alongside other treatments—not create an unspecified need by making the other options seem unattractive. "If the patient says 'I don't like getting blood draws,' then the doctor can say, 'Well there is this other drug, but it's not clear that it will help, it's not recommended unless these other, cheaper, drugs are not effective, and it's going to have side effects," he says. And, as always, dramatic music is a nice touch.
Medicine has an expiration stamp—but Is it actually, you know, serious? Or are those sell-by dates just a Big Pharma racket? Mr. Know-It-All gives you a healthy dose of the truth.
Listen, I’m not going to sit here and tell you that baleen whales are big. You know that. In fact, I bet you know that at 300,000 pounds, the blue whale is the heaviest creature to ever exist. But I bet you don’t know why such filter-feeding whales are so big—because not even scientists know that.
Today, though, one group of researchers offers a fascinating theory in Proceedings of the Royal Society B: Fusing the fossil record and phylogenetic work (that is, determining the relatedness of species to one another), they found that baleen whales probably got colossal just 3 million years ago—a sliver of time in the grand evolutionary scheme of things—and climate change probably triggered the transformation. That, of course, carries troubling implications for how the giants might fare as Earth’s oceans warm and acidify.
Baleen whales—which gulp massive amounts of water and filter out tiny critters like krill—have been around for about 30 million years. But for the vast majority of that time, they were modestly proportioned, topping out at 30 feet or so, or about one-third the length of the modern blue whale. Now, being small offered some perks. For one, it's easier to open your maw when it's smaller.
But at the beginning of the ice ages some 3 million years ago, the oceanic ecosystem transformed on a grand scale. Ice sheets expanded in the north, and runoff dragged nutrients into the sea. An increase in wind-driven upwelling brought even more nutrients up from the depths—as wind blows away the top bit of the water, deeper waters rush up to replace it. This resulted in blooms of plant-like phytoplankton, leading to blooms of zooplankton like krill that feed on the phytoplankton. Whale food began concentrating in certain places at certain times of year.
Being big would have been an advantage for whales in the midst of this climatic transformation in a number of ways (the trait evolved independently in different baleen whale lineages, strongly suggesting it gave them an edge). For one, bigger whales reoriented within the food chain, growing so large that they put themselves out of reach of predators. And a bigger mouth more efficiently gulps zooplankton. If you’ve got more fat reserves, you can make longer migrations to bounce around intercepting these seasonal events, while smaller whales are stuck with more modest concentrations of plankton in their usual foraging waters.
“We think that kind of ecological explanation underpins why we see this shift in body size,” says paleobiologist and study co-author Nick Pyenson of the Smithsonian. “Not just the origin of the very big, but also the extinction of very small baleen whales, which today are only represented in that lineage by the pygmy right whale.” Smaller whales just couldn’t compete in this new world order, but why the pygmy right whale escaped extinction remains a mystery.
But couldn’t whales have grown so large because, well, they just could? Floating in water is a whole lot different than struggling with a big body on land. “But that doesn't hold up when you look at the data,” says Pyenson. “It's not true that as soon as whales got into the water they got very, very big—as big as a blue whale.” After all, baleen whales lived in the water for tens of millions of years before growing into giants.
What this research makes clear is that this is an extraordinary moment in the natural history of Earth. Think about what it takes to sustain not just blue whales, but other baleen giants like gray whales. They fuel themselves for journeys of thousands upon thousands of miles. “There has not been another time when there has been so much large predator biomass reliant on such productive oceans,” says vertebrate paleontologist Erich Fitzgerald, who wasn’t involved in the study. “There is no analog in history. We can go back right to the end of the age of dinosaurs, if you like, 66 million years ago, and these last 3 million years are unique.”
Humanity may be threatening to bring it all crashing down. Minute changes in ocean temperature can have massive cascading effects on ecosystems. If currents change or cold upwelling waters stop surfacing nutrients, phytoplankton don’t bloom and zooplankton don’t feed. When zooplankton don’t feed, neither do baleen whales. If blooms shift to other parts of the sea and the whales can’t find them, that size advantage suddenly becomes a burden.
Climate change made the giants, and while it’s still too early to say exactly how it’ll affect them in the coming decades, they’re certainly in a precarious situation. “One thing we can say though from the fossil record,” says Fitzgerald, “is where you tend to have specialists, and they can be small or they can be titanic in proportion like the blue whale, they are really—and this is a phrase used in the paper—they are really on an evolutionary knife-edge.”
So sure, science knows that climate change made big whales big. Now the challenge is making sure it isn’t climate change that does them in.
Roger Moore died today. Now, you could argue that Moore was not the best James Bond, and I'd be willing to have that discussion at some point, but I think everyone agrees he made significant contributions to the 007 canon. I certainly think so, if only because when I was a kid, Moore was the James Bond I saw in movie theaters. Sean Connery was the James Bond who appeared on television with older, outdated gadgets.
To honor Moore's passing, I thought I'd use physics to analyze one of the many cool scenes in his Bond oeuvre. I considered taking a look at his amazing submersible Lotus Esprit from The Spy Who Loved Me, but I've already written about the physics of submarines. Instead, I'll examine a crazy scene in Moonraker in which the henchman Jaws throws 007 out of a plane without a parachute, and Bond must steal a parachute from some other evil dude while plunging toward earth. Allow me to pose some questions, and answers.
Looking at the clip, Jaws' minion jumps from the plane and Bond follows about five seconds later. If both men were in free fall, Bond would be hosed. There's no way he could catch up, because both men would be accelerating at 9.8 m/s2—the acceleration of a free falling object.
Roger Moore Had the Greatest Gadgets in the History of Bond
The Evolution of the Bond Gadget, From Insane to Reasonable
From Physics With Love: James Bond Cocktail Tricks
Ah, but the men are not in free fall. Free fall occurs when an object is under the influence of a gravitational force alone. But a skydiver is subjected to two forces (for the sake of this discussion, anyway). The first is the downward force of gravity, which depends upon the mass of the object and the gravitational field. The second is air resistance. This force depends upon several things, including air density, the shape of the object, and its velocity.
You can test this yourself, and you don't even have to jump (or be pushed) out of an airplane. Just stick your hand out the window of a moving car. Feel that force pushing it back? That's air resistance. Place your hand straight up like you're making the sign for "stop," and you'll feel greater resistance (force). Now make a fist. You should feel less resistance (because you've decreased the surface area, but that's a topic for another post).
So. Back to Bond tumbling from that plane. Both gravitational force and an air resistance force act on 007 and that minion he must catch up to. At first, the air resistance force and gravitational force act in different directions, because the two guys are moving sideways. But as the skydivers increase their downward speed, the air resistance increases in magnitude and points in the opposite direction as velocity. Within seconds, the air resistance pushing up and gravitational force pulling down add up to zero. With zero net force, each of them tools along at a constant speed. Physicists call this terminal velocity. A normal human achieves a terminal velocity of about 50 to 55 m/s, or around 120 mph.
At this point, the minion is just chillin', observing the scenery and maybe wondering what he will have for dinner. No big deal. Bond, on the other hand, has work to do. He must catch up with the guy, or die trying. All he has to do is decrease his surface area (notice how he moves his arms closer to his body). This decreases the air resistance, allowing Bond to accelerate as he continues plummeting until he again achieves a terminal velocity greater than that doomed henchman. So, yes, Bond could totally catch another skydiver.
Bond certainly has his work cut out for him. He must catch the henchman, fight him, and steal his parachute before donning that chute and fighting Jaws. That's a lot to do. Looking at the video clip, the dive appears to last 115 seconds. How far would Bond fall in that time? Let's do the math.
Allow me to make the rough estimate that Bond moves at a generally constant speed. He eschews the standard skydiving position for most of his dive, so let's assume a terminal velocity of 60 m/s. With this, I can use the definition of average velocity (in the y-direction) to solve for the change in vertical position:

Falling 6,900 meters (more than 22,000 feet) is a stretch. Such an altitude is clearly higher than you'd want to go without supplemental oxygen, but lower than the "death zone" of 8,000 meters at which there is too little oxygen to survive. Still, for Bond to fall that far, the plane would have to be flying even higher, and it would still leave Bond no more than 1,000 meters to pull his 'chute and survive the dive. Overall, I guess this is at least plausible.
No, but it looks funny.
As the scene ends, Jaws breaks his parachute and lands on a circus tent. Suppose he had a speed of 50 m/s when he hit the tent. Looking at the video, the tent slows him to a stop in three seconds. What would be his acceleration? I can use the definition of average acceleration:

Jaws experiences a change in velocity of 50 m/s, so I simply divide this by the time interval of three seconds to get an acceleration of 16.7 m/s2. It turns out that acceleration provides a good indicator of human damage. Too high of an acceleration results in a dead person. Typically, physicists consider acceleration in units of g's where 1 g = 9.8 m/s2. Jaws would have a landing acceleration of only 1.7 g's. According this table of human tolerance to g-force, Jaws should easily survive this landing and live to die another day.
They call it shroom juice, a dark slurry of fungi that will not get you high. Workers dump packages of magic powder into buckets, add water, and stir. Then they grab saplings, dunk the roots into the mix, and drop the baby trees into the ground.
All right, fine, the powder isn’t actually magic, at least not in anything but the ecological sense. It’s loaded with six different species of fungi called mycorrhizae, which form one of the strangest and most mysterious networks in nature. Hidden in the soil, tangled among the tree roots, the fungi both take energy and give energy to the trees, on top of running a protection racket. Trees couldn’t make it without the fungi, and the fungi couldn’t make it without the trees, forming an expansive ecosystem that scientists are just beginning to understand.
Here in West Virginia, where logging and burning and farming have leveled forests and sapped the soil of nutrients, Nature Conservancy scientists and wildlife officials are shrooming in the most sober sense of the word. The problem is that while some types of trees have made a comeback here, red spruce certainly has not. It doesn’t regenerate from its root system like others might, so these crews are replanting saplings by hand, inoculating them with the mycorrhizal fungi that the trees will depend on for the rest of their lives.
Fungi on a pinyon pine root
Some 50,000 species of mycorrhizae lurk in soils all over the world. They penetrate tree roots, exchanging nutrients and shielding the trees from toxins (some species even grow as sheaths over the roots, protecting them from hungry critters), growing so extensively that they end up connecting swaths of forest. The fungi function as an extension of the root system, growing farther and deeper than the roots themselves and squeezing into tighter crevices.
What these researchers have found in Appalachia is that without inoculating the trees with fungi, the things go into transplant shock. They turn yellow and don’t grow so fast, likely because the fungi ecosystem in the soil ain’t what it used to be. “So what we’re trying to do is reduce that transplant shock, give the trees a little bit of extra nutrients, so they can grow a little bit faster,” says Mike Powell, a stewardship manager at the Nature Conservancy.
Not only are there tens of thousands of different species that form mycorrhizal relationships, but the nature of the relationship will differ. For instance, how the fungus and plant exchange nutrients, or whether or not the fungus penetrates the cells of the roots. Part of untangling the mystery of the mycorrhizae, then, is figuring out where you’ll find these different relationships. So Nature Conservancy ecologist Randy Swaty has assembled a mind-meltingly complicated map of the US that shows the various associations pre-European settlement, meaning it indicates what's possible in a landscape (check out West Virginia below).
A map of West Virginia showing different fungal associations
He and his colleagues took vegetation data from the government’s Landfire program, which plots all kinds of things on US land, from simple topography to the amount of fire fuel in a given area. “Now we also have a great understanding of which fungi inhabit the roots of different plants,” says Swaty. “So we're basically able to attribute roughly 9 billion pixels in the United States with a mycorrhizal community based on the plants and the fungi that they would have on their roots.”
Armed with this data, conservationists can better understand the fungal relationships in an ecosystem. So say pines disappear from a particular landscape. “Pines might be the only plants there that host a particular type of fungi,” says Swaty. “So if restorationists in the future try to plant pines on that site, they're going to have to inoculate those seedlings.” This knowledge will grow all the more valuable as climate change transforms American forests, potentially wiping out species that can’t adapt fast enough.
So sure, humans are making a mess of the planet. But maybe a sip of shroom juice can help.
When I was young, there was no such thing as the World Wide Web or video streaming. If you wanted to watch something, you had to wait until it appeared on television. Sometimes you might think, "Hey, I think I'll watch a show," and flip the channels until you found something interesting. This is how I discovered The Mechanical Universe ... And Beyond.
If you are not familiar with this wonderful television program from the mid-'80s, it was essentially a college-level introductory physics class presented by Cal Tech University. It included classroom lectures by Caltech applied physicist David Goodstein, some excellent physics demonstrations, and cool stuff like historical reenactments. The thing I remember most about it is how it mathematically manipulated equations with weird animation. Now that I think about it, those animations probably reinforced the incorrect notion of "moving stuff to the other side of the equation," but still. They were cool.
Now that the internet exists, you can find The Mechanical Universe on YouTube, and you ought to check it out. Beyond being awesome, it shows why the traditional college lecture is dead.
What is the traditional lecture? It is a model of learning in which a teacher possesses the knowledge on a given topic and disseminates it to students. This model dates to the beginning of education, when it was the only way of sharing information. In fact, you occasionally still see the person presenting the lecture called a reader, because way back before the internet and even the printing press, a teacher would literally read from a book so students could copy it all down.
Now, don't get me wrong. The traditional lecture model worked wonderfully for eons. But it is an outdated idea, something that becomes obvious if you watch even a single episode of The Mechanical Universe. Close your eyes and imagine yourself in a college physics course with a professor giving a traditional lecture. Now open your eyes. (I'm speaking metaphorically; obviously, if you closed your eyes, how could you know that I just said to open them? Busted.) Did you envision The Best Physics Lecture EVAR? I doubt it. You probably pictured someone droning on and on in front of a chalkboard or PowerPoint presentation. No way that is more engaging or interesting than an episode of The Mechanical Universe, and if you're a teacher who uses traditional lectures, just stop and play the show instead. Everyone will be better off.
But wait! Perhaps you do something a video can't—you invite students to interact and ask questions. Great! But still, I say play The Mechanical Universe. When someone wants to ask a question, pause the show and let them. That will still beat most lectures.
You may think by now that I think most physics professors are dolts. I promise that's not the case. But traditional lectures simply aren't effective. Research shows students don't learn by hearing or seeing, they learn by doing, a model often called active learning.
Physics faculty should start thinking about how they can go beyond just a traditional lecture. There are some easy things they can do (or students can ask them to do) to make learning more engaging. First, make students read the book outside of class, rather than in class. If your lecture merely covers the material in the textbook, why make students buy the textbook? Now, you may put a different spin on the material, but still. You're merely repeating what students can read on their own. Let them do that on their own time, and use the classroom for experiments and demonstrations and so forth. This is the idea behind a flipped class, something I've done more and more of in recent years. It makes learning more fun.
Another easy change? Start using a student response system. As you discuss a topic, present the class with a multiple choice question and let students discuss it and vote on the answer. If they answer correctly, great, move on. If not, explain why the answer is incorrect and let them try again. I find that the most productive lessons come when the class is split between two answers and everyone tries to convince the other side. For example, if you ask students what happens to the motion of an object under a constant force? Some students might say it will move at a constant speed, while others say it will change speed. Trying to convince their peers leads to great discussions. You might think that sounds like chaos, but in my experience it works terrifically.
Of all the things you can do to make learning more active, one of my favorites is plain old problem solving. Give students a problem—or heck, have them make up the problem and then let them solve it in groups. You would be surprised at how much the students learn by working in groups and then sharing their answers with classmates. This is a go-to activity for me because it's easy to do, and students love that it helps them practice problems that might appear on a test.
These are just a few examples that I've used. You may have others. The point isn't to use any one example but to go beyond the traditional lecture by creating an environment in which students actively participate in class. You may experience some difficulty, even reluctance doing this. If so, take baby steps. Do one thing at a time. But do something, because professors who continue giving traditional lectures might just find themselves replaced by a video.
Fifteen years ago, Kevin Tracey sat in a Washington, DC, conference room surrounded by officials from the Defense Advanced Research Projects Agency. They’d been paying the neurosurgeon to study how doctors could stimulate the vagus nerve—a long nerve that controls everything from blood pressure to sexual arousal—to treat inflammation associated with PTSD. Now they wanted to know: Could he stimulate anything else into submission? He searched his brain. It might work for bleeding, he told them. Which is when he started talking about worms.
C. elegans eats dirt, lives for a few weeks, and has just 959 cells in its transparent worm body. It’s about as primitive an organism as you can find that can still teach you about human biology. Exhibit A, Tracey explained: Poke a hole in a worm and good stuff goes out (cytoplasm) and bad stuff goes in (bacteria). The same goes for humans—just sub blood for worm juice. In both organisms, the immune system responds in the same way: Nerves near the wound fire, calling blood cells and platelets to the damaged area. There was a decent chance, Tracey said, you could hack that response—use electricity to stop people from bleeding to death on the battlefield.
At the time, it was just an idea. But in the decade and a half since, Tracey and other scientists pioneering the field of bioelectronic medicine have turned the concept of a neural tourniquet into a viable antidote to massive blood loss. The military wants to use it to supercharge soldiers’ immune systems before combat. Bill Gates wants to use it to put an end to the number one cause of maternal death worldwide: blood loss during childbirth. If it works, it will be the first real innovation in the field since the leather belt.
Civil War Turns Syria’s Doctors Into Masters of Improvisation
The Unfinished Science Behind the New Wave of Electrical Brain Stimulation
Parasite-Busting Drugs Take the Nobel Prize in Medicine
If the Feinstein Institute, where Tracey is president and CEO, is the scientific home of biolectronic medicine, then Darpa has been its earliest champion. The military research agency started giving Tracey money in the 1990s, helping him decode the language of the nervous system and use it to choreograph the body—stimulating or blocking nerve signals to fight disease and injury. By the early 2000s, Darpa wanted Tracey to expand his portfolio. Which is why they summoned him to Washington.
When Darpa heard his idea for a neural tourniquet—an electric stimulus to the vagus nerve that stopped bleeding—they were more than interested. They asked Tracey if he could prove it worked. “Nope,” said Tracey. “But Chris Czura can.”
That was good enough for the feds. They signed a check and Czura, a molecular microbiologist who at the time was a researcher and lab manager at the Feinstein, spent the next 15 years working on the system. He and his collaborators started by cutting off the tails of anesthetized mice, sometimes stimulating the vagus nerve before amputation. Just that little jolt of targeted electricity cut blood loss by half. “You could see this stream of blood just suddenly turn off,” says Czura. “It was dramatic.”
Encouraged, he moved on to pigs, using electrodes to apply a few hertz of electricity to the vagus nerve. Then he cut out a small section of each pig’s ear and collected the blood. Stimulating the vagus nerve again reduced bleeding time and blood loss by half.
Confident the zaps were staunching blood flow, Czura moved on to figuring out why. He’d always suspected platelets—blood cells that clump together to form clots and stop bleeding—would be key players. But he didn’t understand how the signal led to enhanced clotting. And he never imagined just how elegant a system it would turn out to be.
When you cut your foot or scrape up an elbow, all the nerves in that area reflexively send a signal up through the peripheral nervous system, the same way your knee kicks if you smack it with a rubber hammer. One of the places that signal goes is the spleen, which Tracey and Czura describe as a “big venous lake.” The stress signal dumps neurotransmitters into the lake, where they bind with any platelets that pass through—which is basically all of them, since they circulate through the spleen every few minutes. The platelets head back out toward the cut or scrape, now primed by the neurotransmitters to clot when they get there.
Czura and Tracey discovered that stimulation supercharged the clotting effect. When the enhanced platelets hit a wound (like a spurting rat tail stump), they formed clots thousands of times better than a plain ol’ platelet. And, importantly, it only happened where clotting was needed. “The excitement to us is that the super-platelet won’t do anything unless it comes across either tissue factor or collagen, which is only present in the blood if there’s a break in a blood vessel,” says Czura. That means they don’t have to worry about random clots, which is how you get strokes and other life-threatening conditions.
Of course, they’ll only know that for sure once they can actually test the device—which stimulates the vagus nerve from the surface of the skin—on humans. They’ll soon get that chance. Last November, a Feinstein spin-outfocused on commercializing the neural tourniquet announced it would partner with the Bill Gates-backed Global Good Fund to test the tech on postpartum hemorrhage. Pending regulatory approval, the trials will start later this year at Northwell Hospital, the largest health care provider in New York. The groups are also planning to field test the device at locations in Africa.
The neural tourniquet is just one of the technologies the Gates Foundation is investing in to fight postpartum hemorrhage. It also helped pay for a six-year, 20,000-women trialfor an inexpensive drug invented in the 1950s called tranexamic acid. In results announced last month, the drug reduced maternal bleeding deaths by a third if it was given within three hours. Because it only takes five minutes to super-charge the clotting process, the neural tourniquet could have the same effect, on top of being potentially useful for trauma.
It also has a slightly different use case—one of the reasons why Darpa was interested in the neural tourniquet in the first place. The effect lasts 24 to 48 hours, so if you have a window of hemorrhage risk, you could stimulate the nerve ahead of time. You know, like a scheduled birth, a surgery … or a combat mission. It’d be like putting on a layer of biological armor, a last line of defense under the tactical vest and camo.
Panelists from the White House Frontiers Conference discuss the new frontiers of space, science, medicine, transportation, and cities. And their must haves for frontier expeditions.
Biology has emerged as one of the most important technology platforms of the 21st century. With the arrival of the gene-editing technology Crispr, biology will soon converge with everyday medicine, big agriculture, and artificial intelligence to influence the future of all life on our planet. Crispr, which allows scientists to edit precise positions on DNA using a bacterial enzyme, is already transforming cancer treatment, preventing the spread of disease, and solving global famine. Its trajectory necessarily involves government agencies and commissions, our elected officials, and the courts—and none of them are prepared for what’s coming.
Amy Webb (@amywebb) is the author of The Signals Are Talking: Why Today’s Fringe Is Tomorrow’s Mainstream and is the chief executive of the Future Today Institute, a strategic foresight and research group in Washington, DC.
This was apparent last July, when I participated in a closed-door meeting coordinated by the State Department and the National Academies of Science, Engineering, and Medicine. In the room were research scientists, government officials and policy wonks with PhDs in the hard sciences, and our task that day was to talk about the future of regulation and oversight, as well as competitiveness in the biosciences and security. It didn’t take long for us to reach a troubling conclusion: The US currently has no coordinated biology strategy. As a result, Crispr, along with other emerging technologies, is developing faster than our government’s ability to address it.
Since that day in July, Americans elected a president whose administration has openly disavowed the scientific method, data-based evidence, and basic research, and a Congress with just one PhD scientist. Now our current government leaders are running away from the future, rather than planning ahead for it.
I spent the entire meeting sitting next to Feng Zhang, who at that time, along with MIT and the Broad Institute, was fighting Jennifer Doudna and her fellow researchers at UC Berkeley for ownership of Crispr patents. He was affable, interesting, and tight-lipped about the lawsuit.
But the lawsuit—at least the existence of it—should have been part of our conversation, and it should now be on the agenda somewhere within the Trump administration. The government doesn’t have a national biology policy, so a patent war has sealed our fate.
Like many groundbreaking scientific developments, the Crispr breakthrough happened as a result of competition and collaboration between expert teams. In this case, Doudna’s contributions were publicly funded, while Zhang’s were done partially under the auspices of a private organization. A few months after our meeting, the fate of those patents and of all future Crispr research was decided by a panel of judges at the US Patent and Trademark Office, whose job is to evaluate new research within the context of past inventions, not to map out scenarios for how a patent decision might affect American business and society. When those judges, who are political appointees, awarded the patents to the Broad Institute, MIT, and Harvard, Zhang’s for-profit startup Editas Medicine ended up with the exclusive license to the most important patent, the one governing all future human therapeutic uses of Crispr.1
Technically, it isn’t the job or requirement of the USPTO to consider the long-ranging downstream effects of Crispr, which gives us the ability to permanently remove or add elements to the human genome. So, instead, I’ll ask: What if someone makes a business case against treating certain genetic diseases in poorer populations? Or to allow wealthy Americans to augment their babies to be 10 percent smarter? What if a security firm like Blackwater wants to engineer its soldiers to have more muscle mass? This technology could save millions of lives worldwide and help eliminate certain awful diseases, but it could also be used in ways that might make you uncomfortable.
What about the future of your genetic privacy? Scientists need to store vast amounts of data for human genomes, and space may fill up by 2025, according to researchers at the University of Illinois at Urbana-Champaign. As the Crispr ecosystem improves, our storage needs will explode along with the computing power and requirements for acquiring, analyzing, encrypting, and safeguarding our genomic data. The Federal Communications Commission eased regulatory requirements on internet privacy rules without thinking through the consequences. What will happen to the privacy of your genome, which will be stored by third parties under terms that are subject to politicking?
Human Embryo Editing Gets the OK—But No Superbabies
Blood Diseases Could Show Crispr’s Potential as Therapy
Scientists Hack a Human Cell and Reprogram It Like a Computer
Crispr can be used to engineer agricultural products like wheat, rice, and animals to withstand the effects of climate change. Seeds can be engineered to produce far greater yields in tiny spaces, while animals can be edited to create triple their usual muscle mass. This could dramatically change global agricultural trade and cause widespread geopolitical destabilization. Or, with advance planning, this technology could help the US forge new alliances.
How comfortable do you feel knowing that there is no group coordinating a national biology strategy in the US, and that a single for-profit company holds a critical mass of intellectual property rights to the future of genomic editing?
While I admire Zhang’s undeniable smarts and creativity, for-profit companies don't have a mandate to balance the tension between commercial interests and what’s good for humanity; there is no mechanism to ensure that they’ll put our longer-term best interests first.
Without a plan, the US is left with the existing democratic instruments of change: patents, regulation, legislation, and lawsuits. And society is trusting our lawmakers, political appointees, and agency heads to apply those instruments to biological technologies that could literally change the future of humanity.
The American public doesn’t need the blunt hammer of the patent office, and any sweeping legislation or series of regulations would doom this technology from achieving its potential. Instead, government needs a new, sophisticated toolset to anticipate and confront what’s on the horizon. This is what we ought to do next.
First, government officials must recognize biology as a technology platform—a difficult request to make of this White House and Congress, which often conflate genetics with divinity. Not all scientists are card-carrying atheists. Religion and science can coexist; it is possible to believe both in God and the scientific method. Talking about biology as a technology platform, using a common lexicon, can help move that conversation away from cherished beliefs toward practical applications.
Second, the US needs a dedicated group of nonpartisan scientists, technologists, ethicists, policy experts, and futurists to develop strategic plans on biology, artificial intelligence, and robotics. Part of their mandate must be to educate our elected officials on emerging science and technology. It can be a reboot of the former Office of Technology Assessment, or a new Department of the Future, but the group should form a connective tissue between legislators and the scientific community.
With that group in place, they’d map out ways to resolve the opposing forces of commerce and collaboration in the public interest. A national biology strategy is a start, but it cannot be the end game. Any plan will need to be continually tweaked and updated, because the future responds to every decision made in the present.
1Correction appended May 17, 2017, 12 pm EDT: This story has been modified to clarify the ownership of Crispr patents.
For all their periodic tables, styrofoam ball-and-pencil models, and mouth-garbling vocabulary, chemists really don't know jack about molecules.
Part of the problem is they can't really control what molecules do. Molecules spin, vibrate, and trade electrons, all of which affect the way they react with other molecules. Of course, scientists know enough about those scaled-up reactions to do things like make concrete, refine gasoline, and brew beer. But if you're trying to use individual molecules as tools, or manipulate them so precisely that you can snap them together like Lego pieces, you need better control. Scientists aren't all the way there yet, but recently scientists at the National Institute of Standards and Technology solved an early challenge: controlling a single molecule's behavior.
Self-Assembling Molecules Like These May Have Sparked Life on Earth
Cement Is Stronger When Its Molecules Are Busted
Watch Theodore Gray Describe His Periodic Table Table
At the very basic level, controlling a molecule would let scientists learn more about it. "This is a long-standing problem," says Dietrich Leibfried, a physicist with NIST's Ion Storage Group in Boulder, Colorado. "Everything around us is made out of molecules, but it's hard to precisely find out about them." And that would have practical applications. For instance, NIST keeps tables of molecular properties that astrophysicists consult when they're reading the spectral signatures of faraway stars and exoplanets. Filling in those blanks would support predictions of whether some exoplanet can support life. With enough control, scientists won't just get a better look at molecules—they'll manipulate matter.
But for now, they are still experimenting. Scientists know how to control atoms using cold vacuum and lasers—so at NIST, scientists' limited molecular control builds on that knowledge. Their research, published yesterday in Nature, describes their experiment: They begin with a vacuum chamber, a 3-inch box containing a tiny electrode, which itself holds a single positively charged calcium atomic ion. Then come the molecules: Ionized hydrogen gas, which the scientists leak into the vacuum chamber until a single H2 reacts with the calcium atom.
Now the ionized atom and the ionized molecule are trapped together. But they're repelled by their positive charges, and the force of the repulsion sends them vibrating—like two magnets when you bring them close. They're also spinning, like a lopsided barbell hurled into the air.
So the scientists set out to freeze the pair in place, again calling on their skills of atomic control. First they fire a low-energy laser at the calcium atom, cooling it and stopping its motion—and because it's coupled to the hydrogen molecule, the hydrogen stops vibrating as well. That's the easy part. The calcium-hydride is still rotating. "That rotation, the spinning along the horizontal or vertical plane, is the hardest thing to control," says Leibfried. Imagine trying to stick Legos together if they were spinning independently. Leibfried and his group do know how to stop, and even alter the spinning. They figured that out last year using lasers tuned to specific frequencies.
All that rigamarole is worthless if you don't know which way the molecule is pointing, though. And if you want to check in on the molecule—by firing another laser—you set it into random motion once again. So instead the NIST scientists fire a teeny tiny laser at the calcium atom, causing it to wiggle. Because it is connected to the hydrogen molecule, it picks up on the molecule's state. And Leibfried and his team can "read" that state by examining the way the laser's light scatters when it encounters the calcium atom. The whole intricate choreography between them lasts about a millisecond, and at the end they can see if the molecule behaved as it was directed.
So what's the point of all that? If you can control with certainty the orientation of a molecule, it's one step closer to sticking them together exactly how you want—no more tossing compounds in a beaker and praying for the right kind of bubbles. Or, to return to the Lego analogy, you can understand—and manipulate—how molecules stick together.
This discovery builds off work done by Leibfried's mentor, Nobel winner David Wineland, who did the foundational atomic control work behind atomic clocks based on single trapped ions. But unlike atomic clocks—which changed the scale at which scientists could measure time, and led to breakthroughs like GPS—this process isn't ready to revolutionize chemistry just yet. Scientists need to fine-tune their control, and have yet to proof the concept on molecules besides hydrogen. Having just one molecule would be like trying to build a city from Legos using only 2×4 bricks.
Your brain is great and all, but it has a serious limitation: You can’t just download new information instantly, like in The Matrix. Robots, however, certainly can. Just imagine a future where they’re hooked up in the cloud—when one of them learns something, they all learn something. Let’s just hope that certain something is nice, like how to give hugs.
The problem, though, is that you can’t just have a little rover learn to grasp something, then expect that knowledge to translate into a hulking bipedal robot. But new research out today from the MIT Computer Science and Artificial Intelligence Laboratory takes a big step toward making such seamless transfers of knowledge a reality. It all begins with a little robot named Optimus and its friend, the famous 6-foot-tall humanoid Atlas.
The researchers started by teaching Optimus—a two-armed robot meant for bomb disposals—how to pull a tube out of another tube. First, they gave it some information about how different objects require different manipulations. Then they held its hand in a sim. “Imagine kind of a videogame where the robot is inside that 3-D world,” says roboticist Claudia Perez-D’Arpino, co-author of the study. “With the mouse you can basically grab the hands and move them around.”
This way, you don’t have to be a gifted coder to be able to command a robot. And it’s all the more intuitive for the operator because it’s a lot like how humans learn: Toddlers have a knowledge base of, say, grasping a binky, but can recontextualize that knowledge of manipulation as they encounter new objects.
Now, how to transfer the robot’s skills to a biped Atlas many times its size? After all, this bot has a new challenge: not falling on its face. “So mathematically that can be written as another series of constraints,” says Perez-D’Arpino, “which if you can imagine is like, keep your center of mass within some region.” Essentially, the operator has to give the new robot some rules, like how to balance correctly, to perform the same task as Optimus. Combine those rules with what Optimus has already learned about manipulating the tubes, and you get a smooth transfer of knowledge. It's not an automatic handoff, to be sure, but it's a start.
At the moment, Atlas can only do the handoff in a simulator. But the development is a glimpse into a future where, more and more, robots communicate without humans at all. They might, for instance, teach themselves to pull tubes out of tubes through a process known as reinforcement learning—essentially trying and trying and trying until they finally get it right.
Imagine the power of this in a factory setting: If one robot learns how to manipulate something more efficiently, it can distribute that knowledge to its comrades through the cloud. And with tweaks like what Perez-D’Arpino has demonstrated, that knowledge might even work with other species of robot as well. Meaning soon enough, robots will think gooder without human help and disseminate those skills freely.
Skills like hugging, right?
Right?
A half-inch-long moth that devours kale, broccoli, and Brussels sprouts may not inspire the same fear as a Zika-carrying mosquito, but the two insects have something in common. Both are being genetically tweaked by UK-based biotech firm Oxitec. Last year, the company made news when it proposed a Florida-based trial of a self-destructive mosquito—designed to stop the spread of Zika virus in native Aedes aegypti mosquitoes.
Now, Oxitec wants to test a genetically modified version of the male diamondback moth to mate with—and eventually destroy—a pest that damages $5 billion worth of cruciferous crops every year worldwide. Like the mosquito, the moth passed laboratory and greenhouse trials and now must pass approval in a open field test. But while the mosquito had to wind its way through the FDA, Oxitec's moth faces a different set of regulatory hurdles at the USDA. Officials at that agency are currently reviewing whether to allow Cornell University and Oxitec to release tens of thousands of GM moths into a 10-acre site in New York.
For decades, agricultural researchers have battled insect pests that are growing resistant to chemical pesticides. The diamondback moth has a short reproductive cycle and lays a lot of eggs, which speeds up the development of resistance. “It is able to select for resistance for a wide variety of toxins,” says University of Georgia entomologist David Riley. “If I had to pick a poster child for resistance worldwide, this would be in my top 10.”
Already, the diamondback moth has become resistant to 95 different chemical compounds. That’s a huge problem for growers in southern states like Georgia and Florida, where up to 15 generations of veggie-munching larvae are born every year on constantly rotating crop fields.
Diamondback moth larvae chow down.
In the past, scientists have released sterile male insects—zapped with gamete-bursting radiation—to help crash the population of pests like the New World screwworm (which infected livestock and people) or the onion maggot. But this would be the first GM sterile pest control effort.
Oxitec scientists made two tweaks to their moth. The first is the killer. “Our diamondback moth carries a self-limiting gene, that prevents female offspring from surviving to adulthood,” says Oxitec research lead Neil Morrison. “This allows us to release adult males only, which are harmless to the crop. When they mate with a pest female in the field, none of her female progeny would survive.” The next generation of larvae won’t have any female moths to mate with—and the population dies off. They also added a gene for a protein that glows red under UV light. Why? That’s so researchers can distinguish the trial moths from wild diamondback moths and see if the GM moths (imported from the UK) are spreading beyond the test site.
USDA officials have already received more than 600 public comments on the GM diamondback moth proposal since the docket opened last month. Backers include ag scientists and biotechnology advocates like Nobel chemistry laureate Richard Roberts, who argues that reducing pesticides will boost food production. “The food security challenge ahead is formidable—especially given the fragile environment—and the innovation pipeline must be enabled to drive sustainable growth in agriculture,” Roberts wrote.
Creating Zika-Proof Mosquitoes Means Rigging Natural Selection
Florida Votes to Release Millions of Zika-Fighting Mosquitos
Save the Galapagos With GMO Rats. What Could Go Wrong?
But other commenters, like Dana Marsh of Bloomington, Ind., worry about the unintended consequences of releasing a GM moth. If a gene drive escapes beyond the targeted area and proliferates widely, it could engineer the destruction of an entire species. “As of today, we do not have complete understanding of how every insect and plant works,” Marsh wrote. “We still do not grasp the diversity that currently exists. The consequences of genetic engineering are untold and could prove fatal to our precious ecosystem. PLEASE leave nature alone.”
The company says any GM moths that survive the experiment will be killed by pesticides or the region’s cold winters. Still, some local organic farmers say the dead larvae produced when genetically engineered males mate with female moths could stay on the vegetables and pose potential risks to the environment and health. “The release of any novel organism into the open is a significant issue,” Liana Hoodes, policy adviser for the Northeast Organic Farming Association of New York told Bloomberg BNA. “Clearly, insects don’t stay in the borders of the land that’s owned by Cornell.”
USDA officials are accepting comments until May 19. Oxitec said if its trial is approved, the trial would take less than a month to complete—though that timeline might be ambitious. Oxitec’s genetically engineered mosquito has been released in parts of Grand Cayman, Panama, and Brazil—places where Zika is a big threat. But even though the Florida trial was approved by residents of Monroe County, those plans have stalled: No single neighborhood wants to be the site of the mosquito release. The same may be true for a 10-acre plot in upstate New York.
Scientists in California are breeding and releasing mosquitos into Zika hotspots. While it may seem like they're making matters worse, they are actually releasing a kind of biological trojan horse.
The Cassini space probe is going to dive through Saturn's rings again on Wednesday, the third of a planned 22 orbits threading that planetary needle as the probe continues a ballistic death-drop inward. And like the first ring-crossing two weeks ago, this one required a bit of complicated piloting. Remote-controlling a robot spaceship from 750 million miles away ain't like dusting crops, as Han Solo might say. (RIP.) (Spoilers.)
Cassini's first dramatic pass through the rings of Saturn on April 26 involved some acrobatics. Step one: Get a gravity boost from the moon Titan. In fact, that's how Cassini has been moving around the system since its arrival in 2004—rather than burn precious propellant, the craft tucks into Titan's orbit and then slingshots back out again. This most recent boost was a delicate one, just 609 miles above Titan's surface and not even ten miles above the moon's wan atmosphere. Space, as we keep telling you, is hard.
"Any gravity assist that we do is never going to be perfect, because we can’t model everything perfectly," says Sonia Hernandez, a mission design engineer for Cassini at the Jet Propulsion Laboratory. "Two days after the last Titan flyby we did, we performed a maneuver to put Cassini on its trajectory again. It was very tiny."
How tiny? First, the spacecraft team put Cassini's three reaction wheels to work. They're spinning discs that, when their rotation slows, cause the entire spacecraft to move around the flywheel's axis. They're a way to change the probe's orientation without burning fuel. That's how the spacecraft team points the Reaction Control System thrusters—smaller than the main engine and powered by hydrazine—in the right direction.
Then Hernandez's navigation team sent the signal to fire the RCS thrusters for just 177 seconds, at 155 millimeters per second. In other words, the burn moved the craft in a different direction by about 80 feet.
Cassini Is Ready to Sacrifice Itself for the Good of the Solar System
Here Are the Best Shots From Cassini’s Flybys of Enceladus
Cassini Spies Mysterious Object Named ‘Peggy’ at Edge of Saturn’s Rings
Here's a complicated bit: Even though that first maneuver, carefully calculated weeks in advance, happened before the first ring crossing, the team did it to correct the orbit on the third crossing—the one happening Wednesday. That's how these loop-the-loops work: Small problems turn into big ones unless you deal with them early.
And on Wednesday they're going to do it again—to make sure dive number 13 is perfect. "We're going so close to the atmosphere and the rings, there's all these perturbations," Hernandez says. So this one will be even more delicate.
Cassini's high-gain antenna starts out pointed directly at Earth—good for receiving all these commands. Using the reaction wheels again, they'll make two careful turns over the course of about 45 minutes. It can take upwards of an hour and a half for commands to reach Cassini, depending on where it is in orbit, so these sequences are all pre-set. By the end, the antenna will be turned about 120 degrees away from Earth.
Then, says Joan Stupik, of Cassini's guidance and control team, they'll fire the RCS thrusters again. This time it's just 22 mm per second for 24 seconds. It's precision flying to make sure the science team gets the exact data it wants—pretty pictures, sure, but also new information about the size of the particles in the rings, what they're made of, and where that material comes from. (The geyser-y moon Enceladus is a contributor.)
Cassini'll keep circling Saturn until September 15—NASA's “Grand Finale”—when the little probe will dive into the planet. "On the last route that we do around Saturn, we actually encounter Titan one last time," says Hernandez. "It’ll give us a tiny push, a goodbye kiss, and that’s going give us our final push into the atmosphere." Cassini will collect data all the way down, of course.
Twenty years ago, the Cassini spacecraft blasted off from earth on an epic journey to find out more about Saturn. Now that journey comes to a glorious end.
No three strung-together letters in the English language are more loaded than L-S-D. Say them out loud to elicit images of strung out hippies waving their hands around and making things out of flowers, or of an innocent youth’s mind snapping under the weight of acid. Or just really bad art.
Psychedelics have a brand problem, but early studies suggest drugs like LSD and MDMA could treat disorders like post-traumatic stress disorder. Operative word being could. Bad branding means bad funding, so while those preliminary studies are promising, they’re also relatively rare.
Which is why today an organization called Fundamental is launching a crowdfunding campaign to finance an ambitious series of studies—designed under the watchful eye of the FDA, mind you—into how psychedelics might treat a range of psychological disorders. So should you be inclined, you can kick in cash to fund what is shaping up to be a bold and bizarre new frontier in medical research.
Fundamental came from the brain of Rodrigo Niño, a real estate developer in New York who in 2011 was diagnosed with melanoma. Following two surgeries, Niño—understandably terrified of death—traveled to the Peruvian jungle to try ayahuasca, a powerful psychedelic famed both for its violent upheaval of the human digestive system and its tendency to take users on intense spiritual journeys. (Not exactly the most data-driven beginning to a psychedelic science campaign, but there it is.)
“Right after my first session—my ceremony, they call it—I was completely off my fear of dying,” Niño says. “Completely gone, you know. And then I had to know if what had happened to me was placebo effect caused by the hallucinations, or if in fact I had been physiologically cured.”
How Ecstasy, Aspirin, and LSD Look Under the Microscope
How an Army of Deadheads (And Their LSD) Invented Silicon Valley
Inside the LSD Museum That the DEA Somehow Hasn’t Torn to the Ground
Problem is, you can’t just call up the federal government and ask for some money, pretty please, to test a schedule 1 drug on people. And good luck getting pharmaceutical companies interested in natural drugs they can’t slap a patent on. “The issue is that [psychedelics] don't make money, and because they don't make money traditional capital sources have no interest in them,” says Niño. And so Niño founded Fundamental to take psychedelics research to the people.
It works like this: Anyone can donate money through the fund-raising website CrowdRise, specifying what kind of psychedelics research they want to fund. This money lands in a fund operated by a grantmaking organization called Charities Aid Foundation America that then vets which researchers it doles out the money to. Niño's aiming for $2 million initially, with the possibility of additional campaigns in the future.
One of the first beneficiaries of the fund will be Amanda Feilding, a legendary figure in the psychedelics movement and, as it happens, a full-blown countess with the most proper British accent you ever did hear. The UN made a terrible mistake, she says, when in 1961 it passed the Single Convention on Narcotic Drugs, essentially Just Say No to Drugs in treaty form. "What we've been trying to do for the last 20 years," Feilding says, "is provide governments and the UN with the scientific evidence so that they can amend or withdraw the conventions prohibiting these substances or lower them from schedule 1 to schedule 3 or 4 so that doctors can prescribe them and research can be done." To do that, though, she's relied on donations from individuals or grants from other institutions.
The money she receives through crowdfunding will go toward studying LSD microdosing, which you've no doubt heard of by now, with (deep breath) neuropsychopharmacologist David Nutt at Imperial College London. In it, they'll have subjects complete certain tasks while in an fMRI to image their brains. Everyone from Silicon Valley techies to creative-industry types love the idea that in low doses the drug could heighten alertness and creativity without the pesky hallucinations. Science will sort that out, but in a study published last year, Feilding and partners gave the world the first look at how LSD affects the brain (itself financed in part with crowdfunding). Meaning researchers are taking the first steps toward understanding how LSD and other psychedelics impact the mind.
Another beneficiary of the crowdfunded cash will be Rick Doblin, executive director of the Multidisciplinary Association for Psychedelic Studies. Over the last three decades, MAPS has raised some $40 million for research into the therapeutic potential of psychedelics. But it's not enough—phase three of Doblin's study into using MDMA to treat PTSD will set the group back $25 million ($10 million of which they've pulled in from two overachieving donors). And they're not expecting much help from the government—though they did once get a $2.1 million grant from the state of Colorado to study PTSD with marijuana.
This isn't MAPS's first tango with crowdfunding, either. It has used Indiegogo to fund a psychedelic harm reduction program at Burning Man, and again for a study that tested MDMA on traumatized veterans. But those campaigns were asking for total commitments of tens of thousands of dollars, not millions.
With its cut of this new, larger round of crowdfunding, MAPS plans to bring sufferers into a clinic for three sessions of supervised dosing, after which the patient stays for the night. This is combined with 12 hour-and-a-half-long psychotherapy sessions. In a similar study published by the group in 2013, researchers found that doses of MDMA helped participants improve their PTSD symptoms long-term.
Contrary to what you might expect for a schedule 1 drug, the issues with MDMA research, Doblin says, aren’t regulation but funding and training therapists in a novel form of treatment. (To train for this, the FDA is allowing MAPS's therapists to try MDMA themselves.) MDMA is widely available for research purposes, and indeed the stigma of psychedelics is fading. “What’s really changed over the last 10 years has been the willingness of major researchers at major institutions to get involved,” says Doblin. Psychedelics are no longer fringe—none other than Johns Hopkins is in on the game now. “So the real issues now are not regulatory.”
Money. They need money, because drugs are expensive and rigorous scientific studies are complicated, no matter what plane of reality you occupy.
Welcome to the Institute of Illegal Images, San Francisco's museum of LSD blotter paper art.
Fidget spinners are the new Rubik's Cube. Or maybe the new Tamagotchi. Or ... I don't know. Pick your fad. You see these toys, ostensibly designed to help kids fidgety concentrate, everywhere now. Seriously. Everywhere. A fidget spinner is basically a small bearing mounted in a piece of plastic or other material. You hold it and spin it. I guess it's sort of amusing.
Now for my question: How long will it continue to spin? Of course, this depends upon the starting angular velocity. Now, to find the angular velocity ... oh, wait. Perhaps I should first discuss basic rotational kinematics. Suppose I have a rotating object of some kind. Like, say, a bicycle wheel. I can determine the angular position at any point. I will call this θ.

If the object continues to rotate such that θ changes, I can describe the rate of this change as angular velocity using the symbol ω. I define the average angular velocity as:

Yes, that looks a lot like the definition of linear velocity. But what if that spinning object is slowing down (or speeding up)? The change in angular velocity can be described by the angular acceleration with the symbol α.

If I know the starting angular speed and I assume a final angular speed of zero radians per second, I can calculate the spin time:

All I need is the angular acceleration—assuming it remains constant as the spinner slows. I could calculate the angular acceleration based on the change in angular velocity, but this isn't so simple to measure. The spinner moves too quickly to get a good video of its motion, so I will use a laser in a rig I built to measure the change in the angular velocity.

Basically, the laser shines down onto a light sensor. As the spinner spins, it occasionally blocks the sensor, interrupting the laser. By measuring the values from the light sensor, I determine the spin rate. But this creates a couple of problems. First, the light change rate and the rotation rate differ because the three "lobes" in the spinner create multiple openings during each rotation. Second, the spinner will spin for a significant amount of time such that it would be difficult to analyze it all at once.
This is what a part of that light data looks like.
Now for the fun trick. Instead of looking at a giant plot of light vs. time (the full data is over 2 minutes), I will plot the Fourier transform of this data. What does that even mean? Well, suppose this data is made of many different trigonometry functions (like sine and cosine) with different frequencies (which it essentially is). These trig functions have different amplitudes. So, the Fourier transform shows the amplitude of different frequencies, so you can determine the frequency of oscillation.
If I select a small portion of the laser data at the beginning of the run, I get the following Fourier transform:

That giant peak at 20.14 Hz is the frequency of the spinning spinner (at least during this time interval at the beginning of the run). The other peaks on the Fourier transform occur because of the multiple laser openings in the spinner—just ignore these for now. To determine the angular velocity, I simply multiply this frequency by 2π and I get 126.54 radians/second.
What if I create a Fourier transform for the entire data set? I would get a wide peak for the rotation of the spinner since the rotation rate decreases—that wouldn't be very helpful. Instead, I will take small sections of the data and find the angular velocity. By using a small data set, the angular velocity is mostly constant. Then I can make a plot of angular velocity as a function of time:
The slope of this line is the angular acceleration with a value of -1.346 rad/s2 and since the data looks fairly linear, the angular acceleration is mostly constant. Now, I can find out how long the spinner will spin. With a starting angular velocity of 140 rad/s (a little bit faster than the example data above), it would spin for 104 seconds. If you want it to spin even longer, then just spin it faster. Doubling the starting angular speed will double the time. That is your answer.
Charlotte Drury, Maggie Nichols, and Aly Raisman talk to WIRED about the skill, precision, and control they employ when performing various Gymnastic moves and when training for the Olympics.
The polyphagous shot hole borer, a brown-black beetle from southeast Asia, never gets bigger than a tenth of an inch. It breeds inside trees; pregnant females drill into trunks to create networks of tunnels where they lay their eggs. The beetles also carry a fungus called Fusarium; it infects the tunnels, and when the eggs hatch, the borer larvae eat the fungus.
Unfortunately Fusarium also disrupts the trees’ ability to transport nutrients and water. Holes where the beetle bored into the tree get infected and form oily lesions. Sometimes sugars from the tree’s sap accumulate in a ring around the hole—that’s called a “sugar volcano.” The tree dies, and the wee baby beetles fly off to continue the circle of disgusting life.
This would just be a scary story for arborists and tree-huggers, except: Fusarium dieback is on track to kill 26.8 million trees across Southern California in the next few years, almost 40 percent of the trees from Los Angeles to the Nevada border and south to Mexico. That’s more than just an aesthetic tragedy. It means that thousands of human beings are going to die, too.
I’m not just being a monkeywrenching fearmonger. Dead trees mean dead people, and scientists are finally starting to figure out why. In the 1990s, spurred by a program to plant half a million trees in Chicago, researchers started trying to quantify the value of a tree beyond the fact that one is, like, at least slightly more lovely than a poem. It’s a field of study today called ecosystem services. “I’ve been trying to quantify the impacts of trees on rainfall interception, pollutants in the atmosphere, cooling and energy used by buildings, CO2 stored and emitted,” says Greg McPherson, a research forester with the US Forest Service who conducted the latest study of SoCal’s trees. “But I think those are the tip of the iceberg.”
Dire Glimpses of What Pollution Is Doing in Bangladesh
Stunning Images Show the Earth’s Imperiled Water
One Scientist’s Crazy Bet to Save the Bees: Join Monsanto
And at the base? Public health impacts—and differences in illness and death in populations that live near greenery versus those that don’t. It’s only been in the past few years that anyone has been willing to go out on a limb and associate morbidity and mortality numbers with nature. Oh, sure, everyone agrees that trees pull particulate-matter pollution out of city air. Simply by dint of being shady, trees reduce the “urban heat island” effect that drives people to run their AC all the time, a contributor to climate change. And, yes, trees inhale carbon dioxide, another win for the climate.
But fighting disease is a whole other question. What is a “dose” of nature? What’s the response curve? By what mechanism would a walk in the park alleviate, let’s say, heart disease? Is it the park? Or the walk? (Some Japanese researchers think trees literally emit life-giving chemicals, like that weird M. Night Shyamalan movie where trees kill people, but in reverse. No, wait, that’d be people killing trees, which actually happens. The converse, then.)
Whether the mechanism is stress reduction, pollution reduction, or increased physical activity, somehow trees make a difference. The biophysics is less important than the epidemiology. In 2013 another researcher with the US Forest Service named Geoff Donovan took advantage of the fact that another beetle, the emerald ash borer, killed 100 million trees across 15 states in the US. Using statistical models to rule out the impacts of a whole bunch of other potentially confounding factors—race, education, income—Donovan’s team was able to connect illness with places that had ash borer infestations and concomitant loss in tree cover (which you can see in satellite imagery).
His result: Counties with borers had 6.8 additional deaths per year per 100,000 adults from respiratory disease, and 16.7 deaths from cardiovascular disease. Over the arc of the paper, that means 100 million dead trees—roughly 3 percent of tree cover on average—killed 21,193 people. “The implicit thing I’m saying here is that if you either kept the trees or increased the amount, you’d get the opposite effect,” says Donovan, now on a sabbatical at Massey University’s Center for Public Health Research in New Zealand. “I don’t think it’s the worst assumption in the world.”
Donovan isn’t the only one on the case. A 2015 meta-analysis of the few studies that had tried to take up the issue showed that higher exposures to green space, even controlling for things like poverty and education level, indeed resulted in a statistically significant reduction in death from cardiovascular disease. Other outcomes, like higher-birthweight babies and lower rates of antidepressant prescriptions, have also shown up in the literature.
That means that if Southern California doesn't somehow stave off the loss of 11 percent of its tree cover, that loss is going to be deadly over time. “It’d probably be unwise to try and just turn the crank and say, ‘That’s going to be X thousand people,’” Donovan says. But the risk isn’t one of overstatement. Southern California has a much higher population density than the area he studied. “You might anticipate a major public health impact.”
That’s what McPherson is worried about, too. He was collecting data on California trees and Fusarium dieback for a journal article when he met John Kabashima, an entomologist working for the University of California on the Fusarium problem—an invasive pest that wasn’t jeopardizing crops but landscape. Kabashima realized that McPherson’s data might be what he needed to get some bureaucratic attention. What McPherson had come up with was, as he says, “the first statewide assessment for California, and probably the first nationally to combine satellite data and field plot data, and to incorporate the benefits and services of trees.” By his count, if the beetles spread as widely as he’s predicting, it could cost $1.4 billion in lost ecosystem service benefits—not counting the public health cost.
The next step will be figuring out what to do about the bugs. “A normal response to an invasive pest means millions of dollars would be thrown at it,” Kabashima says. “This one has received hundreds of thousands.” The people he’s working with at least know that it’s not enough to cut down an infected tree. If you don’t chip it, the beetles inside survive to infect another host. And the little holes and sugar volcanoes tend to show up first1 on the north side of the trunk or limb. “You have to get out and walk around each tree, which we’re doing in Orange County parks,” Kabashima says. “We go out on off-road Segways. We can cover square miles in a day.”
Meanwhile, all over the state, McPherson and other forestry researchers are looking for new species of trees to replace the ones sure to be lost. Resistance to shot borers and Fusarium won’t be the only criteria. “We developed a five-step process for identifying promising trees, scoring them on factors like drought tolerance, salinity tolerance, invasiveness,” McPherson says. Even characteristics like root depth might be important—deeper roots mean less destruction of sidewalks. “We’ve narrowed it down to 12 new species for coastal Southern California and 12 for the inland.”
The problem is, it takes a lot longer to grow experimental tree species and see if they’re up to spec than it does for drought, polyphagous shot borers, and fungus to do their work. The race is on—and not for all the usual reasons. “We don’t think of trees as something essential to our urban infrastructure, like roads or sewers. In fact, we see them as something that can interfere with those things,” Donovan says. “But health benefits are where it’s at. Trees are an essential part of our public health infrastructure.” If you believe that the ballpark value of a statistical human life, stated most coldly, is around $7 million, then the potential of tens of thousands of additional lives lost makes the cost of saving trees, and getting healthier ones planted, a bargain.
1UPDATE 5/9/17 10:20 AM Corrected to more accurately reflect the progress of polyphagous shot hole borer infestation.
Most people think of carbon dioxide as a poison, but in nature it’s a building block. Find out how we can imitate coral reef by using CO2 as a raw material for the creation of concrete.
Brian Greene is one of those physicists. You know the type: Blessed with a brain capable of untangling the mysteries of the universe, and a knack for clearly explaining it all to the rest of us schlubs.
His enthusiasm for doing these things keeps him quite busy, what with the three best-selling physics books for grown-ups, a children's book about time dilation(!), a few TV specials, and, of course, a TED talk. Oh, and he and his wife have, since 2008, spearheaded an annual science-themed takeover of New York. The World Science Festival runs from May 30 to June 4, with talks, performances, and interactive events in all five boroughs.
An ambitious schedule, to be sure. Opening night includes a performance of Greene's next book (working title: Until the End of Time) exploring humanity's place in the unfolding universe. Other events explore how the brain works, and how to use the scientific method in the kitchen. A biologist will lead a sailboat cruse of New York Harbor, and Mario Livio will set up some barrel-sized telescopes in a bid to disprove every New Yorker's fervent believe that you can't stargaze in the city.
Greene hopes the festival both sates and stirs the public's appetite for science. No easy feat these days, when politics has shifted science from something people do to something they march for, argue over, and believe in. I gave Greene a call to ask how how the joy of science, and the thrill of figuring things out, might prevail when it seems science is under siege.
Given the debates over science right now, I'd like to start by asking a basic question: What is science?
Science is our most powerful tool for evaluating what’s true in the world. It’s a perspective on reality that allows you to grasp what’s right and what’s not. And, in the best of cases, use that knowledge to manipulate and control the world to the betterment of everyone.
What area of science, besides physics, is most exciting to you right now?
Research on the brain, absolutely. Neuroscience is astoundingly deep and compelling. I like to organize the big mysteries into three categories: You have the origin of universe, the origin of life, and the origin of consciousness. And that last one, what conscious is and means, is deepest of all.
How do you feel about labels like pro-science and anti-science becoming political epithets?
It’s deeply unfortunate, because fundamentally science is not a partisan issue. The facts of how the world and universe are put together transcend party lines. But we’ve come to a very strange place in American democracy where there’s an assault on some of the features of reality that one would have thought, just a couple years ago, were beyond debate discussion or argument. Now, I’m not naive as some of my colleagues, who say we need to have a government run on the principles of science. There's an art to governing that figures out how to balance conflicting desires with limited resources that science can't solve. But, there shouldn't be a disagreement on basic facts.
Do you think anything positive will come of this?
Well there was a time, and still some scientists feel strongly this way, when scientists had nothing to do with the political arena. I think in best possible universe we would love if that made sense, but in ours it doesn't. Scientists need to stand up, and they have. Marches are step in good direction. And scientists are feeling encouraged to run for office. I think it's a good step forward that the most rational, sensible, and rigorously-thinking individuals are injecting themselves into that arena.
Why do you think it's possible for the public to accept some scientific facts but vehemently deny others. For instance, physicists said the gravitational ripple detected by LIGO was caused by two black holes colliding 1.3 billion years ago. Nobody witnessed that, it was an extrapolation of data and known physics. Climate change is also an extrapolation of data and known physics (and chemistry), but is vehemently debated.
Well, it’s an interesting question. I can give two answers. The most straightforward is that if two black holes colliding had a radical impact on public policy, then maybe more people would be debating the science. My other answer is that I suspect many of the people who don’t accept the scientific basis of climate change probably haven’t thought a lot about things like gravitational waves, or inflationary cosmology.
How would you go about decoupling politics from science?
I am an optimist, and I do like to think if we could just grab hold of the educational system and showed kids and teens power of science, how it's a potent and powerful tool to unravel grand mysteries, and reveal the workings of everyday life, then it wouldn’t matter whether they grew up to be Democrats, Republicans, liberals, or conservatives. Where they were on the political spectrum would of course still bear weight on how they understood the implications of science, but it wouldn't become an attack on understanding itself.
Political debates don't just occur beyond science. You may be the most famous proponent of string theory, which has had its ups and downs in terms public acceptance. What's your view of how string theory and supersymmetry has been treated by both physicists, and the media, in the past decade?
It’s a very interesting question, and it speaks to the fact that even science itself is a discipline pursued by flesh and blood human beings. In this particular case, any self-respecting string theorist always points out that these ideas are hypothetical, and there are a number of questions they cannot answer. And without data we don't know if they can be right or now. But, in a curious way, a handful of non-string theorists made it appear that somehow we didn't want people to know these things. I think that wound up coloring public perception that string theorists were hiding something. In terms of the field of string theory itself, it's not so much as it's gone through ups and downs, but that there have been more, and then less, exciting times in terms of new discoveries and understandings.
Finally, I'd like to ask where you stand on one of the most important scientific questions of our time. Pluto: Planet or dwarf planet?
You know, in my heart of hearts, Pluto is a planet. But, thinking as a scientist, I do recognize that if you make Pluto a planet, then consistency demands you have to include a bunch of other objects of similar size and characteristics. It's just that, when you are brought up with Pluto being a planet, or brought up with any certain way of thinking, it can be hard to let that go. That's kind of the same challenge that America is having with everything right now, isn't it?
I did it. On Saturday afternoon, a few hours after Eliud Kipchoge ran a stunning, historic marathon in 2 hours and 25 seconds in Monza, Italy—narrowly missing his goal of breaking the two-hour marathon mark for Nike's Breaking2 initiative, but obliterating the current world record and everybody’s idea of what is possible in the sport—I ran a half-marathon on the same course in 1:26:52. My time was more than three minutes beneath my goal of 90 minutes, and almost exactly 10 minutes quicker than my personal best for the half-marathon, which I set last November in Lancaster, England. After crossing the line, I celebrated by collapsing on the tarmac, and closing my eyes for what seemed like a long time.
The curious thing is, my initial burst of endorphin-rich joy at finishing the race in a previously unthinkable time has given way to more complicated emotions. Six months ago, I would have thought I would be ecstatic to run so quickly. Back in December, 90 minutes for a half-marathon—13.1 consecutive miles at six minutes 51 seconds per mile—seemed impossibly fast. Now I’ve not only beaten 90 minutes—I’ve demolished it. So why did I find myself, hours after the race, in a bland hotel room in central Milan, with a tear running down my cheek? Exhaustion is part of the answer, but not all of it.
Breaking2
———
This is the last in a series chronicling Nike's attempt to run a two-hour marathon.
On Saturday, I ran with four friends, all of whom are much more accomplished runners than I: Knox Robinson, a running coach from New York; Mike Doyle, the editor of Canadian Running magazine; Alex Hutchinson, a writer for an American running magazine; and, for the first three or four miles, Becs Gentry, a Nike coach from London who had guided me through the early part of what I took to calling my Breaking90 training program. None of us had enjoyed perfect preparation: we’d all been up since three in the morning to cover the Breaking2 attempt, and had spent nine hours on our feet, subsisting on pastries and too many espressos. Every one of us was tired before the starting hooter blared at 12:45 pm.
We ran behind the same pace car that led Kipchoge. The display mounted on top of the car spat out our split times as we crossed timing mats around the one-and-a-half-mile junior circuit at Monza. We wanted to run at a pace of four minutes and 10 seconds per kilometer (or six minutes and 43 seconds per mile). And that, pretty much, is how we started. In fact, we went a little quicker than goal pace for a while. But within half an hour, the race got hard, and I began to feel occasional discomfort in my hips and quads. Maybe I was a little tight from work, and travel.
The weather, meanwhile, was overcast—in the mid-60s Fahrenheit when we started (15 or so degrees hotter than optimal marathon conditions), with over 70 per cent humidity. It could have been worse. A sunny May afternoon in Italy could have been a furnace, and a sub-90 half-marathon would have been extremely difficult for me in such heat. But still, the temperature didn’t make it easy, and I sympathized with Kipchoge having to run in that humidity. There is something invigorating about gulping clean, dry air.
At around five miles into the race, rain began to pour, rattling the roofs of the empty stands like coins dropping into a money box. We were all drenched. My shirt began to cling uncomfortably to me; eventually, I decided I’d be happier shirtless. Puddles began to form on the track. My shoes squelched as water pooled around my toes. Knox made a joke about how this was the perfect weather for a guy from Manchester—where, famously, it rains more than it shines. But I lost my flow, and my split times began to drift dangerously.
Knox sensed my discomfort, and began to talk to me. “Let’s just get a little rhythm back here,” he said.
Why You’ll Never Run a Sub 2 Hour Marathon—But the Pros Might
Nike’s Controversial New Shoes Made Me Run Faster
The Secret to Training for a Marathon: Just Keep Running
I tucked in behind Knox, Mike, and Alex, and I tried simultaneously to keep my strides light and forget I was running. It worked: My splits regained their regularity. However, some of the thoughts that crowded my head were overwhelming. I have read that, high on mountains, in their tents at night, many solo alpinists recall long conversations with lost or imaginary friends. Runners don't generally hallucinate, unless they are irredeemably exhausted, but serious effort in a race can dredge up submerged parts of your psyche. (Later on Saturday evening, over a celebratory glass of wine, Mike Doyle told me he experienced similarly crystalline and profound thoughts when he was 20 miles deep, and hurting, in the Philadelphia Marathon.)
For instance, after taking a water bottle around an hour into the race, I recalled one of my last training sessions the week before the attempt. I had run laps around a lake near my house, and my four-year-old son Rory had worked as my assistant, gleefully handing me my water bottle as I came around each lap. For the past six months, he has been more excited about what he has called my “special race” even than I. (On Saturday morning, I had choked up as I watched a video my wife had made for me, featuring Rory and his baby sister Annabel wishing me luck.) My excuses for failure—the weather, my tiredness—were real. I could explain all that to my wife, who would understand. But the thought of returning home and reporting my failure to Rory was painful—more painful than whatever was happening to my own body.
Later in the race, a much older memory sprang me like a mugger. I thought of my oldest brother, Ben, telling me one day in 2006, outside my mother’s house in South London, that he thought Dad would have been proud of me. My father, also called Ben, had died in a helicopter accident when I was two years old, and my older brothers were eight and ten; you can, perhaps, imagine the effect of that tragic event on our family. I don’t know why, but on a Formula One racing track in Italy, naked to the waist, heart pounding at 169 beats per minute, legs and lungs urging me to stop, that simple, beautiful memory from more than a decade ago came to sustain me.
With less than two laps remaining of my race and my splits still on target, I knew I was going to break 90 minutes. That joyful feeling was accompanied by a moment of serendipity. As we were rounding the curve for our last circuit of the course, the church bells of the Oratorio San Luigi Gonzaga in Vedano al Lambro, a village just outside the track at Monza, began to toll. It was as if God himself were ringing the bell for our final lap. We all broke into broad smiles, and our pace began to quicken.
In the final straightaway, encouraged by my pals to leave nothing on the course, to expend every last ounce of energy, I raised my knees and pumped for home. Cheered on by a group of Nike employees and sports scientists — many of whom had not slept in two days, all of whom had worked on Breaking2 for months, and who all should have been sound asleep — I ran my final mile in six minutes and 12 seconds, and fell to the ground.
Much will be written, not least by me in an upcoming story for WIRED, about the science and technical acumen of the Breaking2 attempt, which my own smaller Breaking90 effort mirrored. Certainly, Kipchoge benefited from advanced and meticulous thinking about shoe design, nutrition, and race structure. I, too, was helped by the Nike team’s unobstructive and wise counsel about how and when to train, eat, and rest. Sports science, without doubt, took me to a place I didn’t think I could reach.
Science, however, can’t unlock the magic of running entirely. Kipchoge has qualities that cannot be recorded on a treadmill. Likewise, the effect of running cannot be understood only in numbers.
Before I engaged in this project, I already loved professional marathon running as a writer and observer. I spent three years writing a book about it. But I have only recently experienced serious running as a participant. As I have done so, I’ve discovered a sport that I cherish not only for the benefits it has given my body, but for the kinship I have felt with other runners and—most important—the way it has nourished my mind, and my soul. Measure that.
Eliud Kipchoge fell short of running a marathon in under two hours. In a special Nike event, the long distance runner beat the current marathon world record coming in at 2 hours and 25 seconds, but because the event wasn't sanctioned it won't count. Here's how Nike controlled every variable in an attempt to break two.
MINNEAPOLIS — On the first Saturday this May, residents of the Cedar-Riverside neighborhood socialized in the shadows of their towering public housing complexes. Chattering men sat for haircuts while head-scarved women shopped the stalls lining the blue-bricked Riverside Mall. At nearby Currie Park, training-wheeled biker boys chased each other, circling a drained wading pool where girls played a game of tag. Their relatives picnicked in the shade.
Not so visible was the fact that this Somali-American community—Minnesota's largest—is currently experiencing the state's worst measles outbreak in nearly three decades. On Thursday, the count rose to 44 confirmed cases across three counties. Public health officials are now scrambling to keep the disease from reaching pockets of unvaccinated kids in other parts of the state. This means advising accelerated shot schedules to doctors and parents, and considering quarantine orders for anyone who has been exposed. The health officials know that once a measles outbreak gets going, it can be very difficult to contain, even if they expected it.
And they totally expected it. Over the last decade, anti-vaxxers have fortified this corner of Minneapolis into a bastion for pseudo-science. It all began with higher-than-normal rates of severe autism in the Somali community. And when state and university researchers failed to understand why the disorder hit so hard here, families went looking for answers elsewhere: friends, and the all-knowing internet. In came the anti-vax partisans, whose success with these frightened parents has turned the neighborhood into a beachhead for what should be a preventable disease.
How Panicked Parents Skipping Shots Endanger Us All
Why Did Vaccinated People Get Measles at Disneyland? Blame the Unvaccinated
How to Get Silicon Valley’s Anti-Vaxxers to Change Their Minds
Measles kills about 10,000 children a year in Somalia. So when the first wave of refugees from that country's civil war brought their kids to Minneapolis in the 1990s, vaccination rates were high—around 90 percent by 2004. Four years later, Somali parents began noticing a rising number of autistic children in their community. They cried out to city officials, who enlisted researchers from the University of Minnesota, CDC, and NIH to launch an investigation. Those epidemiologists found that autism rates in Minneapolis’s Somali community were higher than the national average. But the rate was identical to autism in Minneapolis's white population.
What was different was that Somali children didn't just get autism, they consistently got the most extreme version of it. By comparison, only about a third of non-Somali kids with autism were also diagnosed with intellectual disabilities—such as delayed speech or difficulty understanding abstract concepts and social rules. For Somali autistics, it was 100 percent.
“In some ways that finding was very consistent with what we were hearing from the community—that autism hits Somali kids much harder,” says the study’s lead author, Amy Hewitt. But her results only told people what they already suspected, not what they were looking for: What was causing the autism.
Not that it would have mattered much if she had. By the time Hewitt’s study was published in 2013, anti-vaccination activists had already claimed Cedar-Riverside. Andrew Wakefield—the study-falsifying founder of the modern anti-vaxx movement—met with upset Somali families three times between 2010 and 2011. And Minneapolis health officials say it’s not hard to draw a line between this targeted misinformation campaign and the current measles outbreak. Kris Ehresmann, director of the health department's Infectious Disease Division, points to the precipitous drop in vaccination rates in recent years: Today, only four out of every 10 Somali kids in the city are protected against the disease. Even as recently as last Sunday, groups like the Vaccine Safety Council for Minnesota organized a public meeting to help Somali families understand their legal rights to refuse the same vaccinations public health officials are anxiously urging them to get.
“The answer used to be education—the more educated you were on the issue the more likely you were to get vaccinated,” says Michael Osterholm, director of the University of Minnesota’s Center for Infectious Disease Research. Those days are over, he says. In no small part thanks to the proliferation of misleading information on the web. “The challenge is for scientists to be humble and acknowledge that in this day and age facts will not win the day,” he says.
Minnesota’s Health Department has been trying to take a page from that playbook. It has changed up its tactics in the past few years—hiring Somali nurses and outreach workers to pave inroads into the community, and working with imams and other leaders to create a network of trusted individuals who can combat internet-fueled fears. And they’ve begun taking up alternative communication strategies. Rather than use bullet-pointed barrages of facts, they illustrate their points with stories about, say, someone whose child came down with measles.
Catherine Mary Healy, the director of the Center for Vaccine Awareness and Research at Texas Children's Hospital, says this is a better strategy for communities that haven’t been exposed to an outbreak of vaccine-preventable diseases. “When they come across misinformation, they interpret it in a different way than you or I might because they’re unable to weight the risk/benefit ratio appropriately,” she says. Healy works with many Latino immigrant groups in Texas, where she says vaccination efforts are highly successful because so many of them have experienced measles firsthand.
This echoes what officials around Minnesota have been clear about from the beginning of the outbreak: This isn’t an immigrant problem or a Somali problem, it’s an unvaccinated problem. But there is reason to be hopeful that evidence-based information can still win the day. Research shows that the most important factor for parents making the decision whether or not to immunize comes down to their doctors' opinion. And the investments Minnesota has made in providing tailored health care services to Somali families, especially those with autistic children, may be beginning to pay off.
The Brian Coyle Community Center, which bustles with basketball-crazed kids, is in the middle of the Cedar-Riverside neighborhood—the outbreak's epicenter. And just a few days ago, the community center's walls had been pasted with flyers promoting the connection between the vaccine and autism. But the Somali staff decided to take them down. On request, the man working the front desk Saturday afternoon (who didn't want to share his name) produces a crumpled advertisement for last weekend's anti-vaxx public meeting. "I don't understand these people," he says, shaking his head. "With my children, if the doctor says to do something, we do it. They know best."
Each year the World Health Organization determines which of the thousands of influenza variants are most likely to circulate. Flu vaccines then use these variants at the starting point when developing the annual flu shot. Find out what’s inside the influenza-warding shot.
In his 1824 book, Reflections on the Motive Power of Fire, the 28-year-old French engineer Sadi Carnot worked out a formula for how efficiently steam engines can convert heat—now known to be a random, diffuse kind of energy—into work, an orderly kind of energy that might push a piston or turn a wheel. To Carnot’s surprise, he discovered that a perfect engine’s efficiency depends only on the difference in temperature between the engine’s heat source (typically a fire) and its heat sink (typically the outside air). Work is a byproduct, Carnot realized, of heat naturally passing to a colder body from a warmer one.
About
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Carnot died of cholera eight years later, before he could see his efficiency formula develop over the 19th century into the theory of thermodynamics: a set of universal laws dictating the interplay among temperature, heat, work, energy and entropy—a measure of energy’s incessant spreading from more- to less-energetic bodies. The laws of thermodynamics apply not only to steam engines but also to everything else: the sun, black holes, living beings and the entire universe. The theory is so simple and general that Albert Einstein deemed it likely to “never be overthrown.”
Yet since the beginning, thermodynamics has held a singularly strange status among the theories of nature.
“If physical theories were people, thermodynamics would be the village witch,” the physicist Lídia del Rio and co-authors wrote last year in Journal of Physics A. “The other theories find her somewhat odd, somehow different in nature from the rest, yet everyone comes to her for advice, and no one dares to contradict her.”
Unlike, say, the Standard Model of particle physics, which tries to get at what exists, the laws of thermodynamics only say what can and can’t be done. But one of the strangest things about the theory is that these rules seem subjective. A gas made of particles that in aggregate all appear to be the same temperature—and therefore unable to do work—might, upon closer inspection, have microscopic temperature differences that could be exploited after all. As the 19th-century physicist James Clerk Maxwell put it, “The idea of dissipation of energy depends on the extent of our knowledge.”
In recent years, a revolutionary understanding of thermodynamics has emerged that explains this subjectivity using quantum information theory—“a toddler among physical theories,” as del Rio and co-authors put it, that describes the spread of information through quantum systems. Just as thermodynamics initially grew out of trying to improve steam engines, today’s thermodynamicists are mulling over the workings of quantum machines. Shrinking technology—a single-ion engine and three-atom fridge were both experimentally realized for the first time within the past year—is forcing them to extend thermodynamics to the quantum realm, where notions like temperature and work lose their usual meanings, and the classical laws don’t necessarily apply.
They’ve found new, quantum versions of the laws that scale up to the originals. Rewriting the theory from the bottom up has led experts to recast its basic concepts in terms of its subjective nature, and to unravel the deep and often surprising relationship between energy and information—the abstract 1s and 0s by which physical states are distinguished and knowledge is measured. “Quantum thermodynamics” is a field in the making, marked by a typical mix of exuberance and confusion.
“We are entering a brave new world of thermodynamics,” said Sandu Popescu, a physicist at the University of Bristol who is one of the leaders of the research effort. “Although it was very good as it started,” he said, referring to classical thermodynamics, “by now we are looking at it in a completely new way.”
In an 1867 letter to his fellow Scotsman Peter Tait, Maxwell described his now-famous paradox hinting at the connection between thermodynamics and information. The paradox concerned the second law of thermodynamics—the rule that entropy always increases— which Sir Arthur Eddington would later say “holds the supreme position among the laws of nature.” According to the second law, energy becomes ever more disordered and less useful as it spreads to colder bodies from hotter ones and differences in temperature diminish. (Recall Carnot’s discovery that you need a hot body and a cold body to do work.) Fires die out, cups of coffee cool and the universe rushes toward a state of uniform temperature known as “heat death,” after which no more work can be done.
The great Austrian physicist Ludwig Boltzmann showed that energy disperses, and entropy increases, as a simple matter of statistics: There are many more ways for energy to be spread among the particles in a system than concentrated in a few, so as particles move around and interact, they naturally tend toward states in which their energy is increasingly shared.
But Maxwell’s letter described a thought experiment in which an enlightened being—later called Maxwell’s demon—uses its knowledge to lower entropy and violate the second law. The demon knows the positions and velocities of every molecule in a container of gas. By partitioning the container and opening and closing a small door between the two chambers, the demon lets only fast-moving molecules enter one side, while allowing only slow molecules to go the other way. The demon’s actions divide the gas into hot and cold, concentrating its energy and lowering its overall entropy. The once useless gas can now be put to work.
Maxwell and others wondered how a law of nature could depend on one’s knowledge—or ignorance—of the positions and velocities of molecules. If the second law of thermodynamics depends subjectively on one’s information, in what sense is it true?
A century later, the American physicist Charles Bennett, building on work by Leo Szilard and Rolf Landauer, resolved the paradox by formally linking thermodynamics to the young science of information. Bennett argued that the demon’s knowledge is stored in its memory, and memory has to be cleaned, which takes work. (In 1961, Landauer calculated that at room temperature, it takes at least 2.9 zeptojoules of energy for a computer to erase one bit of stored information.) In other words, as the demon organizes the gas into hot and cold and lowers the gas’s entropy, its brain burns energy and generates more than enough entropy to compensate. The overall entropy of the gas-demon system increases, satisfying the second law of thermodynamics.
The findings revealed that, as Landauer put it, “Information is physical.” The more information you have, the more work you can extract. Maxwell’s demon can wring work out of a single-temperature gas because it has far more information than the average user.
But it took another half century and the rise of quantum information theory, a field born in pursuit of the quantum computer, for physicists to fully explore the startling implications.
Over the past decade, Popescu and his Bristol colleagues, along with other groups, have argued that energy spreads to cold objects from hot ones because of the way information spreads between particles. According to quantum theory, the physical properties of particles are probabilistic; instead of being representable as 1 or 0, they can have some probability of being 1 and some probability of being 0 at the same time. When particles interact, they can also become entangled, joining together the probability distributions that describe both of their states. A central pillar of quantum theory is that the information—the probabilistic 1s and 0s representing particles’ states—is never lost. (The present state of the universe preserves all information about the past.)
Over time, however, as particles interact and become increasingly entangled, information about their individual states spreads and becomes shuffled and shared among more and more particles. Popescu and his colleagues believe that the arrow of increasing quantum entanglement underlies the expected rise in entropy—the thermodynamic arrow of time. A cup of coffee cools to room temperature, they explain, because as coffee molecules collide with air molecules, the information that encodes their energy leaks out and is shared by the surrounding air.
Understanding entropy as a subjective measure allows the universe as a whole to evolve without ever losing information. Even as parts of the universe, such as coffee, engines and people, experience rising entropy as their quantum information dilutes, the global entropy of the universe stays forever zero.
Renato Renner, a professor at ETH Zurich in Switzerland, described this as a radical shift in perspective. Fifteen years ago, “we thought of entropy as a property of a thermodynamic system,” he said. “Now in information theory, we wouldn’t say entropy is a property of a system, but a property of an observer who describes a system.”
Moreover, the idea that energy has two forms, useless heat and useful work, “made sense for steam engines,” Renner said. “In the new way, there is a whole spectrum in between—energy about which we have partial information.”
Entropy and thermodynamics are “much less of a mystery in this new view,” he said. “That’s why people like the new view better than the old one.”
The relationship among information, energy and other “conserved quantities,” which can change hands but never be destroyed, took a new turn in two papers published simultaneously last July in Nature Communications, one by the Bristol team and another by a team that included Jonathan Oppenheim at University College London. Both groups conceived of a hypothetical quantum system that uses information as a sort of currency for trading between the other, more material resources.
Imagine a vast container, or reservoir, of particles that possess both energy and angular momentum (they’re both moving around and spinning). This reservoir is connected to both a weight, which takes energy to lift, and a turning turntable, which takes angular momentum to speed up or slow down. Normally, a single reservoir can’t do any work—this goes back to Carnot’s discovery about the need for hot and cold reservoirs. But the researchers found that a reservoir containing multiple conserved quantities follows different rules. “If you have two different physical quantities that are conserved, like energy and angular momentum,” Popescu said, “as long as you have a bath that contains both of them, then you can trade one for another.”
In the hypothetical weight-reservoir-turntable system, the weight can be lifted as the turntable slows down, or, conversely, lowering the weight causes the turntable to spin faster. The researchers found that the quantum information describing the particles’ energy and spin states can act as a kind of currency that enables trading between the reservoir’s energy and angular momentum supplies. The notion that conserved quantities can be traded for one another in quantum systems is brand new. It may suggest the need for a more complete thermodynamic theory that would describe not only the flow of energy, but also the interplay between all the conserved quantities in the universe.
The fact that energy has dominated the thermodynamics story up to now might be circumstantial rather than profound, Oppenheim said. Carnot and his successors might have developed a thermodynamic theory governing the flow of, say, angular momentum to go with their engine theory, if only there had been a need. “We have energy sources all around us that we want to extract and use,” Oppenheim said. “It happens to be the case that we don’t have big angular momentum heat baths around us. We don’t come across huge gyroscopes.”
Popescu, who won a Dirac Medal last year for his insights in quantum information theory and quantum foundations, said he and his collaborators work by “pushing quantum mechanics into a corner,” gathering at a blackboard and reasoning their way to a new insight after which it’s easy to derive the associated equations. Some realizations are in the process of crystalizing. In one of several phone conversations in March, Popescu discussed a new thought experiment that illustrates a distinction between information and other conserved quantities—and indicates how symmetries in nature might set them apart.
“Suppose that you and I are living on different planets in remote galaxies,” he said, and suppose that he, Popescu, wants to communicate where you should look to find his planet. The only problem is, this is physically impossible: “I can send you the story of Hamlet. But I cannot indicate for you a direction.”
There’s no way to express in a string of pure, directionless 1s and 0s which way to look to find each other’s galaxies because “nature doesn’t provide us with [a reference frame] that is universal,” Popescu said. If it did—if, for instance, tiny arrows were sewn everywhere in the fabric of the universe, indicating its direction of motion—this would violate “rotational invariance,” a symmetry of the universe. Turntables would start turning faster when aligned with the universe’s motion, and angular momentum would not appear to be conserved. The early-20th-century mathematician Emmy Noether showed that every symmetry comes with a conservation law: The rotational symmetry of the universe reflects the preservation of a quantity we call angular momentum. Popescu’s thought experiment suggests that the impossibility of expressing spatial direction with information “may be related to the conservation law,” he said.
The seeming inability to express everything about the universe in terms of information could be relevant to the search for a more fundamental description of nature. In recent years, many theorists have come to believe that space-time, the bendy fabric of the universe, and the matter and energy within it might be a hologram that arises from a network of entangled quantum information. “One has to be careful,” Oppenheim said, “because information does behave differently than other physical properties, like space-time.”
Knowing the logical links between the concepts could also help physicists reason their way inside black holes, mysterious space-time swallowing objects that are known to have temperatures and entropies, and which somehow radiate information. “One of the most important aspects of the black hole is its thermodynamics,” Popescu said. “But the type of thermodynamics that they discuss in the black holes, because it’s such a complicated subject, is still more of a traditional type. We are developing a completely novel view on thermodynamics.” It’s “inevitable,” he said, “that these new tools that we are developing will then come back and be used in the black hole.”
Janet Anders, a quantum information scientist at the University of Exeter, takes a technology-driven approach to understanding quantum thermodynamics. “If we go further and further down [in scale], we’re going to hit a region that we don’t have a good theory for,” Anders said. “And the question is, what do we need to know about this region to tell technologists?”
In 2012, Anders conceived of and co-founded a European research network devoted to quantum thermodynamics that now has 300 members. With her colleagues in the network, she hopes to discover the rules governing the quantum transitions of quantum engines and fridges, which could someday drive or cool computers or be used in solar panels, bioengineering and other applications. Already, researchers are getting a better sense of what quantum engines might be capable of. In 2015, Raam Uzdin and colleagues at the Hebrew University of Jerusalem calculated that quantum engines can outpower classical engines. These probabilistic engines still follow Carnot’s efficiency formula in terms of how much work they can derive from energy passing between hot and cold bodies. But they’re sometimes able to extract the work much more quickly, giving them more power. An engine made of a single ion was experimentally demonstrated and reported in Science in April 2016, though it didn’t harness the power-enhancing quantum effect.
Popescu, Oppenheim, Renner and their cohorts are also pursuing more concrete discoveries. In March, Oppenheim and his former student, Lluis Masanes, published a paper deriving the third law of thermodynamics—a historically confusing statement about the impossibility of reaching absolute-zero temperature—using quantum information theory. They showed that the “cooling speed limit” preventing you from reaching absolute zero arises from the limit on how fast information can be pumped out of the particles in a finite-size object. The speed limit might be relevant to the cooling abilities of quantum fridges, like the one reported in a preprint in February. In 2015, Oppenheim and other collaborators showed that the second law of thermodynamics is replaced, on quantum scales, by a panoply of second “laws”—constraints on how the probability distributions defining the physical states of particles evolve, including in quantum engines.
As the field of quantum thermodynamics grows quickly, spawning a range of approaches and findings, some traditional thermodynamicists see a mess. Peter Hänggi, a vocal critic at the University of Augsburg in Germany, thinks the importance of information is being oversold by ex-practitioners of quantum computing, who he says mistake the universe for a giant quantum information processor instead of a physical thing. He accuses quantum information theorists of confusing different kinds of entropy—the thermodynamic and information-theoretic kinds—and using the latter in domains where it doesn’t apply. Maxwell’s demon “gets on my nerves,” Hänggi said. When asked about Oppenheim and company’s second “laws” of thermodynamics, he said, “You see why my blood pressure rises.”
While Hänggi is seen as too old-fashioned in his critique (quantum-information theorists do study the connections between thermodynamic and information-theoretic entropy), other thermodynamicists said he makes some valid points. For instance, when quantum information theorists conjure up abstract quantum machines and see if they can get work out of them, they sometimes sidestep the question of how, exactly, you extract work from a quantum system, given that measuring it destroys its simultaneous quantum probabilities. Anders and her collaborators have recently begun addressing this issue with new ideas about quantum work extraction and storage. But the theoretical literature is all over the place.
“Many exciting things have been thrown on the table, a bit in disorder; we need to put them in order,” said Valerio Scarani, a quantum information theorist and thermodynamicist at the National University of Singapore who was part of the team that reported the quantum fridge. “We need a bit of synthesis. We need to understand your idea fits there; mine fits here. We have eight definitions of work; maybe we should try to figure out which one is correct in which situation, not just come up with a ninth definition of work.”
Oppenheim and Popescu fully agree with Hänggi that there’s a risk of downplaying the universe’s physicality. “I’m wary of information theorists who believe everything is information,” Oppenheim said. “When the steam engine was being developed and thermodynamics was in full swing, there were people positing that the universe was just a big steam engine.” In reality, he said, “it’s much messier than that.” What he likes about quantum thermodynamics is that “you have these two fundamental quantities—energy and quantum information—and these two things meet together. That to me is what makes it such a beautiful theory.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Breaking2 is over, but you can still watch it here.
As part of WIRED’s exclusive look at Breaking2, Nike’s attempt to break the two-hour marathon mark, our writer, photographer, and videographer spent six months following the training of the three elite athletes who hope to run 26.2 miles in 120 minutes or less.
Breaking2
———
This is the latest in a series chronicling Nike's attempt to run a two-hour marathon.
On Saturday, Zersenay Tadese, the half-marathon world-record holder from Eritrea; Lelisa Desisa, a two-time Boston Marathon champion from Ethiopia; and Kenyan Eliud Kipchoge, gold-medalist at the Rio Olympics and the best marathoner in the world—stepped up to the starting line at the Autodromo Nazionale Monza race track in northern Italy to race against each other and history itself.
Nike considers the Breaking2 project its “mission to Mars,” and for months its team of designers, scientists, coaches, and statisticians has worked tirelessly to help one (or more) of these athletes cross the finish line in historic time. Watch as a record long thought unbreakable comes crashing down, or watch as the world’s best runners, aided by the world’s top scientists, come up short.
You can still watch the race, and catch up on all our coverage.
One of the world's finest distance runners came so close to achieving the greatest feats of athleticism in history: a sub two-hour marathon. To do it, the Eliud Kipchoge should have maintained an average pace of at least 13.1 miles per hour. So, we timed how long WIRED staffers could run at that speed. Needless to say, we didn't last long. Here's why only a handful of people in the world could ever come close to a two-hour marathon.
You know the internet is just full of videos, right? One that recently caught my eye shows Granny playing with her dog (or as my kids say, doge). Suddenly the dog goes after a bone or something and takes poor granny (or grandma, nana, or my personal favorite, big nanny) for a ride.
Funny, right? No. Not at all. It's either fake or there's a very hurt granny somewhere. This is why I'm here—to provide careful, scientific analysis of this video to determine whether it is real or fake. Honestly, I suspect it's bogus. I looks entirely too extreme. But I've been wrong before, so let's do some video analysis.
I will of course use my favorite (and free) tool, Tracker Video Analysis. However, I must make some assumptions about the scale of a few things in the video. Who knows how tall granny really is, but I'll go with 1.6 meters because she seems short. Either that or the dog is really tall. Now I can mark the position of the dog and granny:

By fitting a linear function to the position of the dog, I find that his (or her; I can't tell from here) speed is around 1.15 m/s (2.57 mph). That seems reasonable. After granny goes flying, she achieves a final speed of about 4 m/s (9 mph). That seems ... less reasonable.
Ah, I hear you asking, "But what about granny's acceleration?" From the plot, it looks like she accelerates from a stationary position to her flying speed in just under 0.2 seconds. Using the definition of average acceleration, I can calculate her value:

An acceleration of 20 m/s2 comes to just over 2 g's. High, but not impossible. That's like granny hanging from one arm while supporting her identical twin with the other. Possible, but unlikely. I'm not 100 percent certain about granny arms, but it seems like that would lead to a dislocated arm or something gross like that.
This brings us to granny's vertical motion. Based on my video measurements, her center of mass increases by about 10 cm when the dog yanks her off the ground. Now, why would she do this? Increasing her vertical velocity would require an upward force, but it appears that the dog leash remains slightly below horizontal. That means it would pull granny down even is it rotated her. Now, it's possible granny leaped when the dog gave a good yank. But I have to conclude this video is fake.
Diamonds might not be the rarest of geologic materials, but they can be some of the most valuable. Where do we get them all? It takes extremely high pressures to reorganize carbon into diamonds—pressures higher than can easily be mimicked by humans or even created by processes within the Earth’s crust. No, diamonds have to come from the Earth’s mantle, hundreds of kilometers beneath our feet.
But how do those diamonds get to the surface for us to collect (and sell)? The answer lies in some of the oddest and rarest volcanoes on the planet.
Kimberlites are volcanic eruptions that bring material from the depths where diamonds can form. Yet, unlike many geologic processes, a kimberlite eruption could launch rocks from the mantle at over 250 kilometers an hour! Yes, you read right: Kimberlite eruptions might be rocket ships from the Earth’s interior.
Kimberlite magma is what geologists call “ultramafic.” All that means is it is low in silica and high in magnesium (relative to other magma). What makes them cool is that they are likely coming directly from the mantle—that layer of rock beneath the Earth’s crust. Even at its thickest, the crust is only ~70 kilometers thick, but the source of kimberlite magma is likely over 200 kilometers down. So, in kimberlite deposits, we find all sorts of chunks of mantle rocks and minerals, along with chunks of the crust that the kimberlite blasted through (we call these chunks “xenoliths”—foreign rocks).
Kimberlites themselves are best described as “carrot-shaped,” where they widen at the top and narrow at depth until they reach the dyke of magma that was the pathway from their source deep in the mantle. The tops of the pipes might be tens to hundreds of meters wide, but at depth, they are likely only a few meters across.
When they erupt, they produce piles of broken volcanic debris (pyroclastic material) and the cone is filled with kimberlite breccia made from magma, xenoliths, and anything else in the way. They never have lava flows and even the debris isn’t large volume, probably millions of cubic meters rather than the billions (and more) cubic meters of more typical explosive volcanic eruptions.
They aren’t common. Most kimberlites are found in areas of the oldest rocks on Earth known as continental cratons. There are some found outside those cratons (such as the kimberlites of Kentucky and Arkansas) but they are still typically found where the rocks are old. Geologists aren’t too sure why that is, but globally, you find a lot of places where there is old crust like Canada, Brazil, Siberia, South Africa, northern China, and Australia.
Hawaiian Volcano Action
Most kimberlites are old, too, forming from the Proterozoic (between 541 million and 2.5 billion years ago) up to the Cretaceous (79-145 million years ago). However, there are a few places where geologists think youngest kimberlites may have erupted, including the Igwisi Hills in Tanzania that might be only ~10-20,000 years old and the ~30 million-year-old Kundulungu Group on the DR Congo. So, kind of like big flood basalt provinces and komatiite lavas, kimberlites seem to have been more common in the planet’s past.
That doesn’t mean they can’t happen today! What might a kimberlite eruption be like if we had one in the middle of Kentucky (or really anywhere in central North America)?
This is where things get a little more speculative. Considering we’ve never seen a kimberlite eruption, we have to try to back out the events that happen and the timing of the eruption using clues in the rocks—like how minerals shatter, the types of material found in the deposits, and the shapes of the pipes. Really, it comes down to making, well, magmatic soda.
Kimberlite eruptions likely start when a carbon dioxide-rich magma forms from melting the mantle. That magma could end up with almost 20 percent carbon dioxide by weight, which is much higher than typical magma (that might be only a couple percent). This magma forms 250 kilometers below the surface and is so low in density, it begins to rise rapidly.
As it rises, all the CO2 begins to come out of solution and form a tip on the rising magma. That CO2 foam sneaks into cracks and shatters the rock, allowing for more rising. Behind it follows the kimberlite magma that is still degassing at faster and faster rates, making a magmatic foam that trails the CO2 foam. Really, it is like one big magma soda bottle whose top has been popped. By the time the magma reaches the surface, the foam tip might be 2 to 4 kilometers long moving through that ~1-3 meter pipe.
With all this foam rising through rocks that are under pressure, that dramatic change in stress causes the walls of the pipe to shatter, adding more material to the kimberlite magma as it rises. At times, the kimberlite magma is likely rising behind the CO2 and magma foam at 30 to 50 meters per second. That’s over 100 kilometers per hour.
The gas and foam? By the time it’s close to the surface, it might be moving close to 300 to 600 meters per second … over ~1,000 kilometers per hour! So, the trip from the mantle to the surface might only take one hour to get all the gas, foam and magma to the surface
Now, if you’re on the surface before a kimberlite eruption, this means that you won’t have much in the way of signs that an eruption is going to occur. Once the process starts, you would guess that earthquakes would start to be measured from depth and rapidly rising towards the surface as the magma moves and shatters rock. You would also likely get some tremor associated with the magma moving through the pipe.
However, the rapid nature of the event means that earthquakes might be the only sign until the kimberlite magma and foam is near the surface, when [totally speculative] we could notice an increase in CO2 emissions from the ground or very fast deformation of the area where the eruption will occur. Humans have never experienced at kimberlite eruption, so this would be a whole new world of monitoring and mitigation to prevent casualties if this happens under a populated area.
Once the foam tip of the kimberlite magma reaches the surface, there is going to be a big explosion. All that compressed gas and magmatic foam will now expand rapidly, creating a massive jet of CO2, volcanic debris, random chunks of rock from around the vent, magma and whatever else might be in the way. Depending on how much stuff is mixed it, the plume could rise as fast as over 1 kilometer per second, so it could reach 20-30 kilometers height in a few minutes—so, think something like the eruption of Mount St. Helens in 1980.
However, that rapid decompression creates a blast wave that will travel down as well as up. The wave will propagate back into the pipe at half the speed of sound, creating more degassing of the magma continuing to rise and extending the explosive eruption.
At the same time, the drop in pressure in the pipe causes the walls of the pipe to start to collapse, heralding the beginning of the end. All the magma in the pipe will cool rapidly and solidify, mixing with all the debris to create the mixed up kimberlite breccia. The waves of all these decompression explosions will resonate in the pipe, making a pulsing explosive eruption. However, the whole thing would likely be over in tens of minutes as the wall collapses and the rising magma cools.
The surrounding landscape would be covered in volcanic ash and debris, some made of erupting magma, some made of chunks of mantle and crust xenoliths. The deposit wouldn’t likely be thick, but you could imagine anything within a few kilometers of the vent would be hit with a rain of ballistic bombs and shockwaves from the eruption.
The crater might only be the size of a large sinkhole, but the halo of volcanic debris would extend tens of kilometers. (Sadly, it wouldn’t rain diamonds. They mostly end up in the solidified magma in the crater or dyke beneath it).
After the eruption, the crater, now filled with the porous debris of the eruption, would likely fill up and form a small crater lake. Luckily, kimberlites appear to be monogenetic—that is, they erupt once and are done. Less luckily, they tend to form in clusters, so whatever area experiences the first kimberlite eruption might expect more to come. However, the timing is unknown. Would it be in hours, days, months, years? We don’t know.
In the end, an eruption of a modern kimberlite would be one of the more dramatic geologic events we’ve experienced. In the span of what might be as little as an hour, material from the mantle would be thrown to the surface in a massive explosion that ends as fast as it started. The area around the vent would be devastated, but likely no long-lasting or wide-reaching impacts would be noticed (unless maybe a bunch of kimberlites erupted within days of each other?) Hopefully, it happens far from human populations so we can just enjoy the scientific bounty that would come from such an eruption. Just another way the Earth can make life exciting for those of us who populate the surface.
HIV has no cure. But it's not quite the implacable scourge it was throughout the 1980s and 1990s. Education, prophylactics, and drugs like PrEP have cut down its transmission. Anti-retroviral treatments keep HIV-positive people's immune systems from collapsing.
But still, no cure. Part of the problem is HIV's ability to squirrel itself away inside a cell's DNA—including the DNA of the immune cells that are supposed to be killing it.
The same ability, though, could be HIV's undoing. All because of Crispr. You know, Crispr: The gene-editing technique that got everyone really excited, then really skeptical, and now cautiously optimistic about curing a bunch of intractable diseases. Earlier this week, a group of biologists published research detailing how they hid an anti-HIV Crispr system inside another type of virus capable of sneaking past a host's immune system. What's more, the virus replicated, and snipped HIV from infected cells along the way. At this stage, it works in mice and rats, not people. But as a proof of concept, it means similar systems could be developed to fight a huge range of diseases—herpes, cystic fibrosis, and all sorts of cancers.
Blood Diseases Could Show Crispr’s Potential as Therapy
China Used Crispr to Fight Cancer in a Real, Live Human
The Long-Shot Bid to Put Crispr in the Hands of the People
Those diseases are all treatable, to varying degrees. But the problem with treatments is you have to keep doing them in order for them to work. "The current anti-retroviral therapy for HIV is very successful in suppressing replication of the virus," says Kamel Khalili, a neurovirologist at Temple University in Philadelphia and lead author of the recent research, published in Molecular Therapy. "But that does not eliminate the copies of the virus that have been integrated into the gene, so any time the patient doesn't take their medication the virus can rebound." Plus treatments can—and often do—fail.
Gene therapy has promised to revolutionize medicine since the 1970s, when a pair of researchers introduced the concept of using viruses to replace bad DNA with good DNA. The first working model was tested on mice in the 1980s, and by the 1990s researchers were using gene therapies—with limited success—to treat immune and nutrition deficiencies. Then, in 1999, a patient in a University of Pennsylvania gene therapy trial named Jesse Gelsinger died from complications. The tragedy temporarily skid-stopped whole field. Gene therapy had been steadily getting its groove back, but the 2012 discovery that Crispr could make easy, and accurate, cuts on human genes, has added even more vigor.
Crispr as an agent for curing HIV has its own problems. For one, it has to be able to snip away the HIV from an infected cell without damaging any of the surrounding DNA. HIV mutates and evolves, so Khalili and his co-authors couldn't just program their Crispr system with a single genetic mugshot. Instead, they had to target enough unchanging sections that were also critical to the virus' survival.
Their next challenge was delivering the system to a critical mass of infected cells. First you have to get it past the immune system—which is programmed to attack any non-foreign object entering the body. In a gene therapy trial in 1999, a patient named Jesse Gelsinger died because his immune system overreacted to the viral vectors carrying the therapy. So doctors hoping to prescribe gene therapy have to be aware of patients' prior viral exposure.1 These researchers avoided the problem by packing their Crispr system inside a type of virus called AAV (short for adeno-associated virus). "AAVs are a very small helper virus, they can't actually replicate in a cell on their own unless they have another virus there to help it along," says Keith Jerome, a microbiologist at the Fred Hutchinson Cancer Research Center in Seattle. "The great thing about AAVs is they cause essentially no immune system response in humans."
In order to get approved for human use, this type of Crispr-borne cure would have to be both safe and effective. This study got part of the way. This study was going strictly for efficacy: Does this work? Khalili and his co-authors treated mice and rat model with strains of HIV that were latent—hiding away in cellular DNA—and others where the HIV was actively replicating. Then they used it on mice grafted with human cells. In all three cases, the HIV rates went down significantly.
Other good news on the safety front: There's no evidence their trial made any off-target cuts. But they'll need to run more experiments to make sure that's absolutely the case, probably using primate models, since their DNA is closer to humans'. They also have to make sure the treatment gets rid of enough HIV, so that it doesn't just replicate itself back to harmful levels. "In actual human patients there’s no way that a Crispr gene therapy will ever get 100 percent of HIV," says Paul Knoepfler, a stem cell biologist at UC Davis. "How highly efficient will be 'efficient enough' to make a clinically meaningful impact?"
Khalili believes he can get close enough. According to him, the Crispr system doesn’t need to eliminate all the HIV-infected cells—just enough so an HIV-patient’s immune system can get strong enough to take care of the rest on its own. "I strongly believe in the gene editing strategy, and with my 30 years in HIV research, I think this is the one that is going to take us to the end."
He's not the only optimist. "The advantage of using a virus as your delivery system is it can infect virtually every cell," says Jianhua Luo, a pathologist at the University of Pittsburgh. Luo is using a similar Crispr-in-a-virus system to target cancerous DNA in cells.
And curing HIV could be a proof-of-concept for other diseases—even genetic diseases people are born with. Even though the virus starts as a simple infection, once it becomes part of a person's chromosome, it essentially becomes a genetic disease. Imagine a world where, instead of removing her breasts, Angelina Jolie could instead have taken a dose of genes that snip away the BRCA2 genes that threatened her with cancer. That's the difference between a treatment and a cure.
1Update 2:50 pm EST 5/26/17: This story has been updated to correct information about the gene therapy trial that resulted in Jesse Gelsinger's death. It was an adenovirus, not an adeno-associated virus, that was used as a vector in that trial.
The ability to edit DNA is already here, but the effects of doing so are hardly understood. Genetics experts explain the potential pitfalls and alluring upside of using the technology on athletes.
Of all the provisions of the Affordable Care Act—“Obamacare,” if you’re on a first-name basis—the one that seemed the most uncontroversially humane was the guarantee that insurance companies could not use so-called preexisting conditions to deny coverage. If you had a chronic illness or had recovered from something and lost your insurance, or if you quit or got fired, you could still get onto a plan.
But the odds say that sick people stay sick or get sicker, and insurance companies don’t make a profit by paying out. By voting to repeal the ACA and replace it with … well, with something, not totally clear what, the Republican-led House of Representatives seems to have nuked the preexisting condition guarantee. The new bill, which passed in a close 217-213 vote, allows insurance companies to charge sick people more. According to one nonpartisan analysis, it allocates enough money to cover those higher rates for just 5 percent of people with preexisting conditions.
Think it can’t get worse? Hold, as the saying goes, my beer. The ACA specifically protected against discrimination for preexisting conditions that showed up through genetic tests. You might not be sick yet—in technical terms, the illness has not manifested—but if you, for example, test positive for one of the pathogenic variants (a less X-Manly term than “mutation”) in the BRCA gene that predisposes you to breast cancer, you could still get covered. If the House bill becomes law, that protection vanishes.
Obamacare’s Demise Is a Looming Disaster for Mental Health
Not Even Insurance Companies Want Obamacare Repealed
Obamacare Paved the Way for That Gattaca-Style Employer’s Law
Advances in genomics in the seven years since the ACA became law haven't helped scientists better define a preexisting genetic condition. The more you know about genetics, the more conditions start to look preexisting. But multiple interacting genes and environmental effects mean it's hard to tell what'll turn from potential to real. The issue is "penetrance"—what proportion of people who have a pathogenic variant will actually get the disease? “You could say all of us have a preexisting condition of import, and it’s just a matter of when we’re going to manifest it,” says Eric Topol, a genomicist at the Scripps Research Institute. “Very few of us are genetically bulletproof.”
It’s not like nobody saw this debate coming. Even back when sequencing a human genome cost $100 million, policymakers and scientists were trying to figure out how to safely get data from people while simultaneously keeping insurers and employers from using it to screw them. After a decade of debate, the result was the Genetic Information Nondiscrimination Act, a 2008 bill that aimed to protect people’s genetic privacy. GINA wasn’t perfect—it doesn’t extend to long-term care and life insurance, for example.
GINA also doesn’t quite define illness. It protects family history and tests of DNA, RNA, proteins, metabolites, and other indicators—a panopoly of -omics beyond just genomics. But it doesn’t protect you if you already have symptoms. So then the question is, what counts as a symptom? Is a person only sick when they first start feeling pain? When a doctor prescribes a drug? Or when something changes on a cellular level? “When you don’t have symptoms and you aren’t disabled or have some other significant clinical syndrome, does that mean that it’s preexisting? When it’s encoded in your DNA or other parts of your intrinsic self?” Topol asks.
In other words, when does “preexisting” turn to “existing?” More sophisticated, more widely available, and less expensive tests make that area greyer and greyer. An example: Let’s say you have a pathogenic variant in a Long QT gene, associated with sudden cardiac death. (Three main genes account for three quarters of cases.) “GINA would protect against employers and health care from discriminating against you on the basis of that genetic finding, but once you got an abnormal EKG, that’s no longer a genetic piece of information, and they could discriminate,” says Robert Green, a medical geneticist at Harvard Medical School. The ACA fixed that problem. “Obamacare closed that gap between predisposition and manifestation. If you take away Obamacare, you open this vast grey area again.”
That same issue comes up with every biochemical variation of clinical significance. Find it via sequencing? Covered. Find it via a blood test? You’re out. “It just makes it messier and more unclear, because of the overlap between manifestation and genetic testing,” says Anya Prince, a lawyer and researcher at the Center for Genomics and Society at UNC. “The law always has difficulty in defining complex science, and complex science that’s changing rapidly.”
Keep right on holding my beer, because thanks to the ACA becoming law just a couple years after GINA, nobody ever really had to find out what GINA will actually protect. “These definitions haven’t been fully tested because they haven’t shown up in court,” Prince says. Just an example: Fiscal 2013 saw 333 employment discrimination complaints based on GINA … and 90,000 based on everything else—mostly the Americans With Disabilities Act. Most people have never even heard of GINA. If Congress and the President replace Obamacare with something like what the House has cooked up, that’ll change, because GINA will be the only way to force insurance companies to cover people with preexisting conditions.
Unless, listen, you might as well just drink that beer at this point, because in early March a House committee passed HR 1313, which says that GINA doesn’t apply to workplace wellness programs. If the bill passes, or becomes part of a bigger bill, employers could ask for genetic information under the guise of creating healthier environments. But since employers are the ones who carry insurance, they could also just fire people who pop a bad test.
Despite how grim all this sounds, it might turn out OK. Genetics isn’t destiny. “Insurance companies don’t know how to deal with this, because there aren’t good metrics to put into their underwriting algorithms,” Green says. So maybe people don’t actually need to firewall their genetic information. Insurers won’t be able to do anything with it today, because nobody understands it well enough. And then, maybe in years or decades, genetic tests will actually lead to better health outcomes. That’d be a win for insurers and the insured.
Unless, of course, people don’t participate in the studies. Green has been working on the kind of research that might someday turn into those win-win outcomes—like, for example, his lab’s work sequencing the genomes of a bunch of newborn babies. When his team went to sign people up, they got a 10 percent recruitment rate—to get 300 families they had to ask 3,000. “There are a lot of reasons," says Green, "but about the third most common was concerns about privacy and discrimination.” If people feel afraid to be part of the data-gathering effort, it’ll inhibit those blue-sky results. The policy will have failed citizens and science alike.
This story originally appeared on Grist and is part of the Climate Desk collaboration.
When George McFadden sits at his computer to analyze crop photos, he looks like a doctor pointing out trouble spots on an X-ray. He identifies unnatural lines, “blob-like” patterns, and streaks clouding a field. All can indicate a troubling diagnosis.
“Can you see these little dots?” McFadden asks, pointing at a thermal shot of a tomato field that has suffered from a defective irrigation system. The dots on the image revealed that the system’s drip line had tears in it, he says. Watering the field became “like taking a straw, putting a bunch of pinholes in it, and trying to pump water through it.” The tomato grower used the image to show the manufacturer that the irrigation line was defective.
“Pretty striking,” McFadden says, still examining the screen. The 32-year-old field agronomist works for Ceres Imaging, a start-up in Oakland, California, that uses aerial imagery to help farmers optimize water and fertilizer application. The company is part of a growing contingent of technology start-ups vying to transform one of the state’s most powerful industries—agriculture—for a future in which its most important input grows increasingly scarce, and every drop counts.
California is the country’s top agricultural producer, growing two-thirds of the nation’s fruits and nuts and more than a third of its vegetables. Golden State farms and ranches constitute a $54 billion annual industry. The state’s ag-focused economy means growers have historically been power players in politics, especially in discussions about apportioning water.
California Overcame 1/100 Odds to Beat Its Epic Drought
5 Easy Rules for Drought-Friendly Cooking
Maybe the West’s Water Wars Aren’t as Bad as You Think
But as growth, drought, and climate change have increased scarcity and led to louder calls for conservation, the industry’s clout has been waning.
In 2015, during a record-setting drought, Gov. Jerry Brown ordered cities and towns to reduce water use by 25 percent—the first such mandatory cutback in state history. It prompted some to criticize the agriculture sector’s consumption—which makes up 80 percent of state use—and question why the industry was spared.
Brown defended the decision, saying farmers were already among those hardest hit. Many faced huge cuts to water allotments from state and federal systems and had to pay overblown sums for the water they could access. This was particularly hard on farmers because they operate with narrow profit margins, and more than 80 percent of California farms are small and family-owned. A few months after the order for cities, as the drought slid into its fourth year, some farmers were slammed with further restrictions.
A study released last summer estimated that the drought would cost California’s agricultural industry more than $600 million in 2016. For 2015, the estimate was $2.7 billion.
And though much of the state has gotten drenched this winter—over 70 percent of California is now out of drought—the long-term forecast for severe water shortages remains unchanged. In April, Brown ended the state of emergency for most parts of California; it had been in effect since January 2014. But climate change will continue tightening the state’s water supply. To keep crop yields high, or even just to stay in business, farmers will have to become more calculating.
That’s where technology comes in.
Silicon Valley, the nation’s most powerful tech hub, sits in the middle of California’s most productive farmland. To the east lies the Central Valley, growing crops like almonds and walnuts; to the north is Napa Valley, with its world-famous grapes; and to the south is the Central Coast, the “salad bowl of the world.”
Despite their proximity, the agriculture and technology sectors haven’t had much interaction. Though both are powerful forces in the state—ag a long-time influencer, tech a newer one—the cultural divide between the two is vast.
But bridging that gap could help solve one of agriculture’s most pernicious problems: water scarcity. Technologists are betting their solutions will ensure a steady stream of revenue for both industries in an increasingly dry world.
Jenna Rodriguez, Ceres’ product manager, was raised in Linden, California, a small agricultural town on the northern tip of the San Joaquin Valley in the center of the state.
“I’ve grown up listening to growers talk about the water situation,” Rodriguez tells me. “They’re growing food to also feed their families. And when a family farm has water allocations cut back to zero percent, it can make or break the income for a family.”
After spending summers driving tractors and bailers at her parents’ hay harvesting business, Rodriguez got a PhD in hydrological sciences. Now she’s based in the Central Valley town of Ripon, working to bring Ceres’ technology to more farmers throughout the area.
The day we spoke, Rodriguez had just finalized plans for Ceres’ launch into Hawaii, where its imaging system will be used on tropical crops like pineapple and coffee as well as commodities like corn and soybeans. When it launched in 2014, Ceres initially focused on lucrative nut crops in the Central Valley. Then it expanded to other crops in California, the Midwest, and even Australia. In total, the company now analyzes hundreds of thousands of acres for its clients.
Some of Ceres’ aerial technology is similar to what has been used by other imaging companies for decades. But Ceres’ chlorophyll measurements are a proprietary product. Its image processing and the guidance it offers to growers are also unique.
To assess fields, Ceres hires pilots who fly their aircraft low over the ground. The company attaches special cameras focused on particular wavelengths to assess water stress, chlorophyll content, and biomass—all indicators of health in a crop. Within 24 to 48 hours, growers can access processed imagery on devices like phones or tablets, which McFadden says are popular with growers in the field.
Then someone on Ceres’ small staff of 24, often Rodriguez or McFadden, will work with growers to explain the significance of the patterns and colors expressed in the images. Blue and green indicate healthy plants, while red and yellow show water stress and potential irrigation problems.
Ceres images from a 160-acre almond study.
“It’s like this constant battle of maintaining and operating your irrigation system,” says McFadden, sitting at a gray folding table in Ceres’ bare-bones office. “A big thing with tomatoes is identifying the leaks. Currently [growers] have teams of people who will go and walk each row of the tomato field, which is a pretty inefficient use of time.”
According to independent field tests, the imagery works. Since 2014, Ceres has teamed up with the University of California Cooperative Extension, a program that has provided agricultural data to growers in California for over a century. The extension has worked on several studies with Ceres, including a trial for the Almond Board of California that measured the response of nuts to different rates of watering.
In that study, data from Ceres images matched well with the extension’s ground measurements, says Blake Sanden, who headed up the trial. He’s an irrigation and soils management adviser for the extension program—or, as he calls himself, in a voice as slow as molasses, “the water and mud guy for Kern County,” which sits at the southern end of the San Joaquin Valley.
Ceres’ relationship with the extension program has helped the company gain trust with sometimes-skeptical farmers. Sanden says the extension’s government-funded trials are “the gold standard of efficacy” for new products in the ag market.
Even with that kind of validation, though, it takes effort to convince growers that a new product isn’t snake oil. Farmers tend to be skeptical of change and hesitant to acknowledge that they’ll need to cede more water to other uses in the state.
Sanden told me the Central Valley’s attitude toward a water-stressed future can be summed up in two words: “Fear and trepidation.”
Farmers, who Rodriguez calls “the original stewards of the environment,” are not prone to waste. But in the past, many California growers had cheap, consistent access to water distributed by systems like the Central Valley Project, a federal network of reservoirs and irrigation channels. More recently, though, programs developed to keep growers flush have dried up or apportioned some of them much less water than in the past—down to nothing.
“The attitude used to be, ‘I can find water,’” says Sanden. “I would say that 30, 40 years ago there was an attitude of hope, overconfidence—whatever you want to call it—that some of the restrictions on pumping water [would] go away.” He says growers expected decision-makers “to come back to reality and understand that we’ve got to make money in California and grow food.”
But the restrictions didn’t go away. Instead, they became stricter. Those constraints, along with the drought, have threatened grower livelihoods across the state. The uncertainty has made farmers friendlier to new technologies. For many, it’s been the only way to survive.
Dave Santos, who grows apricots, cherries, and almonds in Patterson, a Central Valley town sandwiched between I-5 and the San Joaquin River, remembers the advent of drip irrigation, which his 900-acre farm has used for more than 30 years. Since then, he says, a lot more innovation has sprung up—so much so that 67-year-old Santos leaves some of it to his son, like experimenting with aerial imaging.
“I’m just a neophyte in all of this stuff,” he says. “We’re trying to do our best.”
Today, growers like Santos and his son attend conferences to learn about the latest tools and meet with a rotating cast of salespeople who pitch them on new products and services.
“Obviously, with the California drought, anything that can help with water efficiency, they’re willing to spend time and listen to see what’s available,” McFadden says. “In general, all these costs are increasing, but the revenue is not. So how do they deal with that? Become more efficient.”
Scott Bryan and Tom Ferguson meet me in a French café in San Francisco’s Financial District. Next door is a hip ice cream shop, and above that is their tiny office—in a coworking space—from which they run the only California-based start-up accelerator focused specifically on water innovation.
Each year, Imagine H2O handpicks about 10 start-ups working on “solving” water. Competition is fierce. The nonprofit’s staff of four and a group of judges comb through about 100 applications for each cycle. They’re looking for a special sauce that includes commercial potential, interesting technology, and solutions that keep the customer in mind.
“There are a lot of people in water who just have an idea,” says Bryan, the group’s president. “It can be something on the side.”
“Let’s tow icebergs down from Alaska—which is a thing,” offers Ferguson.
“Which is a thing,” Bryan says.
“Apparently,” Ferguson adds wryly.
The day we sit down together in January, they’d just begun working with the 12 companies selected for the 2017 program. Over the course of nearly a year, the selected companies will work with industry mentors, hone their pitches, and liaise with potential customers, partners, and investors. Imagine H2O’s goal is to get start-ups from point A to whatever a company envisions as point B. Since 2009, the program has helped 650 companies that are working on water scarcity and conservation in more than 30 countries.
In March, Imagine H2O held a swanky champagne reception for its new cohort at a San Francisco event space bathed in blue light. While suited attendees milled around the room, the cohort’s entrepreneurs floated next to display tables, ready to pitch their technologies.
Last year, Ceres was among Imagine H2O’s chosen cohort. At the 2016 reception, the company was selected from the larger group as the program’s Water Data Challenge winner.
“They had clearly spent a hell of a lot of time with their customers,” says Ferguson, the vice president of programming. “Your network is crucial.” Farmers want to know: “What’s my neighbor doing? Does he trust it?” he says.
“It’s the relationship component,” adds Bryan.
“Totally,” Ferguson agrees. “That’s kind of the determinant of virality—to use a terrible San Francisco phrase.”
What many tech entrepreneurs get wrong, according to the two, is assuming growers have an unlimited capacity to adopt new technology. “Is a farmer going to have 10 different phones with 40 different apps on each phone?” Bryan asks. “No. The farmer is going to be like anyone else—they’re going to use technology, but they’re going to use the stuff that has some kind of return for them.”
Rodriguez says there’s still “a substantial barrier in general between ag tech and agriculture.” But people working at the intersection insist both sides have the desire to find technological solutions that address the water crisis.
“The agricultural industry wants to be more productive, they want to make money, they want to be profitable,” says Graeme Jarvis, an Imagine H2O accelerator judge and mentor, who has worked in start-ups and who helped build John Deere’s precision agriculture business unit.
“It just so happens that by leveraging technology and new ways of understanding how to drive those in-field decisions,” Jarvis says, “[you] actually end up having this secondary or complementary benefit, which is better water stewardship. That said, it’s early days in the realization of a lot of this.”
To succeed, technologists will have to meet farmers in the field.
Bryan says, “The biggest mistake people make: They don’t understand what the on-the-ground needs and limitations are.” You can only grasp that by talking, and especially listening, to growers. “If you don’t, you’re just another entrepreneur with a gadget looking for a problem.”

California is in an unprecedented drought, but you gotta eat, right? Well, some foods use way more water than others. We chewed through a ton of data to make this bite-sized video. For you! -Brent
Christopher Richins has had a somewhat obscure to-do item—“start a multinational space company”—on his list since he was a young guy. Back then, he worked for Sea Launch, a company that put Russian and Ukrainian rockets on Norwegian ships, sent the ships sailing toward the equator, and launched satellites from the middle of the ocean. “This was the coolest, most James-Bond thing I’d ever heard of,” Richins says. He learned about the space industry, satellites, how to launch stuff and work across borders. Years later, asteroid-mining and Earth-observing entity Planetary Resources recruited him as their third employee. And it was then, thinking about the needs of their spacecraft and the many others to come, that he knew what his future multinational company should do: get data back down from space.
Lots of companies today, like Planet, are launching low-overhead satellites. Each of them takes in many bytes of data—of Earth, of your hideout, of whatever. But they have to bring all that data down somehow, and they don’t all have the resources to build their own global communications networks. Richins is trying to fill that hole, with a sharing-economy model for satellite downlinking.
From low-Earth orbit, satellites—the hundreds of them that live there—can only blip-blip-blip their data to a ground station when they’re in its line of sight. That only happens once every hour and a half and lasts less than 10 minutes. So satellite-makers can only drag digital bits from the sky for a small part of the day from any given spot on the globe.
The well-heeled corporations of the world simply build world-spanning antenna networks to stay in touch with their satellites. “But for smaller companies that aren’t NASA or the Department of Defense, it’s difficult to have dishes around the world,” says Richins. What if, though, he could dole out the extra time on those dominators’ ground stations, and sell it to the little guys? They have extra time, after all, since any given station only sees a given satellite for a few minutes at a time.
That was 2012.
Richins held onto the idea for three years. The world wasn’t ready: In 2012, fewer than 40 satellites between 1 and 50 kilograms went to space, according to industry analyzer SpaceWorks. But by 2015, when he co-founded a company called RBC Signals with Olga Gershenzon, that number had risen to more than 120. The company went to antenna owners—whose names they won’t disclose but whose locations are here—and said, essentially, “Hey, we know you’re not using those dishes all the time. If we share our revenue with you, will you let us parcel out those extra hours to others?”
To date, the proprietors of around 30 antennas have said, “Yeah, sure,” helping RBC Signals launch its “infrastructure as a service” in 2016. RBC is also planning to build its own sharing-economy-style network. They’re targeting the projected 2,400 small satellites that will hitch rides to orbit between 2017 and 2022. These little-guy operators won’t want to blow their money on expensive antennas. The non-NASAs and non-DoDs can pay just for the amount of time they need, with a kind of tiered subscription service.
Tiny DIY Satellites from Stanford
RBC isn’t the only acronym that caters to this industry’s need for ground stations. Norwegian company KSAT has added smallsat-optimized antennas to its own existing network, while Italian LeafSpace is building a web of receivers meant just for small things. Spaceflight Networks also offers a “data plan” type of access to their antennas and those of partners.
So those 2,000+ eensy satellites that could launch in the next five years will have options—networks both bespoke and cobbled-together, dedicated and reappropriated. That competition will drive the cost of communication down further, letting the existing smallsat community and future startups focus on what their instruments can do and what their observations mean—not how and when to get them down to the ground.
And the emerging down-to-Earth industry ensures this: No little-orbiter-maker will ever have to build its own ground stations if it doesn’t want to.
I have a tradition of doing a little analysis on one of my favorite movie franchises to celebrate Star Wars Day (May the Fourth Be With You). Given that Rogue One: A Star Wars Story recently appeared on DVD and various streaming services, I think its OK to look at that film without worrying about spoilers. In this case, I will calculate Darth Vader’s power output as he uses the Force.
If you’ve seen the movie, you know about the awesome scene at the end where the Sith Lord opens a can of whoop ass on some Rebel troopers. Vader uses the Force to pin a rebel against the ceiling and hold him there for just a while before slicing him with his light saber. That seems excessive, but I guess Vader wanted to make the rebel wait awhile before killing him because the Dark Side makes you cranky.
To calculate the power needed to lift this poor fellow, I can estimate how high Vader lifts him and at what speed. If I guess that the rebel is 1.75 meters tall (the average height of a man here on Earth, so I’ll assume the same holds true on whatever planet the rebel calls home) then I can get an approximate scale for the scene. With that, I can use video analysis to plot the vertical position of the rebel as a function of time:

Just to be clear, I am using Tracker Video Analysis to get data from this video. But you can see from this plot that the rebel rises at nearly constant speed. Using the slope of this line, I get an upward velocity of about 3.3 m/s. The video provides another important measurement—how high the rebel rises. Based on the graph (which plots his approximate center of mass), Vader lifts him about 1.5 meters. Oh, wait! I need just one more thing—the time it takes Vader to lift the guy. This whole motion takes about 0.46 seconds.
Now for some physics. Let me start with power, defined as the rate at which work is done. Mathematically, it looks like this:

I can determine the work done by Darth Vader by looking at a system consisting of the rebel and the ship (I assume the ship uses some method of generating Earth-like gravity). The work-energy principle dictates that the work done on a system is equal to its change in energy. The rebel-ship system exhibits two types of change in energies—kinetic energy and gravitational potential energy:

Knowing the velocity and the change in vertical position, all I need is the mass of the rebel. Again, assuming he is a fairly ordinary person (and he must be; if he were extraordinary, he surely would have found a way to avoid getting killed), he has a mass of around 70 kg. I also can assume the inside spaceship has a gravitational field of about 9.8 N/kg (just like on Earth, where Gareth Edwards filmed this). Putting in these values, Darth Vader does 1,410 Joules of work. Because he does this in just 0.46 seconds, he has an output of 3,065 watts. That is the power of the Force.
In 2006, the Defense Advanced Research Projects Agency vowed to build, within four years, a brain-controlled prosthetic arm indistinguishable from the real thing. Yet after hundreds of millions of dollars and more than a decade of engineering, most limb replacements (even those wired straight to the noggin) struggle to mimic human gestures. Cracking the neural code for movement was trickier than expected.
The trouble lies in getting past conscious thought. “Capturing the body’s innate ability to just know what to do is something really lacking from all prosthetics today,” says Mike McLoughlin, who manages the prosthetics program at Johns Hopkins. Any inputs the arm receives—whether from the sparks of nerves left over on a stump, or directly from motor neurons in the brain—require explicit instructions, which humans are bad at doling out. So the latest attempts at a true bionic arm simply avoid the problem of intention by using machine learning and computer vision to make decisions.
Imagine you’re learning to play the guitar. Every time you attempt a new chord, you must think about where to place your fingers. And you must do that over and over, consciously placing them in space, thinking about timing and pressure. Once you’ve developed that muscle memory, your brain simply says “play a C chord” and you do it. You can even do it without looking. It doesn’t work that way for people with prosthetic limbs.
“Reaching out to grab something is never going to be a subconscious reaction,” says McLoughlin. “You always have to look, you always have to think.” Which explains why truly bionic limbs remain elusive. Some of the most advanced neurally controlled prosthetics, like those developed at Johns Hopkins Applied Physics Laboratory with Darpa money, receive inputs from an electrode array on the motor cortex, which controls voluntary movement. But that approach does not provide the level of control over how limbs minutely adjust and react that researchers previously thought it might. And if your brain can’t capture muscle memory, it can’t convey a learned response to an artificial arm. The arm must learn on its own.
This is where computer vision and deep neural networks come in. Machines are far better than humans in making the complicated calculations of distance, speed, force, and shape underpinning motor muscle memory. So researchers are training artificial limbs to make decisions once left to the people using them—things like how quickly to accelerate toward a cup of coffee, and what kind of grip to make to pick it up.
In a study released Wednesday in Journal of Neural Engineering, British scientists used more than 500 images of graspable objects, each categorized into one of four possible grips, to train an artificial vision system. Then they mounted a camera on a prosthetic arm, which used computer vision algorithms to “see” and grab the object 80 percent of the time without its users physically stimulating their muscles to adjust the grip. The researchers used a myoelectric arm, which works by placing electrodes on the skin to detect the nerve signals an algorithm translates into instructions for the arm. Myoelectric prostheses are the type widely available to 100,000 or upper limb amputees in the US.
Neural control is not yet ready for prime time, but computer vision is already making existing artificial arms significantly more agile—up to 10 times faster than anything on the market. There are still limitations of course. Grip dimensions aren’t automatically adjusted for size, for example, meaning such a hand would make the same motion for an 8-ounce latte and a 24-oz drip coffee, requiring the user to make fine adjustments. Such an arm also struggles as the object moves further away, because it “sees” them as smaller than they actually are.  But bioelectronic engineer Kianoush Nazarpour, who led the study, says that writing software to handle such challenges seems relatively straightforward. “The system could easily become more intelligent,” he says.
A True Bionic Limb Remains Far Out of Reach
It has to if science is to create a truly bionic arm. A growing number of researchers at places like Johns Hopkins, the University of Pittsburgh, and Carnegie Mellon are combining vision learning with brain-computer interfaces. The idea is to let users make intuitive decisions and lead the prosthetic to the general area, then allow the machine within the machine take over. The computer can answer niggling questions like “Where is the object? How far am I from the object? How wide must I open the hand to grasp it? How much force is required to lift it?” That more closely mimics how the body works anyway. After all, the brain says, “grab that,” not “rotate your wrist 46 degrees and pinch downward with .38 newtons of force.”
But that’s also why it’s going to be tricky to figure out. “Seamlessly integrating neural control with automatic control is more art than science,” says Steven Chase, a neural engineer at Carnegie Mellon. “It’s going to be a lot of trial and error.” Which means making a fully bionic arm will require more machines, and more brains, too.
The challenge that awaits Eliud Kipchoge, Zersenay Tadese, and Lelisa Desisa, the elite athletes who this weekend will attempt the first sub-two hour marathon, defies characterization. The metrics alone are daunting: Breaking the two hour threshold will mean maintaining an average pace of at least 13.1 miles per hour. (For reference, the current world record, 2:02:57, translates to an average pace of 12.786 miles per hour.) But that number fails to convey the psychological and physical fortitude needed to pull it off. No... to fully appreciate what it will take to complete a two-hour marathon, you really have to try running one yourself.
So that’s exactly what we did. For the video above, we borrowed a treadmill and invited members of WIRED’s running club to see how long they could cruise at 13.1 miles per hour. Then, to put our feeble attempts in perspective, we spoke with Michael Joyner, an expert on the physiology of elite athletes who’s been studying the limits of marathon running for decades. In the video above, he helps explain why a two-hour marathon pace feels like sprinting to us, and a brisk jog to Kipchoge, Tadese, and Desisa—and why one of these men could make history this weekend.
Oh, and be apprised: If you want to try this for yourself, maybe don’t do it on a treadmill. (Turns out this is kind of dangerous!) Instead, head to your local track. Lace up your shoes. Take a few laps at a slow, steady roll, to get your blood moving. Then, whenever you’re ready: Pump your arms, let loose your stride, and start your timer. You’re shooting for 17.25 seconds or less for the first 100 meters; 69 seconds or better for your first 400. Still running? Impressive. Now just hold that pace for another 104 laps.
You will fail—probably spectacularly! That’s OK. When your body inevitably gives out, you’ll hobble away with a newfound respect for the trials that awaits Kipchoge, Tadese, and Desisa this weekend. Granted, they’ll be running their race under ideal conditions, in highly customized—and highly controversial—shoes. But let’s be honest: They’re gonna need all the help they can get.
After so many years in the public eye, skunks have lost their pizzazz. It’s not their fault, it’s just that we’ve all forgotten how bizarre they are. Very few animals can fire sulphurous fluids out of their bums to incapacitate their foes, after all. Very few. But good on skunks, really, for keeping it weird.
One particular variety, the western spotted skunk—which balances on its front legs before it sprays you, as if that’s a charming consolation—just got even weirder. In a study published today in the journal Ecology and Evolution, researchers report that the two-pound terror has evolved into three genetically distinct groups, called clades, in an intriguing way: not with geological isolation (the classical impetus for getting populations to diverge genetically) but with climatic isolation. That is, dramatic climate change led to a genetic splintering of the species.
To Save an Endangered Fox, Humans Turned Its Home Into a War Zone
This Jay Is Evolving in a Very, Very Weird Way
Something Other Than Adaptation Could Be Driving Evolution
It’s actually fairly easy to get a new species. Just run a river or a mountain range through a population, splitting it in two. In their isolation, the groups will eventually grow so genetically distinct that they can no longer mate and produce offspring. Boom, two new species. The spotted skunk is kind of up to the same thing, though it hasn't diverged enough to become new species, but instead three clades: western (California, Nevada, Baja California), Arizona, and east-central (Texas and Mexico). Though the clades existed in different geographical areas, they weren't necessarily cordoned off from each other by geological boundaries.
By melding climate models and genetic work that showed when the spotted skunk began diverging, the researchers determined that the three clades likely got stuck in isolated pockets of actually habitable habitat during the Pleistocene Ice Age. “The idea is that these suitable conditions would contract when glaciers were expanding—it was cooler periods—and then expand during the interglacial periods,” says mammalogist and study co-author Adam Ferguson of the Field Museum.
The spotted skunk's divergence began about 1 million years ago, and continued as glaciers in North America expanded and contracted over millennia. "Unlike the anthropogenically induced climate change we are experiencing today, the change in temperatures and rainfall patterns was more gradual," Ferguson says, "occurring over thousands to tens of thousands of years." These fluctuations as the glaciers moved in and out probably created suitable wooded habitats for skunks, and destroyed others, as groups of the creatures evolved in isolation.
Really, it’s not hard to see how this could come about. Say a forested area started drying out, and grasslands took over for dying trees. “The western spotted skunks really depend on cover and thick areas for protection from aerial predators,” Ferguson says, “and so crossing these open grasslands might not have been possible for them per se.” Western America’s newfound plains were just as restrictive for the skunk as new rivers or mountain ranges would have been.
The beauty of it all is that scientists can use this data to get a better picture of a disorderly climatic future. “By projecting into the past and understanding what happened to this species, it could give us an idea of how changing climates of the future could potentially change at least the distribution of suitable areas for this species,” Ferguson says.
The spotted skunk’s evolutionary journey is also a reminder that climate change affects different creatures in different ways. Warming oceans are definitely bad for coral, for instance. But other species will adapt to a planet in flux, like the spotted skunk did during the Pleistocene.
Problem is, it’s not just human-made climate change that’s the issue, but human-made everything. Urban development in particular threatens mammals all over the world. “If there's bigger freeways, and all these other things dividing up the land, it's going to be harder for small populations to persist,” says ecologist Craig Benkman.
But here's to the continued survival of the spotted skunk. I for one am glad it got its groove back.
To save the endangered island fox and its home off the coast of California, scientists went to war on invasive species like feral pigs and aggressive ants.
In the background of Colorado Springs, Pikes Peak dominates the sky. But just to that mountain’s southeast looms another geological ripple. Cheyenne Mountain—a rounded, rocky thing that rises 9,565 feet above sea level—looks wild and quiet. But deep inside the mountain, a crew of humans toils in one of the nation’s most secure military installations. Shielded by 2,500 feet of granite, these people gather and analyze data from a global surveillance system, in an attempt to (among other, undisclosed things) warn the government’s highest officials of launches and missile threats to North America.
Their military mole-city, completed in the mid-1960s amid Cold War worries, is—when fully buttoned-up—highly resistant to nuclear bombs, electromagnetic bombs, electromagnetically destructive behavior from the sun, and biological weapons. It’s designed to do its job, and let those inside do theirs, in the worst of worst-case scenarios. And with escalating fears about North Korean aggression and nuclear capabilities, Cheyenne Mountain’s ability to predict and survive a nuclear attack resonates more than it did just a few months ago.
As I drive up the hairpinned road toward the entrance to the mountain, made famous in fictional form by War Games and Stargate, signs warn me off with increasing aggression. But I'm allowed: I'm here for a rare tour of the mountain’s innards.
When I arrive at the visitor check-in, Fox News plays on the overhead TV. A sign beneath says not to change the channel, and a uniformed officer reads me a document that says I can't have explosives and that the employees can use deadly force to protect the site. Fair enough.
Soon, badge on blazer, fully briefed, I walk with four escorts—two men who are civilians and two women who are military officers—toward this constructed cave. It is, perhaps, the place on this planet most able to cut itself off from the rest of Earth. And the hardest part for those who work there is not spending all their time subterranean but knowing that in those worst cases—of which "North Korean nukes" is the example most used during my visit—everyone they care about will be outside the thing that’s keeping them safe.
Much of that safety comes from the complex’s very underground-ness. And given that miners had to excavate 693,000 tons of granite to make the Cheyenne Mountain Complex, you might expect its entryway to wow. But the mountain itself is so tall, so sheer, that the 22-foot, two-lane arch leading to the north tunnel looks especially puny by comparison.
Rusty Mullins—deputy director of the Air Force’s 721st Communications Squadron—is leading this tour. He walks down the edge of the tunnel’s asphalt road as he talks about the site, concrete barriers forcing him at intervals to step back onto the sidewalk. The granite makes a half-cylinder around us, bolts knocked into the rock like some kind of sadistic climbing gym.
People get used to these depths, the disconnection. “You learn to live without any sky,” he says, “without any outside but what’s on TV.” There's a lot of TV, though—sets in the work rooms that show the world beyond that arch.
The tunnel curves ahead of us, a skew that will route nuclear (or whatever) material and send it out through the south entrance. The blast doors that lead to the complex's buildings branch off from the tunnel at around 90 degrees, so any material will glance off rather than slam into them.
The Subtle Art of Watching North Korea Build Nukes
North Korea Probably Can’t Strike the US Yet—But It’s Still Plenty Scary
Visit the Nuclear Bunkers That House a Million People in Beijing
We pass through one open 25-ton door—it'll get closed in the event of a potential or impending threat to the area—and enter a rock-walled room with a second such door at its far end. A slight breeze blows by. It's coming from deeper in the mountain, the result of a purposeful over-pressurization so radioactive or bio-particles won’t seep inside the complex. They’d have to make that perpendicular turn and then swim upstream. And they won’t. That’s physics.
Back when humans were cold warring, one of these doors stayed closed. When both get shut, their uber-deadbolts and substantial concreteness keep everything out. Today, these main blast doors remain open unless something truly terrible and threatening happens (like 9/11, the last time the gates shut for serious) or the employees do a drill: what they call a button-up scenario, a practice as much human as mechanical.
Because it’s not enough for the mountain’s welded-metal buildings to sit on springs that can take a nuclear or earthquake hit, which they do, or for its pipes to be bendy, which they are. It’s not enough for the managers to know they have 6 million gallons of water stored in pools carved right out of the rock, or 510,000 gallons of diesel. They have to know the humans can do their jobs—best of times, worst of times, regardless of how sad or scared they are. And the electronics that let them do those jobs have to continue functioning, even as they're cut off from an outside that, in a real emergency, might not have working electronics.
Mullins points inside the second door. Everyone’s “button-up bags” (essentially sleepover kits) are already inside. Locked in, people share bunked cots. They eat MREs—meals ready to eat, whose calorie-dense contents are almost as indestructible as the complex itself. They breathe filtered air that comes in through blast valves. Their lives run on six generators, an internal 10.5-megawatt power plant (nearby, there’s a giant door that says, “Without power, it’s just a cave.”). Any supplies they need come from cabinets and cages that they call “Wal-Marts,” where they’ve stashed extra fan belts and connectors and whatever else.
Mullins leads us through that second blast door, where an awning like those fronting old apartments juts from the first of 15 buildings. “WELCOME TO CHEYENNE MOUNTAIN COMPLEX,” it says. When the employees enter this building during a button-up—real or contrived—they can’t go back out. And no one can come in.
Inside, we cross little walkways that can move independently of the buildings. These structures don't sit together in some giant cavern: They're encased in a series of tunnels. The trim color changes from building to building, so you know where(ish) you are. As we walk, we pass a regular medical clinic, a dentist, a self-checkout store, and also the world’s most secure Subway. (One assumes that in the event of a long-term lockdown, the inhabitants would not be eating so fresh.) Down one hallway is Norad’s alternate command center, where they’ll go if shit slams into fans. We do not go inside.
Mullins leads us through staircases and hallways into a gym set up for spin class. On a normal day, an instructor might be yelling about cadence over some jacked-up pop song. But if there’s a bombing or an earthquake, this exercise room morphs into a hospital. The curtains at the front, which I hadn't noticed at first, would close over medical bays. The area would fill with bleeding people clutching broken arms and medical responders to treat them—as fast as possible, so they could get back to work. “Because you have a job on the mountain,” says Mullins.
“I’m gonna depress you,” Mullins had said before we even went inside the mountain. He described his family—wife, kids. And how for his whole career here, he’s had to tell them that if there’s some kind of Event, they’re on their own. “I’m going to be in the mountain doing my job,” he says to the people he loves most, “and I can’t help you.”
That’s a big reason Cheyenne needs run-downs. People have to practice leaving their families, friends, and favorite Starbucks locations, imagining that everyone could burn up in a nuclear blast, be left in an apocalyptic electromagnetic pulse scenario, or become bio-weapon infected.
They usually hold it together. On 9/11, just a few of the hundreds locked inside wanted to go home. One was distraught enough that officials took him to the chapel so he could sit, think, calm down. The chapel—which comes with a chaplain, whose support is bolstered by mental health services—is bland, non-denominational. It’s the kind of quiet that makes your ears ring. After 10 or 15 minutes, the guy came out. He went back to work.
They all do. “No matter how bad it is outside, I’m going to do my job here," Mullins says. He says it like a mantra.
That work all these people need to get back to—it involves computers. And their physiological aliveness wouldn't do much good if the computers were dead or cyber-compromised, would it? That's why the complex's shielding, both physical and digital, matters so much.
If an electromagnetic pulse hit near the mountain, it could knock out everything around Colorado Springs. But inside the complex, computers and the lines feeding into them stay safe. The rock sheath attenuates the electromagnetic waves, as do the metal buildings: They're giant Faraday cages.
Mullins leads us to a room where that protection matters a lot: the Global Strategic Warning/Space Surveillance Systems Center. Inside, the occupants have kindly put up simulated surveillance screens so I can't see what's actually going on with the world. A gray-haired civilian is in charge of smiley young staff sergeants. They're all standing in front of a herd of monitors, bluelit by a wall of screens showing (fake) maps, (fake) aircraft, (fake) bar graphs. It's exactly what you'd imagine would be inside a place like Cheyenne Mountain if you were going to make a movie about it, which people have.
Down here, they spend their time watching the skies they can’t actually see for evidence of missiles, suspicious space behavior, launches, tests—is that heat signature from North Korea a threat, or nah? They ingest information and determine what’s going on that’s good, bad, neutral, what should go up the chain to the decisionmakers. And it's here that they decide when it's time to shut Cheyenne's doors.
As we make our way back toward those (still open) doors, after three or so hours playing cavepeople, I ask how they isolate themselves digitally. If their whole operation relies on data from beyond the mountain’s mouth, how can they be so sure nothing else can get in?
We're outside the buildings now, in one of the rooms that just has granite for walls. Well, Mullins says, some of their systems don’t connect to outside networks, ever. That’s failsafe. For the rest, he turns to one of the uniformed personnel with us—Major Mica Myers, who’s been pretty quiet, cutting in to add details here and there to Mullins's narration. But I've felt the whole time like she was watching over this excursion. Turns out, I might not be wrong.
“I’m gonna use the 'DCO' word,” Mullins says. “OK, you think?”
She agrees.
“Defensive cyber operations,” he says. He points to Myers and says she, director of operations for the 721st Communications Squadron, leads them.
While they won’t say what, exactly, “defensive cyber operations” means for Cheyenne Mountain, the Joint Force Commander’s Guide to Cyberspace Operations gives a general definition: They “provide the ability to discover, detect, analyze, and mitigate threats, to include insider threats.” Basically, they identify and freeze attempts to infiltrate their cyber systems. Mullins and Myers say they have plans for various scenarios, and proof that the plans work. Proof, they repeat. And although they don't say what this means, it seems safe to assume they have repelled infiltrators either real or simulated.
Mullins looks around at the rocky walls that surround us as we turn back toward the tunnel, toward the outside world.
“We’re having a defensive cyber conversation in the middle of a cave,” he says. He laughs. That is, though, kind of Cheyenne Mountain’s whole thing: Protecting high-tech stuff with the planet itself, a womb around solid civil and anti-hack engineering.
Mullins steps back out into the tunnel. “Does it look any different going this direction?” he asks.
I can see actual light at the end of this actual tunnel, so yes.
When we step through the arch, the world is different from the way we left it. It has, as it always will (for better or worse or worst), gone on without us. The air feels colder, even though it’s noon now. Charcoal clouds hover over the eastern plains. There’s a beat of thunder. A fleet of military sirens goes off 1,000 feet below, back in town. This is just a test, they tell me.
The new budget is here! The new budget is here! To the relief of scientists and perhaps the chagrin of the White House, the omnibus bill that’ll keep the government governmenting until September doesn’t look anything like what President Donald Trump asked for. Like, it’s upside down. Trump wanted to crush the National Institutes of Health; they’re getting a $2 billion increase. Trump wanted to eviscerate the Department of Energy, especially the blue-sky ARPA-E programs, but it’s getting a small bump up. And so on.
In fact, as budgets go, this one looks very much like the last decade or so of federal science funding. Which is the problem. Because budgets are statements of priorities. What the government pays for is what the government thinks is important. And the US government is getting that wrong.
Focusing on health and medicine seems logical. Ideally, biomedical research lessens human suffering. Hosing down the NIH with $2 billion for priorities like Alzheimer’s and the Precision Medicine Initiative might save lives down the line, and it’s also the kind of thing that aging members of Congress can wrap their heads around most easily, because old people are scared of dying. (The average age of a congressperson is around 60; voters over 71 years old typically have huge election turnout.)
But the $246 million the pharmaceutical industry spent on lobbying in 2016 probably didn’t hurt. Perhaps neither did the $95 million from hospitals, the $84 million from health professionals, or the $78 million from health services companies. (Those numbers are all from the Center for Responsive Politics, a nonpartisan, nonprofit group that tracks money in politics.)
If you really wanted to make a dent in quality of life and the incidence of disease, you’d focus on prevention—on getting people to quit smoking, maybe exercise, stop eating so much sugar. That’s not as glamorous as sequencing the genomes of a million people, I’ll grant you. You certainly wouldn’t roll back rules governing vaping, which is what the Food and Drug Administration just did. You’d also spend a lot of money on disease surveillance and fighting outbreaks of exotic, would-be pandemics where they happen. Yet as the journalist Laurie Garrett has been pointing out on Twitter, leaked pages from the proposed fiscal 2018 budget—the one that comes after this fiscal 2017 omnibus—zero out health efforts by the State Department all over the world.
Meanwhile, to grow the American economy and stave off a climatic apocalypse, prioritize physics and engineering. Don’t, for example, freeze existing contracts at ARPA-E. More efficient sources of renewable power, better ways to store it, and better ways to distribute it are the industries of the future, and the sector is adding well-paid jobs faster than any other in the US economy. Yet the feds are averting their eyes. The omnibus all but flatlines renewables and tosses $50 million to “advanced coal technology.” (Oil and gas lobby in 2016: $119 million. Alternative energy lobby: $6.9 million. I’m not saying! I’m just saying.)
You might even use the National Science Foundation, DOE, and other grant-giving agencies to widen job opportunities for physicists and engineers so that they won’t find jobs at Facebook or on Wall Street so compelling. I’m wandering out onto a limb here, but if you think that researchers should be working on more compelling problems than algorithmic stock trading or a household robot that can tell you if your shoes match your jacket, well, give them a reason.
Observation Deck: Science vs. Politics — In a Way
Now, to be clear: A half-million people die of cancer in the US every year. It is totally kosher to decide to fund the cancer moonshot (even though years of research hasn’t made as much of a dent in cancer as reducing the number of smokers seems to have). If national policy is going to make a priority of unraveling the gnarly problem of how the human brain works—to deal with neurodegenerative disorders, to make brain-computer interfaces possible, to hasten the singularity, whatever—then the Brain Initiative is a solid priority.
But funding those things means less money for understanding genomes, for turning yeast or some other malleable lifeform into the factory of the future, cranking out new antibiotics or spider silk or biodegradable plastic. This is how budgets work. The US science budget has made domestic biomedicine more urgent than industry and technical innovation. When success in prevention and public health interventions are cheaper and more effective than innovative new drugs, that might not be the right move.
Amazon spent 10 percent of its revenue on R&D in 2016. Alphabet, Google’s parent company, spent 14 percent. At Intel it was more than 20 percent. The omnibus bill spends 0.81 percent of US gross domestic product on R&D including what the country pours into defense. You want to run the country more like a business, as Republicans are fond of saying? Start by spending money on innovation at levels that turn this article into a straw-man argument. Fund it all.
Engineering a tomato resistant to a pernicious fungal disease doesn’t seem like it’d be the easiest part of a plant pathologist’s job. But compared to getting that tomato to market? It’s a snap.
At least, that’s how Sophien Kamoun sees it. Kamoun studies plant diseases at the Sainsbury Laboratory in England, and in March his team published a paper describing a tomato they’d tweaked. Using the gene-editing technique Crispr/Cas9, Kamoun’s group snipped out a piece of a gene called Mildew Resistant Locus O, or Mlo. That deletion makes the tomato resistant to powdery mildew, a serious agricultural problem that takes a lot of chemicals to control.
Kamoun’s “Tomelo” actually looks a lot like a naturally occurring tomato, a mutant with the same resistance. “At least in the tomato plants we have, there was no detectable difference between the mutant and the wild type,” Kamoun says. “Obviously we’d need to do more detailed field trials, but there was certainly nothing obvious.”
But for now, that’s where Kamoun’s work stops. European regulations make the tomato essentially illegal—he and others can do the science, but probably can't get it to field trials, and certainly can't get it to market. “There’s more clarity in the US. One could probably get approval. But in Europe, it’s a big question mark,” he says. “I’m very frustrated by this, I have to be honest. Scientifically this plant is no different from any mutant we’d get from traditional breeding or traditional mutagenesis. I really don’t understand what the problem is.”
If you're wondering how agriculture is going to feed 10 billion people on a climatically chaotic, hotter, more disaster-prone planet, you might not understand the problem, either.
A Monsanto Exec’s Selfie-Snapping, Live-Tweeting Campaign for GMO Acceptance
China Hates GMOs. Problem Is, China Really Needs GMOs
Monsanto’s Newest GM Crops May Create More Problems Than They Solve
Figuring out where genetically engineered plants and animals fit into safety regulations hasn’t been easy on either side of the Atlantic. Philosophically, US regulators take a “substantial equivalence” perspective; if the new thing looks basically like an old thing that’s unregulated, or already well-regulated, it’s probably fine. The EU goes for the precautionary principle: If it comes from a genetically modified organism, it gets regulated.
“Our regulations don’t cover everything. They cover a group of organisms produced a certain way using plant pests,” says John Turner, director of Biotechnology Risk Analysis Programs for the US Department of Agriculture’s Animal and Plant Health Inspection Service. What he means is, his group has traditionally looked at genetic modification done with viruses or bacteria that injected genes into other organisms, or even with “gene guns” that literally punched genetic material into cells.
If that sounds strict-ish, well, under those guidelines the US has approved thousands of genetically modified organisms. But modern gene editing techniques like Crispr have gotten out ahead of the regs. If "transgenics" is the old-school insertion of genes from one organism into another, proponents of more subtle engineering approaches would like you to call the insertion of a gene from one strain of a species into another "intragenics" and the alteration of a gene within a species "cisgenics." That's some elegant neologizing, and more importantly those new words don't have the same regulatory history. “Some things are simply outside the scope of our regulations,” Turner says. “With respect to genome editing, these things are still pretty new, and we’ve been making decisions for the most part on a case-by-case basis.”
USDA has approved about 30 organisms made in these new ways, most recently a non-browning mushroom and a waxy corn. As a national policy, it’s a little ad-hoc. Supposedly, new rules are on the way.
In Europe, though, regulators have essentially made all genetically engineered foods illegal … by avoiding a decision as to their legality. “They haven’t figured it out yet, and it’s mainly a political issue. Everything depends on how you define gene editing,” says Mauro Vigani, a senior research fellow at the Countryside and Community Research Institute.
In 2007, the European Union set up working groups to clarify how newer cis- and intragenic techniques fit into its existing prohibition of most genetically modified foods. That group put out a report in 2012, but only some of recommendations were made public. The commission was supposed to report again in 2015, but delayed the release of its findings. It delayed again in 2016. So France went to court, asking the European Court of Justice for a ruling … which is supposed to hit sometime in 2018. Maybe 2019.
Meanwhile, European agronomy is on hold. The first paper on Crispr, arguably the easiest and cheapest of the gene editing techniques, came out in 2012. No matter what you think of the intellectual property chaos around it, that’s a long time to go without government rules. “At the end of the day, there’s no regulation of new plant breeding techniques either on gene editing or cis- or intragenetics, and that means that no new crops obtained with this methodology are authorized,” Vigani says.
That’s not necessarily crazy. It’s worth worrying about the unintended consequences of releasing engineered foods into the world. “We look at whether it’s more susceptible to diseases or insects. We look to see whether it’s producing a compound that might be toxic to nontargets,” says Turner of the USDA. “We also consider weediness, whether it could make the crop into a weed, or whether there are wild relatives that could become weedy.”
Kamoun tried to get his tomato strain into a well-known tomato genetics repository at UC Davis, but the director turned him down—worried, apparently, about the repository’s policy on sharing research and samples. “Unless there’s a reliable database that lists the current regulatory regime by country, we’d be in the uncomfortable position of doing the research ourselves, then if we got it wrong, being responsible for exporting a product considered transgenic to a country that does not accept them,” the director, Roger Chetelat, wrote. He suggested Kamoun get in touch with the USDA.
An email to the USDA, Kamoun says, went unanswered—though it’s possible he emailed the wrong people.
Meanwhile, the Europeans kicking la boîte down la rue really bums out their industrial, scientific, and farm sectors. “It’s already 10 years that EU regulators and politicians discussed this topic, and they haven’t come to a decision yet,” says Petra Jorasch, manager of plant breeding and innovation advocacy for Euroseeds, the European seed association.
And meanwhile, what’s a researcher like Kamoun supposed to do? “He can try to approach his administration, I guess,” Jorasch says. She doesn’t sound hopeful.
Kamoun doesn't, either. After I emailed and told him all this—nothing he didn't already know, I suspect—he sent a note back. "My dream is that these new traits get rapidly acquired. I could of course start the paperwork and apply in the US, but this would be so slow as it's a case by case situation and we can now generate many useful lines," Kamoun wrote. "But yes, clearly the US is the only option now."
When hosting a party where genetically modified foods are what’s for dinner, is it proper etiquette to warn your GMO-averse friends ahead of time? Mr. Know-It-All offers sage advice on how to handle.
Energy is the answer to so many questions. Pull back a spring-powered toy car and rolls forward. Why? Energy. You used your phone all day and it suddenly shut down. Why? No energy. A child eats six bags of M&Ms and runs around screaming and giggling because he has energy. But then he doesn't want to clean his room because he has no energy. But what is energy?
Ah... that is the real question. To answer it, let me start with a story about three friends, Alby, Bobby, and Cami. I completely made those names up.

Alby has 10 dollars and gives it all away. Bobby ends up with $5 and Cami gets $4.99.

Do you see the problem? Yes, there is a missing penny. Where is it? When Alby gives away all of his money, that penny must end up somewhere. Even if he dropped it, it still exists. The total amount of money must add up to $10.
If you like, you can all this rule the conservation of money. No matter what happens, the amount of money must be the same before and after something happens. Oh, and here is something else to know about money: It's not real. Oh sure, the bills and coins are real, and if you lost a $10 bill you'd be unhappy about it. But money merely represents something else. You could use a dollar to buy a ball or a snow cone, but those things exist even if you don't have money. Money simply exists as a useful way for humans to trade things.
OK, what does this have to do with energy? Well, energy is just like money. Let me start with an example. Suppose I load a ball into a spring-powered ball launcher and fire that ball straight into the air. Here are the three phases of this action:

The launcher fires the ball with a compressed spring. That spring stores energy, called spring potential energy. Release the ball and it moves under some unspecified speed as the spring releases its energy. Now the ball has energy, called kinetic energy. When the ball reaches its maximum height, its speed is, for just an instant, zero. At that point, the ball has gravitational potential energy.
The Wacky Physics of Firing a Ball Out of a Moving Cart
Let’s Do the Shocking Physics of Why Power Lines Sag
How Hard Does Thor Hit Hulk in That Ragnarok Trailer? Let’s Do the Physics!
Now, if you calculate the potential energy before releasing the ball, it might be something like 10 joules. (A joule is a common unit of energy, named for British physicist James Prescott Joule.) Launch the ball and it might have nine joules of kinetic energy and 1 joule of gravitational potential energy because it is now aloft. As the ball rises, it has less kinetic energy and more gravitational potential energy. Once the ball reaches its maximum height, it has 10 joules of gravitational potential energy. Although the type of energy changes, the amount never does. Energy is just like money—it is conserved.
How about another example. The nucleus of an atom usually features some combination of protons and neutrons. A version of carbon carbon 14 has six protons and eight neutrons, but it is not entirely stable. Over times, the carbon 14 atom experiences radioactive decay (this explains how carbon dating works) through a process called beta decay. This decay leaves a nitrogen atom called nitrogen 14 and an electron. They both possess kinetic energy, because it turns out that mass is another type of energy from the famous expression E = mc2.

Now for the cool part. Based on the difference in mass of carbon and nitrogen, beta decay lets you find the total amount of energy. Let's say this was 10 joules. You can also measure the speed and calculate the kinetic energy of the electron. Suppose you got a value of 3.99 joules. Doing the same with the nitrogen, you might get an energy of 6 joules. Yes, there is a missing 0.01 joule of energy. Does this mean that energy was not conserved? No, it means the missing energy must be somewhere else.
This actually happened, and in 1930, Wolfgang Pauli suggested that another particle in the beta decay process held that energy. He was right—that particle is called a neutrino. That's crazy when you think about it. Humans arrive at this calculation because it is useful and because the total energy must be the same before and after something happens. When it appears that energy is not conserved, it is only because of some previously unknown factor.
There you go—a simple answer to the question, "What is energy?" I hope you understood it, because I wrote it for the Flame Challenge. That's an international competition where scientists explain something in terms an 11-year-old would understand.
Charlotte Drury, Maggie Nichols, and Aly Raisman talk to WIRED about the skill, precision, and control they employ when performing various Gymnastic moves and when training for the Olympics.
Radioactive material gets a bad rap, what with radiation and fallout and nuclear waste and all. But it offers some practical uses. One of the coolest (OK, maybe the coolest) is using radioactive carbon to determine the age of old bones or plants. To understand this, you must first understand radioactivity and decay.
When an element undergoes radioactive decay, it creates radiation and turns into some other element. Of course, the best way to understand something is to model it, because the last thing you want to do at home is experiment with something radioactive. Here are two ways to model radioactive decay.
Before doing any modeling, you must first understand one key idea: Each atom in a sample of material has an essentially random chance to decay. The rate of decay depends upon the number of atoms you have. This means that as more of these atoms decay you have a lower rate of radioactive decay. I know can be hard to wrap your head around, so let's model it with a six-sided die.
Start with 100 objects. You can use Lego bricks, pennies, beans—anything you can easily count. Then find a six-sided die. You will roll it for each of the 100 objects. If you roll a one, then that object decays and turns into something else. Every time you roll a one, put that object into a separate pile. Count the remaining objects and repeat the process until half of them have decayed. This is called the half-life—the amount of time required for one-half of a given number of atoms to disintegrate.
My son and I used colorful plastic tiles. We only had 80 though.

It took a while, but we finally got pretty close to 40 tiles left. The plot of the number of tiles as a function of the number of turns looks like this:
Notice that in the first run, 11 tiles decayed. The last run started with 49 tiles and only six decayed. So you can indeed see that the number of objects that decay depends upon the number of objects you have. But it's hard to see this with so few tiles. What if I start with 1,000 tiles? It would be a huge hassle to roll a die 1,000 times. Instead, let's write a computer program.
You can easily write a Python program to simulate rolling a die 1,000 times. Here's the basic outline of the code:
Here is the program. Press play to run it and click the pencil to edit or review the code. Notice the visual display of the spheres and a graph below that.
I thought about making the yellow spheres turn another color (to more accurately represent radioactive decay), but making them vanish more closely mimics the die-rolling exercise. Now for some homework. You might need to modify the code to find the answers, but don't worry. You can't break it. If you mess it up beyond repair, just reload the page and start over.
If you want one more homework question, I have one. You can derive this if you like, but here is the mathematical model for the decay of some atoms.

In this expression, N0 represents the starting number of atoms, r is the probability that something decays (per second) and t is the time (in seconds). Do the data above agree with this mathematical model?
I consider carbon dating one of the coolest applications of radioactive decay. You probably know about it from paleontology. Suppose you find some old bones. Of course the first question you might have about these bones is how old they are. You can determine that with carbon dating. OK, technically carbon dating doesn't tell you the age of the bones, but rather when the animal they come from stopped breathing.
Carbon dating relies upon the presence of carbon-14, an isotope of carbon. To understand an isotope, you need to know a little about the structure of atoms. Atoms, of course, are made of three things: electrons, protons and neutrons. If the atom is neutral, it has the same number of electrons and protons. If you start with the simplest element, you have one proton and one electron. You know this element as hydrogen. Add one neutron and you have hydrogen-2, an isotope.
The most common form of carbon is carbon-12. It has six neutrons, six protons and six electrons. It is stable and does not decay. You see a lot of carbon-12 in atmospheric carbon dioxide. Some of that carbon-12 gets exposed to cosmic radiation and turns into carbon-14, which has eight neurons. Carbon-14 is radioactive, with a half-life of 5,700 years.
So what makes this useful? Plants. Plants take in carbon dioxide during photosynthesis, and end up with some small amount of carbon-14. Animals eat those plants, and then other animals eat those animals, and soon everything has some amount of carbon-14. And when any of those things dies, they stop taking in carbon-14. (Yes, I know, they stop taking in everything, but I'm only interested in carbon-14 here.) The carbon-14 within that plant or animal begins to decay. By measuring the relative abundance of carbon-14 vs. carbon-12 (and, technically, carbon-13), you can work backward to figure out when that plant or animal stopped taking in fresh carbon-14. In other words, you know when it died.
Want an example? I'll make up something similar to carbon dating so you can see what is going on. I'll build a model using a lot of spheres. Most of them are yellow, but 20 percent of them are blue (OK, technically, they're cyan). The blue spheres are radioactive, and decay at the same rate I used in the example above. Click play to see what that might look like.
Again, I made radioactive spheres disappear when they decayed. This is fine, because when carbon-14 decays, it produces nitrogen-14. It's no longer carbon. But you could imagine that if you knew that the sample started with 20 percent blue spheres and you knew their half-life, then you could determine the age by examining one frame from the animation. This is exactly how carbon dating works, but with dinosaurs instead of models.
Partisan gerrymandering—the practice of drawing voting districts to give one political party an unfair edge—is one of the few political issues that voters of all stripes find common cause in condemning. Voters should choose their elected officials, the thinking goes, rather than elected officials choosing their voters. The Supreme Court agrees, at least in theory: In 1986 it ruled that partisan gerrymandering, if extreme enough, is unconstitutional.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Yet in that same ruling, the court declined to strike down two Indiana maps under consideration, even though both “used every trick in the book,” according to a paper in the University of Chicago Law Review. And in the decades since then, the court has failed to throw out a single map as an unconstitutional partisan gerrymander.
“If you’re never going to declare a partisan gerrymander, what is it that’s unconstitutional?” said Wendy K. Tam Cho, a political scientist and statistician at the University of Illinois, Urbana-Champaign.
The problem is that there is no such thing as a perfect map—every map will have some partisan effect. So how much is too much? In 2004, in a ruling that rejected nearly every available test for partisan gerrymandering, the Supreme Court called this an “unanswerable question.” Meanwhile, as the court wrestles with this issue, maps are growing increasingly biased, many experts say.
Even so, the current moment is perhaps the most auspicious one in decades for reining in partisan gerrymandering. New quantitative approaches—measures of how biased a map is, and algorithms that can create millions of alternative maps—could help set a concrete standard for how much gerrymandering is too much.
Last November, some of these new approaches helped convince a United States district court to invalidate the Wisconsin state assembly district map—the first time in more than 30 years that any federal court has struck down a map for being unconstitutionally partisan. That case is now bound for the Supreme Court.
“Will the Supreme Court say, ‘Here is a fairness standard that we’re willing to stand by?’” Cho said. “If it does, that’s a big statement by the court.”
So far, political and social scientists and lawyers have been leading the charge to bring quantitative measures of gerrymandering into the legal realm. But mathematicians may soon enter the fray. A workshop being held this summer at Tufts University on the “Geometry of Redistricting” will, among other things, train mathematicians to serve as expert witnesses in gerrymandering cases. The workshop has drawn more than 1,000 applicants.
“We have just been floored at the response that we’ve gotten,” said Moon Duchin, a mathematician at Tufts who is one of the workshop’s organizers.
Gerrymanderers rig maps by “packing” and “cracking” their opponents. In packing, you cram many of the opposing party’s supporters into a handful of districts, where they’ll win by a much larger margin than they need. In cracking, you spread your opponent’s remaining supporters across many districts, where they won’t muster enough votes to win.
For instance, suppose you’re drawing a 10-district map for a state with 1,000 residents, who are divided evenly between Party A and Party B. You could create one district that Party A will win, 95 to 5, and nine districts that it will lose, 45 to 55. Even though the parties have equal support, Party B will win 90 percent of the seats.
Such gerrymanders are sometimes easy to spot: To pick up the right combination of voters, cartographers may design districts that meander bizarrely. This was the case with the “salamander”-shaped district signed into law in 1812 by Massachusetts governor Elbridge Gerry—the incident that gave the practice its name. In an assortment of racial gerrymandering cases, the Supreme Court has “stated repeatedly … that crazy-looking shapes are an indicator of bad intent,” Duchin said.
Yet it’s one thing to say bizarre-looking districts are suspect, and another thing to say precisely what bizarre-looking means. Many states require that districts should be reasonably “compact” wherever possible, but there’s no one mathematical measure of compactness that fully captures what these shapes should look like. Instead, there are a variety of measures; some focus on a shape’s perimeter, others on how close the shape’s area is to that of the smallest circle around it, and still others on things like the average distance between residents.
The Supreme Court justices have “thrown up their hands,” Duchin said. “They just don’t know how to decide what shapes are too bad.”
The compactness problem will be a primary focus of the Tufts workshop. The goal is not to come up with a single compactness measure, but to bring order to the jostling crowd of contenders. The existing literature on compactness by nonmathematicians is filled with elementary errors and oversights, Duchin said, such as comparing two measures statistically without realizing that they are essentially the same measure in disguise.
Mathematicians may be able to help, but to truly make a difference, they will have to go beyond the simple models they’ve used in past papers and consider the full complexity of real-world constraints, Duchin said. The workshop’s organizers “are absolutely, fundamentally motivated by being useful to this problem,” she said. Because of the flood of interest, plans are afoot for several satellite workshops, to be held across the country over the coming year.
Ultimately, the workshop organizers hope to develop a deep bench of mathematicians with expertise in gerrymandering, to “get persuasive, well-armed mathematicians into these court conversations,” Duchin said.
A compactness rule would limit the range of tactics available for drawing unfair maps, but it would be far from a panacea. For starters, there are a lot of legitimate reasons why some districts are not compact: In many states, district maps are supposed to try to preserve natural boundaries such as rivers and county lines as well as “communities of interest,” and they must also comply with the Voting Rights Act’s protections for racial minorities. These requirements can lead to strange-looking districts—and can give cartographers latitude to gerrymander under the cover of satisfying these other constraints.
More fundamentally, drawing compact districts gives no guarantee that the resulting map will be fair. On the contrary, a 2013 study suggests that even when districts are required to be compact, drawing biased maps is often easy, and sometimes almost unavoidable.
The study’s authors—political scientists Jowei Chen of the University of Michigan and Jonathan Rodden of Stanford University—examined the 2000 presidential race in Florida, where George W. Bush and Al Gore received an almost identical number of votes. Despite this perfect partisan balance, in the round of redistricting after the 2000 census, the Republican-controlled Florida legislature created a congressional district map in which Bush voters outnumbered Gore voters in 68 percent of the districts—a seemingly classic case of gerrymandering.
Yet when Chen and Rodden drew hundreds of random district maps using a nonpartisan computer algorithm, they found that their maps were biased in favor of Republicans too, sometimes as much as the official map. Democratic voters in the early 2000s, they found, were clustering into highly homogeneous neighborhoods in big cities like Miami and spreading out their remaining support in suburbs and small towns that got swallowed up inside Republican-leaning districts. They were packing and cracking themselves.
This kind of “unintentional gerrymandering” creates problems for Democrats in many of the large, urbanized states, Chen and Rodden found, although some states—such as New Jersey, in which Democratic voters are evenly spread through a large urban corridor—have population distributions that favor Democrats.
Chen and Rodden’s work indicates that biased maps can often arise even in the absence of partisan intent, and that drawing fair maps under such circumstances requires considerable care. Maps can be drawn that break up the tight city clusters, as in Illinois, where the Democratic-controlled legislature has created districts that unite segments of Chicago with suburbs and nearby rural areas.
Nevertheless, Chen and Rodden write, Democratic cartographers have a tougher job than Republican ones, who “can do strikingly well by literally choosing precincts at random.”
Since drawing compact districts is not a cure-all, solving the gerrymandering problem also requires ways to measure how biased a given map is. In a 2006 ruling, the Supreme Court offered tantalizing hints about what kind of measure it might look kindly on: one that captures the notion of “partisan symmetry,” which requires that each party have an equal opportunity to convert its votes into seats.
The court’s interest in partisan symmetry, coming after its rejection of so many other possible gerrymandering principles, represents “the most promising development in this area in decades,” wrote two researchers—Nicholas Stephanopoulos, a law professor at the University of Chicago, and Eric McGhee, a research fellow at the Public Policy Institute of California—in a 2015 paper.
In that paper, they proposed a simple measure of partisan symmetry, called the “efficiency gap,” which tries to capture just what it is that gerrymandering does. At its core, gerrymandering is about wasting your opponent’s votes: packing them where they aren’t needed and spreading them where they can’t win. So the efficiency gap calculates the difference between each party’s wasted votes, as a percentage of the total vote—where a vote is considered wasted if it is in a losing district or if it exceeds the 50 percent threshold needed in a winning district.
For instance, in our 10-district plan above, Party A wastes 45 votes in the one district it wins, and 45 votes each in the nine districts it loses, for a total of 450 wasted votes. Party B wastes only 5 votes in the district it loses, and 5 votes in each of the districts it wins, for a total of 50. That makes a difference of 400, or 40 percent of all voters. This percentage has a natural interpretation: It is the percentage of seats Party B has won beyond what it would receive in a balanced plan with an efficiency gap of zero.
Stephanopoulos and McGhee have calculated the efficiency gaps for nearly all the congressional and state legislative elections between 1972 and 2012. “The efficiency gaps of today’s most egregious plans dwarf those of their predecessors in earlier cycles,” they wrote.
The efficiency gap played a key role in the Wisconsin case, where the map in question, according to expert testimony by the political scientist Simon Jackman, had an efficiency gap of 13 percent in 2012 and 10 percent in 2014. By comparison, the average efficiency gap among state legislatures in 2012 was just over 6 percent, Stephanopoulos and McGhee have calculated.
The two have proposed the efficiency gap as the centerpiece of a simple standard the Supreme Court could adopt for partisan gerrymandering cases. To be considered an unconstitutional gerrymander, they suggest, a district plan must first be shown to exceed some chosen efficiency gap threshold, to be determined by the court. Second, since efficiency gaps tend to fluctuate over the decade that a district map is in force, the plaintiffs must show that the efficiency gap is likely to favor the same party over the entire decade, even if voter preferences shift about somewhat.
If these two requirements are met, Stephanopoulos and McGhee propose, the burden then falls to the state to explain why it created such a biased plan; perhaps, the state could argue, other considerations such as compactness and preservation of boundaries tied its hands. The plaintiffs could then rebut that claim by producing a less biased plan that performed as well as the existing map on measures like compactness.
This approach, the pair wrote, “would neatly slice the Gordian knot the Court has tied for itself,” by explicitly laying down just how much partisan effect is too much.
The efficiency gap can help to identify plans with strong partisan bias, but it cannot say whether that bias was created intentionally. To disentangle the threads of intentional and unintentional gerrymandering, last year Cho—along with her colleagues at Urbana-Champaign, senior research programmer Yan Liu and geographer Shaowen Wang—unveiled a simulation algorithm that generates a large number of maps to compare to any given districting map, to determine whether it is an outlier.
There’s an almost unfathomably large number of possible maps out there, far too many for any algorithm to fully enumerate. But by spreading their algorithm’s tasks across a massive number of processors, Cho’s team found a way to create millions or even billions of what they call “reasonably imperfect” maps—ones that perform at least as well as the original map on whatever nonpartisan measures (such as compactness) a court might be interested in. “As long as a particular facet can be quantified, we can incorporate it into our algorithm,” Cho and Liu wrote in a second paper.
In that paper, Cho and Liu used their algorithm to draw 250 million imperfect but reasonable congressional district maps for Maryland, whose existing plan is being challenged in court. Nearly all their maps, they found, are biased in favor of Democrats. But the official plan is even more biased, favoring Democrats more strongly than 99.79 percent of the algorithm’s maps—a result extremely unlikely to occur in the absence of an intentional gerrymander.
In a similar vein, Chen and Rodden have used simulations (though with many fewer maps) to suggest that Florida’s 2012 congressional plan was almost surely intentionally gerrymandered. Their expert testimony contributed to the Florida Supreme Court’s decision in 2015 to strike down eight of the plan’s 27 districts.
“We didn’t have this level of sophistication in simulation available a decade ago, which was the last major case on this topic before the [US Supreme] Court,” said Bernard Grofman, a political scientist at the University of California, Irvine.
The Florida ruling was based on the state constitution, so its implications for other states are limited. But the Wisconsin case has “potential incredible precedent value,” Grofman said.
Grofman has developed a five-pronged gerrymandering test that distills the key elements of the Wisconsin case. Three prongs are similar to those Stephanopoulos and McGhee have proposed: evidence of partisan bias, indications that the bias would likely endure for the whole decade, and the existence of at least one replacement plan that would remedy the existing plan’s bias. To these, Grofman adds two more requirements: simulations showing that the plan is an extreme outlier, suggesting that the gerrymander was intentional, and evidence that the people who made the map knew they were drawing a much more biased plan than necessary.
Source: Wendy K. Tam Cho, using PEAR algorithm
If the Supreme Court does adopt a gerrymandering standard, it remains to be seen whether it will require evidence of intent, as Grofman’s standard does, or instead focus on outcomes, as Stephanopoulos and McGhee’s standard does.
“Do we believe that districts should come as close as possible to fair representation of the parties?” Rodden said. “If so, we shouldn’t really care about whether [gerrymandering is] intentional or unintentional.” But, he added, “I don’t know where the courts will end up coming down. I don’t think anyone knows.”
The choice has major ramifications. Last year, Chen and David Cottrell, a quantitative social scientist at Dartmouth University, used simulations to measure the extent of intentional gerrymandering in congressional district maps across most of the 50 states; they uncovered a fair bit, but they also found that on the national level, it mostly canceled out. Banning only intentional gerrymandering, they concluded, would likely have little effect on the partisan balance of the US House of Representatives (although it could have a significant effect on individual state legislatures).
Banning unintentional gerrymandering as well would lead to a more radical redrawing of district maps, one that “could potentially make a very big change to the membership of the House,” McGhee said.
That decision is up to the court. But there’s plenty of work left for gerrymandering researchers, from understanding the limitations of their measures (many of which produce odd results in lopsided elections, for instance) to studying the trade-offs between ensuring partisan symmetry and, say, protecting the voting power of minorities or drawing compact districts. Collaboration between political and social scientists, mathematicians, and computer scientists is the ideal way forward, Rodden and McGhee both say.
“We should be encouraging cross-pollination and bringing in outside ideas, and then debating those ideas robustly,” McGhee said.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Time to count down some dangerous volcanoes. I've gone through what might make a volcano dangerous and how I tried to rank dangerous volcanoes, developing a points system based on population, magma type, volcano type, and past large explosive eruptions. Looking at some recent articles about "dangerous volcanoes," my ranking comes to some pretty different conclusions. What my ranking boils down to is what volcano has the highest potential for mass casualties based population, style of eruption and potential for large explosive events.
I'll start with some honorable mentions that fell outside the top 10 (in order of increasing danger): Pululagua (Ecuador), Guntur, Gede-Pangrango and Semeru (Indonesia), Popocatépetl (Mexico), Colli Alban (Italy), Dieng Volcanic Complex and Tengger Caldera (Indonesia), Nyiragongo (DR Congo), and Merapi (Indonesia).
Here are the top 10 (with people living within 30 kilometers and 100 kilometers listed.)
10. Santa Maria, Guatemala (1.25 million/6.2 million): This volcano might be best known for its most active vent, Santiaguito. It has the tendency to erupt explosively with VEI 6 eruption as recently as 1902.
9. Taal, Philippines (2.38 million/24.8 million): Taal is a lake-filled caldera that produced four VEI 4 eruptions in the last 200 years and a VEI 6 eruption only ~5,500 years ago (VEI stands for Volcanic Explosivity Index, and it tops out at 7). Combine that explosivity with abundant water to add to potential explosive eruptions and the large population that could be impacted by ash, and you have a very closely-watched volcano. Taal is monitored by PHIVOLCS, the Philippine volcano monitoring agency.
8. Coatepeque Caldera, El Salvador (1.2 million/6.5 million): Coatepeque is the first "dark horse" in the top 10. It gains points for erupting rhyolite and dacite, both magmas prone to large, explosive eruptions. It is also centrally-located in El Salvador, so a large eruption would likely impact the capital of San Salvador along with the city of Santa Ana. Like Taal, it is a lake-filled caldera, adding to its potential danger by potentially increasing explosivity or mudflows (lahars).
7. Corbetti Caldera, Ethiopia (1.2 million/9.8 million): Now, this is a real under-the-radar volcano. The Corbetti caldera lies within an even older caldera and has produced pyroclastic cones (explosive eruptions of lots of volcanic debris) and obsidian flows, meaning it has the right style of eruption and right composition to potentially experience a big explosive eruption. Not much is known about the Corbetti Caldera, so it is hard to constrain its recent activity. However, it is close enough to Addis Ababa that a large ash-rich eruption might cause quite a humanitarian crisis.
6. Tatun Group, Taiwan (6.7 million/9.8 million): Much like the Corbetti Caldera, Tatun is not a well-known volcano in a country most people don't associate with volcanism. However, as I wrote about recently, the Tatun Group has all the signs of a volcano that is still potentially active. It is also nestled close to Taipei, so you could imagine an eruption that produced another andesite dome could wreak havoc on the city, mainly from ash fall or mudflows.
5. Vesuvius, Italy (3.9 million/6.0 million): Did you really think Vesuvius wouldn't be in the top five? The volcano is one of the most dangerous on Earth thanks to its numerous explosive eruptions—and the city of Naples, which is slowly crawling up its flanks. The fact that it doesn't fall at the top of this list (heck, it's not even the most dangerous in Italy) betrays how hazardous the other volcanoes might be. Vesuvius has been quiet since 1944, so we're full into the "complacency" phase where most people don't remember the last eruption—never a good place to be when 6 million people could be impacted by an explosive eruption.
4. Ilopango, El Salvador (2.9 million/6.7 million): This is another caldera in El Salvador. But unlike Coatepeque, it has erupted in the last 200 years (1880 to be exact). Around 450 CE, Ilopango had a VEI 6 eruption that covered much of El Salvador with ash and brought down Mayan cities across the region. Today, San Salvador sits directly next to the this lake-filled caldera, so the significant danger from this caldera remains after 1,500 years.
3. Aira Caldera, Japan (0.9 million/2.6 million): The population around the Aira caldera might be lower than most of the top 10 volcanoes, but its frequent eruptions (from Sakurajima) and history of large eruptions means it poses a large danger to those 2.6 million people within 100 kilometers. Over the past 10,000 years of the Holocene, the Aira caldera has had a half dozen VEI 4, 5, and 6 eruptions—so don't be fooled by the constant din of smaller explosions from Sakurajima over the last decade.
2. Michoacan-Guanajuato, Mexico (5.8 million/5.8 million): Here's the thing about the Michoacan-Guanajuato (M-G) volcanic field: All three population radius values are the same: 5.8 million. Yes, almost 6 million people live within 5 kilometers of this volcanic field that has produced pyroclastic cones generated by explosive eruptions. It has produced numerous VEI 3 and 4 eruptions over the Holocene from 1,400 vents. This means it hasn't had big eruptions like some of the top 10. But the frequency, potential explosivity, and population in the widespread volcanic area makes it a high risk.
1. Campi Flegrei, Italy (3.0 million/6.0 million): If you're the sort of person who wants to worry about Yellowstone, maybe you should turn your attention to the Campi Flegrei instead. Not only is it a restless caldera with a more recent history of very large explosive eruptions, it is also smack in the middle of an area with over 6 million people ... and it's partially under the Bay of Naples. All these factors mean that if the Campi Flegrei has a new bout of explosive eruptions, the hazards could exceed those of any eruption in modern history. That being said, the last eruption (Monte Nuovo in 1538) was actually a fairly-benign cinder cone.
Now, this ranking is highly subjective. There can be a multitude of ways to measure danger, so I'm sure people will disagree with this list. Volcanoes like Etna, Cotopaxi, Ruiz, Fuego, and more didn't make the top 20—mostly because I chose to put emphasis on the style and composition of magmatism.
The big important point here: This list highlights the most potentially "dangerous" volcanoes based on their past behavior (mainly) and the potentially for mass casualties. There will always be volcanoes that might only have a few thousand people living near it that could erupt and kills hundreds of people.
I think of eruptions like El Chichón in 1982 as a great example. The volcano wasn't even really recognized as a threat until it erupted and killed around 1,900 people. It is unlikely that we can eliminate all volcanic threat, and as the global population grows, the danger posed by volcanoes we can identify as hazardous—and those we don't recognize as hazardous—will only increase. Funding volcanic research and monitoring along with emergency management organizations is the only way we can hope to protect ourselves from major volcanic disasters.
Forensic science—the techniques law enforcement applies to cases, CSI-style—desperately needs an overhaul. Although investigators regularly use procedures like fingerprinting, hair strand analysis, and bite mark matching, evidence of their accuracy remains scant. Worse, experts often overstate their effectiveness in court. To bolster the field's validity, the Obama administration launched the National Commission on Forensic Science in 2013. Attorney General Jeff Sessions announced this week that he won’t renew the commission, a decision critics condemned.
Sounds like another salvo in the Trump administration’s (very real and much criticized) war on science, right? After all, the commission did a lot of good work when it cast doubt on common forensic methods and reviewed questionable expert testimony. By disbanding the panel, you might think Sessions is, willfully or not, promoting junk science that could imprison the innocent or free the guilty. But the commission was in trouble long before he came along, and commissioners narrowly voted in January against renewing it. The sudden outcry is both late and improperly aimed at the attorney general.
The commission's problems mirrored those of forensic science at large: too many cooks, and too few of them graduates of culinary school. Only about a third of its 37 members were forensic scientists, and the lawyers outnumbered them. Law enforcement officers and scientists in other fields split the remaining seats. Everyone's priorities conflicted, and no one could reach a consensus on what to fix or how.
This isn't a problem cops and lawyers working with chemists, physicists and other "pure" scientists can crack. Everyone sees only their part of the process, and the other side's shortcomings. Only forensic scientists with a deep understanding of science and the law can do the rigorous work needed to ensure the field meets the standards of science.
Forensic science started at about the time of the Enlightenment, with the sort of police work you might associate with Sherlock Holmes. All these years later, the field still remains heavily dependent upon, and influenced by, the cops and detectives gathering evidence at the crime scene. "If somebody didn’t do a good job collecting, it's lost and gone forever," says Peter De Forest, a forensic scientist at the John Jay College of Criminal Justice. Even if collection goes well, lab technicians might process samples incorrectly, or misinterpret the results due to confirmation bias. According to the Innocence Project, mistakes by lab technicians account for 46 percent of all convictions that are tossed because of improper DNA analysis.
This should come as little surprise. Although many forensic scientists have backgrounds in science and experience in the lab, many more are technicians without specific subject-area backgrounds. And when people who aren't scientists do science, they inevitably make a mistake. The solution is making sure technicians have a background in the relevant field—something De Forest specifically recommended. That didn't go over well. "People on the commission feel threatened because many of them aren't scientists either," he says. "They're lawyers or law enforcement. But this is one of the most scientifically demanding things you can do."
Still, you can't simply bring a scientist out of a laboratory into a crime scene. Although training or in specific scientific fields might improve, say, DNA testing or bite mark analysis, scientists with the relevant experience are used to working in controlled conditions. Some of their best practices simply won't apply when drawing samples from a floater pulled out of the East River. This lack of field experience may have made some of the committee members drawn from the pure sciences unsympathetic to the pressures forensic scientists face.
Forensic science is the child of a bad marriage between pure science and law enforcement.
And some of the lack of rigor those scientists see among forensic scientists can be attributed to lawyerly spin. "When we go to court, lawyers pressure us to go beyond yes and no answers," says Henry Lee, a forensic scientist and director of the Henry C Lee Institute of Forensic Science at the University of New Haven. "Forensic scientists should have higher ethical standards, but the courts should know science isn't black and white."
Nor is it always helpful, says a forensic scientist who did not want to be named for fear of professional retaliation. He said the commission recommended that even forensic laboratories that analyze computers and such adhere to the same practices and standards as labs that do blood work. But pornographic images and financial records are not a biohazard. Holding all labs to such rigorous standards wastes time and money, the forensic scientist said. "Some of the things they recommended were ill informed," he says.
Forensic science is the child of a bad marriage between pure science and law enforcement—two fields that don't understand each other, or the problems facing the field. "Forensic science, especially things like fingerprint and tireprint matching, have been under the thumb of non-science law enforcement personnel," says Ralph Ristenbatt, a forensic scientist at Pennsylvania State University. "And the government's answer is this commission—to allow them to continue to dictate how things get done."
Sessions' decision not to renew the National Commission on Forensic Science will not break forensic science. It was already broken. The commission couldn't fix the problems plaguing forensic science because it shared them. The only way to fix forensic science is for the cops and the lawyers and the pure scientists to step aside and let the forensic scientists who know their field bring it the scientific rigor, and practical flexibility, they need to do the job.
As part of WIRED's exclusive look at Breaking2, Nike’s attempt to break the two-hour marathon mark next month in Monza, Italy, our writer is using the same training regime, apparel, and expertise as Nike’s three elite athletes to try to achieve his own personal milestone: a sub-90-minute half-marathon. This is the fourth in a series of monthly updates on his progress.
Last week, I travelled for work to Bangui, the capital of the Central African Republic. The trip came at a bad time for my upcoming attempt to complete a sub-90-minute half-marathon in Monza, Italy. I should have been in the meat of my training regimen, doing a high-mileage week. Instead, I was in Bangui, chasing interviews in an unstable, poor, and fiendishly hot country in crisis. Moreover, there was nowhere to run. Even though the violent chaos I witnessed during my last trip to C.A.R. in 2014 had subsided somewhat, in the capital at least, the considered opinion among my journalistic colleagues was that the city remained too unsafe for a foreigner to gambol in the streets alone. I felt the scratchy panic of an addict whose dealer had left town.
There was no question of not running. I had sweated too much, and progressed too far, to ruin my chances of breaking my record with several days of inactivity. In any event, I realized I had passed over a threshold, one that separates runners from people who run. The question of whether I would train had been replaced by how and when. The Australian marathon great Derek Clayton covered between 160 and 200 miles a week while working a full-time job. I have no idea how he did that, but clearly there was no question of his not running.
In Bangui, therefore, I found a solution. I rose early each day, ate some bread and honey in the kitchen of the ex-pat French restaurateur whose room I had rented, joined my incredulous driver, who would tap on the car’s thermometer (in the 80s, even at 7:00 am) as if to remark upon my folly, and rode across town to the Ledger, a $300-dollar-a-night expat and politician’s hotel that possesses, I believe, the only treadmills in Bangui. There, in the sweatbox gym, I performed the exact same sessions I would have done at home: mile repetitions, a progressive six-miler, an easy run, a long run. The satisfaction that accompanied the end of such workouts was not only physiological. Impressing the familiar pattern of my program in a steamy, chaotic place was a little victory. I held on to that feeling, trying to understand why it mattered to me. And then, on my way back from the gym on my final morning, I scribbled a single word in my notebook: “speed-bag.”
One summer, a decade ago, I joined a boxing gym. I had been assigned to profile Ricky Hatton, the British boxer and crowd favorite. I didn’t know much about boxing, and I didn’t want to be found out. So, I joined a gym—a joint in appropriately insalubrious surroundings, underneath a railway arch in Vauxhall, London. The club drew both serious fighters and faddy dilettantes from the professional classes, like me. (Our number included a member of parliament, whom I had great pleasure punching repeatedly in the face one morning before breakfast.)
The Secret to Running a Faster Marathon? Slow Down
Nike’s Controversial New Shoes Made Me Run Faster
Think Exercise Is Hard? Try Training Like a Nike Super-Athlete
The Secret Lab Where Nike Invented the Power-Lacing Shoe of Our Dreams
I committed myself to mastering the various stations of the gym—the heavy bag, the floor-to-ceiling bag, the medicine ball, the skipping rope—with patchy results. My nemesis was the speed-bag, a stuffed teardrop hung from a wooden circle at shoulder height or above, depending on the size of the fighter. Worked correctly, the speed-bag should become a blur as it rebounds and responds to a boxer’s lightly whirling fists. The sound of a speed-bag in action is one of the most satisfying in sports, like a teller counting a wad of money.
For weeks, I couldn’t get the hang of it. I hit the speed-bag once, twice, three times, but soon it was moving too fast and I would lose the flow as it ricocheted limply away from my fists. The harder I tried, the worse I became. A real fighter eventually took pity on me and offered some advice: Stop trying to hit the bag. Instead, I should simply decide upon a rhythm, and drill that rhythm into the plane where the bag hung. The speed-bag was not chiefly a measure of hand-eye coordination, he explained, but of rhythm and self-control. At the very next attempt, I worked the bag for a minute or more until, thrilled at the sound of counted money in my ears, I lost concentration, and the infernal teardrop wobbled to a standstill.
The best runners have learned the lessons of the speed-bag. Of course, the sport rewards effort and willpower. But I now understand, as I approach my race in Monza, that it also rewards steadiness and rhythm. A training program consists of different sessions, deployed through a series of weeks and months before a competition. In order to achieve a result far in the future, it’s necessary to mold, and then hold in one’s head, a plan to reach that goal. Real life—a job, a family, travel—makes that plan much harder to adhere to, but doubly satisfying to achieve.
The author recovering from an early training session at Nike HQ in Beaverton, Oregon in November, 2016.
Meanwhile, you must plan intelligently, and impress your own rhythm, within your training sessions. Your brain works as hard as your lungs. Through practice, monstrous error, and advice from the sports scientists at Nike, I have developed a fine understanding of my limits and paces. I know, for instance, that any effort where my heartrate rises to within four or less strokes of my maximum of 175 beats per minute is unsustainable for more than a mile or two; if I drop the effort by a couple of beats, my legs can seemingly go all day. After six months of training and self-monitoring, I can now tell you the difference between these two efforts without wearing a heart-rate monitor. The key on race day will be to lock in on the infinitesimally lower effort.
This kind of self-control is more difficult than it sounds, because it’s hard to think straight when your body is under stress, which is why being alive to the details of training matters. Indeed, I have begun to find intense satisfaction, perhaps incomprehensible to non-runners, in the correct execution of a specific session. This feeling is not a high, as such. It’s more profound than that: the sound of money being counted.
My toughest “threshold” set in my training is four repetitions of 2,000 meters. On a perfect day, I run the first repetition in eight minutes exactly. Each subsequent repetition should finish 10 seconds faster than the last, with the final repetition clocking in at precisely seven and a half minutes—a time much faster than my race pace. I recently ran that set, almost to the second, at lunchtime at my local track in Manchester, England. The place was empty except for me and my training partner, and I was barely conscious of giving a victory air-punch when I finished. The triumph was not only in knowing that I was fitter than I had been, but, equally important, in knowing that I was in control. My new-found mastery manifested itself in a performance 10 days later where I ripped a minute per mile out of my previous best time, achieved last year in my first Coniston 14—a hilly, nearly-14 mile race in England's Lake District.
Runners, led by Eliud Kipchoge (third from right, front), setting their watches at the start of an early morning run in Eldoret, Kenya in January, 2017.
Eliud Kipchoge, the Olympic marathon champion and the favorite of the three elite athletes attempting to break the two hour barrier in Monza, feels the same, deep lure of control. In every repetition of every set he completes, in every mile of every long run, he could go faster. But he runs only what he is required to run. (Naturally, even in races, he is controlling himself—because if he blazed the first mile of a marathon in his personal best for the mile, he would never finish the race. Instead, he runs only what he knows he can sustain.)
Occasionally, on a final rep of a training session, he will scorch ahead, just to feel the blood pumping with the finish line in sight. Twice now, I have watched him burn his teammates on a final repetition of a 12 x 1,200 set with a radiant look on his face. But the grand idea is to execute the set perfectly, not the repetition. Kipchoge told me recently that when he stands on the start line, he fears none of his competitors, because “I believe my training has been the best.” The best, not the hardest. Whether his preparation will be enough to break two hours in Monza next month is another matter. Kipchoge admits no doubts. The rest of us will know soon enough.
Meet the Nike Zoom Vaporfly 4%. It's the shoe Nike claims can make any runner 4% more efficient. The company is trying to prove this by attempting to break the last barrier in running: a sub-two-hour marathon.
Every month, more than a thousand New Yorkers grab their phones, dial 311, and cry rat. The city’s official hotline is increasingly inundated with rodent complaints—topping 31,000 in 2016.1 And if you’re going by residents’ social media accounts, the city is under siege, overrun by a bold horde of pizza-grabbing, escalator-riding, snowdrift-tunneling rodents.
Bill De Blasio is only the latest New York mayor to tackle the vermin scourge. Others have tried poisons and traps, special task forces to sniff out breeding hot spots, even a “rodent academy” to teach public employees how to be first-line responders to the Rattus norvegicus epidemic. Nothing has worked. So now, De Blasio’s administration is testing a more targeted approach to hit rats where it hurts: right in the ovaries.
That’s right, we’re talking about rodent contraception. Not with hormones, but with a compound that knocks female rats into early and permanent menopause. (It also messes with sperm, but only temporarily.) The city first ran trials of ContraPest in subway stations back in 2013 as part of an NIH grant project. And this year they’ll be testing it a few residential buildings around the city to see how effective it is indoors. If it works, it could change the way humans deal with pests of all kinds—from stray dogs and cats to deer and wild pigs.
ContraPest works by attacking oocytes, the egg precursors that every female mammal is born with. The active ingredient in the product is something called 4-vinylcyclohexene diepoxide, or VCD. It’s tongue-numbingly spicy, but totally non-toxic—except to oocytes. Specifically, it binds to a receptor that, when activated by a survival factor, keeps the egg precursor alive and healthy during a female’s baby-making years. But when VCD binds, it interferes with that signal and the oocyte dies. “The chemical destroys the eggs in their very smallest form so the animals can’t ovulate anymore,” says endocrinologist Pat Hoyer, who spent two decades figuring out the mechanism in mice and rats. No eggs, no offspring.
Hoyer’s lab at the University of Arizona used VCD to make a model for studying menopause in humans, along with associated diseases like osteoporosis and metabolic syndrome. The compound is much more representative than simply cutting the ovaries out, another popular animal model for menopause. The one drawback is that it takes longer: Ovaries fail months after a few weeks of daily VCD treatment, because some oocytes will already be available for ovulation—on the path to becoming eggs—when treatment starts.
Which is why Cheryl Dyer formulated ContraPest with a second active ingredient: triptolide. The bitter chinese herb targets any oocytes that have already been activated by hormones, preventing them from fully maturing. The one-two punch prevents female rats from ovulating with just a few nights’ worth of feeding. The chemicals also break down in the rat liver within 30 minutes, so they don’t pee or poop it out into the water supply.
The final trick was to make the stuff tasty to rats. Dyer, who co-founded the biotech SenesTech with Loretta Mayer, one of Hoyer’s post-docs, worked for 10 years to find a bait formulation that didn’t burn like the world’s hottest habanero or overwhelm with bitterness. So she coated the active ingredients to smother their flavor and added a lot of fat, salt, and sugar to make it more appealing than the smorgasbord offered by New York City’s trash bins.
Because rats do 40 percent of their feeding in the first hour they’re awake, they tend to visit the same spots over and over once they find something they like. Dyer says the trick is getting the bait to highly trafficked rodent freeways. That’s what the city is doing now—tracking tiny rat pawprints to figure out where they sleep and eat. Then they’ll start putting the liquid contraceptive out in the urban wild.
In SenesTech’s first trial a few years ago, rat populations near subway stations went down by 43 percent. And, as Dyer points out, a contraceptive solution is more resilient against population rebounds than a poison-based one. Killing off a subset of rats will just open up more shelter and food for others. And with a single mating pair capable of producing 15,000 pups in one year (when you count all their progeny's progeny), it doesn’t take long for the population to make up lost ground. Not to mention the fact that poisons don’t get broken down by rats’ livers, meaning they stay in their bodies and get ingested by other animals that eat them, including dogs and cats. “This stuff never goes away,” says Dyer. “We can do better than killing.”
The Environmental Protection Agency agrees with Dyer. Last summer the agency granted approval for use of ContraPest on brown and black rats, the most prevalent in New York, and the two species that have invaded cities on every continent save Antarctica. The company is now working on a formulation for mice, and they’ve even begun testing on feral pigs in Texas, where local management agencies have taken to shooting hog families from helicopters in an effort to contain them. The New York Department of Health said in a statement that it typically evaluates products like ContraPest for up to a year before making decisions on whether or not to use it more widely. But if it goes well, by 2018 rats might be the New Yorkers having the easiest time getting birth control.
1UPDATE 2:10 pm Eastern 04/17/17: This story has been updated to correct the number of rodent complaints reported through 311 in 2016. The story has also been updated to reflect the accurate scale of the city's pilot project.
To save the endangered island fox and its home off the coast of California, scientists went to war on invasive species like feral pigs and aggressive ants.
Hang onto your EVA suits, space nerds. NASA’s Saturn-exploring Cassini mission is back with another tantalizing hint that the planet’s moon Enceladus might be able to support life. Not only does the icy moon have a global water ocean, a new study suggests that the ocean might even be producing a kind of food—though it’s way too early to know if any microbial mouths are munching on it.
Which is not to say someone’s running a sandwich shop down there. The food in question is the molecular hydrogen Cassini’s instruments detected in a sample of Enceladus’ plumes—the giant geysers of water that erupt through the icy crust on the moon’s southern hemisphere. If the scientists’ calculations are correct, the hydrogen is being produced by hydrothermal reactions, the same kind that sustain extremophiles in deep sea vents on Earth. So if there are any little Enceladians down there in the deep, they might be able to live off that subsurface chemistry too.
Cassini’s target was always Saturn. Anything it gleaned from tiny, icy, little-studied Enceladus was just a bonus, and the mission’s scientists didn’t expect the moon to be nearly as interesting as it’s turned out. “Planetary scientist have a rule for how planets work: Bigger bodies are warmer and more geologically active,” says Christopher Glein, a geochemist and planetary scientist at the Southwest Research Institute. “Enceladus completely flies in the face of that.” Cassini’s researchers discovered the moon’s explosive plumes in 2005, and 10 years later found evidence of a massive subsurface ocean. So when Enceladus had another secret locked underneath its icy crust, nobody was surprised to be surprised.
NASA Finds More Evidence of Water Plumes on Jupiter’s Moon Europa
The first indication that hydrothermal reactions—which take place in water under high pressure and temperature—might be going on in Enceladus’ ocean came about a year ago. That’s when Cassini dove into the moon’s plumes to take samples from the ocean without having to drill through the icy crust. The explorer detected silica nanograins, which only come into being when rocks and liquid water meet at high temperatures (about 90 degrees Celsius, or 200 Fahrenheit). Such a reaction would also produce molecular hydrogen, or H2. “If a rock contains iron and these minerals react with liquid water under high temperature, the iron takes up oxygen atoms,” Glein says. And if you steal the oxygen out of a water molecule, you’ll be left with two hydrogens bonded together all by their lonesome.
When the team first detected the H2 molecules, it was difficult to distinguish what was data coming from the plume sample, and what was noise. (Many reactions can produce H2, including ice particles moving at high speeds banging into titanium components within Cassini). And even after some noise-canceling recalibration, finding that the plume contains H2 still doesn’t mean it was a product of a hydrothermal reaction. “If you asked an astronomer if finding hydrogen was important they’d laugh, because most things are made out of hydrogen,” Glein says. “And if you were to ask someone who specializes in icy moons, they’d laugh at you too.”
But because Cassini didn’t detect any helium along with the hydrogen, Glein determined Enceladus hadn’t picked up the H2 molecules from the gaseous soup after the big bang. And because there were no O2 molecules in the plume either, the team ruled out radiolysis—a process that typically generates H2 in icy, atmosphere-less moons by bombarding ice with radiation until the hydrogen and oxygen molecules part company. So hydrothermal reactions were the most logical conclusion.
So there’s some hot rocks at the bottom of the ocean. Who cares, right? Well, Cassini’s team sure did. “We were just biting our nails seeing if these calculations were going to work out,” says co-author and Southwest Research Institute planetary scientist Hunter Waite. And with good reason: On Earth, microbes called methanogens can live off the products of hydrothermal reactions. With a little bit of carbon dioxide, which the plume also contains, methanogens can use H2 for respiration. (That’s the energy-producing reaction that happens within the cell, not breathing.) “There’s just this constant hose of hydrothermal fuel at the bottom of the ocean,” Glein says. “But we don’t know if any organisms are feeding on it.”
It’ll be a while before they can find out. The byproduct of methanogenesis, as the name suggests, is methane—and Cassini has detected methane in Enceladus’ plumes as well, though not its source. But Cassini’s team has already gotten all the data on Enceladus it can: The craft has already passed the moon by and is headed for a crash landing on Saturn. NASA is going to have to go back to Enceladus. “We’re going to want to be very careful with that next Enceladus mission,” Waite says. “We know a lot about life on Earth. But if you go in with DNA sequencers, expecting to find something Earthlike, you might miss it.” Scientists might understand the ins and outs of hydrothermal reactions and methanogens, but they still have to assume that aliens will be, well, alien.
Dangerous volcanoes are everywhere. But, how dangerous is dangerous or deadliest? Recently, I tried to tackle that notion and codify some ideas about what makes one volcano dangerous when it erupts, while another is merely spectacular. It turns out you can condense a lot of the hazard into a few main factors—and many of them depend on how many people make their homes near these volcanoes.
To start building my list of most dangerous volcanoes, I mined the Smithsonian Institute/USGS Global Volcanism Program database. I found 63 volcanoes with over 10 million people living within 100 kilometers (62 miles)—and 14 of those have erupted in the last 200 years. Gede-Pangrango in Indonesia has over 40 million people living within 100 kilometers of the volcano. And 32 of the 63 volcanoes with 10 million people living within 100 kilometers are in Indonesia.
If we expand this and think about places where at least 1 million people live within 30 kilometers (18 miles), Indonesia is far and away the top, with more volcanoes with 1+ million living within 30 kilometers than the next four countries combined: Philippines, Guatemala, El Salvador, and Mexico.
Beyond those population figures, the risk calculation gets trickier: Volcanoes aren’t as binary as “snake venomous, snake not venomous.” To simplify things, I decided to identify the main factors that influence immediate danger from a volcano. One thing many people don’t know: Deaths from volcanic eruptions are low compared to deaths from the consequences of large eruptions (like cooler global temperatures from volcanic aerosols). Here, I’m talking about rapid impacts of eruptions (minutes to days).
I decided that there are four main factors for deciding how dangerous a volcano might be:
Population: We know this one already. I looked at the number of people living within 5, 10, 30 and 100 kilometers of the volcano, then weighted the numbers. As you go out from the volcano, the hazards generally decline. A large population further away might only experience some ash fall or mudflow confined to a river valley—though those can be very deadly if you’re not careful. Populations closer to the volcano bear the full brunt of pyroclastic flows (likely the biggest killer), heavy ash fall, and lava flows (although they aren’t that deadly).
Type of magma erupted: Different types of magma produce different types of eruptions. Sticky, silica-rich magma like rhyolite and dacite can produce explosive eruptions (bad) while silica-poor magma like basalt tends towards lava flows (less bad). So, a volcano that tends to erupt rhyolite and dacite gets a higher score and the converse for basalt.
Type of volcano: Here, I’m getting at the style of eruption as well, but looking at the form of the volcano. Most typically, volcanoes that erupt sticky lavas like andesite and dacite form stratovolcanoes. Sometimes you also get calderas from big explosive eruptions. When you have water involved, you could get more explosive eruptions from silica-poor magma. More points if you have a volcanic form that implies explosivity, fewer points if it suggest more effusive eruptions of lava flows.
How often do eruptions occur: Now, this could be the hardest variable to factor in. You would think that the more a volcano erupts, the more dangerous it is. But this is only kind of true. More eruptions is less good, but they tend to be smaller if there are more. Likewise, fewer eruptions may mean that the eruptions could be larger—like Pinatubo in the Philippines, which hadn’t erupted since 1450 CE until its massive 1991 blast. So, I looked in the Global Volcanism Program database for how many eruptions a volcano has had since 1500 CE (though that record is woefully incomplete for many volcanoes).
The “Big Bang” bonus: This last factor gets at the Pinatubo problem. If I found a caldera-forming eruption in the volcano’s history or a VEI 5+ eruption (i.e., a big bang), then I gave the volcano a few points extra. Helps all those volcanoes that might not erupt often, but have the capability of a very large eruption. However, you never know when a volcano that hasn’t had a big eruption could generate one.
You might have noticed that I ignored the monitoring factor—how carefully groups are keeping an eye on a potentially explosive volcano. It is really hard to quantify, so I decided to ignore it. Hopefully we can assume that none of these volcanoes near large populations would have all their signs of unrest ignored/missed.
Combine these factors in a fancy equation and voila, I generated a list of “dangerous” volcanoes. That list will come in the next post, but just as a teaser, here are some volcanoes that people might be surprised did not make the top 20: Yellowstone (not even close), Hood, Fuji, Rainier, Pinatubo, Nevado del Ruiz, and Cotopaxi. What made it? I’ll reveal that soon.
This story originally appeared on Grist and is part of the Climate Desk collaboration.
What Jonathan Sanderman really wanted was some old dirt. He called everyone he could think of who might know where he could get some. He emailed colleagues and read through old studies looking for clues, but he kept coming up empty.
Sanderman was looking for old dirt because it would let him test a plan to save the world. Soil scientists had been talking about this idea for decades: Farmers could turn their fields into giant greenhouse gas sponges, potentially offsetting as much as 15 percent of global fossil fuel emissions a year, simply by coaxing crops to suck more CO2 out of the air.
There was one big problem with this idea: It could backfire. When plants absorb CO2 they either turn it into food or stash it in the ground. The risk is that if you treat farms as carbon banks, it could lead to smaller harvests, which would spur farmers to plow more land and pump more carbon into the air than before.
Back in 2011, when Sanderman was working as a soil scientist in Australia (he’s now at Woods Hole Research Center in Massachusetts), he’d figured out a way to test if it was possible to produce bumper crops on a piece of land while also banking carbon in it. But first, he needed to get his hands on that really old dirt.
Specifically, he needed to find a farm that kept decades of soil samples and precise records of its yields. That way he could compare the amount of carbon in the soil with the harvest and see if storing carbon kneecapped production.
Sanderman’s office was in the southern city of Adelaide, directly across the street from the Waite Agricultural Research Institute. The researchers there supposedly had the soil and records that Sanderman needed, dating back to 1925. But no one had any idea where to find the dirt. After numerous dead ends, a chain of clues led Sanderman into the basement of a big research building down the road, covered in greenhouses.
The basement was a big, dimly lit room full of floor-to-ceiling shelves crammed with boxes in various stages of disarray. He walked the rows slowly, scanning up and down until they were in front of his nose: scores of gallon jars made of thick, leaded glass with yellowing labels. “Like something you’d find in a second-hand store and put on your shelf,” Sanderman says.
He felt a rush of excitement. Then he squinted at the labels. There were no dates or locations. Instead, each bore a single series of numbers. It was a code, and Sanderman had no clue how to crack it.
The question that Sanderman wanted to answer was laid out by the Canadian soil scientist Henry Janzen. In 2006, Janzen published a paper, “The soil carbon dilemma: Shall we hoard it or use it?” Janzen pointed out that since the dawn of agriculture, farmers have been breeding crops that suck carbon out of the air and put it on our plates, rather than leaving it behind in the soil.
“Grain is 45 percent carbon by weight,” Janzen told me. “So when you truck away a load of grain, you are exporting carbon which, in a natural system, would have mostly returned to the soil.”
Janzen has the rare ability to explain complicated things with such clarity that, when talking to him, you may catch yourself struck with wonder at an utterly new glimpse of how the world works. Plants, he explained, perform a kind of alchemy. They combine air, water, and the sun’s fire to make food. And this alchemical combination that we call food is, in fact, a battery—a molecular trap for the sun’s energy made of broken-down CO2 and H2O (you know, air and water).
Sugars are the simplest batteries. And sugars are also the building blocks for fat and fiber, which are just bigger, more complicated batteries. Ferns, trees, and reeds are the sum of those parts. Bury these batteries for thousands of years under conditions of immense heat and pressure, and they transform again—still carrying the sun’s energy—into coal, oil, and gas.
To feed our growing population, we keep extracting more and more carbon from farms to deliver solar energy to our bodies. Janzen pointed out that we’ve bred crops to grow bigger seeds (the parts we eat) and smaller roots and stems (the parts that stay on the farm). All of this diverts carbon to our bellies that would otherwise go into the ground. This leads to what Janzen dubbed the soil carbon dilemma: Can we both increase soil carbon and increase harvests? Or do we have to pick one at the expense of the other?
Sanderman thought he could help answer those questions if he could crack the codes on those glass bottles. But the codes on the labels didn’t line up with the notes that Waite researchers had made. After a flurry of anguished emails, Sanderman tracked down a technician who had worked at Waite 25 years earlier, and she showed him how to decode the numbers. Finally, after a year of detective work, he could run his tests.
In January, Sanderman and his colleagues published their results. Carbon wasn’t simply going into the ground and staying there, they found; it was getting chewed up by microbes and floating into the air again. Fields with the biggest harvests had the most carbon turnover: more microbes chewing, while carbon gas streamed out of the soil.
Bizarrely enough, these same fields with the biggest harvests also had the most carbon in their soils. How could this be?
To answer that, it helps to think of carbon like money. We have an impulse to hide our savings under a mattress. But if you want more money, you have to invest it.
It’s the same with carbon. Life on earth is an economy that runs on carbon—the conduit for the sun’s energy. You have to keep it working and moving if you want your deposits to grow. The more busily plants and microbes trade carbon molecules, the more prosperous the ecological economy becomes.
That’s the key—you’ve got to use carbon to store carbon. By amping up harvest and turning up the volume on the microbes, sure, you get higher carbon emissions, but you also get more vigorous plants sucking up even more carbon. That, in turn, gives the plants enough carbon to produce a big harvest with a surplus left over to feed the dirt.
“You can have your soil carbon and eat it, too,” Sanderman says.
Is all this too good to be true? Soil scientist Whendee Silver at UC Berkeley had some reservations about Sanderman’s methods. She wondered if the Australian soils that he studied might have changed during decades of storage, and if the results would have been different if researchers had looked at more than just the top 10 centimeters of soil.
That said, Silver thought Sanderman’s conclusions made sense: Grow more stuff, and you get more carbon left behind in the soil. Rattan Lal, director of the Carbon Management and Sequestration Center at Ohio State, also gave the study his seal of approval.
The implications are huge. The study suggests we can slow climate change simply by feeding people. But there’s a gap between discovering something and putting it to use.
The Unlikely Battle to Pass the Nation’s First Carbon Tax
Farmers Are Manipulating Microbiomes to Help Crops Grow
A Smart Sensor That Quantifies the Soil in Your Garden
Solving one puzzle often opens up many, many more. Humphry Davy invented the electric light in 1802, but lightbulbs weren’t available for regular use until Thomas Edison’s day, 75 years later.
In this case, Sanderman’s sleuthing provides a proof of concept. To apply it, farmers would have to get more plants turning carbon to sugars on every acre of land. Now scientists and policy makers just need to find the barriers that prevent farmers from putting this knowledge into practice.
One issue is that the high-yield Australian fields in Sanderman’s study were growing grass, not wheat or corn. Grass directs its carbon into roots that stay in the soil, while grains are bred to shove carbon into their seeds. That doesn’t compromise the point of the study; the grass was still able to produce tons of hay for harvest while also making the dirt carbon-rich.
But it does add a new riddle: How do we get food crops to act like grass and spend more of their carbon budget on their roots, while still producing bountiful harvests?
The simplest answer, Janzen says, would be to boost yields. Anything farmers can do to allow more plants to thrive—like improving nutrition, irrigation, and protection from insects—will mean more carbon flowing into the soil. And in the long run, breeding for more roots as well as more grain will be a key to getting carbon into the ground without losing food production. Ultimately, that requires improving on photosynthesis, which is as difficult as putting a man on the moon (yep, scientists are working on it).
Another approach is to grow plants on fields that would otherwise be bare. By rolling out a carpet of green during the winter, farms could suck more carbon from the air into the soil. Some farmers are already doing this—growing cover crops like clover and ryegrass and experimenting with a suite of techniques often called “climate-smart agriculture.”
But there’s yet another barrier here: money. For farmers, the costs of planting cover crops often outweigh the immediate benefits. That’s why Ohio State’s Lal argues that farmers should get some help. “We have to recognize that farmers are making an investment that benefits society as a whole,” she says. “They should be compensated. My estimate is $16 per acre per year.”
Some companies have already started paying farmers to employ these techniques, says Roger Wolf, director of the Iowa Soy Association’s environmental programs. These corporations see a trend toward sustainability, with more of their customers pushing for environmental stewardship, and are trying to get out in front of it. The food and cosmetics giant Unilever and the grain trader ADM offer farmers a premium price for adhering to practices that accrue carbon.
Ever since people began pushing seeds into the dirt, we’ve been eating away the carbon from our topsoil. Now we’re finally developing the knowledge necessary to pump that carbon back into the ground. We have a proof of concept and Sanderman has taken the next logical step: He’s working on creating the tools farmers need to put this knowledge into practice. It’s one more link in the chain humans are forging to hold back the worst ravages of climate change.

Want to know how crickets are farmed and turned into powder? Of course you do. You'll be eating the stuff soon enough.
Never tell California the odds. Not only has the state recovered from its record-breaking drought, it did so in record time. According to a new NOAA study looking at 445 years of climate data, California had a 1 percent chance of breaking the drought in just two years.
You remember the drought, right? Between 2012 to 2015, California's epic dry spell—its worst in recorded history—depleted reservoirs, melted mountain snow, and forced farmers and cities to recklessly suck up groundwater reserves. Things got so dire that, in 2014, governor Jerry Brown signed a state of emergency curtailing water use for cities and official business. Then came 2016's El Niño, then 2017's commute-clogging, weekend-ruining, and infrastructure-crippling onslaught of storms. These two years of successive soaking prompted Brown, barely a week ago, to announce that the state's drought emergency had ended.
As far as twists go, this is about on par with blowing a 3-1 lead in the NBA finals.
Like its basketball teams, California's climate has a variable record. Rain and snowfall can fluctuate as much as 50 percent year-to-year. The only problem is those swings don't happen with discernible regularity. And climate change is making them more dramatic. "What we see in the historic record is an increase in the likelihood of warm and dry periods, punctuated by wet conditions," says Noah Diffenbaugh, climate scientist at Stanford's Woods Institute for the Environment.
And that history goes way deeper than the hundred or so years people have been seeding the state with thermometers and rain gauges. "What we used are existing climate reconstructions based on tree ring data, which show river and stream flows," says Eugene Wahl, the study's lead author and a paleoclimatologist for NOAA's National Center for Environmental Information.
If you remember from summer camp, tree rings roughly equate to years, and thick ones mean that year was wet. Wahl correlated ring samples with stream flow data from the surrounding area—and by comparing stream flow rates to precipitation and temperature data from the same year, made a reasonable linkage between tree rings and climate.
To make sure their correlation was sound, Wahl and his co-authors calibrated tree rings to stream flow for over 60 years of data—from 1916 to 1977. Even better, they tested the results by using it to predict precipitation and temperature from 1896 to 1915, years they also had instrument records for. Once they were confident the method worked, they used it to recreate streamflow volumes across California as far back as 1571. And because the state has so many old trees, they were able to reconstruct the climate at a fairly fine scale (1/2 a degree latitude by 1/2 a degree longitude, for any geography geeks in the audience). And they did this for California's many climate regions—from the wet northwest, to the arid southeast.
But scientists have been piecing together past climates since the early days of geology. All this legwork was really so Wahl and his team really could figure out the rate at which California recovered from its most recent drought relative to those in the past. "So what we did was look at the lowest precipitation deficits and see where they were in relation to the years that followed," says Wahl. Imagine each of those low years is an empty bucket. The following year, the bucket gets a little more water. A little more the following year. Then less. Then more. Less. More. More. More. On and on until the bucket is full.
Those four and a half centuries of data revealed that the median recovery time for a dry spell like the one that happened between 2012 and 2015 was 39 years. And say you don't trust those historical reconstructions? Using just the instrument data, the median recovery time is still 29 and a half years. That puts a two-year recovery like the one California just saw at the skinny end of the distribution—a 1 percent chance.
Wahl is careful to say that his study only looks at dryness, not drought. The latter is a more formal definition that accounts for things like soil moisture, which is impossible to recreate by proxy. Drought also tends to imply how much water is available for people and crops. Because Wahl and his co-authors were able to recreate dryness conditions on such fine geographic scales, their analysis shows how regions that are now California's most populous and agriculturally valuable are also the ones that typically get hit hardest by dry spells—and take the longest to recover.
Information like this is critical in California—a drought-prone economic powerhouse. "We spend a lot of money every year recovering from climate and weather disasters," says Diffenbaugh. Such as the drought, which cost California about $600 million. But studies like this could be endangered: President Trump's budget proposes huge cuts to NOAA's climate monitoring divisions. Anyone wanna take a bet on how that will turn out?
Last year was the hottest year since scientists started keeping records in the 19th century. It's no fluke---because it's humanity's fault.
I can't imagine a blockbuster movie about superheroes without some cool physics. After all, these aren't dramas, but action movies with jumping and flying and punching. Of course, the point of the jumping and flying and punching is to advance the story, not provide a physics lesson. But nothing says they can't do both.
The first trailer for *Thor: Ragnarok *provides a great chance to do this. The physics lesson comes at the end, when Thor meets The Hulk in an arena. They run toward each other at top speed, then jump into the air for what I'm sure will be an epic collision. This being a trailer, Marvel doesn't reveal what happens next, but I think it's safe to assume Thor delivers a pretty serious blow. I'll model this collision in two ways—the Hollywood way and the real way, because of course the Hollywood way is not at all how a real-life Thor and the Hulk would collide. And yes, I essentially just said superheroes are real.
Let me set up the physics for you. The Hulk definitely weighs quite a bit more than Thor. For the sake of argument, I'll say he has three times the mass of Thor. Seems reasonable. Watching the trailer, it looks to me like Thor and The Hulk run toward each other at similar speeds and jump about the same height, colliding in the middle of their trajectory. In this model, which is what I expect to see in the movie, Thor punches The Hulk with great force, sending the green giant flying.
Here is a simulation I made using Trinket. Click play to run it as often as you like. If you want to look at the code behind it, here you go.
No one would complain with this outcome, even if this just isn't how physics works. Even if this is how it plays out on screen, I will still enjoy the movie.
OK, now let's look at what would really happen. Do you know what makes this collision so great, aside from the fact it features Thor and The Hulk? Both of them are in the air. That means only two forces act on them. The first is gravitational force pulling them down. Boooring. The second is the force Thor exerts on The Hulk when he lands the punch (or hits him with a club). But wait! If Thor pushes on The Hulk, The Hulk also pushes back on Thor with a force of equal magnitude in the opposite direction.
Remember, force is nothing more than an interaction between two objects. If Thor pushes The Hulk to the right with a force of 1,000 Newtons, then The Hulk pushes back on Thor to the left with a force of 1,000 Newtons. That's just the nature of forces.
So what does this mean for Thor and The Hulk? The momentum principle says the total force on an object changes that object's momentum, with momentum being the product of mass and velocity. So you see what must happen here: Thor and The Hulk have essentially the same total force during the punch (assuming everything happens so quickly that I can ignore gravity), and so they will experience the same change in momentum. However, given that The Hulk has three times the mass of Thor, he will experience a far smaller change in velocity.
Now for a simulation.
No matter how hard Thor hits The Hulk, he is going to get thrown back. It's just physics. Now for a bonus gif.

This is similar to the numerical calculation above, but real life. The two carts have different masses, but the same magnitude force acting on them. You can see that Thor loses every time. But, as I said, I don't expect the movie to use the best physics. And honestly, I am OK with that. It's just a movie. However, I think you can still have a physics-like collision and be part of the story. If Marvel made me the science advisor, I might suggest the scene unfold like this:
Thor and The Hulk run toward each other for their epic showdown and leap into the air. In his rage, The Hulk doesn't notice that Thor is about to land the first blow. Thor whacks him mightily, dazing The Hulk but flying back across the arena into the wall. Thor gets up, adjusts his helmet, and tells Atlas, "Physics still works in this universe. I forgot that the beast has a larger mass than mine and momentum is still conserved. If I had my trusted weapon, Mjolnir, its significant mass would surely win the day."
Maybe that's why you don't see physics professors writing movie scripts.
https://www.youtube.com/watch?v=v7MGUNV8MxU
Florida’s citrus growers are running out of time. Since 2005, when a deadly disease called citrus greening first showed up in the state, they’ve been fighting a losing battle to slow the spread of the sugar-sucking bacterium behind the scourge. Today, it has infected 90 percent of Florida’s citrus groves. Fifth-generation farmers are abandoning their acres, packing factories are shuttering their operations, and the state is hemorrhaging a billion dollars every year.
One way to ensure the survival of Florida’s citrus industry—and most of the country’s orange juice—is to produce resistant trees, either with traditional breeding or genetic engineering. Both approaches are proving problematic. Despite exhaustive searches, no one has found a naturally immune tree. And it will take 10 to 20 years to engineer and approve an artificially resistant tree, even with new gene-editing tools like Crispr. Growers can’t wait that long. So to buy them time, one local citrus company is developing something more like an arboreal vaccine, using a genetically modified virus to deliver bacteria-killing spinach proteins.
The treatment starts with a harmless virus that lives in nearly all of Florida’s citrus trees. Years ago, University of Florida plant pathologist Bill Dawson modified a local strain of the citrus tristeza virus so that anyone could insert new bits of DNA into its genome, turning it into a protein factory—otherwise known as a viral vector. Meanwhile, Southern Gardens Citrus, one of the world's largest orange juice manufacturers, was getting on the biotech bandwagon. Initially, the company planned to rid Florida of the citrus greening disease by breeding genetically modified trees. But each one takes at least three years just to mature. “What Florida needs is something quick,” says Dawson, “because whatever you’ve heard, it’s worse than you can imagine.” Something quick, Southern Gardens realized, would be to instead inoculate trees with the vector, which they then licensed from Dawson's lab.
With the vector as its syringe, Southern Gardens just needed a drug to inject. Luckily, it had already screened dozens of genes in its search for a resistant tree—from vegetables, viruses, and even a pig. One particularly promising set of genes came from the spinach plant, coding for a group of antibacterial proteins called defensins. (You have defensins too, fighting off microbes in your saliva.) These spinach defensins worked especially well to chop up C. liberibacter, the bacterium that kills citrus trees by choking off their food supplies.
Southern Gardens inserted the spinach genes into the viral vector; all that was left to do was take it to where the bacteria live.
Trees don’t have veins, but they do have a circulatory system of sorts. Called phloem, these pipe-like tissues transport water, sugar, and nutrients from the roots to the leaves and fruit. That’s where C. liberibacter make their home, living off the trees’ food, preventing the fruits from accumulating sugars (that’s why they stay small and green, hence “greening disease”), and eventually starving the tree to death. To access that network, Southern Gardens grafts a vector-treated plant onto the infected tree. As the virus replicates inside the tree, it becomes a spinach defensin factory, and those proteins travel through the phloem attacking every C. literibacter they meet.
“You can think of like a slow release vitamin tablet,” says Tim Eyrich, Southern Gardens’ vice president of research and commercialization. “We’re attacking the bacteria where it lives, and we can do it without changing the biology of the tree at all.” That last bit is really important for the company, which has struggled to convince Americans that GMOs are the only way to keep domestic orange juice on the breakfast table. OJ made from transgenic fruit may carry a GMO label, while juice made from a vector-treated tree would not. And while they’re not giving up hope on a transgenic approach to replace Florida's lost acres just yet—Southern Gardens is currently testing trees with the same spinach defensins—they’re still a long way from winning regulatory approval, not to mention hearts and minds.
In the meantime though, their weaponized virus is bringing new hope to an increasingly desperate industry. According to economists at the University of Florida, the state’s production has decreased by 70 percent since 2005. Farmers are spending on average $1,000 more per acre to grow less fruit, as they try everything from chemical cocktails to nighttime heat treatments to hold off the disease.
In February, Southern Gardens Citrus applied for a permit to release its engineered virus on hundreds of test acres of juicing orange trees, expanding on the limited trials it has been conducting since 2010. On Monday, the US Department of Agriculture posted a notice that it intends to conduct an environmental impact assessment to review the potential risks of granting such a permit. That review, which is pretty standard for a biological pesticide, should take about two years. If all goes well, growers could be using it as soon as 2019, pending additional EPA approval. That's lightning speed compared to making a resistant tree from scratch. But for now, Florida's families in economic limbo can only hope it's fast enough.
When hosting a party where genetically modified foods are what’s for dinner, is it proper etiquette to warn your GMO-averse friends ahead of time? Mr. Know-It-All offers sage advice on how to handle.
Prime numbers, the indivisible atoms of arithmetic, seem to be strewn haphazardly along the number line, starting with 2, 3, 5, 7, 11, 13, 17 and continuing without pattern ad infinitum. But in 1859, the great German mathematician Bernhard Riemann hypothesized that the spacing of the primes logically follows from other numbers, now known as the “nontrivial zeros” of the Riemann zeta function.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
The Riemann zeta function takes inputs that can be complex numbers — meaning they have both “real” and “imaginary” components — and yields other numbers as outputs. For certain complex-valued inputs, the function returns an output of zero; these inputs are the “nontrivial zeros” of the zeta function. Riemann discovered a formula for calculating the number of primes up to any given cutoff by summing over a sequence of these zeros. The formula also gave a way of measuring the fluctuations of the primes around their typical spacing — how much larger or smaller a given prime was when compared with what might be expected.
However, Riemann knew that his formula would be valid only if the zeros of the zeta function satisfied a certain property: Their real parts all had to equal ½. Otherwise the formula made no sense. Riemann calculated the first few nontrivial zeros of the zeta function and confirmed that their real parts were equal to ½. The calculation supported his hypothesis that all zeros had this property, and thus that the spacing of all prime numbers followed from his function. But he noted that “without doubt it would be desirable to have a rigorous proof of this proposition.”
A century and a half later, proving the Riemann hypothesis remains arguably the most important unsolved problem in pure mathematics — one whose solution would fetch a $1 million Millennium Prize from the Clay Mathematics Institute. Conversely, as the number theorist Enrico Bombieri wrote in his description of the problem, “the failure of the Riemann hypothesis would create havoc in the distribution of prime numbers.”
As mathematicians have attacked the hypothesis from every angle, the problem has also migrated to physics. Since the 1940s, intriguing hints have arisen of a connection between the zeros of the zeta function and quantum mechanics. For instance, researchers found that the spacing of the zeros exhibits the same statistical pattern as the spectra of atomic energy levels. In 1999, the mathematical physicists Michael Berry and Jonathan Keating, building on an earlier conjecture of David Hilbert and George Pólya, conjectured that there exists a quantum system (that is, a system with a position and a momentum that are related by Heisenberg’s uncertainty principle) whose energy levels exactly correspond to the nontrivial zeros of the Riemann zeta function. Each of these energy levels, En, corresponds to a zero of the form Zn = ½ + iEn, which has a real part equal to ½ and an imaginary part formed by multiplying En by the imaginary number i.
If such a quantum system existed, this would automatically imply the Riemann hypothesis. The reason is that energy levels of quantum systems are always real numbers (as opposed to imaginary), since energy is a physically measurable quantity. And since the En’s are purely real, they become purely imaginary when multiplied by i in the formula for the corresponding Zn’s. There is never a case where an imaginary part of En is multiplied by i, canceling out its imaginary property and rendering it real, so that it then contributes to the real component of Zn and changes it from ½ to something else. Since energy levels are always real, the real parts of the zeros of the zeta function would always be ½, and the Riemann hypothesis would therefore be true.
Physicists have been searching since 1999 for a quantum system whose energy levels correspond to the zeros of the zeta function. In a paper published on March 30 in Physical Review Letters, Carl Bender of Washington University in St. Louis, Dorje Brody of Brunel University London and Markus Müller of the University of Western Ontario proposed just such a candidate system. But it’s a weird one — and outside experts say it’s too soon to tell whether it will lead to a proof.
Normally, physicists describe quantum systems using highly symmetric mathematical matrices whose solutions, or “eigenvalues,” correspond to the system’s energy levels. The symmetries of these matrices usually guarantee that imaginary numbers cancel out and the eigenvalues are real, so that these matrices make sense as descriptions of physical systems. But for 20 years, Bender and Brody have studied matrix descriptions of quantum systems that relax the usual symmetry requirements and respect a weaker property called parity-time (or PT) symmetry. Following a 2015 conversation with Müller, they discovered that they could write down a PT-symmetric matrix whose eigenvalues correspond to the nontrivial zeros of the Riemann zeta function. “This came as a real surprise to us,” Brody said. However, because the matrix was only PT-symmetric, instead of following the usual stricter symmetries, it isn’t guaranteed to have real eigenvalues — the property that would ensure that corresponding zeros have real parts equal to ½.
The researchers spelled out several arguments for why the eigenvalues of their matrix are probably real, and why, in that case, the Riemann hypothesis is probably correct, but they came short of proving it. “Whether it will be difficult or easy to fill in the missing steps, at this point we cannot speculate,” said Brody. “Further work is needed to get a better feeling as to the scale of difficulty involved.”
Experts say that the new proposal is interesting, but that it’s far from certain whether the authors’ arguments about their unconventional quantum system can be made rigorous. “I would need more time to give a relevant opinion about the significance of their findings as a strategy towards the Riemann hypothesis,” said Paul Bourgade, a mathematician at New York University. In particular, Bourgade said, he would like to explore in more detail how the proposed quantum system compares with one previously proposed by Berry and Keating that has not yielded a concrete proof.
If physicists do someday nail down the quantum interpretation of the zeros of the zeta function, according to Bourgade, this could provide an even more precise handle on the prime numbers than Riemann’s formula does, since matrix eigenvalues follow very well-understood statistical distributions. It would have other implications as well; Berry hopes that a quantum system underlying the primes would serve as a simple model of chaos, demonstrating how chaotic behavior related to the primes can arise out of a nonchaotic quantum system. But we aren’t there yet. Considering how long the Riemann hypothesis has resisted a conclusive proof, Berry urged caution in reading too much into any partial progress. “This latest contribution to the Riemann hypothesis perfectly exemplifies Piet Hein’s dictum,” Berry said: “Problems worthy of attack prove their worth by hitting back.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
While sperm has been successfully frozen for decades, it wasn’t until 1999, when flash-freezing procedures were introduced, that eggs could also be stored in cryobanks. By 2012, the American Society for Reproductive Medicine had announced that the freezing of a woman’s own eggs for possible use later in life—otherwise known as “social freezing”—would no longer be considered experimental.
Excerpted from Babies of Technology: Assisted Reproduction and the Rights of the Child by Mary Ann Mason and Tom Ekman.
The Society’s approval was meant to apply to infertile mothers who could not produce their own healthy ova—not career women deferring their childrearing years. This point was clarified by Eric Widra, a physician and co-chair of the American Society for Reproductive Medicine committee that made the recommendation, in a 2012 PBS interview: “We think it’s premature to recommend that women freeze their eggs to preserve their own fertility for later. But we recognize that there is a strong impetus to do so and if centers proceed with that service that we carefully counsel the patients as to the pros and cons. So we would love to say, yes, please go and do this. But it comes with both personal and societal and scientific ramifications that we aren’t prepared to say we understand yet.”
Nevertheless, fertility clinics across the United States quickly embraced the removal of the “experimental” designation to suggest that egg freezing was now safe for mothers wishing to save their own eggs for later in life. High-profile companies like Facebook announced that they would support egg freezing for their female personnel in order to retain valued employees. It is not uncommon for women in their mid-thirties and beyond to abandon their careers to have a family. In addition, having younger women save their eggs may prevent extensive costs associated with fertility treatment later in their lives and careers.
Mary Ann Mason is Professor of the Graduate School, UC Berkeley, and a Faculty Affiliate of the Berkeley Center for Law & Technology. Tom Ekman is a science teacher and writer, and was formerly the Director of Content Development for National Geographic Maps Technology Division.
In April 2015, the heads of Facebook and Virgin Airlines, Sheryl Sandberg and Richard Branson, appeared on national television to defend Facebook’s $20,000 egg-freezing benefit for women employees.
Branson quipped: “How can anybody criticize [Facebook] . . . for doing that? . . . It’s the woman’s choice. If they want to carry on working, they can carry on working. If they haven’t managed to find the man of their dreams by 35–36–37–38, freeze the eggs—it makes sense the earlier you can freeze them the better . . . We at Virgin want to steal the idea and give it to our women.”
In San Francisco, the “egg whisperer,” fertility doctor Aimee Eyvazzadeh, threw several egg-freezing parties for employees of major tech companies. The women were invited to enjoy free drinks and appetizers at a trendy San Francisco restaurant where they could listen to a presentation about the benefits of freezing their eggs. Fertility experts were on hand to field their questions. In Manhattan, Eggbankxx, a New York-based fertility clinic, hosted a similar event for professional women called “The Three F’s: Fun, Fertility and Freezing.” Wine and appetizers preceded a presentation similar to the one in San Francisco. The women who attended were emailed aggressively afterward with offers of special financial plans and large discounts for signing up.
“Have you thought about freezing your eggs?” co-author Mary Ann asked two young women at a San Francisco lunch for professional women. “Do you mean for sale?” one woman responded. “I probably wouldn’t do it—unless I was really desperate for money. But I would consider donating them to a close friend or family member . . . No, I take that back: I could not bear to see a baby who was mine and didn’t know it. And you have to take all those drugs. But I have seriously considered freezing them for later. First I need to make partner in my law firm. And by then, I figure I will be at least 37.”
The second woman offered: “I have been thinking about it as a career option for me, but I am only 28—I have a couple of years to decide. And then I worry that if I did do it, and waited until my forties, I would not be a good mother—I’d be too tired.”
Social freezing has now become one of the fastest-growing sectors of the fertility industry. Frozen eggs give hope to the swelling ranks of professional women who do not have time to have a baby. When a woman joins a firm after earning her M.B.A. or J.D., she has to show that she has “the right stuff,” which often means grueling sixty-hour workweeks and frequent travel. The traditional child-bearing years (the late twenties and early thirties) clash with the major career-building years. Men do not share this “baby penalty” at work. For professional women, the more hours they work, the fewer children they are likely to have, according to the 2000 census. The opposite is true for professional men: the more hours that they work—up to fifty-nine hours a week—the more children they are likely to have.
Marcia C. Inhorn, a professor at Yale University who researches fertility issues, asks if women like herself will now feel forced by their demanding livelihoods to seek egg freezing:
Employers may come to expect women to postpone childbearing through egg freezing. Women may be pushed into a burdensome and costly medical procedure that cannot provide guaranteed future fertility outcomes. Also, an increased age difference between mothers and their children may lead to poorer, less energetic parenting, as well as an increased likelihood that children will lose their mothers early on. Moreover, promoting egg freezing as a quick-fix technological solution does not solve the unfavorable employment policies that cause women to lean out of their careers . . . My female graduate students often ask me for advice on how to become a successful professor, while also having kids. I usually tell them to look for a supportive partner who has a nontraditional, flexible career path.
When my co-author Mary Ann became the first woman graduate dean at U.C. Berkeley in 2000, she was thrilled to see that the number of women entering the Ph.D. and professional school graduate programs slightly outnumbered the men. Did this mean that the hard-fought feminist revolution of the 1970s had finally been won? Hardly. Looking around the conference table of deans, Mary Ann found that she was the only woman. The scanty percentage of current female faculty was nowhere close to the number of women, coming up the graduate school pipeline. A similar story could be told, of any corporate or governmental career ladder across the nation. Women enter at levels not known before, and they achieve a certain, degree of success . . . but they rarely rise to the top.
The Do Babies Matter? project, Mary Ann’s ten-year-long research effort at Berkeley, examined the effect of childbirth on several professions: academia (with an emphasis on science), law, medicine, and business. While women have flocked to these professions in unprecedented numbers over the past thirty years—often surpassing men in the number of professional degrees earned—they have also dropped out, or dropped down into the second-tier of their professions, in massive numbers. Given the pressures of career and family life for these professional women, freezing eggs can seem an attractive option.
In academic science, a career path that the federal government has strongly promoted for women by channeling significant federal funds their way, the gender gap remains a major problem.
Often, women who originally hoped to make the long trek to become a research professor (which can take ten years with Ph.D. and postdoc requirements) abandon this goal before they look for their first tenure-track job. Childbirth is the main factor driving this attrition.
According to a National Science Foundation survey of all scientists, women with a partner and child are 35 percent less likely to seek a tenure-track job than a man with a partner and child. Single women, on the other hand, do just about as well as single men in obtaining that first job. The mothers who do begin a tenure-track position suffer again at the time of their tenure decision. They are 27 percent less likely than fathers to capture the golden ring of tenure. The average woman scientist is about thirty-five when obtaining her first tenure job, and forty when attaining tenure. They do not have the luxury of waiting until they have reached that long-sought-after job security to have a baby.
The attention given to egg freezing may be diverting employers from making changes that would keep talented, educated women in the pipeline.
Fortunately, the tide is starting to turn. Today, there are more family-friendly structural changes being implemented in the workplace:
• At U.C. Berkeley, new faculty mothers were offered two teaching-free semesters (new fathers were offered one semester). Recruitment has been greatly invigorated, and twice as many children have been born to assistant professor mothers.
• The California legislature used Mary Ann’s Do Babies Matter? Research to show that the greatest leak in the pipeline for women scientists occurs in the graduate school and postdoc years, when women are most likely to have babies. California passed a law mandating a strong leave policy with a right-to-return for both mothers and fathers who had babies during their graduate school years. This law applies to all institutions of higher education in California.
• Netflix led the corporate charge to reform the structure of the workplace by offering both mothers and fathers up to a year of paid leave after childbirth. As Netflix’s chief talent officer Tani Cranz, explained: “We want employees to have the flexibility and confidence to balance the needs of their growing families without worrying about work or finances. Parents can return part-time, fulltime, or return and then go back out as needed. We’ll just keep paying them normally, eliminating the headache of switching to state or disability pay. Each employee gets to figure out what’s best for them and their family, and then works with their managers for coverage during their absences.”
Changing the structure of the workplace to allow mothers and fathers to continue their careers while taking time out for childbirth and family needs may seem more expensive for the industry. Yet in the long run, it has proved to be a good strategy for recruiting and retaining valuable academics. For instance, the federal government spends several hundreds of thousands of dollars investing in a single science student through graduate school and the postdoctoral years. That is all lost if a woman scientist drops out of the pipeline.
A family-friendly workplace allows sufficient time off for childbirth and baby-bonding, flexibility regarding business travel, and other support. To be successful, it must include fathers. Unfortunately, all of the attention given to egg freezing may be diverting employers from making critical structural changes that would keep talented and highly educated women in the pipeline.
The American Society for Reproductive Medicine announcement was not supposed to be an endorsement for social freezing, but women of means are increasingly embracing the freezing option as part of their family planning. Fertility clinics report a huge upsurge in demand for egg freezing driven by the new market for social freezing.
Children born of frozen eggs face more possible health complications than children of sperm donors. The eggs will have been extracted through a medical procedure and frozen in a commercial egg bank for an indefinite period of time. Each of these steps introduces possible health consequences. Most important, the combination of the fertility drugs used to boost egg production and the insertion of multiple embryos into the womb (primarily in the United States, where this practice is still allowed despite having been banned in Europe) results in a high number of double, triple, and even quadruple births.
Multiple births are vulnerable to cerebral palsy, learning disabilities, blindness, developmental delays, mental retardation, and infant death—largely because they are often born prematurely with very low birth weights. Infant mortality rates for twins are four to five times that of single births, and for triplets the rate is higher still.
The widespread use of frozen eggs for in vitro fertilization is too recent a phenomenon for any research on the effects on children’s health. Certainly, there is evidence that children born preterm may have lifelong health effects. Until recently, egg freezing was only recommended for cancer patients facing potentially sterilizing chemotherapy, as the process involved low success rates and high cost. There is still some concern about the viability of freezing and the possible health effects for the child.
Marcy Darnovsky, executive director of the Center for Genetics and Society, comments:
Our concern is that a lot of fertility clinics, hundreds really, are already aggressively marketing this procedure for elective purposes . . . But I don’t think any woman wants to experiment with her own health or experiment with her children’s health . . . we don’t have adequate data about either the short-term risks or the long-term risks of egg extraction. . . I really hope that the fertility industry will . . . step up to the plate and really make it clear that they’re not recommending this at the current time for elective purposes, and that they hold the toes to the fire of their members who are advertising it that way and marketing it that way.
Women’s advocates who have been working for decades to institute family-friendly policies in the workplace are concerned that egg freezing may be a hollow victory for working mothers—a diversion from the more important discussion that needs to take place.
Darnovsky says: “We shouldn’t be asking women to bear these risks just so they can have a family. We should be putting in place policies that make sure women have equal pay for the work that they do, to make sure that they don’t hit glass ceilings, that there are family- friendly policies in workplaces, and that we’re not assuming that women are the sole or the major caretakers for children.”
This article is an excerpt from Babies of Technology: Assisted Reproduction and the Rights of the Child by Mary Ann Mason and Tom Ekman, new from Yale University Press. Published by permission.
Mars is Earth's inhospitable cousin. And like anybody with a wayward family member, Earthlings can't help trying to figure out what went wrong. So people throw a lot of money at the red planet, usually in the form of spacecraft. NASA's Maven mission explores Mars' upper atmosphere, puzzling over how a planet that was once wet and potentially habitable ended up a dry, cold desert world.
Metal could solve the mystery. In a new study, Maven mission scientists found that Mars' ionosphere is full of metal ions, tiny charged flecks of meteorite pulled apart by atmospheric forces. Because those ions are hardy and trackable, scientists can use them to study the mysterious dynamics of the planets' upper atmosphere, including its tendency to escape out into space. Plus, scientists have already observed similar metal ions at work in Earth's (very different) atmosphere. Scientists sorting out how these upper layers sort themselves out are rethinking their assumptions about how worlds work.
Earth and Mars get their metal ions from the same source: the constant hail of micrometeorites beating down on their atmospheres. As the flecks of meteorite burn up, charged particles in the atmosphere rip electrons away from metal atoms, turning them into positively charged ions. (Maven has found iron, sodium, and magnesium ions in Mars' atmosphere.) And that's not just interesting trivia: "It's like dropping ink in water," says Mehdi Benna, planetary scientist at NASA's Goddard Space Flight Center. "You can see which way the current moves." Before Maven detected these ions, the dynamics of Mars' upper atmosphere were darn near invisible, which makes gravity waves or a phenomenon like atmospheric escape pretty hard to study.
"We were surprised to see these metal ions in the Martian atmosphere," says Benna. That's because Mars doesn't have Earth's strong, planet-wide electromagnetic field—which protects the atmosphere from being stripped away by the solar wind. Mars used to have a magnetosphere, but the planet lost its dynamo as it cooled. Scientists didn't think metal ions would be likely to stick around.
In fact, Maven wasn't even looking for them at first. But when the spacecraft—which is in an unusually low orbit, and carries an ultra-sensitive neutral gas and ion mass spectrometer—picked up some metal ions after the comet Siding Spring grazed Mars (at a distance of 82,000 miles) in 2014, the team decided to make keep Maven's eye out for more. After two years, they've realized the ions are a permanent fixture, but behave totally differently than those in Earth's atmosphere.
On Earth, the electromagnetic field and ionospheric winds sort upper atmosphere metal ions into distinct layers. But on Mars, that only happens near super magnetic patches of the planet's crust. "In most of Mars' atmosphere, the ions are all mixed up together, and that's a surprise," says David Brain, Maven co-investigator and atmospheric scientist at the University of Colorado at Boulder. According to Brain, whatever (still mysterious) process keeping all the ions snuggled up together might affect how atmospheric particles leave the upper atmosphere in ways scientists haven't yet understood. It'll take studying these metal ions further to see if that's true.
But these ions have implications beyond Mars, too. "We should now be able to find the physics of how this works and apply it to other planets," Benna says. "Most planets with thick atmospheres should have these ions." So this is less the key to Mars' atmospheric woes and more a tool to compare and contrast different kinds of atmospheres in order to better understand all of them—including Earth's.
I’m not going to tell you to stop having babies, but the hard truth is that the population of Homo sapiens will skyrocket to perhaps 9 billion by 2050. More people means more emissions, which means runaway global warming, which means the agriculture required to feed those 9 billion people is in peril.
Call humanity a cancer, call it a virus, but neither of those things ever invented a robot to eyeball corn plants. Allow me to introduce you to Vinobot, the little rover on a mission to make sure crops weather global warming.
Vinobot has a partner in this, a solar-powered tower that keeps watch over a field. Using 3-D cameras, it builds a picture of the crops, looking for individual plants under stress. Should the tower spot something awry, it dispatches Vinobot. The rover uses its robotic arm to create a detailed 3-D model of the plant, showing scientists the exact angles of leaves, for instance, to determine how different kinds of corn handle drought. (Corn leaves tend to droop in the heat, reducing the surface area exposed to the sun.)
Vinobot also collects humidity, light intensity, and temperature data at the bottom, middle, and top of the shoot. “We collect those at three different heights because we're trying to study how the density of planting is affecting the individual plants,” says the robot’s developer, Gui DeSouza, of the University of Missouri. If humanity expects to feed itself in the coming decades, it must figure out how to increase yields, which means packing more crops into the same amount of land.
All of this data will help scientists understand how corn will fare in a warming world. And corn is just the start: Vinobot could trundle into fields of other crops as well. “We have a growing population that demands more and more food,” says DeSouza. “We have to produce more, we have to optimize the field, we have to optimize the planting, we have to increase the density. How are we going to adapt to those needs?”
Robots are certainly a start. Because good luck getting humans to stop making babies.
I know you've all seen lists like this before: what is the "world's most dangerous volcano?" Most of the time, that discuss devolves quickly into something about "supervolcanoes," which is very exciting and all because they can generate massive eruptions. However, they are far from being the "most dangerous" volcano.
But wait, what does "most dangerous" even mean for volcanoes? Are you talking about a volcano that is most active? That would be Kilauea in Hawaii and that's far from "most dangerous". What about the biggest volcano? That's hard to quantify, but maybe it would be Yellowstone ... and you'll see why it is definitely not the "most dangerous." What about largest eruption? Even that isn't the best measure of "most dangerous" because some very large eruptions have been in the middle of nowhere.
So, what would I consider "most dangerous?" It comes down to a few key factors:
Really, what I consider as "most dangerous" is a volcano that has the highest likelihood of significant numbers of casualties in an eruption. What is "significant numbers?" Maybe over 500 deaths? So, that got me thinking about deaths in volcanic eruptions over the last ~120 years. And guess what? We've gotten better at not being killed in large numbers by volcanic eruptions.
The number of deaths in volcanic eruptions over the last 117 years. The data is culled from a variety of sources.
What is this chart saying? Well, at least to me, there are two things: (1) there is a constant, base level of deaths (fewer an 100 deaths per event) that can be chalked how volcanoes can be unpredictable on the local scale and (2) since the start of the 20th century, large casualty events with >1000 deaths have been going down. Now, there is one exception after 1902: Nevado del Ruiz in Colombia in 1985, where 28,000 people died in volcanic mudflows (lahars), but it might actually be the exception that proves the rule.
Why are deaths down? It is clearly not because there are fewer of us. Population has increased by about 5 billion since 1900, so there should be more people living near volcanoes, especially in places growing quickly like South and Central America along with southeast Asia. It is clearly not that volcanic activity is lower—more or less, the Earth's volcanic engine keeps chugging along at rates that we've seen over the last 10,000 years (at the very least).
My bet is that this is the signal that we're just getting better at monitoring volcanoes, planning for eruptions and mitigating the results of an eruption. One of the best examples of when the lack of volcano monitoring and hazard plan was most evident was that outlier, the eruption of Nevado del Ruiz in Colombia. In that case, the dissemination of information to people living near the volcano was so chaotic and the disagreement about what the volcano might be next was so prominent that tens of thousands people died. This could have been avoided as there was potentially hours between the start of the eruption and the arrival of the lahars in the towns that were hit, so people could have walked to safety.
It is scenarios like that at Nevado del Ruiz that keep volcanologists up at night. So, when deciding what volcanoes might be more dangerous, you really need to think about both the volcano and people. That's what I'll get into as I try to figure out what are really the "most dangerous" volcanoes in Part 2.
As a physics faculty, I have two jobs. The first is coach. I help students wrestle with concepts and ideas. That makes me something like Gregg Popovich of the San Antonio Spurs, but with more equations. I'm also an evaluator. I determine how well students understand the material I've taught them. Yes, I find it odd that I do both of these things. It's like having Popovich coach the team and referee the game. But that's how it is in education.
Usually, I use this space to talk about my helping students understand physics. (Or I ponder important questions like whether Spider-Man can jump onto a ferry or Han Solo can survive light speed.) Today, I want to talk a little about evaluation—or what you might call "grading." If you think about it, there are three methods you can use here. Which method you use depends upon how you view the concept of "grades" and what you think about student populations.
I'm not sure this is truly "traditional," but I imagine this is the default method and the one you know best. It works like this: A student takes a test. The instructor grades it and uses the numerical score (say, 78 out of 100) to determine a percentage and assign a letter grade. Typically, an A represents a score of 90 to 100 percent, a B represents a score of 80 to 89, and so on, with anything less than 60 percent being an F. Top-notch schools that take things seriously might ramp that up a bit so an A is 93 to 100 percent, a B is 85 to 92 percent, and so forth. That would show you mean business. Stricter scales seem especially popular among high schools. I'm not sure why.
But traditional grades assumes that the instrument used to evaluate students (usually a test) is accurate. If every student scores well, they all earn an A. If everyone bombs the test, everyone fails. This means the class average could be any value, not just a C. Such a test is like a ruler: It measures the students without regard for any other factor.
Ah, but what if the test is not accurate. What if you've made it just a bit too hard, or easy? You might "curve" the grades. This method assumes the student population is a normal, or at least stable, distribution. Given a normal (statistically and behaviorally) class of students, you would expect only a few to earn an A and only a few to fail. The class average should be a C.
With this method, you administer the test and grade it. After determining the raw scores, you adjust individual result so the results follow a normal distribution with a few students get A's and a few getting F's. In other words, you create a bell curve, following whatever calculation needed to use the raw score to produce the reported grave. In other words, the highest grade might be a 93, so you might make a score of 83-93 represent an A.
Grading on a curve is less like a ruler than a race. Since it assumes the distribution includes just a few A grades, each student must race to the top. Continuing that metaphor, it doesn't matter what your race time is, only that you finish first (or maybe second). I'm not suggesting that this method is wrong, only that it carries consequences.
My students often ask, "Are you going to grade on a curve?" I suspect they're really asking, "Will you add points to everyone's grade?" Grading on the curve doesn't mean adding points. It means adjusting the grades so the overall results follow a normal distribution. Although I don't grade on a curve, I've always thought it would be fun to do so with simple test. I think students would freak out if they scored 91 out of 100 and earned a C.
On second thought, maybe that isn't a good idea.
The third method, called standards based grading, looks a bit like traditional grading, but doesn't assume the testing method is reliable. Instead it assumes there are accepted standards of learning. For my introductory physics course, those standards might include:
Standards based grading is not about scores, but skills. Some teachers prefer detailed standards, but I tend to like more general benchmarks. In either case, standards define a skill or proficiency you expect students to possess. Completing a test might represent just one estimate of a student's level of understanding. Perhaps you have them come back to the concept later in the semester with another way of showing they truly understand it.
I do this in my courses. I typically start each semester with a set of standards I share with my students. During the semester, I periodically quiz them. Student who don't perform well on the quiz can create a brief video in which they solve a problem to demonstrate their understanding. I encourage everyone to continue creating problem-solving videos throughout the semester to improve their standard score.
This approach works well in my experience. It places the emphasis on what students understand, not how they perform on a test. And I find it surprising how quickly the videos reveal what a student understands, and where they might need help. You don't have to use videos, of course. There are lots of ways to implement standards based grading. Shawn Cornally, Frank Noschese, and Andy Rundquist are three people with some great ideas. If you are looking for some help, I suggest you start with these three.
Over the next 45 years, the population of people older than 65 will double, according to the Department of Health and Human Services. As the population ages, our healthcare system will face millions of frail patients whose bodies are beginning to shut down. The considerable advances in medical technologies over the past century will help some elderly people live longer lives. For others, however, these technologies serve to prolong the dying process.
Jessica Nutik Zitter, MD, MPH is an expert on the medical experience of death and dying and the author of Extreme Measures: Finding a Better Path to the End of Life. Zitter is board-certified in the specialties of pulmonary/critical care medicine and palliative care medicine.
As an ICU physician, I've used technologies like breathing machines and feeding tubes to save lives that would have been lost just a few decades earlier. But I've also seen the substantial costs, both human and financial, of some medical advances. Many patients die protracted deaths while being kept alive by machines—which, research shows, they would not have chosen had there been adequate communication about their options beforehand.
One example is the mechanical ventilator, or breathing machine, the trademark tool of intensive care medicine. When breathing machines were first widely used in the 1930s and '40s, they did what humans could never have imagined a generation earlier: They kept young polio victims alive until their bodies cleared the virus that had decreased their ability to breathe. Thanks to these miraculous machines, tens of thousands of patients who would otherwise have died recovered and went home. Over the next several decades, it was assumed that everyone wanted and deserved access to these treatments. And so breathing tubes were routinely inserted without question into almost anyone with respiratory failure, even patients clearly at the end of life from old age or serious illness.
But unlike the polio victims, whose young and otherwise healthy bodies were often able to spring back to life after the illness had passed, the frail elderly and terminally ill are much less likely to recover from the conditions that cause respiratory failure. Yet that is rarely discussed: The technological imperative—if we have it, we should use it—is assumed by doctors and patients alike.
But my experience tells me that if patients actually understand what living on these technologies looks like, they will think twice before accepting these treatments.
Prolonged Mechanical Ventilation, or PMV, is a condition of permanent dependence on machines. It is much more common in the elderly, and as the population ages and more elderly patients are at risk for respiratory failure, more patients will be placed on breathing machines, unlikely ever to be freed from them. Patients with PMV cannot live at home and must remain in facilities where they are cared for by personnel trained to run these machines. These patients live permanently connected to life-prolonging machines by tubes surgically placed into their necks and stomachs. Most will never get out of bed again, eat independently, or talk. Their arms will be tied down in order to prevent tubes from becoming dislodged. This is how they will live until they die. And for some, this might be acceptable.
Telemedicine Could Be Great, if People Stopped Using It Like Uber
Assisted Death Laws Won’t Make It Better to Die in the US
Your Healthy Lifestyle Won’t Necessarily Make You Healthier
But most are never given the choice. Ample data show that patients who are provided with a full range of information about their prognosis and treatment options choose to receive less technological intervention than those placed on the default treatment path. Yet it is alarming how rarely doctors talk with their patients about the range of options and the burdens of these celebrated technologies.
The human suffering brought about by this default use of technology is clear, and the fiscal cost to society is impossible to ignore. In the case of prolonged mechanical ventilation, one study showed that the predicted cost per year for a 65-year-old patient was $82,411. For an 85-year-old, it was $206,000. Given the increasing numbers of elderly patients on prolonged mechanical ventilation, the projected costs are staggering.
Doctors must come out from behind our technologies and catheters. With medicine's heightened focus on technology, physicians have allowed our communication skills to atrophy. But doctors owe patients critical information in order for them to retain both the autonomy and the dignity they deserve.
The general public has a role to play as well, by considering and documenting preferences for end-of-life care. This will require every patient to take a more active role, first seeking out information about her illness and prognosis, and then creating a plan that is shared with loved ones and healthcare professionals. Completing a state-specific Advance Directive—which documents broad preferences around the use of technology for serious illness—is a good first step. And these goals and preferences should be revisited with regularity as time passes and medical conditions change.
Physicians, patients, and family members must look carefully at the tendency to assume that technology can solve every problem and acknowledge that sometimes the best way to take care of a sick person is not with a machine.
You become aware of Anicka Yi’s work before you even step into the gallery. The pungent odor of decomposing kombucha leather, the acrid tang of burning paper, or the cloying sweetness of deep-fried flowers wafts through the halls. The New York–based artist specializes in olfaction, evoking reactions through sense of smell. “I create scents based on a narrative,” Yi says, “whether it’s about capturing an elusive memory or trying to profile a human being.”
For her latest solo exhibition, which opens April 21 at the Guggenheim Museum in New York, Yi sought art in an unlikely place—the armpit—extracting the essence of human sweat as a meditation on identity. She translates the chemical compounds into scents, which are emitted alongside live bacteria sculptures. “I’ve always maintained that scientists and artists have a lot in common,” Yi says. “There’s a lot of experimenting—and failure—involved.”
There are also surprises. While interpreting her odorous sample set into a sweat-inspired fragrance, Yi experimented with notes of ginger, cypress, and coriander. Sounds enchanting enough to be a perfume: Armpit, by Givenchy.
Swab
Yi collects sweat and bacteria samples from subjects.
Isolate
A forensic scientist uses chromatography to distill each sweat sample into its chemical compounds.
Identify
A Parisian perfumer helps Yi translate the compounds into scents.
Support
Biologists regulate the temperature and nutrition for the bacteria to thrive.
Exhibit
With plexiglass and resin, Yi creates a petri-dish-like vessel to display her living sculptures. The representative scents waft through the room.
This article appears in the April issue. Subscribe now.
Meet the stentor, a gigantic single-celled organism that can regenerate and ink like a squid.
It's not unusual for space agencies to wax lyrical about how their work exploring all that lies beyond Earth's atmosphere is for the shared benefit of humankind. It's probably expected. So when, at a panel during the 33rd annual Space Symposium in Colorado Springs, the head of the Russian space agency says things like "How should we collaborate for the benefit of all of us to get the best result?” and “We need to find the way how can we do it together,” nobody seems to question his motives.
Which maybe they should have, since that country's space agency, Roscosmos, hasn't sent significant representation to the symposium in over 20 years. During this panel, which included 14 other space-agency leaders, Roscosmos general director—a dark, handsome man named Igor Komarov—puts special emphasis his country's desire to collaborate with the fledgling space programs of emerging nations, like Vietnam and Venezuela. Komarov sticks to feel-good terms like "cooperate" and "collaborate" when he talks about international partnerships—which he and other Roscosmos reps do throughout the symposium. But his agency's motivation seems more about another C-word: customers. Last year, the Russian government restructured Roscosmos as a state-run corporation, and the cash-strapped organization is using these altruistic overtures to cultivate nascent space programs into new customers dependent on Russia's 60 years of orbital expertise.
Russia is, of course, not the only space organization looking to profit in the name of higher ideals—SpaceX can only reach Mars and save civilization if it launches a lot of satellites. And mutually beneficial partnerships have been key to space exploration since the fall of communism gave way to the idea that space exists beyond the borders and nationalism of Earth. But such idealism overlooks the endeavor's roots in the fertile soil of nationalist competition, the still-present remnants of that country-centricity, and something else: money.
Right now, Roscosmos isn't just chasing ideals: It's struggling to survive. "Their program is in a very fragile condition, despite what Komarov, et al., were saying in Colorado," says John Logsdon, founder and former director of the Space Policy Institute at George Washington University. He lists some Russian issues: recent budget cuts; problems with the Proton rocket; delays with their next-generation Angara rocket. "It’s a program that’s in trouble," Logsdon says.
Space transcends borders in very pragmatic ways, in addition to the get-along ones. Roscosmos already partners with big space players like the US, Canada, and Europe, and with nascent programs like those in Vietnam and India. It is working with the European Space Agency on the ExoMars—an orbiter (which is fine), a lander (which is not), and a future rover (which is ¯\_(ツ)_/¯). NASA charters Russian Soyuz rockets to hurl astronauts, science experiments, and plenty of other US space stuff into orbit. And cosmonauts and astronauts have floated side-by-side in the International Space Station for so long that they speak a hybrid language called Runglish.
Such collaborations arise partly from practicality. If you’ve listened to anyone knowledgeable talk about actually doing that, you’ve heard the tired phrase “space is hard.” Also expensive. So when it comes to the hardest of that already hard stuff, sharing the technical and financial burdens is the only way to make it happen. “It’s always natural for any country to say, ‘I want my own indigenous capability on launch and satellite,’” says Steve Isakowitz, president of the Aerospace Corporation. “The reality always hits that we all don’t print money, and so we don’t have infinite resources."
And so countries work together. It's at once the best way to solve problems that affect multiple nations, like what to do about space debris, and the best way to spend less and still accomplish the same goal. But, spending less isn't the only path to financial success. Making more also helps a lot.
What better place to announce that you’d like to collaborate with "emerging space nations"—future customers—than at a gathering where 30+ countries are there to hear it? These countries will need launches, hardware, and expertise—and perhaps, just perhaps, they also have natural or human resources that Russia does not.
At a Roscosmos-only press conference convened later in the symposium, Komarov explains what Roscosmos is—which is to say, not at all like NASA. It is a “state corporation:” An umbrella company owned by the government. That’s new. The “Roscosmos State Corporation for Space Activities” took charge of the country’s space program and regulations in January 2016. And this Roscosmos isn't just in charge of launching rockets and doing science. Its mission statement (printed right there on its website) puts it in the business of “placing orders for the development, manufacture and supply of space equipment and space infrastructure objects,” “international space cooperation,” and “setting the stage for the future use of results of space activities in the social and economic development of Russia.”
Roscosmos has a near monopoly on the Russian space industry. It encompasses more than 60 companies and 250,000 people. And in the spirit of collaboration, it is using those resources to do new things, like develop technology, Earth observation capacity, and communications systems for Vietnam, Venezuela, Brazil, Mexico, and Chile. Oh, and they are helping those countries develop their own experts—and space policies. In other words, Russia—in “helping”—is also shaping not just how the international space industry shapes up but also how it functions politically.
There’s no reason Roscosmos has to help. So what's their angle? When asked directly whether Roscosmos why it is putting so much effort on collaboration, Sergey Savelyev, who’s in charge of the agency's international operations, comes clean. “These are prospective clients,” he says to me. Then, he chuckles.
Put another way, if Russia helps new nations develop functional space agencies and aerospace industries, then, suddenly, those places have space needs (costs) where there were none before. And who will they turn to for those needs? And with whom will they share their bounty of resources, data, infrastructure? Russia, of course.
Space agencies don't usually say much about money-making or resource-gathering when they talk about partnerships. "For the good of humanity" sounds better. But it's not like Russia's customer focus, to use corporate speak, makes them unique. "They’re kind of come-lately to the party," says Logsdon. "That’s been key to the Chinese international space cooperation strategy for a number of years, to work with emerging countries." China doesn't so much want customers as, say, sources of raw material. And to that end, they've been working with Brazil for three decades. "They've also been targeting sub-Saharan Africa and other Latin American countries as potential partners for a number of years," says Logsdon.
There's no reason to shame Russia for its ambitions. On the other hand, if you are rooting for some other space agency—or space corporation—you might have other worries. For instance, will Russia's growing market capture cut into SpaceX's customer base?
Not necessarily. "Saying they want to do it doesn’t mean they’re going to do it," says Logdson. "First, they have to be successful in seeking new partners." They're experienced, sure, and they've got the goods. "But their hardware is not in very good condition; their industrial base is hurting," he continues. I don't see this as a threat to other space countries as much as it is a shift in direction for a program that’s in a bit of trouble."
And that's a smart strategic move. Space may be hard—but it’s a lot easier if you’re flush.
Your DNA guides the plot for the story of you. And among the many important spoilers it contains is whether you are likely to contract certain diseases—things like Alzheimer's, Parkinson's, and celiac. On Thursday, the FDA approved genetic testing company 23andMe to sell customers foreshadowing about their susceptibility to these, and seven other heritable genetic traits.
That's a huge win for the company, which originally billed itself as a pioneer in quantified health care. However, the FDA curtailed the company's ambitions three years ago when it shut down 23andMe's health information services. The feds didn't think 23andMe's information was clear or complete enough for consumers. People could misinterpret the results and seek out treatment they didn't need, for example. Since then, the company had scaled back its ambitions to providing mostly hereditary data—including a social media component to connect you with long lost family who also took the test. This week's decision puts 23andMe closer to its original mission. But getting there took work, not just in making more accurate tests, but figuring out better ways to share the results.
Back when 23andMe started in the late 2000s, the company, "really did not understand that this might be FDA regulated," says Kathy Hibbs, 23andMe's chief legal and regulatory officer. As 23andMe's service grew to include more and more information about things like migraines or Alzheimer's risk, the feds piled on more and more pressure. The company tried to work with the FDA, but ultimately received a curtailment letter in November 2013.
Hibbs joined 23andMe a few months later, in early 2014, and has since then been a force in the company's huge effort to win back the FDA's approval. However, the feds typically base these rulings off precedent—and 23andMe's services were basically unprecedented. Without a recipe for what kind of data they would need in order to approve the company's services, 23andMe and the FDA would just have to figure it out as they went along.
One of the bigger challenges was making DNA test results comprehensible to the average person. "People aren't good at making sense of probability" says Steven Heine, a psychologist at the University of British Columbia who studies the ways humans think about genetics (but not with 23andMe). If someone sees a high risk result, like a 95 percent chance of contracting a disease, they tend to round up and assume they have that disease, even though one in twenty people don't. If the risk is a middle number like 30 or 60 percent, it's hard to know what to think.
To solve this problem, a plethora of 23andMe scientists—a task force including geneticists, biostatisticians, study design experts, a genetic counselor, and a medical doctor—iterated through versions with focus groups. That means large studies across demographic groups including people of different ages, ethnicities, education levels, and genders. They repeated these multiple times, finally landing on test labels and statistical reports understandable to 90 percent of the people they tested.
The labeling challenge is similar to that for at-home HIV tests. But while those tests do carry the risk of a false-negative, the result is usually more clear-cut than a test for genetic information. A person can positively have a given genetic mutation but never get the disease. And some diseases with a genetic component have nuanced risk profiles, complicated by mutations in other genes, epigenetic fluctuations, or environmental factors like diet or exposure to certain chemicals. 23andMe had to fine-tune the reports for each of the ten diseases separately to make sure that the subtleties of disease risk for each were easy to understand.
Heine, who is a 23andMe customer, says these new services are quite different from what the company offered before the 2013 ruling. He says then, his results included precise numbers that were hard to understand, like, you have a 16.2 percent of contracting this or that horrible disease. Oookay, so does that mean only freak out 16.2 percent of your total capacity for freak outed-ness, or what?
The problem with those early numbers is they were aggregations based on multiple weakly disease-associated genetic mutations. The newer tests only examines genes with really strong associations. Heine, whose soon-to-be published book DNA Is Not Destiny: The Remarkable, Completely Misunderstood Relationship between You and Your Genes discusses these problems in relating to genetic risk, says people have a much easier time understanding these strong genetic linkages. What's more, these kinds of tests are even more scientifically-grounded.
Most of these fixes focus on mitigating overreactions. But Heine and other researchers are finding that the opposite is also true. "In general, people don't seem to be as worried as researchers imagined they would be," Heine says, though the amount of distress does correlate with how bad the news is. That said, 23andMe's new labeling does include specific language about how being anxious is normal, and advising people to speak to a doctor if they're feeling too anxious about their genetic information.
Now that 23andMe has approved lab standards for genetic testing and labeling, getting approval for additional genetic tests will be much more straightforward. And because 23andMe has one of the largest databases of human genomes in the world, they have the potential to find new genes for disease. The company may have been over-promising in the past, but as a research tool, 23andMe has a lot of promise.
If you want to put an eye out, by all means, fly a quadcopter indoors. Shred the houseplants and ruin the linens. Give the dog a complex.
Really, it’s a shame that drones don’t work so hot indoors. After all, they’d be immensely useful for, say, decommissioning chemical and energy plants by providing a view of overhead pipes and such that terrestrial rovers can’t reach. Rescue roboticist Robin Murphy of Texas A&M may have solved this problem—with marsupials.
Well, ones made of metal and plastic, to be clear. These are the so-called marsupial robots: A drone as the baby tethered safely to the mother robot, in this case the military-grade PackBot tracked vehicle. (Yes, the marsupial metaphor is perhaps a bit stiff, but better than “umbilical robots.”) “The nice thing about the tethered UAVs is that people in the chemical and nuclear industry don't freak the minute you say you're going to fly a UAV indoors,” says Murphy.
You can forgive their reaction. After all, pretty much everything in a decommissioned plant can explode. Also, the visibility sucks, the place teems with obstacles, and you can at any moment lose your connection to the drone, sending it crashing into ... something you don't want it crashing into. Scream around with a UAV and you’re asking for disaster. Literally.
Tether it to a tracked vehicle, though, and you’ve got yourself a safer way of navigating dangerous environments. Fotokite is made for this sort of thing, autonomously sticking to one place like, well, a kite—only not super boring. “It's patented algorithms that we have that make a drone estimate its position and localize itself based off of tension from a tether, not based on things like GPS or optical sensors or things like that,” says Fotokite CEO Chris McCall.
The mother robot provides stability, and power, too. Tapping into its mother’s battery lets the UAV stay airborne far longer than your typical quadcopter. What's more, the tracked vehicle can handle the heavy lift of processing. For instance, it might carry a bulky lidar unit to map the environment with lasers.
And so mother and child can work together to better explore perilous indoor environments. Kinda sounds like a Michael Bay movie but without all the explosions.
North Korea, led by inveterate saber-rattler Kim Jong Un, is drawing attention for a recent spate of missile tests. Last Wednesday, the isolated nation launched a medium-range missile into the Sea of Japan. The test signaled several things: That North Korea wants to be taken seriously as a regional power, it has weapons capable of hitting distant targets, and—perhaps most seriously—that it is developing stronger nuclear weapons.
Nuclear experts believe North Korea is prepping for its sixth nuclear test. And the signs aren't just out in the water. From satellite imagery, and other sources, they are looking for evidence that the isolated nation is planning to load a bomb onto a railcar, drive it deep into a mountain tunnel, and detonate—perhaps as soon as April 15, which is the birthday of the nation’s founder, Kim Jung Un's grandfather Kim Il-Sung.
That potential test, and all the very real incidents preceding it, already stirred up the geopolitical hornet's nest. Soon after last Wednesday's launch, President Trump told Japan's Prime Minister Shinzo Abe that "All options are on the table," in dealing with North Korea. Before meeting with China's General Secretary Xi Jinping this week, Trump said he would unilaterally confront North Korea if China did not rein in its unruly neighbor.
Whether or not this leads to a full blown crisis, the experts are keeping watch for signals of the test itself. “With any nuclear weapons program, you need to validate your engineering and your computer modeling,” said Joseph Bermudez, Jr., senior analyst at 38North.org, a project sponsored by the US-Korea Institute at the School of Advanced International Studies at Johns Hopkins University. He says this is North Korea's probable mode at the moment.
To verify his hunches, Bermudez has been poring over commercial satellite images taken every few days of North Korea’s Yongbyon Nuclear Complex, where the scientists make the bomb, and at the Punggye-ri Test Facility where the bombs are detonated inside horizontal tunnels that cut through the heart of a mountain. Bermudez compares images over time, looking for frame to frame changes. Is that water flowing down the mountain site drainage from bombsite tunnels, or just melting snow? Do piles of earth and rock mean the North Koreans are cleaning up an old tunnel, or building a new one?
It's guesswork buttressed by years of expertise. “We cannot look into the tunnel,” he said. “We cannot say definitively from satellite imagery alone that a test is imminent. We can say they have taken the steps to be able to conduct a test at any time of their choosing.” But he can say that this current activity is very similar to what he saw prior to nuclear tests in 2016. In the past two weeks, for example, he’s seen a large formation of people in front of the test site’s administration building, as if a high dignitary had come to the site. At the same time, the North Koreans have covered up equipment and supplies, and moved stuff around when commercial satellites don’t have an optimal angle during daytime fly-bys. There was also the arrival recently of an unusual airplane at a nearby commercial airport that signaled the visit of a North Korean VIP.
North Korea's goal with these tests is probably to boost their nuclear device's yield, while also making it smaller and lighter. One way to do that is to use less nuclear material—either plutonium or enriched uranium—and add deuterium or tritium gas, explained David Wright, co-director of the global security program at the Union of Concerned Scientists. These elements can also help super-charge the nuclear yield. “By pumping extra neutrons into the system, you can increase the amount of plutonium that fissions before it blows itself apart,” he said. Then they need to test it, to make sure the bomb itself can withstand the pressures of being launched atop a missile.
Once they settle on the nuclear recipe, the scientists will connect it to sensors to tell them what happened in the milliseconds between detonation and the time the sensors are fried to a crisp. “It’s as much a science experiment as anything else,” says Jeffrey Lewis, director of the East Asia Nonproliferation Program at the Middlebury Institute for International Studies in Monterey, CA. “You want all the basic data, so you have to install instrumentation, the cabling and sensors, even though it doesn’t survive the blast.”
And even though the tests are science, the pursuit is political. Whatever happens beneath that mountain has the power to shake the world.
To scientists, citations are currency. No, you can’t use them to put gas in your car or food on your table. But surviving in academia means publishing papers people want to read and, more to the point, cite in their own research. Citations establish credibility, and determine the impact of a given paper, researcher, and institution. Simply put, they fundamentally shape what people believe.
The problem with this lies in determining who’s citing whom. Over the last few decades, only researchers with subscriptions to two proprietary databases, Web of Science and Scopus, have been able to track citation records and measure the influence of a given article or scientific idea. This isn’t just a problem for scientists trying to get their resumes noticed; a citation trail tells the general public how it knows what it knows, each link a breadcrumb back to a foundational idea about how the world works.
On Thursday, a coalition of open data advocates, universities, and 29 journal publishers announced the Initiative for Open Citations with a commitment to make citation data easily available to anyone at no cost. “This is the first time we have something at this scale open to the public with no copyright restrictions,” says Dario Taraborelli, head of research at the Wikimedia Foundation, a founding member of the initiative. “Our long-term vision is to create a clearinghouse of data that can be used by anyone, not just scientists, and not just institutions that can afford licenses.”
Here’s how it works: When a researcher publishes a paper, the journal registers it with Crossref, a nonprofit you can think of as a database linking millions of articles. The journal also bundles those links with unique identifying metadata like author, title, page number of print edition, and who funded the research. All of the major publishers started doing this when Crossref launched in 2000. But most of them held the reference data—the information detailing who cited whom and where—under strict copyright restrictions. Accessing it meant paying tens of thousands of dollars in subscription fees to the companies that own Web of Science or Scopus.
Historically, just 1 percent of publications using Crossref made references freely available. Six months after the Initiative for Open Citations started convincing publishers to open up their licensing agreements, that figure is approaching 40 percent, with around 14 million citation links already indexed and ready for anyone to use. The group hopes to maintain a similar trajectory through the year.
“It’s not that much actual work to do it, it’s just about flipping a switch and getting publishers to agree to releasing this data,” says Taraborelli. He called on the open-access movement to focus its effort on citation data in September. In the months since, heavyweight publishers like Springer Nature, Taylor & Francis, and Wiley—three outfits that publisher nearly 25 percent of all peer-reviewed journals— are among the 29 making their references freely available.
“It will make our customers’ lives easier by helping data scientists to mine a large body of references in one go,” says Steven Inchcoombe, chief publishing officer at Springer Nature. The company signed on in February and already has provided reference lists to more than 6 million articles in about one-third of its 3,000 journals, with the rest coming later this year. Elsevier, which owns Cell and The Lancet, not to mention some 30 percent of Crossref’s citation data and Scopus, is sitting things out for now. The initiative can’t hit its goal of 100 percent coverage without bringing the Dutch publisher aboard. But that’s not stopping Taraborelli from thinking about how to divine deeper truths beyond just citation metadata.
“We really want to have the ability to mine the contents of the entire paper,” he says. “Because then we’re talking about enabling the provenance of specific facts.” He points to a Wikimedia project that examined every published paper on the Zika virus (only about 1000 studies). Aided by machine learning, the team built a map connecting the dots between statements that get shared as facts online (think Wikipedia, Britannica.com, etc) and specific papers. The idea being, with enough data you could determine how common truths get formed, and trace them back to the primary source. Rigorous analysis like this he says, would advance fields much more quickly than at their current rate. And it would make it easier to figure out where the general public is getting its information.
That could come in handy when trying to combat the rising tide of alternate facts. And while science is more about narrowing the windows of uncertainty than claiming truth with an upper-case “T,” there’s always room to improve existing methods for doing that, starting one footnote at a time.
Octopuses are aliens living on Earth. They solve puzzles, use tools, and communicate with color. They also squirt ink, open jars, and occasionally pull a prank or two. Given their remarkable intelligence and cunning ways, it takes a lot to surprise the biologists who study these wonderful creatures and their equally weird cousins the squids and cuttlefish.
But when Stanford University geneticist Jin Billy Li heard about Joshua Rosenthal’s work on RNA editing in squid, his jaw dropped. That’s because the work, published today in the journal Cell, revealed that many cephalopods present a monumental exception to how living things use the information in DNA to make proteins. In nearly every other animal, RNA—the middleman in that process—faithfully transmits the message in the genes. But octopuses, squid, and cuttlefish (but not their dumber relatives, the nautiluses) edit their RNA, changing the message that gets read out to make proteins.
In exchange for this remarkable adaptation, it appears these squishy, mysterious, and possibly conscious creatures might have given up the ability to evolve relatively quickly. Or, as the researchers put it, "positive selection of editing events slows down genome evolution." More simply, these cephalopods don't evolve quite like other animals. And that could one day lead to useful tools for humans.
First, a primer. You can think of RNA as a copy of the genetic instructions for a protein. When an animal cell makes a protein, it starts by transcribing the DNA—duplicating the A, T, C, and G nucleotides into RNA strands of A, T, C, and U. The RNA, composed of ribonucleic acids, is a lot like DNA but without the double helix structure. It's shorter, too, often containing only the information needed to create one protein at a time. Ribosomes, the little machines that create proteins, read the RNA sequence like a blueprint.
Once the organism—a human, a praying mantis, a quokka, whatever—transcribes DNA into RNA, it often splices out large chunks of material called introns that aren't needed to produce the protein. But occasionally, a special sequence in the RNA lets it hook up with an enzyme that does true RNA editing. That enzyme can edit a single spot in the RNA, turning an A into an I, which the ribosome interprets as a G.
Subbing out one spot in the code may seem like a minor switcheroo, but it can change how—or whether—a protein functions. Theoretically, it changes the genome's level of complexity: Humans possess just two copies of a given gene, but add a few RNA editing sites and the number of protein variants rises exponentially. An animal could use RNA editing to change how its proteins work if its environment changes. For instance, some RNA in squid get edited when the weather changes so that their proteins work properly at different temperatures.
Although most organisms posses the enzyme needed for gene editing, it isn't widely used. In fact, University of Michigan evolutionary biologist Jianzhi George Zhang says RNA editing is, in most cases, deleterious. The consensus among folks who study such things is Mother Nature gave RNA editing a try, found it wanting, and largely abandoned it.
Rosenthal, a neurobiologist at the Marine Biological Laboratory, was a grad student studying a specific protein in squid when he got an an inkling that some cephalopods might be different. Every time he analyzed that protein's RNA sequence, it came out slightly different. He realized the RNA was occasionally substituting A's for I's, and wondered if squid might apply RNA editing to other proteins. Rosenthal, a grad student at the time, joined Tel Aviv University bioinformaticists Noa Liscovitch-Braur and Eli Eisenberg to find out.
In results published today, they report that the family of intelligent mollusks, which includes squid, octopuses and cuttlefish, feature thousands of RNA editing sites in their genes. Where the genetic material of humans, insects, and other multi-celled organisms read like a book, the squid genome reads more like a Mad Lib.
So why do these creatures engage in RNA editing when most others largely abandoned it? The answer seems to lie in some crazy double-stranded cloverleaves that form alongside editing sites in the RNA. That information is like a tag for RNA editing. When the scientists studied octopuses, squid, and cuttlefish, they found that these species had retained those vast swaths of genetic information at the expense of making the small changes that facilitate evolution. “Editing is important enough that they’re forgoing standard evolution,” Rosenthal says.
He hypothesizes that the development of a complex brain was worth that price. The researchers found many of the edited proteins in brain tissue, creating the elaborate dendrites and axons of the neurons and tuning the shape of the electrical signals that neurons pass. Perhaps RNA editing, adopted as a means of creating a more sophisticated brain, allowed these species to use tools, camouflage themselves, and communicate.
That remains nothing more than a hypothesis. The researchers can't say precisely when and where the RNA is edited or to what end, although they have shown that editing key brain proteins changes the function of those proteins. Rosenthal hopes to discover what role RNA editing might play in the intelligence of these undersea predators, because he wonders whether it might lead to treatments and therapies for cystic fibrosis and other diseases. Given the ethical boundaries around genome editing with tools like CRISPR, he’s exploring how to use RNA editing instead. That remains a distant goal, but Rosenthal finds comfort knowing that these earthbound aliens have been doing it for millions of years and might one day share their secret.
In addition to rising seas, longer droughts, and all manner of weather nightmares, global climate change promises to make flying just a bit more hellish.
British scientists ran computer models simulating a doubling of atmospheric carbon dioxide from pre-industrial levels of 280 parts per million to 560 ppm—something expected to happen later this century, as the figure now stands at about 400 ppm—to see how it might affect clear air turbulence. The short answer? Flying is going to get a whole lot bumpier. This will trouble the airline industry, which in any given year experiences nearly 800 “turbulence encounters” that result in hundreds of minor injuries and dozens of major injuries. Don’t worry, though. Airlines are developing on-board systems to warn pilots to trouble and stabilize aircraft.
The study, released today in Advances in Atmospheric Science, builds on work Reading University meteorologist Paul Williams published in 2013. Although the earlier paper found that climate change bring bumpier flights, the results failed to distinguish between turbulence that knocks flight attendants off their feet and mild bumps that spill your cocktail.
“The 2013 paper was a first cut, which showed we can generally expect more turbulence as a whole, but didn't dig down into the details,” Williams says. “The new paper looks separately at each different strength category for the first time, so we have individual predictions for the future of light turbulence, which is perfectly safe but distresses nervous fliers, and severe turbulence, which throws people around and can hospitalize people.”
Before getting to that, let’s define clear air turbulence. Unlike the turbulence caused by thunderstorms or downward microbursts that hit before planes land, the disturbances Williams studied occur above 30,000 feet when fast-moving jet streams create eddies and cascades of unstable air. Jet streams—there are two each in the northern and southern hemispheres—are relatively narrow rivers of air that circle the globe from west to east. They meander somewhat from north to south, following the contours of hot tropical air and cold polar air. And climate change has them speeding up.
That means bumpier flights. Williams’ work finds that light turbulence will increase by 59 percent, and light-to-moderate turbulence will increase by 75 percent. In other words, expect more spilled drinks and dire warning to buckle up. Things get worse from there, with the incidence of increasingly severe turbulence going up, up, and up. Truly severe turbulence—jolts harsh enough to toss people around—will climb 149 percent.
Pilots know how to handle turbulence, of course, and get real-time updates of turbulent air from air traffic control and other pilots on the same flight path. Technology can help, too.
A few years ago, European aviation companies led by the French company Thales developed an on-board LIDAR system that could spot clear air turbulence up to 18 miles ahead of the plane. It worked, but it weighed 440 pounds, wasn’t terribly effective, and LIDAR still costs a fortune. “Right now, it’s too expensive,” says Paul Vrancken of the Institute of Atmospheric Physics at the German Aerospace Agency. “There’s not so much interest from the aeronautics industry to do it.”
Still, some big names in aviation want to tackle this problem. The new Boeing 787-10 Dreamliner, for example, sports nosecone sensors that detect turbulence ahead and send signals to computers controlling the rudder, ailerons, and other control surfaces to dampen the effects of turbulence before passengers even feel them.
Technology and training may mitigate the impacts of increasingly turbulent flights, but in the meantime, it may be a good idea to keep your seat belt fastened, even when the light isn’t on.
Of the many great things promised by Crispr gene editing technology, the ability to eliminate disease by modifying organisms might just top the list. But doing that requires perfecting something called a gene drive. Think of gene drives as a means of supercharging evolution to, say, give an entire population of mosquitoes a gene that kills the Zika virus. The trouble is, organisms develop resistance to gene drives, much like they eventually outwit pesticides and antibiotics.
Researchers dedicate no small amount of time and thought to creating gene drives that can outsmart evolution because the potential payoffs are so great. The lowly mosquito transmits dozens of diseases that kill more than a million people every year, making it the deadliest animal in the world. Pesticides, mosquito nets, and medicine won't solve the problem, but gene drives might—provided scientists can make them less likely to succumb to the genetic mutations that might render them useless.__ __
In a paper presented today in Science Advances, Harvard scientists used computational models to test a means of doing just that. The resulting gene can spread to 99 percent of a population in as few as 10 generations, and persist for more than 200 generations without the mosquitoes (or any other population) developing a resistance. Although the researchers did not test their method by tinkering with real mosquitoes, their modeling creates a blueprint for anyone eager to build a more successful gene drive.
Simply put, a gene drive makes a specific gene spread through a population more rapidly than would happen through nature alone, something geneticists refer to as "super-Mendelian inheritance." Typically, this means inserting a bit of DNA into the genome of an organism—say, Aedes aegypti, the primary transmitter of the Zika virus. When the modified, or transgenic, mosquito mates with a wild mosquito, their offspring carry one one copy of the “drive gene” directly opposite its natural counterpart. The drive gene snips out the normal gene and inserts a copy of itself, doing this over and over and over again until every mosquito carries two copies of the drive gene—and therefore, resistance to Zika. That’s the idea, anyway. But because nature is imperfect, mistakes happen. More specifically, mutations happen. The very act of cutting out the normal gene makes the whole system more susceptible to mutations. And if enough of them add up over time and across a population, the drive gene can actually become recessive.
To fight back, science must develop a gene that works even if it isn't perfectly copied, says computational biologist Charleston Noble, the paper's lead author. “The trick is to decouple the cost of resistance and the cost of the drive.”
Noble’s team suggests doing this through a technique called recoding that genetic engineer and paper co-author George Church is developing. Because of redundancies in genetic code, there are times when you can do things like change a C to a T or a T to an A and still get the same proteins even though the DNA sequence is different. To offer an oversimplified explanation, it means you can create a drive that targets a gene essential to survival or reproduction. If the drive inserts smoothly, great. The gene drive drives on. If it doesn't insert itself smoothly, no problem. The mosquito dies, or does not reproduce. And, because the new code for the essential gene doesn’t exactly match the target it replaced, it won't get snipped itself.
“This kind of approach is definitely the direction the field is going to have to go,” says Philipp Messer, a molecular geneticist whose lab at Cornell is among the few testing gene drives in insects. “Whether or not it works experimentally is still an open question.” You can rattle off countless reasons why a method that works beautifully in computer modeling might utterly fail in the wild. Just one example—Noble’s simulations assumed an infinite number of mosquitoes all equally likely to breed with each other. Here in the real world, oceans and mountain ranges and other natural barriers might create populations the gene-driven mosquitos can't or don't reach.
Plus, not all bugs evolve resistance equally. Even within a single species, variations in individual genomes make it hard to predict how effectively a drive gene will insert itself into a population. "All these models assume there's one fixed rate at which these things arise," Messer says. "But that doesn't seem to be the case." Right now, Messer is looking at the rate at which resistant mutations occur in a Drosophila gene drive system. That work remains under peer review, but his lab is already finding mutation rates much higher than previously reported. That suggests the battle against gene drive resistance is far from over, even with an arsenal that includes tools like Crispr.
The first particles in the universe formed when a hot, dense blob exploded. Physicists think that under the extreme conditions of the Big Bang, light transformed into matter: the electrons, protons, and neutrons that later became you and me.
But physicists are confused about how that transformation occurred. Back in the '90s, physicists showed they could convert light into matter by colliding two extremely powerful beams of radiation. They also found that light produces equal amounts of antimatter at the same time. The very first particle of matter would have met an antimatter cousin in an annihilating ka-boom. No more matter.
Yet clearly, matter exists. For some reason, more matter formed than antimatter just after the Big Bang, and physicists don’t know why. “It’s one of the very biggest mysteries in the universe,” says physicist Don Lincoln of Fermilab.
For the last 50 years, they’ve been hunting, both in the lab and in their equations, for processes that produce more matter than antimatter. One contender: a predicted radioactive process where two neutrons turn into two protons within an atom. Theorists think that this process, known as neutrinoless double-beta decay, produces two electrons—but no antimatter. Two new morsels of matter pop into existence in the universe, and detectors should be able to measure them. If this process occurred enough times right after the Big Bang, it could explain where all the extra matter comes from.
Here’s the catch: No one has ever seen two neutrons turn into two protons. Previous experiments and theoretical calculations indicate the process is more likely to occur in specific atoms, such as germanium and xenon; when two neutrons in a germanium atom become protons, the atom becomes a new element, selenium. In a paper published today in Nature, researchers use measurements from their super sensitive detector to calculate that it would take more than 1025 years for half a germanium crystal to decay into selenium via this process. That’s a quadrillion times longer than the age of the universe. “It’s really a very rare event,” says physicist Peter Grabmayr, a member of the Germanium Detector Array collaboration that authored the paper.
Grabmayr isn’t dismayed by those odds. You don’t need half a crystal to turn into selenium to confirm this process occurs, he says. They just need to detect a handful of atoms decaying. If any atom within their 80 pounds of germanium crystal turns into selenium, they can detect the energy of the two electrons produced, which should appear as light when they hit their detector. To avoid other sources of radiation like cosmic rays tripping the detector, they’ve placed the germanium in a vat of cold liquid argon, a mile underground a mountain in central Italy.
It’s still possible that they’ll never detect this process, says Lincoln. “But that’s just an opinion,” he says. “It’s not one that I would stand behind. I would not be shocked if an experiment like this proved my intuition wrong.”
In the meantime, physicists are investigating other processes that might explain why the universe is made of matter. In particular, they want to discover all the ways that antimatter differs from matter because any discrepancy could reveal why their fates diverged in the early universe. Last December, the Alpha experiment at CERN measured properties of antihydrogen, the antimatter particle of hydrogen, but didn't discover any unexpected differences from hydrogen. In January, the Large Hadron Collider Beauty experiment found that when a type of particle called a lambda-b baryon decayed, its decay products flew off at a slightly different angle than those of its antimatter counterpart.
Over the next decade, Fermilab is planning to build an 800-mile underground particle accelerator from Illinois to South Dakota called the Deep Underground Neutrino Experiment. The goal of the experiment is to beam particles called neutrinos and their antimatter counterparts over a long distance, says Lincoln. If neutrinos behave differently from antineutrinos, it could belie another reason why matter outweighs antimatter in the universe.
These searches are useful even if they come up empty, Grabmayr says. The whole point is to figure out the rules that govern the universe. If the particular process that Grabmayr is rooting for doesn’t exist, they can still use its nonexistence to rule out many proposed theories.
Grabmayr’s group plans to keep watching their germanium for the radioactive decay for another two years. Eventually, they want to use up to a ton of germanium in their detector. More germanium means better likelihood of the radioactive decay. “At some stage we will detect it,” Grabmayr says. But for now, they wait.
In Miami Beach, they call it “sunny-day flooding.” You’ll be hanging out downtown under clear blue skies—only to see, whoa, the streets slowly filling with water.
Miami Beach, Florida, is a coastal city built on porous limestone, so as climate change melts polar ice into the oceans, water is literally pushed up out of the ground. “It’s an eerie, scary, unnerving feeling, like something out of a sci-fi movie,” says Philip Levine, mayor of the city of 90,000. On days when Miami Beach actually gets a coastal storm, it can see a 2-foot flood.
So the city decided enough is enough. Levine has begun a $400 million resilience plan that calls for installing high tech drainage systems and painstakingly raising the roads several feet. “It’s not fun to go and raise people’s fees,” Levine says. But what choice do they have?
Think States Alone Can’t Handle Sea Level Rise? Watch California
Rogue Scientists Race to Save Climate Data from Trump
Nations Be Damned, the World’s Cities Can Take a Big Bite Out of Emissions
Global-­warming denialists, including, at times, the new US president, claim that climate change isn’t happening. This is abject nonsense—ask anyone who lives near an ocean. They’re all dealing with the unsparing laws of physics, and the 2.6 inches the sea rose between 1993 and 2014. Flooded basements don’t care whether you believe burning carbon-based fuel is raising Earth’s temperature or not.
That’s why coastal cities worldwide are pumping more than $280 billion a year into an Adaptation Economy, which puts a price tag on preparing for the future. That amount is increasing by more than 4 percent a year in well-off, developed cities.
The money is propelling some ingenious engineering. The Dutch are great at this—Rotterdam already has sophisticated dikes, and the city is building newfangled “water plazas,” buildings with reservoirs that sequester rainfall, letting it seep out into the ground or into wading pools for kids instead of adding to floods. Other innovations even have aesthetic value: In China, Dutch engineers are building a “sponge city” that uses a network of grass gardens and ponds to absorb runoff, an approach they call “living with water.”
Dutch firms, long specialists in the arts of living below sea level, are suddenly in high demand. “Climate change, sea level rise, and the risk of flooding is a great business opportunity for us,” says a somewhat rueful Piet Dircke, head of water management for Amsterdam-­based adaptation firm Arcadis, which is working on the sponge city.
This isn’t only about atoms; the Adaptation Economy has to move bits too. If you want to understand how and where water will inundate the coasts, you have to model it. Cloud supercomputing and lidar—the tech that helps self-driving cars “see”—have already produced better estimates of storm surges. Even greater puzzles remain, such as how intertidal marshland is affected by encroaching salt water. That stuff is “really difficult” to model, says Scott Hagen, director of the Center for Coastal Resiliency.
Consider this a cursed area of innovation—it shouldn’t be necessary. But it is, and we need far more of it. That’ll require political action: States and the federal government need to give cities more dough, and Congress should reform flood insurance so that people have greater incentives to protect their homes, with, say, tide-proof ground floors, or to avoid building in endangered coastal areas. Meanwhile, impoverished coastal cities worldwide are in urgent need of foreign aid—before rising seas create humanitarian and refugee crises.
I recently visited an experimental sponge park created by architect Susannah Drake on the edge of an old toxic canal in Brooklyn. Drake filled recessed concrete boxes with soil and plants specially designed to absorb and dissipate flood runoff. It was post­apocalyp­tically peaceful and strange. This is the one silver lining of our predicament: If we get adaptation right, we’ll not only preserve our cities—we’ll upgrade them.
This article appears in the April issue. Subscribe now.
Graham Johnson is an artist with a curious muse: the human cell. He’s the Matisse of mitochondria, the Goya of the Golgi apparatus. Twenty years ago he graduated from a quiet corner of Johns Hopkins where students draw cadavers instead of cutting them up. At first, Johnson stuck to the medical illustrator canon, animating cells in a classic, cartoonish style. But he dreamed of constructing three-dimensional, data-driven models that could capture all their beautiful complexity. For that, he’d need computers, lots of them. And some really powerful microscopes.
Johnson found them both at the Allen Institute for Cell Science—a Seattle-based research center established in late 2014 by Microsoft co-founder Paul Allen. (Before getting recruited to the center, Johnson completed a PhD in computational biology and molecular graphics.) Today, he and the institute’s team of nearly 50 cell biologists, microscopy specialists, and computer programmers revealed what they’ve been working on the past two years: the Allen Cell Explorer. It’s the largest public collection of human cells ever visualized in 3-D, which serves as fuel for the project’s engine: the first-ever deep learning model to predict how cells are organized.
To create their model of the organic shapes and structures inside the cell, the Allen team trained deep learning algorithms on 3-D images of more than 6,000 induced pluripotent human stem cells. But first they had to make those images. They dyed each cell’s outer membrane and nuclear DNA to stand as lighthouses in a sea of cellular noise. Then they used Crispr/Cas9 gene editing to fluorescently tag well-known proteins in structures like microtubules and mitochondria. Powerful microscopes captured the multicolored light display.

The Allen team then used computer software to stitch dozens of these flat images into 3-D renderings of cells—just like radiologists do with CT scans. These are the images you can spin around in the platform’s browser tool; click here and see the mitochondria, click there and see the nucleus. “After decades of reductionist biology, it’s exciting to put the pieces back together into a whole picture of the cell,” says Johnson, who directs the institute’s animated cell group.
From that library, Johnson’s group took hundreds of measurements—from distances between structures to protein density. Their algorithms used those numbers to kick out predictions of where certain structures should go in a cell. If you give the model pictures of the nucleus and the cell membrane and the microtubules, it can tell you where to find the mitochondria, for instance.
“It’s sort of like if you’ve got some images of the wheels of cars,” says Susanne Rafelski, who leads the institute’s assay development team. “Just from that limited information, it can predict the make and model of those cars.” The website currently showcases comparisons between the model’s predictions and 2-D image data, but future iterations will allow users to generate and explore cells in three dimensions. Down the road, Rafelski says they’ll be able to zoom around time as well as space—seeing what happens to a cell through its life when it’s growing, dividing, damaged, or dying.
But the explorer isn’t just for digital tinkering. Its cell catalog contains detailed information for all of the institute’s fluorescently-tagged human stem cell lines, which any scientist can order online at the cost of distribution (around $600). Rafelski and Johnson are hoping that the suite of resources they’ve compiled convinces more cell biologists and drug developers to conduct their studies with stem cells instead of the cancer cell-derived cultures that are prevalent today.
Cancer cell lines are widely used to test drugs and vaccines for everything from HIV to heart disease. (HeLa, the most commonly used human cell line, is involved in more than 11,000 patents.) And while those are easy to work with and keep alive for a long time—HeLa is 66 years old this year—they also have high mutation rates, which mean scientists using the same cell lines years apart might get wildly different results. “We’re really trying to make a standardization tool,” says Rafelski. “There’s so much variability when we develop prototypes to make different disease models. It’d be much better if we were all testing on the same basic cells.”
The timing is auspicious; stem cells are getting easier (and cheaper) to work with. And scientists are getting better and better at nudging them into becoming different kinds of cells, from neurons and cardioblasts to pancreas precursors and liver buds. Which means that Johnson may soon have more muses than he bargained for. But for now, he’s concentrating on the masterwork in front of him.
In many physics problems, we simply ignore the air drag force. Why? Because of two reasons. First, the effects of air drag are often small when dealing with falling balls and rolling carts (a staple of intro physics labs). Second, calculating the motion of an object with air resistance is really difficult, because the drag force increases with velocity—it’s non-constant. The normal equations in your physics course are created with the assumption of constant acceleration and constant forces.
But just because a force is difficult to model doesn’t mean we should skip it. Here is a classic approach to exploring air drag.
Let’s start with a model and then see if it works. Here is a common way to calculate the magnitude of the drag force on a moving object.

In this equation:
Like I said, this is just the magnitude of the drag force. Its direction is always in the opposite direction of the velocity.
There are two things I need to do: Confirm that the model above agrees with experimental data and measure the drag coefficient for some falling object. I could estimate the other properties (density of air and cross sectional area). But if I look at something like a falling tennis ball, the drag force will just be too small to even notice. The solution is to drop something with a significant area, but low mass. Here is one of those things.
Yes. A coffee filter is perfect for exploring air drag. When you drop it, it mostly moves straight down in a stable position because of the angled sides. Also, when it is dropped from a reasonable height (like 2 meters) it will still reach terminal velocity. Finally, you can stack coffee filters and drop them. If you stack three filters, you effectively change the mass of the object but not the drag force parameters (shape and area).
But what is terminal velocity? I will answer this by considering a falling coffee filter. Suppose I let it drop from some height. Right after release, the coffee filter isn’t moving. Since it has a zero velocity, the drag force is also zero. The only force on the filter is the gravitational force (which is the mass multiplied by the gravitational field g) so that it accelerates down with an acceleration of -9.8 m2 (like any other falling object).

However, since the filter is accelerating down, it will increase in speed. An increase in speed means that there is now a drag force pushing up (since it’s moving down).

Now that there are two forces on the filter, the total force in the vertical direction is smaller than it was when it was first released. With a smaller force, it will have a smaller acceleration—but it will still accelerate. Eventually, its speed will reach a point at which the air drag force is equal in magnitude to the gravitational force.

With equal magnitude forces, the net force is zero. This means the acceleration is also zero m/s2. At this point the filter no longer speeds up and this is called the terminal velocity. The terminal velocity is very useful in that it can be used to find the drag coefficients. So, at terminal velocity I can write the following expression. Note that since the drag force is proportional to the velocity squared, I am just going to combine all of the other constants into one constant that I will write as K.

I can use this relationship to get some data. Here’s what I will do. I will drop a coffee filter and find its terminal velocity. Then I will stack two coffee filters and drop them. With two filters, the mass and thus terminal velocity will be higher. Repeating this, I can get mass and terminal velocity data to use for a graph that will then give me the drag coefficient.
How exactly can I get the terminal velocity for a falling coffee filter? There are several methods that would probably work, but in this case I am going to use a motion detector. This device sends out a pulse of sound and measures the time for that pulse to reflect off an object and come back to the detector. It’s a pretty useful device for measuring motion in one dimension. In this case, I can just put the detector on the floor (pointed up) and then drop a coffee filter on top of it. Here is the data that I get.

Notice that the filter accelerates at first, but towards the end of its motion around 2.5 to 3.0 seconds it seems to be moving at a constant velocity. By fitting a linear function to this part of the data, I can get the velocity of the filter—which would be the terminal velocity. For this particular run you can see the slope is 1.730 m/s.
Now I just need to repeat that exact same drop multiple times (I did it five times) so that I can get an average terminal velocity. But I want to plot a linear function (because linear functions are easier to deal with). If I make a plot of mass vs. velocity, it shouldn’t be linear—but mass vs. velocity squared would be. Here is that graph. The error bars on the graph are the standard deviation of the five runs. Also, I’m not sure why—but this works out much better in the end if I force the fit to go through the origin (instead of letting it have a y-intercept).

The slope of this linear function is not the drag coefficient. However, we can find it from the slope. Since this is a plot of terminal velocity squared vs. mass, I could write this relationship as:

The slope of this function should be equal to g/K. If I assume g = 9.8 N/kg and the slope is 1164 m2/(kg*s2) then the drag constant would be 0.00842 N*s2/m2.
What if I take a bunch of coffee filters and drop them from some height? Could I model the position vs. time for this falling stack? Let me start with an actual filter drop.

I can get position-time data from this video using video analysis. Here is that data.

But how can I see if my model agrees with this data? The answer of course is to create a numerical model (I will use python because it’s awesome). By using a numerical model, I can break the motion of this coffee filter into small time steps. During each of these time steps I can assume that the drag force has a constant value (which is approximately true). This takes one complicated problem and replaces it with many many simpler problems. The many problems are so simple that even a computer can solve them.
I will just jump right into the program. Here it is. Just click the “play” button to run it. Oh, and you can change parts of the code and see what will happen—you probably should.

This doesn’t give exactly the same values as the data from the video, but it’s pretty close. Here is a plot combining both the numerical model and the video analysis data.

From that plot you can see that the model doesn’t completely agree with the data—but it’s pretty close. I’m mostly happy.
Here are some extra questions for you.
 
 
Computers used to require entire buildings to operate. Now they fit in our pockets. Similarly, factory-size electronics manufacturing is approaching a contraction. Want proof? Look at that $50 printer on your desk and imagine, instead of using it to spit out a hard copy of that thank-you note, that you used it to print some digital memory.
Not enough memory to power a laptop. Think smaller, like smart tags to inventory all the crap in your workshop. A study published today in the journal American Institute of Physics has a proof of concept for laser-printed memory cells—basically analogous to transistors—onto flexible sheets of plastic and foil. They aren't ready for prime time, but in the long term, printing your own memory cells could help democratize electronics just like 3-D printers did for hardware.
Depending on your line of work, making these things should only take a fraction of your paycheck. First, there's the $50 laserjet printer. Then get your hands on some PET foil sheets to print the circuits on—a roll of the stuff will cost you a few bucks, max. The priciest component is the silver ink, which runs between $300 and $400 for a 50 milliliter cartridge. Well, priciest depending on how much you spend on the rudimentary education in electrical engineering you'll need to design your own memory cell matrices.
Specifically, you'll be printing resistive memory, a sort of bridge between existing short term and long term memory systems. Traditional digital memory types use transistors—semiconductor-based on and off switches—to store information. Resistive memory is different. "In resistive memory, the logical state is defined by the resistance of each cell, which can be either high or low, representing the 0 or the 1 for binary information," says Bernard Huber, an engineer at the Munich University of Applied Sciences.
His team didn't invent resistive memory; it's been around since 2008. But what's exciting is that they've shown how to make resistive memory so cheaply. Simply download or design the cell schematic (each cell represents one bit, so five lines of silver ink hashed with five perpendicular lines of silver ink gives you 25 bits), load your PET foil substrate, and click print. Compare that to the laser lithography used to make Flash drives—the current standard for long term memory—which takes lots of spinning, heating, and cleaning.
Downsides, there are a few. First, these resistive memory cells aren't quite long term. "The maximum we got was 1.6 times 10 to the fourth power seconds," says Huber. That's about 4.4 hours, for anyone who doesn't think in scientific notation. Then there's the endurance factor: Each cell could switch states between 1 and 0 about 1,000 times before wearing out. And desktop printable electronics will never approach the memory capacity to power anything like a laptop.
Which is fine: America needs those high tech manufacturing jobs anyway. But say you owned a brick and mortar store. This technology could let you print out your own tags for your goods, storing all kinds of information about the price, sales status, and so on. Huber hinted at other, more creative applications, like custom brains for simple robots.
In the meantime, Huber and his co-authors are continuing to work out the kinks, and make these cells remember information longer without burning out. Then comes the important part: Printing simple circuits for fun and profit.
Zika virus in the US didn't end when Congress (belatedly, stingily) funded relief efforts last fall. In some parts of the country, unprecedentedly warm winters meant the *Aedes aegypti *mosquitoes carrying the virus—which has the potential to cause devastating birth defects—never even stopped biting. South Texas saw continuous Zika cases throughout the season. And even in cooler states, Zika can survive the winter inside cold-proof *Aedes aegypti *eggs.
Zika virus research hasn't halted either. In addition to developing vaccines, some scientists are studying the details of Zika transmission in hope of stopping the infection before it crosses the placenta and infects a developing fetus. Targeting the infection at that stage is kind of like trying to stop a tsunami by describing the structure of H2O. But it's good to have lots of options, considering how hard it is to test and bring drugs to market for pregnant women and fetuses—the people Zika hurts most.
Focusing in on Zika's ability to cross the placenta makes sense: It's the primary route infections take when passing from mother to developing fetus. "If we can prevent that," says Nassim Zecavati, a pediatric neurologist at Georgetown University Hospital, "there’s a good chance of preventing perinatal infection altogether." But how do you stop a teensy virus like Zika from moving through the internal systems of an infected host?
By cutting it off at the cell membrane. Bio 101 refresher: Viruses can only replicate their genetic material by entering a host cell and hijacking its genome-replication equipment. But they need to latch onto the cell membrane and inject their DNA or RNA through it first. Since Zika, like its mosquito-borne cousin dengue, is a pathogenic flavivirus, the Rensselaer team started out with a hunch. "Dengue virus envelop proteins bind to cell membrane sugars called glycosaminoglycans—GAGs for short," says So Young Kim, a biochemist and PhD candidate at Rensselaer. "We thought maybe Zika virus can do the same thing."
They can. To test whether Zika was likely to bind to glycosaminoglycans at all, Kim compared its envelope proteins to other flaviviruses' known to bind to those sugars. Then she tested how well Zika bound to a variety of glycosaminoglycans—including those found on the surface of human placental cells. Using that information, she's working on a infection prevention method: a drug containing glycosaminoglycans Zika will find so irresistible, the virus will turn up its nose at the sugars in human cell membranes.
But that drug is more-or-less hypothetical, a long way off from clinical trials. And not everyone is convinced of its utility. "It's going to be interesting science," says Peter Hotez, dean of the National School of Tropical Medicine at Baylor College of Medicine."But you would need to have it administered when you’re about to be pregnant. The practicality is not in your favor." The drug wouldn't be a cure, it would be a preventative measure: If you're already pregnant and already infected, it's likely to late to beef up your placental defenses.
Slightly more practical? A vaccine, which would work on anybody, any time before virus exposure. More than a few are in development, some made from DNA that produces virus-inactivating proteins and others from inactive forms of the virus. One DNA vaccine developed by the National Institute of Allergy and Infectious Diseases moved into broader human testing last week.
The NIAID vaccine is promising, but still a long way from nation-wide implementation. And the trial participants won't include any pregnant women. "It’s going to require an enormous amount of safety testing by the FDA," Hotez says. "They won't be available for pregnant women and women of reproductive age for the next few years." That's typical. Without testing, it's hard to justify the risk of exposing a developing fetus to a new drug. It could even cause birth defects—the Zika outcome scientists are hoping to eliminate. A scientific irony: The people for whom Zika creates the most risk are the riskiest to vaccinate.
So developing alternate drugs to augment or take the place of vaccines is still important. Zecavati sees Kim's research as a potential last ditch effort, useful in the inevitable case of a mother who refuses or does not receive a vaccine. But neither vaccines nor placenta-protecting drugs will arrive in time for people at risk of contracting Zika this summer. "The only thing we have to prevent Zika is mosquito control," Hotez says. "The question is whether or not there's a political will to fund it." Hiring people to spray insecticide, treat standing water, distribute inset repellant and birth control, and canvas clinics for undiagnosed Zika cases is expensive. But the cost of letting Zika run unchecked for another season is bound to be higher.
Late last Thursday, an elevated part of I-85 in Atlanta collapsed due to a fire. If you've been to Atlanta, you know that traffic can be a problem—and closing one of the major roads will just make things worse. And it might be bad for a while: This thing could take months to repair.
But what if the city didn't fix the collapsed part of the interstate and instead built a ramp? Cars could just drive up to the ramp and jump over the gap. Could that work? Let's find out.
If you take a car and launch it at some angle from a ramp, there is really only one force acting on it while it's in the air—gravity. This means that after it leaves the ramp the car will do two things. First, it will have a constant horizontal velocity since there are no horizontal forces. Second, it will have a constant vertical acceleration with a value of -9.8 m/s2, since the only vertical force is the gravitational force. Physicists call this type of situation projectile motion.
So with projectile motion we really have two independent problems. We can treat the vertical motion and the horizontal motion as two separate cases that only share one thing—the time. Before writing down these two problems, let me start with a diagram. I am going to have to launch my car at some angle so that it has both an initial y-velocity and an x-velocity.

If a car is launched with an initial velocity, v0 at some angle θ then it will have the following x and y-velocities to start off with.

This will give the following two kinematic equations for the x and y-directions. I am going to skip some of the steps (for time) and use the distance traveled horizontally as d (just like in the diagram) and the distance traveled vertically will be zero (it starts and ends at the same spot).

I want to use these two equations to solve for the launch angle (θ) to travel some distance (d) with an initial velocity (v0). The one variable I don't really care about is the time (Δt). If I solve the second equation for Δt and substitute it into the first equation, I get:

With a little bit of algebra and a trig identity, I can get this into the following form:

I know what angle to launch the cars—well, I need to put in some values into the equation above. First, I need the car speed. I am going to design this for cars traveling at 60 mph (26.8 m/s). Second, I need the jump distance. This was a bit more difficult. However, based on my explorations in Google Maps I found the exact location of the interstate collapse. Using the distance measuring tools, I get a gap distance of 94 feet (28.6 meters). Putting these values into my launch angle equation, the ramp should be 11.4 degrees. That seems reasonable.
Now for the ramp—two ramps actually. There will be identical launching and landing ramps, so that a car traveling exactly 60 mph will land on the second ramp without a crash. If I want the ramp to be about three car lengths long, then it would be a triangle with a hypotenuse of about 10 meters. Here are the other dimensions.

That ramp is fairly high at the end (about 2 meters). In order to prevent more car damage, I would create a transition ramp that looks like this.

Here I have exaggerated the dimensions so you can see that there are two slopes. If you only have one ramp, the car will have a large change in momentum due to the direction change. This change in momentum can break the ramp—which is exactly what happened when the MythBusters built a ramp for their rocket car. With two angles, the car will change momentum a little bit with each bend and hopefully not break anything. You could do the same thing for the landing ramp.
But even with this double angle and double ramp, this might be quicker to build than it would be to replace the missing section of the interstate. Also, it would serve as a type of speed enforcement. If you are driving too slow you won't make the jump. If you are driving too fast, you will miss the landing ramp. Oh, it would also be cool to watch. Everyone driving over the jump would also be required to yell "YEEEEEEE HAAAWW".
Like it or not, science is politics. And despite one party's attempt to brand itself as the voice for all science, the reality is fans from either side of the ideological spectrum kneel before the altar of data. Or rather, their respective altars. Because Liberals and Conservatives tend to prefer to read about different types of science.
The proof is in the perusing. A new study published today in Nature Human Behavior looked at which science books and which political books people shopping at Amazon.com and BarnesAndNoble.com typically bought together. The huge consumer dataset revealed some pretty clear trends: Red-tinged readers prefer applied science, like criminology or medicine, while lefties alight upon books that explore science for science's sake, like zoology, or abstract physics. And while it's great that the reasoned examination of facts appeals to everyone, the study seems to suggest that—unsurprisingly, but depressing nonetheless—people seek out the stuff that supports their worldview.
"The political divisions within this country seem to be splitting the country into mutually hostile camps that increasingly don't like each other, and don't engage intellectually," says Michael Macy, computational social scientist at Cornell University, and co-author for the study. "We were interested to find out if science might be able to serve as a bridge between these camps." He and his co-authors figured that books might be a good way of assessing peoples' actual interest in science. And, rather than fuss over low response rates and other survey design flaws, they figured they could just scrape sales trends from online bookstores instead.
If you've been to Amazon, you're probably familiar with one of their most successful sales tactics: Recommending books other customers have bought alongside the one you are currently browsing. The data underlying these suggestions is right there in the open, in Amazon's API.
The authors started with a two "seed" books: Barack Obama's Dreams of My Father, and Mitt Romney's No Apology. For each, they scraped the top 100 results from the "Customers Who Bought This Item Also" section. Then they repeated this process with every book on those lists—looking for other books customers bought alongside—and again with the results from those books. They repeated this cycle again and again until they had a complete library of nearly 1.5 million books. Then they began winnowing out all the political and science-based books, based on Amazon.com's classification system—many political books are labeled for conservative or liberal readers. In order to delineate subcategories of science, they used the Dewey Decimal and Library of Congress systems of categorization.
From that thicket of data, they parsed. "There are two important general differences between the two ideologies," says Macy. "Liberals tend to be more interested in basic science that is motivated by intellectual puzzles, empirical exercises, philosophical musings, and conservatives are looking for solutions, problem solving, and applied research." A liberal might be more likely to buy a bundle of books featuring Al Franken and Carl Sagan; while conservative shopping carts would be full of Star Parker and Mary Roach.
The second broad trend is a little more nuanced. Liberals tend to purchase science books that are interesting to anyone who is interested in science, regardless of whether they read political books. And conservatives are more cloistered, preferring science books that are only of interest to people who buy conservative political books. For instance, a liberal reader of books on environmental science is more likely to read something with broad popular appeal, like Andrea Wulf's The Invention of Nature: Alexander von Humbolt's New World, whereas a conservative reader would go for something more niche, and mostly of interest to other conservative readers, such as Lukewarming: The New Climate Science that Changes Everything.
So where's the common ground? Dinosaurs, mostly. "And perhaps that not too surprising, given that it's not an area of research that's particularly politically-charged," says Macy. Which seems counterintuitive, given the whole creationism thing, but that's not what the data says. Physical science topics more generally were the least partisan, followed by life sciences—biology, environmental science, zoology—and finally the social sciences, like psychology, which might as well have trenches.
The Amazon data has its limitations. "The biggest is that we don't have individual-level purchase data," says Macy. They had to draw their conclusions from broad aggregate trends, which could mean they are missing nuances in Amazon's algorithm that could be skewing the data one way or anther. To account for this, they repeated the entire experiment on Barnes & Noble's online store. Interestingly, the two websites did not share a high number of people making the exact same co-purchases of this political book to that science book. However, the high level correlation between political ideology and scientific discipline held.
Science isn't the only topic where Macy and his co-authors are investigating partisan social divides. They maintain a website called www.lifestyle-politics.com where they rate things like sports teams, professional wrestlers, and TV shows based on users' Twitter feeds. "What we've found is there's a strong correlation between ideology and cultural preferences that have seemingly nothing to do with ideology." For instance, if enough people who follow ideologically conservative accounts like @realDonaldTrump and @FoxNews also follow @ChickfilA and @BigBangTheory, that fast food restaurant and that TV show also get grouped as conservative cultural touchstones.
To Macy, these cultural trends seem to match what's happening in science. "We don't know for sure, but we speculate that lots of interest in science is politically-motivated, and people are interested in reading about science that supports their political views," he says. If scientists want to do a better job of making their research more accessible—which they probably should if they don't want their line of work targeted by the same kind of ideological philocide currently being perpetuated against climate science—they should try to preach beyond their own choir.
Smartphone apps are great at lots of things, from sending selfies, to solving late-night taco cravings. And even if they occasionally fail, the stakes are usually low—unless you consider being carnitas-less at 2 am a life-threatening situation. The exception is with apps that claim to measure your heart rate, monitor your blood pressure, and improve your memory, coordination, or vision.
These apps aren't offering diagnoses, but they do give users an impression about their health. And for the most part, the FDA lets these apps run wild, not requiring them to go through any rigorous vetting process to ensure they are both safe and effective. But that doesn't mean nobody is holding them accountable. Last week the New York Attorney General’s office settled with three mobile health apps it alleged were misleading consumers and engaging in questionable privacy practices. The apps—which include Adidas subsidiary Runtastic, MIT Media Lab spinoff Cardiio, and Matis-made “My Baby’s Beat”—all claim to monitor the human heartbeat using only a smartphone’s camera or microphone and some proprietary algorithms. Per the terms of the settlement, the developers paid a combined $30,000 in penalties and agreed to change their advertising language and update their privacy policies to more transparent, opt-in data-collection agreements. It's a relatively small sum, but the legal action sets a precedent for how states might fill in for oversight gaps at the federal level. It also exposes another potential pitfall for companies trying to cash in on the mushily regulated mobile health industry.
Are you going to die if you entrust your smartphone with monitoring your vital signs and it gets them wrong? Probably not. But the vast majority of apps that claim to do these things don't have to prove they work before they pop up in Apple iTunes or Google Play, which has led to an explosion of them in the last five years. Today you can download more than 156,000 health and wellness apps to track everything from calories to periods.
And yet, no single piece of legislation specifically regulates the mobile health industry. Instead, a constellation of laws each claim a bit of piecemeal authority. Last year the FDA tried to inject some clarity with a new guidance that breaks down health apps into three buckets. The first contains things that are clearly medical devices—like an app that analyzes the content of your urine, just by looking at a photo of a pee-soaked chemical strip, or an app that tells you if a rogue mole is actually a cancerous melanoma. These trigger a formal FDA approval process (which involves clinical trials). The second bucket contains wellness apps—products that help you track your sleep, what you’re eating, how many steps you’re getting, and what your moods are like. These are at the other end of the regulatory spectrum and require no federal clearance. The third bucket contains everything in between. These are apps that could meet the definition of a medical device, but because they don't actively market themselves as lifesaving, fall outside the (short-staffed, budget-strapped) FDA's attention span.
The FDA's position is based on a simple risk-cost analysis; an app that isn't going to kill someone isn't worth enforcing. Bradley Merrill Thompson, a partner at Epstein Becker Green, who specializes in regulatory law for digital health, says it's a reasonable strategy. Mostly. "The marketplace does quite well policing itself when the financial and public health risks are low," he says. "Consumers will shut down any business where the truth is easily discoverable, but they're never going to conduct clinical trials to figure out if something works."
Which would be great—for app makers—if the FDA were the only federal agency involved in this. But medical apps suggesting some sort of outcome could also trigger unwanted attention from the Federal Trade Commission—which protects consumers from fraud. Does this thing do what it says it does? Over the last five years the FTC investigated, fined, and barred a handful of mobile app companies from making unsubstantiated medical claims that their products could do things like Read blood pressure! Pick out cancerous moles! "Turn back the clock" on your eyesight!
So many developers got in trouble that last April the FTC released a set of best practices to help other developers avoid similar predicaments. Suggestions include minimizing data collection and investing in multifactor authentication. It also partnered with the FDA and two other Health and Human Services offices to create an interactive developer tool to help companies comply with all the different laws on the books.
But since then, the FTC has backed off. This February, the agency's director of the Bureau of Consumer Protection stepped down. Jessica Rich was adamant about pursuing health-app fraud. (She once testified in front of Congress, saying: "If consumer health data is used for unanticipated, harmful purposes, consumers could lose confidence in the health IT sector. As the nation's foremost protection agency, the FTC is committed to protecting health information collected by these entities.") It's unclear whether the 26-year agency veteran left voluntarily or was forced out. Regardless, her departure means the agency is reexamining its aggressive stance on consumer privacy, including in mobile health.
Which is what makes the New York case significant. Every state has its own rules regulating how apps collect user information, and getting onto iTunes or Google Play means having to be compliant with all of them. These laws become more relevant as the new administration rolls back the regulatory efforts of all federal agencies while Congress slashes online privacy protections for private citizens. So the battle for mobile health data looks like it will be fought increasingly in state courts, particularly in places with stricter laws, like New York and California. The key word here is increasingly. Because the one thing that's for certain, smartphones aren't going anywhere.
As he was brushing his teeth on the morning of July 17, 2014, Thomas Royen, a little-known retired German statistician, suddenly lit upon the proof of a famous conjecture at the intersection of geometry, probability theory, and statistics that had eluded top experts for decades.
About
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Known as the Gaussian correlation inequality (GCI), the conjecture originated in the 1950s, was posed in its most elegant form in 1972 and has held mathematicians in its thrall ever since. “I know of people who worked on it for 40 years,” said Donald Richards, a statistician at Pennsylvania State University. “I myself worked on it for 30 years.”
Royen hadn’t given the Gaussian correlation inequality much thought before the “raw idea” for how to prove it came to him over the bathroom sink. Formerly an employee of a pharmaceutical company, he had moved on to a small technical university in Bingen, Germany, in 1985 in order to have more time to improve the statistical formulas that he and other industry statisticians used to make sense of drug-trial data. In July 2014, still at work on his formulas as a 67-year-old retiree, Royen found that the GCI could be extended into a statement about statistical distributions he had long specialized in. On the morning of the 17th, he saw how to calculate a key derivative for this extended GCI that unlocked the proof. “The evening of this day, my first draft of the proof was written,” he said.
Not knowing LaTeX, the word processer of choice in mathematics, he typed up his calculations in Microsoft Word, and the following month he posted his paper to the academic preprint site arxiv.org. He also sent it to Richards, who had briefly circulated his own failed attempt at a proof of the GCI a year and a half earlier. “I got this article by email from him,” Richards said. “And when I looked at it I knew instantly that it was solved.”
Upon seeing the proof, “I really kicked myself,” Richards said. Over the decades, he and other experts had been attacking the GCI with increasingly sophisticated mathematical methods, certain that bold new ideas in convex geometry, probability theory or analysis would be needed to prove it. Some mathematicians, after years of toiling in vain, had come to suspect the inequality was actually false. In the end, though, Royen’s proof was short and simple, filling just a few pages and using only classic techniques. Richards was shocked that he and everyone else had missed it. “But on the other hand I have to also tell you that when I saw it, it was with relief,” he said. “I remember thinking to myself that I was glad to have seen it before I died.” He laughed. “Really, I was so glad I saw it.”
Richards notified a few colleagues and even helped Royen retype his paper in LaTeX to make it appear more professional. But other experts whom Richards and Royen contacted seemed dismissive of his dramatic claim. False proofs of the GCI had been floated repeatedly over the decades, including two that had appeared on arxiv.org since 2010. Bo’az Klartag of the Weizmann Institute of Science and Tel Aviv University recalls receiving the batch of three purported proofs, including Royen’s, in an email from a colleague in 2015. When he checked one of them and found a mistake, he set the others aside for lack of time. For this reason and others, Royen’s achievement went unrecognized.
Proofs of obscure provenance are sometimes overlooked at first, but usually not for long: A major paper like Royen’s would normally get submitted and published somewhere like the Annals of Statistics, experts said, and then everybody would hear about it. But Royen, not having a career to advance, chose to skip the slow and often demanding peer-review process typical of top journals. He opted instead for quick publication in the Far East Journal of Theoretical Statistics, a periodical based in Allahabad, India, that was largely unknown to experts and which, on its website, rather suspiciously listed Royen as an editor. (He had agreed to join the editorial board the year before.)
With this red flag emblazoned on it, the proof continued to be ignored. Finally, in December 2015, the Polish mathematician Rafał Latała and his student Dariusz Matlak put out a paper advertising Royen’s proof, reorganizing it in a way some people found easier to follow. Word is now getting around. Tilmann Gneiting, a statistician at the Heidelberg Institute for Theoretical Studies, just 65 miles from Bingen, said he was shocked to learn in July 2016, two years after the fact, that the GCI had been proved. The statistician Alan Izenman, of Temple University in Philadelphia, still hadn’t heard about the proof when asked for comment last month.
No one is quite sure how, in the 21st century, news of Royen’s proof managed to travel so slowly. “It was clearly a lack of communication in an age where it’s very easy to communicate,” Klartag said.
“But anyway, at least we found it,” he added—and “it’s beautiful.”
In its most famous form, formulated in 1972, the GCI links probability and geometry: It places a lower bound on a player’s odds in a game of darts, including hypothetical dart games in higher dimensions.
Imagine two convex polygons, such as a rectangle and a circle, centered on a point that serves as the target. Darts thrown at the target will land in a bell curve or “Gaussian distribution” of positions around the center point. The Gaussian correlation inequality says that the probability that a dart will land inside both the rectangle and the circle is always as high as or higher than the individual probability of its landing inside the rectangle multiplied by the individual probability of its landing in the circle. In plainer terms, because the two shapes overlap, striking one increases your chances of also striking the other. The same inequality was thought to hold for any two convex symmetrical shapes with any number of dimensions centered on a point.
Special cases of the GCI have been proved—in 1977, for instance, Loren Pitt of the University of Virginia established it as true for two-dimensional convex shapes—but the general case eluded all mathematicians who tried to prove it. Pitt had been trying since 1973, when he first heard about the inequality over lunch with colleagues at a meeting in Albuquerque, New Mexico. “Being an arrogant young mathematician … I was shocked that grown men who were putting themselves off as respectable math and science people didn’t know the answer to this,” he said. He locked himself in his motel room and was sure he would prove or disprove the conjecture before coming out. “Fifty years or so later I still didn’t know the answer,” he said.
Despite hundreds of pages of calculations leading nowhere, Pitt and other mathematicians felt certain—and took his 2-D proof as evidence—that the convex geometry framing of the GCI would lead to the general proof. “I had developed a conceptual way of thinking about this that perhaps I was overly wedded to,” Pitt said. “And what Royen did was kind of diametrically opposed to what I had in mind.”
Royen’s proof harkened back to his roots in the pharmaceutical industry, and to the obscure origin of the Gaussian correlation inequality itself. Before it was a statement about convex symmetrical shapes, the GCI was conjectured in 1959 by the American statistician Olive Dunn as a formula for calculating “simultaneous confidence intervals,” or ranges that multiple variables are all estimated to fall in.
Suppose you want to estimate the weight and height ranges that 95 percent of a given population fall in, based on a sample of measurements. If you plot people’s weights and heights on an x–y plot, the weights will form a Gaussian bell-curve distribution along the x-axis, and heights will form a bell curve along the y-axis. Together, the weights and heights follow a two-dimensional bell curve. You can then ask, what are the weight and height ranges—call them –w < x < w and –h < y < h—such that 95 percent of the population will fall inside the rectangle formed by these ranges?
If weight and height were independent, you could just calculate the individual odds of a given weight falling inside –w < x < w and a given height falling inside –h < y < h, then multiply them to get the odds that both conditions are satisfied. But weight and height are correlated. As with darts and overlapping shapes, if someone’s weight lands in the normal range, that person is more likely to have a normal height. Dunn, generalizing an inequality posed three years earlier, conjectured the following: The probability that both Gaussian random variables will simultaneously fall inside the rectangular region is always greater than or equal to the product of the individual probabilities of each variable falling in its own specified range. (This can be generalized to any number of variables.) If the variables are independent, then the joint probability equals the product of the individual probabilities. But any correlation between the variables causes the joint probability to increase.
Royen found that he could generalize the GCI to apply not just to Gaussian distributions of random variables but to more general statistical spreads related to the squares of Gaussian distributions, called gamma distributions, which are used in certain statistical tests. “In mathematics, it occurs frequently that a seemingly difficult special problem can be solved by answering a more general question,” he said.
Royen represented the amount of correlation between variables in his generalized GCI by a factor we might call C, and he defined a new function whose value depends on C. When C = 0 (corresponding to independent variables like weight and eye color), the function equals the product of the separate probabilities. When you crank up the correlation to the maximum, C = 1, the function equals the joint probability. To prove that the latter is bigger than the former and the GCI is true, Royen needed to show that his function always increases as C increases. And it does so if its derivative, or rate of change, with respect to C is always positive.
His familiarity with gamma distributions sparked his bathroom-sink epiphany. He knew he could apply a classic trick to transform his function into a simpler function. Suddenly, he recognized that the derivative of this transformed function was equivalent to the transform of the derivative of the original function. He could easily show that the latter derivative was always positive, proving the GCI. “He had formulas that enabled him to pull off his magic,” Pitt said. “And I didn’t have the formulas.”
Any graduate student in statistics could follow the arguments, experts say. Royen said he hopes the “surprisingly simple proof … might encourage young students to use their own creativity to find new mathematical theorems,” since “a very high theoretical level is not always required.”
Some researchers, however, still want a geometric proof of the GCI, which would help explain strange new facts in convex geometry that are only de facto implied by Royen’s analytic proof. In particular, Pitt said, the GCI defines an interesting relationship between vectors on the surfaces of overlapping convex shapes, which could blossom into a new subdomain of convex geometry. “At least now we know it’s true,” he said of the vector relationship. But “if someone could see their way through this geometry we’d understand a class of problems in a way that we just don’t today.”
Beyond the GCI’s geometric implications, Richards said a variation on the inequality could help statisticians better predict the ranges in which variables like stock prices fluctuate over time. In probability theory, the GCI proof now permits exact calculations of rates that arise in “small-ball” probabilities, which are related to the random paths of particles moving in a fluid. Richards says he has conjectured a few inequalities that extend the GCI, and which he might now try to prove using Royen’s approach.
Royen’s main interest is in improving the practical computation of the formulas used in many statistical tests—for instance, for determining whether a drug causes fatigue based on measurements of several variables, such as patients’ reaction time and body sway. He said that his extended GCI does indeed sharpen these tools of his old trade, and that some of his other recent work related to the GCI has offered further improvements. As for the proof’s muted reception, Royen wasn’t particularly disappointed or surprised. “I am used to being frequently ignored by scientists from [top-tier] German universities,” he wrote in an email. “I am not so talented for ‘networking’ and many contacts. I do not need these things for the quality of my life.”
The “feeling of deep joy and gratitude” that comes from finding an important proof has been reward enough. “It is like a kind of grace,” he said. “We can work for a long time on a problem and suddenly an angel—[which] stands here poetically for the mysteries of our neurons—brings a good idea.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Bennett Schwartz is one of the nation’s leading memory experts, and when I visited him in his office at Florida International University, he was standing at his desk. A soft sunlight crowded the room. Large windows framed the palm tree-lined quad outside.
Dressed in a short-sleeved shirt and slacks, Schwartz appeared to be quietly talking to himself, with hushed, mumbled words, and for a long moment, it seemed as if he was some sort of monk, living in another, more esoteric world.
“Hi?” I said tentatively.
Schwartz turned around, putting the book away with an easy gesture.
It turned out that when I walked into the room, Schwartz was honing his Scrabble skills. He had a Scrabble tournament the next day, and he was practicing words from a book devoted to the game. “The director is allowing me to play against the good Scrabble players,” Schwartz told me, laughing. “I have to make sure I know my words.”
So how exactly does one of the nation’s premier memory experts develop his Scrabble skills?
Well, Schwartz uses a type of self-quizzing: In order to hone his expertise, he’s constantly interrogating himself to see if he can recall various Scrabble words. So when Schwartz stops at a red light—or if he’s just waiting in his office—he’ll ask himself questions about what he’s learned as well as what he aims to learn.
The learning strategy has shown clear results, helping Schwartz become a nationally ranked player, boosting his Scrabble rating by more than 25 percent over the past few years.
Known as retrieval practice, the approach to learning fills the recent literature on memory, sometimes showing effects some 50 percent more than other forms of learning. In one study, one group of subjects read a passage four times. A second group read the passage just one time, but then the same group practiced recalling the passage three times.
Ulrich Boser (@ulrichboser) a senior fellow at the Center for American Progress.
But when the researchers followed up with both groups a few days later, the group that had practiced recalling the passage learned significantly more. In other words, subjects who tried to recall the information instead of rereading it showed far more expertise.
Within the learning sciences, retrieval practice is sometimes referred as the testing effect since the practice is a matter of people asking themselves specific questions about what they learned. But in many ways, the idea goes much deeper than self-quizzing, and what’s important about retrieval practice is that people take steps to recall what they know. They ask themselves questions about their knowledge, making sure that it can be produced.
More concretely, retrieval practice isn’t like a multiple-choice test, which has people choose from a few answers, or even a Scrabble game, where you hunt in your memory for a high-point word. Retrieval practice is more like writing a five-sentence essay in your head: You’re recalling the idea and summarizing it in a way that makes sense.
In this regard, we can think of retrieval practice as a type of mental doing: It’s a way to create the webs of meaning that support what we know. As psychologist Bob Bjork told me, “The act of retrieving information from our memories is a powerful learning event.”
A lot of the benefit of retrieval practice is explained by the nature of long-term memory, and Schwartz describes long-term memory as being like the sprawling, chaotic bedroom of a teenager. It might seem messy from the outside, but the disorder makes a lot of sense to the people who live there. "Each of us has his or her own subjective organization strategies, which may not be obvious to others, but give us each or own unique associations and memory," Schwartz told me.
The key to recalling memories is to have cues that closely link up with a memory, Schwartz says. When we have strong associations, we can more easily recall a word or idea. Because if "cues change, finding that missing word or event is like looking for a particular t-shirt among the chaos of the mess," he says. We simply can't find it.
In this regard, retrieval practice helps us recall memories because it more closely links cues and their associated memories. It pushes us to foster associations—and make more durable forms of knowledge. As Schwartz argues, "it’s the growing mess and the lack of the organizational cues that makes memory fade."
The benefits of retrieval practice go far beyond facts or games, and there are ways to use self-quizzing to develop conceptual understanding. One student recommends that people first create a pile of cards that lists facts. Then people should build a second pile that asks things like “give a real life example” or “draw this concept.” In this approach, people learn by picking one card from the first pile and a card from the second pile, and then executing the task.
Retrieval practice doesn’t have to be written down, either. When I was in college, I worked as a teaching assistant for a class that relied on a form of retrieval practice, and once a week, I would gather a group of students in a classroom and verbally ask them rapid-fire questions. The class was relatively short—just 45 minutes a week. But it was easy to see the effects of the free-recall type of practice, and the more the students retrieved their knowledge, the more they learned.
As for Schwartz, he did well at the Scrabble tournament on the day after our interview. The director placed him in one of the highest divisions, and he went against some of the best players in the state of Florida. Using his technique of retrieval practice, Schwartz managed to notch wins in about a third of his games. As Schwartz modestly joked in a follow-up email, “I didn’t finish last, so I did OK.”
There's a problem with retrieval practice and Schwartz is pretty familiar with it. In recent years, Schwartz has been encouraging students in his classes at Florida International University to use retrieval practice and he repeatedly asks them to explain about what they know.
But Schwartz's students have not been all the supportive of the new approach to learning. “My students have to take a quiz every week,” he told me, “and they would much prefer not to take a quiz every week. They don’t like it. They complain. Every week, I get excuses about dead grandmothers.”
It's pretty easy to see why the students don't like the quizzes. Retrieval practice is uncomfortable. It requires additional work. The frequent quizzing pushes students to constantly recall things from memory, and while the students end up posting higher grades on the final exams, they have to put forth a lot more mental energy.
Schwartz argues that this is simply the nature of learning. Developing a skill takes cognitive toil, and we can see the benefits of this sort of struggle in our brain. Indeed, it appears that at a very basic neurological level, grappling with material in a dedicated way promotes a type of shifting of our neural circuits—and a richer form of expertise. "If we process things too easily, they do not form into memories," Schwartz says.
A version of this idea has long intrigued Yuzheng Hu. A researcher on brain plasticity at the National Institutes of Health, Hu has studied something called white matter. A type of neural transmission cable, white matter helps distribute messages throughout the brain. It makes information flow more effectively, allowing electronic pulses to jump more easily from one neuron to the next. If the brain has a form of wiring, white matter serves as the copper, the substance that conducts the messages.
In one of his first studies, Hu and some colleagues decided to see if certain types of learning might boost white matter within the brain, and so they compared a group of young subjects who received a rigorous type of math training and those who didn’t.
The data suggested that the more rigorous math approach built up the brain’s transmission material, and under the bright scan of the MRI machine, certain white matter zones of the brain like the corpus callosum pulsed stronger in people who had studied the harder approach to learning math.
Hu’s study was built on a decade’s worth of research that shows that there’s very little fixed about the brain. Our brain systems are not like a piece of metal, something rigid in its fundamentals, hard and inflexible. The brain, instead, is something that can change, something that can adapt to its environment, more neural cloud than neural cement.
If you master karate, for instance, there are clear, structural changes that happen to white matter structures within your brain. Similar changes happen when we learn to juggle—or learn to meditate.
This idea has a number of important implications for how we develop a skill. For one, there are far fewer neural set points than we generally believe. Our brains are not fixed at birth. Our mental abilities are not preprogrammed. For a long time, for instance, people believed in the notion of “critical periods,” or the idea that we need to acquire certain skills at particular times in our life. But except for a few narrow abilities, we can acquire most skills at any time.
But what’s new—and really most important—about this line of research is how exactly the brain builds new structures, and it seems that the brain creates white matter in an effort to deal with mental struggle. When there’s a large gap between what we know and what we can do, the brain shifts its structures to address the issue. Some years ago, a group of German researchers described a new way of understanding why this happens, and they argue that when “demand” overwhelms the brain’s “supply,” we build new neural structures.
For his part, Hu argues that the brain responds to an opportunity to learn. When faced with a tough situation, it rises to the occasion. “This is your brain optimizing the way you perform that task,” Hu told me. “If you practice something a lot, your brain will think, ‘This is important,’ and you will develop a strategy to better perform it.”
Schwartz goes a little further. He argues that white matter and learning might not actually be all that different, at least at some philosophical level. "I am a materialist. I don’t like the idea that our brain is somehow different from what we consider to be ourselves," Schwartz says. Thus, for him, "white matter connectivity is the neural correlate of cognitive engagement and growth. They are one in the same—rather than one resulting from the other."
It's possible to go too far here. Learning is about a lot more than developing a specific axon in the brain. But more broadly what's clear is that the brain itself seems to understand the value of repeatedly quizzing, of learning as a form of cognitive work.
Memory researchers haven’t always believed in the power of mistakes. Before the advent of researchers like Bennett Schwartz, struggle wasn’t considered part of the learning process, and in more passive, more behaviorist models of learning, mistakes are exactly that: mistakes. They show that people are not learning properly. Blunders are a sign that someone is doing something wrong.
But it turns out that understanding doesn’t transfer immutably from one person’s head to another. Our brain is not a simple storage device, a lockbox, or a warehouse that serves as some type of memory depot. Instead, we have to make sense of ideas, to grapple with expertise, and that means that errors are simply unavoidable.
This notion is pretty clear in something like retrieval practice. If you’re constantly asking yourself questions, you’re bound to have some misses. Someone like Schwartz, for instance, often gets something wrong as he practices for his Scrabble tournaments, unable to recall a high-value Scrabble word like "qat."
Just as important, errors create meaning. They build understanding. When I interviewed Schwartz in his office, for instance, he asked me the question: What’s the capital of Australia?
Unless you’re from Australia, your first guess is probably the same as my first question: Sydney. But that’s not correct.
Second guess? Maybe Melbourne? Again, wrong.
Or perhaps Brisbane? Perth? Adelaide? All those answers are also off the mark.
The correct response, it turns out, is Canberra.
I know. Weird. If you’re not from Australia, the answer of Canberra probably comes with a buzzing shock of surprise. That's certainly what happened to me when Schwartz finally told me the answer. It was an odd little eye-opener.
But that feeling—that zinging moment—is learning. It’s the signal of a shift in understanding. When we make a gaff, we search for meaning, and so we learn more effectively. This idea goes back to memory being like a messy, teenager's room. If there’s an error—and it’s a salient one—we put a red, Sharpie-made x on the memory, noting exactly where it's located in the room. Within our brains, we’re telling ourselves: Remember this idea. It’s important.
Of course, no one likes mistakes. Errors are sharp and painful, humiliating and demoralizing. Even the smallest of gaffs—a misspoken word, a blundered errand—can haunt people for years. In this sense, mistakes make us rethink who we are; they’re a threat to our being.
But still, errors help us gain skills. They create mastery. When I last checked in with Schwartz, he had won a tournament in St. Myers, landing second place and taking home $80 in award money. He was still processing what he had done wrong, figuring out how to gain from his mistakes: "I could not play that damn 'v,'" he told me.
But Schwartz was also focused on what he needed to get better, to win at the next Scrabble tournament. "I need some time to study if I want to break to the next level," he told me. In other words, he needed some more time to quiz himself.
Excerpted from Learn Better by Ulrich Boser. Copyright © 2017 by Ulrich Boser. With permission of the publisher, Rodale Books. All rights reserved.
Launching rockets is a risky business. So, in addition to those precious payloads, every mission carries an insurance policy juuust in case something goes awry. Private insurers handle most of it, but the federal government offers a backstop for those truly unusual catastrophes—think rocket nosediving into an elementary school—that would max out the private coverage.
And it turns out a Cape Canaveral cataclysm—or, if you prefer, a Wallops Island walloping, or a Vandenberg devastation—could cost that program far more than it expects. A report from the Government Accountability Office says the federal program undervalues its launch insurance and ought to update its estimates. Given that the entire space launch insurance industry bases its rates on those same estimates, any update could make even commercial updates more expensive.
First, a little history. In 1988, Congress recognized that private insurers lacked the resources to insure against a truly epic disaster and passed amendments to the Commercial Space Launch Act. The updated law required the Federal Aviation Administration to create a safety cushion of up to $1.5 billion—about $3.1 billion today, adjusted for inflation. "The federal insurance was put in place because there was a fear that the private companies wouldn't be able to take risk of launching and being insured and so it was a way of allowing the commercial market to develop," says Alicia Cackley, director of the GAO's financial markets and community investment team.
Space insurance isn't just a good idea, it's required. Every company launching a rocket must buy a policy—available through private insurers—based on calculations made by the FAA. These calculations depend on the type of rocket, the location of the launch site, and other variables. Do the math and you get something the insurance industry calls the maximum probable loss. The FAA caps it at $500 million.
Problem is, the program—and its estimates of maximum probable loss—need to get with the times. It still bases the cost of human casualties on 1988 values: $3 million per life. "We don't think that 1988 estimate is realistic anymore, just based on cost of living increase, if nothing else," says Cackley. The FAA also uses the casualty rate to set the value of property damage, so that, too, is undervalued. A truly horrific accident could leave the government paying out far more than expected.
Or not. The 1988 law does not guarantee that Congress will pay out the FAA's liability coverage—lawmakers would have to vote on it. "The companies all believe the federal government will stand by its commitment, and they operate as if that will happen," says Cackley. But if the cost vastly exceed FAA estimates, Congress could balk, leaving the launch company holding the bag.
Updating those figures, as the GAO recommends, poses other risks to the industry. "If the cost of the casualty number were to increase dramatically, that could reasonably increase the rate that private insurers have to charge," says Cackley. And that puts the FAA in a pickle, because it doesn't want to price private insurers out of the game. Nor does it want to put commercial launch companies out of business.
But representatives from insurance companies consider change in rates negligible, and in fact encourage the FAA to get with the re-estimating. "More accurate and less uncertain maximum probable loss calculations may allow more launch operators, more launch pads, and a more open launch environment, hence, increasing the number of space liability policies and the size of this market," says Denis Bensoussan, head of aviation for Beazley, an insurance company that covers space.
And the impact might not even be that bad. "I'd be surprised if insurance is 1 percent of the overall launch cost per flight," says Chris Kunstadter, senior vice president at XL Catlin, a large insurance company that covers spaceflight. And the overall risk to the government is actually quite low—it has never had to actually pay out for a catastrophic launch disaster.
The bigger threat to the industry, says Kunstadter, is the proliferation of small satellites. These need less insurance coverage, because they are cheaper to manufacture. They're also so light that many companies launching small satellite constellations put a dozen or so extras into orbit under the assumption that some will fail. "So maybe you don't need as much insurance, and that can have an effect on our market," he says. Space is risky, but not always in the ways you expect.
If humans are going to get to Mars, they're going to need rockets with some serious liftoff power. NASA’s Space Launch System is the most powerful rocket in the world and engineers are going to blast it, for testing purposes, of course.
The world knows no toughness like that of the water bear, which looks like a cannon wearing a pair of wrinkled khakis. This microscopic critter can survive boiling water (and alcohol too, just to be safe), some of the lowest temperatures in the universe, and blasts of radiation that would kill a human. In the slightly edited but still immortal words of Austin Powers: “Why won’t the water bear die?”
The question has for decades baffled scientists, who suspected the water bear—also known as a tardigrade—mobilizes a sugar called trehalose to reinforce its body and keep its cells from swift destruction. But no longer. In a paper out today in Molecular Cell, researchers claim they’ve found an exclusively tardigradean protein that the creature produces, forming it into a glass bead. It’s in this state that the water bear can pull off such extreme feats of survival—which might be very convenient for human medicine one day.
The problem with the trehalose theory, as it turned out, was that while many other organisms like nematode worms and brine shrimp use it to survive desiccation, not all water bear species produce the sugar under stress. Some of those other organisms produce enough trehalose to make up 20 percent of their body weight. The water bear? Only about 2 percent. Pitiful, really.
That didn’t jibe with the water bear’s uncanny toughness. So researchers looked closely at the genes that turned on as water bears dried out. At the top of the list of the switched-on genes: those that encode what are known as intrinsically disordered proteins. Those amino acid chains don’t have a neat 3-D structure like most proteins, so they act very loosey-goosey and strange. “One of the things we're really interested in is figuring out how exactly these tardigrade intrinsically disordered proteins are working,” says biologist and study co-author Thomas C. Boothby of University of North Carolina at Chapel Hill. “It's a really interesting question about how a protein without a defined three-dimensional structure can actually carry out its function in a cell.”
Regardless, Boothby and his colleagues seem to have pinpointed the genes that make the water bear’s life-saving proteins. “We went on to show that if you reduce expression of these genes in tardigrades, they can no longer survive desiccation very well,” Boothby says. “If you take those genes and put them into organisms like bacteria and yeast, which normally do not have these proteins, they actually become much more desiccation-tolerant.” The water bear's secret ingredient can make other organisms up to 100 times hardier.
The mechanism of these intrinsically disordered proteins looks a lot like how the trehalose sugar protects animals like nematode worms from dessication. Like something out of a fairy tale or the very least an ‘80s movie, the protein turns the water bear into a frozen glass figurine, a process known as vitrification. Normally, dessication crystallizes living cells, shredding up proteins and DNA in the process. But with the gentler, smoother process of vitrification, the water bear can ride out the desiccation, only to reanimate once it hits water perhaps 30 years later.
Great news for the hardy little water bear, and potentially even better news for humanity. Vaccines, for instance, are extremely fragile, requiring refrigeration as doctors ship them around the world. That costs a whole lot of money and means the vaccines are easily destroyed. “One potential application would be to use these tardigrade proteins to stabilize vaccines or pharmaceuticals in a dry state that you can keep at room temperature and not have to worry about refrigeration during transportation and storage,” says Boothby.
Not bad, little water bear. Consider me sorry for the cannon-wearing-khakis thing.
If you believe the geopolitical soothsayers, the US and China are headed for an economic confrontation. President Donald Trump has marked his first 100 days in office with efforts to roll back many of his predecessor's climate-conscious programs, making business easier for fossil fuel companies, car manufacturers, and chemical companies. China is taking the opposite tack. In the past few years, China has surpassed the US in electric vehicle sales, renewable energy capacity, and recently announced it was investing $365 billion to keep the momentum going.
That investment puts China in a prime position to lead the world in clean energy, selling its innovations to other countries looking to cut their energy bills. So if a trade war breaks out between China and the US, it may well be over clean energy.
The rest of the world is pursuing—with various levels of enthusiasm—a shift to clean technology. At the 2015 Paris climate talks, 197 nations agreed to curb their emissions and reverse the trend of global warming. Keeping those promises will require replacing much of the world's fossil-fueled energy infrastructure with alternatives like wind, solar, and batteries. “It’s certainly clear that we don’t have all the technologies we need to meet the climate challenge,” says Mark Muro, senior fellow at the Brookings Institute’s Metropolitan Policy Program. “So that means that innovation and R&D are absolutely crucial coins of the realm.” According to a recent report by Advanced Energy Economy, clean energy is worth $1.4 trillion worldwide. Coins indeed.
The US has historically been a huge innovator in clean energy. An American invented solar panels, Vermont had the first megawatt wind turbine, and naturalized citizen Elon Musk (ever heard of him?) has built the first industrial-scale battery company and arguably deserves the credit for mainstreaming electric vehicles. And the US still has a lot of momentum; renewable energy sources are starting to drop in price such that they actually undercut fossil fueled power without government subsidies.
A lot of those US innovations started off with government help. The Department of Energy's $32 billion loan program helped many clean energy companies get their start. (And despite high-profile failures like Solyndra, the overall failure rate of companies enrolled in the program is only 2.7 percent.) The DOE also funds a lot of early-stage research—through programs like ARPA-E, and tech transfer programs at many of its national labs—nurturing technology until it's ready for private companies to commercialize. "The vast majority of new technologies that scale up and become big biz success stories have many years of life symbiotically linked with government,” says Ion Yadigaroglu, partner and managing principal at Capricorn Investment Group. “There are no private subsidies for the kind of early-stage R&D work that flows from the government, and if you shut that down, then X years after the fact you see a lot less innovation flowing to the private market.”
These are exactly the kinds of programs that President Trump and congressional Republicans have threatened to axe. Also worrying are the administration's stances on immigration and trade policies. "We rely on being magnet for talented people in order to keep innovating in this sector," says Yadigaroglu. Elon Musk, who Yadigaroglu calls a "one man show in saving the world" was an immigrant.
As for trade policies, Trump hasn't yet outlined anything formal, but he has repeatedly harped on China's "stealing" of American jobs. China, for its part, says that while it doesn't want a trade war with the US, it would win.
That could be true, at least for clean energy. China is crushing it when it comes to adopting renewables. And US energy companies have already accused the country of unfairly subsidizing its solar panels to price out American competitors. Those American companies can take their case to court (in this case, the US Department of Commerce) and resolve the issue via tariffs on Chinese panels. But an all-out trade war would undercut the US's ability to moderate these issues in a low-key way.
Coupled with China's skill at producing low price goods, that could effectively shut the US out of the global market. “We’ve been here before,” says Muro. “Like when we lost the flatscreen TVs to foreign producers.” He’s talking about how US tariffs against foreign TV producers—beginning in the 1970s—essentially cut the domestic market out of global competition.
The US is really good at innovating, and American clean energy firms are still pretty strong sellers of technology to the rest of the world. But some indications show that's slowing down. "We’re just completing an analysis of clean-tech patenting," says Muro, of his recent work with the Brookings Institute. An interesting trend he's seeing is that growing number of the clean energy patents being filed in the US are originating in European and Asian countries. "Which does fit this hypothesis that there may well be a swing offshore of dominance of technology," he says.
That's not to say the US clean energy sector is down for the count. Wind and solar have both largely outgrown federal subsidies, and in many places compete with fossil fuel sources. The intermittency issue—the power goes off when the sun don't shine and the wind don't blow—is slowly going away. (Hawaii just opened a solar power plant with 25 megawatts of battery storage; Elon Musk recently swore he could provide Australia with up to 300 megawatts of battery storage within 100 days.)
And of course, nobody can read the future. Especially when it comes to energy, the crystal ball can be quite foggy. Trump might become a clean energy believer—$200 billion domestic contribution to GDP is hard to ignore for long. If that day comes, it'll be quite a confrontation indeed.
Who runs the world? Lithium-ion batteries! (Sorry, Beyoncé.) Ever since Sony commercialized the chemistry in 1991, Li-ion cells have powered everything from the Mars Curiosity rover to the device you’re using to read these words. The tech has endured for good reason: It charges quickly, fits a ton of energy into a slim package (lithium is the lightest metal and is highly reactive), and is generally pretty safe. But when things go wrong, they go very wrong. Scores of Galaxy Note 7s and hoverboards have succumbed to the fiery embrace of a malfunctioning Li-ion battery. Here is what’s inside the little fuel packs that power your life—and how they can turn into battery flambé.
Lithium Cobalt Oxide
To store or release energy in a slim, efficient Li-ion power pack, lithium ions ping-pong between two electrodes: a sheet of lithium cobalt oxide and a sheet of graphite. When you charge your new drone (or smartphone or laptop or hover­board), electrons flowing in from the outlet help lure lithium ions out of the LiCoO2, which then migrate to the graphite electrode and wait to be released—along with electrons (energy!)—later. Within the Li electrode, cobalt and oxygen form sturdy layers of octa­hedrons, which keep the molecule from collapsing as ions enter and exit. But at high temperatures that edifice can crumble, contributing to a very combustible situation.
Graphite
A mineral form of pure carbon, best known as the writey part in pencils. Graphite forms the second electrode, and the lithium ions lodge within it as the battery charges. Then, when you turn on your MacBook, those ions get pulled out of the graphite to journey back to the lithium cobalt oxide, a process that produces the electricity to let you browse Facebo—we mean respond to all those work emails.
Polypropylene
A thin slab of this plastic keeps the electrodes apart. The separator, as it’s called, is perforated with micron-scale holes to let Li ions pass through. It’s often to blame if your phone becomes a smoking inferno—faulty separators can let the electrodes touch, triggering a process known as thermal runaway. That can quickly generate huge amounts of heat (up to 1,700 degrees Fahrenheit) and pressure, causing the flammable substances present to burst into flames.
Ethylene Carbonate
This clear, flammable, organic solvent helps shuttle ions back and forth. If a malfunctioning battery heats up enough (say, from a bad separator), the hot liquid can escape the case, react with oxygen in air, and kaboom!
Lithium Hexa­fluorophosphate
A white powdery substance that’s dissolved into the ethylene carbonate, LiPF6 just sort of floats around in the battery, supplying additional lithium ions to speed up charging and discharging. This compound is not flammable. Yay! But it does burn skin on contact. Boo! ­
This article appears in the April issue. Subscribe now.
Technically it ain’t brain surgery, but let’s just say you wouldn’t want to do a cochlear implant while sleepy or distracted. So it’s a good thing this surgery robot can't be either of those things. It drills into the bone behind the ear, watching with two shining eyes. The bit passes just half a millimeter from the facial nerve, and another half a millimeter from the taste nerve, before entering the spiraling cochlea of the inner ear. Here a human deposits an electrode.
The first robot-assisted cochlear implant in a clinical trial, which researchers describe today in the journal Science Robotics, doesn’t just enhance a surgeon’s dexterity like the by-now-common da Vinci robot might. “We are interested in doing something with the robot that a surgeon is not able to do,” says study co-author Stefan Weber of the ARTORG Center for Biomedical Engineering Research. This thing gives surgeons superpowers, allowing them to “feel” through tissue by measuring how the drill bit’s force changes against bone or flesh. All that precision means, for one, removing less bone to get to the inner ear.
This thing should also make surgeons nervous. I mean, not this robot per se, but the vanguard it’s a part of. Because while surgeons are in total control of the cochlear bot, more machines are coming that will automate much of medicine. And that future will make for a potential regulatory—and public relations—mess.
So today Science Robotics also published an editorial proposing a classification system for medical robots based on their level of automation. The scale goes from zero (no automation, like tele-operated robots) up to five (fully autonomous machines that can perform whole surgeries on their own). Level five is a long ways off, but robots are spawning so many questions about the nature of medicine, the field needs to start talking about them now.
Today, robots and surgeons still have to hold each other’s hands: The cochlear robot needs the human to tell it when to start and stop, and the human needs the robot to avoid nerves on the way to the inner ear. Call it codependency—level 1 autonomy. “The robot provides the guidance and provides for this particular task very accurate sensing, because it's the particular task that will really affect the entire surgery,” says Science Robotics editor Guang-Zhong Yang, a co-author of the editorial.
That kind of human-robot interaction, though, is a stepping stone to truly autonomous machines. “When you come down to level 4 and 5 autonomy, the robot here is not only a medical device but also effectively practicing medicine,” says Yang. That gets sticky, because while the FDA approves medical devices, medical associations keep an eye on doctors. So do both parties get a stake in this case?
The FDA, for its part, will continue to evaluate robots, no matter the level of automation, on a case by case basis. "The appropriate classification and premarket review submission for a device depends on factors including the risk posed by the device and the device’s intended use," says FDA spokesperson Stephanie Caccomo. "The introduction of autonomous functions would be evaluated during premarket review."1
Create autonomous robots to handle wildly complex tasks like surgery and you also run the risk of losing that knowledge. If a robot replaces a certain job in the operating room, it'll be hard to compel doctors to train up that same skill—even though it'll probably be crucial to have skilled human backups. Doctors will essentially become robot supervisors, and universities will offer very different training, leaving a certain amount of medical knowledge to live within the robot.
Then there’s the matter of convincing the public to trust machines with their lives. Self-driving cars are one thing, but a robot doctor is something else entirely. Like with robocars, you’re better off with a fully autonomous robotic surgeon. They won’t make mistakes and they won’t tire. But that doesn't mean humans will trust them. How do you convince someone that a cold, calculating machine may save their life one day?
All big questions that roboticists are only beginning to tackle. But one thing is for sure: Even if you never need a cochlear implant, Dr. Robot will soon be seeing you.
1UPDATE 3/15/17, 5:10 PM ET: This story has been updated to include comment from the FDA, which could not reply at length by press time.
Landing rockets is SpaceX’s schtick. You know, we know, we’ve all covered it to death. But for the private space company’s latest mission, there will be no smooth reentry—not on land, not on a robot boat, nowhere. For once, SpaceX is just pulling off a regular old rocket launch. For one of the last times ever.
If today’s weather holds up—and the Air Force’s weather team says there’s a 90 percent chance it will—SpaceX’s Falcon 9 rocket will carry the EchoStar XXIII satellite aloft at 1:35 AM EST. If you are awake and reading this at that godforsaken hour, you can watch it live here:

The Falcon 9 won’t be guiding itself back to Earth because its payload is too heavy, and its orbit too high to chance a failed landing. So if the launch is successful, it will be one of the last times SpaceX fires off an expendable rocket. The commercial space giant is planning rocket upgrades that should make landings easier, even under difficult conditions like these.
The mission specifications take the Falcon 9 rocket way outside its comfort zone: EchoStar XXIII, a commercial communications satellite that will provide television broadcast services for Brazil, weighs about 12,000 pounds. And it’s going over 22,000 miles above the Earth’s surface into geostationary transfer orbit—a high orbit with an extra-funky egg-like elliptical shape that sends the craft to its peak before it skims just a few hundred miles above Earth.
Heavy payloads and high orbits both put serious strain on a rocket’s fuel reserves. Packing enough fuel is a classic space exploration problem. (Getting enough lift is tricky, and doing it without going broke even more so.) To make their rockets reusable, SpaceX had to add even more fuel in a reserve, to reignite the Falcon 9’s engines and slow the rocket down as it comes in for a landing.
But if you’re trying to haul something especially heavy up especially high—requiring a propellant-guzzling high-speed launch—you’re likely to burn through a portion of that extra fuel. EchoStar XXIII will burn through so much the Falcon 9 won’t be able to complete a drone boat landing, which are more fuel efficient than ground landings because the boat can move to meet the descending rocket.
SpaceX knows what happens when they try to land an under-fueled Falcon 9:
Looks like early liquid oxygen depletion caused engine shutdown just above the deck pic.twitter.com/Sa6uCkpknY
— Elon Musk (@elonmusk) June 17, 2016

That June 2016 Eutelsat/ABS mission brought two telecommunications satellites to geostationary transfer orbit (sound familiar?), but SpaceX chanced the landing. The rocket delivered the payloads without a hitch, but on return, they ran out of fuel much more quickly than expected. The result? A hard landing, a big old cloud of smoke, and a broken rocket teetering into the sea.
The company doesn’t seem too eager to try it again. Plus, while it has successfully landed missions after reaching geostationary transfer orbit before, landing after a long descent means the rocket ends up coming down way faster and hotter than usual. And SpaceX has already seen their rocket go to pieces over that problem, too.
But that doesn’t mean SpaceX always plans to abandon high-flying rockets carrying heavy loads. According to another set of Musk tweets, in the future, SpaceX will relegate heavier payloads like EchoStar XXIII to the beefed-up Falcon Heavy—made of three Falcon 9 cores strapped together—which is supposed to take its maiden voyage sometime later this year. Or they could move to the Block Five, which Musk claims will be the final version of the Falcon 9. He’s hinted at its increased thrust and more robust landing legs, and that it should fly by the end of the year.
For now, though, payloads like EchoStar XXIII call for Falcon 9 suicide missions. And since this is SpaceX, you should look forward to the rocket upgrade—but don’t expect it to come in on schedule.
You’ve heard the hype: The quantum computer revolution is coming. Physicists say these devices will be fast enough to break every encryption method banks use today. Their artificial intelligence will be so advanced that you could load in the periodic table and the laws of quantum mechanics, and they could design the most efficient solar cell to date. And they’ll be here soon: Writing in Nature earlier this month, Google researchers said they anticipate the first commercial quantum computers in five years, and the company wants to build and test a 49-qubit—that’s “quantum bit”—quantum computer by the end of this year. Some experts say that a 50-qubit computer could outperform any conventional computer.
But there's a big problem: By its nature, you can't save or duplicate information on a quantum computer. All that computing power is of little use if you can't back up your work. You can convert quantum data and put it on a traditional storage device, but all that converted data takes up a lot of space. So physicists are hunting for reliable, super-compact hard drives made of new materials—including DNA.
Quantum computers are so powerful exactly because of their data density. A classical computer reads, stores, and manipulates bits: 1’s and 0’s. A quantum computer uses qubits: tiny quantum objects that can be in two states—both 1 and 0—at the same time, as long as you’re not looking at it. And if you control a quantum particle in a superposition of two states, you can perform tasks in parallel, which speeds up certain computational tasks exponentially. That speed won’t improve your Netflix experience or make Microsoft Excel more bearable, but it will be much faster at running search algorithms or simulating complicated systems like organic materials or the human brain.But the weirdness of quantum mechanics has its drawbacks. Its laws permit superposition, but they also forbid anyone from copying a quantum particle. “It’s called the ‘no-cloning theorem,’” says physicist Stephanie Simmons of Simon Fraser University in Canada. Say that a quantum computer programs an atom to be in a specific quantum state that represents a set of numbers. It is physically impossible for the computer to program another atom to be in the exact same quantum state.
So Simmons proposes a roundabout way of storing quantum data: First, you’ll need to convert it into binary data—translating the numbers that describe quantum superposition into simple 1's and 0's. Then, you store that converted data in a classical storage format. In other words: hard drives. Super compact ones, because the size of each quantum data file from a 49-qubit computer will be on the scale of 40,000 videos.
To store that much data, quantum computer developers need new data storage technologies, Simmons says. Commercial drives aren't compact enough right now. A single quantum file would occupy a stamp-sized area on a solid-state hard drive.
So one alternative storage contender is DNA. Published earlier this month in Science, scientists demonstrated a method that could store 215 petabytes, or 215 million gigabytes, in a single gram of DNA. At that density, all of humanity’s data could fit in a couple pickup trucks. Unlike conventional hard drives, which only store data on a two-dimensional surface, DNA is a three-dimensional molecule. That extra vertical dimension lets DNA store much more data per unit area.
Plus, it lasts a long time. "Think about your CDs from the '90s," says computer scientist Yaniv Erlich of Columbia University, who worked on the research. "They're probably a bit scratched, and you can't read the data accurately. But DNA can store information for a very long time. We can read DNA from skeletons thousands of years old to very high accuracy."
Another super-compact technology encodes bits in single atoms. Last week, researchers at IBM published that they stored a bit in a single atom and successfully read the data back. To do this, they embedded holmium atoms on a chip and used electronics to control the direction of the inherent magnetic field produced by each atom. They found that they could control the atoms independently when they were spaced just a nanometer apart. So basically, it's possible to encode one bit per atom. You can't get more dense than that, says physicist Chris Lutz of IBM. Commercial hard drives store a bit in at least 100,000 atoms—and even a DNA base pair is made of some thirty atoms.
Both of these methods, like quantum computers themselves, are years from being commercial technology. DNA is expensive to synthesize and takes a long time to read out. And to store data in single atoms, you have to keep the atoms extremely cold—close to absolute zero—because otherwise, the atoms will interfere with each other and overwrite their data. On top of that, the quantum computing crowd will need to develop algorithms to efficiently compress and convert quantum data to binary—and then design hardware to execute those algorithms.
Even as Google prepares to run its 49-qubit quantum computer, it’s still not clear how quantum computers will back up their information. "I see huge challenges coming our way," says Simmons. Because if quantum computers don’t back up their data, autosave won't be coming to the rescue.
Thanks to the superposition principle, a quantum machine has the potential to become an exponentially more powerful computer. If that makes little sense to you, here's quantum computing explained.
Millions of Americans are living a lie. You might be one of them: The homeowners who live near a flooding California river, the landlords in a slowly subsiding Southern city, or the business operators on a shoreline inundated a Nor'Easter's storm surge. And all around the country, the federal government is dramatically undervaluing the risk of flooding to their homes or businesses.
The lie originates from the National Flood Insurance Program, which sets rates for 5 million people living in flood-prone areas—based on flood projections that are sometimes decades out of date. Even when the projections are updated, the program lets people pay the old, underpriced insurance rates. That's left the program $24 billion in debt, running an annual deficit of $1.5 billion. Congress is currently holding hearings to reauthorize the NFIP and put it back in the black. But updated, truthful flood insurance prices could put millions of home and business owners deep in the red.
The insurance business relies on bad things happening, but not to too many people at once, and also somewhat predictably. Floods are typically the opposite: widely destructive and wildly unpredictable. For a long time, private insurers wouldn't cover them. So in 1968, the federal government stepped in with the NFIP. FEMA, the agency in charge of the program, sets its rates on so-called Flood Insurance Rate Maps, which show which properties could conceivably be inundated by an overtopped river, storm surge, or other flooding event. But the maps also determine who must buy flood insurance. So the NFIP rate doesn't just protect people from losing their homes, it influences property values and mortgage rates for millions.
Congress must reauthorize NFIP every five years, and right now it's gearing up to rewrite the reauthorization with hearings in both the Senate and House of Representatives. But they're facing the same issues that have plagued the program for over a decade.
The program's troubles began with Hurricane Katrina. The storm was so destructive, NFIP had to borrow $17 billion from the US Treasury to pay out claims. FEMA, which runs NFIP, hadn't prepared for a storm like that. The storm also exposed a major problem with NFIP's rates: Many of them were (and still are) based on old flood maps. This means they were paying rates that were much lower than they should have been, based on the known risk. Or, even where new flood maps did exist, NFIP would only apply those to new constructions, and grandfather in existing property owners at old, low rates. Suddenly, the program was a huge point of contention in Congress, and, in 2008, legislators failed to reauthorize it.
That didn't mean the program—and its problems—went away. Between 2008 and 2012, Congress passed 16 temporary NFIP extensions. These helped destabilize housing markets, because there were sometimes tiny gaps between extensions. "Even some very short lapses can cause a tremendous ripple effect," says John Dickson, president of NFS Edge Insurance Agency. "Realtors can’t move homes because can’t they rate the risk."
Congress finally reauthorized the act in 2012, and even fixed the grandfathering rule (to some extent). But that meant some rate payers saw huge increases. "If you own a house in a flood zone and the map changes underneath you, it can cause your premiums to go up and your property values to drop," says Lloyd Dixon, director of the RAND Center for Catastrophic Risk Management and Compensation. "That's real money, and they'll write their congressman." So in 2015, Congress rolled back some of these ambitious reforms. This year's reauthorization would seek to find a balance between keeping NFIP solvent without pricing people out of house and home.
This tricky balance extends into homeowner psychology. "The price of a flood insurance policy is an important signal to a homeowner, renter, or business about the flood risk they face," says Trotter. "If NFIP policyholders do not understand their true risk, they may not make the right decisions about how to protect their homes or businesses and their own safety." Because of that, the 2015 update to NFIP requires FEMA to tell homeowners their true flood risk, regardless of their rate. The same law also gradually increases the grandfathered, and therefore undervalued, rates to reflect the actual risk. Sometimes the signal is overwhelming: In 2013, after FEMA redrew the flood maps in parts of New England, a Boston woman's flood insurance jumped to $68,000.
But that doesn't solve all the problems. NFIP still needs new maps, which FEMA's deputy administrator Roy Wright has been hammering on in recent congressional hearings. Last week, he went before the House Financial Services Committee, and today at the Senate banking committee.
The National Flood Insurance Program could help itself by promoting private insurance. In the past five to 10 years, computer analytics have grown to the point where private companies can estimate flood risk. "Geomodels are coming online to help companies understand the things that drive flood risk," says Dickson. These include wind, elevation, and fluid dynamics. "Flood is the only natural disaster where manmade influences can actually impact the scale of the flood itself," he says, so these models also include updated maps on water irrigation systems, what's paved and what's not, and even larger scale, esoteric measurements like shoreline erosion caused by sea level rise.
Congress wants to encourage private insurers. But NFIP's low rates make it difficult for private insurers to compete, and the fact that private insurers can't compete makes it hard for NFIP to raise its rates. Dickson says the current reauthorization could show promise for private flood insurance, if its rates more accurately reflect true risk.
Without an election to contend with, and a united Congress, NFIP's reauthorization is probably in the bag. The biggest struggle will be between the cost-cutting philosophy guiding Congress's ruling party, and the fact that many of them come from states filled with low to mid-income people living in flood zones. Somewhere in that tension, the new truth of American flood protection will emerge.
Last week, as tens of thousands of US and South Korean soldiers gathered at a base in Iwakuni, Japan for an annual joint military exercise, North Korea fired four ballistic missiles from Pyongyang into the sea off Japan’s northwest coast. In a world where the US is headed by a Twigger-happy political neophyte and the risk of a Cold War reboot looms larger with each Wikileaks disclosure, this demonstration wasn’t just an empty display of dictatorial propaganda. It was a reminder that the nuclear threat is still alive and well.
But even if you’ve taken a decades-long break from stocking your fallout shelter, the federal government hasn’t. Over the last ten years the US has poured millions of dollars into technologies and treatments it hopes to never have to use, but could, in the event of a nuclear catastrophe. From assays that measure radiation exposure to cell therapies that restore dwindling blood cells to liquid spray skin grafts, government officials are now far better equipped to deal with diagnosing and treating people if the unthinkable were to happen. And the next generation of treatments are being funded right now.
In 2006, the Department of Health and Human Services established the Public Health Emergency Medical Countermeasures Enterprise to coordinate federal solutions to large-scale public health threats, including the nuclear one. Pretty much every agency you can think of is involved—CDC, NIH, FDA, DoD, DHS, USDA, VA, and OEM, among others. But in terms of nuclear countermeasures, three programs nested within HHS do the bulk of the heavy lifting.
The NIH’s National Institute of Allergy and Infectious Disease is the first stop; it runs clinical and preclinical trials for promising technologies. Then there’s the Biomedical Advanced Research and Development Authority—Barda—which is basically a taxpayer-backed investment firm that develops these potential drugs, vaccines, treatments, and supplies and ushers them through FDA approval. Finally there’s Project BioShield, which Barda uses to contract with companies when their products are almost ready, ensuring a national market. To date, the program has acquired 12 products related to a nuclear blast or reactor meltdown, some FDA-approved, some still in late stage development, but all destined for the Strategic National Stockpile, the CDC-managed backup supply of drugs and medical supplies for use in a public health emergency. And each class of products addresses a different part of the threat.
The first is diagnosis. When a person is exposed to high levels of radiation, unpaired electrons careen around their cellular machinery, breaking DNA and causing damage to every organ, including the bone marrow. This means you can’t generate new red blood cells, white blood cells, and platelets, so you can’t fight off infections or coagulate your blood. People usually don’t start feeling the effects of acute radiation syndrome for 24 to 48 hours, but damage to their cells’ DNA starts almost immediately. Which is why you need a reliable diagnostic device; following a nuclear event, people who feel well might actually be in danger, and people who weren’t exposed will want treatment just to be safe.
So using Project BioShield, Barda has acquired two diagnostic devices, known as biodosimeters, to tell the difference. One works by measuring gene expression, the other by visually analyzing cell nuclei. “In the event of a nuclear event, the countermeasures we’ve procured will be precious resources,” says Joe Larsen, acting director of Barda's division of chemical, biological, radiological, and nuclear medical countermeasures. “We’re going to end up with a lot of worried well demanding treatment, and we can only afford to treat people that need it.”
That treatment, at least right now, consists of injections of immune-boosting cytokines, developed for cancer patients to restore depleted white blood cells lost during radiation treatments or chemotherapy. Project BioShield has acquired three such cytokine treatments—but, Larsen notes, they won’t work for about 20 percent of people. For them, the only option will be bone marrow or cord blood transplants, which come with the extra obstacle of having to be matched with a donor. So Barda is now looking for cellular therapies that don’t require any donor matching to their portfolio—a universal treatment. “That could shore up gaps in our initial capability to treat radiation.” And they’ve got at a few promising options coming down the pipeline.
Barda recently signed a $188 million contract to develop a stem cell therapy produced by California-based Cellerant Therapeutics, which restores white blood cells in leukemia patients who've had theirs taken out by chemotherapy. The cells are cryopreserved and shelf-stable, important features for a stockpile item. But the treatment is focused on white blood cells, and radiation exposure doesn't limit itself to the immune system's front-line fighters.
To that end, NIAID is funding clinical trials for a placenta-derived stem cell treatment developed by an Israeli company, Pluristem, that has shown the ability to restore all three blood cell lines—red and white blood cells, as well as platelets—in animal models. Like Cellerant's, the treatment comes cryogenically frozen along with a thawing device to deploy it easily in the field. The cells stay viable on liquid nitrogen inside their canisters, so you don’t have to worry about losing them if the power goes out. From their injection site, the placental stem cells sense stress signals in bone marrow tissues, and send more than 20 signaling molecules to repair and restore their functions. The company isn’t testing efficacy in humans, for obvious reasons. But Pluristem says their animal studies showed close to 100 percent survival rates with the treatment, compared to 30 percent without.
Arik Eisenkraft, who began working on an ARS application for Pluristem’s technology following the Fukushima disaster, isn't surprised that a potential solution to nuclear radiation would come out of a place like Israel. “We live in a world of imminent threats, not theoretical ones,” he said. “Even though we don’t have the same budgets and the same scope of institutes, what we do have is a real sense of urgency.”
Neither Barda nor Pluristem could confirm whether or not a contract is somewhere in their future. But the agency did say it was looking at all the options. And with Barda’s budget cut by $160 million last year and an uncertain future for disaster preparedness funds in a Trump administration, there’s no time like the present for some urgency of their own.
On a plate, a single banana seems whimsical—yellow and sweet, contained in its own easy-to-open peel. It is a charming breakfast luxury as silly as it is delicious and ever-present. Yet when you eat a banana the flavor on your tongue has complex roots, equal parts sweetness and tragedy.
In 1950, most bananas were exported from Central America. Guatemala in particular was a key piece of a vast empire of banana plantations run by the American-owned United Fruit Company. United Fruit Company paid Guatemala’s government modest sums in exchange for land. With the land, United Fruit planted bananas and then did as it pleased. It exercised absolute control not only over what workers did but also over how and where they lived. In addition, it controlled transportation, constructing, for example, the first railway in the country, one that was designed to be as useless as possible for the people of Guatemala and as useful as possible for transporting bananas. The company’s profits were immense. In 1950, its revenues were twice the gross domestic product of the entire country of Guatemala. Yet while the United Fruit Company invested greatly in its ability to move bananas, little was invested in understanding the biology of bananas themselves.
United Fruit and the rest of the banana industry did what industries do. They figured out how to do one thing well—in this case, grow one variety of banana, the Gros Michel. Moreover, because it is difficult to get domesticated bananas to have sex (they are puritan in their proclivities, blessed with virtually no seeds), the Gros Michel was reproduced via suckers, clonally. Cuttings from the best specimens were replanted. As a result, virtually all bananas grown in Guatemala, in Latin America in general, and around the world for export were genetically identical. Identical in the way that identical human twins are identical and even a tiny bit more so. For industry, this was great. Bananas were predictable. Each was like each other. No banana was ever the wrong size, the wrong flavor, the wrong anything.
It is hard to overestimate how unusual the situation of bananas in the middle of the last century was—unusual not just in the history of humanity but also in the history of life. There is a patch of aspen trees in the Wasatch Mountains of Utah that many argue is the largest living organism on earth. It comprises some thirty-seven thousand trees, each of which is genetically the same as the other, and the argument goes that the trees, collectively, represent a single organism because they are identical and connected by their roots. But requiring pieces of an organism to be connected in order to be considered part of a collective is arbitrary. The ants in an ant colony, for example, are clearly part of the colony, even when they’re not physically in the nest. All this is to say that an argument can be made that large groups of genetically identical plants, even if not connected, may reasonably be considered a single organism. If one makes such an argument, the banana plantations of Central America in the 1950s were not only the largest collective organism alive at that point, they also may well have been the largest collective organism ever to live.
Rob Dunn (@RobRDunn) is a professor in the department of applied ecology at North Carolina State University and at the Natural History Museum of Denmark at the University of Copenhagen.
Economically, growing just a single clone of bananas was genius. Biologically, it posed problems. These problems had already been noted, for example, in the British production and export of coffee in the 1800s. At that time, the British drank coffee, not tea. They drank coffee exported from their colony Ceylon (now Sri Lanka). Early on in Ceylon, coffee plantations were planted among wild forests. When the British took Ceylon from the Dutch in 1797, they began to expand coffee production on the island. Investment in the coffee plantations by the English, both at home and abroad, “was unlimited; and in its profusion was equaled...only by the ignorance and inexperience of those to whom it was entrusted.” As the demand for coffee increased, it was planted in large monocultures—that is, vast areas of only a single variety of tree. Coffee on one hill, coffee on the next. Not a taller, wilder tree to be seen. There were 160,000 hectares of the central uplands planted in coffee. The coffee brought real affluence—banks, roads, hotels, and luxury. It was an unbridled success, or seemed to be.
Harry Marshall Ward, a British fungal biologist visiting Ceylon in 1887, warned farmers that farming such large plantations of a single variety of coffee would cause problems. Pests and pathogens, once they arrived in the plantations, would devour them. This was, he thought, particularly true of coffee rust, which was already present in Ceylon, but it would also be true of any other pest or pathogen that arrived. Nothing would stop such an organism from quickly devouring all the trees, since they were all of the same variety—and thus equally susceptible to whatever threat might arise or arrive—and planted very close together. This is exactly what happened. Coffee rust wiped out the coffee of Ceylon and, subsequently, much of the rest of the coffee of Asia and Africa. Coffee growers replanted with tea.
Ward had predicted that the coffee of Ceylon would be devastated. As the plantations of bananas expanded across the American tropics, scientists made similar predictions. These scientists noted that in the native range of bananas lived a great diversity. There were big ones, small ones, sweet ones, sour ones, hard ones, soft ones, bananas as dessert, and bananas—plantains, really—consumed as sustenance. In those same regions one could also find an extraordinary diversity of pathogens. But in the cultivated world of bananas, the scientists pointed out, because a single genetically identical variety of banana was planted everywhere, were any banana-attacking pathogen to arrive, it would mean trouble. Any pathogen that could attack a single banana plant, even one, would be able to kill all of them. If the banana companies had listened to these warnings, they might have planted a diversity of banana varieties or a variety that would be resistant to the most likely pathogens. But why would they? The single clone of the Gros Michel banana was the most productive anyone had ever found. Planting anything else would mean losing money.
Then the inevitable happened. A malady arrived—Panama disease (now more often called fusarium wilt), caused by the pathogen Fusarium oxysporum f.sp. cubense. Panama disease started to wipe out banana plantations in 1890. Nothing precluded its spread or even promised to slow it. Seen from above, the plantations across Latin America started to look like the lights had been turned off. Patches of bright green went black. Whole landscapes went black. In the Ulua valley of Honduras alone, thirty thousand acres were infected and abandoned within the first year in which Panama disease arrived. Nearly all the banana plantations in Guatemala were devastated and, once devastated, abandoned, because it was quickly figured out that the pathogen, having arrived, could lurk in the soil for years (or even, as we now know, decades).
United Fruit Company’s leaders believed that if they were able to find another banana, one that vaguely resembled the Gros Michel but was resistant to the pathogen, it could be planted on the abandoned land and the banana empire could be restored. This plan, however, was based on a farcical set of assumptions. It assumed that consumers would simply accept whatever banana you sold them as long as it looked more or less the same. In addition, it overlooked the reality that no replacement banana had yet turned up — no good option, anyway. The only banana that seemed both pathogenresistant and similar to the Gros Michel was a banana called the Cavendish. The Cavendish tasted very different from the Gros Michel. It had “off flavors” and was less sweet. What it had going for it, though, was that you could plant it even where Panama disease was present in the soil and it wouldn’t die (and it still doesn’t).
Over the next several years, the Cavendish banana would prove to be the only banana that both looked like the Gros Michel and would resist Panama disease. So it was that without any other real options, and having helped to overthrow a democratically elected government so as to continue to be able to produce cheap bananas, the United Fruit Company started to plant the Cavendish across hundreds of thousands and then millions of acres. They then began to export it to the United States, along with a massive advertising campaign lauding the benefits of the banana. It worked.
Just as the British had earlier switched from coffee to tea (substituting one caffeinated drink in a cup for another), Americans switched from the Gros Michel banana to the Cavendish. The advertising was so good that the new banana, the Cavendish, was even more successful commercially than had been its predecessor, the Gros Michel. Bolstering the Cavendish’s sales was the shift of American populations to cities, where the connection between what consumers bought and what grew well locally had been severed. Sales of the Cavendish banana were strong, and they continue to be.
It is with very few exceptions the only kind of banana you find in stores outside the regions where bananas grow. Its success fuels the economies of whole countries. It is the biggest export of Costa Rica, Ecuador, Panama, and Belize and the second most valuable export for Colombia, Guatemala, and Honduras. If you were born after 1950, you are unlikely to have ever purchased any banana other than the Cavendish clone—other than what is now the world’s largest organism. To the extent that anyone worried about diseases affecting the Cavendish, it was because of black leaf streak (Mycosphaerella fijiensis), which was not nearly as bad as Panama disease. Panama disease, meanwhile, had become a thing of the past. The Cavendish remained resistant in part because the pathogen itself is not very diverse and so relatively unable to adapt.
Industry, we learn from the story of the Cavendish banana, will plant the crop that grows most easily and supply it to us whenever we want. It will encourage us to want it all the time. It will tend to plant crops in ways that produce the greatest yield, even if that mode of production has costs; even if it also puts the very crop the industry depends on at risk. Cavendish bananas are all genetically identical. Each banana you buy in the store is the clone of the one next to it. Every banana plant being grown for export is really part of the same plant, a collective organism larger than any other on earth, far bigger than the clonal groves of aspens.
This giant organism is now at risk of exactly the same sort of population crash that befell the Gros Michel, and a new strain of Fusarium, a close relative of the pathogen that causes Panama disease, has evolved. It can kill both Gros Michel and Cavendish bananas. This strain has already spread from Asia to East Africa and seems likely to make its way to Central America. This should be extremely worrisome. But what should be more worrisome is that the same is true of most of our crops, most of the plants that we most depend on, a list of species that is shockingly and increasingly short.
The simplification of the agricultural world and our diets has come with benefits. They are the same benefits that accrued to the United Fruit Company (rebranded in 1984 as Chiquita Brands International, a.k.a. Chiquita)—the ability to produce a large amount of food on a given area of land. In concert with the homogenization of agriculture, we have figured out how to grow more food per acre than ever before—ten times more food than ten thousand years ago, perhaps a hundred times more than fifteen thousand years ago. As a result, a smaller number of people on earth go hungry today than at any other moment in the last thousand years. Modern science has brought us food in abundance, just as it brought the United Fruit Company affluence. Yet this abundance, like the affluence of modern banana companies, is tenuous, dependent on our ability to protect the very few species on which we now depend. The problem is that nearly all those key species are in trouble, because in simplifying the production of our food we achieved short-term benefits at the expense of long-term benefits—and, for that matter, at the expense of long-term sustainability.
The problem we face is the consequence of the preferences of our brains, reinforced by the incentives of industry. We live in a thoroughly modern world with brains and bodies that evolved in an environment where sweets, fats, proteins, and salt were all hard to get. We have simple ape brains and simpler ape nervous systems. Our ancestors evolved taste buds that rewarded them when they found food that provided these necessities. Our environment has changed. Our needs have changed. But our taste buds remain the same. We experience pleasure when we eat these substances, our body’s way to reward us for having found them. Our brains, meanwhile, are wired to spot shiny, bright fruits. As a result, the world we were most likely to create is one in which our foods appeal simply to these ancient preferences. This is precisely what we have done and precisely what one encounters in the grocery store, where the foods in the greatest abundance are now perfectly matched to our ancient needs despite our modern waistlines. Inasmuch as we demand (or at least buy) the same things regardless of the time of year, the foods in the grocery store are never out of season. What’s more, whereas the fruit and vegetable aisles of some grocery stores are relatively diverse, the vast majority of the calories in our diets come from the processed foods found in the rest of the store, foods that can stay on the shelf long beyond the seasons of the plants (or animals) from which they are made.
Globally, we favor the crops that best satisfy our ancient needs at the lowest cost, regardless of how far they might have to travel and regardless of the season. The more urban our civilization becomes, the more disconnected it becomes from the life on which we depend and thus the more extreme our demand for simple products regardless of the season. The crops that are expanding—in terms of the area over which we plant them—are not those that are the most flavorful or nutritious but rather those that are used to produce sugar (sugarcane, sugar beets, corn) and oil (oil palms, olives, canola).
That we have created such a simple world seems dissatisfying, but just because something is dissatisfying doesn’t mean it won’t suffice. Theoretically, we could live off of a diminishing number of crops. We could even get by on a single crop. Potatoes, for example, provide nearly all the nutrients we need, as do cassava and sweet potatoes. But just as our demand for a few basic foods whenever we want them was predictable, so, too, were the problems these crops are now facing. The more we feed ourselves according to our most primitive desires, the more we create a world dominated by just a few productive crops—crops that are threatened by their very commonness. Even coffee is at risk again. Having learned nothing from Sri Lanka, we have once more planted varieties of coffee that are susceptible to coffee rust in large plantations, and the rust is back. That these crops are nearly all at risk today from pests, pathogens, and climate change is not a fluke. Given our preferences, it was nearly inevitable.
The risk to our crops comes in direct proportion to the ways in which we have simplified agriculture. Nearly every crop in the world has undergone a very similar history—domesticated in one region, then moved to another region, where it could escape its pests and pathogens. But these pests and pathogens, in our global world of airplane flights and boat trips, are catching up. Once they do catch up, there are only very few ways to save our crops, and all of them depend on biodiversity, whether in the wild or among traditional crop varieties. This was true with the banana. Saving banana production around the world depended on finding the Cavendish banana, which relied on the work of the farmers that produced and grew it in the first place. Saving the banana when the Cavendish collapses will depend on our finding yet another variety and having similar luck. Alternatively, someone might be able to breed a new, resistant banana using some mix of new technologies and ancient varieties. But if they are going to do so, it will need to be soon.
The more we heed our basic instincts for cheap sugar, salt, fat, and protein in whatever form we want it, whatever time of year we want it, the more we create a simple agricultural world and the more we will depend on the diversity of life with which that same agriculture competes on a finite planet. This book is the story of scientists racing to save the diversity of life in order to save our crops and in order to save us. It is the story of a puzzle we must solve. The ancient rules of life leave us relatively few ways to arrange the pieces.
Excerpted from Never Out of Season: How Having the Food We Want When We Want It Threatens Our Food Supply and Our Future, Copyright © 2017 by Rob Dunn. Used with permission of Little, Brown and Company, New York. All rights reserved.
The best thing about pi is finding it in places you don't expect—like, say, a random walk. What is a random walk? Excellent question! Let me show you.
Start at some location. The simplest location to start with is at the origin so x = 0 meters. Now flip a coin. Heads? Great. Move one meter to the right. Tails? One meter to the left. Repeat as often as you like. Congratulations. You've completed a random walk in one dimension. Normally I would draw a diagram to explain this, but instead I'll make a random walk in python. Click play to start and the pencil to see the code.
Examining the code might help you see what's going on. But this is basically how it works:
But I don't want to make one random walk. I want to run it a whole bunch of times and see what happens. Let me start by taking 100 random steps. Of course, if I run it once, I could end up anywhere between -100 and +100. But if I do this 100-step walk 1000 times, I can determine where I end up on average. This histogram shows 1000 random walks of 100 steps in one dimension:
I could find the average of these values, but why bother? It seems clear that the average ending position is back at the origin. That makes sense. If I am equally as likely to go left or right after many steps, I am very likely to have just as many left steps as right steps and end back where I started.
How about a plot of the total distance from the origin to the end of the walk? This is a plot of the absolute value of the final x-position—this is same as the total distance from start to finish of the walk.
Yes, it looks crazy. In fact, the average final distance (not position) for this run is 7.848 and not zero. But it's not crazy. If you look at the first histogram showing the final x-position, yes, the highest occurring final position was x = 0. But if you look at the number of x = -1 and x = +1, they outnumber x = 0 and you have only positive values. These two things give a non-zero average distance.
OK, I've kept you waiting long enough. Today is Pi Day and you came looking for pi, so I will give you some pi because I always write about pi on Pi Day. Of course you've realized that the average distance for a random walk depends on the number of steps. That makes sense, right? But it turns out the average distance also depends on pi. Here is the relationship (please don't ask me to derive this):

In this expression, n is the number of steps. From this, I can use the random walk to find a value for pi. Here's the plan: Run the random walk for 10 steps (do it 1000 times to get an average). Repeat for 20 steps, 30 steps, and so on. If you plot the average distance squared versus the number of steps, you should get a straight line with a slope equal to 2/pi:
Here the slope is 0.631. If I set this equal to 2 over pi the pi would be 3.1696. Not exactly pi (3.1415...), but close enough for me. It's conceivable that you could make a plot that yields a better estimation of pi. You might change the number of runs to do that. When the program gets to higher steps (like near 1000) I probably ought to run more than 1000 runs because it's very possible to get much higher deviations from the expected value. Oh, well—that's something you can try. Here is an online version of this calculation in case you want to play with it.
I might be obsessed with random walks. Someone send help before I lose control. In the meantime, I might as well make a 2-D random walk. It's just like the 1-D walk except I can take each step in one of four directions—+x, -x, +y, -y. Yes, this is still a discrete random walk (a lattice random walk) such that each step has a size of 1 unit and I am always at a coordinate location with integer values.
Here is my visual 2-D random walk with 100 steps, but you can change that in the code if you like.
To help with the visualization, I change the color and size of both spheres that represent the start and finish of the walk. I find it fun to watch. OK, now for some useful stuff. Let's say that I take 100 random steps and I repeat this 1000 times. What is the average ending distance from the starting point? Here is a histogram:
This gives an average distance of 8.820 units. Perhaps this isn't terribly useful. But as with 1-D, you see a relationship between the average distance and the number of steps:

Once again, I can plot the average distance squared vs. the number of steps. In this case, the slope will be pi divided by 4:
From the slope of this data, I get a value of pi at 3.136. Not too bad. It isn't the best way to find pi, but it's still fun.
I promise this will be the last random walk, at least in this post. This walk also is in 2-D, but with a difference. Instead of moving in the x or y direction, this one takes a step size of one at a random angle. This means the moving ball doesn't have to end up with an integer value for the final coordinate.
Does this matter for the distance traveled? Here is same plot of distance squared vs. number of steps:
Looks like it still works. Yay for pi, the hidden ninja of the physical world. It keeps popping up in places you wouldn't expect.
You didn't think you'd escape Pi Day without some homework, did you?
In the last 800,000 years, Earth has chilled and thawed its way through eight ice ages, each lasting 40,000-100,000 years, with shorter, warmer interglacial periods—like the present. But why? Why didn't Earth just freeze the one time and stay that way?
You're glad it didn't, because the periods between glaciations are when you get things like agriculture, city-states, plumbing, sunbathing, and the Nintendo Switch. Human civilization happened because something reversed a cooling trend about 20,000 years ago.
A new study, published today in Nature Geoscience, has a hypothesis what that something was: plants. Or, more specifically, a complicated process in which plants wear down certain kinds of rocks, and how those rocks remove carbon dioxide from the atmosphere as they wear down—leaving just enough CO2 out there to trap solar warmth, and gradually bring summer back.
Over the course of its 4.5 billion year history, Earth has trended hot. Ice ages are relatively rare, and could happen for many reasons. For instance, continental drift might uplift a mountain range, that would then speed up the rate of erosion. The connection between rocks turning to dirt and ice ages isn’t an obvious one, but the geochemistry is solid. When rainclouds form in the atmosphere, carbon dioxide gets pulled into the millions of little droplets. When that rain falls on silicate rocks—which make up about 90 percent of the Earth’s crust—the rocks dissolve and react to create carbonate. This flows into streams, rivers, and finally the ocean, where single-celled organisms use the carbonate to make shells1.
The big clue in this study's theory of how those interglacials come about came from ice core data taken from Antarctica. That ice is old enough to record global carbon dioxide levels from the past 800,000. "Everybody focuses on the fact that temperature and CO2 go up and down together through the eight ice ages recorded in these samples," says Eric Galbraith, co-author of the study and paleoclimatologist at the Catalan Institution for Research and Advanced Studies in Barcelona. "Nobody had really paid much attention to the fact that the lowest points of these ice ages always had the same lowest value of atmospheric CO2 concentrations."
But this would probably take a long time to kick in. So it’s possible that another type of microscopic critter, the single-celled phytoplankton that take the role of plants in the ocean, made for a faster-acting thermostat. When CO2 in the atmosphere and surface ocean gets scarce, phytoplankton have a harder time growing. That means less dead phytoplankton, which slows down the pump of carbon-rich dead plankton to the deep sea. Less carbon sinking into the deep ocean means more at the surface that, wave by wave, flushes back into the atmosphere. What really caught Galbraith's attention is the fact that the carbon dioxide levels never really got lower than 180 parts per million. Something, he thought must be keeping holding it from going lower.
Galbraith and his coauthor, Sarah Eggleston, at the Universitat Autònoma in Barcelona, looked into several possible explanations. For instance, maybe deep sea water (which can't physically get below -2 degrees C) was acting like a reservoir of warmth. But no, no, there wouldn't have been enough deep sea water interacting with the atmosphere to force that kind of change. Besides, this ignored the CO2 signal.
Several other competing hypotheses came and went, until finally Galbraith recalled a paper published several years ago by the late Mark Pagani, former director of Yale University's Climate & Energy Institute. "He made a similar argument based on much older data at much longer time scales," says Galbraith. "His idea suggested that plants were changing the rate at which rocks weathered."
The connection between rocks turning to dirt and ice ages isn't an obvious one, but the geochemistry is solid. When rainclouds form in the atmosphere, carbon dioxide gets pulled into the millions of little droplets. When that rain falls on silicate rocks—which make up about 90 percent of the Earth's crust—the rocks dissolve and react to create carbonic acid. This flows into streams, rivers, and finally the ocean, where single-celled organisms use the carbon to make shells.
So where do plants fit into this? Attached to the roots of many plants are microscopic fungi called mycorrhizae that, among other things, help increase the rate at which silicate rocks weather. When the weather gets cold, the plants die off, the fungi do less weathering—the weather itself stops raining so much—and the levels of CO2stay stable.
But wait, there’s more. Another type of microscopic ocean critter called phytoplankton absorb CO2 at the ocean surface and sock it away in the deep ocean when they die and sink. Although today there is plenty of CO2 available at the surface, when CO2 was extremely low these little guys would have grown more slowly. As a result, less sinking of dead little critters into the deeps would have left more CO2 at the ocean surface where it could, wave by wave, flush back into the atmosphere. And unlike the extremely slow plant-weathering process Pagani suggested, changes in phytoplankton could happen in the geological blink of an eye—only a few hundred years.
From there, the story should be familiar: Carbon dioxide traps solar energy, and more solar energy, and several thousand years more solar energy, and eventually it's time for the agricultural revolution.
Climate change-doubting wags will no doubt point to this study as evidence that, even if humans are warming the planet, some vegetational failsafe will keep things from getting prohibitively warm. Plants will just grow more and slurp up all that atmospheric carbon before it permanently borks Earth's climate, right? Not so fast. "Of course plants do grow faster when there's more CO2, but the rate at which they do that is not enough to keep up with human emissions," Galbraith says. The time frame for the plant-carbon-antifreeze cycles described above is thousands of years. Anthropogenic warming is happening at the century scale. And the geophysical, chemical, and biological mechanisms involved in Earth's temperature modulation work very differently when the planet is very cold versus when it is warm-ish and heating up rapidly. Like now.
This study isn't the be all-end all of ice age termination. For one thing, nobody has any solid evidence for the cycle as Galbraith describes it. His idea is just the best one to explain the correlation between ultra-low ice age temperatures and the apparent minimum level of Pleistocene-era CO2 levels, such as an extremely cold period around 60,000 years ago. In other words, he's just getting warmed up.
1 UPDATE: 03/17/2017 12:37pm ET — This paragraph, along with a few other places in the story, have been updated to clarify some important nuances in the carbon cycle.
I love pi. No, not pie. Pi. The number. This crazy number shows up in all sorts of weird places. If you take the square root of the gravitational field (*g *= 9.8 N/kg) you approximately get pi. Place a mass on a spring and let it oscillate? Yup, you get pi. Pi also one of the 5 super numbers in this magical equation:

These are just a few of the reasons I tend to get excited by Pi Day—March 14, for the uninitiated—and try finding new ways of celebrating it. This year, I decided to make an artistic expression of pi using my Raspberry Pi computer. Pretty cool, right? I wanted to somehow create something I can display in the hallway at school to share pi with students. Then they too might find themselves addicted to the awesomeness of pi.
You can calculate pi all kinds of ways, but most people just Google it: 3.1415926535897 and on and on and on to infinity. But what's the fun in that? Wouldn't you rather figure it on your own? Of course you would. This is my favorite way of calculating pi:
Yes, this seems crazy, but it works every time. A picture might help. This one shows 1000 random points calculated using the process I just outlined. Those with an r value less than 1 are red; those greater than 1 are blue.

Notice that the red points approximate a quarter circle. In fact, the ratio of red dots to total dots should be the same as the ratio of the area of a quarter circle (with a radius r) to the area of a square with a side length r. The equation looks like this:

There you go—one cool way of finding pi using random numbers. Let me give you the Python code so you can play with it yourself. Click play to run and the "pencil" to see the code.
If you want some homework, see how many random points you need to get a calculated value of Pi to display correctly to the second decimal—so 3.14. What about the next digit at 3.141? You can keep going if you like. I often do.
Now, I admit that using random numbers to calculate pi is nothing new. But building an interactive poster that uses Raspberry Pi to find pi might be new.

Yes, it's a bit crude, but that's how I like it. I glued a Raspberry Pi to the poster. It runs a version of the random number calculation of pi. The board features a 16x2 LCD character display that provides a real-time value of the estimate of pi, along with a running count of the number of points. I added a reset button; push it and the random pi calculation starts over.
Because I am a physics professor, I added three pages of info explaining pi, how to calculate pi, and all the strange places you can find pi. I also added a beeper that emits a small (and very annoying) beep every time the computer adds another point to the diagram. I like the beeper because it draws attention to the poster as people pass it.__ __I also really like the completely unnecessary LCD display because it looks cool.

OK, I'm sure you want to build one yourself. Here are some tips. (If you want to print this out, here is a pdf.)
To be honest, the 16x2 LCD required a bit more work than I would have liked. In the end, this guide on Adafruit did the trick. I found the buzzer and the beeper pretty easy using the python gpiozero library for the Raspberry Pi. This library makes using the input-output pins on a Raspberry Pi super easy.
For actual calculation, I use the python turtle library. It creates a graphical window with a small "turtle" you can control with Python. It's not the fastest for calculations but it comes installed on the Raspberry Pi. If you want to look at the code, here it is. Make sure your LCD, buzzer, and button use the same gpio pins as Raspberry Pi.
That's it. I hot-glued everything onto poster board and put a monitor next to it. Oh, one last thing. Disable the screensaver on your Raspberry Pi so everyone can enjoy pi as much as you do.
In the 1960s, the charismatic physicist Geoffrey Chew espoused a radical vision of the universe, and with it, a new way of doing physics. Theorists of the era were struggling to find order in an unruly zoo of newfound particles. They wanted to know which ones were the fundamental building blocks of nature and which were composites. But Chew, a professor at the University of California, Berkeley, argued against such a distinction. “Nature is as it is because this is the only possible nature consistent with itself,” he wrote at the time. He believed he could deduce nature’s laws solely from the demand that they be self-consistent.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Scientists since Democritus had taken a reductionist approach to understanding the universe, viewing everything in it as being built from some kind of fundamental stuff that cannot be further explained. But Chew’s vision of a self-determining universe required that all particles be equally composite and fundamental. He conjectured that each particle is composed of other particles, and those others are held together by exchanging the first particle in a process that conveys a force. Thus, particles’ properties are generated by self-consistent feedback loops. Particles, Chew said, “pull themselves up by their own bootstraps.”
Chew’s approach, known as the bootstrap philosophy, the bootstrap method, or simply “the bootstrap,” came without an operating manual. The point was to apply whatever general principles and consistency conditions were at hand to infer what the properties of particles (and therefore all of nature) simply had to be. An early triumph in which Chew’s students used the bootstrap to predict the mass of the rho meson—a particle made of pions that are held together by exchanging rho mesons—won many converts.
But the rho meson turned out to be something of a special case, and the bootstrap method soon lost momentum. A competing theory cast particles such as protons and neutrons as composites of fundamental particles called quarks. This theory of quark interactions, called quantum chromodynamics, better matched experimental data and soon became one of the three pillars of the reigning Standard Model of particle physics.
But the properties of individual quarks seemed arbitrary, and in another universe they might have been different. Physicists were forced to recognize that the set of particles that happen to populate the universe do not reflect the only possible consistent theory of nature. Rather, an endless variety of possible particles can be imagined interacting in any number of spatial dimensions, each situation described by its own “quantum field theory.”
Geoffrey Chew giving a seminar in Berkeley, California, in 1961.
The bootstrap languished for decades at the bottom of the physics toolkit. But recently the field has been re-energized as physicists have discovered novel bootstrap techniques that appear to solve many problems. While consistency conditions still aren’t much help for sorting out complicated nuclear particle dynamics, the bootstrap is proving to be a powerful tool for understanding more symmetric, perfect theories that, according to experts, serve as “signposts” or “building blocks” in the space of all possible quantum field theories.
As the new generation of bootstrappers explores this abstract theory space, they seem to be verifying the vision that Chew, now 92 and long retired, laid out half a century ago—but they’re doing it in an unexpected way. Their findings indicate that the set of all quantum field theories forms a unique mathematical structure, one that does indeed pull itself up by its own bootstraps, which means it can be understood on its own terms.
As physicists use the bootstrap to explore the geometry of this theory space, they are pinpointing the roots of “universality,” a remarkable phenomenon in which identical behaviors emerge in materials as different as magnets and water. They are also discovering general features of quantum gravity theories, with apparent implications for the quantum origin of gravity in our own universe and the origin of space-time itself. As leading practitioners David Poland of Yale University and David Simmons-Duffin of the Institute for Advanced Study in Princeton, New Jersey, wrote in a recent article, “It is an exciting time to be bootstrapping.”
The bootstrap is technically a method for computing “correlation functions”—formulas that encode the relationships between the particles described by a quantum field theory. Consider a chunk of iron. The correlation functions of this system express the likelihood that iron atoms will be magnetically oriented in the same direction, as a function of the distances between them. The two-point correlation function gives you the likelihood that any two atoms will be aligned, the three-point correlation function encodes correlations between any three atoms, and so on. These functions tell you essentially everything about the iron chunk. But they involve infinitely many terms riddled with unknown exponents and coefficients. They are, in general, onerous to compute. The bootstrap approach is to try to constrain what the terms of the functions can possibly be in hopes of solving for the unknown variables. Most of the time, this doesn’t get you far. But in special cases, as the theoretical physicist Alexander Polyakov began to figure out in 1970, the bootstrap takes you all the way.
Polyakov, then at the Landau Institute for Theoretical Physics in Russia, was drawn to these special cases by the mystery of universality. As condensed matter physicists were just discovering, when materials that are completely different at the microscopic level are tuned to the critical points at which they undergo phase transitions, they suddenly exhibit the same behaviors and can be described by the exact same handful of numbers. Heat iron to the critical temperature where it ceases to be magnetized, for instance, and the correlations between its atoms are defined by the same “critical exponents” that characterize water at the critical point where its liquid and vapor phases meet. These critical exponents are clearly independent of either material’s microscopic details, arising instead from something that both systems, and others in their “universality class,” have in common. Polyakov and other researchers wanted to find the universal laws connecting these systems. “And the goal, the holy grail of all that, was these numbers,” he said: Researchers wished to be able to calculate the critical exponents from scratch.
What materials at critical points have in common, Polyakov realized, is their symmetries: the set of geometric transformations that leave these systems unchanged. He conjectured that critical materials respect a group of symmetries called “conformal symmetries,” including, most importantly, scale symmetry. Zoom in or out on, say, iron at its critical point, and you always see the same pattern: Patches of atoms oriented with north pointing up are surrounded by patches of atoms pointing downward; these in turn are inside larger patches of up-facing atoms, and so on at all scales of magnification. Scale symmetry means there are no absolute notions of “near” and “far” in conformal systems; if you flip one of the iron atoms, the effect is felt everywhere. “The whole thing organizes as some very strongly correlated medium,” Polyakov explained.
The world at large is obviously not conformal. The existence of quarks and other elementary particles “breaks” scale symmetry by introducing fundamental mass and distance scales into nature, against which other masses and lengths can be measured. Consequently, planets, composed of hordes of particles, are much heavier and bigger than we are, and we are much larger than atoms, which are giants next to quarks. Symmetry-breaking makes nature hierarchical and injects arbitrary variables into its correlation functions—the qualities that sapped Chew’s bootstrap method of its power.
Alexander Polyakov receiving the Physics Frontiers Prize in Geneva, Switzerland, in 2013.
But conformal systems, described by “conformal field theories” (CFTs), are uniform all the way up and down, and this, Polyakov discovered, makes them highly amenable to a bootstrap approach. In a magnet at its critical point, for instance, scale symmetry constrains the two-point correlation function by requiring that it must stay the same when you rescale the distance between the two points. Another conformal symmetry says the three-point function must not change when you invert the three distances involved. In a landmark 1983 paper known simply as “BPZ,” Alexander Belavin, Polyakov and Alexander Zamolodchikov showed that there are an infinite number of conformal symmetries in two spatial dimensions that could be used to constrain the correlation functions of two-dimensional conformal field theories. The authors exploited these symmetries to solve for the critical exponents of a famous CFT called the 2-D Ising model — essentially the theory of a flat magnet. The “conformal bootstrap,” BPZ’s bespoke procedure for exploiting conformal symmetries, shot to fame.
Far fewer conformal symmetries exist in three dimensions or higher, however. Polyakov could write down a “bootstrap equation” for 3-D CFTs—essentially, an equation saying that one way of writing the four-correlation function of, say, a real magnet must equal another—but the equation was too difficult to solve.
“I basically started doing other things,” said Polyakov, who went on to make seminal contributions to string theory and is now a professor at Princeton University. The conformal bootstrap, like the original bootstrap more than a decade earlier, fell into disuse. The lull lasted until 2008, when a group of researchers discovered a powerful trick for approximating solutions to Polyakov’s bootstrap equation for CFTs with three or more dimensions. “Frankly, I didn’t expect this, and I thought originally that there is some mistake there,” Polyakov said. “It seemed to me that the information put into the equations is too little to get such results.”
In 2008, the Large Hadron Collider was about to begin searching for the Higgs boson, an elementary particle whose associated field imbues other particles with mass. Theorists Riccardo Rattazzi in Switzerland, Vyacheslav Rychkov in Italy and their collaborators wanted to see whether there might be a conformal field theory that is responsible for the mass-giving instead of the Higgs. They wrote down a bootstrap equation that such a theory would have to satisfy. Because this was a four-dimensional conformal field theory, describing a hypothetical quantum field in a universe with four space-time dimensions, the bootstrap equation was too complex to solve. But the researchers found a way to put bounds on the possible properties of that theory. In the end, they concluded that no such CFT existed (and indeed, the LHC found the Higgs boson in 2012). But their new bootstrap trick opened up a gold mine.
Their trick was to translate the constraints on the bootstrap equation into a geometry problem. Imagine the four points of the four-point correlation function (which encodes virtually everything about a CFT) as corners of a rectangle; the bootstrap equation says that if you perturb a conformal system at corners one and two and measure the effects at corners three and four, or you tickle the system at one and three and measure at two and four, the same correlation function holds in both cases. Both ways of writing the function involve infinite series of terms; their equivalence means that the first infinite series minus the second equals zero. To find out which terms satisfy this constraint, Rattazzi, Rychkov and company called upon another consistency condition called “unitarity,” which demands that all the terms in the equation must have positive coefficients. This enabled them to treat the terms as vectors, or little arrows that extend in an infinite number of directions from a central point. And if a plane could be found such that, in a finite subset of dimensions, all the vectors point to one side of the plane, then there’s an imbalance; this particular set of terms cannot sum to zero, and does not represent a solution to the bootstrap equation.
Physicists developed algorithms that allowed them to search for such planes and bound the space of viable CFTs to extremely high accuracy. The simplest version of the procedure generates “exclusion plots” where two curves meet at a point known as a “kink.” The plots rule out CFTs with critical exponents that lie outside the area bounded by the curves.
Surprising features of these plots have emerged. In 2012, researchers used Rattazzi and Rychkov’s trick to home in on the values of the critical exponents of the 3-D Ising model, a notoriously complex CFT that is in the same universality class as real magnets, water, liquid mixtures and many other materials at their critical points. By 2016, Poland and Simmons-Duffin had calculated the two main critical exponents of the theory out to six decimal places. But even more striking than this level of precision is where the 3-D Ising model lands in the space of all possible 3-D CFTs. Its critical exponents could have landed anywhere in the allowed region on the 3-D CFT exclusion plot, but unexpectedly, the values land exactly at the kink in the plot. Critical exponents corresponding to other well-known universality classes lie at kinks in other exclusion plots. Somehow, generic calculations were pinpointing important theories that show up in the real world.
The discovery was so unexpected that Polyakov initially didn’t believe it. His suspicion, shared by others, was that “maybe this happens because there is some hidden symmetry that we didn’t find yet.”
“Everyone is excited because these kinks are unexpected and interesting, and they tell you where interesting theories live,” said Nima Arkani-Hamed, a professor of physics at the Institute for Advanced Study. “It could be reflecting a polyhedral structure of the space of allowed conformal field theories, with interesting theories living not in the interior or some random place, but living at the corners.” Other researchers agreed that this is what the plots suggest. Arkani-Hamed speculates that the polyhedron is related to, or might even encompass, the “amplituhedron,” a geometric object that he and a collaborator discovered in 2013 that encodes the probabilities of different particle collision outcomes—specific examples of correlation functions.
Researchers are pushing in all directions. Some are applying the bootstrap to get a handle on an especially symmetric “superconformal” field theory known as the (2,0) theory, which plays a role in string theory and is conjectured to exist in six dimensions. But Simmons-Duffin explained that the effort to explore CFTs will take physicists beyond these special theories. More general quantum field theories like quantum chromodynamics can be derived by starting with a CFT and “flowing” its properties using a mathematical procedure called the renormalization group. “CFTs are kind of like signposts in the landscape of quantum field theories, and renormalization-group flows are like the roads,” Simmons-Duffin said. “So you’ve got to first understand the signposts, and then you can try to describe the roads between them, and in that way you can kind of make a map of the space of theories.”
Tom Hartman, a bootstrapper at Cornell University, said mapping out the space of quantum field theories is the “grand goal of the bootstrap program.” The CFT plots, he said, “are some very fuzzy version of that ultimate map.”
Uncovering the polyhedral structure representing all possible quantum field theories would, in a sense, unify quark interactions, magnets and all observed and imagined phenomena in a single, inevitable structure—a sort of 21st-century version of Geoffrey Chew’s “only possible nature consistent with itself.” But as Hartman, Simmons-Duffin and scores of other researchers around the world pursue this abstraction, they are also using the bootstrap to exploit a direct connection between CFTs and the theories many physicists care about most. “Exploring possible conformal field theories is also exploring possible theories of quantum gravity,” Hartman said.
The conformal bootstrap is turning out to be a power tool for quantum gravity research. In a 1997 paper that is now one of the most highly cited in physics history, the Argentinian-American theorist Juan Maldacena demonstrated a mathematical equivalence between a CFT and a gravitational space-time environment with at least one extra spatial dimension. Maldacena’s duality, called the “AdS/CFT correspondence,” tied the CFT to a corresponding “anti-de Sitter space,” which, with its extra dimension, pops out of the conformal system like a hologram. AdS space has a fish-eye geometry different from the geometry of space-time in our own universe, and yet gravity there works in much the same way as it does here. Both geometries, for instance, give rise to black holes—paradoxical objects that are so dense that nothing inside them can escape their gravity.
Existing theories do not apply inside black holes; if you try to combine quantum theory there with Albert Einstein’s theory of gravity (which casts gravity as curves in the space-time fabric), paradoxes arise. One major question is how black holes manage to preserve quantum information, even as Einstein’s theory says they evaporate. Solving this paradox requires physicists to find a quantum theory of gravity—a more fundamental conceptualization from which the space-time picture emerges at low energies, such as outside black holes. “The amazing thing about AdS/CFT is, it gives a working example of quantum gravity where everything is well-defined and all we have to do is study it and find answers to these paradoxes,” Simmons-Duffin said.
If the AdS/CFT correspondence provides theoretical physicists with a microscope onto quantum gravity theories, the conformal bootstrap has allowed them to switch on the microscope light. In 2009, theorists used the bootstrap to find evidence that every CFT meeting certain conditions has an approximate dual gravitational theory in AdS space. They’ve since been working out a precise dictionary to translate between critical exponents and other properties of CFTs and equivalent features of the AdS-space hologram.
Over the past year, bootstrappers like Hartman and Jared Kaplan of Johns Hopkins University have made quick progress in understanding how black holes work in these fish-eye universes, and in particular, how information gets preserved during black hole evaporation. This could significantly impact the understanding of the quantum nature of gravity and space-time in our own universe. “If I have some small black hole, it doesn’t care whether it’s in AdS space; it’s small compared to the size of the curvature,” Kaplan explained. “So if you can resolve these conceptual issues in AdS space, then it seems very plausible that the same resolution applies in cosmology.”
It’s far from clear whether our own universe holographically emerges from a conformal field theory in the way that AdS universes do, or if this is even the right way to think about it. The hope is that, by bootstrapping their way around the unifying geometric structure of possible physical realities, physicists will get a better sense of where our universe fits in the grand scheme of things — and what that grand scheme is. Polyakov is buoyed by the recent discoveries about the geometry of the theory space. “There are a lot of miracles happening,” he said. “And probably, we will know why.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Namrata Udeshi knows how to globally analyze the proteomics of human cells. You'd be forgiven for having no idea what that means or why it matters—it's a complicated technique that you'd need years of post-graduate training to master. But for now, just know it's important for disease research. Udeshi is a group leader in a proteomics lab at MIT's Broad Institute, working long days to understand the intricacies of cellular life. She's also the mother of two toddlers, with almost no free time.
And yet, every day, she spends hours learning the programming language Python.
"Ever since I started my post-doc, I realized that it would be great to get data analysis automated," says Udeshi. "But I didn’t know how to program, so I would go and find someone who knew and ask them for help." That was annoying and limiting. Now, she's enrolled in an intro to programming class through Harvard Extension School. Udeshi is hardly alone: When I asked a handful of post-doc biologists eating brunch in Boston last week how many were teaching themselves to code, every hand went up. They all realized that their curriculum was missing a core element, and they’ve set about rectifying the omission—on their own.
The Fight to Legalize a Machine That Melts Flesh From Bone
The Cognitive Bias President Trump Understands Better Than You
A Radical Vision of the Universe Returns to Electrify Physics
It's surprising that it's come to this. In biology, big data is the thing. Every day, biologists go into the lab to coax data out of living matter—more and more data, with the advent of biological tools like Crispr/Cas9. Udeshi used to be able to trace her data in Excel, but in the past five years, those data sets have gotten bigger and bigger. "We cannot manually look through 15,000 data points anymore," she says. To analyze it all, biologists need to write programs specifically tailored for their experiments.
Graduate programs realize that computer scientists aren't the only ones who need computational skills, and they're correcting the issue—slowly. Since 2015, the National Institute of Health has been pushing to add skills training, including coding, to biomedical graduate training, though it hasn't yet reorganized its grant priorities to require these skills. Outside of specialized computational biology and bioinformatics programs, most basic biological graduate programs don't require coding classes.
At UCSF, newly minted department head Anatol Kreitzer is trying to revamp the curriculum for neuroscience grad students. "Our curriculum is 30, 40 years old," he says—it requires some statistics and lots of speciality neurobiology, but no coding. One of Kreitzer's first actions as department head was to assemble a committee to figure out the best way to incorporate coding into the neuroscience program's core curriculum. It might take a while, but it's a start.
In the meantime, working scientists who need to know this skill now turn to books, online courses, and night classes. And mostly, to each other.
Udeshi chose to take a formal course. Sam Myers, a bio-analytical chemist in Udeshi's lab, is teaching himself R by simply "Googling everything." Taking an online course is the middle ground option.
Adam Granger, who graduated from UCSF's neuroscience department three years before Kreitzer took over, would have jumped at the chance to learn coding while he was earning his PhD. Instead, he enrolled a few months ago in an online Python class through the website Code Academy. When he leaves his bench at Harvard, where he's a post-doc in electrophysiology, he opens his laptop at home and goes into a coding vortex. Arpiar Saunders, a genetics post-doc at Harvard, did the same when he learned the language R, though he took a class offered by competing site Code Camp.
Beyond the basics, all of them end up relying on an informal apprenticeship within their labs. Whoever knows the secrets of coding becomes the wizened elder who schools the younger folk—except often the age dynamic is reversed.
"It must be a huge pain in the ass for the coding experts in the labs," says Saunders. When he first started his neuroscience PhD program years ago, he improbably became that person—simply because he'd bought a book on the language Perl over the summer and taught himself the syntax. People in the lab treated him like the expert. "And I'm not a good programmer. I am a barely proficient programmer," he says.
When Saunders became a post-doc, he found an actual expert to help him. "I realized that just the way he held his laptop was completely different from me. His fingers were spread wide open over the keys in this diagonal format, and I just knew I'm fucked, I'm fucked in this whole field," Saunders says. "I type like an old person. These kids, they interact with their computers in a completely different way." Saunders is in his early 30s.
But he's right that this problem is generational. People getting a PhD in neuroscience from Harvard now can take a bootcamp in MatLab in their first year—though it's still optional. As these biologists can attest, it shouldn't be. Not only is coding a core skill that gets the basic work of biology done, it's also taught them to look at problems in new ways. Above all, they agree, coding liberated them.
As tools evolve to allow biologists to gather ever-more-massive quantities of data, people like Kreitzer will find a way to make coding a core part of scientific education. Until then, the biologists will have to go it alone.
Dirk Wallinger leaves his espresso beverage behind and steps out of a coffeeshop on the campus of Denver’s Metropolitan State University. His coat open against the cold, he points across the way to a nearly-constructed structure—the college's metal-and-glass-fronted Aerospace and Engineering Sciences Building. When the building is finished this summer, Wallinger’s company—York Space Systems—will set up a 7,000 square-foot shop inside, creating and eventually controlling satellites (maybe even one you create) with the help of students.
Wallinger gestures toward the fourth floor. “York will be right up there,” he says.
From right up there, York plans to mass-produce a satellite platform that does for the space industry what the industrial loom did for clothing.
Most of the time, if you want to send a serious satellite into orbit, you commission a one-off instrument, built to your personal measurements. But Wallinger’s company is creating a universal platform that can carry whatever sensors, transmitters, receivers, cameras, and experiments a customer wants. That lowers cost and speeds up the design process. And it will help the burgeoning satellite industry—a $168 billion industry in 2010 that had grown to $208 billion by 2015—move from prescription to over-the-counter mode.
What York will offer is a small-but-not-tiny flyer, much more capable and bigger than a CubeSat. York's satellites cost less than other similar-sized satellites because they come in bulk. They can go into any orbit and do Earth observation, meteorology, or communication, without any modification to the platform. And the company plans to produce—with the help of those student-workers—up to 200 satellites a year, shipping orders out as soon as they’re placed. Bigger, customized satellites take months or years to get off the ground.
York isn’t the only company to have thought, "Standard satellites? I'm brilliant!" Mitsubishi has one called the DS2000, which 14 satellites will use by 2018. Russia first developed its Universal Space Platform, nicknamed Viktoria, in the 1990s. Around the same time, France made the multi-use Proteus, currently undergirding five satellites.
And plenty of companies mass-produce satellites to cut down costs. You can buy your from-the-factory CubeSat kits right here, for instance, while Lockheed Martin has used the same analogy Wallinger is fond of—“the Model-T of satellites”—to describe its LM700. They made 90 of those, nearly all of which became the Iridium communications satellites. Today, Lockheed's standard platform is the A2100, used by the likes of your favorite weather satellite, but it's vastly more expensive than York's small model. A company called OneWeb says it will outpace York's capabilities, building three satellite clones per day, but they're not for sale: They're for OneWeb's global internet service.
York, on the other hand is open for business. Right now, 33 customers—evenly split between governmental and not—have signed on to York's particular vision. One of its biggest initial deals is the Army’s Space and Missile Defense Command. In a late-2017 mission called Harbinger (of what?), York will send up its base-model satellite, equipped with radar and a laser that beams back to the ground. After that, a launch company called Vector has agreed to at least six satellite liftoffs between 2019 and 2022. But the company won't just dig into the defense industry's pockets. “A healthy commercial space industry will advance science, not just the military,” Wallinger says. With the satellites' cost—$675,000 for the base model—pure scientists could come aboard.
So while York’s founding principle—standardized sats, en masse—is not a new one, the company may be the first to bring the idea to The People. They'll sell, to anyone, affordable and mass-produced satellites that weigh 143 pounds and can carry 187 more. These are small enough to stay out of Lockheed's space (and big enough to stay beyond the CubeSats'), occupying a middle ground for the person who just wants a cheap, Goldilocks-sized satellite, right now. "The world will always need Ferraris," Wallinger says, referring to whiz-bang custom sats from the world's Lockheeds. "But you don't need one to go get a loaf of bread."
Wallinger thinks Denver is the right place to make the satellites that can go get those proverbial grain products. As he says this, students stream in and out of the coffeeshop, walking past the shadow of York’s future home. As part of internships and senior design projects, they'll work there with York's engineers.
The newly-constructed building where they'll work is a $60 million bid by the university and the state of Colorado to train its own future tech workers, rather than importing them. The building's 118,000 square feet will hold classrooms, labs, industry partners like York, 3-D printers, CNC machines, and a curriculum that local aerospace companies helped design.
It's not a bad bet. Colorado's aerospace economy is second only to California's, home to legacy companies like United Launch Alliance, Lockheed Martin, and Ball Aerospace—and Sierra Nevada Corporation, Northrop Grumman, Boeing, DigitalGlobe, and Raytheon. Startups are settling in, too, like Boom Technology, which is making a supersonic aircraft, and robotics company Altius Space Machines. Somebody has to work at all these places.
Programs like the one at Metropolitan State aim to train Coloradans for those space sector jobs. And the school has both low tuition and students who actually represent Colorado’s demographics pretty well, with roughly equal numbers of men and women, around 40 percent students of color. If one of the commercial space industry's goals is to increase access to space—as York hopes its satellites will—a good way to start is to increase access to the industry itself.
Walter White got it all wrong. If you want to chemically dissolve a body, as the Breaking Bad protagonist ordered his partner Jesse Pinkman to early in the series, you don't just dump it in a bathtub full of acid. "They should have used an alkaline, like potassium hydroxide," says Samantha Seiber, vice president of research at Bio-Response Solutions. On the show, the body dissolved in a few days. "In real life that method would have taken months, without heat, without water circulation," she says. "Watching that was extremely frustrating to me." That's because getting rid of bodies is Seiber's family business.
Bio-Response, based in Danville, Indiana, specializes in building machines for liquid cremation, a fast, environmentally-friendly, and controversial method for disposing of the deceased. Only a handful of states have legalized the practice. The latest battle is taking place in Nevada, where yesterday the state's legislature held a hearing to discuss AB205, a bill that would legalize the chemical dissolution of the dead. Liquid cremation's biggest opponents are typically religious groups, who believe uninhabited corporeal vessels ought not be liquefied and sent spiraling down a drain. Which is a fair, if oversimplified interpretation, of how this process actually works.
Liquid cremation—or, as folks in the biz call it, alkaline hydrolysis—originated in the late 1800s as a way to turn dead livestock into plant food. A century later, a pair of researchers at Albany Medical College adapted the process to liquefy human remains. Early adopters included large research facilities seeking a quick, clean way to get rid of donated cadavers that had, uh, outlived their usefulness. The University of Florida installed the first system 22 years ago. But with legal opposition from heavyweights like the Catholic Church, only 13 states have legalized the practice so far.
Liquefying a body is a chemical reaction involving water, heat, and some sort of alkaline agent—potassium hydroxide and sodium hydroxide are popular choices. "If you think back to high school chemistry, water has a neutral pH of 7," says Dean Fisher, director of UCLA's Donated Body Program. Hydrogen is the workhorse of dissolving the chemical bonds holding a body's molecules together, and even neutral water has enough of the stuff to break down flesh, if given enough time. Alkalines—the opposite of acids—have a higher pH, and catalyze the water to attack those chemical bonds much more quickly. The machines also keep the water flowing, another crucial factor that Walter White screwed up.
And of course, the machines make sure the dissolving bath stays nice and hot. "Heat in every sense speeds up decomposition," says Seiber. "It's the same reason you don’t leave a bologna sandwich with mayo out in the sun, which I’ve done before." BioResponse's two models offer different heat settings for human remains. (The company also makes machines for cremating pets.) They look basically the same: a 10 foot by four foot stainless steel cylinder covered in pipes and nozzles attached by hoses to tanks, all propped up on a trapezoidal dolly—large enough to hold a 500-pound body. "The difference between the machines is the temperatures they operate at, and the time it takes for them to complete the process," she says. The low heat, 208 ˚F models take about 14 to 16 hours for one body. The other machine, which is pressurized to prevent its solution from boiling, operates at around 302 ˚F. At that temperature, bones slough flesh in four to six hours.
In the end, just bones remain—plus any pacemakers, orthopedic implants, and detritus from plastic surgery. "You see fake fingernails, glass eyes, dental fillings, mercury, and gold, all that stuff can be recycled," says Fisher. The bones themselves are pristine, if a bit brittle. These get passed along to a machine called a cremulator, which pulverizes the bone into an ashy powder, which gets passed along to the dead's family. This is similar to how cremation by fire works, except the bony remains are quite charred by the time they come out of the kiln. Plus, the fire doesn't make them quite as brittle, so they don't pulverize to as fine a powder as does the chemical method.
What of the flesh? It's true, it all goes down the drain. "It looks like a weak coffee, like a light roast color," says Seiber. The color comes from the body's pigments. But it's not sludgy like you might expect (and, in fact would be the result if you were to use acid, like Jesse Pinkman did). The machine flushes the liquefied body along with about 285 gallons of the water/alkaline solution used to dissolve it. That might sound like a lot of water, but it's less that person would have used during just three days while alive.
This coffee-colored stuff drains to the wastewater treatment plant. Which is why, every time Seiber sells a new machine, she spends weeks, perhaps months, of permitting with local governmental agencies to ensure that the output from the liquid cremation operation isn't pumping toxins or disease back into the system. "The pH is reduced so the liquid is treated to whatever the authority wants it reduced to right inside the system," says Seiber. And overall, the system is way more eco-friendly than either burial or cremation by fire. It requires no embalming chemicals (besides the ones the family might have requested to preserve the body for a funeral or viewing), and emits a quarter of the carbon produced by fire cremation. Also, it won't burn the mercury in certain dental fillings. However, beginning the afterlife with a clean conscience will cost you: Seiber says alkaline hydrolysis costs roughly $500 more than traditional cremation—which can range between $1,500 to $4,000.
Under Nevada's proposed law, any crematories using an alkaline hydrolysis machine would have to go through the state's Division of Environmental Protection and make sure any liquid they produced wasn't polluting the local supply. And because alkaline hydrolysis has such a smaller environmental footprint, the law would allow any liquid-only crematories to operate in residential zones. Liquid cremation isn’t legal in Indiana, where Seiber’s family’s business is located. After her grandparents died—and were cremated the old fashioned way—Bio-Resources spent $30,000 on a failed lobbying attempt to try and get the state’s government to legalize alkaline hydrolysis. Not that business is bad; the company has sold more than 20 human machines, and about a hundred for pets.
Seiber gets called every so often from legislators considering bills to legalize liquid cremation. And she says the number one question she gets has nothing to do with the ethics of flushing a dissolved body down the drain. "They want to make sure nobody does with our machines what Jesse did to that drug dealer in the bathtub," she says—covering up the traces of a murder. Sure, no DNA or RNA is left after alkaline hydrolysis, which she counters there are plenty of less expensive (and traceable—after all, she does keep invoices) ways to get rid of a body. Still, she can't get that fictional botched acid bath off her mind. "They had the right chemicals out in their trailer the whole time."
There might be older fromances out there, but by most accounts the bond between humans and yeast has been the most prolific. (Also, try to name another fungal romance.) People have been messing with yeast for millennia, ever since ancient hominins first turned wild strains of the fungus into the civilization-supporting fermenters that still make everything from beer and bread to tempeh and fish sauce. That meddling has accelerated in the last twenty years since scientists sequenced the yeast genome, yielding microbes that can burp, fart and secrete biofuels, insulin, antibiotics, and tons of other novel micro- and macromolecules useful to human industry. And soon, the takeover will be complete. Scientists have now designed an entirely artificial yeast genome and constructed more than one-third of it. They say they’ll have a 100% synthetic yeast up and fermenting by the end of the year.
In seven papers published today in Science, representing a decade of work by hundreds of scientists across four continents, the Synthetic Yeast 2.0 project reports the first fully designed, and partially completed, made-from-scratch eukaryotic genome. Eukaryotes—organisms whose cells have a nucleus and other defined organelles—encompass all complex life: yeasts, plants, hamsters, humans. So writing a custom genome for one is a big deal by itself. But the artificial yeast will have a more stable, easily manipulable genome for scientists to work with, and for the chemical, pharmaceutical, and energy industries to use for a new generation of drugs, biofuels, and novel materials.
Joel Bader was sitting in his office in the department of biomedical engineering at Johns Hopkins University School of Medicine when he heard excited voices coming from the coffee lounge outside his door. Jef Boeke, then the director of the High Throughput Biology Center at Hopkins and biochemist Srinivasan Chandrasegaran were talking about what it would take to build all the DNA in a yeast from scratch.
It was 2006, and Bader, who taught computational medicine classes, quickly pointed out that any ambitions of synthesizing a genome of that size (~11 million base pairs) would need some serious computing and software support. So he signed on as Sc2.0’s third team member. Back then, the project was based solely at Johns Hopkins, where Boeke began offering an undergraduate class called “Build a Genome.”
During the first few years, dozens of bright-eyed molecular biology majors got used to keeping odd hours—and keys to Boeke’s lab—as they learned how to string together short snippets of nucleotides into longer, 750-base pair blocks. Other researchers then assembled these chunks into larger and larger stretches of the smallest yeast chromosome, chromosome 3. Then they began putting them strategically into live yeast, which spliced these pieces together into even larger sequences using a naturally-occurring yeast pathway called homologous recombination.
Each section took a long time to build, so as Boeke’s students and colleagues finished a sequence, they’d turn it into a plasmid (a circular, self-contained piece of DNA), and inject it into yeast or E. coli for safe keeping. The lab’s freezers were often filled with hundreds of plates in various states of suspended animation, all holding different pieces of the chromosomal puzzle. Only once they were all complete could they wake up the cells, and put them in new yeasts to finish the final assembly steps.
Boeke has since moved Sc2.0's base of operations to NYU Langone, and Bader has taken over the reigns at Johns Hopkins High Throughput Biology Center. Over time, the team outgrew both labs, and came to encompass more than 500 scientists in ten labs around the world in places like China, Australia, and Scotland.
Bader’s software team at Hopkins built the programs that guide and execute the project’s workflow, setting rules for chromosome design, so the different labs can work on their own chromosomes individually, parallelizing the process and speeding things way up. In 2014, the international consortium revealed its first fully artificial chromosome. Getting those first 272,871 base pairs of it took eight years.
Today's announcement adds five more chromosomes, plus the completed design of the rest—for a total of 17. Any zymologists in the crowd might notice this is one more chromosome than wild yeasts have. The story of how that last one came about starts with the fact that yeast DNA—like all DNA—is full of mistakes and redundancies.
Sc2.0 began as a project to make yeasts better at producing chemicals useful to humans. Evolution optimized yeast for lots of things, but not for industrial production of enzymes or antibiotics. That didn't require remaking the yeast genome verboten, just removing destabilizing DNA from the genome and refactoring the whole thing so future researchers could customize their yeast for whatever compound they wanted to crank out.
One of the biggest changes the researchers introduced was to place 5000 DNA tags throughout the genome that act as landing sites for a protein called “Cre” that can be used to create on-demand mutations. When the protein comes in contact with estrogen it scrambles the synthetic chromosomal sequences—deleting, duplicating, and shuffling genes at random.
By building in these “SCRaMbLE” sites—it stands for Synthetic Chromosome Recombination and Modification by LoxP-mediated Evolution—scientists can start with a test tube filled with a million genetically-identical synthetic yeast cells, randomly reshuffle their genes, and then expose them to different stresses, like heat and pressure, or ask them to make different molecules. It's kind of like natural selection on speed, and allows scientists to easily identify new strains that can survive better in specific environments, or be better factories for things like fuels and drugs.
“We’re shortcutting evolution by millions of years,” says bioengineer Patrick Cai, who first became acquainted with the project as a post-doc in Boeke’s lab in 2010. “Our goal here is not engineering a particular kind of yeast, but the kind of yeast that is amenable to engineering.” Cai now runs his own lab at the University of Edinburgh, where he’s building that extra 17th chromosome. It's the only chromosome that's built completely from scratch.
Cai took on the project after starting his own lab once he left Johns Hopkins—and by that time all 16 extant chromosome projects had been divvied up. His task was to stash all the yeast's transfer RNAs—molecules that ferry amino acids into the right order during protein synthesis. Transfer RNAs are an essential part of the cell’s protein-making machinery, but are notoriously unstable because of how often they're transcribed.
Sc2.0’s scientists figured it would be better to harvest them from their scattered chromosomal locations and put them all together in one place. They call it, the “party” chromosome. “All the troublemakers got their own dedicated chromosome where they can do whatever they want,” says Cai. “That means they’re not causing breakage everywhere else in the genome, so it’s super stable. More stable than anything that exists in nature.”
Sc2.0's yeast DNA isn't just more stable, it's more concise. After all the editing and reworking, the artificial genome is eight percent smaller than a wild yeast’s. Its structure is less prone to unpredictable mutations (the kind that stymie chemical manufacturing), and the tRNA-laden 17th chromosome will give the organisms—once the genome is fully synthesized—near-infinite possibilities for manipulation.
Which is exactly what any good industrialist wants to hear. Jay Keasling, the chief executive officer of the Joint BioEnergy Institute and a professor at UC Berkeley, where his lab engineered yeast to produce the malaria drug, arteminisin, is looking forward to the day when yeast are designed 100% from-scratch. “That gives us a lot more control to build things into the organism so that it doesn’t grow under specific conditions, or produces more of your product.” he says. “There are all kinds of possibilities for the future to make these organisms industrially relevant.” The Sc2.0 team plans to be finished before the end of this year.
Of course, for any yeast—even a completely synthetic one—to become a blockbuster application, it must have complementary systems to efficiently separate, recover, and purify the products. Sc2.0 is leaving that up to industry to figure out. They’ve already entered into one corporate partnership and have three other companies interested (although they wouldn’t share further details.) And while they haven’t yet zipped together the final As, Ts, Cs, and Gs, they’re already thinking bigger than yeast. Later this spring the group is organizing a meeting in New York to talk about driving down the cost of genome building technologies. The end goal? Move from yeasts to plants, maybe even one day to humans. “That will be at least ten times as hard,” says Boeke. “But we plan to forge ahead.” At least ten times as hard to make, and probably way harder to sell to the ethics committee.
The Environmental Protection Agency never plays the popular kid in Washington D.C.'s outsize high school drama. But lately, it's been bullied more than usual: President Trump proposed slashing the agency's budget, appointed career EPA-hater Scott Pruitt to lead it, and ordered it to reverse course or delay dozens of numerous ambitious environment-saving, climate change-fighting regulations. On top of all that, the agency endures endless razzing in the halls of congress.
A pair of longtime EPA antagonists just introduced bills imposing new restrictions on how the EPA uses science to develop policy. On the surface, both appear benign. The first orders the agency to make any data used to develop policy publicly available whenever possible. The second bans for three years any scientist who received an EPA grant from serving as an EPA advisor. At first read, the legislations look like earnest efforts to foster greater transparency in science. With context, however, it's clear that the goal is making the EPA less scientific and less powerful—permanently.
Both bills come from Republicans on the House Committee on Science, Space, and Technology, which spends a considerable chunk its time gaslighting the EPA and other federal agencies that conduct environmental research. The key player in this ongoing campaign? Representative Lamar Smith, a Texas Republican and climate skeptic who, in his opening remarks to a science committee hearing last month titled "Make the EPA Great Again," made his thoughts on the agency clear: "The EPA has proposed some of the most expensive and expansive and ineffective rules in history."
During his past four years as committee chair, Smith has convened numerous hearings on climate science. He's subpoenaed communications between scientists he disagrees with. And he's repeatedly written legislation combatting what he sees as political bias in the EPA's science. It is beyond the scope of this article to examine whether the EPA's science truly contains political bias. However, the bills Smith and Oklahoma Republican Frank Lucas propose aren't designed to root out such things. It's all right there, if you read them.
Let's start with Smith's bill, the Honest and Open New EPA Science Treatment Act of 2017. Yes, that spells honest. Smith possesses a flair for naming bills, and this one essentially rehashes the Secret Science Reform Act he introduced, and Democrats quashed, last year. The Honest Act fills barely four pages. It would bar the EPA from proposing, finalizing, or disseminating any new rule unless the agency used the "best available science" in drafting it, maintained detailed records on all of that data, and posted everything online. That last bit includes a few caveats, as the agency can't post anything that identifies private individuals, exposes trade secrets, and so on.
Smith claims he simply wants to give the public a chance to check, by reproducing or replicating, the EPA's homework. So what's the problem? "The act is worded in a way to prevent the EPA from using science that is not reproducible or transparent," says Melody Brown Burkins, an adjunct professor in science policy at Dartmouth College. "It is effectively saying that the practice of peer review, the gold standard of how science moves to policymakers, is secretive and can’t be trusted." In practice, the legislation would let EPA-adverse politicians and industry lobbyists (who are far more concerned with the EPA's rule-making process than Joe Public) question the agency's science in ways that may not be appropriate.
Appropriate, that is, given the type of studies the EPA often uses. Public health surveys, for example, often take decades to compile and are therefore prohibitively difficult to reproduce. The same is true of studies examining single events, like whether climate change could have fueled Hurricane Sandy or how the Deepwater Horizon disaster impacted Gulf Coast wildlife. Smith's law would give regulation-averse industries an opening to present their own research as equal to the EPA's.
But those are all could-happens. Nobody truly knows what's in Smith's heart. The only conclusively pernicious part of his bill is the money it allocates for the EPA to do this transparency-seeking diligence: $1 million. That's the total set aside not just for compiling and presenting the data, but also for reviewing any contested outcome. Even if Smith wrote his bill with the best of intentions, this piddle of federal dollars sets the EPA up to crumble under the denial-of-service swarm of litigation it would receive at every step under these guidelines. It effectively proves Smith wants the EPA to do as little science as possible, transparent or otherwise.
The second bill, the EPA Science Advisory Board Reform Act of 2017 (a far less satisfying acronym), is similarly sheepskin-clad. Lucas, its author, wants to ensure EPA scientific advisory boards (independent panels Congress mandated 40 years ago to ensure the vetting of rules for scientific integrity) are free from conflicts of interest by "requiring members of the board to disclose their professional backgrounds, and opening the board’s research to public review." In practice, this would bar any scientists who have received an EPA grant from vetting the EPA's science. It also makes the advisory boards more "inclusive," meaning industry representatives could serve more broadly. Foxes, welcome to the henhouse.
Both bills revolve around the idea that the EPA uses bad science. In a statement announcing the bills, Smith even said so: "An open and honest scientific process is long overdue at EPA. American taxpayers have often had to foot the bill for regulations and rules based on hidden science that has not been available for review by the public." Much of the EPA's track record suggests otherwise. Then again, things like the ongoing (yes, still) water crisis in Flint, Michigan, shows the EPA could do a better job turning its science into something actionable.
So what would truly pro-science legislation look like? "I don't have any serious problem with phrasing asking for 'best available science'," says Brown Burkins. "It's the whole package of suggesting that the EPA's administrator cannot do what he or she has always done because that process can no longer be trusted." A real shoring up of science would need, first and foremost, more funding, not a measly million bucks. And more checks and balances, through independent policy groups—not industry flacks and climate skeptics. Industry could (should, even) have a voice, but not at the expense of undermining peer review.
Most of all, any bill including such things would enjoy bipartisan sponsorship and support. Good luck getting that from Smith. Or anyone in the Republican party, for that matter. Smith wasn't in the habit of aisle-crossing during the Obama administration, so he's not likely to take it up now that Republicans control Washington. Especially given that, since November, GOP lawmakers, not to mention Trump and EPA Administrator Scott Pruitt, have focused inordinate amount of political attention on taking down the EPA. One freshman congressman even introduced a single-sentence legislative moonshot to kill the EPA altogether. All of which is to say Smith's long game could finally come to fruition. And unlike the proposed budget cuts, executive orders, and questionable appointments that might last no longer than a Trump administration, the legislation drafted by Smith and Lucas would haunt the agency forever. They would legally bind the EPA to bureaucratic hurdles that would trip up all but the most ineffectual environmental rules. That kind of bullying would permanently damage anyone.
This story originally appeared on High Country News and is part of the Climate Desk collaboration.
In the middle of a clearing, beneath the bright blue bowl of the western Colorado sky, two scientists stood chest-deep in a pit dug into the snow. Crisp morning sunlight glinted off the blocky metal trowel in Andrew Hedrick’s hand. Hedrick, a hydrology technician, stuck the instrument into the solid white wall before him, withdrew it—now loaded with snow—and delicately skimmed off the extra flecks. Amid the muffled whine of snowmobiles ferrying researchers and equipment around the field site, he weighed the snow, then emptied his trowel, ready for another sample. Researchers will use the information Hedrick collected to validate new ways to measure snow and calculate the density of the snowpack—which, together with its depth, indicates how much water it contains.
A handful of other scientists, from agencies and universities across the globe, swirled around the clearing and among nearby spruce trees. The researchers measured the snow with poles and rulers, radar and microwave sensors, and even packed a cooler with snow samples destined for micro-CT scanning at a lab in New Hampshire. Overhead, sensors affixed to airplanes made similar measurements throughout the day.
Snow delivers about 60 to 70 percent of the West’s water supply. The snowpack is an icy natural reservoir that swells throughout the winter, then melts during the summer, providing rivers, agricultural fields, and communities with water. But the amount of moisture in snow varies, and keeping track of how wet snow is across an entire landscape—information essential to the resource managers, farmers and scientists who forecast water supplies and flood potential—has proven difficult. To figure out the best way to do it, Hedrick and about a hundred other researchers converged on snowy, flat-topped Grand Mesa, in western Colorado, for an ambitious scientific treasure hunt in February. For three weeks, they took measurements and tested dozens of instruments and methods, looking for the optimum suite of sensors to survey what one scientist calls “the holy grail” of snow-sensing research: the amount of water held within the snow.
A military research plane flies over Grand Mesa and collects snow surface temperature, radar data and other information to help researchers determine the density of the snow and how much water it contains.
Government agencies monitor the Western snowpack at hundreds of locations across the region. But, useful as those point-measurements are, they don’t tell the whole story, says Kelly Elder, a research hydrologist with the U.S. Forest Service in Fort Collins, Colorado, and the leader of the team that managed the ground campaign at Grand Mesa. Aerial measurements from a satellite or a plane that cover the entire landscape—not just a series of individual sites—are also necessary. “If we can measure from space, or from the air, then there’s hope,” Elder says.
Satellites are already doing some of that work: For decades, they have provided information about how much of the globe is covered by wintertime snow. But that’s not enough, says hydrologist Jessica Lundquist at the University of Washington in Seattle. “What that doesn’t tell you is, OK, do you have a thin little bit of snow, or do you have a really deep pile of snow?” Without corresponding knowledge of how deep and dense the snow is, researchers don’t know how much water is stowed within it.
Years of research suggest that there isn’t a “silver bullet” sensor that can measure snow’s water content on its own from a satellite, says Jeffrey Deems, a scientist at the National Snow and Ice Data Center in Boulder, Colorado. One goal of the Grand Mesa research—part of a planned five-year project, the first year of which is supported by a $4.5 million NASA grant—is to find the ideal combination of instruments that might someday be launched on a satellite to monitor how much water is in snow worldwide. “We have all of these different techniques,” he says. “They all have shortcomings, but they all have advantages as well—and if we can put the right advantages together, we can solve the problem.”
Kelly Elder, a research hydrologist with the U.S. Forest Service and one of the leaders of the field campaign on Grand Mesa, looks at snow crystals through a hand lens. "A great day for me is having time to dig a snow pit and wallow in it," he said while collecting snow samples.
The project’s aerial sensors include LIDAR, a “really fancy range-finder,” Deems says, that uses lasers to measure distances; by comparing scans taken before and after a snow dump, researchers can calculate snow depth. Measurements of the microwave radiation naturally emitted by the Earth are also part of the project: Different densities of snow modify the microwave signal as it moves skyward, so monitoring it from above can give an estimate of snowpack density. Additional airborne sensors measure the temperature of the snow and the albedo, or how much sunlight it reflects—factors that can affect how quickly it melts.
One of the difficulties of measuring snow from planes or satellites is that trees can get in the way. About half of the snow-covered landscape in the Western U.S. is vegetated, so for an aerial sensor to work in the West, it must be able to measure the snow beneath the canopy cover. Grand Mesa, with its 53-square mile flat top and range of forest types—from open shrub-land to dense stands of spruce and fir—is an ideal location to test how well different instruments deal with trees in the absence of other complicating factors, like large changes in topography. To ground-truth the aerial assessments, researchers conducted manual measurements—such as weighing trowels of snow—at about 100 locations across the mesa.
The researchers also visited another area, a steep alpine basin near Silverton, Colorado, to expose their instruments to a different set of conditions. That will help them find a suite of sensors that can work equally well over open Alaskan tundra and a forested Colorado hillside, Deems says. “Can we make the same system work and be resilient to these very different environments?”
And even in the same place, conditions can change, sometimes within just a few hours. As the temperature on Grand Mesa climbed through the early afternoon, scientists shed their gloves and hats, dripping water puddled beneath a snowmobile trailer, and the snow along the footpath from the researchers’ basecamp to the nearest field site took on the consistency of a heavy, jacket-soaking snowball. NASA scientist Ludovic Brucker crouched down to scoop loose snow from underneath a surface crust, demonstrating the layers of differing density that often form within a snowpack. When he stood, a white research plane bearing light reflection and temperature sensors and other snow-sensing instruments flew overhead, its faint roar mingling with the chatter of a nearby creek.
This January, a video game chip started a scientific reckoning. It all began when some "microchip archaeologists" photographed the chip—the MOS 6502 microprocessor that lived inside Atari—and built a digital model of its interconnections. Then some neuroscientists put it to the test. One by one, they knocked out the transistors in their map, trying to get at what the circuit was for. It's similar to what neuroscientists do when they lesion a part of the brain, or silence single neurons. Their project was simple: Could they use the arsenal of neuroscience methods to get at the function of a simple circuit?
They failed. Miserably. The scientists' experiments didn't produce much information about Donkey Kong, Space Invaders, or Pitfall—just which transistors you could knock out and turn the game off. The result was damning for researchers pursuing the connectome, a bottom-up recreation of all the brain's interconnections. To the neuroscience community, the message was clear: Brain scientists may have plenty of bottom-up data about the brain, but they're far from using that data to understand how the organ works. "For all of these approaches, we haven't really thought through how to ultimately get at an understanding of the brain based on the data we're getting," says Konrad Kording, a neuroscientist at Northwestern and one of the study's authors.
But even Kording thinks there’s hope for the future of the connectome. It just isn’t quite what people think.
To get a connectome, neuroscientists bounce beams of electrons off of neural tissue, creating nanoscale images of cell membranes and organelles—in some cases, even little bubbles of neurotransmitters. Then they can trace the long, thin axons and dendrites, ultimately building a map of all their interconnections.
And just having the map isn't enough. For the microprocessor example, "we know exactly how transistors behave, and we can simulate them," says Shawn Mikula, a neuroscientist at the Max Planck Institute for Neurobiology in Germany working toward a whole-brain connectome of the mouse. "But for the cellular connectome, we don't know the individual properties of neurons very well." Neurons have complex electrical properties, and synapses that can be active or silent. They can also release different neurotransmitters onto many different types of receptors. Neurons, Mikula says, “are worlds unto themselves.”
But even if they could collect the data about all of that variation—and both Mikula and Kording are skeptical—it’s a big jump from a simulation to understanding the brain. Kording says that if he could give a colleague a hard disc with the whole human connectome on it right now, they wouldn’t know what to do with it. “The shocking thing is that even the brightest people I know in neuroscience just say, ‘Well, someone's going to figure it out.’” Neuroscientists want to figure out how neurons influence each other, gaining a broader understanding of how the brain computes. But his microprocessor paper suggests that, at least with the tools neuroscientists have now, that wouldn’t be possible.
Mikula—along with many in the field—is less fatalistic. He actually uses the Atari microprocessor in his talks as an example of how bottom-up structure can be used to predict function. “Because in the paper that was exactly what they did,” Mikula says. “They had a circuit structure and they ran simulations on it to determine the function. The paper was actually proving the point of connectomics.” The "functions" were simple—on/off switches and clocks—but they still figured them out.
Just like Kording's experiments didn't get all the way to understanding the microprocessor, a connectome might not give you a perfect simulation of the brain. But it could simplify certain research problems in neuroscience. A neural map could constrain the problem of where a piece of information could go if, say, it came in to your brain through your eyes—letting researchers trace the path through the maze of the brain. In Mikula's view, the connectome is a tool for asking questions about the brain, rather than the answer to the question of how the brain works.
The connectome could also clear up one area of neuroscience that desperately needs some structure: neuroanatomy. Classically, neuroscientists have used traces of different chemicals to parcel the brain out into little modules, like the dopamine-rich substantia nigra, which works to provide rewards. That way of thinking works pretty well for some brain areas, Mikula says, but when you get to more complex processing regions “it starts to get a bit fuzzier.” The connectome could fundamentally change how scientists talk about the brain’s structure. “You can see whether it actually makes senses to talk about having discrete processing modules or discrete areas in the brain,” says Mikula.
To really tap the potential of the connectome-as-tool, though, neuroscientists will need to figure out how to build them faster. Comparing multiple connectomes could help explain disease pathology or between-species differences in brains—but right now, building just a single connectome is a huge endeavor. The most successful method involves imaging the top of a cube of brain, then slicing off a very thin layer with a diamond-tipped blade and imaging the next layer in the cube. The slices are destroyed in the process, so you only have one shot to get the images.
Mikula is one neuroscientist working to build connectomes faster. He's developing a “whole mouse brain on tape” with software that can randomly access different parts of the brain and image at different resolutions. Neuroscientists could target certain parts of the brain at lower resolution and target high-priority spots for high-res imaging later. Eventually, it might be possible to label different chemicals in the cells on the tape as well to get at the functional differences between different types of cells. Mikula may never get to a whole-brain simulation—but that won't stop him from trying.
The Connectome is a comprehensive diagram of all the neural connections existing in the brain. WIRED has challenged neuroscientist Bobby Kasthuri to explain this scientific concept to 5 different people; a 5 year-old, a 13 year-old, a college student, a neuroscience grad student and a connectome entrepreneur.
Fiery. Fierce. Sinus-clearing. Anyone who’s stood within wafting distance of a marathon knows the smell: Tiger Balm. The classic pain-relieving ointment can be traced to a Chinese herbalist who began selling it in the 1870s; subsequently commercialized by his sons, it’s been sold around the world ever since. The balm promises to “work where it hurts,” and many medical and sports professionals agree: A vigorous rub of the ointment on a sore muscle or bruised body part eases the aches. But unlike ibuprofen or aspirin, Tiger Balm doesn’t attack the source of the pain. It’s made of so-called counterirritants, substances that cause mild hot and cool sensations on your skin to distract your brain. In other words, Tiger Balm won’t pounce on your pain—but it will confuse and delight your nerve endings (and your nose) with tingle-inducing ingredients.
__Camphor __One of the muscle balm’s two active ingredients (meaning the FDA recognizes its medicinal properties), camphor reduces pain through distraction by activating your skin’s temperature sensors and tricking you into feeling cold. There’s only so much input the nervous system can handle in one location, so forcing the body to focus on the chills has the effect of masking the underlying pain.
MentholThe other active player, menthol, is an alcohol extracted from mint oil. Like camphor, it triggers your cold receptors, which might be why it reduces blood flow and swelling—just like an ice pack. Because it also seems to interact with opioid receptors, menthol may have painkilling effects beyond its powers of distraction and inflammation reduction.
Dementholized Mint OilThough it’s the byproduct of the menthol extraction process, this stuff still contains menthol—but it’s cheaper in this form. Plus, it’s 26 percent menthone (menthol with two fewer hydrogen atoms), which may help the other ingredients penetrate the skin.
Cajuput, Cassia, and Clove OilsMore neurological tricksters. These oils are believed to be counterirritants like menthol and camphor, except they can simulate heat as well as cold. The FDA isn’t convinced they can relieve pain, but clove oil is widely used as a natural analgesic.
Paraffin PetrolatumThe carrier for all the other ingredients, this is a mixture of a hard wax (paraffin) and a soft petroleum jelly, aka Vaseline. Both are crude oil derivatives made up of chains of carbon and hydrogen. But the chains in paraffin tend to be slightly longer, and the degree of attraction between these mol­ecules is proportional to the surface area. So the paraffin mol­ecules stick together and stay harder at room temperature, while the petrolatum spreads more easily. Together they make a semisolid that softens at just around body temperature—the purrrfect vehicle for delivering brightly burning tiger tingles.
This article appears in the March issue. Subscribe now.
If you're an environmentally conscious consumer, you probably own more than a few devices bedazzled with an Energy Star logo. Every laptop or dryer or refrigerator that meets the logo's energy efficiency expectations—established by a program within the Environmental Protection Agency—gives you a tiny planet-saving dopamine spike.
But that program could soon lose its federal sponsorship. Yesterday, energy industry wire service E&E News revealed that a draft EPA budget (leaked by an unnamed source) calls for shuttering the program. Energy Star's responsibility could be transferred to an outside group—possibly run by an inter-industry consortium. Tech companies would definitely score by controlling the specs behind the stamp, which eco-conscious consumers flock to as a sign of quality. Wait, what's that you say about conflict of interest? Insiders say the industry has been policing its environmental impact for a few years already, and things are going just fine.
Energy Star debuted in 1992, and the first products to earn the label were computers and desktop monitors. But the program has has grown far beyond consumer tech. The EPA and Department of Energy have developed standards (all optional) for refrigerators, HVAC units, swimming pool pumps, entire homes, and just about anything else that taps sufficient amps. And as more products in an industry meet its standards, Energy Star responds by raising the bar—triggering more innovations in efficiency. "The program's managers typically want to improve the program so it covers about 25 percent of a market," says Jennifer Amann, a program director at the American Council for an Energy Efficient Economy. Computers are already on version 6.1 of their Energy Star specifications.
Overall, that system has worked pretty well. On an individual level, buying an Energy Star compliant piece of tech probably won't shave more than a few dollars a year from your energy bill. But at the scales that people use personal electronics—TVs, laptops, smart phones, routers, etc—the savings are huge.
And the Energy Star brand does wonders for the companies selling their devices: In 2015, a Consortium for Energy Efficiency survey showed that brand recognition was around 90 percent in 2015; over 45 percent of the survey's respondents said they'd knowingly purchased an Energy Star product. Plus, the tech sector doesn't really mind it. "By being voluntary, market-oriented, and ultimately consumer-driven, it has had the flexibility to keep pace with the tech industry," says Doug Johnson, vice president of tech policy at the Consumer Technology Association.
So the tech industry probably doesn't want to lose Energy Star any more than the EPA does.
The leaked EPA document leaves the option for transferring the program to the industry. "I think we're open to discussing all of these issues with other Energy Star stakeholders, and the EPA," says Johnson. "Energy Star might be changed or transformed." For hints of what the transformation might entail, he points to the industry-developed voluntary agreements for set-top boxes initiated in 2012. Set-tops are covered under Energy Star, but manufacturers and retailers were worried that federal or state agencies might institute mandatory minimum efficiency requirements on their industry. So they proposed their own regulations as a precautionary measure. They had lower efficiency standards than the Energy Star agreement, but required much wider adoption.
That seems like it could be a reasonable trade off. But of course, a voluntary program could be less reliable in its assessment of products' actual energy efficiency. Under Energy Star, each product has to get tested by a third-party to make sure it meets spec. An industry-led Energy Star would require more investment from the companies themselves, to check their own work and police their competitors' compliance. And they'd have to keep those standards up to keep federal regulators out of their hair for good.
Then there's the matter of keeping consumers invested in the Energy Star brand. Buyers could be dissuaded by the perception that the foxes are guarding the greenhouse. "I can’t really specifically speak to whether government has more or less credibility than a third-party owner," says Sarah Griffith, communications director for the Consortium for Energy Efficiency, which promotes Energy Star to consumers. "We work with quite a few third parties, and they have their agendas just like we do. But, if they are fact-based, respectful, and work hard then they could be able to uphold consumer credibility."
Energy Star spans nearly 60 different product classes—14 of which fall in consumer and office technology. To make an industry-led Energy Star valuable for the companies, they'll need to maintain that consumer buy-in—and organize themselves just as well as the EPA has over the last quarter century. Imagine the thousands of companies across all of Energy Star's sectors coming together and agreeing, not only on a suitable structure to absorb Energy Star, but also to fund it. Currently, that Energy Star brand is subsidized by $57 million in taxpayer dollars. That's a tough sell.
Next week, I'll head off into the wilds of Arizona and Utah to visit some great geologic locales. Along with geoscience students from Denison University, I'll visit classics like the Grand Canyon and Zion—along with a place where only 1,000 years ago, some of the most recent volcanism in the lower 48 states occurred. It is a place where over 600 volcanic vents have been active over the past 6 million years. It is a place where lava flows dammed a river and sent molten rock up and down a canyon for over 10 miles. It is a place where Native Americans likely watched an eruption and made art with the lava that was pouring out over the land.
You can see all these sights at the San Francisco volcanic field, near Flagstaff, Arizona. There you'll find terrain coated with basalt lava flows and cinders, a composite volcano that is also the tallest peak in the state of Arizona, and some strange rhyolite domes that produced folded rocks and steep flows.
Now, those of you with an idea of what causes volcanic activity are possibly wondering: Why Arizona? It isn't near a subduction zone like the Cascade Range or Aleutians. It seems like a spot that shouldn't see much volcanism at all ... except that Arizona is on the edge of the Basin & Range province, where North America is stretching. Some of the much older volcanism in Arizona comes from this spreading, and the San Francisco volcanic field could be another piece of that puzzle.
But it isn't quite that simple. If you look at the composition of the basalt erupted at the San Francisco volcanic field, it looks much more like lava you might expect from Hawaii. This could mean that these volcanoes are fed by mantle plumes under the area—though there isn't a lot of evidence beyond the lava composition.
We find a lot of evidence that the basalt that is the source of the SFVF melted the crust under the volcanic field. The lavas are fill of xenoliths, or "foreign rocks". These samples of material from the crust vary from olivine-and-pyroxene-rich chunks that might come from the bottom of the crust all the way to chunks of limestone that come from near the surface.
So, what can you see when you visit the San Francisco volcanic field? Well, here's a sample of some of the places I'll get to see for the first time next week.
Merriam Crater and Grand Falls
Merriam Crater and some other nearby volcanic vents erupted ~20,000 years ago (based on exposure dating of the lava flows and paleomagnetism) and sent lava flows over 10 kilometers towards the Little Colorado River canyon. When it reached the canyon, the flows spilled down into the river and blocked it ... and then proceeded to flow up and down the canyon over 15 kilometers. Not only that, but the lava filled the canyon at Grand Falls and continued to flow another kilometer beyond the river. The channel of the Little Colorado River then rerouted around these lava flows (see above). I'm sure it would have been spetacular to see lava pouring into the 65-meter (~215 foot) deep canyon and then continue flowing down the canyon. An ephemeral lake likely formed after the eruption (although the Little Colorado is a very intermittent river) until the new channel was carved.
SP Crater
In the northern part of the San Francisco volcanic field lies SP Crater and its surrounding sea of volcanic vents. This area of volcanism has been steadily producing eruptions for over a million years—on average, one volcanic eruption every 15,000 years. A study by Conway and others in 1998 put a chance of an eruption in the area of SP Crater at ~13 percent over the next 1,000 years. SP Crater itself (above) is possibly 70,000 years old (although the date is not great and it looks younger than that age) and the youngest, called V4626, was only about 10-16,000 years ago. (And wonder where "SP Crater" got its name? Apparently a rancher in the late 1800's thought it looked like a, well, shit pot.)
Sunset Crater
Sunset Crater is the youngest known eruption in the San Francisco volcanic field. It erupted only about 1,000 years ago, when the ancestors to the Pueblo lived in the area. When the eruption occurred, a 10-kilometer fissure opened, creating a "curtain of fire" made from multiple lava fountains that also produced lava flows. Quickly the fissure eruptions coalesced into a single vent that became Sunset Crater. That vent continued to erupt, constructing a 300-meter-tall cinder cone made mostly of loose volcanic debris called "scoria".
Cinder cones are prone to breaches that let lava flows escape, and that is what happened at Sunset Crater. Two long lava flows come from the base of the cinder cone: Bonito and Kana'a. In the former, chunks of the cinder cone were carried hundreds of meters away from the cone during the eruption.
With people living in the area, you might expect that the eruption had some impact on their lives. There is archaeological evidence that people moved 30 kilometers to the north because much of the area near Sunset Crater was covered in coarse volcanic scoria. However, as you got further away, only volcanic ash fell, which actually helped the growth of crops. There are also "corn rocks" that have impressions of maize cobs and it is speculated that the Native Americans near the eruption may have intentionally put these cobs in/near the lava to make these!
We do know that the San Francisco volcanic field is still an active volcanic area, with the highest likelihood of a new eruption coming from somewhere in the eastern part of the volcano field. Any new eruption is likely to take the form of something like Sunset Crater—so probably spectacular, but not particularly dangerous to those living near the field.
There is a lot more to see across the San Francisco volcanic field. If you want to take a volcanic vacation, check out a field guide by Sarah Hanson that captures a lot of geology and history of these volcanic wonders. I'm excited to see these for the first time next week on the trip—be sure to watch on Twitter for pictures as we explore the active volcanoes of Arizona.
This video of Vietnamese police—I think they're Vietnamese police—climbing a wall with a pole left me thinking, "What the heck?" I don't doubt that it works, even if I don't think I'd try it. But how does it work?
The best way to look at the problem is to create a force diagram, also called a free body diagram. In physics, a free body diagram shows an object and all of the forces acting on that object. An arrow shows the direction and magnitude of each force. If an object is in equilibrium, all these forces must add up to zero (technically the zero vector). So let's say that the people climbing the wall move slowly enough that everything remains in equilibrium.
I'll start with the fellow climbing the wall. Here is a force diagram when the other guys are holding the pole at some arbitrary angle. In case you can't tell, I've drawn a circle to indicate the man on the wall.

There are essentially four forces on this wall climber. There is of course the gravitational force pulling down. Since this force depends on the mass of the object, I will write it as mg where m is the human mass and *g *is the gravitational field. The other three forces are contact forces from the wall, the pole and the wall again (as friction). Assuming the pole exerts a force only in the direction of the pole, I can break the forces into x and y components and write this as:

Here you can see that the pole pushes both horizontally and vertically with the amount of force dependent upon the angle of the pole. I can get an expression for the magnitude of this pole force, but I will first assume that the static friction pushing up is at its maximum value. This means that the magnitude of the friction force will be:

With this, I can eliminate the friction force from the two equations.

Now I can substitute for N and solve for the pole force.

If I use a human mass__ __of 70 kg and a coefficient of static friction at 0.7 then I can plot the pole-force vs. angle.
Notice that you need the greatest force as the guy on the wall starts his climb, when the pole angle is zero degrees. That's because you need the largest friction force because the pole pushes only in the horizontal direction. As the fellow climbs the wall, this pole force decreases because he doesn't require as much friction as the pole helps push him up. The pole force reaches its minimum at a bit more than 50 degrees, but as the angle increases the climber has less friction helping him out. At a pole angle of 90 degrees the pole only supports his weight.
But even at the start of his climb the two guys pushing him need only about 1000 Newtons (just a little bit more than the weight of the climber). So this is clearly feasible. Still, I offer one recommendation: Have the person with the lowest mass climb the wall. That will make everything easier.
NOAA's new GOES-16 satellite—the most advanced tool in the agency's weather-prediction arsenal—doesn't just take pretty pictures. It also might keep an unexpected bolt of lightning from crispifying you while you're barbecuing this summer. Or give you ample warning before a tornado scoops you up inside your house, Dorothy style.
See, before GOES-16 was launched last year, scientists didn't always have good way to check a storm's intensity or lightning risk. Especially if that storm was hanging out in the middle of the ocean, or even in a mountainous region like the western US, where the topography fouls up radar signals.
GOES-16 changes all that. Its lightning mapper is sensitive enough to see lightning brewing inside storm clouds over the Americas and their surrounding oceans. And that data should help climate scientists and weather forecasters warn people about weather hazards—from severe thunderstorms to tornadoes to wildfires—a lot faster and more accurately.
It’s a common sense scientific advancement, and one that meteorologists depend on—as they depend on other NOAA satellite data. Which is why it’s surprising that the program is the latest thing on President Trump’s budget chopping block.
According to an Office of Management and Budget memo* The Washington Post* got ahold of, the Trump administration wants to cut NOAA’s entire budget by 17 percent. The satellite program stands to lose 22 percent of its budget ($153 million) if the proposal goes through—the harshest cuts NOAA is facing. It’s hard not to see that move as political: Scientists use NOAA’s satellites, including GOES-16, to monitor how Earth’s climate continues to change, despite (debunked) claims from various government leaders that the rate of change has slowed. But if this is political payback, it’s pretty irresponsible. NOAA’s satellite data does a lot in the way of keeping citizens safe, and GOES-16 is the best of them.
Why is the lightning mapper so much better than your average storm chaser? It boils down to location and bandwidth. The lightning mapper (and the rest of GOES-16's instruments) are in geostationary orbit over the the Americas, while previous lightning-detection instruments have been in low Earth orbit. "Now we’re just staring at the same place, so we can monitor the trends and life cycle of the lightning," says Steve Goodman, GOES-16 senior scientist. "Before we just got snapshots." And since lightning's trends influence things like wildfires, figuring out its hotspots will be especially helpful in high risk (and bone dry) areas like California.

NOAA
Scientists are also working with about a 1,000 times more information than they had before. GOES-16 has a bandwidth of about 7.7 megabits per second—a big step up from its stodgy 8 kilobit per second predecessors. "When we flew over big storms, the old instruments would saturate," Goodman says. The sensors would poop out just when meterologists needed them most.
Now, monitoring intense storms is kind of the lightning mapper's specialty, and measuring flash rates fills in a longstanding meteorological blindspot. Radar can detect precipitation, but it can't tell you how intense a storm is. That's determined by updraft, which pulls raindrops way up high and bangs particles together until things start getting electric. No sensor can measure updraft strength, but NOAA has found a correlation between updraft and lightning activity.
"All of these things are connected," says Goodman. "If the storm is developing and becoming very intense, the in-cloud lightning activity, which we can now detect, starts having high flash rates." The rapid-fire flashes are hard evidence of a storm revving up its engines. Which is especially helpful if you're an aircraft more than about 300 miles off the coast, where land-based radar can't see you.
Like the rest of GOES-16, the lightning mapper instrument also produces some pretty striking imagery. "I can’t wait to see lightning dancing on cloud tops on television," Goodman says. "But not just because it's impressive. It’s going make warnings come sooner, and help people personalize the risk better." With fewer false alarms and more time to get inside, GOES-16 will help make sure you're not the one getting cooked at the next family barbecue.

NOAA
Today, in his first speech to his staff at the Department of Housing and Urban Development, newly minted Secretary Ben Carson delivered an extemporaneous disquisition on the unparalleled marvel that is the human brain and memory. "There is nothing in this universe that even begins to compare with the human brain and what it is capable of,” he began. “Billions and billions of neurons, hundreds of billions of interconnections.” It was a tangent in a speech about how in America, anything is possible.
It was also a speech that described slaves brought to the US as “immigrants.” That’s problematic, but Carson is not an expert on slavery. He is, however, an expert on brain-type things; before entering politics Carson was a distinguished neurosurgeon.
So it’s strange how wrong his understanding of memory seems to be.
Here’s what Carson said:
It remembers everything you've ever seen. Everything you've ever heard. I could take the oldest person here, make a hole right here on the side of the head, and put some depth electrodes into their hippocampus and stimulate, and they would be able to recite back to you verbatim a book they read 60 years ago. It's all there; it doesn't go away. You just have to learn how to recall it. But that's what your brain is capable of. It can process more than 2 million bits of information per second. You can't overload it. Have you ever heard people say, "Don't do all that, you'll overload your brain." You can't overload the human brain. If you learned one new fact every second, it would take you more than 3 million years to challenge the capacity of your brain.
This is all pretty wrong.
"It's utter nonsense," emailed Dan Simons, a psychologist at the University of Illinois who specializes in attention and memory. "We can't recall extended text verbatim unless we deliberately memorized it for that purpose (certainly not books we happened to read 60 years ago), you can't trigger accurate recall of detailed memories with an electrode (and long-term memories aren't stored in the hippocampus), we don't store a perfect and permanent record of our experiences (it's not all there just waiting to be probed), and you can't just 'learn how to recall it.'"
You already knew this. Sometimes you remember something important, and other times you don’t. Sometimes you remember things inaccurately. Two people can remember different things about the same experience.
But Carson is suggesting that the accurate memories are in there somewhere. “This is a hot button issue,” says Steve Ramirez, a neuroscientist at Harvard. “Do we form memories and do they always stick there? Or are there certain things that we forget and they're just kind of lost forever?”
The prevailing hypothesis is that the brain actually does both, encoding some things but not others. The ones that stick around tend to be the salient ones, the ones that hurt, that made an imprint. People don’t naturally store everything that happens because it takes energy to make a memory, so the brain seems to prioritize things with emotional color. The boring stuff like where you put your phone gets pared away.
The Cognitive Bias President Trump Understands Better Than You
The Strange Case of the Woman Who Can’t Remember Her Past—Or Imagine Her Future
Scientists Turn Bad Memories Into Good Inside the Brains of Mice
Yeah, but what about the electrode thing? “To say that you could do that assumes that you know where you could actually find those memories, which we don't know how to do,” Ramirez says. There are a lot of different kinds of memories—how to ride a bike versus your phone number versus where you were when you heard about 9/11—and while some of them hang out in a part of the brain called the hippocampus, the really permanent stuff seems not to rely on the hippocampus at all. That’s why people like Patient HM, a famous subject of memory research who had most of his hippocampus removed when he was a young man, could remember a lot of stuff about his childhood.
In a few cases, neurosurgeons have managed to get a patient to recall a memory by zapping the hippocampus with an electrode, but they can’t control it the way Carson suggests—if the memory of page 47 of Hamlet is even in there. “We're barely able to activate fear or pleasure in an animal, let alone one among the constellation of memories that humans actually have,” Ramirez says.
Even if scientists could do it, they might not be getting an accurate memory. “Memory isn't a videotape library. It's a dynamic, adaptive system,” says John Coley, a cognitive scientist at Northeastern. “The way we encode and recall memories changes them. It may be ‘in there,’ but unless you use it, it can go away, and the way you use it can change it.”
Which brings us to Carson’s third point. Can the brain be overloaded? It’s sort of a squishy question, but we can assume he’s talking about the brain running out of space to store information, or not being able to process new information. Here, Carson has a point. Cognitive scientists agree that long term memory has an effectively unlimited capacity, but the brain’s processing power is indeed finite.
The human brain tries to sort information precisely to avoid that kind of overloading. That’s great, until it isn’t. Cognitive biases, such as when people tend to remember things that are more specific or assume something is true because they’ve heard about it a lot, exist primarily to help your brain avoid having to compute too much.
In 60 years, you probably won’t remember this article. You might not remember Ben Carson. But remember this: The brain is a lot more complex than one speech has room for.
These days, more people are working from home, shopping from home, and yes, even seeing the doctor from home. Last year more than a million people traded the waiting room for the comfort of their own couch—which sure beats thumbing through a sad collection of creased magazines.
Today, telehealth is touted as one of the chief ways to deal with rural residents left behind by hospital consolidation, as well as the 20 million new patients the Affordable Care Act brought into the health care system. Its value hinges on the premise that patients will use telehealth options instead of going to the doctor or the urgent care clinic. But a new study released today shows that people are using phone-a-physician services in addition to in-person visits, not as a substitute. And the result of the Uber-ization of health care is an increase in overall costs.
In April 2012, CalPERS Blue Shield started covering telehealth visits for their 300,000 insurance enrollees. Over the next year and half, 2,943 of them came down with a respiratory infection. Two-thirds of those cough-stricken Californians went straight to the doctor. The other third picked up the phone first, using the newly covered direct-to-consumer service offered by a telehealth company called Teladoc.
This seemed like good news—using Teladoc brought down the cost of the average bronchitis episode because patients avoided unnecessary testing and imaging. But when researchers at RAND, a public policy thinktank, looked at whether those calls replaced in-person visits over those 18 months, they found that happened less than 12 percent of the time. In the long-term, spending actually went up $45 per Teladoc patient. They weren’t going to the doctor any less frequently. “If you make something easier to access, people will use it,” said Lori Uscher-Pines, one of the authors of RAND’s paper, published today in Health Affairs. “That lower threshold means that people are using this as an add-on service.”
Patients who use telehealth on top of their normal health care visits add strain to an already overburdened health care system. RAND found that patients who used Teladoc tended to be younger, healthier, tech-savvy city dwellers—not the rural and elderly populations the technology is supposed to be targeting. And because the service takes place outside of the normal health care flow, the physicians on the other end of the line don’t usually have access to each patient’s health records, and the visit may not make it into the patient’s history. Health care experts call this “fragmentation.”
“Telehealth has to be integrated fully into a total care system,” says Mario Gutierrez, executive director of the Center for Connected Health Policy. “It can’t just be a one-off. That’s not health care.” He’d like to see telehealth move away from the convenience economy model where you only dial up when you’re feeling down and out. Instead, he sees a huge opportunity to use it to manage chronic disease and engage people in preventive care. That means embracing telehealth as an essential service, not an add-on.
A few institutions have a jump on this. The US Veterans Administration has reduced hospital admissions by 20 percent and costs per patient by $1,600 each year with its telehealth program. In the world of private health care, Kaiser Permanente leads the pack; last year more than half of its 110 million patient interactions happened online or over the phone.
When patients call or set up a video consultation through Kaiser’s web portal, they get a few options. They can schedule a call or appointment with their primary physician (which could take a day or could take weeks), or they can talk to an on-call emergency room physician right away. If they choose that option, they might get Dennis Truong on the other end, an ER doc who also leads Kaiser’s telemedicine and mobility efforts in the mid-Atlantic region.
Truong can pull up patients' health histories, and he can easily transfer them to an urgent care clinic or a specialist within the Kaiser network. “An integrated system is the backbone of what telehealth should be for patients,” he says. “I can hand off their care to the next physician who sees them, whether that’s later today or a year from now. It closes the loop.”
Integration might be the gold standard, but not everyone in the telehealth industry is keeping up as well as Kaiser and the VA. In 2016 Teladoc recorded 952,081 virtual visits, up from 600,000 in 2015, and 300,000 the year before that. By 2020, the telehealth industry is estimated to be worth $34 billion. It could make a big dent in America’s overextended health care system, if providers and patients use it responsibly.
Truong, who trained in the emergency rooms of Detroit, spent his early days as a doctor treating people who treated the ER as their first and last stop for health care. Their records were incomplete, fractured. It made it hard to care for them. “We couldn’t get down to the meat and bones,” he says. “That’s how I feel about a lot of these companies. There’s no closing the loop.”
To do that, policy and technology can do a lot of the heavy lifting by providing coverage and incentives in the right places. But for telehealth to fully deliver on its promise, people have to start treating their health care less like an Uber you summon in a thunderstorm, and more like a car that has to carry you the next 500,000 miles.
Baxter is but a child, with bright eyes and a subtle grin. It sits at a table and cautiously lifts a can of spray paint, then dangles it over a box marked “WIRE.” The error seems to smack Baxter across the face—its eyebrows furrow and blush appears on its cheeks. It swings its arm to an adjacent box marked “PAINT” and drops in the can with a clunk and that spray-paint rattle.
“Good,” says a voice off-screen, as Baxter’s face reverts to a grin.
Baxter is in fact a robot, and an industrial one at that, with hulking arms meant for lifting much larger things than cans and wire. Its face is not flesh, but a screen. And its decisions are not entirely its own, but those of a human sitting across the table—a woman with electrodes strapped to her head. The setup detects a particular signal in her brain's electrical activity when she sees a mistake. In real time, the woman telepathically scolds Baxter for choosing the wrong box, and the robot corrects.
Researchers didn’t set out to embarrass an innocent machine, but to push further into the frontier of human-robot interaction, as they detail in a paper published online today. More and more, you’ll be interacting with machines: You’ll share hospital corridors with robots delivering food and medicine, and you could even fly a plane with your thoughts alone. For the time being, though, interacting with robots is crazy-awkward—they’re stilted and, well, robotic. The challenge now is socializing them.
Today, communicating with the machines is mostly about typing or vocalizing commands, which creates lag time. Letting Baxter read your mind takes milliseconds. “It's a new way of controlling the robot that I actually like to think of as being natural, in the sense that we aim to have the robot adapt to what the human would like to do,” says MIT roboticist Daniela Rus, a co-author on the study. Namely, don’t put the paint in the wrong box, dummy.
The underlying technology is shiny and new and complex, but the idea is straightforward. When you notice a mistake, your brain emits a faint type of signal, known in neuroscience as an error-related potential. But that’s among all the other electrical chaos coursing through your brain that an EEG picks up, so machine learning algorithms sniff out the signal. When Baxter is about to make a mistake, the system translates the error-related potentials in the woman’s brain into code a robot understands.
The human and machine are communicating at the most basic of levels—not speech but the electrical signals that prelude speech. “The paper shows an interesting capability in terms of doing this in real time,” says Carnegie Mellon roboticist Aaron Steinfeld. The researchers' machine learning algorithms are so powerful, they can sort the error-related potentials from the other electrical noise to immediately create something the robot can comprehend.
Now, you may have been hearing recently that a robot will one day steal your job. I can’t guarantee that’s untrue, but a world is coming in which robots work alongside humans. Imagine a robot assistant helping you assemble Ikea furniture. “The robot could actually be passing the human different pieces of the chair,” says roboticist and study co-author Stephanie Gil of MIT. “So maybe a chair leg or an arm rest. And the human is actually using his hands to put these different pieces together.”
But you shouldn’t have to constantly bark orders at your assistant, right? “We don't want to have to explicitly use verbal cues or a push of a button, something that's very unnatural for the human to communicate with the robot,” Gil adds. “We want this to be very natural and almost seamless.” And nothing is more seamless than a robot reading your mind.
This technology operates as a binary at the moment—Baxter only knows if it’s doing something wrong or something not wrong—but you can expect the range of communications to diversify as the technology matures. Detecting emotions, for example. “We're also very interested in the potential for using this idea in driving,” says Rus, “where you have passengers in an autonomous car and the passengers’ fears or brain signals—I mean this is getting futuristic—but the brain waves from the passengers get used by the car to adjust its own behavior.”
Backseat drivers, rejoice.
Astronomer Kevin Schawinski has spent much of his career studying how massive black holes shape galaxies. But he isn’t into dirty work—dealing with messy data—so he decided to figure out how neural networks could do it for him. Problem is, he and his cosmic colleagues suck at that sophisticated kind of coding.
That changed when another professor at Schawinski’s institution, ETH Zurich, sent him an email and CCed Ce Zhang, who actually is a computer scientist. “You guys should talk,” the email said. And they did: Together, they plotted how they could take leading-edge machine-learning techniques and superimpose them on the universe. And recently, they released their first result: a neural network that sharpens up blurry, noisy images from space. Kind of like those scenes in CSI-type shows where a character shouts “Enhance! Enhance!” at gas station security footage, and all of a sudden the perp’s face resolves before your eyes.
Schawinski and Zhang’s work is part of a larger automation trend in astronomy: Autodidactic machines can identify, classify, and—apparently—clean up their data better and faster than any humans. And soon, machine learning will be a standard digital tool astronomers can pull out, without even needing to grasp the backend.
In their initial research, Schawinski and Zhang came across a kind of neural net that, in an example, generated original pictures of cats after learning what “cat-ness” is from a set of feline images. “It immediately became clear,” says Schawinski.
This feline-friendly system was called a GAN, or generative adversarial network. It pits two machine-brains—each its own neural network—against each other. To train the system, they gave one of the brains a purposefully noisy, blurry image of a cat galaxy and then an unmarred version of that same galaxy. That network did its best to fix the degraded galaxy, making it match the pristine one. The second half of the network evaluated the differences between that fixed image and the originally OK one. In test mode, the GAN got a new set of scarred pictures and performed computational plastic surgery.
Once trained up, the GAN revealed details that telescopes weren’t sensitive enough to resolve, like star-forming spots. “I don’t want to use a cliché phrase like ‘holy grail,’” says Schawinski, “but in astronomy, you really want to take an image and make it better than it actually is.”
When I asked the two scientists, who Skyped me together on Friday, what’s next for their silicon brains, Schawinski asked Zhang, “How much can we reveal?” which suggests to me they plan to take over the world.
They went on to say, though, that they don’t exactly know, short-term (or at least they’re not telling). “Long-term, these machine learning techniques just become part of the arsenal scientists use,” says Schawinski, in a kind of ready-to-eat form. “Scientists shouldn’t have to be experts on deep learning and have all the arcane knowledge that only five people in the world can grapple with.”
Other astronomers have already used machine learning to do some of their work. A set of scientists at ETH Zurich, for example, used artificial intelligence to combat contamination in radio data. They trained a neural network to recognize and then mask the human-made radio interference that comes from satellites, airports, WiFi routers, microwaves, and malfunctioning electric blankets. Which is good, because the number of electronic devices will only increase, while black holes aren’t getting any brighter.
Neural networks need not limit themselves to new astronomical observations, though. Scientists have been dragging digital data from the sky for decades, and they can improve those old observations by plugging them into new pipelines. “With the same data people had before, we can learn more about the universe,” says Schawinski.
Machine learning also makes data less tedious to process. Much of astronomers’ work once involved the slog of searching for the same kinds of signals over and over—the blips of pulsars, the arms of galaxies, the spectra of star-forming regions—and figuring out how to automate that slogging. But when a machine learns, it figures out how to automate the slogging. The code itself decides that “galaxy type 16” exists and has spiral arms and then says, “Found another one!” As Alex Hocking, who developed one such system, put it, “the important thing about our algorithm is that we have not told the machine what to look for in the images, but instead taught it how to ‘see.’”
A prototype neural network that pulsar astronomers developed in 2012 found 85 percent of the pulsars in a test dataset; a 2016 system flags fast radio burst candidates as human- or space-made, and from a known source or from a mystery object. On the optical side, a computer brainweb called RobERt—Robotic Exoplanet Recognition—processes the chemical fingerprints in planetary systems, doing in seconds what once took scientists days or weeks. Even creepier, when the astronomers asked RobERt to “dream up” what water would look like, he, uh, did it.
The point, here, is that computers are better and faster at some parts of astronomy than astronomers are. And they will continue to change science, freeing up scientists’ time and wetware for more interesting problems than whether a signal is spurious or a galaxy is elliptical. “Artificial intelligence has broken into scientific research in a big way,” says Schawinski. “This is a beginning of an explosion. This is what excites me the most about this moment. We are witnessing and—a little bit—shaping the way we’re going to do scientific work in the future.”
Hi! How are you?
"Anxious," says Michael Neblo, a political scientist at Ohio State University.
"Undercut," says Karen Alter, a political scientist at Northwestern.
"Wonderful," says Donald Abelson, a political scientist at the University of Western Ontario.
"When am I going to wake up?" says Patrick Jackson, a political scientist at American University.
It's a weird time to be a scientist, with the funding cuts, political snubs, and colleagues going rogue. But when your scientific province actually is politics? The discombobulation goes up by an order of magnitude.
The crux of the issue is this: When the country's most pressing and partisan issues fall within your purview, can you advocate for change without seeming to lose objectivity? Sticky. And it just gets gluey-er. The scientific community isn't too sure the social sciences really "count," and academia in general enforces rigid and esoteric priorities. But a rising trend among political scientists has them taking their ideas outside the ivory tower, directly to citizens.
For sure, eggheads of any stripe still have a lot of ways to talk, mostly to each other—journals, conferences, the internet. And academia rewards staying in your lane. "Unless you're post-tenure, the way to career advancement is impressing your peers," Jackson says. "So you write the same article 12 times with minor modifications so you get cited a lot."
But that road also leads to decreasing relevance. The ivory tower has less and less audience (and influence). "Think tanks and NGOs have a greater ability to weigh in on policy, be less descriptive, and set out recommendations for what to do," Abelson says. "That's had a chilling effect on those in the academy who stay in their silos and do the hard core, abstract science." And the Trump administration famously rejects experts and data.
On the plus side, the current state of American politics (perhaps best characterized by "Yyaaaggghhh!") presents a bunch of opportunities for someone to explain what the hell is going on. "It's a wonderful time to relate theory to practice," Abelson says. (No wonder he's the only one we called who's in a good mood.)
So you get political scientists increasingly wading into active policy debates as talking heads, bloggers, and letter-writing campaigners, or developing tools for the public to speak back to the government. That's what Neblo did when he created a platform for online town hall meetings between politicians and their constituents.
As usual among academics, some people in the field are skeptical about colleagues going public with prescriptive tools and advice. It's either an ethical gray area or totally non-kosher. Then again, scientists are about to march on Washington. But political scientists feel the tension perhaps even more keenly than their colleagues over in the laboratory building. "Because a lot of other scientists don't take us seriously as a science, we want to be very careful," Neblo says.
That's right, this is an interdisciplinary scientific beef. And political scientists can't do much to change that. So increasingly some of them are embracing a new role—and are willing to risk their colleagues' derision if the other option is leaving the significance of their work unstated. "You can’t experiment your way to understanding politics. In the world, nothing is held constant," Alter says. "So doing that is mediocre social science."
It's a tricky balancing act. Having an entire field of researchers with their noses buried in arcane journals read only by their peers is a missed opportunity. But if those researchers dumb down their work, and tailor (or even skew) it to better suit what's trending, or what CNN or Fox News will cover, that's bad science. Then again, no science is even worse. No matter how political scientists are feeling about political science, they all agree on that.
On Saturday morning, the white stone buildings on UC Berkeley’s campus radiated with unfiltered sunshine. The sky was blue, the campanile was chiming. But instead of enjoying the beautiful day, 200 adults had willingly sardined themselves into a fluorescent-lit room in the bowels of Doe Library to rescue federal climate data.
Like similar groups across the country—in more than 20 cities—they believe that the Trump administration might want to disappear this data down a memory hole. So these hackers, scientists, and students are collecting it to save outside government servers.
But now they’re going even further. Groups like DataRefuge and the Environmental Data and Governance Initiative, which organized the Berkeley hackathon to collect data from NASA’s earth sciences programs and the Department of Energy, are doing more than archiving. Diehard coders are building robust systems to monitor ongoing changes to government websites. And they’re keeping track of what’s been removed—to learn exactly when the pruning began.
The data collection is methodical, mostly. About half the group immediately sets web crawlers on easily-copied government pages, sending their text to the Internet Archive, a digital library made up of hundreds of billions of snapshots of webpages. They tag more data-intensive projects—pages with lots of links, databases, and interactive graphics—for the other group. Called “baggers,” these coders write custom scripts to scrape complicated data sets from the sprawling, patched-together federal websites.
Rogue Scientists Race to Save Climate Data from Trump
Gun Violence Researchers Race to Protect Data From Trump
Scientists Prepare to Fight for Their Funding Under Trump
It’s not easy. “All these systems were written piecemeal over the course of 30 years. There’s no coherent philosophy to providing data on these websites,” says Daniel Roesler, chief technology officer at UtilityAPI and one of the volunteer guides for the Berkeley bagger group.
One coder who goes by Tek ran into a wall trying to download multi-satellite precipitation data from NASA’s Goddard Space Flight Center. Starting in August, access to Goddard Earth Science Data required a login. But with a bit of totally legal digging around the site (DataRefuge prohibits outright hacking), Tek found a buried link to the old FTP server. He clicked and started downloading. By the end of the day he had data for all of 2016 and some of 2015. It would take at least another 24 hours to finish.
The non-coders hit dead-ends too. Throughout the morning they racked up “404 Page not found” errors across NASA’s Earth Observing System website. And they more than once ran across empty databases, like the Global Change Data Center’s reports archive and one of NASA’s atmospheric CO2 datasets.
And this is where the real problem lies. They don't know when or why this data disappeared from the web (or if anyone backed it up first). Scientists who understand it better will have to go back and take a look. But meantime, DataRefuge and EDGI understand that they need to be monitoring those changes and deletions. That’s more work than a human could do.
So they’re building software that can do it automatically.
Later that afternoon, two dozen or so of the most advanced software builders gathered around whiteboards, sketching out tools they’ll need. They worked out filters to separate mundane updates from major shake-ups, and explored blockchain-like systems to build auditable ledgers of alterations. Basically it’s an issue of what engineers call version control—how do you know if something has changed? How do you know if you have the latest? How do you keep track of the old stuff?
There wasn’t enough time for anyone to start actually writing code, but a handful of volunteers signed on to build out tools. That’s where DataRefuge and EDGI organizers really envision their movement going—a vast decentralized network from all 50 states and Canada. Some volunteers can code tracking software from home. And others can simply archive a little bit every day.
By the end of the day, the group had collectively loaded 8,404 NASA and DOE webpages onto the Internet Archive, effectively covering the entirety of NASA’s earth science efforts. They’d also built backdoors in to download 25 gigabytes from 101 public datasets, and were expecting even more to come in as scripts on some of the larger datasets (like Tek’s) finished running. But even as they celebrated over pints of beer at a pub on Euclid Street, the mood was somber.
There was still so much work to do. “Climate change data is just the tip of the iceberg,” says Eric Kansa, an anthropologist who manages archaeological data archiving for the non-profit group Open Context. “There are a huge number of other datasets being threatened with cultural, historical, sociological information.” A panicked friend at the National Parks Service had tipped him off to a huge data portal that contains everything from park visitation stats to GIS boundaries to inventories of species. While he sat at the bar, his computer ran scripts to pull out a list of everything in the portal. When it’s done, he’ll start working his way through each quirky dataset.
UPDATE 5:00pm Eastern, 2/15/17: Phrasing in this story has been updated to clarify when changes were made to federal websites. Some data is missing, but it is still unclear when that data was removed.
The Proclaimers would walk 500 miles, and then they’d walk 500 more, just to be the type of people who would walk 1,000 miles. Which is admirable, though relatively tame compared to the antics of the albatross. This enormous bird rides the wind for up to 10,000 miles. Check out this week’s episode of Absurd Creatures to learn more!
Find every episode of Absurd Creatures here. And I’m happy to hear from you with suggestions on what to cover next. If it’s weird and we can find footage of it, it’s fair game. You can get me at matthew_simon@wired.com or on Twitter at @mrMattSimon.
Put a tray of water in the freezer. For a while, it's liquid. And then—boom—the molecules stack into little hexagons, and you’ve got ice. Pour supercold liquid nitrogen onto a wafer of yttrium barium copper oxide, and suddenly electricity flows through the compound with less resistance than beer down a college student’s throat. You've got a superconductor.
Those drastic alterations in physical properties are called phase transitions, and physicists love them. It's as if they could spot the exact instant Dr. Jekyll morphs into Mr. Hyde. If they could just figure out exactly how the upstanding doctor's body metabolized the secret formula, maybe physicists could understand how it turns him evil. Or make more Mr. Hydes.
A human physicist might never have the neural wetware to see a phase transition, but now computers can. In two papers published in Nature Physics today, two independent groups of physicists—one based at Canada’s Perimeter Institute, the other at the Swiss Federal Institute of Technology in Zurich—show that they can train neural networks to look at snapshots of just hundreds of atoms and figure out what phase of matter they're in.
And it works pretty much like Facebook's auto-tags. “We kind of repurposed the technology they use for image recognition,” says physicist Juan Carrasquilla, who co-authored the Canadian paper and now works for quantum computing company D-Wave.
Of course, facial recognition, water turning to ice, and Jekylls turning to Hydes aren't really the scientists' bag. They want to use artificial intelligence to understand fringey phenomena with potential commercial applications—like why some materials become superconductors only near absolute zero but others transition at a balmy -150 degrees Celsius. “The high-temperature superconductors that might be useful for technology, we actually understand them very poorly,” says physicist Sebastian Huber, who co-wrote the Swiss paper.
They also want to better understand exotic phases of matter called topological states, in which quantum particles act even weirder than usual. (The physicists who discovered these new phases nabbed the Nobel Prize last October.) Quantum particles like photons or atoms change their physical states relatively easily, but topological states are sturdy. That means they might be useful for building data storage for quantum computers, if you were a company like, say, Microsoft.
The research was about more than identifying phases—it was about understanding transitions. The Canadian group trained their computer to find the temperature at which a phase transition occurred to 0.3 percent accuracy. The Swiss group showed an even trickier move, because they got their neural network to understand something without training it ahead of time. Typically in machine learning, you give the neural network a goal: Figure out what a dog looks like. "You train the network with 100,000 pictures,” Huber says. “Whenever a dog is in one, you tell it. Whenever there isn’t, you tell it.”
But the physicists didn’t tell their network about phase transitions at all: They just showed the network collections of particles. The phases were different enough that the computer could identify each one. That's a level of skill acquisition that Huber thinks will eventually allow neural networks to discover entirely new phases of matter.
These new successes aren't just academic. In the hunt for stronger, cheaper, or otherwise better materials, researchers have been using machine learning for a while. In 2004, a collaboration that included NASA and GE developed a strong, durable alloy for aircraft engines using neural networks by simulating the materials before troubleshooting them in the lab. And machine learning is way faster than, say, simulating the properties of a material on a supercomputer.
Still, the phase transition simulations that the physicists studied were simple compared to the real world. Before these speculative materials end up in your new gadgets, the physicists will need to figure out how to make neural networks parse 1023 particles at a time—not just hundreds, but 100 sextillion. But Carrasquilla already wants to show real experimental data to his neural network, to see if it can find phase changes. The computer of the future might be smart enough to tag your grandma’s face in photos—and discover the next wonder material.
A very long time ago in a faraway galaxy, a star blew up. When the flash of light finally reached Earth on October 6, 2013, nobody noticed. Not at first. Three hours of supernova photons streamed by before an old telescope perched on a mountain north of San Diego started snapping pics.
The 48-inch Samuel Oschin Telescope is a 60-year old veteran of astronomical missions. Its current duty is to keep a watchout for transient astronomical events—things like gamma ray bursts, gravitational lensing, and supernovae. So when it spied the flicker in the sky, it sent an electronic all-points bulletin to other telescopes around the world, which were able to capture, among other things, the supernova's chemical signature.
The data astounded astronomers. This star, a red supergiant, erupted huge amounts of helium and hydrogen more than a year before fully exploding. That's not how red supergiants were supposed to blow up. And the implications of that discovery, published today in Nature Physics, changes what astronomers thought they knew about how spectacularly large stars die.
Run back the reel a few million years. The red supergiant (known posthumously as SN 2013fs) is about 10 to 15 times the size of Earth's sun, and it is running low on hydrogen and helium. But that doesn't mean the fusion in its core just stops. Instead, like other dying red supergiants, it keeps burning heavier and heavier elements. Eventually, the superhot alchemy leaves behind only iron. "You don't get any energy out of burning iron," says Sean Couch, a theoretical astrophysicist at Michigan State University. "It just gets heavier and heavier, and eventually collapses under its own weight."
Scientists' understanding of that chain reaction has lots of missing links. "Beyond the basics, we still don’t fully understand what drives a star to turn itself inside out," says Couch. Stars of different sizes explode differently. Red supergiants like SN 2013fs cause the most common type of core collapse, a Type IIp supernova, which scientists thought were pretty basic: one big flash and the star is gone. But the telescopes watching SN 2013fs picked up a strange spectral signature, one that is typically found when much larger stars explode.
Larger stars don't just blow up. They're so massive that they experience smaller eruptions prior to the big boom, building up stellar detritus that forms into disks around the star. Again, nobody knows exactly why this happens. But one theory is that when a star's core gets really hot and dense, pairs of electrons and positrons spontaneously form from the plasma. If you remember your 101 physics, positrons and electrons annihilate one another on contact. The core starts to oscillate, sending waves through the star, and eventually it contracts and sheds a huge cloud of particles.
Astronomers didn't think dying red supergiants could belch out star dust like that—but the data say otherwise. The central pieces of evidence come from spectroscopic images taken from a Hawaiian telescope about three hours after the Oschin scope sent out the bat signal. "Looking at this spectral data is like dissecting the light," says Couch. "We see different elements, and by looking at how the signatures move from image to image we can deduce a lot about the underlying features of the stuff that emitted the light." The spectral signature from SN 2013fs paints a fairly clear picture: The star's big explosion tore through a cloud of previously ejected material.
This data from the hours after the red supergiant's explosion offer a paradigm-changing glimpse into what triggered the earlier eruption of gas. For instance, it's possible that the contractions from the star's hardening core sent waves through its molten layers, eventually shocking a shell of thick gas from the surface. Whatever the explanation, it will force astronomers to rethink their theories about how stars evolve. That's pretty explosive.
Ask Adam Booth what gets his adrenaline pumping and he’ll tell you: pitching a tent in Antarctica at sunset. The British scientist battled the remote continent’s biting wind and blowing snow each night as he prepared his shelter on the Larsen C ice shelf. “We were essentially above the open ocean,” says Booth. “Three hundred meters of ice separated us and the sea.”
Along with a team of scientists, Booth spent five weeks on Larsen C in November 2015. They were there to study a rift in the ice shelf, a mass of floating ice the size of Vermont and New Hampshire. Every few days, they set up camp in a different location, drilling 100-meter boreholes with hot water, towing a radar system behind a snowmobile, and analyzing seismic activity. Pairing that field data with satellite imagery, the team—called Project Midas—modeled the internal structure of the ice, which they used to predict the behavior of the shelf.
Now, Booth’s adrenaline is pumping for a different reason. While the rift was about 18 miles long during his 2015 visit, it is now over 100 miles long. In just the last two months, it grew by 17 miles—suggesting that it will calve any day now, forming an iceberg the size of Delaware. Only 20 miles hold the ice together. The scientists are keeping a close eye on it, because the iceberg might finally reveal if their model is right—and how much sea level rise the world could contend with as a result.
Why It’s Impossible to Predict When That Giant Antarctic Ice Sheet Will Split
Let’s Put the Water Back on Top of Antarctica
How Climate Change Became a National Security Problem
The nearby Larsen A and B ice shelves collapsed in 1995 and 2002, respectively, and the glaciers held back by Larsen B have flowed faster into the ocean ever since. But when the Larsen C berg breaks off, it’ll be the first time scientists will be able to monitor a break in near-real time with satellite imagery. “This is completely different from earlier break-ups,” writes Peter Kuipers Munneke, a glaciologist at Utrecht University in the Netherlands who worked with Booth on Project Midas. “We had to rely on twice-monthly photos from space.” Now, they can monitor images every few days.
Just about everything scientists know about how fast ice is flowing comes from satellite observations in space. “You want your model to reflect what’s happening now so you can anticipate how the ice will change in the future,” says Mark Fahnestock, a glaciologist at the University of Alaska Fairbanks. So in December, Fahnestock helped launch the Global Land Ice Velocity Extraction (GoLIVE) project, a free data platform of imagery from NASA’s Landsat 8 satellite.
So far, GoLIVE has catalogued over 500,000 satellite-derived ice flow maps, adding thousands more each month. And later this year, Fahnestock will add data from the ESA’s Sentinel-2 satellite, potentially doubling the number of available images of Antarctica.
All that new satellite data means scientists will be able to check and improve on their models of Larsen C’s long-term stability. One model, based on fracture mechanics, focuses on the physical forces that can cause cracks to grow and spread, making the ice more unstable. Continuum mechanics—which is what the Midas model is based on—instead considers how the ice changes as a result of forces that twist, turn, and stretch the ice.When the Delaware-sized portion of Larsen C breaks off, it won’t immediately impact the stability of the rest of the ice shelf. But it will leave new sections of ice vulnerable to the open ocean—and the two model categories predict very different kinds of behavior for the remaining ice. Some models using fracture mechanics suggest that the rest of Larsen C could become unstable and break apart, while models using continuum mechanics generally say it will remain intact. “You get opposing results, depending on the angle you look at the ice,” writes Kuipers Munneke. “We urgently need a consensus.”
Here’s why that consensus is so urgent: Ice shelves hold glacier ice back, buttressing their movement. When an ice shelf breaks up, it’s “like removing the cork out of the bottle,” says Booth. Ice shelves themselves don’t contribute to sea level rise, because they’re already floating in the ocean. But the glaciers they hold back could cause four inches of sea level rise.
If continuum mechanics-based models like Project Midas' are right, the rest of Larsen C will remain intact for now, and the world can worry about the threat of glacier melt another day. So as the Delaware-sized berg breaks off in the coming weeks or months, Booth and his colleagues will keep a careful eye on the satellite imagery streaming in. Because the reality of sea level rise could be much scarier than pitching a tent in Antarctica.
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.
What’s the difference between physics and biology? Take a golf ball and a cannonball and drop them off the Tower of Pisa. The laws of physics allow you to predict their trajectories pretty much as accurately as you could wish for.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Now do the same experiment again, but replace the cannonball with a pigeon.
Biological systems don’t defy physical laws, of course—but neither do they seem to be predicted by them. In contrast, they are goal-directed: survive and reproduce. We can say that they have a purpose—or what philosophers have traditionally called a teleology—that guides their behavior.
By the same token, physics now lets us predict, starting from the state of the universe a billionth of a second after the Big Bang, what it looks like today. But no one imagines that the appearance of the first primitive cells on Earth led predictably to the human race. Laws do not, it seems, dictate the course of evolution.
The teleology and historical contingency of biology, said the evolutionary biologist Ernst Mayr, make it unique among the sciences. Both of these features stem from perhaps biology’s only general guiding principle: evolution. It depends on chance and randomness, but natural selection gives it the appearance of intention and purpose. Animals are drawn to water not by some magnetic attraction, but because of their instinct, their intention, to survive. Legs serve the purpose of, among other things, taking us to the water.
Mayr claimed that these features make biology exceptional—a law unto itself. But recent developments in nonequilibrium physics, complex systems science and information theory are challenging that view.
Once we regard living things as agents performing a computation—collecting and storing information about an unpredictable environment—capacities and considerations such as replication, adaptation, agency, purpose and meaning can be understood as arising not from evolutionary improvisation, but as inevitable corollaries of physical laws. In other words, there appears to be a kind of physics of things doing stuff, and evolving to do stuff. Meaning and intention—thought to be the defining characteristics of living systems—may then emerge naturally through the laws of thermodynamics and statistical mechanics.
This past November, physicists, mathematicians and computer scientists came together with evolutionary and molecular biologists to talk—and sometimes argue—about these ideas at a workshop at the Santa Fe Institute in New Mexico, the mecca for the science of “complex systems.” They asked: Just how special (or not) is biology?
It’s hardly surprising that there was no consensus. But one message that emerged very clearly was that, if there’s a kind of physics behind biological teleology and agency, it has something to do with the same concept that seems to have become installed at the heart of fundamental physics itself: information.

The first attempt to bring information and intention into the laws of thermodynamics came in the middle of the 19th century, when statistical mechanics was being invented by the Scottish scientist James Clerk Maxwell. Maxwell showed how introducing these two ingredients seemed to make it possible to do things that thermodynamics proclaimed impossible.
Maxwell had already shown how the predictable and reliable mathematical relationships between the properties of a gas—pressure, volume and temperature—could be derived from the random and unknowable motions of countless molecules jiggling frantically with thermal energy. In other words, thermodynamics—the new science of heat flow, which united large-scale properties of matter like pressure and temperature—was the outcome of statistical mechanics on the microscopic scale of molecules and atoms.
According to thermodynamics, the capacity to extract useful work from the energy resources of the universe is always diminishing. Pockets of energy are declining, concentrations of heat are being smoothed away. In every physical process, some energy is inevitably dissipated as useless heat, lost among the random motions of molecules. This randomness is equated with the thermodynamic quantity called entropy—a measurement of disorder—which is always increasing. That is the second law of thermodynamics. Eventually all the universe will be reduced to a uniform, boring jumble: a state of equilibrium, wherein entropy is maximized and nothing meaningful will ever happen again.
Are we really doomed to that dreary fate? Maxwell was reluctant to believe it, and in 1867 he set out to, as he put it, “pick a hole” in the second law. His aim was to start with a disordered box of randomly jiggling molecules, then separate the fast molecules from the slow ones, reducing entropy in the process.
Imagine some little creature—the physicist William Thomson later called it, rather to Maxwell’s dismay, a demon—that can see each individual molecule in the box. The demon separates the box into two compartments, with a sliding door in the wall between them. Every time he sees a particularly energetic molecule approaching the door from the right-hand compartment, he opens it to let it through. And every time a slow, “cold” molecule approaches from the left, he lets that through, too. Eventually, he has a compartment of cold gas on the right and hot gas on the left: a heat reservoir that can be tapped to do work.
This is only possible for two reasons. First, the demon has more information than we do: It can see all of the molecules individually, rather than just statistical averages. And second, it has intention: a plan to separate the hot from the cold. By exploiting its knowledge with intent, it can defy the laws of thermodynamics.
At least, so it seemed. It took a hundred years to understand why Maxwell’s demon can’t in fact defeat the second law and avert the inexorable slide toward deathly, universal equilibrium. And the reason shows that there is a deep connection between thermodynamics and the processing of information—or in other words, computation. The German-American physicist Rolf Landauer showed that even if the demon can gather information and move the (frictionless) door at no energy cost, a penalty must eventually be paid. Because it can’t have unlimited memory of every molecular motion, it must occasionally wipe its memory clean—forget what it has seen and start again—before it can continue harvesting energy. This act of information erasure has an unavoidable price: It dissipates energy, and therefore increases entropy. All the gains against the second law made by the demon’s nifty handiwork are canceled by “Landauer’s limit”: the finite cost of information erasure (or more generally, of converting information from one form to another).
Living organisms seem rather like Maxwell’s demon. Whereas a beaker full of reacting chemicals will eventually expend its energy and fall into boring stasis and equilibrium, living systems have collectively been avoiding the lifeless equilibrium state since the origin of life about three and a half billion years ago. They harvest energy from their surroundings to sustain this nonequilibrium state, and they do it with “intention.” Even simple bacteria move with “purpose” toward sources of heat and nutrition. In his 1944 book What is Life?, the physicist Erwin Schrödinger expressed this by saying that living organisms feed on “negative entropy.”
They achieve it, Schrödinger said, by capturing and storing information. Some of that information is encoded in their genes and passed on from one generation to the next: a set of instructions for reaping negative entropy. Schrödinger didn’t know where the information is kept or how it is encoded, but his intuition that it is written into what he called an “aperiodic crystal” inspired Francis Crick, himself trained as a physicist, and James Watson when in 1953 they figured out how genetic information can be encoded in the molecular structure of the DNA molecule.
A genome, then, is at least in part a record of the useful knowledge that has enabled an organism’s ancestors—right back to the distant past—to survive on our planet. According to David Wolpert, a mathematician and physicist at the Santa Fe Institute who convened the recent workshop, and his colleague Artemy Kolchinsky, the key point is that well-adapted organisms are correlated with that environment. If a bacterium swims dependably toward the left or the right when there is a food source in that direction, it is better adapted, and will flourish more, than one that swims in random directions and so only finds the food by chance. A correlation between the state of the organism and that of its environment implies that they share information in common. Wolpert and Kolchinsky say that it’s this information that helps the organism stay out of equilibrium—because, like Maxwell’s demon, it can then tailor its behavior to extract work from fluctuations in its surroundings. If it did not acquire this information, the organism would gradually revert to equilibrium: It would die.
Looked at this way, life can be considered as a computation that aims to optimize the storage and use of meaningful information. And life turns out to be extremely good at it. Landauer’s resolution of the conundrum of Maxwell’s demon set an absolute lower limit on the amount of energy a finite-memory computation requires: namely, the energetic cost of forgetting. The best computers today are far, far more wasteful of energy than that, typically consuming and dissipating more than a million times more. But according to Wolpert, “a very conservative estimate of the thermodynamic efficiency of the total computation done by a cell is that it is only 10 or so times more than the Landauer limit.”
The implication, he said, is that “natural selection has been hugely concerned with minimizing the thermodynamic cost of computation. It will do all it can to reduce the total amount of computation a cell must perform.” In other words, biology (possibly excepting ourselves) seems to take great care not to overthink the problem of survival. This issue of the costs and benefits of computing one’s way through life, he said, has been largely overlooked in biology so far.

So living organisms can be regarded as entities that attune to their environment by using information to harvest energy and evade equilibrium. Sure, it’s a bit of a mouthful. But notice that it said nothing about genes and evolution, on which Mayr, like many biologists, assumed that biological intention and purpose depend.
How far can this picture then take us? Genes honed by natural selection are undoubtedly central to biology. But could it be that evolution by natural selection is itself just a particular case of a more general imperative toward function and apparent purpose that exists in the purely physical universe? It is starting to look that way.
Adaptation has long been seen as the hallmark of Darwinian evolution. But Jeremy England at the Massachusetts Institute of Technology has argued that adaptation to the environment can happen even in complex nonliving systems.
Adaptation here has a more specific meaning than the usual Darwinian picture of an organism well-equipped for survival. One difficulty with the Darwinian view is that there’s no way of defining a well-adapted organism except in retrospect. The “fittest” are those that turned out to be better at survival and replication, but you can’t predict what fitness entails. Whales and plankton are well-adapted to marine life, but in ways that bear little obvious relation to one another.
England’s definition of “adaptation” is closer to Schrödinger’s, and indeed to Maxwell’s: A well-adapted entity can absorb energy efficiently from an unpredictable, fluctuating environment. It is like the person who keeps his footing on a pitching ship while others fall over because she’s better at adjusting to the fluctuations of the deck. Using the concepts and methods of statistical mechanics in a nonequilibrium setting, England and his colleagues argue that these well-adapted systems are the ones that absorb and dissipate the energy of the environment, generating entropy in the process.
Complex systems tend to settle into these well-adapted states with surprising ease, said England: “Thermally fluctuating matter often gets spontaneously beaten into shapes that are good at absorbing work from the time-varying environment”.
There is nothing in this process that involves the gradual accommodation to the surroundings through the Darwinian mechanisms of replication, mutation and inheritance of traits. There’s no replication at all. “What is exciting about this is that it means that when we give a physical account of the origins of some of the adapted-looking structures we see, they don’t necessarily have to have had parents in the usual biological sense,” said England. “You can explain evolutionary adaptation using thermodynamics, even in intriguing cases where there are no self-replicators and Darwinian logic breaks down”—so long as the system in question is complex, versatile and sensitive enough to respond to fluctuations in its environment.
But neither is there any conflict between physical and Darwinian adaptation. In fact, the latter can be seen as a particular case of the former. If replication is present, then natural selection becomes the route by which systems acquire the ability to absorb work—Schrödinger’s negative entropy—from the environment. Self-replication is, in fact, an especially good mechanism for stabilizing complex systems, and so it’s no surprise that this is what biology uses. But in the nonliving world where replication doesn’t usually happen, the well-adapted dissipative structures tend to be ones that are highly organized, like sand ripples and dunes crystallizing from the random dance of windblown sand. Looked at this way, Darwinian evolution can be regarded as a specific instance of a more general physical principle governing nonequilibrium systems.

This picture of complex structures adapting to a fluctuating environment allows us also to deduce something about how these structures store information. In short, so long as such structures—whether living or not—are compelled to use the available energy efficiently, they are likely to become “prediction machines.”
It’s almost a defining characteristic of life that biological systems change their state in response to some driving signal from the environment. Something happens; you respond. Plants grow toward the light; they produce toxins in response to pathogens. These environmental signals are typically unpredictable, but living systems learn from experience, storing up information about their environment and using it to guide future behavior. (Genes, in this picture, just give you the basic, general-purpose essentials.)
Prediction isn’t optional, though. According to the work of Susanne Still at the University of Hawaii, Gavin Crooks, formerly at the Lawrence Berkeley National Laboratory in California, and their colleagues, predicting the future seems to be essential for any energy-efficient system in a random, fluctuating environment.
There’s a thermodynamic cost to storing information about the past that has no predictive value for the future, Still and colleagues show. To be maximally efficient, a system has to be selective. If it indiscriminately remembers everything that happened, it incurs a large energy cost. On the other hand, if it doesn’t bother storing any information about its environment at all, it will be constantly struggling to cope with the unexpected. “A thermodynamically optimal machine must balance memory against prediction by minimizing its nostalgia—the useless information about the past,’’ said a co-author, David Sivak, now at Simon Fraser University in Burnaby, British Columbia. In short, it must become good at harvesting meaningful information—that which is likely to be useful for future survival.
You’d expect natural selection to favor organisms that use energy efficiently. But even individual biomolecular devices like the pumps and motors in our cells should, in some important way, learn from the past to anticipate the future. To acquire their remarkable efficiency, Still said, these devices must “implicitly construct concise representations of the world they have encountered so far, enabling them to anticipate what’s to come.”

Even if some of these basic information-processing features of living systems are already prompted, in the absence of evolution or replication, by nonequilibrium thermodynamics, you might imagine that more complex traits—tool use, say, or social cooperation—must be supplied by evolution.
Well, don’t count on it. These behaviors, commonly thought to be the exclusive domain of the highly advanced evolutionary niche that includes primates and birds, can be mimicked in a simple model consisting of a system of interacting particles. The trick is that the system is guided by a constraint: It acts in a way that maximizes the amount of entropy (in this case, defined in terms of the different possible paths the particles could take) it generates within a given timespan.
Entropy maximization has long been thought to be a trait of nonequilibrium systems. But the system in this model obeys a rule that lets it maximize entropy over a fixed time window that stretches into the future. In other words, it has foresight. In effect, the model looks at all the paths the particles could take and compels them to adopt the path that produces the greatest entropy. Crudely speaking, this tends to be the path that keeps open the largest number of options for how the particles might move subsequently.
You might say that the system of particles experiences a kind of urge to preserve freedom of future action, and that this urge guides its behavior at any moment. The researchers who developed the model—Alexander Wissner-Gross at Harvard University and Cameron Freer, a mathematician at the Massachusetts Institute of Technology—call this a “causal entropic force.” In computer simulations of configurations of disk-shaped particles moving around in particular settings, this force creates outcomes that are eerily suggestive of intelligence.
In one case, a large disk was able to “use” a small disk to extract a second small disk from a narrow tube—a process that looked like tool use. Freeing the disk increased the entropy of the system. In another example, two disks in separate compartments synchronized their behavior to pull a larger disk down so that they could interact with it, giving the appearance of social cooperation.
Of course, these simple interacting agents get the benefit of a glimpse into the future. Life, as a general rule, does not. So how relevant is this for biology? That’s not clear, although Wissner-Gross said that he is now working to establish “a practical, biologically plausible, mechanism for causal entropic forces.” In the meantime, he thinks that the approach could have practical spinoffs, offering a shortcut to artificial intelligence. “I predict that a faster way to achieve it will be to discover such behavior first and then work backward from the physical principles and constraints, rather than working forward from particular calculation or prediction techniques,” he said. In other words, first find a system that does what you want it to do and then figure out how it does it.
Aging, too, has conventionally been seen as a trait dictated by evolution. Organisms have a lifespan that creates opportunities to reproduce, the story goes, without inhibiting the survival prospects of offspring by the parents sticking around too long and competing for resources. That seems surely to be part of the story, but Hildegard Meyer-Ortmanns, a physicist at Jacobs University in Bremen, Germany, thinks that ultimately aging is a physical process, not a biological one, governed by the thermodynamics of information.
It’s certainly not simply a matter of things wearing out. “Most of the soft material we are made of is renewed before it has the chance to age,” Meyer-Ortmanns said. But this renewal process isn’t perfect. The thermodynamics of information copying dictates that there must be a trade-off between precision and energy. An organism has a finite supply of energy, so errors necessarily accumulate over time. The organism then has to spend an increasingly large amount of energy to repair these errors. The renewal process eventually yields copies too flawed to function properly; death follows.
Empirical evidence seems to bear that out. It has been long known that cultured human cells seem able to replicate no more than 40 to 60 times (called the Hayflick limit) before they stop and become senescent. And recent observations of human longevity have suggested that there may be some fundamental reason why humans can’t survive much beyond age 100.
There’s a corollary to this apparent urge for energy-efficient, organized, predictive systems to appear in a fluctuating nonequilibrium environment. We ourselves are such a system, as are all our ancestors back to the first primitive cell. And nonequilibrium thermodynamics seems to be telling us that this is just what matter does under such circumstances. In other words, the appearance of life on a planet like the early Earth, imbued with energy sources such as sunlight and volcanic activity that keep things churning out of equilibrium, starts to seem not an extremely unlikely event, as many scientists have assumed, but virtually inevitable. In 2006, Eric Smith and the late Harold Morowitz at the Santa Fe Institute argued that the thermodynamics of nonequilibrium systems makes the emergence of organized, complex systems much more likely on a prebiotic Earth far from equilibrium than it would be if the raw chemical ingredients were just sitting in a “warm little pond” (as Charles Darwin put it) stewing gently.
In the decade since that argument was first made, researchers have added detail and insight to the analysis. Those qualities that Ernst Mayr thought essential to biology—meaning and intention—may emerge as a natural consequence of statistics and thermodynamics. And those general properties may in turn lead naturally to something like life.
At the same time, astronomers have shown us just how many worlds there are—by some estimates stretching into the billions—orbiting other stars in our galaxy. Many are far from equilibrium, and at least a few are Earth-like. And the same rules are surely playing out there, too.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Moon mission astronauts spent a lot of time on their butts. Those moon walks may have looked bouncy and peaceful, but cumbersome space suits had Apollo astronauts tripping and falling all over the place. And while it might seem silly—especially the aftermath, which looks like a kid trying to stand up in a bulky snowsuit—it's actually pretty dangerous for one of those suits to meet a shard of lunar regolith. That goes double for astronauts exploring Mars, where stronger gravity would mean falling twice as hard onto rocky ground far less forgiving than moon dust.
So to make sure that doesn't happen, Alison Gibson, graduate researcher at MIT's Man Vehicle Lab, is testing some newfangled space boots.
Since NASA isn't doing manned moon missions at the moment, astronauts wear soft boots that aren't made for walking at all—just floating around outside the International Space Station. Those ones are totally rigid below the knee. But even the relatively bendy moon boots won't cut it anymore. Spacesuit helmets cut astronauts' peripheral vision to basically nil, and pressurized boots are no good at feeling for obstacles. So to stay upright, astronauts spend a lot of time looking down at the ground, which isn't super conducive to science.
But how do you design a system that lets astronauts keep their heads up? "Having tactile cues on you feet is pretty intuitive," Gibson says. "On Earth that's what give us a clue to look down." That's why she designed each boot with two vibrating motors to buzz your big toe as you approach an obstacle. The vibrations ramp up as the boot gets closer and closer, and little orange dots on the boot's visual display become more opaque. And they're pretty maneuverable: Thanks to cheap 3-D printed bases, the boots weigh only a pound each.
Gibson collected motion capture data from 16 participants who strapped on the prototypes, and those tests suggest people spend significantly less time heads-down while wearing the boots. Oh, and they fall less, too. And while she emphasizes that these boots are still prototypes—to qualify for flight, they'd need souped-up sensors and materials that don't give off gases that could mess with the space suit—Gibson is optimistic. "NASA wants totally new EVA suits for Mars," she says, "and I think these could be incorporated into that space suit eventually."
That's a long way off—NASA doesn't expect to even put humans in orbit around Mars until the 2030s—but these boots are exactly the kind of Occam's Razor you need in space. "The astronaut teams on Mars are going to have to be relatively autonomous, since they're not going to have real-time communication with the ground," Gibson says. Whatever the obstacle avoidance tech ends up being, it needs to be simple, robust, and reliable. There's not much NASA can do if an astronaut sends an 'I've fallen and I can't get up' message.
I am super pumped up about the Iron Fist series that will premiere on Netflix soon. At this point, all I really have is this trailer—but that has never stopped me before. Why would it stop me now?
If you don't know anything about Iron Fist, let me say one important thing. Other than being a martial arts guy, he also has the ability to make this superpowered punch. (That's what's going on in the trailer when his hand gets all glowing and stuff.) But how much energy is in his power punch? To answer this question, I'll analyze the scene right at the end of the trailer, where Iron Fist punches the floor and sends at least two humans flying into the air.
Here's the important part for our analysis:
I'm going to make the following estimations and assumptions.
I can find the energy in the punch by using the work-energy principle. This says that if there are no external inputs into a system, the change in energy is zero. For this case, I will use a system consisting of Iron Fist, the baddies, the other debris, and the Earth.
Next I need to pick a starting and ending time to compare energies. I will draw this as a diagram.

In this system, I essentially have just two types of energy to deal with. First, there is the stored energy in the iron fist (the punch, not the person). I guess this would be some type of potential energy, but I'm not exactly sure how the punch works. Second, there is a change in gravitational potential energy. If the starting center of mass of the humans is set at zero potential energy, then the height they rise will give me the total change in gravitational potential energy.
I can write this energy conservation as:

It's really that simple. The energy from the punch goes into the change in potential energy for all the stuff moving up. All I need to do now is put in my values for the mass of the stuff and the height. This gives an estimated energy in the punch with a value of 3,920 Joules. That might seem like a lot, but you probably have 30,000 Joules stored in your smartphone battery. A ham and cheese sandwich has about 1.5 million Joules.
It's not enough energy for a phone, but 3,920 Joules is a large value for a human punch. What about the power? The power is defined as the rate of energy change.

For the iron fist punch, I know the energy already. But what about the time? I can look at the time it takes the Iron Fist to perform this super punch and then calculate the power. Using Tracker Video Analysis (just to get the time), I get a punch time of 0.417 seconds. I'm assuming he builds up all the energy during the swinging motion (but of course that is pure speculation). This would give a power of 9,400 Watts. That is some serious power. Just for comparison, an MMA fighter might have a punch energy of 25 Joules with a power of about 50 Watts.
I can't leave you without homework questions. Here you go.
Two years ago, Diane Hoffman-Kim grew her first brain ball. She started by dropping a few mouse nerve cells into a special nonstick petri dish, and with nothing but each other to hold on to, the cells grew into a sphere less than a millimeter wide: a mini-brain. The biological engineer has since reared thousands more of these organoids, with neurons that spark with lively electrical activity. Except … they still aren't quite alive. Without blood flow of their own, they can't survive without careful monitoring.
Then, last year, one of Hoffman-Kim’s grad students noticed something no one had ever seen before: Her brain balls were spontaneously growing blood vessels.
That tangle of hollow tubes marks the beginnings of a basic circulatory system. “They’re really just newbies,” Hoffman-Kim says. Her brains still can't pump their own blood—they'd need hearts for that—but that isn't stopping Hoffman-Kim from trying to edge them closer to self-sustaining life. She's working with a colleague at Brown to rig up her mini-brains to a mini-circulation source: rows and rows of brain balls sitting on chips, all plugged into a microfluidic motherboard.
In the last five years, researchers have engineered lots of dish-dwelling micro-organs, from itsy bitsy intestines to Lilliputian livers. They've simultaneously made major advances in biochips: small, Flash-drive-sized structures lined with a layer or two of cells and studded with biosensors and microfluidic channels. Those two-dimensional chips are useful for testing, say, how lung cells react to a piped-in toxin, but they're too simplistic to truly mimic organs. That's where organoids like Hoffman-Kim’s brain balls come in. For the first time, 2-D biochips are colliding with 3-D mini-organs—and together they're making some of the best organ simulations ever.
Using these mashups, the idea is that scientists will be able to take a few of your skin cells, grow miniature versions of all your major organs, and put them on a chip. Then doctors can test out the best compounds for whatever disease you might have—not in a mouse, but in a mini-you. “This will enable a new era of personalized medicine,” says Ali Khademhosseini, a bioengineer at Harvard’s Wyss Institute who has been working on both mini-organs and biochips for the last decade.
In a paper that will be published later this month, Khademhosseini's team created a series of chips connecting liver organoids and cancer cells with loops of tiny tubes. They pumped an anticancer drug through the system, tracking whether it killed the tumor cells and whether the liver cells could survive the toxic onslaught. That way, they could optimize a drug dosage that maxed cancer-killing power while keeping the liver out of harm’s way.
This new kind of drug-testing system could make it faster and cheaper to develop new therapeutics. Darpa has been a big funder of this line of research, especially as it aims at treatments for nuclear or biological weapons that are difficult to test in humans. And it could mean the end of animal test subjects; currently, all new drugs must be tested for toxicity on animals before the developer can apply for a human trial. That’s especially great news for diseases that only hit humans, where animal models aren’t that useful in the first place.
Take enteroviruses. Each year they cause over 10 million nasty infections—they’re particularly deadly for newborns—but none of their 71 strains naturally infect mice or rats. “If you think about it, most everything we know about infectious diseases comes from the mouse,” says Carolyn Coyne, a microbiologist at the University of Pittsburgh. So Coyne made a mini-gut instead. In a paper published last month, her team took human stem cells and nudged them to develop into the seven different cell types that make up the human gut. Just like Hoffman-Kim’s mini-brains, Coyne’s cells self-organized into blobs of proto-intestines, complete with finger-like villi. Some enteroviruses targeted certain cells and not others, using them to gain passage into the bloodstream where they cause the most damage.
Still, the mini-gut on its own wasn’t enough to study why those cells got targeted. Coyne suspects it might have something to do with the gut microbiome. She hasn’t yet been able to test her hypothesis, because most gut microbes can’t live in a petri dish along with her mini-guts longer than a day or two. But you know where they can live longer? Yep: on a chip.
Nobody knows which day of the week a six mile-wide asteroid crashed into what would someday be the Yucatan Peninsula. What people do know is that day was around 65 million years ago, and that the days that came after were colder, darker, and filled with fewer and fewer dinosaurs.
The collision reconfigured Earth's life support systems by kicking up huge amounts of dust, vaporizing massive volumes of water, and triggering hundreds of earthquakes and volcanic eruptions. The strike—and ensuing mass extinction—marks one of the most well-known geologic divisions, between the Cretaceous twilight and the Paleogenic dawn. In terms of global impact, humans are like a scattershot version of that asteroid; they have changed the planet so much that many scientists believe modern society deserves its own geologic epoch—the Anthropocene. And while nobody knows which day humans became a force of nature, a pair of scientists believe they have an equation that can pinpoint the year.
This planet is about 4.5 billion years old. For at least three quarters of that time, it has supported life. "The Earth is generally in a state of balance, with feedback loops that keep things like the atmosphere and temperature at equilibrium for deep spans of time," says Owen Gaffney, a writer and co-author of the new study, published in the Anthropocene Review. During those times of equilibrium, lifeforms evolve slowly, extinction is rare, and biodiversity increases. Then along comes an asteroid strike or megavolcano eruption. Or the Earth tilts half a degree on its axis. Each cataclysm alters the atmosphere, temperature, ocean composition, and dozens of other processes that determine what is fit enough to survive.
“You could make such a long list of these things that it gets boring talking about all the ways that humans are changing the planet,” says Erle Ellis, landscape ecologist at the University of Maryland. What’s controversial, he says, is this paper’s attempt to pinpoint a date when human activity came to dominate the biosphere.
Ellis is a member of the Anthropocene working group, tasked with doing the preliminary work that would eventually define the Anthropocene as a formal geological epoch. Formalizing it would be a huge deal, and is hotly controversial among earth scientists. At issue is not whether human activity is changing life on Earth. "There's no controversy there," says Ellis. The real problems vexing the working group are practical: Will all this human activity compress into a stratigraphic smear large enough for future geologists to study? And what is the point of naming a layer of rock that hasn't even formed yet? "Putting a new epoch in now really does nothing useful for stratigraphers and geologists working in bodies of rock to understand formation processes," says Ellis.1
Gaffney's co-author, climate scientist Will Steffen of Australian National University, is also a member of the working group. In the pair's new paper, they describe an equation that compares the effects of the last few hundred years to the baseline conditions of the Holocene, which spans the last 11,700 years. "We looked across the board at key Earth processes, and the speed at which they had changed," says Gaffney. For most of that time, the systems were in equilibrium—held stable by phlegmatic solar activity, a 26.5 degree tilt to the Earth's axis, and an absence of island-sized rocks falling from the sky.
According to their calculations, human activity eclipsed the sun, Earth, and errant falling stars as the dominant process shaping life on this planet around 1950. "This coincides with things like the first nuclear bombs, which put traceable radiation in the atmosphere, which is visible in the sedimentary record," says Gaffney. They aren't the first to fix on the post-war period. The 1950s mark what many researchers call the Great Acceleration, when the booming middle class caused spikes in global GDP, agricultural land use, paper production, dam building, personal vehicle ownership, international tourism, and other markers of consumption. Steffen and Gaffney's equation just adds more oomph to the argument that the Anthropocene began in the same era as color TVs.
Gaffney doesn't shy away from the implications here—that a relatively small subset of humans are responsible for the Anthropocene and all it entails. "Our key conclusion is that the key driver of global change is the production and consumption of goods by the world's upper and middle classes," he says. "Defining the Anthropocene could have broader societal impacts, and the potential to shift world views, similar to how Darwin's theory of evolution, or Copernicus' heliocentric astronomy caused huge paradigm shifts."
Humans aren't the first organisms to reshape the world—nor will they likely be the last. About 2.3 billion years ago, single-celled organisms called cyanobacteria developed photosynthesis, inhaling carbon dioxide and flooding the atmosphere with oxygen. Their collective outgassing is sometimes called the Oxygen Holocaust, as it suffocated much of the anaerobic life that had dominated Earth. Cyanobacteria were so successful that their dead little bodies eventually congealed into vast oil reserves. And when the Anthropocene layer does form, the carbon burned from that oil will be why it has such a nice, dark hue.
1 For what it's worth, Ellis is among a minority of Anthropocene working group members who believes the epoch's roots go far deeper into human history. "The Industrial Revolution happened way before 1950," he says. "You can also consider the incredible effects that agriculture, which is thousands of years old, has had on the biosphere."
Ed Rivera-Valentin grew up in Arecibo, Puerto Rico, less than 15 minutes away from the jungle home of a 1,000-foot-wide radio telescope. When he was four or five, his parents brought him to the observatory for the first time. He saw the telescope’s mesh dish, resting inside a huge sinkhole in the soft rock formations that shape the region. If he had walked around the Arecibo radio telescope’s dish, he would have clocked more than half a mile.
The young Rivera-Valentin was awed. “I worked the rest of the time to make sure I would be able to do astronomy,” he says. He came to the mainland for school and a postdoc but returned for his first permanent job, at the observatory, in 2014.
But just a few years after Rivera-Valentin’s homecoming, the telescope is in jeopardy. The National Science Foundation wants to cut back its Arecibo expenditures, from around $8 million to $2 million a year.
That’s not just a problem for astronomers like Rivera-Valentin. It is a problem for civilization. Because with the telescope that has shaped his life, Rivera-Valentin is trying to save yours. Together with a cadre of other planetary scientists, he sends radio waves flying from Puerto Rico toward asteroids that venture near Earth. Sometimes too near. Sometimes so near they kill the dinosaurs. “It’s not that different from having a super high-powered version of the radar gun that police have,” says Patrick Taylor, Arecibo’s lead radar scientist. But instead of aiming that gun at your Kia, they aim it at a ragged rock in space.
Just this year, over the course of four weeks, three asteroids buzzed between Earth and the moon—and no one knew they existed until they were almost here. Every year, a few dozen asteroids between 20 and 40 feet across do the same. Astronomers are working to find more of these smaller-sized asteroids before they come so close. Then radar systems like Arecibo jump in to size up the asteroid, help map its path, and project that path into the future.
Now, Arecibo faces endangerment and, perhaps, extinction—just like humans could if we don’t know what’s about to hit us.
Only two facilities in the world do serious planetary radar work. Arecibo, obviously. And the Goldstone Deep Space Communications Complex in the Mojave desert. But Goldstone’s priority is communicating with spacecraft, and its many nearby military installations mean operators have to ask permission before turning on their transmitter. Arecibo is also much bigger: It’s 20 times more sensitive, and its transmitters pound out twice as many kilowatts of radio waves. Losing its capabilities would have a huge impact, pun intended, on scientists’ ability to predict and avoid asteroidal threats.
Here’s why you should care: the dinosaurs. They died (except the flying ones), after an approximately six-mile-wide asteroid smashed into Central America around 65 million years ago.
You don’t have to worry about a full-on global catastrophe quite yet. Scientists have discovered almost all the extinction-sized near-Earth asteroids, and none of them have Earth’s number. Astronomers haven’t been able to rule out one big rock—named 1950DA—that could smash here in 2880. But without planetary radar, your great32 grandchildren wouldn’t even know they should be worried.
Species don’t have to go extinct for an asteroid crash to be meaningful, either. Rocks around 100 meters hit here every few thousand years. “These objects cannot cause global destruction,” says astronomer Michael Busch of the SETI Institute, “but they can cause damage on the scale of small country or a big US state.”
With radar observations, scientists like Busch can nail down orbits and accurately fast-forward them hundreds of years, giving civilization advance notice—so we end up more Armageddon than Deep Impact. NASA is already practicing the skills that will be necessary to hustle asteroids to more benign orbits with a mission called DART. They’ll send a spacecraft to the double-asteroid system Didymos, which will “intercept” (read: crash into) the smaller of the two asteroids, changing the rock’s course.
If and when rocket scientists deflect an asteroid, radar astronomers will grade their work, pinging the newly-nudged asteroids and projecting their new, hopefully benign orbits.
“You can’t undo a hurricane; you can’t undo a tornado,” says Rivera-Valentin. “You can undo an asteroid impact.” But only if you know it’s coming.
Right now, the National Science Foundation funds about two-thirds of Arecibo’s operations, backing its radio astronomy and atmospheric work. The remaining third comes from NASA, which funds all the radar research. The NSF wants to cut its Arecibo contribution back over five years, starting in 2018. And in January, it solicited proposals for outside organizations to pick up the fiscal slack.
At the end of that solicitation, new partners could come on board and Arecibo operations could stay the same. If not, Arecibo could become a kind of educational museum, or the foundation could “mothball” the site. They could also blow it to smithereens. (Do note that “explosives, if used, would be limited to low-force charges designed to transfer the explosive force only to the structure designated for removal.”)
Asteroid Mining Sounds Hard, Right? You Don’t Know the Half of It
Introducing NASA’s OSIRIS-REx Asteroid Mission: the Unofficial Comic Strip
What Happens When a Space Observatory Goes Rogue
NASA, for its part, wants to continue funding Arecibo’s radar operations. But their one-third contribution wouldn’t run the telescope one-third of the time and let it sleep for the remainder. That’s not how telescopes work: You have to keep their electronics cool. You have to continually paint their metal so it doesn’t rust. You have to pay the people who do day-to-day work.
Even with the hours the radar team currently gets on the telescope and Goldstone, they can't keep up. With more sensitive instruments and dedicated rock-spotting programs, astronomers are discovering asteroids faster and faster, a pattern that will likely continue for a while. And if it does, and the observing hours stay static? “We just won’t be able to observe 75 or 80 percent of the objects we could detect,” Busch says.
And that means they won’t have a precise idea of where 75 or 80 percent of those objects—from the car-sized ones to the state-crushers—are going, now or in 2417. Now imagine the discovery rate goes up, but Arecibo’s radar facility sits idle. Only Goldstone remains.
Consider this: That’s only one more radar system than the dinosaurs had.
Last week was a rough one for the two largest US commercial space launch companies. On Thursday, United Launch Alliance confirmed it would lay off up to 400 people from its workforce by the end of the year, following a smaller round in 2016. Meanwhile, government investigators apparently reported a flaw in SpaceX’s Falcon 9 rockets—crack-prone fuel-pumping fans.1
Those developments demonstrate how each company is racing to turn a profit sending payloads into space. In that sense, commercial rocket launching is just like any other industry: Cut costs and maximize profits. In ULA’s case, that means slimming the workforce. For SpaceX, the flaws reportedly detailed in the government investigation (which the Wall Street Journal covered first) could indicate that the company has been streamlining its rocket manufacturing a bit too much.
These two companies are important in part because they’re both government contractors. In 2015, government contracts represented 69 percent of revenues in the $5.4 billion global launch industry. That same year, after a two-year long process, SpaceX got certification from the Air Force to bid for national security projects. (It’s now working on an $82.7 million contract for the Air Force to launch a GPS satellite in 2018.) ULA is its only competitor cleared to do the same. SpaceX is slated to resupply the ISS in mid-February, and its eventual goal is to carry astronauts there too.
So these two competitors are fighting to get the biggest chunks of a relatively small market. “It costs hundreds of millions of dollars to build and launch a satellite, which is practically the size of an industry for other industries,” says Carissa Christensen, the CEO of Tauri Space and Technology, a consulting and analytics firm. Because payloads are so expensive, companies (or government agencies) looking to launch them into space want the best deal.
Plus, cost is the thing that determines how big the space market will become. You can’t have a bustling asteroid mining business if it’s too pricey to haul mining equipment to space.
But the two companies have used different strategies to keep clients. ULA, a collaboration between Boeing and Lockheed Martin, has a flawless launch record. They’ve never lost a rocket, but safety comes at a cost. According to ULA’s RocketBuilder tool, strapping your satellite to one of its Atlas V rockets will cost you a minimum of $109 million.2 SpaceX advertises the same service starting at $60 million, and that’ll come way down once Elon Musk starts selling launches on used first stage boosters. “SpaceX is like a jackrabbit, and ULA is a buffalo that realizes it needs to be a jackrabbit,” says Keith Cowing, a former NASA employee who runs the blog NASA Watch.
SpaceX cuts costs by building all its own stuff, innovating quickly, and being able to shuttle payloads right out of the gate. It currently does more lift-off-ing for the government, but its combustion-cluttered track record could be a liability for future gigs. The Falcon 9 that exploded during fueling last September is the second Musk has lost since 2010, when Falcon 9s debuted. The explosion caused a huge gap in SpaceX’s increasingly busy launch schedule.3
SpaceX Dragon Closes in on Space Station
The company also has a reputation in the industry for pushing its engineers to work long days. That might be fine for all the young, hungry talent, but could be a liability for developing a mature engineering culture. “It’ll be a challenge to retain seasoned senior engineers who don’t want to work 100-hour weeks,” says Richard Rocket, the head of analysis firm NewSpace Global.
Largely, the company has shrugged off technical problems as the natural result of building new rockets. (A SpaceX spokesperson says the company has modified the design to avoid the troublesome fuel fans entirely, and that it was an old flaw.) But these issues can counterintuitively be a good sign. “Usually, you don’t know you’re working at full potential until you start encountering problems,” Cowing says. If that’s true, the rocket business might be exactly where it should be: lifting off.
1 UPDATE: 2:34pm ET 02/09/2017 — This sentence has been corrected to reflect that government investigators merely reported the cracks rather than found them.
2 UPDATE: 2:34pm ET 02/09/2017 — This sentence has been updated with the correct name of the rocket.
3 UPDATE: 2:34pm ET 02/09/2017 — This sentence has been corrected. The GAO investigation was prompted by government mandate, not SpaceX’s September explosion.
Lately, you might have noticed an abundance of pictures showing the rings of Saturn. These were recently captured by the Cassini space probe, whose mission will come to a dramatic end in September when it flings itself into the gas giant's atmosphere. One of the coolest things in these images, taken as the probe travels between Saturn's north pole and the edge of its main rings, are the gaps in those rings. But why do those gaps exist?
A planetary ring is essentially millions of particles orbiting a planet in a flat plane. If their mass is small enough, the particles don't interact with each other. They simply orbit the planet. In the absence of a massive nearby object, the only force acting on those particles is gravitational force. You can determine the magnitude of the force like this:

Remember, this is simply the magnitude of the gravitational force—the direction matters too, but I left that off (for now). In this expression, G represents the universal gravitational constant with a value of 6.67 x 10-11 N*m2/kg2. Also, Mp represents the mass of the planet, and mr is the mass of the ring particle.
If a ring-particle follows a circular orbit, this gravitational force must make the ring-particle accelerate toward the center of the planet. Given that this is the only force acting on the particle, the acceleration follows the same direction as the force. I can write this centripetal acceleration in terms of the angular velocity (ω) like this:

This says that the rings closer to the planet must orbit with a greater angular speed. When the particles are further away, the angular velocity decreases. And with this, you see that orbital mechanics dictates that a planetary ring cannot be solid.
OK, but what about the gaps between rings? Suppose a small moon also orbits the planet. In this case, both the moon and the planet exert gravitational forces on the ring-particle.

With the moon in this position, the net force is no longer of sufficient magnitude for circular motion at that orbital difference. Assuming the moon is relatively small (Earth's moon is fairly massive relative to the size of the planet), you'd see a tiny disturbance in the ring-particle's motion. But it shouldn't be a big deal. However, one set of ring-particles will exhibit a significant disturbance. If the orbital angular frequency of a ring-particle is an integer factor of the moon's frequency, then the moon will regularly be in a position to pull on the ring-particle in the same way. Let me offer an example. Let's say a moon orbits at a distance of rm so it has an orbital angular frequency of:

Now imagine a ring-particle with an orbital angular velocity twice that. It will have an orbital distance of:

With this double frequency, the ring-particle will have a consistent nudge that eventually pushes it out of its orbit. It's a bit like pushing a child on a swing at the right frequency. If you push every other cycle, the child climbs get higher and higher. Integer multiples of orbital frequencies are what causes the ring gaps.
Maybe I should make something clear. Although I understand the basics of gravity and orbits, I'm not an astrophysicist. I can create a model based on fundamental principles, but there is a chance that I might miss something important. This is what makes this so exciting. Heck—I'm not even sure of the term "ring gap," but I think you understand what I am saying. (Editor's note: Ring gap checks out, but planetary scientists call the largest gaps divisions. The biggest visible gap in Saturn's rings is the Cassini division.)
Here is the plan for the ring-model.
Let me start off with four particles centered around a position that is 0.8 times the calculated gap radius. Here's the code. Remember you can click "play" to run and "pencil" to edit—yes, you can edit the code if you want. It won't break anything (well, not permanently).
Really, you don't have to run that code—it's not terribly exciting. Here is the important part, a plot of the distance from the earth for the four ring-particles:

Note the variations in the orbits due to the interaction with the moon. That said, they essentially maintain the same orbit.
Next I will move the four ring-particles so that they are very near the ring gap distance. Here is a link to the code (which you can play with if it makes you happy), but really I simply want to show the plot for the orbital radius.

Clearly there is a difference with these four ring-particles. They do not have stable orbits like the ring-particles at a non ring gap position. Why the difference? Since the orbital frequency is around twice that of the moon, these ring-particles experience a regular nudge when they are close to the moon. The particles at the non-ring gap location get a nudge more infrequently.
Here are some problems for you to consider.
As part of WIRED’s exclusive look at Breaking2, Nike’s revolutionary attempt to break the two-hour marathon mark, our writer is using the same training regime, apparel, and expertise as Nike’s three elite athletes—including Olympic gold medalist Eliud Kipchoge—to try to achieve his own personal milestone: a sub-90-minute half-marathon. This is the second in a series of monthly updates on his progress.
On Wednesday, January 26, I ran 10 kilometers through a forest in Kaptagat, Kenya, with Eliud Kipchoge, a few of his friends, and some of the scientists from Nike’s Breaking2 project. It was 4 pm and still blazing hot. We were at 8,000 feet of altitude. The atmosphere was jovial. Philemon Rono, a relentlessly cheerful athlete known to his friends as askari kidogu—“Small Police”—cracked jokes at my expense for at least the first 20 minutes. To be sure, little could have been funnier than me, a very hot 6-foot-5 British man, sweating next to Rono, 5 feet 31/2 inches of pure runner.
All of a sudden, our curious-looking gang went quiet. Having lost a couple of hard-breathing scientists on the way out, casualties to the altitude, we turned around at halfway. For a brief period, with the sun muffled by an avenue of dense trees, nobody in the group said a thing. The pace gently increased from around 5 minutes per kilometer to a little north of 4:40 per kilometer. All you could hear was the hi-hat beat of sneakers on dust and the straining bellows of an outsized mzungu attempting to hang with the Olympic marathon champion.
At a dusty athletics track, Kipchoge and crew run 12 repetitions of 1,200 meters at roughly world-record pace for the marathon.
It was during this period that I reflected upon the happy fact that I was not dead. Kipchoge has run whole marathons almost twice as fast as we were moving at that moment. Why had he chosen not to crank up the pace? Why hadn’t he killed us? Kipchoge is polite to a fault. Was he simply humoring his guests? When we returned to his training camp, another possibility emerged. This was a recovery run, and Kipchoge really does take his recovery runs that slowly. The data the Nike science team analyzed from his GPS watch shows that the kind of run he had done with us was exactly the kind of run he would have done anyway.
The thought remained with me. The previous day, at a dusty athletics track, I’d watched Kipchoge and his training group run 12 repetitions of 1,200 meters at roughly world-record pace for the marathon. (Kipchoge later told me it was “an 80 percent session”—hard but not crazy.) The day after our jog in Kaptagat, I’d watch the same group scorch 40 kilometers—or 25 miles, nearly a whole marathon—in 2 hours, 17 minutes. That, too, was real work. But on the Wednesday in between two intense days, Kipchoge had ambled his way to 20 easy kilometers, jogging in the morning and evening. Meanwhile, at his camp—a simple plot next to fields with cows, containing two tin-roofed bungalows, with no running water and long-drop toilets—he and his friends had spent their non-running time performing chores, listening to the radio, sleeping, and drinking gallons of sweet, milky tea.
I knew Kipchoge was fast. I didn’t understand how slow he could be. This, I thought, might be a moment to learn something.
The runners rinsing off the dust kicked up from their training session.
A few weeks earlier, I had been training at Paddington Recreation Ground, in London, just starting on a set of mile repetitions, when I felt a little pop in my left calf. I ground to a halt. The injury was frustrating, to say the least. I’d been training hard and had been making progress. My times were coming down, my fitness was improving, I felt light. And now—out of nowhere—a setback.
But then I thought: Cowboy up. The leg didn’t feel so bad. I rested for a couple of days, then tried out the calf on a short jog. After two days of decent training—a glorious “progression run,” each kilometer faster than the last, with my friend Pete the Trumpet, plus a great track session—I felt that little pop again and once more stopped dead. I was about 3 miles from home, with no money in my pocket. It was freezing cold. The walk back seemed to take forever.
The Nike team begged me to rest properly. I saw a physiotherapist named Matt Fox, who has worked at Manchester City and Bolton Wanderers football clubs and has seen more than his share of injured calf muscles. He thought the strain was most likely a grade 1 tear of my soleus. He also counseled inactivity. “You can either rest properly now, or you can turn a one-week injury into a six-week injury,” he said. Foxes are smart, I knew.
During my eight days off, I rethought other aspects of my training. Perhaps I’d injured myself because I was working too hard. In addition to five or six runs, many of which were intense, I was also training at CrossFit twice a week—throwing weights around, jumping on boxes, and so on. The CrossFit had been excellent for me but, with the running, I was exhausted. Eventually, something was going to give. Eventually, it did.
The author recovering from the early morning run.
The data that the scientists had collected on me also altered my thinking. Nike has recently contracted a garrulous Chicago physician named Phil Skiba, who has trained many elite endurance athletes, to work on Breaking2. Skiba has developed algorithms that accurately measure and predict training loads. He is particularly interested by fatigue, and the balance between what he calls the “positive and negative effects of training.” In particular, Skiba uses athletes’ training data to predict when, before a race, they should begin their taper—that is, to progressively decrease their volume of training so that they arrive on race day fresh and fast.
Every athlete has a different taper point. Some people need only a few days. Some people need weeks. The variations are explained both by differences in workload and by our physiological differences. Some athletes simply recover quicker from hard training than others, in ways that geneticists and physiologists are still trying to fully understand. Skiba’s data, however, is precise. He and the Breaking2 crew believe that Kipchoge’s taper may have started a day or two late before his previous marathons and that he would have benefitted from around a week of rest rather than his normal five days.
Kipchoge, center, and other runners from the camp on their way to a training session.
Whether it’s worth shifting Kipchoge from his normal patterns for this one race is a concern among the Breaking2 team, especially because routine is psychologically important to athletes. But their analysis shows how a data-augmented approach might yield benefits even for the greatest runners. (As for Lelisa Desisa, another of the three elite runners contesting Breaking2, the Nike scientists believe his taper may be a few days too long.) In my case, based on how I’ve reacted to my training load so far, they believe I should taper for 21 days. 21 days! Clearly, I am more in need of rest than the average lummox.
Back to Kenya. Watching Kipchoge’s group at work, I saw that they never did two intense days back to back; they were always committed to developing their fitness, in the Kenyan parlance, “slowly by slowly.” Patrick Sang, Kipchoge’s coach and a formidable presence in the athlete’s life, explained to me the basis of this philosophy as he stood at the side of the track with a stopwatch in his hand and his red-and-black hoodie fastened tightly around his head. Our conversation had begun when I asked Sang why Kipchoge’s group were doing a 12 x 1,200-meter session on that day.
Sang said this session was to build “speed-endurance”—the ability to maintain a high speed for a long time. But if you thought about only one workout, you missed the point. The idea of a training program, Sang told me, was to improve every aspect of a runner. The approach was holistic. If you scheduled a speed-endurance session for a Tuesday, you needed to make sure that the following day would be light, so that the guys had time to recover before the Thursday long run. Friday would again be light, before a different kind of speed workout on Saturday. Sunday was a day of rest. A good day of training was worth little on its own, but a good month was worth plenty. Slowly by slowly, the athlete’s shape came. “Every session is a building block,” Sang said.
Patrick Sang, Kipchoge’s longtime coach and a proponent of the “slowly by slowly” training philosophy.
Valentijn Trouw, Kipchoge’s Dutch manager, told me something else interesting: He thought Kipchoge never killed himself in training. The only day on which he would drain every resource he possessed was on race day. “Never 100 percent in any session,” Trouw said. “That’s the philosophy.” This approach made sense to Skiba. “The time to open up a can of whup-ass is on race day,” he told me. “Otherwise, you risk leaving your best performance in training, where nobody sees it.”
“Slowly by slowly” is not a mantra that lends itself to hard-charging Western approaches to fitness. How often do we hear that only hard work brings rewards—that the more you put in, the more you get out? Also, many average Western athletes, like me, do so much of their training at a consistent pace. There’s not enough variation or rest in their schedules. The Kenyans, particularly those in Sang’s group, are more sophisticated in their approach. I’ve never seen more-committed athletes, in any sport, anywhere in the world. But they also know it would be crazy to grind themselves into the dust.
On my last day in Kenya, I was talking to Geoffrey Kamworor, a runner with a wide gap-toothed smile and an easy manner that masks a profound belief in his own talents. As a runner, everything about him is purposeful. In training, he leans into bends with his shoulder, kicking up dust behind him, like a young bull on the charge. In competitions, he is fearless. Now in his mid-twenties, he is the reigning world half-marathon champion and the world cross-country champion. He also won a silver medal in the 10,000 meters at the 2015 World Championships in Beijing.
Kipchoge and his team enjoy a post-training tea.
When I asked him what tips he could give to a mzungu attempting to break 90 minutes for the half-marathon, his first thought was to get a good pacemaker. He offered his services. “If you want 4:20 [minutes per kilometer], that’s no problem, I will bring a newspaper,” he said, a bright smile on his face. “If you want 2:50 [minutes per kilometer; 2-hour-marathon pace] that’s also no problem.”
He then became more serious and gave me some real advice.
“Work hard,” he said. “But not every day.”
I wrote that one down.
The United States has no national database tracking fatal shootings by police. The absence of good data has meant that much of the research on the role of implicit racial bias in these deaths takes place inside laboratories. Officers play word association games or go through virtual reality simulations to measure bias. But these methods have some obvious weaknesses. "An officer is not going to killed or injured if they mess up in a simulation," says University of Louisville criminologist Justin Nix.
But Nix wanted to understand what was happening in the real world. So he and fellow researchers turned to one of the only robust datasets that exists on police shootings—a 2015 database compiled by the Washington Post. They analyzed each case based on the victim's race and whether the victim was armed or attacking the officer or other civilians when he or she was shot. The results are staggering, if unsurprising. Of the 990 fatal shootings the researchers analyzed from 2015, police were more than twice as likely to have killed unarmed black civilians as unarmed white civilians.
"That jumps off the page," Nix says. "The next question is: why?"
'An officer is not going to killed or injured if they mess up in a simulation.'
In their study, published today in Criminology & Public Policy, Nix and his colleagues controlled for variables like rates of crime and mental illness in the areas where these shootings took place, in hopes of isolating race as a contributing factor. The more they eliminated other factors, the more implicit bias appeared to play a role, Nix says. Among their other findings: Non-black minorities killed in police shootings were also much less likely than whites to have been attacking an officer or fellow civilians at the time they died.
"I think what this is pointing to is that, when officers are in those very threatening situations, it's a lot harder to compensate for bias," says Jack Glaser, a professor of public policy at U.C. Berkeley, who specializes in racial profiling. "They are terrified and they are influenced relatively more by lower-level, spontaneous processes."
Glaser takes issue, however, with the researchers' core recommendation that police departments introduce training to minimize implicit bias. Research has proved it difficult to rid people of their bias. A better approach, he says, would be to introduce training that minimizes the likelihood that bias will influence officers' actions. "For example, training on de-escalation techniques will reduce the frequency with which situations rise to the level of intensity that implicit biases are going to be relatively influential," Glaser says.
Both Glaser and Nix agree that these results are far from complete. For one, the *Washington Post'*s database—novel though it may be—includes only fatal shootings, which make up just a fraction of all instances of deadly force used by police against civilians. "If we had data on all shootings, not just fatal shootings, then we could speak to the likelihood of using deadly force," Nix says. "Now, we’re working with less than half of all the puzzle pieces."
The federal government doesn't currently require local law enforcement agencies to report such data. Without it, building a central repository in an objective and credible way is nearly impossible for any one group. But change may be coming. At a congressional hearing in September, FBI director James Comey told lawmakers that the bureau is launching a database within two years that tracks use of deadly force by police.
Police Could Get Your Location Data Without a Warrant. That Has to End
An Oral History of Three Days That Rocked America
How Black Lives Matter Uses Social Media to Fight the Power
"Everybody gets why it matters," Comey said at the time.
When it comes to data about race and crime, however, President Trump hasn't always been the most reliable narrator. During the election, he tweeted a false murder rate statistic that earned him a "Pants on Fire" rating from Politifact. And Tuesday, at a meeting with county sheriffs, he said, "The murder rate is the highest it's been in, I guess, from 45 to 47 years," despite the fact it has been in a decades-long decline.
Given Trump's desire to paint a certain archaic picture of "American carnage," it's unclear whether his Justice Department will prioritize the kind of hard number-crunching both Comey and Nix crave. If Trump really is serious about making black communities safer, getting good, true data on the problem might be a good place to start.
Squid and their cephalopod brethren have been the inspiration for many a science fiction creature. Their slippery appendages, huge proportions, and inking abilities can be downright shudder-inducing. (See: Arrival.) But you should probably be more concerned by the cephalopod’s huge brain—which not only helps it solve tricky puzzles, but also lets it converse in its own sign language.
Right now, you're probably imagining twisted tentacles spelling out creepy cephalopod communiqués. But it's not that: Certain kinds of squid send messages by manipulating the color of their skin. "Their body patterning is fantastic, fabulous,” says Chuan-Chin Chiao, a neuroscientist at National Tsing Hua University in Taiwan. They can display bands, or stripes, or turn completely dark or light. And Chiao is trying to crack their code.
Chiao got his inspiration from physiologist B. B. Boycott, who in the 1960s showed that the cuttlefish brain was the control center for changing skin color. Boycott copied his technique from neurosurgeon Wilder Penfield, who treated epilepsy patients by burning out the misbehaving bits of their brains. While their grey matter was exposed for surgery, Penfield* *also applied a gentle current through the electrodes in his patients’ brains. You know, just to see what would happen.
A zap in one spot above the ears caused a tingle in the left hand. In another spot, tingles in the leg. And so Penfield discovered that the sensory cortex is a homunculus, with specific brain areas mapping onto different parts of your body. Over time, scientists tried the electrical stimulation technique on all kinds of animals—including Boycott's cuttlefish.
Chiao tried out the same thing in a related cephalopod, the oval squid—but he took it to the next level. In a paper published in the Journal of Neuroscience in January, he describes putting electrodes in a bunch of different parts of the optic lobe, stimulating them, and recording the resulting body patterns. "Those optic lobes are the mystery of the cephalopod nervous system," says Roger Hanlon, a marine biologist at the Marine Biological Laboratory at Woods Hole.
When Chiao started out, he thought the optic lobe would be organized like the human cortex, with the pigment on different body parts correlating with different locations in the brain: a squidunculus. Not so. “When we finished the experiment," says Chiao, "we looked at the data and it was really puzzling." He'd poke in the left part of the optic lobe, and the squid’s mantle would turn black. Then, he'd poke in the right region—and the same thing would happen. It seemed like the squid's body parts were represented in more than one spot in the optic lobe.
The Critter That’s Mushy as a Jellyfish and Long as a Whale
The Morpho Butterfly’s Blue Isn’t What It Seems
The Astonishing Gecko That Looks Just Like a Tree
After Chiao and his student Tsung-Han Liu had stared at the data for a long time, a hypothesis began to emerge. The squid’s pigment cells are surrounded by muscles that stretch the pigment cells out or let them curl up. Instead of controlling body parts, the optic lobe controls those muscles. When Chiao stimulated in one spot, the squid’s mantle turned dark. Another spot, the mantle got thick, horizontal stripes. Another, the mantle got one thin vertical stripe.
Each part of the body has its own patterns, so a squid can simultaneously have polka dot fins, dark tentacles, and a stripy mantle. It’s like the squid has an alphabet of patterns—14 by Chiao's count—which repeat in a mosaic within the optic lobe. It's like if your keyboard had hundreds of keys, but still only 26 letters. That redundancy, Chiao hypothesizes, is how the squid can execute a new combination as quickly as once a second.
Eventually, scientists will make an even more detailed map of what each individual neuron is doing in this brain area. Hanlon's excited for that day, especially because the squid brain appears to be so different from vertebrate brains. "Their body plan is so bizarre compared to ours that it’s hard to compare their brain structure and function to something that we know," he says.
And for now, Chiao wants to know what different combinations of pigment patterns might mean to an onlooking squid. He's recording the patterns that squid take on when they're together and correlating them to their behavior, like mating and male-on-male aggression. Perhaps soon he’ll find out whether squid are having complex discussions about squid politics, or (more likely) just figuring out who has the bigger mantle.
Squid use a remarkable array of skin patterns to communicate. How? It's all a matter of getting inside their heads.
America is not the great melting pot that poets like Ralph Waldo Emerson once extolled. At least, that’s not the story that DNA tells, according to the genealogy company Ancestry. Using more than 770,000 spit samples taken from their customers over the last five years, its researchers mapped how people moved and married in post-colonial America. And their choices—especially the ones that kept communities apart—shaped today's modern genetic landscape.
The study, published today in Nature Communications, combines a DNA database with family tree information collected over the company’s 34-year history. “We’re all living under the assumption that we are individual agents,” says Catherine Ball, chief scientific officer at Ancestry and the leader of the study. “But people actually are living in the course of history." And from the moment they spit, send, and consent, DNA kit customers become actors in a much larger story—told through the massive data sets companies like Ancestry are accumulating from casual genealogists.
Ball’s team of geneticists and statisticians started by pulling out subsets of closely related people from their 770,000 spit samples. In that analysis, each person appears as a dot, while their genetic relationships to everyone else in the database are sticks. The result, Ball says, “looks like a giant hairball.”
From that hairball her team pulled out more than 60 unique genetic communities—Germans in Iowa and Mennonites in Kansas and Irish Catholics on the Eastern seaboard. Then they mined their way through generations of family trees (also provided by their customers) to build a migratory map. Finally, they paired up with a Harvard historian to understand why communities moved and dispersed the ways they did. Religion and race were powerful deterrents to gene flow. But nothing, it turned out, was stronger than the Mason Dixon line.
“I have to admit I was surprised by that,” says Ball. “This political boundary had the same effect as what you’d expect from a huge desert or mountain range.”
Besides being a cool way to see how your ancestor’s dating pools forged the nation today, though, Ancestry's study has real applications for medical research. A lymphoma study pulling subjects from Minneapolis shouldn’t expect to see the same results as one that recruits in Miami, for example. Populations in different parts of the country have very different genetic makeups—and those differences could be incredibly valuable to a company building personalized cancer treatments, immunotherapy drugs, and other gene-targeted therapeutics.
Ancestry jumped in late to the genetic data game—they launched DNA analysis kits in 2012, five years after chief competitor, 23andMe—but strong sales in the last few quarters have sent it sprinting ahead. Today, Ancestry is valued at $2.6 billion and has one of the largest biobanks in the world, with genetic data from 3 million people. For every saliva sample swimming with DNA, the company analyzes more than 700,000 genetic locations, or SNPs, for single-nucleotide polymorphism. 23andMe, by comparison, is valued at $1 billion, has a database of more than 1.2 million individuals, and reports on roughly 650,000 SNPs per spit tube. But for research purposes, 23andMe says it uses a process called imputation to analyze 15 million variants per person.
“Research purposes” are a lucrative business for 23andMe. When its customers give permission, the company anonymizes their data and hands it over to more than a dozen pharma companies and academic institutions. One of those partnerships, a deal with Genentech to look at the genes of people with Parkinson’s disease, netted 23andMe $10 million.
Ancestry, on the other hand, makes the bulk of its revenue from its genealogical subscribers. It currently has only one research initiative in place, with Calico, the Alphabet spin-off focused on longevity science. The idea is that Calico will mine Ancestry’s customer data to figure out why some people live longer than others, then use that genetic information to develop life-extending therapeutics. But with studies like the one Ancestry published today, 2017 could bring more collaborators sniffing around its powerful database.
That should raise some questions for consumers, says Arthur Daemmrich, a healthcare historian at the Smithsonian Institute’s Lemelson Center for the Study of Invention and Innovation. “These companies can’t tell you today who they’re going to license your data to and for what purpose,” he says. “They’re just trying to be the holder of the data. But if they put samples on ice and keep them frozen forever, does consent cover that?”
Scientists and lawyers haven't agreed on the rules yet. But imagine for a moment what your yet-unborn great-grandchildren might learn from your DNA, preserved by a company like Ancestry. Maybe their robot doctors will use complex algorithms to identify the genes that will one day set their cells wild with cancer—and design a cure. Or maybe they'll just see that you married a fellow genealogy geek and lived happily ever after in California.
Around 11 am Pacific on January 20th, while newly-inaugurated President Trump finished a celebratory lunch in the Capitol Rotunda, Magdalena Cerdá noticed something different about the White House's website: All of its references to climate change had disappeared. Cerdá is an epidemiologist at UC Davis' Violence Prevention Research Program, which focuses on another politicized region of science—gun violence. So she knew what that meant.
"It was a real call to action," Cerdá says. With links to climate data vanishing, she worried the same thing could happen to gun violence data on websites belonging to the Centers for Disease Control and Prevention, the Bureau of Justice Statistics, or the Bureau of Alcohol, Tobacco, Firearms and Explosives. "I was on Amtrak between Berkeley and Sacramento," she says. "So I sent an email to Garen Wintemute saying we needed to start downloading our data immediately."Wintemute, epidemiologist and director of the Violence Prevention Research Program, was prepared. After seeing that climate scientists were systematically downloading crucial information from federal databases, he had drawn up a spreadsheet of the gun-related datasets he uses every day: lists of gun licensees, retailers, and manufacturers; gun tracing data; firearm-related death and injury numbers sorted by categories like race, location, or age. "I basically walked around the building saying, 'Get it done now,'" Wintemute says. So on inauguration day, as Cerdá says, the Violence Prevention Research Program was less of a lab and more of a "little downloading bootcamp."
This wasn't just alarmism. "I’ve been through it before," says Wintemute. During the Clinton years and early in George W. Bush's presidency, he worked with a group of academics who partnered with the Bureau of Alcohol, Tobacco, Firearms and Explosives to study the inner working of criminal gun markets. "We had the reports ready for 2001 and 2002, but their publication was suppressed," says Wintemute. "We were ordered to destroy our copies of the documents."
Wintemute wasn't hankering to repeat that experience, so he instructed his team—barely 10 people, which isn't surprising in this notoriously under-funded field—to create a compendium of gun-related data as a failsafe. "Everyone stopped what they were doing and gathered around a big table in the center of the office," says Aaron Shev, the team's senior statistician. "We had Dr. Wintemute's list, and wrote things up on a whiteboard, assigning jobs. Then we sat there for a day, through lunch, in this frenzy of downloading."
It's not as if they were hacking, or even using some secret scientists-only login. "Everything we downloaded by definition is public information," says Wintemute. "We didn’t go behind any firewalls." So the fact that scientists are worried they'll lose access should probably give you pause.
In many cases, federal information is vital to research in these fields. "I was scared," says Veronica Pear, a data analyst at the Violence Prevention Research Center. She's using federal data for a paper on firearm mortality—tracking hotspots in California between 1999 and 2015—and her work is almost complete. Since the federal query system makes it easy to search the data but cumbersome to download, Pear had never bothered to save the information to her computer. To catch up, "I had to enter around 50 different queries," she says. "I felt frantic."
Rogue Scientists Race to Save Climate Data from Trump
EPA Scientists Worry Their New Boss Doesn’t Want Science
Cutting International Science Funding Will Only Hurt America
After about five hours spent in a flurry of activity, the team had downloaded everything they could think of. Now, it's stored on a secure server at UC Davis, ready if gun violence researchers ever do lose access to federal data on firearm licenses, sales, use in criminal activity, and deaths. It's not as large of an effort as the climate data scraping executed in recent weeks—the datasets numbered in the tens rather than thousands—but that doesn't mean the UC Davis team's work was insignificant. Thanks to funding issues and opposition from the National Rifle Association, very little gun-related data exists. Every scrap counts. "There aren’t a lot of different kinds of data, but they are foundational," says Wintemute. "Every research study on firearm violence begins with a statement on the size of the problem. That's what these data provide."
Should the information disappear from the web, the UC Davis team will have no qualms sharing it. "I think of it is as a public service for the scientific community," says Pear. "And for me, it feels more important now than ever to be vocally on the side of truth."
But for now, they're just watching and waiting. "Every day I go onto the sites, I hold my breath," says Pear. "'Is it going to be there?'" When science becomes politicized, the only thing for researchers to do is let the data speak for itself. The UC Davis team wants to make sure the data is still around to do the talking.
America’s gun addiction is bad. But to understand just how bad it is, you’ve got to see the numbers.
Humans have certain expectations of mice: look cute, eat seeds or whatever, and scamper away from cats. But do not do things like tear scorpions to pieces. And certainly don’t evolve to turn the venom that the scorpions inject you with into a painkiller. We’ll see about all that, says the grasshopper mouse. Check out this week’s episode of Absurd Creatures to learn more!
Find every episode of Absurd Creatures here. And I’m happy to hear from you with suggestions on what to cover next. If it’s weird and we can find footage of it, it’s fair game. You can get me at matthew_simon@wired.com or on Twitter at @mrMattSimon.
Scott Pruitt, Oklahoma's attorney general, is officially the new head of the Environmental Protection Agency. Today, the full Senate voted 52-46 to confirm him to the cabinet-level administrator job. And that has the scientists who work for the EPA freaking out.
The problem is, well, that whole protecting-the-environment thing. Advocacy groups complain Pruitt ignored scientific reports that showed oil and gas drilling have caused an epidemic of earthquakes in Oklahoma and failed to enforce pollution rules that would have stopped waste from Arkansas chicken farms from floating downriver into Oklahoma. As Oklahoma's AG, Pruitt sued the EPA over mercury, methane, and other environmental rules, although he lost.
And he doesn't seem to think climate change is a threat worth studying, either. But lots of EPA science overlaps with climate issues. “It does put people in an awkward position when they can’t mention climate change effects in a report, even though its important to future planning in all kinds of water-related issues,” says one EPA scientist who asked to remain anonymous for fear of retaliation.
So what could happen to that work now that Pruitt is at the helm? It could start taking longer for scientific studies to work their way through the EPA’s bureaucracy. Some material may stay in draft form; EPA rules say if work gets finalized it has to get released to the public. “I would hope that career employees and mid-level supervisors would provide some guidance, and make sure that proper scientific work are not holding anything up that is important,” the scientist says.
That's the real concern—not that all mention of climate change could be forever redacted, but that political concerns could hinder the progress of science. The weeks since President Trump took office have been confusing; the "beachhead" team that arrived during the transition announced plans to review EPA scientific documents on a case-by-case basis, block all state and local grants, and strip the climate change portal on the EPA website. Congress and EPA staff complained and the team reversed all the decisions, although language identifying carbon emissions as the cause of climate change has been scrubbed, according to Climate Central.
The EPA press office did not respond to a request for comment.
If agency bureaucrats vet actual science before releasing it, "you will have an alternative set of facts. That is going to hurt the quality of decisionmaking and ultimately the welfare of the country," says Jeff Ruch, executive director of Public Employees for Environmental Responsibility, a Washington-based watchdog group that sent a letter to the president last week asking him to strengthen existing protections for EPA scientists.
PEER wasn't alone in the letter-writing game. Sigma Xi, the research honor society, wrote to President Trump urging that he maintain a posture of scientific transparency. The American Geophysical Union wrote to ten federal agency heads protesting restrictions on the flow of scientific information. Neither has yet received a reply.
Now, it's true that EPA already has a scientific integrity policy that is supposed to “ensure that the Agency’s scientific work is of the highest quality, free from political interference or personal motivations” as well as conflicts of interest. But enforcement has been uneven, including during the Obama presidency. EPA scientists have faced pressure to change their conclusions about sea level rise in the Chesapeake Bay and the effects of fracking on groundwater supplies.
If his past efforts are any indication, Pruitt will continue on that path. “He has attacked the agency where the science is very solid,” says Andrew Rosenberg, director of the Center for Science and Democracy at the Union of Concerned Scientists and a former deputy director of the National Marines Fisheries Service under President Clinton. “How he will do in an agency specifically set up to do science-based health and safety protections is an open question.”
On the other hand, every transition between presidential administrations comes with uncertainties—but the bureaucracy can be remarkably stable. EPA staffers are more loyal to science and the environment than a president or an administrator, as one long-time EPA official, now retired, put it. Any attempt to quash science or its communication to the public will likely be met with leaked documents, forwarded e-mails and anonymous tweets, no matter how many senators voted to confirm Scott Pruitt.
Editor’s Note 2/17/17 2:30 pm Eastern: This story has been updated with the results of Pruitt’s Senate confirmation vote.
Last night, football fans witnessed the greatest comeback in Super Bowl history. With eight minutes and 30 seconds left in the third quarter, the New England Patriots were down 28-3. But they inched forward until they pushed the game into overtime—a first for the Super Bowl—and Patriots quarterback Tom Brady marched his team down the field to win Super Bowl LI. It was an epic turnaround, but it wasn’t really the Patriots that made it happen. Every mistake that let them come back could have been avoided. And my life at the blackjack table can shed light on what the Atlanta Falcons did wrong.
Jeff Ma was the leader of the MIT blackjack team that inspired the book Bringing Down the House and the movie 21. He founded and sold tenXer, an HR anlytics company, to Twitter, where Ma is currently the Senior Director of Analytics.
Every decision in blackjack can be dictated by simple math: There’s always a right and wrong answer. The average blackjack player loses about three percent of the money they put on the table—but if you play basic strategy perfectly, you reduce the casino’s edge to about 0.5 percent. Still, even if they know the right thing to do, very few people actually play perfect basic strategy. Why? Because humans are subject to cognitive bias. That’s what brought the Falcons down tonight.
It’s more complex than blackjack, of course, but there is a basic strategy for football coaching. If you analyze the thousands of football games played in the last 10 years, you can devise a set of rules that almost always lead to a win. Computers do the hard work: They take all the possible situations and calculate the probability of each outcome. A coach could input possession, down, distance, and score and come up with one best decision for the team at that moment. All you need to do is follow the math. But, like in blackjack, it can be hard to keep focus on time-tested statistical strategies under stress.
Let’s start midway through the third quarter, when the Falcons were up 28-3. At that point, they had about a 98 percent chance to win. The Patriots took a full six minutes to score a touchdown. even with the score tightening, the Falcons had the advantage.
That’s when the crazy stuff happened.
Up 28-9 with two minutes left, the Falcons had a 99 percent chance to win the game. That probability comes from readily available calculators—which run Monte Carlo simulations, taking into account the four variables of possession, down, distance, and score. But then the Falcons made a series of errors in basic strategy.
The Best Super Bowl Commercials, From Avocados to Zelda
Lady Gaga’s Halftime Show Drones Have a Bright Future
How Madden Got So Good at Predicting Super Bowl Winners
The first mistake that quarterback Matt Ryan and the Falcons made was not letting the clock run down to fewer than 10 seconds on every play. Every second that they waste is a second that the Patriots don’t have to advance. Simple.
But in play after play, the Falcons snapped the ball when they didn’t need to, sometimes with more than 20 seconds left to go. In blackjack, this is like standing a soft 17: an ace and six. Normally, a hand that adds up to 17 is a losing hand—but the beauty of the ace is you can play it as an 11 or a one. If you get dealt a 10, you can play the ace as a one and you still have 17. There is no risk to taking one more card. And there is no risk in letting the clock run down under 10 seconds.
The second basic strategy mistake the Falcons made was not rushing the football when they had a comfortable lead. Over the course of the game, the Falcons were gaining an above-average 5.8 yards per rush (the league average is around 4.3 yards per play). Very little good could come from a pass at this point in the game—an incomplete pass is bad, a sack is bad, and a holding penalty is bad—especially when you're already averaging 5.8 yards per rush. Again, this basic strategy would have carried little risk.
All night, the Falcons had played aggressively—and it worked. With 4:40 left, they had the ball at the Patriot’s 22-yard line with an eight-point lead. Again, that's a stunning 99 percent chance to win based on the score, along with the fact that the Falcons had the ball with almost a sure field goal. Any simulation going forward would tell you the Falcons just needed to run. They could have taken a knee three times in a row and kicked a field goal. But instead they decided to pass. And pass. And pass.
Atlanta head coach Dan Quinn and offensive coordinator Kyle Shanahan thought they needed to be aggressive the rest of the night. But all they needed to do was play basic strategy—and let the clock run. What they did was akin to hitting a 15 when the dealer has a six showing: They put themselves at risk. And the result is the craziest turnaround in Super Bowl history.
Math conferences don’t usually feature standing ovations, but Francis Su received one last month in Atlanta. Su, a mathematician at Harvey Mudd College in California and the outgoing president of the Mathematical Association of America (MAA), delivered an emotional farewell address at the Joint Mathematics Meetings of the MAA and the American Mathematical Society in which he challenged the mathematical community to be more inclusive.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Su opened his talk with the story of Christopher, an inmate serving a long sentence for armed robbery who had begun to teach himself math from textbooks he had ordered. After seven years in prison, during which he studied algebra, trigonometry, geometry, and calculus, he wrote to Su asking for advice on how to continue his work. After Su told this story, he asked the packed ballroom at the Marriott Marquis, his voice breaking: “When you think of who does mathematics, do you think of Christopher?”
Su grew up in Texas, the son of Chinese parents, in a town that was predominantly white and Latino. He spoke of trying hard to “act white” as a kid. He went to college at the University of Texas, Austin, then to graduate school at Harvard University. In 2015 he became the first person of color to lead the MAA. In his talk he framed mathematics as a pursuit uniquely suited to the achievement of human flourishing, a concept the ancient Greeks called eudaimonia, or a life composed of all the highest goods. Su talked of five basic human desires that are met through the pursuit of mathematics: play, beauty, truth, justice, and love.
If mathematics is a medium for human flourishing, it stands to reason that everyone should have a chance to participate in it. But in his talk Su identified what he views as structural barriers in the mathematical community that dictate who gets the opportunity to succeed in the field—from the requirements attached to graduate school admissions to implicit assumptions about who looks the part of a budding mathematician.
When Su finished his talk, the audience rose to its feet and applauded, and many of his fellow mathematicians came up to him afterward to say he had made them cry. A few hours later Quanta Magazine sat down with Su in a quiet room on a lower level of the hotel and asked him why he feels so moved by the experiences of people who find themselves pushed away from math. An edited and condensed version of that conversation and a follow-up conversation follows.
The title of your talk was “Mathematics for Human Flourishing.” Flourishing is a big idea—what do you have in mind by it?
When I think of human flourishing, I’m thinking of something close to Aristotle’s definition, which is activity in accordance with virtue. For instance, each of the basic desires that I mentioned in my talk is a mark of flourishing. If you have a playful mind or a playful spirit, or you’re seeking truth, or pursuing beauty, or fighting for justice, or loving another human being—these are activities that line up with certain virtues. Maybe a more modern way of thinking about it is living up to your potential, in some sense, though I wouldn’t just limit it to that. If I am loving somebody well, that’s living up to a certain potential that I have to be able to love somebody well.
And how does mathematics promote human flourishing?
It builds skills that allow people to do things they might otherwise not have been able to do or experience. If I learn mathematics and I become a better thinker, I develop perseverance, because I know what it’s like to wrestle with a hard problem, and I develop hopefulness that I will actually solve these problems. And some people experience a kind of transcendent wonder that they’re seeing something true about the universe. That’s a source of joy and flourishing.
Math helps us do these things. And when we talk about teaching mathematics, sometimes we forget these larger virtues that we are seeking to cultivate in our students. Teaching mathematics shouldn’t be about sending everybody to a Ph.D. program. That’s a very narrow view of what it means to do mathematics. It shouldn’t mean just teaching people a bunch of facts. That’s also a very narrow view of what mathematics is. What we’re really doing is training habits of mind, and those habits of mind allow people to flourish no matter what profession they go into.
Several times in your talk you quoted Simone Weil, the French philosopher (and sibling of the famed mathematician André Weil), who wrote, “Every being cries out silently to be read differently.” Why did you choose that quote?
I chose it because it says in a very succinct way what the problem is, what causes injustice—we judge, and we don’t judge correctly. So “read” means “judged,” of course. We read people differently than they actually are.
And how does that apply to the math community?
We do this in lots of different ways. I think part of it is that we have a picture of who actually can succeed in math. Some of that picture has been developed because the only examples we’ve seen so far are people who come from particular backgrounds. We’re not used to, for instance, seeing African-Americans at a math conference, although it’s become more and more common.
We’re not used to seeing kids from lower socioeconomic backgrounds in college or grad school. So what I was trying to say is: If we’re looking for talent, why are we choosing for background? If we really want to have a more diverse set of people in mathematical sciences, we have to take into account the structural barriers that make it hard for people from disadvantaged backgrounds to succeed in math.
We’ve been hearing more about how these kinds of educational barriers arise in primary and secondary school. Do you argue that they arise in undergraduate and graduate programs as well?
That’s right. At every stage we’re losing people. So if you look at some of the studies people are doing now about people who take Calculus 1, and how many of them go on to take Calculus 2, you’ll find basically that we’re losing women and minorities at these critical junctures. This happens for reasons that we can only speculate about. But I’m sure some of it has to do with people in these groups not seeing themselves as belonging in math, possibly because of a negative culture and an unwelcome climate, or because of things that professors or other students are doing to discourage people from continuing.
The obvious problem with this attrition is that when mathematics draws from a smaller pool, we end up with fewer talented mathematicians. But you emphasized in your speech that denying people math is actually denying them an opportunity to flourish.
Math can contribute in a broad way to every person’s life whether that person actually becomes a mathematician or not. The goal of broadly getting people to appreciate math is not at odds with bringing more people into deep mathematics. Connect with people in a deep way and you’re going to draw more people into mathematics. Some of them, more of them, are going to go to graduate school, and that will necessarily happen if you address some of these deep desires — for love, truth, beauty, justice, play. If you address some of these deep themes you’re going to get more people and a more diverse set of people in deep mathematics.
Some of those desires are easier to relate to math than others. I think people have a somewhat intuitive sense of how a desire for truth or beauty might be realized through math. But you spent a lot of your talk on justice. How does that relate to mathematics?
Justice is a desire that people have, and so it leads to a certain virtue which is to become a just person, somebody who cares about fighting for things that defend basic human dignity. I spent the most time discussing justice in my talk mainly because I feel that our mathematics community can do better; we can become more just. I see a lot of ways in which we can do better and become more virtuous as a community.
Being a mathematician in some ways allows us to see things more for what they are. When people learn not to overgeneralize their arguments, they’re going to be very careful not to think that if you’re poor you’re necessarily uneducated or vice versa. Having a mathematical background certainly helps people to be less governed by their biases.
You’ve been a successful research mathematician, yet you teach at a small college, Harvey Mudd, that doesn’t have a graduate school. That’s kind of unusual. Was there a point where you decided you’d prefer to work at a liberal arts college rather than a big research university?
When I was in graduate school at Harvard I realized I loved teaching, and I remember one of my professors from college telling me that the teaching was better at small liberal arts colleges. So when I was on the job market I started looking at those colleges. I was interested in the research track and willing to do that, but I was also very attracted to the liberal arts environment. I chose to go and I love it; I couldn’t see myself being anywhere else.
And how do you think working at a liberal arts college shapes the way you look at the mathematics community today?
I think one of the things I didn’t address in the talk, but almost did, is the divide in the community between research universities and liberal arts colleges. There is a cultural divide, and the research universities are in some sense the dominant culture because all of us with Ph.D.s come through research universities. And there’s the whole pattern of the dominant culture being completely unaware of what’s going on at the liberal arts colleges. So people come up to me and say: “So, you’re at Harvey Mudd; are you happy there?” It’s almost like assuming I wouldn’t be. That happens all the time, so I find it a bit frustrating to feel like I have to say: “No, this is actually my dream job.”
What are the consequences of this cultural imbalance?
Well, the downsides are, for instance, that many of the people at research universities would never consider taking students from an undergraduate college. That’s the downside; they’re missing a lot of talent. So in many ways the issues are analogous to some of the racial issues that are going on.
I think professors at research universities often don’t realize that there are a lot of bright kids coming through the liberal arts colleges. What I’m addressing is the very common practice right now in certain graduate schools of only admitting people who’ve already had a full slate of graduate courses. In other words, they’re expecting undergraduates to have taken graduate courses before they even get considered. If you have that kind of structural situation, you are necessarily going to exclude a bunch of people who otherwise might be successful.
One barrier you mentioned in your talk arises when senior professors don’t teach introductory classes. Tell me about that.
I’m being a little provocative here as well. I think what that communicates is: “This is not an important enough segment of people for me to put my attention to.” I’m certainly not saying everybody who only teaches senior-level courses has this attitude, but I am saying there are a lot of people who think the math major is basically there for the benefit of students who are going to get a Ph.D. That’s a problem.
Su on the Harvey Mudd campus.
At the Joint Mathematics Meetings there were a number of prizes specifically for women, and a number of women gave invited talks. Has the math community made more progress on gender equality than on racial inclusiveness?
Definitely, racial inclusiveness has not come as far or as fast as gender inclusiveness. Currently about 27 percent of people with Ph.D.s, faculty members, are women, and about 30 percent of the ones who won awards in teaching and service are women. So we’re actually doing pretty well on that front. With our writing awards, which are awards for research and exposition—the fraction of women winning those awards is lower.
Can you look at the process by which gender equality has improved and draw any lessons from that about how to improve racial equality in math?
Many of the practices that work to encourage women in math also work for minorities. Part of the issue here is that there just aren’t that many minorities who come into college interested in doing STEM majors. So there’s something that happened at the secondary and primary school level, and it would help a lot if we could figure out what’s going on there.
You used the metaphor of a “secret menu” in Chinese restaurants. What did you mean by that?
If you go to an authentic restaurant in a big city in New York or California, if you are not Chinese they will give you a standard menu that has things in English and Chinese. But if you’re Chinese, they’ll give you a different menu. Often it’s a menu that is written completely in Chinese and has some additional options that aren’t on the standard menu. And I think that happens in the math community. If you talk to women and minorities they will often tell you they’ve had experiences where people discouraged them from going on, either because they don’t think a woman should be in math, or for other reasons. So I used the metaphor “secret menu” to mean: Do we have a secret menu? And who gets to look at it?
You told a story about a student who was counseled by a professor to choose a different major on the grounds that the student wasn’t good enough to stick with math. Is that common?
I think it’s common. Of course we don’t have any data, but I’ve certainly talked to enough people who’ve had those kinds of experiences to know that it’s very frequent and most of those people are women and minorities.
It’s been almost a month since you gave your speech, and it’s generated a lot of attention on the internet and among mathematicians. What kinds of responses have you received?
Most of the comments have come from people who are grateful to me for mentioning things that haven’t necessarily been discussed, but also for identifying some of the deep, underlying things that cause us to do what we do. I think a lot of people, especially women and minorities, have expressed to me how important it was for somebody to say that. We’ve been having discussions like this in smaller conversations, and a lot of time it’s preaching to the choir, and so having somebody say that in a big address at the national meeting I think felt important and helpful to them.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
What do a Web 1.0 pioneer, a Russian-born fisherman, and a scientist who shoots lasers into poop for a living have in common? America’s first 100 percent vegetarian trout.
Bill Foss, Kenny Belov, and Rick Barrows have spent years weaning their farmed fish off of industrial fish food. You see, even though commercial fish farms can be more sustainable than ocean fishing, the *food *that fattens up those aquatic livestock—made from things like soy, corn, chicken meal, blood meal, and fish meal—is less virtuous. Humans have to hunt fish in the ocean and grind them up into food pellets so that fish in tanks might live.
Last year, Foss and Belov, who own a trout farm together, and Barrows, their diet formulation expert, entered an international competition designed to accelerate the development of fish diets made with novel ingredients. The F3 (fish-free-feed) challenge is a race to sell 100,000 metric tons of fish food, without the fish. Earlier this month, start-ups from places like Pakistan, China, and Belgium joined their American competition at the Google headquarters in Mountain View, CA, showing off feed made from seaweed extracts, yeast, and algae grown in bioreactors.
The timing of the competition is no accident. People eat 150 million tons of seafood every year, and as of 2014, more than half of all those fish, shrimp, and bivalves are raised on farms. But aquaculture is still a very young science, especially when it comes to what farmers feed their fish. “The availability of fish meal has made for really lazy nutritionists,” says Barrows, who recently retired from the USDA Agricultural Research Service, where he spent decades evaluating ingredients and formulating hundreds of fish diets. “You can pretty much put in 50 percent fish meal and you’re set.”
Fish meal—dried and ground up fish bits—and its more lubricious counterpart, fish oil, are made from cheap species that humans don’t eat that much: sardines, herring, anchovies, krill. But lots of other ocean animals do eat them; they’re kind of the linchpin of marine ecosystems. Lose the forage fish, lose a lot more. And as those forage fish catches are getting smaller, fish meal and oil-based diets are getting more expensive. Since 2012, prices have risen more than 80 percent. “Aquaculture is growing so fast that it can’t possibly continue to use any more,” says Kevin Fitzsimmons, a biologist at the University of Arizona and former president of the World Aquaculture Society. “Forage fish are just maxed out.”
So Silicon Valley investors—some of whom are observing at the F3 challenge—are finally ready to double down on sustainable aquaculture. “All these early adopters see the writing on the wall," says Foss, who co-founded a little company called Netscape before he became a trout farmer and sustainable seafood distributor. "The competition just gets them into the same room to help each other see that yes, there is a way forward.” Still, finding the right combination of these alternative ingredients is no easy feat. Lucky for Foss and Belov, Rick Barrows isn’t your average lazy nutritionist.
Outside the Bozeman Fish Technology Center in Montana, a layer of freshly-fallen snow covers the long rows of outdoor fish ponds, drained now for the winter. Inside, it’s a light dusting of finely powdered wheat and corn and soy particles that covers the heavy machinery. Commercial extruders heat up the feed mixture and squeeze it out of tiny holes, making pellets anywhere from 250 microns to 9 millimeters across. There are pulverizers and roll grinders and even something called a spheronizer, that, you guessed it, makes tiny, perfectly uniform spheres. It’s not standard equipment for most feed makers. Barrows got it from a pharmaceutical company in Japan.
When everything is running at once, it gets really loud in here. And hot. This sweltering building is where Barrows makes all the feeds he’s come up with over the years. To test them, he heads over to a nearby warehouse lined with dozens of big blue plastic tubs for raising fish, all snaked together with hundreds of feet of PVC pipe. Spring water from Bridger Creek flows through on command.
The first step is to gauge hatchling approval. For six weeks they feed some baby trout the new diet, while others get a control diet. If it performs well, they test how well the fish take in the nutrients and if they like the taste. To do that, they hand-feed older fish until they get full, monitoring how much they eat, how the fish grow, and how healthy their poops are. That’s where the lasers come in. Barrows uses them to measure a metric he calls “feces durability.” A lot of plant-based diets give trout diarrhea.
“Fish, like other animals, require nutrients, not any specific ingredient,” he says. “People say I’m turning a lion into a vegetarian. But that’s not what we’re doing. We’re turning soybeans into meat." The latest feed Barrows has formulated for Foss and Belov uses omega-3 fatty acids from algae grown in Brazilian bioreactors. They estimate that they save about 40 metric tons of wild-caught fish for every ton of algae-derived omega-3s they use.
The feed also contains ingredients like flax oil and a meal made from the ground-up rejects of the California pistachio industry. (About a quarter of the nuts are too broken or off-colored to make the cut.) Together it provides a balanced, nutritious meal for Belov and Foss’s rainbow trout, which are raised on a farm they own in Susanville, CA. But they’re always looking at new ingredients—like barley proteins and black soldier fly larvae. The goal is to have an arsenal of regionally-produced proteins, so that if one commodity spikes, they can just switch over to a different one.
At the moment, Foss and Belov are their own best customers. They make about 325,000 pounds of feed a year, which all goes to feeding the trout at their farm. At that rate, they know don’t have a hope of winning the F3 challenge. But they also never set out to be feed manufacturers in the first place. They just did it because no one else was. And whether they wanted to or not, they’re now in the business. Last week they sent their first shipment to a trout farm outside of Mexico City. If all goes well with the trials, they’ll officially have their first feed client. It might be a small step. But Foss says the market will do the rest.
“You can see these two trajectories, where our feed prices are coming down and fish oil and fish meal are going up,” he says. “Where those two meet is when we stop being a niche project and start being a mainstream solution to a global problem.”
His best guess for when that happens? About two years from now. So set your sights on 2019 for vegetarian fish suppers—but don’t be surprised if the Google cafeteria beats you to it.
Tears streamed down Luke Kuechly’s face as he sat on the turf. Nothing about the the Carolina Panthers linebacker’s fourth-quarter collision with New Orleans Saints running back Tim Hightower seemed especially gruesome—that Thursday night game in November had seen much harder hits, and more would follow in the six weeks of season left. But the longer the former defensive player of the year stayed down, TV cameras lingering on his slumped form, the clearer it became that something was seriously wrong. Kuechly left the field on a cart, and once again, the NFL’s concussion crisis became impossible to ignore.
The link between repeated concussions and degenerative brain diseases like chronic traumatic encephalopathy is strong enough for the NFL to have settled a $760 million class-action suit with retired players in 2014. Current policy calls for any player showing concussion symptoms to be benched until they pass tests for cognition, balance, coordination, and neck mobility. But even the most thorough sideline test would fail to identify what Robert C. Cantu, co-founder of Boston University's CTE Center, calls “the elephant in the room”: sub-concussive events. Though less dramatic than high-grade concussions, these impacts still destroy neurons, and can occur many times per game—piling on additional long-term damage with each hit.
What the NFL Can Do to Survive Its Concussion Epidemic
How a Mere Prick of the Finger Can Diagnose a Concussion
Why the NFL Sucks at Concussion Testing, And What It Can Do About It
With the world tuning in to Super Bowl LI on Sunday, the NFL will again be the biggest game on a global stage. Yet, in an era of billion-dollar broadcast deals and 4K cameras, it's still impossible to see just what kind of hits lead to brain injuries. Concussion remains largely a “hidden injury,” as theNew York Times called it in 1998. But according to Timothy Gay, a physicist from University of Nebraska–Lincoln, it doesn’t have to be. “If we could get the players to allow accelerometers in helmets, we could measure the forces on a broad epidemiological basis,” he says.
Once the province of missiles and aircrafts, accelerometers are now in everything from cars (to deploy airbags) to smartphones (to signal when to rotate the screen). Gay believes that if every NFL helmet had an accelerometer, researchers could find correlations between force, angle of hits, and incidence of concussion.
The idea isn’t new. Riddell, the NFL’s official helmet manufacturer, began working with built-in accelerometer technology in 2004. According to Thad Ide, Riddell’s senior vice president of research and product development, the technology has been used in research over the last decade at the youth, high school, and collegiate level. Riddell currently sells a helmet carrying a pad loaded with a collection of accelerometers and motion sensors paired with a processor and transmitter. The helmet measures the magnitude of impact, as well as the location, direction, and duration of the hit, sending the information back to a computer on the sideline. The entire tracking system, sold as the Riddell Sideline Response System, is currently being used at over 20 research institutions, including Virginia Tech and UCLA.
This season, University of Texas’s football team used an upgraded version of Riddell’s technology in all its games and practices: The system, called InSite, automatically processes all collision data, and alerts the coaches when a single hit, or a collection of hits, exceeds a head-impact threshold. “InSite actually alerts the sidelines based on certain levels of impact and probabilities of a player potentially suffering a head injury,” Ide says. “It's an extra piece of information and an extra set of eyes on the field that they wouldn't otherwise have."
In 2013, two unnamed NFL franchises participated in a voluntary pilot program of in-helmet accelerometer technology—neuroscientist Kevin Guskiewicz of the University of North Carolina told the New York Times that the program collected data on more than 11,000 impacts. However, the league eventually suspended the program, claiming that the sensors couldn't deliver reliable data. Brian McCarthy, the NFL’s vice president of communications, says that the NFL remains hopeful that it can one day reinstate the accelerometer program. “We want to get to a point where everyone is comfortable with the technology, where everyone is comfortable with the science, with the shared goal of improving player safety,” he says.
But the disputed data has already led to on-field rule changes. Guskiewicz’s pilot-program research identified kickoff returns as the plays with the highest risk of impact. His presentation to the league in 2011, supported by information collected from in-helmet accelerometers, proved compelling enough for the competition committee to move the kickoff line five yards forward, to the 35-yard-line. The goal was in part to shorten the running start afforded to the defensive players, and in part to result in more touchbacks, in which the kicked ball sails into or past the end zone and the play is called dead. The impact was immediate: Between 2010 and 2015, the percentage of kickoffs returned was cut in half. McCarthy highlights the kickoff rule change as one of the 42 changes related to player health and safety “in the last dozen years or so.”
While you might think athletes themselves would be in favor of in-helmet accelerometers, the NFL Players’ Association is also balking at the technology. One representative of the union, who hadn’t been granted permission to speak on the record, said that at the time of the pilot program, NFLPA engineers believed the data was too unreliable to be scientifically valuable. According to the source, when reliable accelerometer technology does become available, the union will reconsider the program.
Gay, though, speculates that players’ concerns might instead be rooted in data privacy—and the financial consequences of newfound transparency. “No player wants that kind of data out there when they go to renegotiate their contract,” he says. “Because the team is going to say, 'Look, you've had three big hits in the last year and we're just worried you're coming to the end of your rope.’” Indeed, in October 2015 the union halted the collection of off-field sleep data. The NFLPA would continue to fight, the source says, to make sure players retain control of their medical and health information.
For most medical research, personal healthcare data is delinked from patient identity. But in the case of NFL accelerometer data, true anonymity is much more difficult. “Because you have a smaller number of players and a smaller number of circumstances, it can be really hard to deidentify information and still have it be useful,” says Michael McChrystal, a professor of privacy law at Marquette University. There’s also, he adds, a burden of granularity: “Are there different concussion issues for linemen than there are for the skill players? It could be a really tricky problem from a research point of view."
But the potential public good remains pressing. While fewer than 1,700 players are in the NFL at any moment, the professional league is fed by millions of college, high school, and youth football players. A robust dataset, collected over years, could begin to uncover specifically dangerous plays, degrees of force, or tackling styles, and lead to changes that affect player safety at all levels—like Guskiewicz’s research with the kickoff line, which was subsequently adopted by the NCAA. However, without that transparency, then short of prohibition, it’s unclear exactly how to make it safe enough to play. And for the most lucrative sports league on earth, that’s just not good enough.
On December 21, Luke Kuechly walked into a press conference. He hadn't played since that hit more than a month prior, but he wore a smile on his face. "I'm doin' good," he said. "I want to play, and I think everybody knows that."
A reporter raised his hand. "You say you want to get back out there," he asked, "but you're a smart guy. You understand what happens to guys with multiple concussions at 50, at—"
"What happens to them?" Kuechly interjected.
"There's more guys who turning up with CTE if they've had multiple concussions," the reporter answered. "That's what the scientists say."
"I'm not worried," Kuechly said. "I'm not a doctor and I trust what our guys say, and I'm gonna play football. That's what I do, that's what I like to do, and I'm not concerned with that stuff until somebody tells me otherwise."
Until the NFL opens itself up to better data, maybe he should be.
A shorter version of this article appears in the February issue. Subscribe now.
Now is the winter of scientists’ discontent. A coalition of researchers is planning a protest march on Washington; rogue federal-agency Tweeters are posting inconvenient truths and anti-administration sentiments; thousands of members of the global research community have pledged to boycott American conferences. All because, as Newton said, every action has an equal and opposite reaction. In the (short) amount of time since the Trump administration came into power, it has limited the public communications of scientific agencies, removed climate science information from the internet, and banned immigration from seven countries.
The administration has yet to take many concrete positions on scientific priorities or funding plans, so onlookers don’t know much about what the landscape will look like, beyond “probably not good for climate science.” But the president ran on a nationalistic, America-first-and-great platform. That doesn’t bode well for science—especially in a world where the biggest discoveries come from collaborative, multinational teams. Now that scientists know atoms and germs and—yeah—anthropogenic climate change exist, they go deeper, bigger, and more expensive. "The scale, cost, and complexity of the facilities required to make progress on the frontiers of many of the current generation of science questions can no longer be borne by a single nation," says Marcia McNutt, president of the National Academy of Sciences.
The United States has historically been a major contributor to these “Big Science” programs—the ones that discover things like the Higgs boson and gravitational waves. Now, in addition to clamping down on communications and travel, the Trump administration has established a "looking out for Numero Uno" ethos that suggests major cuts to such funding. But if Trump tries to improve Americans' lot by reducing spending on unnecessary global collaborations, he will accomplish just the opposite: hurting American science and scientists, and slowing the progress that benefits the entire world.
The balance sheet isn’t the only reason global collaboration is good for science. Different countries have different industrial and natural resources, for one, and their citizens have perspectives and insights that make for better results. “Diversity in science breeds innovation,” says Gabriela Gonzalez, spokesperson for the LIGO Scientific Collaboration, whose instruments directly detected gravitational waves for the first time last February. Though US-based, LIGO currently includes contributions from Germany, the United Kingdom, and Australia, along with more than 1,000 scientists from 15 countries total.
LIGO’s First-Ever Detection of Gravitational Waves Opens a New Window on the Universe
It’s Time to Stand Up for the Climate—and for Civilization
Colliding Black Holes Tell the New Story of Stars
Already, LIGO is preparing to protect itself from brain drain. "We may have to arrange our meetings so that scientists can freely participate," says Gonzalez, "perhaps considering other meeting places than the US, which may affect foreign nationals in the US who have trouble getting abroad." Digital participation is a challenge, too: Regulations on information sent in and out of the country could prevent the team from planning their future hardware and software updates.
At the European Southern Observatory—which, along with American and Japanese partners and help from Canada, Taiwan, and South Korea, runs the Atacama Large Millimeter-Submillimeter Array megaproject in Chile—scientists worry about similar restrictions. “[We are], like many other scientific organizations world-wide, concerned about limiting the ability of scientists to travel in pursuit of their work, and of sharing their work with their peers,” says program manager and astronomer Wolfgang Wild.
Funding-wise, at least, Big Science isn't terribly at risk. Wild says that if a country that contributes to ALMA—the world's most expensive terrestrial telescope—wanted to cut back, the partners could jointly decide how to adjust their shares to make up the difference. The same goes for gravitational-wave science: “If the US is not a leader or major contributor, we expect that the exciting science will attract the needed funding from other sources,” says Gonzalez.
But. A country that steps back from any of these international collaborations would lose some of their rights. Astronomers wouldn’t get to use the telescope as much, for instance, or gain rapid access to data. In science, you’ve gotta give to get.
The same logic goes on the International Space Station—perhaps the greatest symbol of global scientific collaboration, running on the financial and actual backs of 15 countries. “When humanity puts its scientific brain and brawn to work," says the European Space Agency's Pal Hvistendahl, "together as one, we can make humanity progress faster than any single nation or bloc alone."
The European and Canadian space agencies didn’t comment on whether they were worried about the US changing its role on-orbit (example: “It is not up to ESA to have any opinion about internal US matters”). But Gilles Leclerc, the Canadian agency’s director general, did suggest that—no matter what may happen politically—the space station will weather it. “The ISS originated more than 30 years ago,” he says, “and there has been a continuity of efforts through many government changes.”
Western Europe captured by crew members aboard the International Space Station.
Back on the ground, international partnerships feel less certain. The president has banned immigration from seven countries, denied refugees refuge, and made both the Mexican president and the Australian prime minister mad. Also mad are the more than 5,000 scientists who have pledged not to attend any conferences held on American soil while the executive-ordered immigration ban is in place. The list of signatories contains scientists from Belgium, South Africa, Sweden, Germany, Norway, Italy, France, Ireland, and—indeed—the United States itself.
So, yes. It is winter. Scientists are discontent, as are people with more at stake than data, like lives and safety. The foundations of their work—the free flow of both information and people—are under threat. If scientists abroad can't or don't want to come here to collaborate, US scientists lose out on their insights, as well as the opportunity to train international members of the next generation of researchers. Likewise, if scientists and scientist-hopefuls can't come here for work or school, those researchers also lose out on American resources. And if data disappears or stays within certain borders, everyone loses.
Science itself will continue, as will Big Science. The question is whether the US will play a big or small part on the global stage.
It’s only in the past few decades that such planet-scale collaboration has even been possible. All of this happened not long ago: The Cold War ended, ocean-crossing flights became (relatively) affordable, internet connections became powerful and extensive enough to pipe data 10,000 miles away, online archives let researchers share papers for free, wikis let global teams update each other in real-time, email was born, videoconferencing and VOIP made cross-border conversation cheap. The degree of scientific cross-pollination is higher than it’s ever been. We've come so far so quickly, and we—all of us, because that’s who science is for—have so much to lose.
Pity the lowly humans. They’re grounded, and to make matters worse, on two legs instead of four. Bats are the true heroes around here—the only mammals to master powered flight. They're so graceful and helpful, hoovering up all those insects mid-flight.
It was destined, then, that envious humans would harness the bat’s powers in a fantastical new robot, which scientists introduced to the world earlier this week. Although the Bat Bot, or B2 for brevity's sake, doesn't yet have the greatest of battery lives, it’s an impressive feat of engineering with big implications for how designers build the drones of the future.
Birds may be the most numerous fliers, but the bat is an evolutionary marvel. The bones supporting each wing are actually crazy long digits, of all things, covered with a thin membrane. This gives it a flexibility that birds can’t touch, gifting the bat with unbelievable maneuverability to intercept insects on the wing.
That elegance requires some complicated engineering, though. A bat’s wings have more than 40 joints, but the researchers whittled that to nine—five of them controlled by mini-motors and four that are merely passive. This is fascinating from an evolutionary perspective: Natural selection settled on a far more complex system, but science was able to simplify things while retaining the functionality.
Beyond maneuverability, a fixed wing also is far more efficient than, say, a quadcopter. “We don't have to run the motors constantly because we can get lift out of the wings, so it can be more energy efficient,” says Caltech aerospace roboticist Soon-Jo Chung, one of the bot’s developers.
That efficiency and agility could be huge for rescue operations. At the moment, quadcopters don't get along with tight spaces like collapsed buildings. “You have to be able to fly without GPS because you’re underground, and you’ve got a lot of metal and rebar and you don’t have a lot of light,” says rescue roboticist Robin Murphy of Texas A&M.
A bat, though, is a natural in such an environment. (You know, that whole cave thing.) Outfitted with the right sensors and autonomy, a bat bot could search rubble for trapped humans a quadcopter could never reach. After all, if you clip a wall with a drone, that's it. You're done. “One of the nice things about that morphing sort of wing,” says Murphy, “and what’s great about bats and birds in general, is they don’t mind a little bit of a collision.” A flexible wing just might recover from an impact.
And at just 3 ounces, the bat bot is one seriously lightweight flying machine. That, too, is a pivotal consideration for rescue operations: The lighter the robot, the more weight it can afford to carry. "Anything you can do to increase the payload and maneuverability—that's really where the research is going," says roboticist Michael McCarthy of the University of California, Irvine.
But don’t expect the bat bots of the future to look exactly like bats. This is about optimal functionality, not perfect mimicry. “One thing maybe is that even if it's a fixed-wing aircraft with a propeller, maybe it can have this folding wing kind of mechanism,” says Chung. “So then it could make much sharper turns.”
After all, Batman flies like a bat but still looks like a man.
It's been a busy week in the realm of volcanoes. Let's check out some highlights:
Hawaii
The coastline of the Big Island of Hawaii continues to change this week. The lava benches near the Kamokuna ocean entry (part of Kilauea's episode 61g lava flows) have been collapsing for the better part of a few months, and those collapses look like they will be continuing. A crack has formed along the coastline near the ocean entry, and over the last few days it has continued to grow—complete with creaking and groaning at night as the block of lava moves. Most likely, this will become another piece of the island that will fall into the sea. USGS geologists from the Hawaii Volcano Observatory took some infrared heat measurements of the cracked lava bench and found that it reaches over 220ºC (428ºF) only a few meters below the surface.
The severed lava tubes continue to carry lava to the sea, creating some spectacular "hoses" of lava entering the ocean (see video above and below). It might seem like the lava is very similar to water, but it is both much more viscous (by a factor of 10,000 to 100,000!) and much hotter, likely erupting at temperatures around 1200ºC. This means that the lava can explode as it hits the ocean water and trapped steam in the magma tries to escape. These explosions are unpredictable and violent, so while these fountains of lava are impressive, they should be treated with caution—I've seen quite a few photos and videos of tourist boats coming perilously close to these ocean entries.
Tonga
While searching for pumice rafts on satellite images in the area near Tonga, a coastal geomorphologist in New Zealand discovered an active submarine eruption on January 27, 2017. The volcano in question, the ever-so-wonderfully named "Submarine Volcano III," is only about 46 kilometers (29 miles) from the capital of Tonga, but poses no threat to the area at this point. The eruption likely started around January 23, 2017 (based on back-tracking satellite images) and has produced an area of discolored water in the seas to the northwest of Nuku’alofa.
Brad Scott from GNS Science said that SubVol-III (as I'll call it) has been active a number of times over the last 50 years, with an impressive eruption in 1999 that produced a small, ephemeral island. SubVol-III is likely a composite volcano that lies under the surface of the Pacific Ocean. The summit of the volcano is likely only 10 to 20 meters below the surface, so the fact that this eruption has not produced explosive plumes (like we've been seeing at Alaska's Bogoslof) suggests that it is a fairly minor eruption.
Landsat 8 image of the eruption of Submarine Volcano III (discolored water to left) in Tonga, seen on January 27, 2017.
The eruption itself could have gone unnoticed except for the vigilant eyes of geomorphologist Murray Ford from the University of Auckland and the NASA/USGS Landsat 8 Earth-observing satellite. The NASA Earth Observatory posted images of the submarine plume (see above), showing that it stretches over an area over five kilometers (three miles) wide. This eruption from SubVol-III is the first noticed at the volcano since the 1999 eruption, but with its remote location (and being under the sea and all), other eruptions could have easily gone unnoticed. Submarine eruptions such as these are common in the southwestern Pacific.
Other volcano news:
Etna in Italy continues to rumble after its first eruptions of 2017 started last week. Video taken today (February 2, 2017) shows strombolian explosions throwing lava bombs away from the new crater that has formed in the saddle between older vents on the Southeast Crater.
Eruptions at Bogoslof in Alaska appear to have settled down some, although the current activity seems to rise and fall rapidly. The Alaska Volcano Observatory haslowered the alert status to Orange/Watch for the time being. Ash fall from Bogoslof was noticed on some nearby islands along the Aleutian chain, including in Unalaska. The island home of Bogoslof has doubled in size since the eruptions started in mid-December 2016.
Last March, a paper by a geoscientist named Rob DeConto came out in Nature. And as far as geology papers go, it was a big deal: It outlined a new paradigm for how Antarctic ice sheets are impacted by climate change. As the oceans and atmosphere warm, they don't just melt the ice from below; they create honking cracks in glaciers that make it easier for large chunks of ice to break off, slip into the ocean, and disappear. The effects on sea level rise? They could be almost twice what scientists had predicted for the end of the century.
News of that paper soon landed on the desk of Jerry Brown, governor of California and fired-up proponent of climate change science. Now, he has convened a group of seven scientists, including DeConto, to sift through that study and other recent research to calculate new projections for sea level rise—and, importantly, think about what it could mean for California’s coast. Over the next three months, the team will read, discuss, and synthesize. Eventually, they’ll arrive at numbers. And those numbers will have huge implications for the Golden State's infrastructure, planning, and the budgets that support them.
In part, Brown formed this committee because an earlier report on sea level rise, just five years old, might already be out of date. The governors of California, Oregon, and Washington commissioned that report in 2010 to describe how sea level rise would affect the west coast in 2030, 2050, and 2100. Since it was released in 2012, California agencies from Caltrans to the state energy commission have used it to make decisions about huge coastal investments and infrastructure, says Gary Griggs, an oceanographer at UC Santa Cruz who was involved with the report. He's now leading the current committee.
The stakes are high, says David Behar, the climate program director for San Francisco’s public utilities commission. A few feet of sea level rise make a huge difference for shoreline mapping, say, to figure out what areas need to be protected from flooding. Planners not only need to consider the likely level of sea level rise in 2100 (the 2012 report put it around three feet) but also the 5.5 foot rise in the worst-case scenario, plus the 42 inches added by rare but massive storms. (Infrastructure planning isn’t the most optimistic of endeavors.) With prominent, heavily-trafficked locations like San Francisco’s Embarcadero and Oakland Airport just a few feet away from sea level, the cost of a few extra feet of ocean could be billions of dollars.
Planners have developed ways to design around this uncertainty. The best way, says Robert Kopp, a climate scientist at Rutgers, is to make choices so that you don’t accidentally box yourself into a corner 20 years down the line. For example, if you’re building a levee around an island, you plan for 16 inches of rise because that’s what the projections are for 2060. But you leave yourself room at the base of the levee so that, if the numbers are wildly different decades later, you can add to the levee without having to tear anything down. “It’s a classic modular approach,” Behar says.
That strategy works, to a certain extent. But wildly divergent projections—say, a hike of a few feet in sea level—can change not only the degree of preparation, but the types of structures a planner decides to build. Do you shore up a coastline by building concrete seawalls, or mounded dirt levees, or wetlands with vegetation to catch the water? Wetlands are popular in the Bay Area, but they become much less effective if water levels get a couple feet higher.
To help the state make the best infrastructure decisions, the working group needs to decide whether DeConto’s study, and the other research that’s come out recently, is important and robust enough to throw out the old numbers. Nowadays, the climate science community is producing projections at a rapid clip, says Helen Fricker, a glaciologist at the Scripps Institution of Oceanography. But when do new findings supersede the old ones? “The panel needs to decide if this new land ice model is cutting-edge science or consensus science,” says Behar.
Then there’s the gap between arriving at sea level projections and actually translating them into good planning decisions. Typically, climate change guidance goes like this: The state funds scientists to study the literature, puts their findings on a website, and then tells planners to have at it. “This is a uniquely unsuccessful model,” says Behar. The reports are hard for the average person to understand, much less make huge decisions that require taxpayer money. Behar used that 2012 report for his planning, but “it was deeply flawed,” he says. Some parts were unintelligible to anyone without a PhD in oceanography or atmospheric chemistry.
Ideally, planners and scientists would work closely together to produce recommendations for dealing with sea level rise. And as the ocean begins lapping further inland, this close involvement will become more and more important to keep up with the quick pace of sea level rise, the research on it, and the inherent uncertainties in both. “In the end, even if climate change is global, adaptation is all local,” says Kopp. And it's local efforts like California's committee that are tackling the problem head on.
On a clear night last September, at a little Ontario airport, two pilots, two scientists, and an engineer took off in a small plane. They'd pulled the left-side door off its hinges, and a telescope poked out of the portal—not at the night sky, but at the ground below. The team was about to play a very difficult, very windy game of catch.
A couple miles away, their colleagues gathered in a trailer to lob the tiny baseballs: infrared photons, beamed from a laser that tracked the plane along its mile-high trajectory. In the craft cruising above, physics graduate student Chris Pugh and the others pivoted their telescope to catch the photons, one by one. On their best run, they caught over 800,000 photons in just a few minutes, but it wasn't easy. “Out of every 10,000 photons they sent, we’d get one,” says Pugh, who studies at the University of Waterloo. “One to a hundred of them.”
The point of this high-altitude game was to test a technology known as quantum cryptography. For decades, experts have claimed that if executed properly, quantum cryptography will be more secure than any encryption technique used today. They also say it will be one of the lines of defense when quantum computers crack every existing algorithm. But it's hard to pull off; quantum cryptography requires precise control of individual photons over a long distance. Pugh's group was the first to successfully test the technology from ground to airplane.
It works like this: The sender transmits carefully prepared photons, over optical fiber or through the air, to a recipient. The recipient reads the photons like Morse code, with physical signals corresponding to a letter or a number. Instead of listening for long and short beeps, Pugh and his colleagues measured how the photons are oriented—what physicists call polarization. In their setup, photons could be polarized in four directions, and the team translated that polarization into 1’s and 0’s: a binary message known as a cryptographic key. Using that key, a sender can encrypt their information, and only a recipient with the key can unscramble the message.
Quantum cryptography is so powerful because it's physically impossible for a hacker to steal a key encoded using quantum particles. In the quantum world, when you measure or observe a particle, you change it. It’s like Schrodinger’s cat, which is both dead and alive when you’re not looking, but immediately becomes one or the other when you look. If you try to measure a quantum key, you immediately change it—and by design, the sender will know and throw the key out. “It’s secure by the laws of nature,” says physicist Thomas Jennewein, who led the work at the University of Waterloo.
Commercial quantum cryptography products have been around for over 15 years, but they have limited range. "You can guarantee security between the White House and the Pentagon, or from the corner of one military base to another," says Caleb Christensen, the chief scientist at MagiQ Technologies, a Boston-area company that makes commercial quantum cryptography systems. "In the telecom business, that's way too short." So far people have been able to send quantum keys just 250 miles.
This tech will be important when computers become too powerful for current encryption algorithms. It takes today's computers far longer than the age of the universe to decode an encrypted message, but it’ll be a cinch for quantum computers. “It might take hours or days as opposed to age of the universe,” says Pugh.
Still, quantum cryptography won't be tech's security savior. Most hacks today are due to simple human error. “Most times when a corporation gets hacked, it’s not necessarily because someone went in and spliced into their telephone line,” says Christensen. “If you lose all your secrets because someone phishes the e-mail of your middle management, you’re not going to spend millions of dollars installing a quantum cryptography backbone.”
For those with higher security standards, the eventual goal is to deliver quantum keys to a satellite, which could make it possible to send quantum-secured messages across the globe. Last August, the Chinese Academy of Sciences, collaborating with Austrian physicists, launched a satellite called Quantum Experiments at Space Scale, although they haven’t successfully sent it a key.
Jennewein's team has been rehearsing for a satellite mission for over three years. In 2013, they started by sending quantum keys to a moving truck. Now that they've shown they can transmit enough quantum signal through a mile of Earth's atmosphere, Jennewein wants to beam a key 300 miles into the air, to a satellite in low-Earth orbit. With proper funding, Jennewein thinks his team could do it in two or three years. He's optimistic: “The airplane experiment is, in some respects, harder than an actual satellite,” he says. “A satellite has much smoother and more predictable motion than an aircraft.” Just ask Pugh.

They call it the Mighty Mug—and the idea is that it isn’t easily knocked over. Of course, you can tip just about anything over if you try hard enough. But really, how does this work? Let me be clear: I haven’t actually played with a Mighty Mug—so most of this is just physics-based speculation.
It seems the key component of this mug is some type of suction cup on the bottom. When you put the cup down, a rubber seal forms with a smooth table surface. If someone knocks the cup and the rubber bottom moves up a tiny bit, the interior air pressure will slightly decrease to a value smaller than the atmospheric pressure. This atmospheric pressure on the top results in a force that can keep the cup down.
But how do you pick it up then? Honestly, I’m not quite sure. If I had to guess, there must be some type of valve that lets air get back under the rubber seal when you lift it. That’s my super-short explanation.
Now is a good time to review the physics of a suction cup. They don’t actually suck at all. The force from a suction cup comes from the atmosphere. Let’s imagine the atmosphere as a bunch of tiny balls bouncing around.

These tiny balls are moving around, and as they travel, some of them collide with a surface. Since the tiny ball changes momentum when it collides, it exerts a small force on the area. The bigger the area, the more collisions and the greater the force. On a bigger scale, if you know the pressure and the area you can calculate the force.

If you also have air (or balls in the tiny balls model of air) below the surface, there will be two forces. One force pushing down from the collisions above and a force pushing up from the collisions below. With equal pressures, these two forces would be equal, giving a net force of zero. In real life, the pressure above and below the object are not quite the same (at least in a constant gravitational field). This will produce a very tiny upward net force that we call buoyancy. This buoyancy force doesn’t really do anything significant to the Mighty Mug, but it’s fun to mention.
Getting back to the suction cup. There is a net downward force on the suction cup because there is less air underneath the cup (and less pressure) than there is above the cup. However, this means that suction cups don’t work if there is no air. Here is a video showing what happens if you put a suction cup in a vacuum bell and remove the air. But in the end, this cup probably stays up with suction cups.
Now for a good physics estimation. Suppose I push the cup near the top, like this.

Yes, a lot of stuff is going on here. To start, of course, there is the gravitational force and the force from the push. The table pushes up as would be true for any cup. In this case, the table is only pushing up on the right side of the cup. I have put the force from the suction cup as a downward force on the left side of the cup; if you push on the top of the cup and to the right then only the left side of the suction would pull down. The last force is the frictional force, which prevents the cup from sliding.
If I want to solve for this maximum pushing force, I need to use two ideas. First, that the net force is zero. This means that the forces in the vertical direction have to add to zero as well as the forces in the horizontal direction adding to zero. Second, the total torque is zero. These two ideas together mean that the cup is in equilibrium (or not falling over).
OK, I’m going to skip over most of the details in this calculation, so I will just give a few starting notes.
Using that, I can write the following equation for the sum of the vertical forces.

Calculating the magnitude of Fsuck, I get approximately:

The ΔP is the difference in pressure above and below the suction cup—or 0.5 atmospheres. With these values, I get a sucking force of (yes, I know it doesn’t really suck) 127 Newtons. Now I can solve for the force of the table, but I can get by without it.
Now for the total torque. If the net torque is zero about some point on the object, the net torque is zero about any point on the object. I am going to sum the torques about a point on the right edge of the cup. Assuming the upward table force acts on this corner as well as the frictional force, then both of these forces produce zero torque about this point (to make things easier). My torque equation now becomes (if you want more details on torque calculations, check out this post on the mass of Darth Vader):

I decided to just push the cup at the top (because it’s simpler) and I am calling x the distance from the edge of the cup to the location of the suck force. Putting in my values and solving for the push force, I get a maximum value of 49 Newtons (11 pounds). That’s pretty impressive. Probably a normal tap would just be around 2 to 3 Newtons, but I’m just guessing. Oh, also it should be clear that this is just a rough calculation and not to be used for mission critical events involving the non-spillage of beverages.
One final note. If you want a very awesome method to prevent drink spills, check out the physics of the no-spill drink tray. It’s a great demo that you can (and should) try yourself.
As a chemical engineer who studies the motion of fluids, Bill Ristenpart deals with a lot of spattered blood and aerosolized pathogenic mouse phlegm. But when it comes to teaching wary freshman the basics of mass transfer and thermodynamics, the UC Davis professor relies on a less messy (and more potable) liquid: coffee. Beans go through so many complex chemical changes that they can easily form the basis of a whole curriculum.
Findings From UC Davis’ Java Studies
1. Research confirms that buying the same bag of beans every week doesn’t guarantee the same flavor. Even if that roast looks just as dark as your last batch, hyperspectral imaging might reveal it’s suspiciously light.
2. Hints of cardboard, anyone? Coffee tasters tend to get pretty creative with their descriptions, so the lab built a data-driven flavor wheel to standardize the way we talk about coffee. It's the first step on a hunt for molecules responsible for specific coffee flavor notes.
3. Enzyme-treated coffee grounds could become the next hot microbi­ome therapy. Like breast milk, they’re loaded with oligosac­charides, sugars that promote the growth of good bacteria in your gut.
Ristenpart's three year-old course, the Design of Coffee, has become the most popular chemical engineering class in the country, enrolling a quarter of Davis’ freshmen. After spending the semester deconstructing coffeemakers and determining pH levels by taste, the 500-odd students compete to engineer the tastiest brew using the least amount of energy. Which isn’t easy, Ristenpart says, because “we know very little about coffee.” Though Americans down some 400 million cups a day, US researchers don't typically study it; there's little incentive for agencies like the USDA to fund research on a crop grown thousands of miles away in the tropics. Nearly everything about java, from the microbial intricacies of fermentation to the molecular basis of flavor, remains a mystery.
Take the harvesting process. Coffee seeds come from a cherry-like fruit that's stripped away and discarded, producing roughly 500 pounds of waste for every 100 pounds of seeds. Studying alternative uses for that pulp is as much about public health as it is about sustainability, Ristenpart explains: Much of it ends up dumped in nearby waterways. Post-harvest, the green seeds sit around in open air pits for 20 to 24 hours, where sugar-munching microbes induce fermentation—thought to be key to producing beans with complex, nuanced flavor. But it's an untested convention. "It's astonishing to me that in 2017 there isn't consensus over whether fermentation even happens," says Ristenpart.
Those mysteries are mostly out of sight for American coffee drinkers, but recently that's begun to change. As home kitchens have become mini-coffee labs filled with small-batch specialty roasts and hyper-precision coffee scales, consumers are demanding more coffee science and data. So Ristenpart is overseeing the development of a 6,000-square-foot center—with a initial funding from Peet's Coffee—devoted to coffee research: sustainability, chemical makeup, and preparation protocols. Last month, researchers affiliated with the center released the first public genome of the Coffea arabica plant*, *which researchers hope will lead to insights into production and taste. Ristenpart's next target? The industry’s sacred brewing guidelines—calibrated, as Ristenpart tells it, to the tastes of 1950s housewives. “There are all these rules of thumb out there,” he says, “but very rarely does anyone have hard data to back it up.” Hear that, coffee snobs? Time to go back to school.
This article appears in the February issue. Subscribe now.
This morning, Senate Republicans moved Rep. Tom Price one step closer to the highest office of public health, rolling over a Democratic boycott in the finance committee and advancing his confirmation as Secretary of Health and Human Services to a full Senate vote. But while Dems still have plenty of unanswered questions about the ethics of Price’s financial dealings, health care communities are already thinking about how he might lead the agency into the future. During a confirmation hearing last Tuesday, Price came out against electronic health records, the digital histories patients make every time they see their doctor or go to the hospital. “We’ve turned physicians into data entry clerks,” he said, arguing that the burdensome recording systems need an overhaul.
He may not be wrong. For almost a decade, hospitals have been waiting for EHRs to usher in a shiny new era of standardization and high quality health care. But while federal laws and incentive programs have made health care data more accessible, the vast majority of hospital systems still can’t easily (or safely) share their data. As a result, doctors are spending more time typing than talking to patients. And it’s wearing on them; physician burnouts jumped from 45 to 54 percent between 2011 and 2014,according to a Mayo Clinic study. The number one thing those doctors would change? Streamlining the EHR process. And the most popular strategy circulating among health care technologists is blockchain.
For a refresher, blockchain is the distributed accounting platform that makes cryptocurrencies like bitcoin possible. But wait, you say! Isn’t that like what the dark internet uses to exchange drugs and cyber weapons and stuff? I don’t want that near my medical records!
Breathe. OK. While blockchain is most best known for powering bitcoin, it’s really a generic tool to keep secure data in a distributed, encrypted ledger—and control who has access to that ledger. Rather than having one central administrator that acts as a gatekeeper to data—a list of digital transactions—there’s one shared ledger, but it’s spread across a network of synchronized, replicated databases visible to anyone with access. Which gives it unprecedented security benefits. Hacking one block in the chain is impossible without simultaneously hacking every other block in the chain’s chronology.
This makes blockchain incredibly appealing to the doctors and hospitals that need secure access to a patient’s entire health history. “Now is probably the right time in our history to take a fresh approach to data sharing in health care,” says John Halamka, chief information officer at Boston-based Beth Israel Deaconess Medical Center. For the past decade, Halamka has been responsible for health care data standards in the US, first under the Bush and then the Obama administration. He sees a blockchain-underwritten future in which a patient’s every health care interaction goes into a ledger every provider can see. “The EHRs may be very different and come from lots of different places,” Halamka says, “but the ledger itself is standardized.”
Every time a digital transaction takes place, bits of code group it into an encrypted block with other transactions happening at the same time. For bitcoin, this would be a flurry of buying and selling. For EHRs, it might be all the things that happen to you on a doctor’s visit (blood work, a new prescription, maybe some X-rays). Then people validate the transactions—in health care, likely a physician or pharmacist trusted with an access key. Then the software timestamps each validated block and adds it to a chain of older blocks, in chronological order. The sequence shows every transaction made in the history of that ledger, whether it be bitcoin sales or a knee replacement procedure. Get it? It’s a chain of blocks. Blockchain.
Halamka gives a simple example: prescriptions. Say that one medical record shows a patient takes aspirin. In another it says they’re taking Tylenol. Maybe another says they’re on Motrin and Lipitor. The problem today is that each EHR is only a snapshot; it doesn’t necessarily tell the doctor what the patient is taking right now. But with blockchain, each prescription is like a deposit, and when doctor discontinues a medication, they take a withdrawal. Looking at a blockchain, a doctor wouldn’t have to comb through all the deposits and withdrawals—they would just see the balance.
And crucially for patient privacy and security, hospitals and pharmacies don’t have to send data back and forth to see it. They just all have to point to the same common ledger.
So does it work? For prescriptions, at least, initial results are promising. Halamka recently teamed up with researchers at the MIT Media Lab to test a blockchain application pilot called MedRec. Team-lead Ariel Ekblaw put the authentication log to work at Beth Israel, tracking six months of inpatient and outpatient medication data with MedRec code deployed through virtual machines at MIT. They recorded blood work records, vaccination history, prescriptions, and other therapeutic treatments, simulating data exchange between institutions by using two different databases within Beth Israel. The results were so positive that Ekblaw is already starting to plan more pilots with larger networks of hospitals.
MedRec is still an early prototype, not ready for widescale deployment any time soon. But government health technologists see its promise. Last year, the Office of the National Coordinator for Health Information Technology—the part of HHS that helps health care providers adjust to new, digital paradigms—held a blockchain competition. MedRec was one of the 15 winners, along with entries from major health care players like The Mayo Clinic and insurance giant, Humana. And in January, the Food and Drug Administration announced a research partnership with IBM Watson to find ways to safely share data from EHRs, clinical trials, genetic sequencing, and even mobile wearables using the blockchain approach. The technology is still in its infancy when it comes to health care applications, but in a recent poll of health care executives, IBM found that 16 percent of them intend to implement some sort of blockchain solutions by the end of this year.
Whether or not a Price-led HHS will jump on the blockchain bandwagon remains to be seen. The 21st Century Cures Act, signed into law in December, should push some of these interoperability issues to the top of the agency’s agenda. But officials at the ONC say nothing is happening until the new administration reviews the finer points of the law. Price’s office did not respond to a request for comment.
Halamka says no one from Trump’s team has yet contacted him, but that Price’s record on EHR leaves open the door for a blockchain future. “He likes the ability to see a patient’s longitudinal history and he doesn’t like the burdens on physicians,” he says. In a way, blockchain is the best kind of distribution of labor.
Editor's Note 12:10 Eastern: This story has been updated with the results of a Senate committee vote, following a rules suspension by GOP lawmakers.
Azi Torkamani is eight months pregnant and working 12-hour shifts at St. Joseph’s Hospital in Syracuse, New York. She's a resident physician in family medicine1. Some days she has so many patients she doesn't have time to stop and eat lunch; she eats as she walks from floor to floor treating patients. On Monday, as she took rounds in the family medicine clinic, she was distracted by a nagging worry: How am I going to manage once the baby is born?
This was not the normal worry of a first time mother. Torkamani is Iranian—and, in an instant, President Trump’s new immigration ban upended her family’s future. Downstream effects of the ban are hitting the entire healthcare system, from doctors like Torkamani to nurses and all the way to patients.
It wasn't supposed to be like this. Torkamani has a green card to work permanently in the US, and her mother had obtained a visa to come from Iran to care of her grandson for a year. But Torkamani's mother was en route from Tehran to America when the president signed the executive order banning travel into the US by anyone from Iran and six other predominantly Muslim countries. As she sat awaiting takeoff on a flight from Turkey to Dulles, officials boarded the plane and removed her. After 15 hours in custody, they sent her back to Tehran.
"I was depending on my Mom," Torkamani said Monday, on the phone in the hospital. "Now I don't know what I'll do." With Torkamani's 80-hour work weeks and frequent overnight shifts, she'll need full-time care when the baby is born—care that will be very hard to pay for on a resident's salary. Without help, she may not be able to complete her residency, which lasts another two years.
"I can't go back to Iran," Torkamani says. "I can't quit now. I had to take all medical board exams. I [spent] so much money to [qualify to] get into this medical residency. I can't just quit and say goodbye."
Torkamani is not alone. By one estimate, a quarter of all doctors in the US were not born in this country. Across the healthcare industry, from medical students to doctors to pharmacists to orderlies, lives were changed in an instant last Friday. Some, like Torkamani, because they were separated from family. Others because they were prevented from even doing their work.
According to the Association of American Medical Colleges, 260 doctors from the banned nations—Iraq, Iran, Libya, Somalia, Syria, Sudan, Yemen—applied for residencies like Torkamani's this year. The AAMC believes these doctors may not be able to practice in the US because of the timing of the order. That trend will likely continue, says immigration expert Elizabeth Cohen of Syracuse University, who is alarmed by the leaked draft of another looming executive order that would change the H-1B visa program and potentially bar much-needed nurses and high-skilled healthcare experts from entering the country.
As was widely reported over the weekend, Iranian-born Samira Asgari was on a flight to the US to start work at the Harvard-affiliated Brigham and Women's Hospital researching tuberculosis treatments when the president signed his executive order. She was forced to return to Switzerland. Seyed Soheil Saeedi Saravi was about to join a cardiology research group at the same hospital when his visa was suspended Saturday. Cleveland Clinic doctor Suha Abushamma, an H-1B visa holder originally from Sudan, was forced back to Saudi Arabia, where she had been visiting family, on Saturday. Boston University reported that two students in its School of Public Health were barred from returning to the US to study. The list goes on.
Trump’s Immigration Crackdown Could Spark a Tech Brain Drain
The Machinery Is in Place to Make Trump Protests Permanent
Trump’s Ban Leaves Refugees in Civil Liberties Limbo
And it's not just those who work in healthcare who are affected: Patients travel all over the world to be treated in American hospitals. Abdollah Mostafavi flew to San Francisco from Iran on Friday for a hip replacement surgery, but was detained at the airport, his daughter told the LA Times. Mostafavi, though, is a legal permanent resident of the US with a green card. After six hours in a room with other Iranian nationals, authorities released him.
The impact of the order isn't limited to patients from the countries it lists. The AAMC estimates that those 260 resident doctors who may now not be able to practice in the US would each have seen 3,000 patients a year: 780,000 in total. That's an especially large cut to confront as the new administration and Republican Congress plan to dismantle the Affordable Care Act, which will result in millions losing health insurance. The effects could be bleak: fewer people actively seeking out preventative and nonemergency care; more people showing up to emergency rooms for high-cost and higher-risk treatment. Not to mention, the 1AAMC1 projects the US will face a shortage of 90,000 doctors by 2025.
The country already suffers from a massive nursing shortage, which would be worsened if the president signs the order changing the H-1B visa program. "Since the 1940s we've been not only recruiting nurses from other countries but actually in some cases getting people into training abroad and then bringing them to America," says Cohen. "This H-1B shift could really reduce the population of highly skilled doctors and nurses."
Hospitals and medical schools are scrambling to figure out how to support those affected in their communities. George Q. Daley, dean of Harvard's medical school, sent a letter to all staff on Sunday expressing outrage over the executive order. "The impact of this executive order on faculty, students, and staff has the potential to resonate far into the future, and to alter how we work with colleagues around the globe and how we care for international patients," he wrote. He noted that Harvard’s Global Support Services were reaching out to all students to give them advice. Boston University President Robert Brown said Monday that BU was in contact with all 118 members of its community from those nations, and that the school would be offering guidance and holding forums for the international community.
The president of Partners HealthCare, which manages Brigham and Women's Hospital among many others, wrote to staff on Monday. "Partners has been in close communication with both Governor Baker and Attorney General Maura Healey," the statement said, "who are working together to pursue short term remedies via the court system. The longer term outcome is obviously impossible to predict at this early stage." Brigham doctors in white coats joined a protest against the ban on Saturday. Like so many hospitals, it depends on international workers, and workers specifically from the nations banned by the new order.
Much is still unclear. One of the dangers of racing to implement a vaguely-worded order without preparation is that people on the ground don't quite know how to handle or interpret it. That can lead to abuses and mistakes. This, too, has had direct health consequences. At JFK on Friday, a woman with diabetes was detained without access to medication for hours. Turkish authorities forced Torkamani's mother out of her wheelchair in order to move her to a holding area before they sent her home to Iran. They made her drag two large suitcases when she could barely walk. "They treated my mom like a criminal," says Torkamani. "She told me, 'I got numb in all my hands and legs and I couldn't talk. I was about to faint.'" Torkamani's due date is in 10 days. When she thinks about meeting her first-born child, she, too, is speechless with fear.
*Correction 9:38pm ET on 2/1/2017: An earlier version of this story incorrectly abbreviated the Association of American Medical Colleges. Additionally, Torkamani's last name was misspelled. Her residency type as well as one quote have been altered to match the speaker's intentions. *
You may have heard me say this before, but I firmly believe there are few topics more fundamental to study than the workings of our planet. The earth sciences aim to unravel how the lithosphere, atmosphere, hydrosphere, and biosphere operate—and how they operate together. It is a science of synthesis. And it’s one that needs to move forward, both because of the great service the earth sciences perform for society and the understanding of world-shaping processes that they advance.
Now, earth scientists are not always the first researchers you see on TV or in articles about science, even if the topic is plainly within their realm. Since the days of Carl Sagan, the physicists have carved out a nice role as the most prominent popular science communicators. There isn’t anything wrong with that. But earth scientists are experts on the Earth—and when questions arise about climate, hazards, and resources, it makes sense to reach out to the researchers who have the most knowledge and expertise in those earthy topics.
Even though physicists have the strongest voice in science communication, there are many more outlets for earth scientists to spread their knowledge. The free flow of communication from scientist-to-scientist and scientist-to-public is the key to making sure the country continues to value science: funding research across all sciences, appreciating and acting on the results, and continuing to give scientists the freedom to accumulate and interpret data to draw evidence-based conclusions. This isn’t some vast conspiracy of scientists trying to get rich, but rather people dedicated to knowing how the planet operates.
If you’re excited about learning about earth science, a great way to start is to follow earth scientists and earth science organizations on Twitter. Many of us strongly value communicating with the public, mostly because we’re just that excited about what the science is doing and discovering. So I’ve created a list of people and organizations to follow to get you started in earth science. Scientists are people, so expect a lot of science but also a lot of other things that make them tick (or make them angry, or thrilled).
This, by no means, is an all-inclusive list. Rather, it’s a cherry-picked list of my favorites. If you have other earth scientists who you think need a follow, leave them in the comments and I can amend the post over time—especially with international and non-English language geoTweeters. Give me a reason to add them so I can add details for people looking for science on Twitter.
Enjoy!
1861-2 atmospheric river storm destroyed 1/3 of CA’s taxable land: much more damage than 1857 #Tejonquake and 1906 SF quake.
— Dr. Lucy Jones (@DrLucyJones) January 9, 2017

@drlucyjones: Dr. Lucy Jones is an earthquake expert who recently retired after years of service with the U.S. Geological Survey. She quells a lot of the crazy ideas and “predictions” people have about earthquakes, especially in California.
When I do science, I work with scientists of all ages, colors, genders and nationality. When we are inclusive, we do better. #USofScience
— Anne Jefferson (@highlyanne) January 20, 2017

@allochthonous and @highlyanne: This duo write Highly Allochthonous and run all-geo.org. Both faculty at Kent State University, Anne Jefferson studies water, especially in urban environments while Chris Rowan works on paleomagnetism and tectonics. Both are strong advocates for science, science communication and diversity/inclusivity in the sciences.
Great paper that I refer back to time & time again. https://t.co/5Y4xZmHqPv
— Tessa Hill (@Tessa_M_Hill) January 27, 2017

@Tessa_M_Hill: Tessa Hill is a professor at UC Davis and an expert on climate and oceans. She is also (along with the aforementioned Anne Jefferson) an AAAS Public Engagement Fellow.
Finally got my @geosociety lecture tour talks onto @figshare! First, ‘The Geological Hubble’: https://t.co/PgP4t8IlnW…(1/n)
— Christopher Jackson (@seis_matters) January 27, 2017

@seis_matters: Christopher Jackson studies basin analysis (in other words, how sediment and tectonics creates and fills valleys and depressions on Earth) at Imperial College. He’s also about to head off on a lecture tour for the Geological Society of America.
#FridayFeeling one of first experiments trying to make #lava go under #water #syracuselavalab https://t.co/tDmqdbOtfE – lava wasn’t keen
— Ben Edwards (@lava_ice) January 13, 2017

@lava_ice: Ben Edwards is a volcanologist at Dickinson College working on volcanoes in far-flung places like Iceland and Chile. He also likes to dabble in making his own volcanoes with the help of the folks at Syracuse University.
Kennda Lynch @marsgirl42 : need terrestrial analogs for Mars groundwater-fed lakes like paleolakes in Western USA @AstrobiologyNAI @saganorg pic.twitter.com/BMUapg9Sy5
— Jennifer Glass (@methanoJen) January 17, 2017

@methanoJen: Jen Glass is faculty Georgia Tech and studies geobiochemistry and how microbes impact methane production. She’s a prolific tweeter and activist as well, trying to protect the freedom of speech for scientists and making science as open and accepting as possible.
Beautiful, snowy Kamchatka via @NASAEarth https://t.co/gfhNrtifr7 pic.twitter.com/ZWjdj9xUiD
— Allen Pope (@PopePolar) January 20, 2017

@PopePolar: Allen Pope studies glaciers, snow and ice using satellites. He’s an invaluable resource for keeping track of how climate change in impacting our frozen water.
Any young girls out there interested in geosciences, @USGSVolcanoes helps host GeoGirls on Mount St. Helens: https://t.co/xzuPYaZ1cN pic.twitter.com/yOEoLkHEQ7
— Janine Krippner (@janinekrippner) January 27, 2017

@janinekrippner and @alisongraeting: Together, Janine Krippner and Alison Graetinger keep track of all the volcanic events that even I have troubling keeping up with. Beyond that, they run In the Company of Volcanoes as well.
#whyIMarch Because science and reason matter. #womensmarchSF pic.twitter.com/kl9RuNk1Kv
— Jessica Ball (@Tuff_Cookie) January 22, 2017

Can’t educate about climate science without the data! Thank you @ClimateCentral – looking forward to your next 100 days #climate100 https://t.co/yOC0O4wko8
— Dr. G (@guertin) January 21, 2017

@guertin: Dr. Laura Guertin is passionate about geoscience education and getting students involved in research to get them excited about earth sciences.
Who knew you could go hiking on a volcano in the East Bay? (A very old and folded one…) pic.twitter.com/lR74Jh69aR
— Jessica Ball (@Tuff_Cookie) January 17, 2017

@tuff_cookie: Jessica Ball is a volcanologist currently with the USGS and a former Geological Society of America Congressional Fellow. She also writes Magma Cum Laude on the AGU Geoblogs.
Scientists offer an empirical method of assessing future risk. Policy makers ignore them at their citizens’ peril.https://t.co/kHh3dVrFen
— Callan Bentley (@callanbentley) January 28, 2017

@callanbentley: Callan Bentley is faculty at Northern Virginia Community College and a prolific blogger, tweeter and geologic illustrator.
Sneak preview. Design by Dot Little, published by @BodPublishing Available 10 February https://t.co/JxyL4XGo8M pic.twitter.com/VSGLhkwBrU
— David Pyle (@davidmpyle) January 13, 2017

@davidmpyle: David Pyle is a distinguished professor at Oxford University, an author of several books on volcanoes and an important drive of Oxford Sparks and STREVA (strengthening resilience to natural hazards in volcanic areas).
Lottery-style program to subsidize earthquake retrofitting of your California home. Register by Feb 27. https://t.co/g2ZvkPdKDJ https://t.co/71E7bv4FgJ
— Austin Elliott (@TTremblingEarth) January 26, 2017

@TTremblingEarth: Austin Elliot is a post-doctoral researcher studying active tectonics and also writes (and tweets) about earthquakes.
If you need even more (and you do), Ron Schott (@rschott) keeps an exhaustive list of geologists on Twitter.
New: Italy’s Etna has its first eruption of 2017, Erta’Ale has first lava flows in decade and Alaska keeps busy: https://t.co/74Dd2mnVgY
— Erik Klemetti (@eruptionsblog) January 27, 2017

And there is also yours truly: @eruptionsblog
(Note: Some of these accounts are US government agencies, so I can’t guarantee they’ll remain the great sources of science information moving forward.)
All #volcanoes in the #Cascade Range of Washington and Oregon are at normal background levels of activity this week. https://t.co/B13e6mk1wr pic.twitter.com/pgbT52b81n
— USGS Volcanoes (@USGSVolcanoes) January 27, 2017

@USGS and @USGSVolcanoes: The U.S. Geological Survey has a ton of useful Twitter accounts to shell out lots of great information. You can start with the main survey account and I, of course, recommend the Volcanoes account to keep track of all the rumblings of the volcanoes monitored by the USGS. If earthquakes interest you, try @USGSBigQuakes.
“These results support the recommendations of the Paris Agreement…”
Via #AGUblogs from #AGUpubs @AGU_Eoshttps://t.co/i6YczM8QWl
— Am Geophysical Union (@theAGU) January 28, 2017

@theAGU and @AGU_Eos: These two are the main account for the American Geophysical Union and the Union’s magazine, Eos. You’ll find tweets on new earth science research, posts from the great AGU blog network and statements of policy from AGU (@AGUSciPolicy), including their letter denouncing restrictions on how scientists can communicate. If you’re into volcanoes, you can also try the @AGUvgp account for the Volcanology, Geochemistry and Petrology group.
Not all #mummies come wrapped in linen. This #PhotoFriday, see incredible photos of alpine animal ice mummies: https://t.co/92bm6UR4Ba pic.twitter.com/L4JZ2CROJc
— GlacierHub (@GlacierHub) January 27, 2017

@GlacierHub: The account for Glacier Hub, a nexus of information about glaciers and the impact that changing climate has on glaciers along with the communities impacts by glaciers.
New post, old trowelblazer: Gertrude Bell by @artefactual_KW featuring an awesome hand-drawn map of her exploits https://t.co/KZiDLmXmPG
— trowelblazers (@trowelblazers) January 26, 2017

@trowelblazers: This account sends out links and information about the multitude of women who had played important roles in the history of archaeology, palaeontology & geology along with what women are doing across these fields today.
ICYMI on our #SpeakingofGeoscience Blog:
‘My DHA-RA @GeoCorps Experience in Washington, DC’https://t.co/RmwpiKES6e https://t.co/qSnfylIvM4 pic.twitter.com/T1AyHsztEK
— geosociety (@geosociety) January 27, 2017

@geosociety: The Geological Society of America, bringing you new earth science research and links to fascinating articles on our planet from across the internet.
How did our planet get its #water? Find out over on this great post on for #EGUBlogs: https://t.co/AEYlSyJLkE #AGUBlogs pic.twitter.com/JJnQ7QCrVK
— EGU (@EuroGeosciences) January 28, 2017

@EuroGeosciences and @EGU_GMPV: The European Geosciences Union, the European twin of AGU along with their section of Geochemistry, Mineralogy, Petrology, Volcanology.
New paper from @volcano_mel et al using seismic data to unravel quiescent-explosive transition at SHV. https://t.co/OTYA6zbs8s pic.twitter.com/X1jD6dy6Ga
— STREVA (@StrevaProject) January 10, 2017

@STREVAProject: I mentioned the STREVA (strengthening resilience to natural hazards in volcanic areas) Project above, but it deserves it’s own listing. The Project looks to help areas in danger from volcanic activity through outreach and research.
You should also check out my post on the various volcano observatories you can follow on Twitter.
Journalists (again, just some of my favorite science journalists, but there are many, many more out there!)
Eek, a close call in space tomorrow. #Spacedebris threatens 1 of the SWARM magnetic-field satellites. Controllers may boost its orbit. https://t.co/hneqoamoEH
— Alexandra Witze (@alexwitze) January 24, 2017

@alexwitze: Alexandra Witze writes for Nature and is the co-author of Island on Fire about the eruption of Laki in Iceland.
Curiosity did a wheel survey today. Lookit the pebble rolling around inside the wheel, tik-tunk, tik-tunk, tik-tunk… pic.twitter.com/Hp72ONUcPJ
— Emily Lakdawalla (@elakdawalla) January 28, 2017

@elakdawalla: Emily Lakdawalla writes for the Planetary Society about space exploration and extraterrestrial geology.
Dear California: A few good storms doesn’t end a major drought. https://t.co/vmpZqDHKnp
— Mika McKinnon (@mikamckinnon) January 27, 2017

@mikamckinnon: Mika McKinnon is a trained geophysicist and a freelance science writer (along with sci-fi science consultant)
Last time the Earth was this warm, sea level was a whole lot higher https://t.co/1Hyyb0YIRq
— Scott K. Johnson (@SJvatn) January 23, 2017

@SJVatn: Scott Johnson is science journalist for Ars Technica and science editor for Climate Feedback.
He visto cosas en mi vida que pocos han visto, pero esto me ha superado por completo. Parte de mi corazón se quedó en Amatrice. https://t.co/3VFaMG6OtG
— davidcalvo (@teideano) December 12, 2016

@teideano: David Calvo is a member of INVOLCAN (@involcan), a volcano monitoring group in the Canary Islands and a radio/TV personality covering science.
Sinkhole is where Temescal Creek, long culverted, used to run 100 years ago. Another creek, the former Kohler Creek, was assigned its name. https://t.co/12ve3yOaVs
— Andrew Alden (@aboutgeology) January 23, 2017

@aboutgeology: Andrew Alden has been writing about geology on the internet longer than most people and has wrote some fascinating stuff about the geology of the Bay Area (amongst many other topics).
Died #OTD in 1873, Victorian geologist Adam Sedgwick who teached a young Darwin the skills of a field geologist https://t.co/vL7eNSuxHB
— David Bressan (@David_Bressan) January 28, 2017

@david_bressan: David Bressan writes about geology for Forbes, tackling the long and winding history of the discipline.
Crazy times in the #Arctic — “What has happened over the last year goes beyond even the extreme.” https://t.co/DMrjh9o63f
— EARTH Magazine (@earthmagazine) January 25, 2017

@earthmagazine: As the title implies, Earth magazine digs deep into the planet to cover all the news and research in geosciences.
Looking at Earth (I’m a big fan of satellite views of Earth, and you should be too!)
Despite the winter storms, there is still a long way to go before California makes up its snow-water deficit. https://t.co/cyWFGk5zzJ #CAwx pic.twitter.com/xIfHSoy2YH
— NASA Earth (@NASAEarth) January 28, 2017

@NASAEarth: The NASA Earth Observatory has thousands of amazing images of the planet and keeps them coming on our ever-changing world. They also help us visualize the vast amount of data that can be collected by earth-observing satellites.
Glaciers in Alaska’s National Parks: Monitoring Change https://t.co/eNMrNu61LI #storymap
— NASA Landsat Program (@NASA_Landsat) January 25, 2017

@NASA_Landsat: This account follows all the great work done by the NASA/USGS Landsat missions that watch the planet.
Unnamed Location (77.0°N 15.8°W) September 11th 2015 pic.twitter.com/N9534SmWnj
— Landsat Bot (@LandsatBot) January 28, 2017

@Landsatbot: This automated account is great if you like random satellite views of the planet from Landsat 8. Sometimes they are an area of geologic wonder, sometimes they are “unnamed location” in the middle of the ocean or arctic.
Extensive fresh lava flows on Ethiopia’s Erta Ale Volcano. Images from January 16 and 23. pic.twitter.com/UlcuNyqLaJ
— Planet (@planetlabs) January 28, 2017

@Planetlabs: Their armada of Doves are some of the newest earth observing satellites.
The robots are coming. They’re coming to drive your car, they’re coming for your job, and they’re coming for your heart. Like, you may literally have a robotic heart one day.
That is, if a peculiar new device is any indication. Researchers have developed a robotic sleeve that fits over the heart (well, a pig’s heart at the moment) and pumps like the organ would itself. The idea is that if a patient is going into cardiac arrest, the best way to help the heart is to be the heart. One day that may mean patients with cardiac problems could get their own robotic heart to kick in if their ticker starts to give way.
This is the vanguard of a new breed of robots that not only get along with humans (when was the last time you had a pleasant experience with a crushingly powerful industrial robotic arm?), but interact safely with their flesh. And it’s forcing humanity to reconsider what a robot even is in the first place—because more and more, the robots will become a part of us.
So, the heart robot. Its “muscles” are made of silicone, compressing and twisting thanks to a series of actuators powered with air. In the lab, the researchers chemically induced cardiac arrest in pigs wearing the device, then monitored the electrical output of their heartbeats. "In the study, we ended up using a pacemaker to override the electrical activity of the heart, and pace it so we were controlling the rate at which the heart beat," says lead author Ellen Roche, a biomedical engineer at the National University of Ireland, Galway. "Then, with that same signal, we were controlling our device." Working with the heart, the robot increased blood flow through the aorta by 50 percent.
Doctors already use implantable pumps called ventricular assist devices to help ferry blood out of a weakened heart to the rest of the body. “But because the blood then is in contact with foreign materials and moving parts, it can risk clotting,” says Roche. That means patients have to take blood thinners, whose side effects include uncontrollable bleeding—not ideal.
What Roche’s robot does is supplement the heart, not replace it. And because it’s made of a pliable material—making it what’s known as a soft robot—it’s less irritating to flesh.
Expect to find these kinds in your body at some point, because humanity is in a great robotic transition. New soft robots made from silicone or rubber do away with the clunkiness and are more, well, squishy and inviting. Plus, they're generally cheaper to make. “A lot of people are scared about robots,” says roboticist Pietro Valdastri of Leeds University. “They still think about the Terminator. Those are mainly the robots of the past, now the field is evolving toward collaborative and safe robots, that's for sure.” They're great big softies, really.
So think robotic factory arms that can sense humans around them, as opposed to just crushing intruders. But also robots that heal us from the inside. “A conventional robot is rigid joints and programmable kinds of motions—it's metallic hinges,” says Roche. “But more recently the whole field of soft robotics has been growing a lot, and more the way I think of a robot is something that can design a task and be completely under user control or be preprogrammed to complete a task.” That means that a robot doesn’t always need to have legs or wheels. Sometimes it’s just a sleeping bag wrapped around a heart.
The FDA, then, is about to see a flood of robotic medical devices (Roche plans to take her robotic heart to human trials after more extensive animal trials). But whether an implantable device qualifies as a “robot” isn’t really of the agency’s concern. “The device’s intended use and indications for use, not necessarily whether it is implanted, determines its regulatory pathway,” says FDA spokesperson Stephanie Caccomo. Each device is judged on its own merits, not whether it happens to be a robot.
Still, get ready for robots in your body. But don’t worry, they’re soft and darn near cuddly—for your heart, at least.
Gun violence research is notoriously hard to fund. Since 1996, when an National Rifle Association-pressured Congress pulled all Centers for Disease Control and Prevention funding for gun-related research, UC Davis epidemiologist Garen Wintemute has shelled out over $1 million of his own money to keep his Violence Prevention Research Program going. That investment has paid off; 20 years later, the UC Davis program is one of the best in the country, and this week published a study finding registered California gun owners with a DUI or other alcohol-related crime on their record are four to five times more likely to be arrested for crimes involving a firearm.
These findings are consistent with national thinking—many states, from Maryland to Pennsylvania to Indiana, have laws that restrict alcohol abusers' access to guns. But prior to this study, that common sense didn't have hard data to back it up. Without CDC funding, researchers like Wintemute have had to cobble together smaller sums available through the National Institutes of Health, the National Institute of Justice, whatever they can scrape up from private foundations, and new state-funded projects like California's Firearm Violence Research Center. But greater challenges lie ahead. Between a GOP-controlled Congress and an NRA-friendly president, gun violence researchers worry they'll share climate scientists' cash-free fate.
Congress Refuses, So California Funds Its Own Gun Violence Research Center
America’s Got a Gun Addiction. These Numbers Prove It
Why America Still Doesn’t Have Any Good Data on Guns
Wintemute's research isn't political: Data never really is. His most recent work merely shows a strong correlation between alcohol and gun abuse, even stronger than other variables he and his team tracked: being young, being male, having a history of violence.
But gun violence research becomes political when its results suggest policy change—usually, further restrictions on gun ownership. And the tension between data-based policy recommendations and pandering to political interest groups like the NRA results in toothless laws. "State law all over the country says if you abuse alcohol, you shouldn't be allowed to use firearms," says Wintemute. "In theory, the challenge isn’t to sell people on the idea, it's changing how we enforce it."
Some states' laws regarding guns and booze are subjective and open to interpretation. Many prohibit alcohol abusers from buying guns, but who's to say where that line should be drawn? "We should do for drinking and shooting what we did for drinking and driving—create something quantitative and enforceable," says Wintemute. In Washington, DC, for example, two DUIs disqualifies you from gun ownership. With more laws like that, Wintemute argues, it would be easier to collect data on the effectiveness of enforcement measures and design new, more effective ones.
That data might be a long time coming. Trump's team is much cozier with the NRA than the Obama administration had been, which has emboldened them to pursue a more aggressive agenda. "They are pushing for a national reciprocity law that says if you’ve got a gun permit in any state, you can take that gun anywhere in the country," says John Donohue, a professor of law and economist at Stanford University who studies gun policy. That's bad news for gun control-friendly states like California and New York, and for reputable researchers. "The federal government probably will not fund any research, unless they throw money to people who they know will come out with some NRA-sponsored conclusion," says Donohue.
The federal funding channels that still exist are drying up: The NIH started calling for firearm violence-related research in 2013, but the program expired this month—and predictably, Congress hasn't renewed it. The National Institute of Justice put out a request for gun violence research proposals last year, and while the new administration hasn't retracted approval yet, scientists are skeptical: "It's looking pretty grim, grimmer than usual," says Frederick Rivara, an epidemiologist at the University of Washington's School of Public Health. "When Sessions takes over the Department of Justice, I wouldn't expect that to last."
That leaves just a few solutions for researchers looking to study the impact of firearms and stay cash positive. Wintemute and Rivara expect private groups to shoulder some of the burden, and hope that state and local research funding efforts will continue. The country already has two good examples in California appropriating $5 million to establish its Firearm Violence Research Center, and Seattle allocating $153,000 for similar injury prevention research.
And contingency plans are underway to protect what data researchers have. "There are efforts to download the firearms violence data compiled by federal agencies, in case that data disappears," says Wintemute. "Climate change researchers have been doing the same. It's important to recognize that this is not specific for firearm violence research." From environmental research to Zika virus prevention, the GOP-controlled Congress has proved itself tightfisted in funding politicized scientific inquiry—even when that research is vital. If these trends continue, gun violence research might be another Congressional casualty.
In December of 2015, Magdalena Sanz Cortes was seeing patients at Texas Children's Hospital and teaching classes in the ob-gyn department at the Baylor College of Medicine, when she got a call from Barranquilla, Colombia. Miguel Parra Saavedra, a gynecologist in the coastal metropolis, was worried by all the patients coming to his office showing signs of the mysterious Zika virus that had hit Brazil less than a year before. Alarming reports of microcephaly—abnormally small heads and brain damage caused by the Zika virus—were beginning to come out of Colombia’s neighbor to the east. Sanz Cortes and Parra Saavedra decided to start monitoring Barranquilla’s pregnancies to see if a similar wave would hit Colombia.
It never did.
In Brazil, more than 2,300 Zika-infected babies have been born with microcephaly since 2015. In Colombia, the world’s second largest outbreak has produced far fewer: only 82. Taking total population into account, that's still more than an order of magnitude less. The disparity has bewildered public health officials, and caused many to question the link between the birth defect and Zika. The US Centers for Disease Control formally declared a causal link between microcephaly and the virus in April of 2016, citing a compelling accumulation of data from Brazil, including finding Zika virus in the brain tissue of affected infants. But Colombia's microcephaly cases never materialized.
By collecting detailed brain images of more than 200 developing fetuses from expecting mothers along the Colombian Caribbean coast, Sanz Cortes and Parra Saavedra think they have found something close to an explanation: Microcephaly wasn’t appearing in just a few cases, it was appearing in just the worst cases. Zika was still causing significant brain damage even in babies without below average-sized skulls.
“We now suspect that microcephaly is just the end of the spectrum,” says Sanz Cortez, who presented the results of her team’s study at a meeting of the Society for Maternal-Fetal Medicine on Friday. “But the brain tissue stops growing well before that.” What this means for doctors, she says, is that simple head measurements are not enough to make a diagnosis for the suite of developmental defects caused by the virus—sometimes called congenital Zika syndrome. Physicians should also be using ultrasound and fetal MRIs to identify less obvious signs of infection. While early detection won't change outcomes—doctors still don't have a treatment for the disease’s congenital effects—it’s important for expectant mothers to mentally and emotionally prepare for the challenges ahead. Especially if those challenges aren't obvious in the delivery room.
Despite the fact that those results haven't yet been peer-reviewed, they do match up with another recent study from a group of researchers in Seattle and Brazil who reported on 13 Zika-infected infants born with normal-sized heads. As they got older, these children began to develop new complications. Their heads didn’t grow as fast as the rest of their bodies; they had strange muscle weaknesses and spasms. MRIs showed that inside their skulls they had similar symptoms to the Colombian babies—decreased brain volume, too much fluid in certain brain tissues. That means there may be a lot of babies out there who have yet to show signs of congenital Zika defects. And if brain scans don’t expose them, time will.
“How these infants fare in the first year of life is an absolutely critical public health question that right now we don’t know the answer to,” says Margaret Honein, chief of the CDC’s Birth Defects Branch. She heads up a task force that monitors babies born to Zika-infected mothers in the US and its territories. They’ve learned a lot in the past year: Even moms without symptoms can pass on the virus and its congenital effects to their babies, for example. And the first trimester is probably the most dangerous time to get infected. “What we know less about is the full range of bad outcomes that can happen from these infections,” says Honein.
A California City Is Fending Off Zika by Releasing 40,000 Mosquitoes Every Week
Zika Causes Microcephaly, the CDC Confirms
Testing for Zika Isn’t the Only Way to Keep the Blood Supply Safe
And scientists know even less about how Zika causes these bad outcomes. How does the virus get into a fetus? Where does it go once it’s inside? Why does it do more damage in some than others? That kind of research is nearly impossible to do in humans—so instead, scientists are relying on monkeys to fill in the gaps.
At the University of Wisconsin, virologist Dave O’Connor has been infecting macaques with Zika and carefully tracking the virus' progress through time and tissues. During the pregnancies, his team repeatedly samples blood and amniotic fluid to determine how quickly the virus can infect a fetus. They take fetal MRIs to make measurements on how the baby macaque is growing. And once the pregnancy is complete, they analyze more than 60 different fetal tissues for little bits of Zika DNA and virus-induced damage.
They hope their studies, when combined with epidemiological data, will shed some light into the big black Zika box. Right now the CDC estimates that about 5 percent of babies born to Zika-infected moms show clinical signs of birth defects. Other studies in Rio de Janeiro put it closer to 30 or 40 percent. Getting to the bottom of that will take a real collaborative effort from the scientific community, a framework O'Connor says they're going to need going forward.
“Zika is not the only virus that causes birth defects,” he says. “It’s one of many that seems to have found a niche in a new place due to globalization, and that’s a trend that isn’t going to reverse itself.”
The threat of more emerging diseases may be lingering on the horizon, but back in Barranquilla it's still Zika that's on Parra Saavedra's  mind. While it's not easy to get a fetal MRI in Colombia, ultrasound is widely available through most of the country. If Colombia's doctors can screen for the disease's congenital defects, they may turn up more of the missing Zika cases.
Usually it takes years of study to become an archaeologist—four years of reading books and a few more learning in the field before you finally get to play in the dirt professionally. Sarah Parcak wants to get that down to a few minutes. Parcak is a space archaeologist, which means she spends her days looking for buried archaeological sites in high resolution imagery. It's a technical job: Parcak, who teaches at the University of Alabama at Birmingham, has a PhD, and so do most of her archaeology colleagues. But Parcak believes that finding and protecting all of Earth’s hidden gems is going to take more than just professionally-trained scientists.
So today, Parcak is launching GlobalXplorer, an online platform that will teach anyone with a computer and an internet connection how to spot archaeological sites in satellite imagery. Parcak first outlined her plans for the platform last year at TED, explaining that the most time-consuming part of her job was scouring thousands of square miles of satellite imagery for visual anomalies on Earth’s surface. Crowdsourcing that process, she hopes, will allow professional archaeologists to speed up the discovery process.
It's worked in the past. Since it launched in 2012, citizen science platform EyeWire has enlisted more than 200,000 people to help map the 3-D structures of neurons, resulting in a peer reviewed paper in Nature. Another, called Galaxy Zoo, is using crowdsourcing to classify galaxies in order to better understand how they form, and yet anther, Foldit, focuses on identifying the folding patterns of proteins. Archaeology, by contrast, tends to be lower-tech, but crowdsourcing makes a lot of sense when you think about how much land still needs to be explored.
To make that happen, first Parcak needs to train the rest of her collaborators. Before someone can begin looking for sites, they'll watch tutorials on spotting landscape features commonly associated with looting pits, encroachments, and underground archaeological sites. You learn, for example, that looting pits come in clusters; they often take on a contrasting color to the rest of the satellite image, and sometimes they're marked by parallel lines that signal machinery like a bulldozer was used.
Just like terrestrial archaeology, GlobalXplorer is centered around expeditions. The first is in Peru, where online archaeologists will look at more than 200,000 square kilometers of land, broken into 100x100 meter tiles scrubbed of GPS and mapping data. “We treat it like patient data,” Parcak says. Citizen scientists will click through these tiles, which increase in difficulty as they progress. People start with identifying looting, and after 500 completed tiles make their way to identifying encroachment structures, or illegal construction that happens on a site. Finally, after they've gotten the hang of looking at satellite imagery, the platform allows them to say whether or not they see some form of archaeological site. More than discovery, the goal of the platform is to eliminate large swaths of land that have no archaeological features. “All it leaves for us [the professionals] is the 1 to 5 percent of the landscape that might actually contain something interesting,” says Parcak.
To ensure quality, each player will get a score based on their accuracy. “If the people answering have a higher overall ranking, we’ll need fewer people to say a thing is a thing,” says Parcak. Once enough people say they see (or don’t see) something, Parcak and her team will look at the tile to verify that there is, in fact, an archaeological feature before passing that information along to archaeologists on the ground.
Like EyeWire and Foldit, the success of GlobalXplorer depends on a lot of people using it regularly. “We needed to create something that people wouldn’t just dip into for 10 minutes,” Parcak says. She and her team designed GlobalXplorer to work like a game. The crowd ranked score is part of that. But Parcak wanted to give people other incentives to come back. As you make your way through the platform, you unlock extra videos, photos, and stories made for the platform by National Geographic. Later, archaeologists in the field can take their virtual participants with them on digs, using video and livestreamed Periscopes.
If it works, GlobalXplorer will provide a satisfying feedback loop that directly connects citizen scientists to the people doing on-the-ground research. And that, you can imagine, is the biggest incentive of all. Because it's one thing to play a game—it's another to see with your own eyes how your contribution pushes science forward.
For many, it's the start of a new semester of physics labs. That means there are new students in that introductory course. Of course, no one is really 100 percent ready to start these labs—but that's OK. Here are three big ideas that I find students need to work on to be successful in lab.
This is a pretty easy problem to fix, but I think I should go over it. How do you convert units? Let's say some students find the mass of an object as 123 grams—but to get the weight in Newtons, they need this mass in kilograms. Here's what some students might say:
OK, we need to convert this mass from grams to kilograms. I know we have to do something with 1,000 but I can't remember if we divide or multiply by 1,000. Wait, I got it. A kilogram is bigger than a gram so we need to multiply by 1,000 to get kilograms. Wait, that doesn't make sense. Maybe it's divide.
This is the process many students use to convert values. There's an easier way. In short, you just multiply by one. Let me just start with the above example converting a mass in grams to kilograms. Suppose I have 123 grams and I multiply by the fraction 4/4, like this:

Yes, (4/4) is the same as the number 1. If you multiply a number by 1, you get the same thing. So, in this case you would just end up with 123 grams. Boring.
Any time you multiply by 1, you don't change that value. Also, 4/4 is the same as 1 because you are dividing a number by the same thing. What if I replace the denominator with (2*2)? Nothing happens because 2 times 2 is still 4.
There is another fraction that is equivalent to 1. Here it is.

1,000 grams and 1 kilogram are the same thing, so this fraction is equal to 1. Now I can multiply my original mass by this fraction.

Now for the magic. I can treat the unit of grams just like a variable. Grams divided by grams is one. These units cancel and I am left with the unit of kilograms.

Boom. That's it. Unit converted. In short, all you have to do is multiply by 1. Choose a fraction such that the numerator and denominator are equivalent (even if different units) and arrange it so the unit you want to convert cancels. It's that simple.
OK, let me include one problem that pops up quite often. Suppose I have a volume of 3213 cm3 and I want to convert this to cubic meters. Here's how I would do that (remember that there are 100 centimeters in 1 meter).

If I just multiplied by the fraction (1 m/100 cm), the cm3 wouldn't actually cancel. I need cm3 in the denominator. The only way to do this is to cube the fraction—but that's what I get.
One last thing. You might be in introductory physics and you might be a carpenter. However, it's not cool to use imperial units. It's just going to cause problems. Stick to metric units. Trust me on this one. Trust me.
Converting units isn't a super big problem—but that's not true for graphs. Just about every introductory lab needs to make a graph. Why? Here is my short answer.
With a graph, you can show a functional relationship between two quantities. If that relationship is linear then you can find the slope of the best fit line and this slope probably means something.
Let's just make a graph and then I will have something to talk about. Suppose I take a meter stick and a spring. I will hang the spring from some vertical mount and then hang some masses from it. I can record both the mass and the position of the bottom end of the spring. Here's what that data might look like.

What is the relationship between position and mass? Students are often tempted to just find the ratio of mass and length—but that doesn't work in this case (because I made the data so it would fail). No, the first step is to just plot the data on actual tree-based graph paper. I encourage students to use paper instead of a computer-based plotting tool so that it's clear that they understand what's actually going on. If you use a program to make a graph, sometimes the program will fix some things for you such that you don't make mistakes. Mistakes are good if you are trying to learn.
Now for the graph. I did this on graph paper, just for you.

I didn't just plot the data points from above—I also added a "best fit" line. On plain graph paper, you can add this line by using a straight edge to just approximate a line that comes the closest to all the points. This best fit line doesn't even have to hit any of the points—it is absolutely not just connecting the dots.
Once I have a best fit line, I can then find the slope. There are two points on this best fit line (I circled them in red) that I am going to use to find the slope. Remember to use points on the line that you created and not the points you plotted. Also, the further away the points are from each other, the better it will be.
Now I can find the slope.

Let me make some important comments on this calculation as well as some errors students make.
That's enough notes to at least get you started.
This last tip is difficult because it really depends on the instructor's ideas for the lab. So, in this case I'm giving student advice for the way I run lab classes.
Here are some things students say that make me worry.
"How many data points do you want me to take?"
"What else do you want me to do in lab?"
"Do you want me to make a graph for this data?"
I think you get the idea. Many students treat the lab as something that they (the students) need to do for me (the instructor). But I don't want minions that do lab work, I want scientists that build experiments. The lab isn't about following instructions, it's about building a model.
Yes, it's true. I do give some instructions before lab—but that's just to get students started. What I really want is for students to play around with stuff, build a model and then find some way to use that model in an interesting way. That's the way science works and that's how I want my lab to work.
Artificial intelligence is luring science into dangerous waters. To make scientific publishing more efficient, commercial publishers now rely more and more on editorial software systems. These are beginning to transform peer review from interaction between humans into interaction between humans and AI. We should think twice before allowing autonomous AI systems to decide what research warrants publication.
Janne I. Hukkinen (@JIHukkinen) is professor of environmental policy at University of Helsinki, Finland, and editor of Ecological Economics.
Peer review is the backbone of science. Researchers across disciplines trust that exposing one's research to criticism by fellow researchers improves quality and contributes to scientific advancement. As with any professional procedure, the system has its flaws. But it's still widely accepted as the best available quality control for science.
From its birth in the late 1600s, scientific publishing has grown to an estimated 28,000 scholarly journals generating more than 1.8 million articles per year in 2014. The glut in publishing has raised quality concerns, so publishers have reacted by introducing editorial management software systems. These speed up the processing of manuscripts from submission to review to publication. And they do amazing things.
I have hands-on experience with the Evise system, which Elsevier piloted in 2015 in the environmental journal Ecological Economics, which I edit. When an author submits a manuscript, Evise links with plagiarism-checking software. It suggests reviewers on the basis of the content of the manuscript or allows the journal editor to choose reviewers. And it communicates with another software program called Scopus to check the profile, scientific performance, and conflicts of interest of reviewers. During review, Evise automatically handles all correspondence between editors, reviewers, and authors. It reminds reviewers to review a paper, un-invites them if they don't respond, and invites alternate reviewers when necessary. After the editor's decision, it sends decision letters to authors and thanks the reviewers.
Whether Evise is already an AI system is a matter of perspective. John McCarthy, one of the pioneers of AI research, famously noted: "As soon as it works, no one calls it AI anymore." Evise has emergent AI features, as it can check for plagiarism and suggest reviewers. Yet decisions about quality, significance, and novelty are still in the hands of human editors.
What really matters for knowledge production, however, is who governs and regulates the use of AI. An ever-larger share of scientific publishing is in the hands of commercial publishers, with the top five most prolific publishers (Elsevier, Taylor & Francis, Springer, Wiley-Blackwell, and Sage) accounting for more than 50 percent of all papers in 2013; all use editorial software systems.
Peer review in a commercial publishing house such as Elsevier takes place under stringent efficiency requirements. Of the roughly 1,500 manuscripts submitted annually to Elsevier's Ecological Economics, about 250 are published. Each editor agrees to process up to 100 manuscripts per year, with a target time of 2 months for processing a manuscript from submission to final decision.
Without thoughtful corporate governance and public regulation, there's a danger of a runaway process here. The demand for efficiency and the promise of computerized editorial management push peer review more and more toward AI. Elsevier's human organization defends the technology aggressively. During piloting, Evise malfunctioned chronically and began acting like Hal from 2001: A Space Odyssey, sending terse letters unbeknownst to the editors. Despite pleas to abandon the system because trusted reviewers were rapidly dropping out, Elsevier resisted. They instead formed a team to tackle the problems, with some success.
Thanks to AI, Computers Can Now See Your Health Problems
If You Look at X-Rays or Moles for a Living, AI Is Coming for Your Job
Sure, A.I. Is Powerful—But Can We Make It Accountable?
Imagine for a moment that publishers continue their business-as-usual and embrace AI in peer review. AI performance will increase precisely where human editors today invest most of their time: choosing reviewers and judging whether to publish a manuscript. I don't see why learning algorithms couldn't manage the entire review from submission to decision by drawing on publishers' databases of reviewer profiles, analyzing past streams of comments by reviewers and editors, and recognizing the patterns of change in a manuscript from submission to final editorial decision. What's more, disconnecting humans from peer review would ease the tension between the academics who want open access and the commercial publishers who are resisting it.
But AI could be a powerful assistant in peer review. Institutional rules would need to specify what AI must, may, and may not do. Designing rules for AI is not that different from designing rules for humans. Put simply, AI is a uniquely rich and efficient system of intelligence built from collective human intelligence. It's important to understand AI this way, because setting limits to human behavior is likely to be more acceptable than regulating new technology.
The scenarios raise critical questions for the future of science. How will the writing of scientific articles change when authors know their text will be evaluated by AI instead of humans? AI combines superb reasoning with lesser capacity to consider values, such as ranking two scientifically equal texts on the basis of their social relevance. A peer-review system driven by AI might offer benefits, like an increase in objectivity, since a computer is judging the science rather than a complicated human, as well as drawbacks, as the AI might not be able to divine what's new knowledge. Either way AI will change how authors game peer review.
Why should humans bother with authoring? Researchers and publishers have developed instructions for how to get published. Following instructions is something AI excels in. Could not one AI system craft scientific manuscripts for quality check-up by another AI system?
And whose new knowledge does peer review by AI produce? Unhindered, AI can refine new knowledge without human intervention. Given the subtle differences between AI and human intelligence, it's reasonable to expect that an autonomous AI system will generate its own criteria for judging what passes as new knowledge.
New knowledge which humans no longer experience as something they themselves have produced would shake the foundations of human culture.
In the 1980s, during the nascent days of the satellite communications industry, Luxembourg foresaw the fat cat it could become. The tiny European nation, known for steel manufacturing and tax breaks, provided financial support and passed regulations that allowed its homegrown satellite company, SES, to thrive. And because it provided that early support, one of the globe’s smallest countries came to host the world’s second-largest commercial satellite operator.
Luxembourg liked the way that went down. And now, 30 years later, the country is positioning itself to iterate on that plot, in a different off-Earth industry: asteroid mining.
Asteroid mining is what it sounds like: going to the solar system’s hard bodies, extracting valuable resources, and using them to make something new. If humans are going to become a spacefaring species, they can’t launch all the necessaries from Cape Canaveral. Instead, spacecraft needing a top-off could get fuel from asteroid ice. Humans could scrape space materials to construct orbiting hotels. Martian colonists needn’t bring excessive carry-ons: They could draw building materials and H2O from their home body or an asteroid.
Bold forecasters speak of a full-on celestial supply-chain. In that version of the future, the entities that control that supply chain—doing the mining and selling the resources—will become very rich. They will, in a way, rule that final frontier. In 2016, Luxembourg began taking steps toward dominating the industry, and so potentially the flow of cash and commodities beyond Earth.
Let us establish a few things: First, Luxembourg is rich. According to the World Bank, its per capita GDP is $101,450, compared to the US’s $56,115. Second, it's tiny, smaller than Rhode Island with the population of Albuquerque, New Mexico. Historically, Luxembourg has attempted political neutrality, and nearly half of its residents are foreign nationals. If you believe the kinds of studies that quantify happiness, it is feeling good about all of the above. In fact, Luxembourg is a bit analogous to a utopian space colony: small, confined, welcoming of outsiders, well-off, politically and psychologically stable.
And while it doesn’t have the weight of billions of people to throw around, Luxembourg does have capital, low taxes, small fees for sending money in and out, and customer confidentiality. That’s sent many an American company to Luxembourgish (yes, a real word and the right one) banks. It’s also allowed Luxembourg to build up that bomb-ass GDP, and the knowledge that when it lures companies—or industries—in, capital continues to flow, as when it got in on the ground floor of satellite communications.
Luxembourg stepped onto space mining’s ground floor early last year, when the Ministry of Economy announced the Space Resources initiative. Key to the program, said the official statement, “will be the development of a legal and regulatory framework confirming certainty about the future ownership of minerals extracted in space from Near Earth Objects such as asteroids.” In November, the country drafted a law permitting companies to own the resources they obtain from space. It has also pledged an investment of at least 200 million euros in forms like R&D grants and purchasing equity in companies. Deputy prime minister Etienne Schneider has said that the country can also reimburse companies up to 45 percent of what they invest in R&D. Come hither, ye companies.
And they have gone thither: The two major mining players based in the US, Planetary Resources and Deep Space Industries, have now established, or will soon, legal offices within Luxembourg. “Luxembourg certainly is taking a very visionary step,” says Chris Lewicki, president and CEO of Planetary Resources. It’s been good for his corporation. The country has purchased equity in and given grants to it. Along with the European Space Agency, Luxembourg has also collaborated on Deep Space Industries' Prospector-X mission, which will use a nano-spacecraft to test some of its asteroid technologies.
In its investments, Luxembourg can double down on the space industry with impunity, in a way the US really can't. Both companies have had contracts with NASA that fund their R&D. Luxembourg can do the same through the European Space Agency, says Meagan Crawford, Deep Space’s chief operating officer. “But then on top of that, Luxembourg also does direct investment, through loans, through equity, through all sorts of different financial mechanisms,” she says. “That’s not something we see from the US government.”
It’s not just about money. “It’s about creating a legal environment in which asteroid mining is not only feasible but encouraged,” says Crawford. That’s why Luxembourg became the first European country to draft that legal framework, in November, for organizations’ right to own what they obtain from space rocks. The US has a similar law, but it applies only to people and corporations that count as United States citizens, while Luxembourg’s requires only an office address inside its borders, which essentially everyone has (don’t you?).
The money, the legislation, and the response from corporations suggest the country has ambitious goals—and perhaps a reasonable shot at achieving them. “But here we’re in the land of speculation,” says Zoe Szajnfarber of George Washington University, who studies the dynamics of innovation in the space and defense sectors. “What Luxembourg is trying to be is either the Silicon Valley of space mining, where they’re able to attract a lot of talented people and keep them there, or the Delaware of space company headquarters, where they’re attracting a lot of companies who see value in the tax advantages.” But transforming a place into Hub of X is not easy or guaranteed, and we don’t really know how it happens. “It’s kind of like becoming a movie star,” says Szajnfarber.
Plus, the wait between now and digging up asteroids—which could happen in the mid-2020s, if you listen to the hopeful estimates based on aggressive launch and tech demonstration schedules—also leaves space for another country to hoover up opportunities. Deep Space Industries, for example, is talking with space agencies beyond Europe and the US. And the yawning years before the industry takes off give those competitors time to make themselves more attractive. “What [Luxembourg is] offering is really not enough to make people stay if it’s a multibillion-dollar industry and everyone can provide similar levels of investment or tax incentives,” says Szajnfarber.
So while it’s unlikely Luxembourg would totally lose its grip, it’s also unlikely the company could maintain a complete monopoly. “If something materializes up there that’s really profitable, they will be one of—not the one, one of—the countries that benefits from whatever happens,” says space policy and international affairs professor Henry Hertzfeld of George Washington University.
So maybe your favorite space hotel’s new hull will come from, and support, a company based somewhere else. But you may well be buying the metal for your Martian solar panels, the water for your jet pack thrusters, and your lifetime supply of platinum from a company 49 percent owned by a tiny, wealthy nation.
As far as algae goes, Pseudo-nitzschia is an outright menace. It produces a neurotoxin, domoic acid, that works its way up the food chain, building up in shellfish like crabs and clams. And when humans eat those shellfish, the neurotoxin leads to a disorder known as amnesic shellfish poisoning—featuring symptoms like seizures, memory loss, and even death.
This is a particular problem for the west coast, which in 2015 took a $100 million hit when a domoic acid outbreak forced officials to shutter crab fisheries. A nasty surprise, but perhaps an increasingly predictable one. Because a study out today in the Proceedings of the National Academy of Sciences uses two decades of data to make a pivotal connection: Pseudo-nitzschia gets particularly toxic in masses of warm Pacific waters like El Niño and the less-romantic-sounding Pacific Decadal Oscillation. That’s going to help scientists better anticipate such outbreaks—and make sense of a mysterious algal menace in seas that are rapidly transforming.
How mysterious? For one, no one knows for sure why Pseudo-nitzschia pumps out domoic acid in the first place. And the neurotoxin doesn’t even have an effect on shellfish like crabs, but on mammals like humans that feed on crabs. “Why the algae produce something that causes toxicity several layers up the food web, we have no idea if it's just an accident or what,” says lead author Morgaine McKibben, a biological oceanographer at Oregon State University.
The link between warm water events and algal toxicity may be a clue. Cold, deep waters tend to carry more nutrients, because that’s where all the nice organic material sinks and decays. The more nutrients, the better phytoplankton (little plant-like organisms that soak up sunlight) like algae can grow—think of it like fertilizing a garden.
Hotter waters like those of El Niño, then, could present a challenge for Pseudo-nitzschia. “These warm waters usually have less food for the algae, and perhaps the domoic acid allows them to out-compete other phytoplankton,” McKibben says. But that, she warns, is still but a guess.
What she and her colleagues were able to determine, however, is that the toxicity of the algae follows a particular pattern. First off, the Oregon Department of Agriculture has been monitoring the levels of domoic acid in razor clams since the early ‘90s. The scientists took that data and correlated it with climate cycles like the Pacific Decadal Oscillation and El Niño, plus local water temperatures and data on plankton communities. In so doing, the authors found that the warmer the ocean cycles along the west coast, the more toxic and widespread the outbreaks of domoic acid.
That’s invaluable information for the officials monitoring domoic acid levels in fisheries up and down the coast. With more data, NOAA—which funded the study—can locally forecast algal blooms like it forecasts the weather. “Say maybe a month out, the Department of Fish and Game can give them a heads up that the conditions look good,” says Marc Suddleson, who manages NOAA’s algal event-monitoring program, “but you may be dealing with this a few weeks or a month ahead of time.”
And with even more data, they can refine their models. “The other aspect is being able to make predictions on maybe a weekly basis, maybe a bulletin,” says Suddleson. “We told you to be ready, here it comes.”
Complicating factors, though, is that while climate change is reliably warming the oceans, it's not clear how it affects the production of warm water masses. A change of just a few degrees can have outsized impacts on something as complicated as an oceanic ecosystem. “If climate change favors these warm ocean conditions—which we don't know that it will—but if it does, yeah that would be a problem,” McKibben says.
Brace yourselves, shellfish fans.
Patient Number Two was born to first-time parents, late 20s, white. The pregnancy was normal and the birth uncomplicated. But after a few months, it became clear something was wrong. The child had ear infection after ear infection and trouble breathing at night. He was small for his age, and by his fifth birthday, still hadn't spoken. He started having seizures. Brain MRIs, molecular analyses, basic genetic testing, scores of doctors; nothing turned up answers. With no further options, in 2015 his family decided to sequence their exomes—the portion of the genome that codes for proteins—to see if he had inherited a genetic disorder from his parents. A single variant showed up: ARID1B.
The mutation suggested he had a disease called Coffin-Siris syndrome. But Patient Number Two didn't have that disease's typical symptoms, like sparse scalp hair and incomplete pinky fingers. So, doctors, including Karen Gripp, who met with Two's family to discuss the exome results, hadn't really considered it. Gripp was doubly surprised when she uploaded a photo of Two's face to Face2Gene. The app, developed by the same programmers who taught Facebook to find your face in your friend's photos, conducted millions of tiny calculations in rapid succession—how much slant in the eye? How narrow is that eyelid fissure? How low are the ears? Quantified, computed, and ranked to suggest the most probable syndromes associated with the facial phenotype. There's even a heat map overlay on the photo that shows which the features are the most indicative match.
“In hindsight it was all clear to me,” says Gripp, who is chief of the Division of Medical Genetics at A.I. duPont Hospital for Children in Delaware, and had been seeing the patient for years. “But it hadn’t been clear to anyone before.” What had taken Patient Number Two’s doctors 16 years to find took Face2Gene just a few minutes.
Face2Gene takes advantage of the fact that so many genetic conditions have a tell-tale “face”—a unique constellation of features that can provide clues to a potential diagnosis. It is just one of several new technologies taking advantage of how quickly modern computers can analyze, sort, and find patterns across huge reams of data. They are built in fields of artificial intelligence known as deep learning and neural nets—among the most promising to deliver AI's 50-year old promise to revolutionize medicine by recognizing and diagnosing disease.
Genetic syndromes aren't the only diagnoses that could get help from machine learning. The RightEye GeoPref Autism Test can identify the early stages of autism in infants as young as 12 months—the crucial stages where early intervention can make a big difference. Unveiled January 2 at CES in Las Vegas, the technology uses infrared sensors test the child's eye movement as they watch a split-screen video: one side fills with people and faces, the other with moving geometric shapes. Children at that age should be much more attracted to faces than abstract objects, so the amount of time they look at each screen can indicate where on the autism spectrum a child might fall.
In validation studies done by the test's inventor, UC San Diego researcher Karen Pierce,1the test correctly predicted autism spectrum disorder 86 percent of the time in more than 400 toddlers. That said, it's still pretty new, and hasn't yet been approved by the FDA as a diagnostic tool. “In terms of machine learning, it’s the simplest test we have,” says RightEye’s Chief Science Officer Melissa Hunfalvay. “But before this, it was just physician or parent observations that might lead to a diagnosis. And the problem with that is it hasn’t been quantifiable.”
A similar tool could help with early detection of America’s sixth leading cause of death: Alzheimer’s disease. Often, doctors don't recognize physical symptoms in time to try any of the disease's few existing interventions. But machine learning hears what doctor's can't: Signs of cognitive impairment in speech. This is how Toronto-based Winterlight Labs is developing a tool to pick out hints of dementia in its very early stages. Co-founder Frank Rudzicz calls these clues “jitters,” and “shimmers:" high frequency wavelets only computers, not humans, can hear.
Winterlight's tool is way more sensitive than the pencil and paper-based tests doctor's currently use to assess Alzheimer's. Besides being crude, data-wise, those tests can’t be taken more than once every six months. Rudzicz’s tool can be used multiple times a week, which lets it track good days, bad days, and measure a patient's cognitive functions over time. The product is still in beta, but is currently being piloted by medical professionals in Canada, the US, and France.
If this all feels a little scarily sci-fi to you, it's useful to remember that doctors have been trusting computers with your diagnoses for a long time. That's because machines are much more sensitive at both detecting and analyzing the many subtle indications that our bodies are misbehaving. For instance, without computers, Patient Number Two would never have been able to compare his exome to thousands of others, and find the genetic mutation marking him with Coffin-Siris syndrome.
But none of this makes doctors obsolete. Even Face2Gene—which, according to its inventors, can diagnose up to half of the 8,000 known genetic syndromes using facial patterns gleaned from the hundreds of thousands of images in its database—needs a doctor (like Karen Gripp) with enough experience to verify the results. In that way, machines are an extension of what medicine has always been: A science that grows more powerful with every new data point.
1UPDATE 3:00 pm Eastern 1/9/17 This story has been updated to correct Dr. Pierce's relationship to RightEye; she is the author of the GeoPref Autism Test, which was licensed and further developed for commercialization by RightEye. An earlier version of this story incorrectly cited Dr. Pierce as RightEye's inventor.
Hit yourself on the hand with a hammer and it’ll swell to several times its normal size, or so say cartoons. The fiddler crab, though, needs no such encouragement to grow a freakishly enormous limb: One of its claws comes way, way bigger than the other—totally naturally. Why? To fight and get busy, of course. Learn more in this week’s episode of Absurd Creatures!
Find every episode of Absurd Creatures here. And I’m happy to hear from you with suggestions on what to cover next. If it’s weird and we can find footage of it, it’s fair game. You can get me at matthew_simon@wired.com or on Twitter at @mrMattSimon.
America used to be a chemistry powerhouse: It's the nation that made teflon, Super Glue, and Magic Grow dinosaurs. But recently, the number of new chemical substances approved by federal regulators has dropped by half.
That's because last summer Congress issued reforms to the Toxic Substances Control Act, a 30 year old law governing how the EPA and other federal agencies check the boxes that let new chemicals come to market. Industry groups say these significant changes to the review process are to blame, while agency officials say it’s nothing more than temporary growing pains during the initial transition. But one thing seems clear—the slowdown is real.
The Toxic Substances Control Act has been around since early days of the EPA; it gives the agency oversight over a wide range of commercial, industrial and consumer applications of microbial biotechnology—pretty much anything potentially harmful to humans or the environment that’s not food, drugs, cosmetics or pesticides.
That approval process changed significantly on June 22, 2016, when Congress passed an amendment called the Frank R. Lautenberg Chemical Safety for the 21st Century Act. Before, the agency had the authority to take action and halt approval if they found cause for concern with a new substance. But, if they they didn’t, they could simply dropped the review and the chemical could proceed. Now, by law, the EPA must give a full review to every substance, and only those that pass can go to market.
During those first six months prior the 2016 update—under the old review process—the EPA dropped the review of 81 new chemical substances, effectively OK-ing them for market. For cases reviewed between June 23, 2016 and the end of the year, the agency found only 39 substances to be “not likely to present reasonable risk,” the new affirmative head nod toward commercialization. That leaves hundreds of applications stuck in review limbo, with the backlog growing bigger every day.
As a result, some chemists feel like they must dial back their ambitions to pass review. “New chemicals are taking significantly longer to get through the EPA process now,” says Rich Engler, a chemical consultant with environmental law firm Bergeson & Campbell. “If I were advising a client I would tell them to only try to develop something low hazard, because those are the only ones proceeding through right now." Engler previously worked for 17 years as a senior staff scientist in the EPA’s Office of Pollution Prevention and Toxics.
At face value, safe seems better. But while new regulations might keep dangerous chemicals in their beakers, other, more innocuous substances often get unfairly flagged as presenting some level of risk. These include genetically modified microorganisms being used to create all kinds of new chemicals. The term GMO might raise the hackles of people fearing gene-hacked corn and frankenfish, but in regulatory lingo it is a really broad term encompassing some really uncontroversial organisms. These include: Yeasts that fart ethanol; fungi that secrete cellulose; algae that poop biofuels; bacteria that digest oil spills; and a host of other commercially useful microbes that have been spliced with genetic material outside their own genus.
A handful of these helpful GMOs have made it through the post-reform Toxic Substances Control Act EPA (some yeasts and a few fungi with extensive histories of safe use). But chemists and companies are concerned that increased data burdens in the new EPA regulations are stifling efforts to engineer solutions from unlikely places, even as advances in gene editing makes it quicker and easier than ever to do so. In a public meeting the agency convened in December, industry groups urged the EPA to reassess the changes to the new chemicals program.
But besides pulling in additional staff to handle all the new paperwork, there's not a whole lot the agency can do. The Toxic Substance Controls Act reforms from last summer were passed by Congress, and are therefore law. Even the most regulations-averse EPA head would still be statutorily obligated to enforce the new process. Not even president-elect Trump could undo them, should he decide he wants to Make America Formulate Again.
If you came across an animal in the wild and wanted to learn more about it, there are a few things you might do: You might watch what it eats, poke it to see how it reacts, and even dissect it if you got the chance.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Mathematicians are not so different from naturalists. Rather than studying organisms, they study equations and shapes using their own techniques. They twist and stretch mathematical objects, translate them into new mathematical languages, and apply them to new problems. As they find new ways to look at familiar things, the possibilities for insight multiply.
That’s the promise of a new idea from two mathematicians: Laura DeMarco, a professor at Northwestern University, and Kathryn Lindsey, a postdoctoral fellow at the University of Chicago. They begin with a plain old polynomial equation, the kind grudgingly familiar to any high school math student: f(x) = x2 – 1. Instead of graphing it or finding its roots, they take the unprecedented step of transforming it into a 3-D object.
With polynomials, “everything is defined in the two-dimensional plane,” Lindsey said. “There isn’t a natural place a third dimension would come into it until you start thinking about these shapes Laura and I are building.”
The 3-D shapes that they build look strange, with broad plains, subtle bends and a zigzag seam that hints at how the objects were formed. DeMarco and Lindsey introduce the shapes in a forthcoming paper in the Arnold Mathematical Journal, a new publication from the Institute for Mathematical Sciences at Stony Brook University. The paper presents what little is known about the objects, such as how they’re constructed and the measurements of their curvature. DeMarco and Lindsey also explain what they believe is a promising new method of inquiry: Using the shapes built from polynomial equations, they hope to come to understand more about the underlying equations—which is what mathematicians really care about.
In mathematics, several motivating factors can spur new research. One is the quest to solve an open problem, such as the Riemann hypothesis. Another is the desire to build mathematical tools that can be used to do something else. A third—the one behind DeMarco and Lindsey’s work—is the equivalent of finding an unidentified species in the wild: One just wants to understand what it is. “These are fascinating and beautiful things that arise very naturally in our subject and should be understood!” DeMarco said by email, referring to the shapes.
Laura DeMarco, a professor at Northwestern University.
“It’s sort of been in the air for a couple of decades, but they’re the first people to try to do something with it,” said Curtis McMullen, a mathematician at Harvard University who won the Fields Medal, math’s highest honor, in 1988. McMullen and DeMarco started talking about these shapes in the early 2000s, while she was doing graduate work with him at Harvard. DeMarco then went off to do pioneering work applying techniques from dynamical systems to questions in number theory, for which she will receive the Satter Prize—awarded to a leading female researcher—from the American Mathematical Society on January 5.
Meanwhile, in 2010 William Thurston, the late Cornell University mathematician and Fields Medal winner, heard about the shapes from McMullen. Thurston suspected that it might be possible to take flat shapes computed from polynomials and bend them to create 3-D objects. To explore this idea, he and Lindsey, who was then a graduate student at Cornell, constructed the 3-D objects from construction paper, tape and a precision cutting device that Thurston had on hand from an earlier project. The result wouldn’t have been out of place at an elementary school arts and crafts fair, and Lindsey admits she was kind of mystified by the whole thing.
“I never understood why we were doing this, what the point was and what was going on in his mind that made him think this was really important,” said Lindsey. “Then unfortunately when he died, I couldn’t ask him anymore. There was this brilliant guy who suggested something and said he thought it was an important, neat thing, so it’s natural to wonder ‘What is it? What’s going on here?’”
In 2014 DeMarco and Lindsey decided to see if they could unwind the mathematical significance of the shapes.
To get a 3-D shape from an ordinary polynomial takes a little doing. The first step is to run the polynomial dynamically—that is, to iterate it by feeding each output back into the polynomial as the next input. One of two things will happen: either the values will grow infinitely in size, or they’ll settle into a stable, bounded pattern. To keep track of which starting values lead to which of those two outcomes, mathematicians construct the Julia set of a polynomial. The Julia set is the boundary between starting values that go off to infinity and values that remain bounded below a given value. This boundary line—which differs for every polynomial—can be plotted on the complex plane, where it assumes all manner of highly intricate, swirling, symmetric fractal designs.
Lucy Reading-Ikkanda
If you shade the region bounded by the Julia set, you get the filled Julia set. If you use scissors and cut out the filled Julia set, you get the first piece of the surface of the eventual 3-D shape. To get the second, DeMarco and Lindsey wrote an algorithm. That algorithm analyzes features of the original polynomial, like its degree (the highest number that appears as an exponent) and its coefficients, and outputs another fractal shape that DeMarco and Lindsey call the “planar cap.”
“The Julia set is the base, like the southern hemisphere, and the cap is like the top half,” DeMarco said. “If you glue them together you get a shape that’s polyhedral.”
The algorithm was Thurston’s idea. When he suggested it to Lindsey in 2010, she wrote a rough version of the program. She and DeMarco improved on the algorithm in their work together and “proved it does what we think it does,” Lindsey said. That is, for every filled Julia set, the algorithm generates the correct complementary piece.
The filled Julia set and the planar cap are the raw material for constructing a 3-D shape, but by themselves they don’t give a sense of what the completed shape will look like. This creates a challenge. When presented with the six faces of a cube laid flat, one could intuitively know how to fold them to make the correct 3-D shape. But, with a less familiar two-dimensional surface, you’d be hard-pressed to anticipate the shape of the resulting 3-D object.
“There’s no general mathematical theory that tells you what the shape will be if you start with different types of polygons,” Lindsey said.
Mathematicians have precise ways of defining what makes a shape a shape. One is to know its curvature. Any 3-D object without holes has a total curvature of exactly 4π; it’s a fixed value in the same way any circular object has exactly 360 degrees of angle. The shape—or geometry—of a 3-D object is completely determined by the way that fixed amount of curvature is distributed, combined with information about distances between points. In a sphere, the curvature is distributed evenly over the entire surface; in a cube, it’s concentrated in equal amounts at the eight evenly spaced vertices.
A unique attribute of Julia sets allows DeMarco and Lindsey to know the curvature of the shapes they’re building. All Julia sets have what’s known as a “measure of maximal entropy,” or MME. The MME is a complicated concept, but there is an intuitive (if slightly incomplete) way to think about it. First, picture a two-dimensional filled Julia set on the plane. Then picture a point on the same plane but very far outside the Julia set’s boundary (infinitely far, in fact). From that distant location the point is going to take a random walk across two-dimensional space, meandering until it strikes the Julia set. Wherever it first strikes the Julia set is where it comes to rest.
The MME is a way of quantifying the fact that the meandering point is more likely to strike certain parts of the Julia set than others. For example, the meandering point is more likely to strike a spike in the Julia set that juts out into the plane than it is to intersect with a crevice tucked into a region of the set. The more likely the meandering point is to hit a point on the Julia set, the higher the MME is at that point.
In their paper, DeMarco and Lindsey demonstrated that the 3-D objects they build from Julia sets have a curvature distribution that’s exactly proportional to the MME. That is, if there’s a 25 percent chance the meandering point will hit a particular place on the Julia set first, then 25 percent of the curvature should also be concentrated at that point when the Julia set is joined with the planar cap and folded into a 3-D shape.
“If it was really easy for the meandering point to hit some area on our Julia set we’d want to have a lot of curvature at the corresponding point on the 3-D object,” Lindsey said. “And if it was harder to hit some area on our Julia set, we’d want the corresponding area in the 3-D object to be kind of flat.”
This is useful information, but it doesn’t get you as far as you’d think. If given a two-dimensional polygon, and told exactly how its curvature should be distributed, there’s still no mathematical way to identify exactly where you need to fold the polygon to end up with the right 3-D shape. Because of this, there’s no way to completely anticipate what that 3-D shape will look like.
“We know how sharp and pointy the shape has to be, in an abstract, theoretical sense, and we know how far apart the crinkly regions are, again in an abstract, theoretical sense, but we have no idea how to visualize it in three dimensions,” DeMarco explained in an email.
She and Lindsey have evidence of the existence of a 3-D shape, and evidence of some of that shape’s properties, but no ability yet to see the shape. They are in a position similar to that of astronomers who detect an unexplained stellar wobble that hints at the existence of an exoplanet: The astronomers know there has to be something else out there and they can estimate its mass. Yet the object itself remains just out of view.
Thus far, DeMarco and Lindsey have established basic details of the 3-D shape: They know that one 3-D object exists for every polynomial (by way of its Julia set), and they know the object has a curvature exactly given by the measure of maximal entropy. Everything else has yet to be figured out.
In particular, they’d like to develop a mathematical understanding of the “bending laminations,” or lines along which a flat surface can be folded to create a 3-D object. The question occurred early on to Thurston, too, who wrote to McMullen in 2010, “I wonder how hard it is to compute or characterize the pair of bending laminations, for the inside and the outside, and what they might tell us about the geometry of the Julia set.”
Kathryn Lindsey, a mathematician at the University of Chicago.
In this, DeMarco and Lindsey’s work is heavily influenced by the mid 20th-century mathematician Aleksandr Aleksandrov. Aleksandrov established that there is only one unique way of folding a given polygon to get a 3-D object. He lamented that it seemed impossible to mathematically calculate the correct folding lines. Today, the best strategy is often to make a best guess about where to fold the polygon—and then to get out scissors and tape to see if the estimate is right.
“Kathryn and I spent hours cutting out examples and gluing them ourselves,” DeMarco said.
DeMarco and Lindsey are currently trying to describe the folding lines on their particular class of 3-D objects, and they think they have a promising strategy. “Our working conjecture is that the folding lines, the bending laminations, can be completely described in terms of certain dynamical properties,” DeMarco said. Put another way, they hope that by iterating the underlying polynomial in the right way, they’ll be able to identify the set of points along which the folding line occurs.
From there, possibilities for exploration are numerous. If you know the folding lines associated to the polynomial f(x) = x2– 1, you might then ask what happens to the folding lines if you change the coefficients and consider f(x) = x2 – 1.1. Do the folding lines of the two polynomials differ a little, a lot or not at all?
“Certain polynomials might have similar bending laminations, and that would tell us all these polynomials have something in common, even if on the surface they don’t look like they have anything in common,” Lindsey said.
It’s a bit early to think about all of this, however. DeMarco and Lindsey have found a systematic way to think about polynomials in 3-D terms, but whether that perspective will answer important questions about those polynomials is unclear.
“I would even characterize it as being sort of playful at this stage,” McMullen said, adding, “In a way that’s how some of the best mathematical research proceeds—you don’t know what something is going to be good for, but it seems to be a feature of the mathematical landscape.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Click through the gallery to see this week’s helping of the best the universe has to offer. And if you need more when you’re done but can’t wait until the next one, here’s the entir
When three female African-American mathematicians—Katherine Johnson, Dorothy Vaughan and Mary Jackson—became unsung heroes at NASA during the 1960s space race, the US was engaged in a fierce competition to become the world leader in science, technology, engineering, and math, or STEM. As told in the recently released movie Hidden Figures, the trio's groundbreaking calculations for rocket trajectories required programming a complex, first-of-a-kind IBM computer that helped put astronaut John Glenn in orbit. Skip ahead 54 years, and the US is a world leader in scientific innovation and advanced technologies.
Stanley S. Litow (@CitizenIBM) is president of the IBM Foundation and vice president of corporate citizenship and corporate affairs at IBM. He is a former deputy chancellor of New York City public schools.
But in order for the US to remain at the forefront of innovation and not lag behind, we must address the disconnect between the skills required for 21st century jobs and young people's ability to acquire those skills. Fixing this will require us to evolve our approach to public education and training. The latest results of the PISA exam, which assesses science, math, and reading performance among 15-year-olds around the globe, show American students noticeably behind in math scores (below the international average), with science and reading scores remaining flat. This is not a small problem.
In one way, Congress took a bold, bipartisan step toward reversing this downward trend and closing America's skills gap last fall, when the House of Representatives voted 405-to-5 to reauthorize the Carl D. Perkins Vocational and Technical Education Act, which had languished since 2006. The Perkins Act provides more than $1 billion in funding for career and technical education across the US. The bill aligns career and tech education programs with actual labor market demands. Updating this important legislation can and should be an early win for the 115th Congress and the incoming administration.
The House bill passed with overwhelming bipartisan support because leaders from both sides of the aisle recognize the urgent need to better prepare students to succeed in college and career. Backed by hundreds of business, labor, education, and civil society leaders, this much-needed reform will enable the country to invest wisely and prepare America's young people to fill the hundreds of thousands of well-paying jobs that already exist, and that do not always require a four-year college degree. These new collar jobs for holders of two-year degrees do not need to be created or brought back. They're ready to be filled today by people with the right skills, and early action by Congress is essential.
But many of our young people lack the relevant skills or support to move from school to college to career. Too many of our traditional vocational training programs do not prepare students for meaningful careers. It's time to link career and technical education to where the high quality jobs are now and where they will be in the future.
Passage of the Perkins Act can ensure that our nation's essential career and technical education programs will equip graduates for current and future high-wage, high-growth jobs. A revised Perkins Act will help better meet the demands of the 21st century workforce by giving employers the ability to align education directly to needed skills, and blend experiential learning with academic training. The bill also calls for concrete performance metrics to reward success. Such an investment in America's young people will yield long-lasting returns.
An innovative new educational model called P-TECH lets students earn both a high school diploma and an associate degree in a STEM field. Launched by IBM (where I am a vice president) along with education partners in 2011, P-TECH is a rigorous program that aligns strong STEM curricula with essential workplace skills such as problem solving, writing, and critical thinking. Located mostly in underserved communities and requiring no admissions testing or additional spending, P-TECH schools are already delivering tangible results. The program has expanded to 60 schools in urban, suburban, and rural communities, and IBM is working with states to create 20 more P-TECH schools over the next year. P-TECH's measurable results prove that it can and will help thousands of youth achieve success.
Hidden Figures Should Be a Diversity Wake-Up Call for Film
IBM’s School Could Fix Education—And Tech’s Diversity Gap
In Crown Heights, Brooklyn, for example, nearly 35 percent of students from the first P-TECH class are completing their "six-year" program in five years or less, moving directly into good new collar jobs, four-year college degree programs, or both—without the need for costly, non-credit remedial courses. With these kinds of results, it's not far-fetched to envision skilled and motivated P-TECH graduates playing essential roles in America's next moon shot.
If we are to enable a brighter future for American youth through innovative technical education programs, Congress must act quickly and send the Perkins bill to the President's desk for early signature. This fundamental and sorely needed alignment is a win-win for our country. It puts our students on a path to success, and positions our businesses to compete and win in the global economy. Given support and opportunity, our young people can and will succeed.
On Sunday, SpaceX announced its launch would be pushed back further, to January 14, thanks to continuing heavy winds and rains at Vandenberg Air Force Base.
SpaceX's army of hopers and believers have been holding their breath in anticipation of its next launch. They'll be blue in the face just a bit longer: The liftoff, which had been planned for this Sunday, January 8, has been pushed back a day due to foul weather.
The announcement, until now a rumor circulated in local Central California media, was made official today on the website of Iridium, SpaceX's customer for the launch. This is SpaceX's first launch since a Falcon 9 bearing a $200 million satellite blew up on September 1, 2016. This mission isn't a redo of the Spacecom satellite that blew up, but the first in a series of seven for Iridium will launch a total of 70 miniature telecommunications satellites.
The launch, now scheduled for January 9, 10:22am PST, has been delayed because of foul weather. SpaceX's west coast launch pad is located at Vandenberg Air Force base, in the mountainous shore north of the company's Hawthorne, CA headquarters. Starting Saturday evening, Vandenberg, along with the rest of California, will get drenched by an invisible, incredibly moist tendril of air extending from the tropics. Meteorologists expect the so-called atmospheric river to drop upwards of a foot of rain on low-lying areas. Obviously, not the best conditions to launch a rocket.
Especially a rocket launch that every space junkie on the planet will be watching. Another failure would be very bad for SpaceX, because although the company still has a few years worth of launches on the books, it will have a hard time selling future customers on its safety record with a pair of back-to-back launchpad mishaps. And Musk would be selling his launches against the superior safety records of competitors like United Launch Alliance, Sierra Nevada, and Orbital ATK.
However, none of those other companies are landing their rockets, nor do they have any other clear path towards democratizing access to space. (Yeah, yeah, Blue Origin is landing rockets, but not rockets capable of taking big payloads into high orbits, like the Falcon 9 has done time and again.) Last year, the company landed four of its Cape Canaveral-launched Falcon 9 rockets on a droneship. And another two touched down on dry land. Re-launching Falcon 9s would save SpaceX tens of millions of dollars per launch.
Reusability, along with other pioneering budget-busters like highly compressed propellant, are what SpaceX was banking on in order to fulfill its promise of offering spaceflight to civilians. Elon Musk had been promising the first such re-launch would happen in late 2016. Now ... well, let's just get through Monday.
Or Tuesday, or Wednesday, or whenever California's skies clear up enough for liftoff.
Here is classic (and difficult) physics problem that poses an interesting question:
Take two points in space, Point 1 and Point 2. What is the path from point 1 to 2 that a frictionless object could slide in the least amount of time? Assume a constant gravitational field.
Here are two points with different paths. One might be faster, but which would be fastest? The solution to this problem is traditionally called the brachistochrone curve.

This difficult and interesting question has historical significance. The brachistochrone solution contributed to the creation of the calculus of variations. I won't go into details, but I will remind you that Lagrangian mechanics is based on the calculus of variations.
The traditional textbook approach here is to first solve for the time to slide down a curve. Since a curve isn't straight, you must set this up as an integral in which you calculate the time required for many small "straight" segments and add them up. That's not too difficult. The tricky part is finding the function (curve) that gives the minimum value after integrating. It's like a max-min problem in calculus but way harder.
I've gone over the derivation of the calculus of variations when I teach classical mechanics, but I've never been satisfied. I always feel it's sort of a magical and mysterious step to find this function that minimizes the integral, and I just follow along with the textbook much like I just follow my phone's directions when trying to find a new location.
But with any great problem, there is more than one way to solve this. What about some type of numerical solution? Yes, that's what I'll do—at least for one of the solutions.
I have an idea for a game. A physics game with complicated problems. The user (player) tries to guess at solutions without actually solving the problems. Of course, such games already exist— basketball and baseball deal with projectile motion, even if no one actually solves for these trajectories. But what about guessing the energy levels for some quantum object? Or the speed for a stable planetary orbit? That could be a fun game.
But here is a real example: Can you estimate the path that would let a bead slide down a wire in the shortest time? Well, I put together some python code that let's you test your intuition. Here's how to play:
Here is the program.
If you really want to look at the code, here it is. I'll be honest, I still don't completely understand the buttons or mouse interactions, but I got it to work.
Try different paths. See if you can get a faster time. Yes, if you make the track go higher than the starting point, it won't work (hopefully you already tested that). I tried to add comments to the code so you can play with it. There are two things you might change. First, adjust the number of movable points on the path. Second, tweak the location of the second point. Both can be fun.
The key to a numerical solution is to take a complicated problem and break it into a bunch of simpler problems. What if there was just one moveable point between two fixed points?

Here I can move the middle position up and down (with a variable y) and calculate the time it takes to go from Position 1 to 2. Let me make this slightly different than the original problem. In this case, I am going to let the bead begin at Position 1 with some starting velocity. The bead will speed up as it moves to the middle point (assuming I move it lower than the starting point).
Calculating the time to reach the midpoint isn't difficult, but it is a bit tedious. First, I will calculate the speed of the bead at the midpoint. Here, I can use the Work-Energy Principle. Using gravitational potential energy and a change in kinetic energy, I get:

For the travel time, I need the average speed and the distance. The average speed is the sum of the starting and ending velocity divided by two (since the acceleration is constant). I will call the distance for this partial path the variable s. It will have the following value.

Perhaps you noticed that I am calling "down" the positive y-direction. I hope that didn't mess you up. Now I can put the average speed together with the distance to get the slide time. Remember the bead must stay on the wire so it's a one dimensional problem.

Yes, both v2 and s depend on the vertical position—y. But wait! We still aren't finished. Now I must do the same thing for the path from the midpoint to Point 2. Remember two important things. First, the final velocity for the first part is the starting velocity for the second part. Second, it's very possible that the acceleration for the bead is negative (if the wire goes up).
However, the point is that it's entirely possible get an expression for the bead-time in terms of the variable y. With that expression, this things could be turned into a classic max-min problem. It could be done, but it would get messy. So instead, I'm going to do something else.
What if I just put the middle point at some value of y and then calculate the total time. Next, I will move the y position and calculate the total time again. With that, I could make a plot of slide-time vs. y-position. It would be so simple that I could do it right now.
Go ahead and examine the code if you like, but it's a pretty simple program. As you can see, there is a y-position that gives a minimum time. But how do I know it's not just some bogus graph? Maybe it just looks correct because it's curvy and red? Well, there's a few things I know for sure. I know the final speed of the bead. No matter what the path, the Work-Energy Principle dictates that final speed—so that's something I can check. And what about special cases? I can easily solve for the slide-time in the case of a straight line. I can also solve for the time with point 2 directly below point 1 (but that is kind of boring). With these checks, I feel more comfortable about my model.
Now to put this calculation to something more useful. I just need to run the same calculation for every point in my curve. Yes, this can be slow—but it works. Here's what that looks like. Click "play" to start it.
I think that's pretty awesome. Honestly, this took me quite a bit longer to put together than I expected. In the end, it looks pretty nice. Oh, you say it's not the fastest solution? Well, have you ever tried solving this problem on paper? It's pretty tough.
But how does my solution compare to the traditional textbook answer? By the way, if you want to go over the derivation, I suggest looking at Andy Rundquist's post on this.
I won't cover the details of the solution except to say the shortest timed path is that of a cycloid. But I was surprised that it wasn't so trivial to find a cycloid path that started and ended at the right points. I had to make another numerical calculation to find one of the coefficients—but I won't go into that.
In the end, I was able to modify my program to include a cycloid along with my numerical optimization. Here it is—press play to run it. The yellow curve is the analytical solution.
I'm pretty happy.
Don't forget your homework.
Update (1/16/17). In an email conversation with Bruce Sherwood, I was reminded of an old (but famous) physics program called Graphs and Tracks. The basic idea for this program was for a student to adjust a track that a ball could roll down such that the graph of the position, velocity, acceleration matched some preset idea. It was pretty awesome and fairly similar to the code I have produced above.
Good news. The Graphs and Tracks program (created by David Trowbridge) has been updated and is now online. Check it out at graphsandtracks.com.
Billions of years ago, an unknown object sent a seriously bright burst of radio waves into space. They traveled across the universe, past galaxies and clouds of gas and who knows what else. And in 2012, the burst arrived at the Arecibo radio telescope when astronomers happened to be watching.
They kept searching that same spot in the sky. In 2015, they found 16 additional flashes. Then, in August and September 2016, nine more appeared. And this week, astronomers announced that these newest measurements helped them finally zero in on the bursts’ home: a dim dwarf galaxy three billion light-years away. Something inside this tiny galaxy was sending pulses that lasted just milliseconds but packed enormous energy, members of a still-mysterious class called “fast radio bursts.”
Learning that these particular bursts came from long ago, from some bursting object in a galaxy far, far (far) away, is an important step for this field of research. But it’s also like playing Clue and concluding that the crime was committed in the conservatory. To solve the crime, you still have to determine whether the dastardly deed was done by Mrs. Peacock with the candlestick or Colonel Mustard with the rope.
That continuing conundrum shows off the way science works, in a way we don't usually get to see. Astronomers don’t often happen upon a total mystery. Much of their work involves looking directly at objects they know exist—stars, planets, supernovae—and studying processes and properties. Fast radio bursts, though, appeared out of nowhere, unexpected and un-asked-for, coming from question-mark objects with question-mark properties because of question-mark processes. Astronomers now have the privilege of figuring out the what, where, why, and how—from total scratch—and we have the privilege of watching the discovery process take place from its start.
To forecast what will likely happen next in the ongoing case of the super-energetic fast radio bursts, history helps. Specifically, the twentieth-century discoveries of pulsars and gamma-ray bursts, which also began with on-and-off flashes from unknown entities.
The very first on-off from a fast radio burst came in 2007, when astronomer Duncan Lorimer was sifting through archived data, searching for undiscovered pulsars. But instead, he found something that flashed just once, brighter than a pulsar and seemingly much farther away. He didn’t know what he was looking at. Neither did anyone else.
This is a familiar story arc in astronomy. It’s really the best way to find something utterly new: by accident, while searching for something known. It happened to Jocelyn Bell, who was looking for the twinkling of quasars—the superbright cores of galaxies with supermassive black holes in feeding mode—when she stumbled upon a repeating radio blip. It blipped too fast to be any regular star. Was it aliens? Human technology? A planet? A mistake? It wasn’t until she found another blipper that she felt confident it was part of the natural universe at all. Then, when she and her advisor found two more, the blippers became a Thing. After they went public, people proposed more explanations, including the correct one—pulsars, the fast-spinning neutron stars left over after supernova explosions.
Gamma-ray bursts, too, are in encyclopedias because of an accident. In the 1960s, US government satellites were hanging out, watching for the high-energy indications of Soviet nuclear tests. They picked up 16 weird bursts of gamma rays that didn’t match up with nukes’ characteristics. In 1973, the government declassified the discovery and declared that the bursts must have come from space.
But after Lorimer saw his first burst, he didn’t get more of the same from the sky, as Bell and the Soviet-watchers did. No one saw any more fast radio bursts, from anywhere in the sky, for years. People doubted the astronomical origin of the original specimen, suggesting it came from Earth—and, indeed, astronomers in Australia accidentally produced a set of similar radio bursts by opening their microwave door before cooking was complete. There wasn't even a *category *for that kind of behavior.
Since then, astronomers have found 18 sources of fast radio bursts—including the only one that repeats, the one first spotted in 2012. Shami Chatterjee, the lead in this latest discovery, decided to focus efforts there. “This is a good place to go fishing because you’re more likely to see a fast radio burst at this spot,” says Chatterjee. The team began watching the area with the Very Large Array in late 2015, searching for the burster's precise location in space.
They watched for another burst for dozens of hours, in observations in November 2015 and April and May 2016, and saw nothing. “The field of transients is special in that we need to wait for the universe to provide an event for us,” says Casey Law of the University of California, Berkeley, who led the project’s software and data-taking developments. Finally, a burst appeared, in a set of observations that began in August. Then, so did eight more. That dataset allowed the team to pinpoint where the signals came from, a position they later zoomed in on even more precisely with radio telescopes around the world. And once they got images of that same spot from an optical telescope called Gemini North, they saw a faint smudge of shine, more like something you’d try to wipe off your screen than the answer to a big astronomical question. But that smudge was actually a tiny galaxy, around 3 billion light-years distant. Somewhere inside, the astronomers knew lurked the burster.
Artist rendition of the dishes of the Karl G. Jansky Very Large Array are seen making the first-ever precision localization of a Fast Radio Burst, and thereby pointing the way to the host galaxy of FRB121102.
Like with pulsar signals and gamma-ray bursts, finding more instances of fast radio bursts, and perhaps repeat offenders, will allow scientists to learn about them as a population—even before they know what that population is. They can see what common characteristics the members have, characteristics that speak to their physical fundamentals. Pulsars, for instance, all have really stable spins because they are so spherical, dense, and full of angular momentum. They can watch how often the signals occur (or recur), which speaks to how common their originators are in the universe, and how they are spread across the sky.
That latter observation was initial convincing evidence, 18 years after the declaration of gamma-ray bursts’ existence, that they came from outside our galaxy. Astronomers had debated whether gamma-ray bursts were only kind of bright and nearby, or superbright and far away—as they also debated, until this week, for fast radio bursts. The Compton Gamma Ray Observatory, the first to do a true survey for such bursts, showed they came equally from all over the sky, not clustered around the Milky Way. And then, six years later, astronomers caught a gamma-ray burst in the act, nailed down its location, and calculated its distance from Earth (hint: not near our neighborhood). So by the gamma-ray metric, radio burst researchers are way ahead of schedule, having taken just 10 years to discover the same.
Cosmic mysteries do take a long time to explain. Astronomers still haven’t deciphered, for example, the details of why pulsars emit radio waves the way they do. And they don't yet know what inside that distant dwarf galaxy causes these repeating fast radio bursts, or what makes the ones that simply clap on and clap off, or if their origins are the same. Chatterjee says that inside the dwarf galaxy, a weird dude called a magnetar could be sending out giant pulses that interact with cosmic plasma. Or maybe an active black hole at the center of that little galaxy is vaporizing blobs of plasma. But right now, the number of plausible ideas about The Causes of fast radio bursts exceeds the number of bursters.
When scientists stumble upon something—a flash in the darkness that lights up the space between us and Whatever sent it—it’s going to be a while before they can enlighten the rest of us on what that Whatever is.
So while this latest burst announcement is not The Answer, it’s a step up, and sometime soon, another result will stand on its shoulders. Law likes that. “Science produces a community good in that each result contributes to the public understanding of the world,” he says. “Each result builds on those before, so when I publish a paper or my code, I feel like I get to participate in a great scientific story of human discovery.”
After all, mystery novels don’t say whodunit on page two. They present evidence piled on evidence scattered over twists and turns, letting the reader imagine several plausible scenarios, and then, in the final chapter, they reveal what’s really been going on all this time.
Acts of God are on the rise. Insurers now pay nearly four times as much to policyholders hit by natural disasters as they did in 1980. That’s because God has had a major assist from fossil-fueled industrialization. With the Paris climate agreement, the world’s emitters sought to slash carbon across the public and private sectors. The US Clean Power Plan targets coal; China is enacting a cap-and-trade system; India is betting big on solar. The deal sends a clear signal to companies: Invest in green business models. After Donald Trump was elected, more than 360 companies signed a letter urging him to uphold the agreement. Many are hewing to it anyway. Google will reach 100 percent renewable energy in 2017; Facebook and Amazon are following suit. Business leaders will aim to sway Trump from his anti-green stance with economic calculations. Investing in clean energy isn’t just imperative for the environment—it’s essential for US competitiveness.
You already know that you have a microbiome: the bacteria that live in and on your body, subtly (and not so subtly) influencing your health. But while you’d probably love to know what germs are going to make you thinner/healthier/more regular, human microbiome therapies are going to be slow to work their way through the FDA’s approval process. Don’t hold your breath for a magic germ pill. And don’t be so self-centered. See, everything has a microbiome, whether it’s a subway platform or a cornfield. Luckily, research that can help us understand those communities will hit the market a helluva lot faster. One company, Indigo, has been analyzing the bacterial composition of agricultural staples to see how pesticides and fertilizers may have changed the balance of the plants’ symbiotic bugs over time. Indigo then concocted new combinations of germs intended to help crops grow faster or in harsher conditions (it harvested its first crop last fall: 50,000 acres of drought-resistant cotton). Meanwhile, scientists at the Argonne National Laboratory are applying similar research to create healthier, more robust bug populations in hospitals and homes. Call it community immunity.
Biologists worldwide have fallen in love with Crispr for its rapid, efficient gene-editing powers. Now they can swiftly engineer mouse strains with certain defects, letting them study diseases (and explore potential treatments) more easily than ever before. Cures for humans are next. The Crispr startup Editas Medicine expects to launch its first clinical trial, for a congenital eye disease that causes blindness, this year. They’ll load up a virus with tools to snip out the mutated gene, then inject it into a person’s retina. And the US National Institutes of Health has already approved the first wide-scale trial of a Crispr-based cancer treatment: Scientists at the University of Pennsylvania will remove T cells from cancer patients, make three edits, then reintroduce these immune responders back into the body to detect and attack cancer cells. In China, researchers have already made similar tweaks to white blood cells to tackle cancer. But they’re also pushing into dicier territory, using the technique to modify human embryos—and thus potentially future generations. Scientists in the US may be fiddling with similar sorts of heritable modifications in mice but have no plans to do so in people. For now.
Criminalizing drugs is as American as doing them. But the days of outright prohibition are over, as the prescription opioid crisis has burned through the suburbs and drastically shifted the public’s notion of addiction. “Opioid addiction is more relatable than the past perception of heroin junkies lying in the street,” says Katharine Neill, a drug policy expert at Rice University. Reframing drug abuse as a public health problem rather than a criminal one has prompted reform-minded legislation from both parties. And while prescription meds are now being held at arm’s length, recreational drugs are being embraced. Eight states voted to legalize weed in some form in the last election, bringing the total to 29, and researchers are studying the drug’s medicinal applications for everything from migraines to multiple sclerosis. Loosening attitudes toward illicit drugs aren’t limited to weed: Researchers are also testing psychedelics for treating mental disorders like PTSD. While Donald Trump has pledged to combat the opioid crisis by improving access to treatment and abuse-deterrent painkillers, his appointees have also called for stricter marijuana enforcement and drug sentencing. Good luck with that: Such a hard-line stance would run counter to state reforms—and the $6 billion marijuana industry.

A few updates on volcanic eruptions at the start of 2017:
Alaska
The biggest volcanic news of the last month has been the surprising and vigorous eruptions at Bogoslof in the Aleutians of Alaska. After producing a large explosive eruption on December 16, the volcano has followed up by adding more explosions every day or so, many of which reached 5 to 10 kilometers (15,000-35,000 feet) over the small island. Just yesterday (January 4), the volcano produced another explosion that reached 10 kilometers (33,000 feet). The highly explosive nature of these eruptions is likely rooted in the interaction between the new magma reaching the surface and the abundant seawater it meets when it erupts. That water can quickly flash to steam and help fragment the magma into ash, adding to the explosivity of the eruption.
The Alaska Volcano Observatory has released a preliminary map (see below) that show the changes to the island, where these explosive eruptions have destroyed part of the previously existing island and added more land, mainly in the form of volcanic debris from these blasts. AVO currently has Bogoslof on a Red/Warning alert status because of the unpredictable nature of these towering explosive shots, many of which have been obscured by clouds. Instead, evidence of eruptions from Bogoslof has been picked up either by seismometers on distant volcanoes (like Alaska’s Okmok) or via infrasound that detects low frequency sounds from explosion over very long distances (100s to 10,000s of kilometers).
Update 1/4/2017 6:30 PM EST: Looks like Bogoslof had another explosive eruption today. Check out this Himawari-8 loop showing the great shot upwards of the plume and the lateral spreading, creating a plinian column that looks like an umbrella.
The #Bogoslof #volcano in the Aleutians had yet another explosive eruption this afternoon, via #Himawari visible; IR temp down to -58 C pic.twitter.com/pWgq5Q1Rqg
— Dan Lindsey (@DanLindsey77) January 5, 2017

Hawai’i
Meanwhile, over at Kilauea on the big island of Hawai’i, the current action is related to part of the volcano collapsing into the sea. Part of a lava bench/delta (see top) built by lava flows reaching the Pacific Ocean collapsed. These benches are highly unstable, which is why the NPS and USGS try to convince tourists to stay away from ocean entries lest you become part of the collapse into lava and boiling hot steam/water surrounded by acidic vapors (sounds pleasant).
This collapse also means that the lava reaching the ocean is vigorously pouring into the sea, leading to some impressive steam and ash plumes coming from that water-lava interaction (a little like a mini-Bogoslof). The NPS posted a video of the lava from the exposed lava tube shooting into the ocean at the site of the bench collapse. The steam plume comes from lava at 1200°C hitting ocean water at ~15°C—quite a thermal shock that leads to the small explosions you can see in the video.

Iceland
In Iceland, Katla had a small earthquake swarm that produced a M3.5 temblor. This is not unusual for the Icelandic volcano and likely a similar (but much smaller) version of the swarm from earlier in 2016 (which didn’t lead to any eruption). Oh yes, and don’t believe all those news articles claiming that the new study in Earth and Planetary Science Letters is “predicting” more eruptions soon from Iceland like the Eyjafjallajokull in 2010. Instead, that research examined the record of activity and found that they are more common than we might have previously thought (every 44 instead of 56 years, on average). It doesn’t change a single thing about the state of volcanoes in Iceland—we just know more now about how common those eruptions could be on average.

It's the winter avocado harvest at the University of California's orchard in Lindcove, and the fruit jumbled in the back of Eric Focht’s SUV are a palette of earthy tones, some rough and flecked with frosted tips, others green and smooth. The horticulturist selects three miniature fruit, bright green and rotund, which together fit easily in the palm of his hand. “We were thinking of calling it the Lunch Box,” says Mary Lu Arpaia, who oversees the avocado breeding program at the orchard. But for now it’s just an experimental variety, officially known by a string of numbers. It’s too soon to tell if “Lunch Box” will ever be released to the world.
Chances are you haven’t eaten—or perhaps even heard of—an avocado other than the Hass, which makes up 95 percent of the US market. Beloved in guac or trendy on toast, Americans ate 2 billion pounds of them last year, more than quadrupling consumption 15 years ago. But for California avocado growers, that astounding growth is limited by climate and geography. Twenty years ago, a thin coastal corridor in Ventura and San Diego counties met nearly all of the country’s demand for the finicky, water-intensive fruit. Now the region supplies around 10 percent, overtaken by a flood of imports from Mexico and South America as California’s spigots run dry and planted acreage wanes.
The California avocado industry grew on the back of its star variety, and consumers continue to demand more. But Arpaia envisions a time when the avocado will be more like an apple, with unique varieties harvested in different seasons and across an expanded geography—perhaps even here in the stifling heat of the San Joaquin Valley, hundreds of miles north from the coastal epicenter of US production. “If we want to stay on the game as California avocado growers, we need to bring diversity back,” she says. For 70 years, the university’s breeding program has worked in close partnership with the avocado industry, which pays for the care of Lindcove’s experimental trees. But today, in these boom times for the Hass, it’s unclear if the industry is willing to gamble on anything else.
The Hass avocado (which rhymes with “pass,” by the way) was an accidental discovery—a seedling of unknown parentage planted in 1926 by its namesake, postman Rudolf Hass. Farmers heralded the productive and compact tree, and shoppers loved the buttery, nutty fruit. But it wasn’t perfect. “Its single disadvantage is its black color which has been associated in the minds of the public with poor quality fruits,” said a report in the 1945 yearbook of the California Avocado Society. Avocados of the time came in many shapes, sizes, and colors—and the most popular variety, called Fuerte, ripened green.
Shoppers, already skeptical of this exotic, big-seeded snack of Pleistocene megafauna, couldn’t discern unripe from rotten. So in the 1970s, the avocado industry looked to the banana for inspiration, which had perfected the art of pre-ripening fruit for the shopper. Hass fruit has the benefit of hanging for months on the tree—a kind of natural storage—and once off the tree, a carefully controlled supply chain can see the fruit darken just as it reaches grocery shelves. Black skin, to quote a 1980s marketing campaign by the California Avocado Commission, told wary shoppers that a Hass was “ripe for tonight.”
Once demand for the fruit exceeded the output of the brief California harvest season, the US opened its market to Mexican avocados—but only to the Hass, which had been individually cleared as hosts of invasive pests. Since the mid-1990s, imports have dominated the market, enabling packers to streamline year-round supply chain carefully tuned to the Hass’s preferred temperature and humidity. In years when California’s drought-stricken farms fail to produce—as the Avocado Commission expects next year, when yields are expected to plummet by half—they can easily fill the difference. “They give you something that’s very predictable,” Arpaia says.
Just as predictable, however, are the downsides of a Hass monocrop. The explosive growth of Hass acreage represents a petri dish for pests. “In history the worst case of famine and epidemics are because of monoculture” says Patricia Manosalva, a plant pathologist and biochemist at the University of California-Riverside. She points to laurel wilt, a disease spread by an invasive beetle that has felled vast stands of redbay trees, which share the Persea genus with avocados. The disease has gradually spread west from Georgia to Texas—not far from Mexico, the avocado’s ancestral homeland, and a natural germplasm of native relatives. “We may be looking at a nuclear bomb for the industry."
Lindcove sits on the eastern edge of the San Joaquin Valley, where California’s vast expanse of farmland abruptly rises into the chalky yellow foothills of the Sierra Nevada. A northern outpost of the University of California-Riverside, the avocado breeding program here is funded by the California Avocado Commission. Tonight, the first freeze warning of the season is in effect, and workers test out a wind turbine that will blow a gentle breeze over the vulnerable trees. As we walk through the orchard, Arpaia pauses by an early victim—a drooping tree with leaves browned by cold air rushing downhill.
“The industry wasn’t really too keen about me putting a site here,” she says with a shrug. “But I’m stubborn and that’s why it’s here.” Summer in the San Joaquin Valley is even more dangerous. At around 90 degrees, the stomates on Hass leaves begin to close—any warmer and they’ll begin dropping their flowers and fruit. Temperatures at Lindcove regularly climb beyond 100 degrees.
The area could be well-suited to avocados in other ways, Arpaia contends: Despite the drought, water remains cheaper here and salinity—a bane of avocado roots—is less acute than in Southern California, where farmers increasingly rely on recycled water. Each year the program plants three new varieties here and at three other locations in the heart of avocado territory, keeping careful watch for outliers that perform well at extreme temperatures. “We can look at the role of environment and what it does to fruit development,” she says. In addition to experimental varieties—commercial growers call them “telephone number” varieties—the orchards also include heirlooms, like Fuerte.
Arpaia has also planted the industry standard Hass, both to see how to improve its tolerance and as a control. “It’s a very high standard,” Arpaia says. Only a few varieties have surpassed it, and none have caught on commercially. The current prodigal scion is Gem, which was patented and released six years ago. It’s more compact and less water-intensive than the Hass, says Rob Brokaw, who sells seedlings at Brokaw Nursery in Ventura County. “A few years ago, I would have said a new variety no matter what its virtues would have had no chance,” he says. But now he hopes to get ahead of the industry by teaming up with like-minded farmers and a packer to market the fruit through specialty retailers.
The experimental program’s slow progress is largely due to the painstaking process of classical breeding. For avocados, hand pollination is impossible—a single tree can produce more than a million blossoms, but just a few hundred will ever bear fruit. “So you just have to leave it open to whatever is going to come in,” says Focht. Although the breeders try to keep track of the potential dads whose pollen blows through, that’s also tricky because the avocado flower can act as male or female depending on the time of day.
But the real trouble lies in the avocado’s murky genetics. Avocados are remarkably heterozygous, meaning that selection by phenotype can yield surprises—good ones, like the unplanned wunderkind Hass, but mostly frustrations. “You can pick two parents that have a small tree architecture and hope that you’ll get small tree architecture out of it,” says Focht. “But then you get big trees and little trees out of it.” So once seeds are achieved, the team raises hundreds of seedlings—of which about 1 to 2 percent of which are worth keeping—to see what shape they’ll take and what fruit they’ll bear. The entire process, from pollination to patent, typically takes at least 17 years.
The breeding process, in many ways, is out of a different time. “We’re probably 20 years behind the apple industry at this point,” says Arpaia, which underwent its own transition from Red Delicious domination to the dozen or so varieties seen in grocery stores today. A complete avocado genome was only just sequenced in Mexico, and is not yet publicly available. Marker-assisted breeding—mapping phenotypic traits to specific molecular markers in the avocado DNA—is seen as the way to speed up selection, explains Manosalva. But she cautions that selection for complex, polygenic traits related to avocado fruit production likely remains far away.
In the meantime, progress is slow and expensive—prohibitively so, if you ask the program’s funders in the avocado industry. In 70 years of searching for new varieties, the breeding program had failed to produce a new variety that could best the Hass. So in 2014 the industry cut funding to a bare minimum—enough to keep the orchard’s vital germplasm going, but not to plant new varieties. Tim Spann, research program director at the California Avocado Commission, says the industry is concerned about long-term diversity and pests like laurel wilt. But there are more immediate worries for farmers, like the Hass’s tolerance to salinity and common pests like root rot. “We decided we can’t fund everything,” Spann says.
Arpaia acknowledges the program needs to evolve—there’s talk of sequencing the genome at the university and making it public. But she also plans to begin planting more trees at Lindcove this spring with traditional breeding, using a funding source from outside of the industry. And while she has high hopes for the nascent Gem variety, she still holds out hope for something that could truly thrive in the heat of the San Joaquin Valley. “That’s the dream,” she says, gazing across her wayward northern orchard. “Do we have anything out here that’s going to achieve that dream? I’m not so sure.”
California is having a notably wet winter. Since October, a succession of weather systems has greened the Golden State's valleys, whitened its mountains, and washed its rivers and reservoirs in rippling blue-green.
The state is currently between storms. The one that just ended was cold. It dropped snow as low as 2,500 feet in California's highlands. The successor storm, expected to hit on Saturday, will be warmer—forecasters are calling for snow levels to rise up to 9,000 feet. The temperature difference means some of the snow dropped by the former will get melted by rain from the latter. Short term, it could also be a disaster. Flooding could kill people, destroy homes, wreck roads. Longer term, the succession of storms might be a drought-buster. Or it could be the opposite, with rain melting the precious mountain snow that California relies on to survive its hot, dry summers.
These storms are a type of weather system called atmospheric rivers, narrow bands of high altitude, highly-concentrated water vapor. "They start in the tropics, and get carried to the midlatitudes by the front end of storm systems," says Mike Anderson, state climatologist with California's Department of Water Resources. The storms are important contributors to California's annual water budget. They can also beget major disasters. In 1997, an atmospheric river caused over a billion dollars worth of flooding across the state. NOAA is predicting that some rivers and streams—like the Merced River running through the Yosemite Valley—will reach similarly epic levels this weekend.
An atmospheric river's temperature is a major determinant to the ratio of harm to good it does. Cooler storms bring snow. Those that deliver rain are more complex, especially if the rain falls on pre-existing snow—like the storm coming this weekend will. If close to freezing, the rain might compress the existing snowpack, condensing powder into denser crud. "The rain can also trickle through the snow, or it can run right off the top and into a stream," says Anderson. Warm enough rain will melt some of the snow.
And too much warm rain could be disastrous. Snow is salvation against the periodic droughts that parch California. With a thorough winter packing, the mountains can hold much more moisture than all the state's lakes and reservoirs combined. In fact, those lakes and reservoirs are quite finite, which is why it is so important that any snow that falls during the winter stays frozen until summer. Otherwise, any melt that can't fit in the engineered storage systems gets released down the rivers, into the sea.
Flooding and snow melt are more likely when intense weather systems come in quick succession. "It is clear we will have both of these issues this weekend, with a very warm atmospheric river following on the heels of a very cold atmospheric river," says Daniel Swain, a fellow at UCLA's Institute of the Environment and Sustainability. "We have a lot of substantial snowpack in the Sierras right now, and a lot of that at lower elevations might be washed away as rain. I would be surprised if there wasn't a big contribution to snowmelt."
Not everyone agrees. Rain certainly melts snow, but winter rain—even precipitation coming from the warm tropics—might not have enough thermal energy to trigger significant thawing. "What ends up happening is higher altitudes get a lot more snow, and while lower altitudes lose snow, the runoff seeps into groundwater, so it's not all lost," says Marty Ralph, director of the Scripps Institute's Center for Western Weather and Water Extremes. The bigger worry, says Ralph, is flooding.
Worry, indeed. Flood control managers are planning ahead. The Bureau of Reclamation controls water levels at Lake Shasta, California's largest reservoir, which flows into the Sacramento River. He says they are actively monitoring the forecasts, while keeping an eye on downstream tributaries. "We don't want to put our flows on top of other peak flows," says Shane Hunt, a Bureau spokesperson. That could put a lot of pressure on Shasta dam, especially because the mountains surrounding its reservoir—except Mount Shasta itself—are short, and will get most of the weekend's precipitation as rain.
If California doesn't drown in the deluge, it might remember this week fondly come summer. "I think we could look back on this set of atmospheric rivers as the thing that broke the drought," says Ralph. But then, if the weather this winter is any indication, the state will have many more storms in the months to come.
NASA missions come packaged two ways. They're either deep explorations of the familiar—STEREO's focus on the sun, the International Space Station's study of what microgravity does to the human body—or a trip to some crazy place no one has ever seen before. But still, any strange, distant object the agency targets will likely hold some clue about the origins of life. Humans are spacefaring narcissists that way.
NASA's newly announced Lucy and Psyche missions fall squarely into the second category. The robotic missions, planned to launch in 2021 and 2023 respectively, are set to target mysterious, unstudied asteroids. Lucy will follow NASA's Juno mission out to Jupiter to study the Trojan asteroids orbiting with the gas giant, and Psyche will visit an odd metal asteroid, 16 Psyche, in the main asteroid belt—the only object of its kind in our solar system. Both missions hope to build on scientists' understanding of the history of the solar system, its planets, and, of course, life. Psyche may solve some space exploration problems, too. Because a metal asteroid? Sure sounds like a good place to mine.
Both Lucy and Psyche are the most recent missions of the long-running NASA Discovery program, which specializes in missions that are quick, focused, and inexpensive—capped at just $450 million each. Lucy and Psyche beat out a number of Venus-related planetary science proposals (we can already smell that beefcooking), and to be honest, it's clear why.
"Small bodies are really the fossils of planetary formation," says Hal Levinson, a planetary scientist at the Southwest Research Institute and Lucy's principal investigator, in a NASA press conference. Small objects like asteroids and comets are the building blocks of the universe, dating back to the solar system's infancy. Lucy and Psyche's targets have been whizzing around since the Sun was just 10 million years old. Venus ain't got nothing on that.
The Trojan asteroids around Jupiter—named after heroes of the Trojan war—are a vast swarm trapped by Jupiter's gravity. But rather than orbit Jupiter, they share Jupiter's orbit, with one half leading and the other trailing the planet on its trip around the sun. "It's the last never-before-explored population of objects this side of Pluto," says Alan Stern, New Horizons' principal investigator. The mission builds on the work Stern and his team did for New Horizons: According to Stern, much of the team and instrument payload are direct carryovers from the Pluto-bound mission. Asteroid mission whizzes from the OSIRIS-REx are helping out too.
Which isn't to say Lucy isn't breaking new ground with the Trojan asteroids. "One of the surprising aspects about the population is its diversity in color and spectra," says Levinson. That diversity points to the bodies forming in different areas of the solar system with distinct characteristics. Which is why Lucy is going to six of them. "In the history of unmanned exploration we have only visited six main belt asteroids, and that has revolutionized our understanding of the solar system," says Levinson. Studying so many new asteroids at the same time could provide a great leap forward.
Out in the main asteroid belt, 16 Psyche is in a class by itself. It is the only large metal object in the entire solar system. And according to the best scientific guesses, that means it's probably the core of a planet so heavily bombarded that the rocky exterior broke away completely. "This is the only time a human being is ever going to see a core," says Lindy Elkins-Tanton, Psyche's principal investigator and planetary scientist at Arizona State University. "The best we’ve ever imaged it is as a point of light. We don't know what a metal world—its cliffs, its mountains, its impact craters—is going to look like."
When you add the potential planetary science discoveries to the possible gains for space exploration and asteroid mining, scientists start to get pretty psyched about Psyche. "I’m hoping we fill in a whole number of steps in how we get from dust and sand grains and to a planet," says Elkins-Tanton. "And find resources needed by humans as we explore further into the solar system." Specifically, 16 Psyche might have hydrated minerals from which astronauts could extract water. And chances are some of those metals will be worth mining.
Plus, both Psyche and Lucy will build on the asteroid orbiting knowledge scientists have gained in planning and executing missions like OSIRIS-REx, Rosetta, and Dawn, making these small bodies even likelier to be the spacefaring leapfrog points of the future. Which doesn't mean that the path forward for these missions will be easy—both are still challenged by navigational and scientific unknowns. "We have to make science in the biggest increments we can," says Elkins-Tanton. "We're aiming toward big questions." Uncovering the origins of planets is a big ask, but in the 2020s, Psyche and Lucy will give it their best try.
This story originally appeared on High Country News and is part of the Climate Desk collaboration.
Eight years ago, President-elect Barack Obama wanted Colorado Sen. Ken Salazar to be his Interior secretary. David Hayes, who was leading Obama’s transition team for Interior and other agencies, remembers trekking to Salazar’s office on Capitol Hill at least twice to make the case for the Cabinet post.
He had the perfect bait. Three years earlier, Sen. Salazar had led a successful effort to require the Bureau of Land Management to authorize renewable energy projects on public land. The agency was supposed to approve 10,000 megawatts of solar, wind and geothermal electricity by 2015, but under then-President George W. Bush, its congressional mandate went nowhere. Hayes, seeing a rare opportunity, told Salazar that as Interior secretary, he’d have the chance to make renewables on public land a signature issue.
“We talked about renewable energy and how the Interior Department could turbo-charge potential renewable energy on public lands and make up for the historic and long-standing failure to give renewable energy anything like the attention fossil fuels had gotten on public lands,” Hayes recalled in a recent interview.
Salazar took the job, and made clean energy projects on public land a top priority. The initiative took the department from zero to 60 on renewables, and it is a clear example of the paradigm shift that the Obama administration brought to the West and to its energy development.
Eight years later, a new president-elect has dismissed climate change as a hoax, promised to revive coal and other extractive industries, and sworn to cut—or gut—the US Environmental Protection Agency. Come Jan. 20, 2017, many of Obama’s initiatives will be under sustained attack. Some of them won’t survive. But Obama helped transform the West’s view of its energy potential, and he encouraged the region to get involved in the global fight against climate change. Changes like that go deep and may prove harder to undo.
Obama and the West
——————
President Barack Obama’s environmental record reflects an inclination toward compromise and incremental progress: He delisted 29 recovered species, but weakened the Endangered Species Act; he designated over two dozen national monuments, more than any other president, but left other key public lands unprotected; he promoted tribal sovereignty, but made little progress in addressing the systemic inequalities in Indian Country; and he failed in his attempts to loosen Big Ag’s grip over small ranchers.
Yet Obama may well be remembered as the first leader to seriously address the foremost environmental issue of our times—climate change. Though he oversaw surges in oil and gas production, he embraced clean energy and tackled greenhouse gas emissions, drawing deep opposition from the fossil fuel industry along the way.
Now comes a president whose Cabinet choices appear innately friendly to extractives and hostile to public lands and environmental protections. The Republican-backed Trump administration has pledged to roll back as many of Obama’s decisions as it can. Still, it may prove hard for it to undo all the accomplishments of the 44th president.
The president’s work on climate change started slowly. During his first term, Obama spent most of his political capital on the Affordable Healthcare Act and his economic recovery plan to lift the nation out of recession. Following his re-election, however, he focused broadly on domestic energy production and later the growing threat of climate change.
In early 2012, Obama traveled to Boulder City, Nevada, to stand in the midst of a sea of photovoltaic panels at what was at the time the largest facility of its kind in the country. “I want everybody here to know that as long as I’m president, we will not walk away from the promise of clean energy,” he told the crowd. But he also underscored his commitment to drilling. “We are going to continue producing oil and gas at a record pace. That’s got to be part of what we do. We need energy to grow.”
In his 17-minute speech, which was entirely about energy, Obama did not use the term “climate change” once, signaling an administration-wide retreat that continued for many months. Congressional Republicans, some of whom deny that climate change is a threat and others who reject attempts to deal with it as economically risky, kept attacking. Meanwhile, activists grew impatient.
In February 2013, 48 climate scientists and activists were arrested after some of them cuffed themselves to the White House gate, determined to force Obama to make potentially politically perilous decisions to fight global warming, such as rejecting the proposed Keystone XL pipeline. Sierra Club Executive Director Michael Brune, who was among them, told me before the demonstration that their civil disobedience signaled “a new level of urgency regarding climate change, and a growing impatience about the lack of political courage that we’re seeing from the president and from leaders in Congress.” The demonstration also marked a major shift for some mainstream environmental groups, who began prodding the president more and cheering him less. This period also saw the rise of brasher environmental groups like 350.org and WildEarth Guardians, who staged large public demonstrations or tackled the president in the courts.
In response, Obama came out with his Climate Action Plan in June 2013. It outlined a sweeping agenda to use his executive powers to slash greenhouse gas emissions from power plants, reduce methane emissions from oil and gas production and cut the federal government’s carbon pollution. It also recommended preparing communities for bigger storms, rising seas and fiercer wildfires, and it called for better climate science. In January 2014, Obama recruited John Podesta, former chief of staff for President Bill Clinton, to implement the plan. Soon, the administration was ticking off successes.
In his final years in office, Obama has produced a powerful National Climate Change Assessment, preserved vast stretches of land as national monuments, won court battles over its clean car rules and the EPA’s right to regulate carbon pollution from power plants, drafted regulations to slash greenhouse gases, and negotiated major bilateral treaties with China, India and Brazil, as well as the historic Paris Climate Agreement with nearly every nation on the planet. What had started slowly was picking up steam.
Under Obama, the Interior Department started examining climate impacts across broad landscapes, combining the forces of various state and federal agencies and universities. The department set up and staffed 22 landscape conservation cooperatives across the country and eight regional climate centers. The National Park Service, which had no climate change program before Obama, has completed climate impact assessments on 235 of 413 of the nation’s parks—documenting intensified wildfires, hastened snowmelt, vanishing glaciers, rising sea and lake levels, warming streams and displaced plants and animals.
All told, Obama has elevated climate change’s importance for federal land and water managers and invigorated state and local action.
“It’s a gargantuan legacy,” says Douglas Brinkley, a historian at Rice University. “I put him as one of the top environmental presidents in history. He’s not Theodore or Franklin Roosevelt. But he’s in that league with Lyndon Johnson, J.F. Kennedy and Richard Nixon.” Climate change is shaping up to be a major issue for Obama’s post-presidential life. “It’s become personal to him. His wife and daughters have helped him reach this conclusion.”
Obama himself underscored his dedication on a trip to Yosemite National Park in June with the First Lady and their daughters. “When we look to the next century, the next 100 years, the task of protecting our sacred spaces is even more important,” he told some 200 invited guests, against the stunning backdrop of Upper and Lower Yosemite Falls. “And the biggest challenge we’re going to face, in protecting this place and places like it, is climate change. Make no mistake: Climate change is no longer just a threat; it’s already a reality.”
Throughout the West, climate change has exacerbated forest fires, threatened water supplies, flooded communities, killed millions of trees and irreversibly altered the landscape. As these consequences have become clearer, the Obama administration has helped steer the West toward a cleaner energy future.
Eight years after Salazar became Interior secretary, the BLM has approved plans for 15,000 megawatts of renewable power, enough to power millions of homes. Projects providing up to 5,500 megawatts’ worth of power are already built or under construction, mostly in California and Nevada.
By establishing a system for approving renewable energy projects on public lands, the Obama administration helped drive phenomenal growth in renewable electricity in the West and a precipitous drop in prices. “I think it is an unsung part of the administration’s legacy, and I think the administration can and should be taking credit for really creating the conditions for this huge clean energy revolution to take off,” says Rhea Suh, who was assistant secretary of Interior for policy management and budget until she became president of the Natural Resources Defense Council last year.
After Congress passed the Energy Policy Act of 2005, Ray Brady was tapped to be the BLM’s manager for implementing the law. With targets for renewable energy 10 years in the future, nothing much happened. The top staff at the agency gave the new program little notice. Expediting oil and gas production was their chief focus. The agency didn’t even open a renewable energy office. That all changed when Salazar walked in the door.
In his first secretarial order, in March 2009, Salazar moved up the deadline for permitting 10,000 megawatts of clean power on BLM lands three years, to 2012. “We have to connect the sun of the deserts and the wind of the plains with the places where people live,” Salazar said at the time. He pushed his staff to identify specific zones on US public lands suitable for large-scale production of solar, wind, geothermal and biomass energy.
This was a revolutionary vision at the time; there weren’t any large-scale solar plants anywhere in the United State. Brady had to travel to Spain in 2008 just to glimpse the technology. For decades, Brady had been an obscure bureaucrat, but suddenly he found himself regularly summoned to high-level meetings with Salazar and other Interior leaders. Meanwhile, Salazar met regularly with other Cabinet members—including the secretaries of Defense, Agriculture and the Treasury—to knock down barriers to nascent projects.
The timing was right: Obama had campaigned, twice, on the promise of clean energy and its ability to create good jobs for the future. And there was a growing market for renewable power, because many Western states had passed renewable energy requirements, while California was pursuing one of the world’s most aggressive commitments to greenhouse gas reduction.
The enormity of the endeavor really struck Brady when he first visited the Ivanpah Solar Generating System project in San Bernardino County, California, in 2012: Three shining towers, emerging from the desolate desert, each surrounded by a huge circular field of mirrors, 173,500 of them, and covering 3,500 acres of BLM land. (Critics say such facilities endanger birds and other wildlife, but the project stands as a monument to the shifting attitudes toward energy on public lands.)
For much of his career, Brady worked on oil and gas, where drilling pads covered a single acre. “It’s awe-inspiring,” said Brady, who recently retired from the BLM. “I was absolutely amazed by the scope and scale and size of the project. It had not sunk into me before that. It really was, in my mind, the most exciting period in my 40-year career.”
While nudging individual projects forward, the agency’s new renewable energy office worked to track down Western locations suited to solar power. They looked for easy access to transmission lines and big metropolitan areas, lack of conflicts with local tribes, and few risks to endangered wildlife and plants or other fragile natural resources. In these so-called solar energy zones, the agency conducts the environmental analysis up-front, to reduce permitting times. The BLM held its first-ever competitive auction for solar projects in the summer of 2014. Three companies won bids, and one recently started construction in Dry Lake, Nevada, north of Las Vegas.
Interior was much less successful at establishing wind power on public land. The Chokecherry and Sierra Madre wind project in south-central Wyoming, for example, has been a priority since Salazar took the helm at Interior. The enormous project would erect up to 1,000 wind turbines, employing as many as 1,000 people during peak construction, and eventually provide clean electricity to about a million homes. The BLM gave it basic approval in 2012, but many more permitting requirements remained. “To put it bluntly, they lost momentum,” says Bill Miller, president of two ­subsidiaries of the Anschutz Power Company of Wyoming and TransWest Express. Miller still believes in the project despite the delays. He told me: “There is no better wind asset in the country.” And he’s optimistic that he’ll get final approval before Obama leaves office to erect the first 500 turbines.
With plenty of windy places on private land, wind developers may simply ignore public land. But both geothermal and solar projects have a bright future, even under a Donald Trump administration. The price of photovoltaic solar systems continues to drop, making public land attractive for small and mid-sized projects, especially in areas where the agency has done the upfront work, so developers can get relatively quick ­approval. This fall, the administration and California state government completed the Desert Renewable Energy Conservation Plan, which charts a course for developing clean power across 22 million acres of desert. In November, the administration finished the regulations that will govern competitive leasing for renewable power projects on public land.
Still, when it comes to fossil fuels, the administration’s record remains mixed as far as what it did, and didn’t do, for the climate. Obama curtailed fossil fuel pollution but failed to significantly limit industry’s access to the public’s vast fossil fuel resources. Even while promoting renewable energy, the White House simultaneously supported an expansion of oil and gas drilling. Shale gas production grew fourfold from 2009 to 2015, oil production nearly doubled, and oil exports tripled.
On the regulatory side, though, the EPA set new rules to reduce leakage of methane, a potent greenhouse gas, from new oil and gas drilling. Near the end of the administration, the BLM went even further, setting new requirements to reduce methane leaks from existing oil and gas operations on public land.
Obama was slow to apply his climate change principles to fossil fuels beneath federal land. Throughout his administration, the Interior Department continued to lease federal lands for oil and gas development and fought in court against environmentalists’ “keep it in the ground” campaign.
Coal, long the mainstay of US electricity production, declined dramatically during Obama’s tenure, a fact that helped the nation reduce its greenhouse gas emissions. This was primarily due to competition from abundant, low-price natural gas, caused by the boom in hydraulic fracturing. But Obama’s air pollution policies played a role, too. By setting the first-ever limits for mercury and other toxic air pollutants, Obama forced companies to decide whether it was cheaper to install expensive pollution-control devices or switch to natural gas or renewables. “What the Obama administration rules did was force utilities to consider the question about whether or not to keep coal online,” the Sierra Club’s Brune explained.
But most of this progress was the result of the EPA’s work. It was only in the final 18 months of Obama’s term that Sally Jewell, who replaced Salazar as Interior secretary, started scrutinizing the department’s coal policies. She held listening sessions in coal country and in Washington, DC. In January, she set a moratorium on new coal leasing and ordered the first-ever analysis of greenhouse gas impacts from federal coal, which accounts for more than 40 percent of the coal used to produce electricity in the US. In Obama’s last State of the Union address, in January, he declared that it was time to revamp the way the country manages its coal and oil, “so that they better reflect the costs they impose on taxpayers and the planet.”
Despite this, the administration pulled its punches on federal coal until its final days. Most notable was its decision to support Colorado’s plan to allow expansion of coal mining into otherwise roadless national forest areas in the North Fork Valley (where High Country News is headquartered).
In 2014, a federal judge halted an expansion of Colorado’s West Elk Mine because the BLM and Forest Service had failed to take a “hard look” at the climate impacts that an exemption to the roadless rule would create. Environmental groups had sued, demanding that the BLM and Forest Service calculate the costs to society of greenhouse gas emissions from the mining and combustion of that federal coal.
In November, the Forest Service released an environmental impact statement that revealed that its preferred alternative could increase greenhouse gas emissions 433 million tons over time and cost society billions of dollars. Yet it continued to insist that the expansion should take place.
The pollution would come from burning the coal for electricity and from venting methane into the air during mining. Methane is high at West Elk because the coal seams are especially gassy.
Robert Bonnie, undersecretary of Agriculture for natural resources and the environment, justified the decision. “No one is under the belief that we’re going to immediately change the energy mix starting today,” he said. “There’s going to be some level of coal for some time to come.”
But Earthjustice attorney Ted ­Zukoski sees a deep hypocrisy in the decision. “There is a conflict between this administration’s soaring and bold rhetoric on the need to address climate change and its failure to keep fossil fuels in the ground,” he says. “Billions of tons of federal coal were leased on Obama’s watch.”
As for natural gas and oil, the administration purposefully avoided regulations that would slow the upsurge in production. “This administration was not willing or able to take on two fossil fuel industries at the same time,” Brune told me. “And it proactively took many steps to help support the gas industry. We’re going to be wrestling with the effects of that for decades. An increased reliance on natural gas is a disaster for our climate.”
During most of his administration, Obama faced Republicans in Congress who simply refused to legislate. In response, Obama turned to executive action. Now, however, Trump’s win endangers much of the progress he made. Trump has vowed to abandon the Paris climate treaty and cancel the Clean Power Plan. Although the specifics remain unclear, many of Obama’s other climate policies, such as his methane rules, are also at risk. But some important changes may escape Trump’s chopping block. The administration and its policies don’t stand alone, so they can have lasting impact. Obama’s energy and climate change policies augmented on-the-ground realities, such as many Western states’ eagerness to embrace renewable energy and the improving economics of solar power. “They helped facilitate it,” said Mark Squillace, law professor at the University of Colorado at Boulder. “But the story of the West will be about what the states are doing.”
In the Southwest, for example, local, state and federal government officials, scientists and businesses have long worried about the impacts of climate change on water supply, fragile species and wildfire. Obama’s conservation cooperatives and regional climate centers filled a void. “Everybody knew these things were happening,” said Jonathan Overpeck, director of the University of Arizona’s Institute of the Environment. “Now we have a mandate for research and figuring out what can we do about it. We’re trying to not just generate scientific knowledge for the sake of curiosity, but to make sure we’re generating science that’s useful.”
Hayes, meanwhile, who had been tapped for a big role in a Clinton transition, was flabbergasted by the election results. He hopes the Interior Department’s commitment to climate science will survive the new administration.
Even if research continues, many of Obama’s fossil fuel regulations surely will be targeted by Trump’s administration. The new EPA chief and Interior secretary could settle industry lawsuits by asking courts to send Obama’s rules—including the Clean Power Plan, methane rules and BLM’s fracking regulations—back to agencies to rewrite them. Environmental groups would then likely sue to block Trump’s new rules and reinstate Obama’s, and the ensuing legal battles could take years.
“If Trump gets only one term and is replaced by a Democrat, damage will be significant but also limited,” Squillace said. “I think if Trump gets two terms, all bets are off and significant change in public lands and environmental policy will occur.”

Another danger is a possible government “brain drain.” Squillace, for example, was a young lawyer at the Interior Department when President Ronald Reagan appointed Interior Secretary James Watt, who was hostile to conservation. Squillace remembers asking to be taken off one case after another, because he considered Watt’s positions indefensible. After nine months of this, he resigned. Trump may inspire a similar exodus of scientists and lawyers.
Regardless, some of Obama’s climate policies likely will withstand at least the early years of a Trump administration, particularly the BLM’s renewable energy program. If Trump kills the Clean Power Plan, that would take away one driver for big solar projects on public land. But others won’t disappear, most significantly, California Gov. Jerry Brown’s directive that his state gets 50 percent of its power from renewable sources by 2030.
Steve Black, who was Salazar’s counselor at Interior and now is an energy and climate policy consultant based in California, sees other reasons for optimism. More than 100 full-time, career BLM staffers work in renewable energy offices across the West that didn’t exist before Obama. Massive projects like Ivanpah will keep delivering clean power to the grid. “There’s steel in the ground,” he said. “We built 15 utility-scale projects. Those things can’t be changed. I do think there are lasting elements of this legacy.”
Despite Trump’s cheerleading for coal, the new administration is unlikely to rescue the dirtiest fossil fuel. Market forces, namely low natural gas prices, are the main reason for its downturn, but the growing international desire to combat climate change is another. Trump similarly is unlikely to boost oil and gas production, as long as prices are low. For instance, Trump and a Republican Congress may open the Arctic Wildlife Refuge to oil companies, but high costs could deter drilling.
And even with a president and Congress unwilling to tackle tough questions on energy and the climate, states will remain largely responsible for their own energy choices. Even with big utilities fighting hard against solar, low renewable energy prices and state mandates will make the clean energy revolution hard to stop. It’s unlikely that Trump will want to be responsible for killing the good jobs that renewable energy is creating. For all its starts and stops, the Obama administration helped the West embrace a clean energy future that takes climate change into consideration. Trump’s administration won’t be able to change that.
This story was funded with reader donations to the High Country News Research Fund.
Correspondent Elizabeth Shogren writes HCN’s DC Dispatches from Washington.

One chemical reaction rules the world: Water plus rocks plus cement. That's concrete, and it equals most of the built world. Concrete is the spine of skyscrapers, the span of bridges, and the bulk of dams. It is also one of the single biggest contributors to carbon dioxide emissions on Earth. The cement industry in the US contributes about 3.5 percent of the country's emissions. Worldwide, the industry contributes to about 5 percent of all emissions.
So, even though concrete mixing facilities don't blight the landscape like smoking oil refineries or jammed freeways, they are ripe for sustainable innovations. Like this one: Use less concrete by making it stronger. Ironically, this strength could come by making cement—the chemically-reactive powder that forms a brittle paste when exposed to water—with more molecular defects.
"What first comes to mind is that defects are bad for material," says Rouzbeh Shahsavari, a materials engineer at Rice University in Houston. But in a new paper published this week in Applied Materials and Interfaces, he and two co-authors found that when when it comes to concrete, this is not quite the case.
They started by looking for clues to what made cement so strong. Using advanced microscopy techniques, they peered into layers of tobermorite, a cement mixture used by the Romans. They noticed twisting imperfections in the layers of material—in engineering parlance, they're called screw dislocations. Intrigued, Shahsavari started playing with computer models of tobermorite riddled with these imperfections.
"These are not the kind of models where you can fiddle with variable parameters and stuff like that," he says. Unlike weather models, which are too complex to yield exact predictions, these chemical reaction models use quantum calculations to derive specific interactions between individual molecules. When Shahsavari applied pressure to the modeled tobermorite, the imperfections would guide the stress out to the edge, instead of absorbing it—and creating a crack. Even more incredible, the imperfection would spread to neighboring molecules, increasing the material's flexibility.
Concrete is mostly known for being really strong. And while it is also tough, concrete is also known for developing spiderwebbing cracks. "These two parameters, strength and toughness, are often contradictory," says Shahsavari. He believes he could make concrete that is twice as strong by optimizing these screw dislocations.
Every time anyone mixes concrete, they will create imperfections. This is partly because the process itself is messy. But it's also because it's so easy to contaminate the mix. Cement is mostly calcium, silica, and aluminum. But if you put the right impurities in that dry cement—things like sodium, magnesium, and iron—and in certain percentages, then they may increase the number of these desirable imperfections.
Shahsavari wants to play around with impurities. He also thinks he might be able to induce imperfections by messing around with the ambient temperature during controlled situations like pre-cast concrete—huge slabs manufactured in factories and shipped to construction sites. Shahsavari admits that this would be nearly impossible at traditional outdoor pours, where concrete is delivered and mixed in trucks in all sorts of weather.
He and other researchers also need to play around with other cement mixtures. There's no standard chemical definition for cement: People change the ratio between calcium and sodium to tweak the properties of their final mix. Tobermorite has a calcium-sodium ratio of 1:1, making it easy to identify the basic behaviors of screw dislocations and apply them to other cement compositions.
It could be years before all that research is done and Shahsavari's Miracle Ready Mix is standard at every Home Depot. Even then, it won't solve issues like rust corrosion—currently causing consternation for commuters crossing Northern California's new Bay Bridge. "That is a durability problem, because bad chemicals are penetrating the pores into the cement," he says. "But, once you have stronger concrete, your surface area is half, and less exposure to water, so chance of getting damaged would be half." That's a pretty strong argument for a stronger material.
It's been a long time since I've skied. Of course, the fun is in going down the mountain, not going up. But you have to go up to get down, so what's the best way of doing that in terms of energy and power? Let's examine a few options in an excellent example of physics in action.
I can't believe I just said that—it sounds like it's straight out of a middle school textbook. Is this what I have become?
In case you've lived your life on the beach and have never seen a ski slope, let me provide an introduction to chairlifts. You start at the bottom of a hill (or mountain), don your skis, and wait in line. After what seems like ages, you get to the lift and carefully position yourself in front of a moving chair, which scoops you up. Boom—that's it. The chair, which hangs from a cable, carries you to the summit, where you must quickly dismount and hope you don't embarrass yourself by tumbling into anyone or anything.
Simply put, the goal of a chairlift is to move you up an incline. I'll represent this in a simple manner:

Now for physics. This example provides two things to discuss: work and power. The work is merely the change in energy of the skier. Assuming the skier starts and ends at rest, the only change in energy is the gravitational potential energy. This depends only on the height of the slope and the mass of the skier. It doesn't depend on horizontal motion since it doesn't require any force to move the skier in this direction (assuming negligible friction).

What about power? The quantity of work is measured in Joules, but power is measured in Watts. Power is a measure of how fast you can work. If it takes a time interval of Δt to ascend the slope, the power would be:

What is the work and power per person of a traditional chairlift? I offer some rough estimates—it's one thing I excel at because if I'm wrong I can blame the estimate. Suppose the slope is 1,000 feet high (305 meters) and it takes four minutes to ride with a skier mass of 70 kg. I can calculate the work and power:

There we have it. That is the power of a traditional chairlift for one person. If you want to carry more people, the power must obviously increase. But still, 871 watts per person is significant. Let's say there are 100 people riding the chairlift simultaneously (not in the same seat, silly). This would require 87 kilowatts. That leads to a tough power bill (and assumes everything is 100 percent efficient).
Where does this energy come from? That depends on the local power company. It could be generated from nuclear, or natural gas, or something renewable. It depends.
Suppose you want an "off grid" ski resort. That would be pretty cool, but how would skiers ascend the slopes? What about some type of pedal-powered tram? Passengers get inside the train (after removing their skis) and start pedaling.
No matter how you climb the slope, it requires the same amount of work—209 kJoules. Of course normal humans can't produce 800 watts for 4 minutes. That's just crazy. What about something more reasonable like 50 watts? Assuming a human can pedal 50 watts, I can calculate the time needed to reach the top of this slope:

That's more than an hour to climb 1,000 feet. What if you want to go up 3,000 feet? That would take over three hours. That's not such a great vacation—unless your goal is to get some exercise. Then you are in the right place.
The perfect thing for an off-grid ski resort would be solar panels. Let's say the resort uses a 1 meter by 1 meter panel for each skier. What kind of power could you get from this and how long would it take to ascend the mountain? Oh, just assume there is a train with an electric motor if you like.
How much power do you get from a solar panel? Well, the 2best you can do for now is around 175 Watts/m2, so I will use that. Of course since the ski lift will use a 1 square meter panel, it will produce 175 Watts. But wait. There are some important notes.
That's it. With 175 Watts, how long would it take to climb the mountain? Using the same math, I get a time of 19 minutes for 1,000 feet of elevation. That's not super fast, but it's not too bad. The bad part would be using this at night time or in cloudy weather.
I guess we should stick to traditional chairlifts for now.
In the old days, if you wanted to look inside someone’s body, you had to cut them open. Less than ideal. Then came the miraculous X-ray machine, which peered inside the human form, no scalpels required. Nowadays, X-rays can even show your insides *moving *in unprecedented detail, allowing doctors to watch the flow of blood through the heart.
But X-rays have a shortcoming: They can only create images in two dimensions. CT scans, however, image bone in highly detailed 3-D. So what if you combined the two techniques, layering a 3-D CT scan over a 2-D X-ray movie? Well, you’d get something called XROMM, or X-ray Reconstruction of Moving Morphology—technology that’s giving researchers a stunning look at bones in motion. And if a recent paper on the open-source software behind XROMM is any indication, the technique is poised to transform the field of biomechanics.
That has big implications not just for human medicine, but for science’s understanding of animal anatomy. “It lets us understand, for example, the details of how the ends of bones—where they interact at joints—how the shape of that relates to the way that the bones actually move,” says Brown University biologist Beth Brainerd.
Say you’ve got a guinea fowl—kind of a chicken-like bird—and you want to see how its legs actually work. How the bones move, how they fit together, how joints bend. First you implant tiny metal pins in the leg bones, and then give the bird a good CT scan. Next you take X-ray video of it running around an arena. And the computer does the rest: The aforementioned open-source software works with the animation program Maya to layer the CT scan of the bones on top of the X-ray movie, using the metal pins as reference points.
What you end up with is an entrancing video of the guinea fowl’s leg bones in motion. “It's always this weird disembodied bird, just a pelvis and legs,” says Brown’s Stephen Gatesy, who dreamed up the technique. “It looks like something from RoboCop.”
The technology can give researchers a better look at how bones move, and even how feet interact with the ground. Gatesy also has the guinea fowl traipse through radiolucent poppy seeds—which are less visible in X-rays than something like sand, so they don't obscure the bone—to produce footprints. Those tracks can vary greatly depending on the viscosity of the material an animal is treading on. "We're trying to kind of overturn this idea that tracks always look like feet," Gatesy says. "And so the motion becomes critical for that."
Tracking the guinea fowl's movement could be big for paleontology, of all things. Birds are, after all, descended from dinosaurs, and dinosaurs left a lot of tracks behind. But depending on the consistency of mud, the footprints of a single species can vary widely. Theoretically, experiments with guinea fowl could shed light on how dinosaurs' feet shifted as they sunk into the muck. The work could also hint at how dinosaurs got around. Keep in mind that paleontologists only have dino bones to work with—no tendons or muscles. “If we can understand much better how the shapes of bones relate to motion, we can come up with more rigorous interpretations of how dinosaurs might have moved,” says Brainerd.
Beyond birds and dinosaurs, the technology has also peered through a turtle’s shell. (The reptile swings its hips when it walks, in case you were wondering.) And it’s helping model the majesty that is the vacuum-face-feeding technique of fishes. (Many fishes hunt by rapidly opening their maws, creating a vacuum that sucks prey in.)
As for humans, the technique could help determine why, for example, women suffer more ACL injuries than men. (When it comes to people, researchers aren't implanting pins in bones, of course. In this case, technicians manually join the CT scans and X-ray movies.) And perhaps more pressingly, it might be of use for amputees who endure uncomfortable prostheses. “One of the things that some of our collaborates at the VA hospital in Providence are doing is looking at how the residual limb moves inside a prosthesis to try to improve the interface,” says Brainerd.
With the melding of CT scans and X-rays, researchers are finally bringing a hint of Superman to science. Now if we could only get some invulnerability and laser eyes...
When you were a kid, did you dig holes in the ground, lie in wait, and drag your enemies to their doom? Because the tiger beetle sure did. And when it grows up, it’s even more ferocious, running down its prey and tearing it to pieces with massive jaws. Check out this week’s episode of Absurd Creatures to learn more!
Find every episode of Absurd Creatures here. And I’m happy to hear from you with suggestions on what to cover next. If it’s weird and we can find footage of it, it’s fair game. You can get me at matthew_simon@wired.com or on Twitter at @mrMattSimon.
In a lab buried under the Apennine Mountains of Italy, Elena Aprile, a professor of physics at Columbia University, is racing to unearth what would be one of the biggest discoveries in physics.
She has not yet succeeded, even after more than a decade of work. Then again, nobody else has, either.
Original story reprinted with permission from Quanta Magazine, an editorially independent division of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences
Aprile leads the XENON dark matter experiment, one of several competing efforts to detect a particle responsible for the astrophysical peculiarities that are collectively attributed to dark matter. These include stars that rotate around the cores of galaxies as if pulled by invisible mass, excessive warping of space around large galaxy clusters, and the leopard-print pattern of hot and cold spots in the early universe.
For decades, the most popular explanation for such phenomena was that dark matter is made of as-yet undiscovered weakly interacting massive particles, known as WIMPs. These WIMPs would only rarely leave an imprint on the more familiar everyday matter.
That paradigm has recently been under fire. The Large Hadron Collider located at the CERN laboratory near Geneva has not yet found anything to support the existence of WIMPs. Other particles, less studied, could also do the trick. Dark matter’s astrophysical effects might even be caused by modifications of gravity, with no need for the missing stuff at all.
The most stringent WIMP searches have been done using Aprile’s strategy: Pour plenty of liquid xenon—a noble element like helium or neon, but heavier—into a vat. Shield it from cosmic rays, which would inundate the detector with spurious signals. Then wait for a passing WIMP to bang into a xenon atom’s nucleus. Once it does, capture out the tiny flash of light that should result.
The time projection chamber at the heart of the detector is filled with 3.5 metric tons of liquid xenon.
These experiments use progressively larger tanks of liquid xenon that the researchers believe should be able to catch the occasional passing WIMP. Each successive search without a discovery shows that WIMPs, if they exist, must be lighter or less prone to leave a mark on normal matter than had been assumed.
In recent years, Aprile’s team has vied with two close competitors for the title of Most-thorough WIMP Search: LUX, the Large Underground Xenon experiment, a U.S.-based group that split from her team in 2007, and PandaX, the Particle and Astrophysical Xenon experiment, a Chinese group that broke away in 2009. Both collaborators-turned-rivals also use liquid-xenon detectors and similar technology. Soon, though, Aprile expects her team to be firmly on top: The third-generation XENON experiment—larger than before, with three and a half metric tons of xenon to catch passing WIMPs—has been running since the spring, and is now taking data. A final upgrade is planned for the early 2020s.
The game can’t go on forever, though. The scientists will eventually hit astrophysical bedrock: The experiments will become sensitive enough to pick up neutrinos from space, flooding the particle detectors with noise. If WIMPs haven’t been detected by that point, Aprile plans to stop and rethink where else to look.
Aprile splits her time between her native Italy and New York City, where in 1986 she became the first female professor of physics at Columbia University. Quanta caught up with her on a Saturday morning in her Brooklyn high-rise apartment that faces toward the Statue of Liberty. An edited and condensed version of the interview follows.
QUANTA MAGAZINE: How closely do you follow the theoretical back and forth about the nature of dark matter?
ELENA APRILE: For me, driving the technology, driving the detector, making it the best detector is what makes it exciting. The point right now is that in a couple of years, maybe four or five in total, we will definitely say there is no WIMP or we will discover something.
I don’t care much about what the theorists say. I go on with my experiment. The idea of the WIMP is clearly today still quite ideal. Nobody could tell you “No, you’re crazy looking for a WIMP.”
What do you imagine will happen over the next few years in this search?
If we find a signal, we have to go even faster and build a larger scale detector which we are planning already—in order to have a chance to see more of them, and have a chance to build up the statistics. If we see nothing after a year or two, the same story.
The plan for the collaboration, for me and how I drive these 130 people, is very clear for the next four or five years. But beyond that, we will go almost to the level that we start really to see neutrinos. If we end up being lucky—if a supernova goes off next to us and we see neutrinos—we will not have found dark matter, but still detect something very exciting.
How did you get started with this xenon detector technology?
I started my career as a summer student at CERN. Carlo Rubbia was a professor at Harvard and also a physicist at CERN. He proposed a liquid-argon TPC—time projection chamber. This was hugely exciting as a detector because you can measure precisely the energy of a particle, and you can measure the location of the interaction, and you can do tracking. So, that was my first experience, to build the first liquid-argon ‘baby’ detector—1977, yes, that’s when it started. And then I went to Harvard, and I did my early work with Rubbia on liquid argon. That was the seed that led eventually to the monstrous, huge liquid-argon detector called ICARUS.
Later, I left Rubbia and I accepted the position of assistant professor here at Columbia. I got interested in continuing with liquid-argon detectors, but for neutrino detection from submarines. I got my first grant from DARPA [the Defense Advanced Research Projects Agency]. They didn’t give a damn about supernova neutrinos, but they wanted to see neutrinos from the [nuclear] Russian submarines. And then we had Supernova 1987A, and I made a proposal to fly a liquid-argon telescope on a high-altitude balloon to detect the gamma rays from this supernova.
I studied a lot—the properties of argon, krypton, xenon—and then it became clear that xenon is a much more promising material for gamma-ray detection. So I turned my attention to liquid xenon for gamma-ray astrophysics.
How did that swerve into a search for dark matter?
I had this idea that this detector I built for gamma-ray astrophysics could have been, in another version, ideal to look for dark matter. I said to myself: “Maybe it’s worth going into this field. The question is hot, and maybe we have the right tool to finally make some progress.”
It’s atypical that the NSF [National Science Foundation], for someone new like me, will fund the proposal right away. It was the strength of what I had done all those years with the a liquid-xenon TPC for gamma-ray astrophysics. They realized that this woman can do it. Not because I’m very bold and I proposed a very aggressive program—which of course is typical of me—but I think it was the work that we did for another purpose which gave the strength to the XENON program, which I proposed in 2001 to the NSF.
What was it like to go from launching high-altitude balloons to working underground?
We had quite a few balloon campaigns. It’s something that I would do again, and I didn’t appreciate it then. You get your detector ready, you sit it on this gondola. At some point you are ready, but you can’t do anything because every morning you go and you wait for the weather guy to tell you if it’s the right moment to fly. In that scenario you are a slave to something bigger than you, which you can’t do anything about. You go on the launch pad, you look at the guy measuring, checking everything, and he says “No.”
New Neutri­no Anomaly Hints at a Matter-Antimat­ter Rift
A Grand Unified Theory of Physics Really Wishes Protons Would Just Decay Already
Inside the Hunt for a Ghost Particle
Underground, I guess, there is no such major thing holding you from operating your detector. But there are still, in the back of your mind, thoughts about the seismic resilience of what you designed and what you built.
In a 2011 interview with The New York Times about women at the top of their scientific fields, you described the life of a scientist as tough, competitive and constantly exposed. You suggested that if one of your daughters aspired to be a scientist you would want her to be made of titanium. What did you mean by that?
Maybe I shouldn’t demand this of every woman in science or physics. It’s true that it might not be fair to ask that everyone is made of titanium. But we must face it—in building or running this new experiment—there is going to be a lot of pressure sometimes. It’s on every student, every postdoc, every one of us: Try to go fast and get the results, and work day and night if you want to get there. You can go on medical leave or disability, but the WIMP is not waiting for you. Somebody else is going to get it, right? This is what I mean when I say you have to be strong.
Going after something like this, it’s not a 9-to-5 job. I wouldn’t discourage anyone at the beginning to try. But then once you start, you cannot just pretend that this is just a normal job. This is not a normal job. It’s not a job. It’s a quest.
Aprile in her lab at Columbia’s Nevis Laboratories.
In another interview, with the Italian newspaper La Repubblica, you discussed having a brilliant but demanding mentor in Carlo Rubbia, who won the Nobel Prize for Physics in 1984. What was that relationship like?
It made me of titanium, probably. You have to imagine this 23-year-old young woman from Italy ending up at CERN as a summer student in the group of this guy. Even today, I would still be scared if I were that person. Carlo exudes confidence. I was just intimidated.
He would keep pushing you beyond the state that is even possible: “It’s all about the science; it’s all about the goal. How the hell you get there I don’t care: If you’re not sleeping, if you’re not eating, if you don’t have time to sleep with your husband for a month, who cares? You have a baby to feed? Find some way.” Since I survived that period I knew that I was made a bit of titanium, let’s put it that way. I did learn to contain my tears. This is a person you don’t want to show weakness to.
Now, 30 years after going off to start your own lab, how does the experience of having worked with him inform the scientist you are today, the leader of XENON?
For a long time, he was still involved in his liquid-argon effort. He would still tell me, “What are you doing with xenon; you have to turn to argon.” It has taken me many years to get over this Rubbia fear, for many reasons, probably—even if I don’t admit it. But now I feel very strong. I can face him and say: “Hey, your liquid-argon detector isn’t working. Mine is working.”
I decided I want to be a more practical person. Most guys are naive. All these guys are naive. A lot of things he did and does are exceptional, yes, but building a successful experiment is not something you do alone. This is a team effort and you must be able to work well with your team. Alone, I wouldn’t get anywhere. Everybody counts. It doesn’t matter that we build a beautiful machine: I don’t believe in machines. We are going to get this damn thing out of it. We’re going to get the most out of the thing that we built with our brains, with the brains of our students and postdocs who really look at this data. We want to respect each one of them.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Ah, the New Year. A time for resolutions, for self-betterment, for dumping out the old and bringing in the new. Unfortunately, if you celebrate New Year's Eve with champagne and shots, chances are you may spend the first day of the new year on the toilet.
Scientists don’t entirely understand what causes hangovers, but every over-imbiber knows its effects: puffy skin, headaches, nausea, the feeling when you drink water that you just straight-up guzzled acid. And any regular drinker knows the agony and the ecstasy of the hangover poo. It can feel like the only way to relief: the unfortunate, unavoidable end of any excruciating hangover. The morning after the WIRED holiday party—which started with cocktails at a classy bar and devolved, as is tradition, into late-night scream-singing with buckets of beers at a local karaoke joint—saw the shared office bathroom morph into a toxic warzone.
Next-day diarrhea isn’t universal and, for some, alcohol actually causes constipation. But it’s common enough that it has some well-known, and rather unendearing, nicknames—beer shits, day-after-drinking shits (DADS), rum bum, after-grog bog, and so on. Anyone who’s dealt with it knows it can be rough.
But why does it happen?
To start, you have to understand how the body processes alcohol. Food breaks down in the mouth and stomach before nutrients are absorbed into the bloodstream through the small intestine. Alcohol, on the other hand, bypasses some of that system, splashing down into the stomach, which absorbs about 20 percent of alcohol. The rest moves to the small intestine, which absorbs the remaining booze and sends it along to the liver to metabolize. While your liver processes the equivalent of about one drink per hour, the rest of the alcohol circulates throughout your blood system.
Everything Science Knows About Hangovers—And How to Cure Them
What’s Up With That: Why Does Sleeping In Just Make Me More Tired?
What’s Up With That: Why Running Hurts Every Part of Your Body
Alcohol affects every organ in the human body through the blood stream, including the brain. That's part of why, when you drink heavily, you stumble around and slur your speech. And particularly relevant to this discussion, alcohol depresses the secretion of anti-diuretic hormone in the posterior pituitary gland. Also known as vasopressin, it helps your kidneys balance the amount of water in your body. Now, because your body can’t hold on to water as it normally would, you start to expel what you don't absorb—and you find yourself scrolling through Instagram while waiting in the bathroom line to pee. (Once the body catches up, it starts retaining water, which is why you may wake up bloated and puffy.)
Diarrhea is a common side effect of diuretics, sure, but alcohol also inhibits the absorption of liquids in your bowels. Studies of alcoholics show that chronic alcohol consumption affects the sections of the intestine that absorb water and sodium, decreases the activity of the enzymes that break down sugars, and make the mucosa more permeable). All of which leads to—you guessed it—diarrhea.
Plus, when you’ve been pounding boozy beverages—and, hopefully, some water—your body has to process much more liquid than usual. And all the while, the rest of your GI tract is getting wrecked, too. Booze affects the muscles surrounding your stomach and intestine, particularly those that hold on to food for digestion. It also reduces contractions in the rectum, which might "reduce the transit time—and, thus compaction" of the food in your large intestine which, again, can cause diarrhea.
This doesn’t even take into account the indigestion-causing, late-night cheese fries you scarfed down or the mixers you had with your booze. And, depending on the person, artificial sweeteners (see: Diet Coke and rum), gluten (beer), and tannins (red wine) can all cause loose stool.
The best way to avoid a hangover and the dreaded DADS is to eat a balanced meal before imbibing, stay hydrated, and generally take it easy on the booze. But, if you prefer to live on the wild side, be prepared to ride it out the next day in the bathroom. And maybe think about stocking up on Imodium on your way to the party.
If your resolution for the coming year is to spend less time on your commute scrolling through Twitter or playing "Puzzler on the Roof," there's no shortage of fantastic and fantastical new books you can use to take a break from mindless screen time. Curating this year's new arrivals was tough, but we managed to narrow the list down to our top-ten favorites. So in no particular order, we present to you the WIRED Science Desk's completely unscientific picks for best science and science-fiction writing of 2016. Because sometimes you just gotta go with your gut (or rather, your gut microbes).
Patient H.M. by Luke DittrichThere's a certain poetic intrigue to the story of Henry Molaison, the most important neuroscience subject of the 20th-century, as told through the eyes of science writer Luke Dittrich. In the 1950s, it was Dittrich's grandfather, William Scoville, who tried to cure Molaison of his epileptic seizures by removing signifiant portions of his brain. Instead, the lobotomy turned Molaison into a profound amnesiac, living the rest of his life in a series of 30-second increments. Equal parts personal tragedy and scientific breakthrough, Patient H.M. is the story of the most famous brain in the history of neuroscience, and the fight over its fate following the death of its owner in 2008. The book's release sparked some serious controversy this summer, raising questions over Dittrich's treatment of one MIT researcher, adding another dramatic chapter to the now decades-long saga.
I Contain Multitudes by Ed YongWhether you like it or not, the microbiome is just hot right now. But that's not the only reason to read Ed Yong's sweeping history of science's study of the microbial universe. Things get downright personal when he examines what exactly it means to be human when half of your cells aren't human at all. If you can get through the parts of the book that just feel like a long list of bacteria, you'll be rewarded with fascinating and sometimes contradictory revelations about the origins of human life, about health and disease, and even about long-standing notions of selfhood and free will. Like all new ideas, it can be hard to take in at times, but Yong's writing is wonderfully accessible, and he's perfectly suited to acquaint you with your other (bacterial) half.
Weapons of Math Destruction by Cathy O'NeilIn today’s world, if you want to change your fate you’ve got to pray at the altar of the algorithm. From what job you can get to what car you can buy, to the kind of home you can afford to the way the police treat your neighborhood, mathematical models are increasingly the arbiters of your destiny. And, as math guru Cathy O’Neil argues in her newest book, these models are just the latest way America’s institutions perpetuate bias and prejudice to reward the rich and keep the poor, well, poor. It’s a nuanced reminder that big data is only as good as the people wielding it.
Water is for Fighting Over by John FleckIf you just can’t take any more bad news this year, cheer up with John Fleck’s refreshing counter-narrative to long-enduring axiom that water wars will lead to the collapse of civilization in the American West. A veteran water journalist, Fleck chronicles a hopeful future for the region, despite longer, bigger droughts being driven by climate change and a growing population. In 13 concise chapters, largely focusing on the Colorado River, he details how collaboration between states, communities and water rights holders is replacing the winner-takes-all approach of the 20th century, and foretelling a more optimistic outlook on the Western water front.
The Wasp That Brainwashed The Caterpillar by Matt SimonFrom fish that make their homes in the anal cavities of sea cucumbers to spiders that spin webs to mimic bird turds and every evolutionary wormhole in between, the world is filled with some pretty absurd creatures. And WIRED’s very own Matt Simon has captured the strangest of them all in his first book, a veritable parade of eccentric animal profiles that make you laugh so hard you won’t even realize you’re learning a thing or two about niche distribution and adaptive camouflage.
__Too Like the Lightning __by Ada PalmerThis dizzying, self-driving flying freight train of a book will smash your expectations and test all of your daintier constitutions. It occupied WIRED’s Book Club for weeks, as we attempted to dissect the different storylines, a gnarled and twisted family/political tree, and futuristic societal codes in a world where gender is irrelevant. (Careful, spoilers aplenty in there). We found the abrupt ending to be a bit cruel, considering we have to wait until March for the release of the next book in the series, Seven Surrenders. But if you time it right, and place your pre-orders now, you can avoid similar frustrations.
The Obelisk Gate by N.K. JemisinThe second book in a trilogy rarely makes any year-end, best-of lists. Middle books are for stalling, filling in backstories, developing peripheral characters, and all the other less-than-riveting busy work necessary to unleash a successfully climatic final act. But The Obelisk Gate rebukes this formula, jumping straight back into the action of Evil Earth and accomplishing along the way something we would have though impossible—one-upping its predecessor, The Fifth Season (another WIRED Book Club pick). With her rare talents for vivid worldbuilding, seismology geekery, and razor-sharp social commentary, we just can't get enough N. K. Jemisin. If you feel the same way, don't miss her original piece in WIRED's January fiction issue.
__Saga __(comic book series) by Brian K. Vaugn & Fiona Staples
How do you feel about surreal, reality-bending, space opera rom-com epics? Good? Great, then you'll love this series published by Image Comics, now entering its fifth year. Drawing heavily from the tradition ofStar Wars but with a brilliantly original vision, Saga is set in a galaxy in an eternal war with itself. Its heroes are a pair of star-crossed soldiers from two warring races, their story told through the eyes of their against-all-odds love child. We wish the paperback versions contained the letters section, but that's our only complaint with this sweeping, sexy drama of one family fighting to find their place in the universe.
Sleeping Giantsby Sylvain NeuvelA giant, disembodied metal hand buried beneath the Black Hills, and 11-year old girl who dedicates her life to unraveling its myriad mysteries. This is the set-up for first-time novelist Sylvain Neuvel's self-published debut. (It has since been picked up by Sony Pictures and publishing house Del Ray, in that order). Inspired by the comic-book heroines of his childhood, Neuvel's book is packed with badass female protagonists who turn what could be a staid starting point into an illuminating and wholly original scientific thriller. At 320 pages, this standalone story is perfect for the commitment-phobic among you.
Paper Menagerie and Other Storiesby Ken LiuIf you want three reasons to pick up Ken Liu's collection, look no further than the Hugo, the Nebula, and the World Fantasy Award. They are Sci-Fi's top honors, and Liu's core-shattering title story just became the first work of fiction to win them all. The other fourteen stories are carefully curated from the more than 70 he has published over the last dozen years. Each of them cuts straight through to the marrow of your self. Together, they are strands of a carefully constructed web that pull and hint at a massive, life-consuming opus, tamped into a volume that can fit on your bookshelf, if not in the expanses of your mind.
Running a space program ain't easy. Spacefaring missions can take as long to plan and fund as they do to execute, so you'd better bet there's some serious forethought going on. Which sounds like a headache for NASA, but for space fans, it's actually good news. It means—surprises and mishaps aside—an attentive nerd can figure out what to expect from space science in 2017. And while healthy skepticism always leads to better science, we're feeling optimistic about the year's coming missions and discoveries. Here's a breakdown of what we're anticipating most:
For the first time since 1918, a total solar eclipse will be visible across the entire continental US. Norway had one last year and it was pretty dang stunning. But unless they're being used to find new exoplanets, eclipses are more of a novelty than an avenue for new science. Still, a total eclipse is a relatively rare occurrence, and for once, nobody is going to tell you not to look. Even safety-conscious NASA says it's okay to stare at a total eclipse, as long as you wait until the sun is totally shadowed. While the eclipse is still partial, remember: direct sunshine—like bad moonshine—can cause blindness.When: August 21st1Excitement Level: 4/10
NASA's Space Launch System is the most powerful rocket in the world, the kind of rocket NASA needs to get to Mars, and it's just about ready for liftoff—theoretically. The SLS is still deep in its testing stages, and those will continue right up to the rocket's projected 2018 launch date, when it's set to carry the Orion spacecraft on an unmanned mission.
In 2017, the rocket will enter its "Green Run" phase at NASA's Stennis Space Center: a bunch of static booster-firing, resonance-checking test runs. Most of which SLS's subsystems have endured in the past, but this is time there's a 90-degree twist. The green run will be the first time the rocket engines are assembled with the core stage, and they'll be vertical. (You can see what a horizontal booster test looks like here.) We're still a year away from blast off, but at least this year SLS will be pointing in the right direction.When: December 17thExcitement Level: 5/10
After billions of years of space rocks impacting and taking bites out of Earth, NASA wants to get a piece of an asteroid. So last year they launched OSIRIS REx, a spacecraft equipped not only to orbit the carbon-rich asteroid Bennu, but to take some samples back to Earth. It will be a long time until the mission pays off—the spacecraft isn't due to touch back down to Earth until 2023. But in 2017, it will take a crucial intermediary step.
Since its launch in September 2016, the spacecraft has been cruising around the solar system, biding its time. But after a year of chilling in the icy void, OSIRIS REx will fly by Earth and slingshot using its gravity, increasing the craft's orbital speed and tweaking its trajectory to line up with Bennu. The mission's team has the benefit of learning from Rosetta—a masterclass in flying blind near a tiny, hurtling space rock and settling into a wonky orbit—but this calls for some Skywalker-level finesse.When: September 22__Excitement Level: 7/10 __
On deep space missions, navigating pirate-style—by the light of the stars alone—just isn't good enough. And GPS doesn't work once you get out beyond its satellites. So what's a spacefaring navigator to do? In the past, they've had to rely on the Deep Space Network, a system of Earth-bound antenna arrays and an ultra-precise atomic clock. Thing is, space has gotten crowded, and not just with near-earth objects like comets and asteroids: The sheer number of missions is overloading the switchboard.
Enter the Deep Space Atomic Clock, scheduled to launch early next year. The fully autonomous mercury-ion atomic clock will revolutionize space navigation by making it function more like terrestrial navigation. So: improved accuracy, and cutting down on those pesky communications lag times. Call us nerds, but our anticipation of this clock's launch is really making time fly.When: September 2Excitement Level: 8/10
Between GPS-like BeiDou nearing completion, some fancy and powerful new rockets, and a successful 30-day manned mission, China's space program has gone from upstart to obvious rising power in the last year. And it shows no signs of slowing down in 2017. In April, the country will launchthe Tianzhou 1 unmanned cargo craft to dock with their Tiangong-2 space laboratory. This is basically their first resupply mission to their prototype space station. Which, if they're successful, could go a long way toward making a permanent Chinese orbital space station viable—especially important because the ISS is due to be retired in 2024.
And that's not even 2017's big Chinese spaceflight news. At some point in the second half of the year (exact date TBD), China is scheduled to launch its Chang'e 5 robotic sample return mission. If everything goes according to plan, the spacecraft will land on the near side of the moon, collect some samples, and return them to Earth. Have other spacefaring nations done this? Sure. But this is all part of the Chinese space program's plan to show the world its moon boots are just as big as anyone else's. And considering that nobody else can get to the moon right now, it's a non-trivial demonstration of that power.When: April/TBDExcitement Level: 9/10
Nothing stirs a space nerd's soul like new, unexplored horizons, or crash landings. NASA's Cassini mission will deliver on both. The Cassini space probe has been exploring Saturn for 13 years, hanging out with Saturn's moon Enceladus and its geysers. In November 2016, the probe went into the penultimate phase of its life: the ring-grazing orbits, designed to sample the particles that make up Saturn's famous rings. This April, Cassini will descend even closer to the planet's surface, and into a stratum that no craft has ever explored before: the space between Saturn and its rings. From there, the probe will descend until it makes destructive impact with the planet's surface. NASA has promised it will be snapping pictures until touchdown.When: April 22/September 15__ Excitement Level: 10/10__
1UPDATE: This story has been updated to correct the date of the total solar eclipse.2UPDATE 3:45 pm Eastern, 1/4/17: This story has been updated to correct the launch date for the Deep Space Atomic Clock. It is currently slated for September 2017, not a previously planned date in January 2017, and like many NASA launches, it is subject to change again.
Monday is shipping day at MatTek. A truck pulls up to its red brick lab outside of Boston to load box after box, all kept at a cool 39 degrees. The precious, perishable cargo is human skin—thousands of dime-sized pieces in plastic dishes that add up, altogether, to about two whole adult humans’ worth. Every week.
It’s not harvested from people, though. MatTek grows its own human skin, and then sells it to companies that want it—companies that make laundry detergent, makeup, toilet bowl cleaner, anti-aging creams, tanning lotion.
Without lab-grown skin, these companies would be testing products on animals, usually rabbits, shaved to expose patches of naked skin. This practice is straight-up illegal for cosmetics in Europe now, and increasingly ethically dubious everywhere else. With animal testing on the wane, MatTek—along with its chief competitor, Episkin, a subsidiary of L’Oreal—have become go-to sources for synthetic human skin.
Testing on lab-grown medallions isn’t just an ethical choice: It’s practical. “They are a much better simulation of human skin than animals are,” says Carol Treasure, whose company XCellR8 test products for brands like Lush Cosmetics and uses MatTek’s products for some of the work. But truth be told, you wouldn’t recognize MatTek’s skin as human, or even as skin. To the naked eye, the skin—less than fifth of a millimeter thick—looks like thin circles of clear jello.
Mitch Klausner.
So, too, do MatTek’s other tissue models: bits of eye, lung, intestinal, vaginal, and mouth tissue, all grown for testing. The company’s process reduces body parts to their most essential cells—turning surgical waste from biopsies, tummy tucks, and circumcisions into a reliable and standardized product lines, all of which turn into those translucent discs. “You wouldn’t be able to tell the difference just from looking at them,” says president Mitch Klausner.
That uniformity is MatTek’s greatest challenge—and advantage. Human skin is hugely diverse. Take any two people of the same age, sex, and race, and one might be oilier or more sensitive or drier than the other. MatTek’s skin tissue model must react the same way to the same chemicals year after year, even if the originals cells came from two different people and two different parts of the body. It runs a highly tuned skin factory.
To start, MatTek needs a small but steady supply of real human skin, which they could use as seed material to grow large quantities in the lab. Most human cells can only replicate so many times before they die. And thus we all age and we perish.
The company has forged partnerships with local hospitals to get surgical waste from cosmetic surgeries and circumcisions, where patients—or their parents—have agreed to donate excess skin to research. As MatTek has expanded other tissues, it also worked with the National Disease Research Interchange, an organization funded by the National Institutes of Health to disseminate tissues from deceased donors for research.
Klausner was not keen to go into the details of acquisition, citing the anonymity of donors. Indeed, MatTek knows very little about the patients from whom it gets donations. And patients themselves might have a hard time tracing their own donated tissues. When I asked if anyone has ever attempted to find what happened to the skin from their surgery, Klausner dismissed the very idea.
Patients may not even know where to look given that consent forms—part of the standard hospital admissions paperwork—don’t usually specify how tissue will be used. It could go to a patient’s doctor or to a researcher at another university or to a company. “There’s a lot about how research is conducted in the US that the public doesn’t understand,” says Michelle Lewis, a bioethicist at Johns Hopkins Berman Institute of Bioethics.
The process is extremely opaque. But if you’ve had a circumcision or tummy tuck or breast surgery, especially in a Boston-area hospital, your cells could have made it to MatTek, been expanded to cover two football fields worth of skin, and been sent all over the world to labs that test chemicals and skin creams and drugs.
MatTek does keep track of some identifying factors, though. It sorts its skin tissues by age, sex, and race, depending on the intended test. Some are harder to source than others. The company makes some skin containing pigmentation cells—to test products like tanning lotion or lightening creams—and the Asian market is big, especially for skin lightening creams. “Asian skin is a little harder to get,” though, says Klausner: Asian parents are less likely to circumcise their baby sons.
To keep its product mass-market consistent, MatTek's technicians first use enzymes to break down that original piece of donated skin into individual cells. Epidermis actually contains many different types of cells, but the main ones are keratinocytes. So the technicians take the keratinocytes and grow them in a single layer in petri dishes. Then they isolate individual keratinocytes and use them to seed porous inserts in plastic wells.
You could stop here, but MatTek doesn’t just want to grow a mass of undifferentiated skin cells in a dish. It wants to grow skin tissue—with layers of cells that gradually dry and flatten on the surface just as the skin cells on your arm do.
So technicians then follow a detailed, multi-day recipe, with dozens of different measurements that must be correct down to the microliter. Growing human cells in a petri dish is finicky work. Add an ingredient a few hours too late? Forget it, your cells are dead. But done right, those cells will replicate to form a layer 12 cells thick. Air wafts over the top cells, while the bottom layer bathes in a nutrient-rich blood substitute, much like the epidermis on your body. Ten days later, it’s Monday—the company always ships on Monday—and the skin coins are ready to ship.
Before lab-grown skin came along, the way to test whether a chemical would irritate the skin was to use bunnies. Scientist would shave off a patch of fur, smear the chemical on, and check back hours and days later. “Obviously, the more irritating the chemicals, the more gruesome it can be for the animal,” says Michael Bachelor, product manager at MatTek.
With recreated human skin like MatTek’s EpiDerm, it’s a more streamlined process. To test for irritation, you add the chemical to test along with a dye called MTT to a skin circle in a plastic well. MTT turns purple when a cell is alive. Hours or days later, a machine can measure the exact amount of dye in the well and calculate the number of living cells. The more of them are dead, the more irritating the chemical. Of course, this is not a perfect replica of what happens when you spill toilet bowl cleaner on your arm, but the MTT test is a proxy for how easily a toxic chemical can kill cells.
MatTek’s customers also use its skin tissue models to develop anti-aging creams. You don’t go looking for wrinkles in the little clear circles of cells. Instead, scientists can see how anti-aging creams turn on or off genes like those for collagen and elastin, which give skin its youthful bounce. To test anti-aging, companies use MatTek’s full-thickness skin, which includes both epidermis and the next layer of skin, dermis, because cells in the two layers of skin affect each other. These complex but poorly understood phenomena matter when you go to skin on a body.
Which brings us to the limits of current skin tissue models. Even with the full-thickness skin, it’s not exactly like skin. It doesn’t have hair follicles or nerves or oil glands. The protein scaffold on which the cells grow is simplified. MatTek’s tissue models are designed for discrete tests, where machines can look some cells and spit out a single number: This product is this much irritating or that product is that much good at killing skin pigment cells. The models work very well for these tests, but you can’t, say, graft them onto the body and expect them to start growing like skin. They do, after, look like thin clear disks of jello.
To hear a story about sexual harassment in the sciences, just ask a person who has been a woman in the sciences. Chances are, if it hasn’t happened to her, it’s happened to someone she knows.
This year yielded a lot of front-page stories about celebrity professors breaking bad, but it is also the year scientific societies and policy-influencers decided to try to do something about it. And if the momentum holds, 2017 could be the year they do more than try, as they transform new initiatives, brainstorming sessions, reports, and promises into action and cultural change.
The revolution began in October 2015, when Azeen Ghorayshi at BuzzFeed wrote about a Title IX investigation at the University of California, Berkeley. The school had found that lauded astronomer Geoff Marcy had violated harassment policies between 2001 and 2010—and then, having that information, applied no substantial consequences. When Marcy’s story came out in article form, much of the scientific community was outraged that this “father of exoplanets” had taken advantage of his status and then kept that status.
But others were surprised. That guy? Really? Was his behavior really that bad? And as scientific organizations like the American Astronomical Society pulled together new policies, codes of conduct, and workshops, some also expressed doubt this whole harassment thing was really such a big problem. As the year progressed, that chatter quieted.
What Happens When a Harassment Whistleblower Goes on the Science Job Market
Rep Jackie Speier on Why She’s Taking on Sexual Harassment in Science
A New Twist in the Fight Against Sexism in Science
“A lot of people were kind of oblivious to things going on right underneath our noses,” says Eric Davidson, incoming president of the American Geophysical Union. But since his organization has begun tackling the issue, it’s shown him exactly how real the problem is, although he knows it is not a new issue. “I look around and see my female colleagues exchanging glances and saying this has been going on for a long time, and it’s about time we start talking about it,” he says.
The numbers agree: A 2014 study found that 71 percent of female scientists had been sexually harassed while out in the field, and 26 percent had been sexually assaulted. In a 2015 survey of astronomers, 32 percent of respondents reported experience of verbal harassment in their current job specifically because of their gender. Around 9 percent reported physical harassment.1
“There is the sense that if you want to be a woman in science, that’s the extra price you get to pay,” says Janet Stemwedel, the head of San Jose State’s philosophy department, who writes about ethics in science.
And the academic environment doesn’t merely permit such transgressions: It’s their agar plate. “A cynical take is that the forces that allowed Marcy to harass women for so many years—his prestige; his ability to bring in funding; the employment protections he enjoyed as a tenured professor; the outdated, onerous, and secretive nature of sexual harassment investigations—are not anomalies of an outlying department, but in many cases defining traits of academia,” Ghorayshi wrote, in one of several related articles that came after the initial Marcy piece.
Ghorayshi and other reporters sent big sexual harassment cases into your newsfeed about once a month. You could make an old-school calendar out of them (although you probably prefer puppies in baskets and Yosemite as the seasons change).
Here's a timeline of the year's biggest events:
January: California Representative Jackie Speier revealed Title IX documents detailing how University of Wyoming astronomer Timothy Slater had given a student a vibrator, taken people to strip clubs on lunch break, and shared thoughts on women’s bodies. Speier used the case, based on events at the University of Arizona in 2004, to show how offending professors can move between universities while keeping their records secret. Also this month, Caltech suspended physicist Christian Ott for firing a student because he was sexually attracted to her. A BuzzFeed investigation revealed that he had confessed those feelings to another student.
February: University of Chicago professor Jason Lieb resigned (after the school suggested he be fired) for coming on to graduate students during a retreat and having sex with a student who was “incapacitated due to alcohol and therefore could not consent.” Also in February, Science published an article about American Museum of Natural History anthropologist Brian Richmond, whose direct report accused him of assault during a scientific conference in Italy, triggering "a cascade of other allegations against Richmond."
March: Hope Jahren, author of the book Lab Girl, published a New York Times op-ed discussing the pervasiveness of widespread sexual harassment—especially the kind in which a supervisor just can’t keep his star-crossed feelings for a student out of the body of an email. “Since I started writing about women and science, my female colleagues have been moved to share their stories with me; my inbox is an inadvertent clearinghouse for unsolicited love notes,” she writes.
April: Physicist Sarah Gossan—one of Christian Ott’s victims—tweeted, in a set of 33 posts, that she was leaving the research group she co-chaired for the gravitational-wave observatory LIGO and planned to leave the supernova research field altogether, due to Ott’s alleged retaliation against her and fallout from the case.
May: Thomas Pogge is not a scientist, but he is an academic ethicist. And according to a BuzzFeed article, he was accused of sexual harassment in the 1990s at Columbia University; again in 2010 at Yale University, where the complainant also claimed he retaliated against her; and then again-again in 2014 when a European student said he presented job opportunities as rewards for a sexual relationship. A Yale panel voted that there was “insufficient evidence to charge him with sexual harassment.”
June: Ebola and flu researcher Michael Katze, of the University of Washington, hired an administrator with the “implicit condition,” according to a BuzzFeed investigation, “that she submit to his sexual demands.” Katze also tasked another employee with cleaning his residence, buying marijuana, and emailing escorts (yes, that kind), on top of a background of sex-jokes and -comments and two attempts at physical contact. Katze remains employed at the university.
July: In Slate, astronomer Katherine Alatalo spoke of meeting with her department chair to talk about cutting off her working relationship with her supervisor, who made inappropriate comments about her appearance, asked about her sexual activity, and "[mixed] personal attacks with professional comments." The chair's follow-up letter informed her that she was “ceding a remarkable opportunity to work with one of the premier experts [in these fields].”
August: The University of Kentucky’s newspaper reported that entomologist James Harwood, who had resigned without giving a reason, had violated harassment and assault policies (the latter of which are actually called “criminal laws”) with two students. After the article came out, the university announced plans to sue the publication for bringing the case documents to light.
September: Neil deGrasse Tyson said in an interview with BuzzFeed science editor Virginia Hughes that science, and astrophysics specifically, does not have a special problem with harassment. “The issue is not sexual harassment in science,” he said. “The issue is sexual harassment in the workplace, which includes scientific workplaces. So I don’t see that there’s some special kind of solution to that problem needed to be invoked in a scientific community.”
October: The head of the University of Bath astrophysics department, Carole Mundell, faced claims of libel and slander. She had stated that a former supervisor at Liverpool John Moores University, Mike Bode, wrote glowing letters of recommendation for alleged harasser Chris Simpson, ignoring the complaints filed about his behavior. The case was thrown out of court.
November: On a radio show and subsequent article, astronomers at the Australian scientific organization CSIRO detailed instances of harassment and bullying, including three formal allegations of sexual misconduct, two of which were upheld. Chief among them was that from Ilana Feain. The organization barred Feain, who has left the field, from disclosing the findings of the investigation.
December: At the American Geophysical Union’s annual conference, which attracts tens of thousands of scientists, the organization hosted nine sessions related to harassment, ethics, and workplace climate. Earlier in the year, the society had brought together 60 leaders in academia, government, and professional organizations for a workshop called “Sexual Harassment in the Sciences: A Call to Respond.”
With so many documented cases of sexual harassment (as well the women nodding their heads and saying “Duh”), this year’s journalistic investigations showed not just that the field has a problem but that patterns exist. “Once you’ve read five or 10 or 20 cases, you’re like, ‘OK, I don’t really expect the next harasser we find out about is going to be radically different from the ones we’ve heard about so far,’” says Stemwedel. “‘I see the size and shape of the problem.’”
That that quantification and qualification in hand, big-S Science can now enumerate and address its issues.
First, science operates under what Stemwedel calls a “medieval apprenticeship” model—in which students are immersed in their work and are completely dependent on advisers for funding, ability to finish graduate school, and future jobs. Second, university investigations usually remain in locked boxes, not leaking into the larger world unless there is an actual leak or a FOIA request. And third, universities depend on superstar professors for the grant money they rake in.
California congressional representative Jackie Speier has recently introduced legislation to take aim at that last issue. Her proposal attempts to bring transparency to Title IX investigations and force schools to report all violations to the funding agencies—like NASA, the National Science Foundation, and the National Institutes of Health—that give money to offending professors. For their part, those three money-senders have said that any institutions and/or individuals they support must comply with civil rights laws. “There’s still question about how much those words will be met with action,” says Stemwedel.
Legislation, of sorts, has also come from scientific societies like the American Geophysical Union, the American Astronomical Society, the Biophysical Society, and the American Physical Society. They have all created new, more explicit codes of conduct for members and meetings, detailing what harassment is and what happens if you harass. And many society leaders are working together to share resources, procedures, policies, and safe ways for members to report infractions. “These groups do carry a lot of weight in science,” says Ghorayshi. “And lot of the groups did pretty immediately stand up to the challenge of trying to address this problem. That’s also due to the fact that there’s a younger crop of scientists—largely women and a lot of allies—who are trying to draw attention to these issues.”
All of that philosophy, paperwork, and pontification only do so much good. They don’t immediately change behavior. But their aspirational nature does have value. “It’s sending a message to younger scientists that are entering the field that that is the culture to be valued,” says Ghorayshi.
Cultural change like that can take a while. “But just because it takes a while to change doesn’t mean you wait around for it,” says Davidson.
1Update, 12/30/2016, 3:08 pm PT: This article has been corrected to indicate only the percentage of 2015 survey respondents who reported harassment because of their gender. A previous version incorrectly added that number together with the percentages of respondents who had experienced harassment for other characteristics.
Amid the crises and chaos of 2016, life, as they say, went on. And so too did the study of life: Biologists had their work cut out for them handling the Zika virus, a quickly-blossoming science and public health concern that turned out to be our biggest biology story of the year.
But the field forged ahead in other areas. Biologists continued to hammer away at old enemies like HIV, cholera, and antibiotic-resistant bacteria. And their tools keep getting better: Neuroscientists have used slicker cameras and techniques to map out the brain more precisely than ever, and creative researchers are now applying gene editing to tackle everything from HIV to animals on the brink of extinction. Here are some of 2016's biggest moments in biology:
As Zika spread in the US—via travelers and (later) Florida mosquitoes—scientists raced to study the disease in earnest, eventually confirming that Zika causes microcephaly. Meanwhile, public health officials, ob-gyns, and mosquito control experts managed the virus on the ground with Zika kits, counseling for pregnant women, and pesticide spraying. And scientists took to wackier, less-traditional methods as well: hacking disease transmission by modifying the mosquitoes’ genes, or infecting thousands of males with a sterilizing bacteria called Wolbachia. All the while, Congress tussled over whether to put money towards Zika research and prevention, finally approving $1.1 billion towards the fight in September.
Now that scientists have gotten busy applying the much-fêted gene-editing technique Crispr to their work, they’re finally publishing the fruits of their labors. Researchers have edited white blood cells and injected them into patients with lung cancer, edited bone marrow cells to test sickle cell anemia therapies, and edited human embryos (twice!). And forget Jennifer Doudna. Crispr’s real big break came this year: It’s the central plot device for a TV drama, C.R.I.S.P.R, currently being developed by Jennifer Lopez.
As imaging techniques improve, neuroscientists are collecting terabytes of brain data and sifting through it to draw an ever-clearer picture of how it all hangs together. They’ve cut mouse brains into vanishingly thin slices to piece together their neural networks and monitored the brain activity of mice as they watched Touch of Evil or did nothing at all—all to understand human brains better. And to make sure their imaging data was legit, some researchers took a good hard look at the accuracy of techniques like fMRI, which measures blood flow in the brain as a proxy for neural activity.
Scientists have been grappling with creating an HIV vaccine for years. Sure, people with HIV can use antiretroviral drugs like Truvada to keep the virus at bay. But the drugs aren’t perfect, especially if you miss a dose—not to mention that many people with the disease don’t have access to those drugs. But the news is hopeful this year: Researchers around the world have spun up several clinical trials to test possible vaccines and antibodies to fight the virus. And a team reported in Nature that they had successfully created a vaccine to treat the version of HIV in monkeys.
Easy DNA Editing Will Remake the World. Buckle Up.
What Did a Year in Space Do to Scott Kelly?
How Doctors in Florida Are Protecting Pregnant Women From Zika
2016 saw outbreaks of diseases that should be over but aren’t. In Angola, a vaccine shortage has allowed new cases of yellow fever to develop. After two years free of polio cases, Nigeria relapsed, possibly because Boko Haram has made it difficult for organizations to gather accurate health data. And cholera in Haiti had been raging even before Hurricane Matthew exacerbated the country’s sanitation problems in October. Even as biologists make progress on new treatments and vaccines, these diseases remind us that in the messy real world, cures only work if you use them right.
As biologists go, taxonomists are the feistiest of the bunch, always squabbling about giraffe speciation or how to properly define taxonomy. And in 2016, those fights spilled over to Twitter. (Amazing! Some people on the internet were getting righteously indignant about something that wasn’t the election!) But biologists are also now applying new research into the genetics of wild animals to help save them—like putting gene drives in invasive Galapagos rats so they die out, or selectively breeding Tasmanian devils with genes resistant to the face cancer that almost wiped them out.
After spending 340 days in space aboard the ISS, astronaut Scott Kelly alighted back on Earth in March. Now, NASA is inspecting his body and comparing it to his earthbound twin brother Mark’s to see how the rigors of long-term space livin’ affects humans. Did the lack of gravity weaken Space Kelly’s bones? How did it affect the fluids in his body? And what about all that radiation? The sooner scientists find out, the closer humanity gets to sweet, habitable Mars condos.
Scientists in California are breeding and releasing mosquitos into Zika hotspots. While it may seem like they're making matters worse, they are actually releasing a kind of biological trojan horse.
2016 is coming to a close so know it's time to look back on the volcanic action of the year. Take a moment and vote for your top 3 volcanoes that you think deserve the honor of Volcanic Event of the Year — the coveted Pliny Award. If you need some refreshers, check of the Atlantic's review of some cool eruptions, browse through the Global Volcanism Program's Weekly Volcanic Activity Reports or flip back through the posts here on Eruptions.
The polls will be open until January 5 at 11:59 PM Eastern Standard Time, so cast your ballot and I'll count down the top volcanoes right after 2017 gets started.
Go here to vote in the poll!
(I'll try to embed the poll when WP decides to play well with Polldaddy.)
Here are the past winners of the prestigious Pliny:2009: Sarychev Peak2010: Eyjafjallajökull2011: Puyehue-Cordón Caulle2012: Tolbachik2013: Etna2014: Holuhraun-Barðarbunga2015: Colima, Mexico
If you notice any volcano that made news in 2016 missing, leave a comment below!
Maybe 2016 wasn't your year. Buck up, at least your $60 million dollar rocket bearing a $200 million dollar payload didn't explode on the launchpad. Or, perhaps your year was great. Again, some perspective: Did you land four rockets on ocean barges after inserting satellites into orbit around the Earth?
No? Neither? Then lay off the superlatives, because SpaceX probably had both a better, and worse, 2016 than you.
Every instance of the company's fortune or adversity comes because Elon Musk wants space to be a mass market commodity. More explicitly, he wants people to be able to buy tickets to the Red Planet for about $200,000. Now, the mortgage cost of a modest suburban home might not strike everyone as being "mass market." But that's relative to the current cost—millions and millions and millions—of sending a person into orbit. Hence, SpaceX's biggest attention-grabbing endeavor: landing its first-stage boosters on barges.
Which it accomplished four times this year! And that's in addition to the other two rockets it landed on dry land! At the moment, each of those remarkably satisfying landings mostly gave the company a gushing of free publicity. (Plus a bunch of invaluable data for sticking future landings.) But eventually, SpaceX will start selling launches on used rockets—at a 10 percent discount to customers—saving the tens of millions of dollars per launch it would have spent building a new booster from scratch.
Musk's Martian plan—laid out in detail in late September—doesn't just require reusable rockets. It needs big reusable rockets, ones capable of lifting spaceships (or parts of spaceships) capable of carrying 100 to 200 passengers into orbit. SpaceX was supposed to test launch its first such rocket, called a Falcon Heavy, this month. But it punted on that goal, same as it punted on its first relaunch of a Falcon 9, which was also supposed to happen this year.
Two factors caused those delays. First, space is really, really hard, and SpaceX often gives itself really, really ambitious deadlines—the company has repeatedly pushed back its first launch date for the highly anticipated Falcon Heavy. Second, the company has paused its launch calendar until it finishes its investigation into that launchpad explosion, which happened on September 1 at Cape Canaveral.
So far, the investigation seems to have indicted the helium transfer system inside the second stage oxygen tank. This is the fuel system that the rocket would have used to send the Amos-6 telecommunications system into geostationary orbit, 22,000 miles above Earth. Earlier this month, SpaceX signaled that the investigation was probably completed when it announced that it would resume launching sometime in January.
But let's go back to September. Elon Musk made his Mars announcement at the end of the month, during the 67th annual International Astronautical Congress in Guadalajara, Mexico—just three weeks after the Falcon 9 exploded at Cape Canaveral. Audacious? Not quite. Despite the setback, the company was still innovating. Two days before Musk took the stage to announce his Martian colonization plan, the company test-fired its next-generation Raptor engine. With more than three times the thrust of Merlin engines that currently power its Falcon 9 rockets, the Raptor's success is much more important to the company's ambitions than a single rocket's failure.
The explosion was a big downer—on December 8th, British telecom Inmarsat announced it would use Arianespace instead of SpaceX for future launches—but not a company-killing mishap. The company's launch calendar still has more than 70 planned missions. These include an $83 million contract signed in April with the US Air Force to launch the next generation of GPS satellites.
SpaceX Says It’s Ready for Liftoff Again. The FAA Begs to Differ
The SpaceX Explosion: What You Need to Know
Elon Musk Announces His Plan to Colonize Mars and Save Humanity
Politics could be the company's biggest worry. Although the SpaceX has added a lot of private-industry launches in the past few years, a lot of its business still comes from federal contracts. One overarching theme among Donald Trump's proposed cabinet and staff picks is cutting government spending. However, Trump recently named Musk (and several other tech magnates) as one of his strategic advisors. Another positive sign is Trump's chummy relationship with Peter Thiel. Thiel and Musk have been on good terms since they co-founded PayPal back in the 90s.
Worst case, the company's federal contracts dry up, leaving it pining for the days when rocket explosions were its biggest concern. And it will look back to 2016, the year it landed rockets on barges, dreamed about Mars, and had its best year ever.
This year, we kicked things off by telling you how deadly and difficult space is to explore. It can kill you with radiation, giant flying space rocks, and regular old time. And those are just a few of its weapons.
But while space is a pretty dangerous place, it's also incredibly inspirational. If you take science fiction as your model—which we often do—people are at their best when faced with a seemingly insurmountable challenge. So 2016 abounded with launches, newly discovered dwarf planets, steps toward Mars, and victorious ends to long-haul missions. And it's our pleasure show you the highlight reel.
What didn't scientists find in space this year? Well, besides aliens: Even though people were pretty excited about a potentially intelligent signal coming from the Sun-like star HD164595, astronomers say it was likely nothing. But while aliens are still proving elusive, NASA scientists located plenty of new exoplanets where they might be hiding.
Introducing NASA’s OSIRIS-REx Asteroid Mission: the Unofficial Comic Strip
Juno’s Jupiter Mission Faces Its Most Critical Moment
Y’all Need to Chill About Proxima Centauri b
NASA's Kepler space telescope survived some major malfunctions and discovered a whopping 1, 284 new exoplanets this year, some of which are in the Goldilocks zone, where the conditions could be just right for life. Most notably, they caught sight of a little world called Proxima Centauri b, which is not only Earth-like, but also kinda nearby: about 25 trillion miles away. (We know. It isn't *that *close. But this is an exoplanet, not a post office.) In the even nearer Kuiper belt, space geeks found so many dwarf planets and dwarf planet moons that people got bored with their successes.
Closer, inside our own inner solar system, scientists expanded their definition of the Goldilocks zone. Usually liquid water is only possible at a certain (short) distance from a planet's star. But following a flood of data from the New Horizons spacecraft, scientists think Pluto's fractured icy surface might be evidence of a water ocean freezing outward, the water kept liquid towards the dwarf planet's core by enormous pressure. And inside the traditional Goldilocks zone, on Jupiter's famously watery moon, Europa, scientists found more evidence of huge water plumes bursting up through the moon's icy crust. Which would make sampling the ocean for signs of life, or pre-biotic materials like tholins, a whole lot easier.
2016 was also a year of long-awaited payoffs. After a few false starts, the International Space Station got a prototype inflatable extension, a test of tech that will one day let give ISS astronauts a little more leg room. And after a five year journey, NASA's Juno mission arrived at Jupiter this 4th of July. After a nail-biter of an engine burn, it inserted into orbit to study everything from the stormy gas giant's powerful auroras to the secrets of its core. Juno is set to loop around Jupiter 37 times, and its first orbit has already returned stunning images of its never-before-seen poles.
The European Space Agency's major event had a more somber tone. After a 12-year journey to the distant comet 67P/Churyumov-Gerasimenko, ESA's Rosetta spacecraft went into a scheduled destructive orbit and bashed itself into the comet's surface. The mission's end came after two years of data collection, and just a few weeks after Rosetta's cameras spotted the downed Philae lander, thought lost after a disastrously bumpy touchdown. ESA got weepy about it. (Less than a month later, they got weepier: ESA's ExoMars lander, Schiaparelli, crashed into the surface of the Red Planet and was destroyed on impact.)
The Rosetta mission's success bodes well for future missions to small bodies in space, like asteroid mining ventures. This fall NASA launched OSIRIS-REx, its asteroid-sampling spacecraft, and the mission should reach the asteroid Bennu in 2019. When spacecrafts can reliably orbit and land on small bodies, scientists can turn asteroids and comets into mobile refueling stations and radiation shield factories, simplifying long-haul manned missions. Which makes the trek to Mars—something scientists are already testing robots, rockets, building materials, and people for—look a lot more possible.
Still, human space exploration isn't moving only from strength to strength. While the Chinese space program ascends, scoring its first month-long manned mission and nearing completion on its version of GPS, BeiDou, Russia's declines. Even Roscosmos' long-reliable Soyuz rockets are failing, probably because of system-wide corruption, and devastating budget cuts. Post-Brexit, British space scientists are worrying about their spacefaring funds, too.
NASA should (and probably does) share those concerns. 2016 was a phenomenal year for space science, but mostly because of missions that were funded, developed, and launched five, ten, or more years ago—when NASA's budget was closer to 1 percent of the nation's total. In the last few years, it's occasionally fallen below just half of a percent. While it's too early to tell how space will fare under the new administration, it's safe to say that unless NASA gets a larger slice of the budget, space fans may not see a year like 2016 for a good long while.
On April 8, SpaceX will launch an inflatable, inhabitable bouncy castle to the International Space Station and it may be the start of the first hotel chain in Space.
No nation can fight climate change alone. And few have any reason to do so anyway—overhauling your entire economy is pricey work. So, it was pretty remarkable when nearly 200 countries came together in Paris at the end of 2015 and agreed to take action against catastrophic warming. And for most of 2016, it looked like the momentum from that decision might, just maybe, be enough to save the world.
Then, November: The United States elected Donald Trump as president. The President-elect, who had called climate change a hoax prior to his victory, has since signaled that he will attempt to undo the promises and progress his predecessor made on climate. At this point, it's impossible to predict how far Trump will go. And, if he does succeed in killing the US's commitment to global climate treaties, how other countries will react. Rather than speculate, it's better to understand what is at stake.
The bottom line of that late 2015 climate treaty, called the Paris Agreement, is a collective promise to keep average global temperatures from rising 2˚C above pre-Industrial levels—and have them stay as close to 1.5˚C above pre-Industrial levels as possible. The fact that it took more than 20 years of negotiations for the signatories to agree on this is testament to the difficulty of climate diplomacy.
A fair amount of that diplomatic energy went toward getting the US—history's largest emitter—to sign on. The US needed assurances that other up and coming economies, like China (the current highest emitter) would shoulder some of the economic burden of cutting its emissions.
America’s Brief Role as a Climate Leader Is Probably Over
The House Science Committee Thinks the Paris Climate Agreement Stinks
Trump’s Chief Strategist Steve Bannon Ran a Massive Climate Experiment
Last year, the unbelievable happened. China promised cap and trade—an economic system where the country sets a limit on total greenhouse gas emissions, and power plants buy permits to emit some percentage of that upper limit. In the US, the Obama administration unveiled the Clean Power Plan, an EPA rule that essentially forces coal power plants to clean up, or close down. That regulation was built atop on a Supreme Court ruling made several years earlier that allowed the EPA to regulate carbon dioxide using the Clean Air Act.
But the Clean Air Act, it turns out, has some weird wording, wording that opponents of the Clean Power Plan—24 states and a half dozen industry groups—used to challenge the rule's legality in the Washington DC's 9th Circuit Court of Appeals.
The litigation between the EPA and its opponents was set to begin in summer of 2016, but the rule would have begun going into effect before a decision either way. But, at the behest of the rule's opponents, the Supreme Court stated that the Clean Power Plan would not go into effect until the lower court had made its decision.
To some observers (including me), this signaled that the Clean Power Plan was probably doomed. Whatever the lower court decided would be appealed and taken to the Supreme Court anyway—and the high court was seemingly signaling its intent to neuter the rule. Without the Clean Power Plan, the US would be stuck looking like it wouldn't be able to hold up its emissions-cutting commitments to the Paris Agreement. And if the US won't keep its commitments, why should anyone else in the world?
Then Antonin Scalia died. Whew? Not quite. Because then, congress blocked Obama's opportunity to fill that seat. But still, the Clean Power Plan's fate suddenly seemed a bit less bleak, because hey, Hillary was a shoo-in for president, right?.
And that's where things were, until November 8. Donald Trump has called the Paris Agreement a bad deal, and specifically promised to nix the US's commitment to it. Trump can do this several ways. In the least extreme version of events, the US would still be part of the agreement, at least in name, until 2020. It wouldn't actually have to follow through with any of the commitments it made, but would still technically be a party to the ongoing negotiations (oh, there are many) for the next four years. Most extreme—catastrophic, even—Trump could withdraw from the 25-year old UN Framework Convention on Climate Change, which is the basis of all international climate negotiations.
That's not even getting into domestic policy. Trump has offered administration jobs to a who's who of career climate action antagonists: Oklahoma attorney general Scott Pruitt, his choice for EPA administrator, is currently suing the agency over the Clean Power Plan; Rick Perry, his pick for Secretary of Energy, campaigned for president in 2012 on a promise to dismantle the Department of Energy altogether; and, Rex Tillerson, Trump's pick for Secretary of State, is the CEO of ExxonMobil, a company that spent decades funneling money to climate denier groups, despite evidence that the company's own scientists had done research proving climate change is real.
Trump's transition team has also circulated memos through the Energy and State Departments to drum up information on staffers who worked on the Obama administration's international climate efforts.
Of course, all those people—along with the rest of Trump's dream team—await confirmation, after the President-elect's inauguration. In the meantime, President Obama is pushing last minute climate regulations, like this week's ban on offshore Arctic drilling, that he promises won't be easy to undo. But, with a Republican-majority Senate and House of Representatives, and an empty Supreme Court seat, Trump and his regime probably won't be stymied for long in their quest to make America great, the climate be damned.
Randy is 62 years old and stands tall at six foot one. He grew up on a farm in Glasford, Illinois, in the 1950s. Randy was raised with the strong discipline of a farming family. From the time he was five, he would get out of bed at dawn, and before breakfast he’d put on his boots and jeans to milk cows, lift hay, and clean the chicken coops. Day in and out, no matter the weather or how he felt, Randy did his physically demanding chores. Only when his work was complete would he come into the kitchen for breakfast.
Tending to the chickens was hard work—it involved getting into the pen, clearing birds out of their dirty cages, and shooing them into a holding enclosure. This process was always a little scary because the animals could be quite aggressive after being cooped up all night. On one of these occasions, when Randy was 11, a particularly large and perturbed rooster swung its claw and gave him a good spurring on his leg. Randy felt the piercing of his skin and squealed in pain. He said it felt like being gored by a thick fishhook. The rooster left a long gash, and blood streamed down Randy’s leg to his ankle. He ran back to the house to clean the wound, as chickens are filthy after a night in their cages.
Excerpted from The Secret Life of Fat: The Science Behind the Body's Least Understood Organ and What It Means for You by Sylvia Tara.
Some days later, Randy noticed a change in his appetite. He was constantly hungry. He felt drawn to food and thought about it all the time. He started eating in between meals and overeating when he finally sat down to dinner. Randy had always been a skinny kid, but in the course of the next year, he gained about 10 pounds. His parents thought it might be puberty, though it seemed a little early. His pudginess was also unusual given that everyone else in the family was thin. Randy was no stranger to discipline. He forced himself to eat less, switched to lower-calorie foods and exercised more. But by the time he was a teenager, he was bouncing between 30 and 40 pounds overweight. He says, “I gained all of this weight even though these were some of my most active years on the farm.”
Randy’s family supported his efforts to control his weight. They made lower-calorie foods, gave him time to exercise, and didn’t pressure him to eat things he didn’t want. However, he continued to struggle with his weight through college. Randy kept thinking back to the moment everything changed. He had been the skinniest kid among his friends. And then he got cut by that chicken.
In Mumbai, India, Nikhil Dhurandhar followed his father Vinod’s footsteps in treating obesity. But Nikhil ran into the same obstacle that had bedeviled obesity doctors everywhere. “The problem was that I was not able to produce something for patients that could have meaningful weight loss that was sustainable for a long time,” he says. “Patients kept coming back.”
Fate intervened in Dhurandhar’s life one day was when he was meeting his father and a family friend, S. M. Ajinkya, a veterinary pathologist, for tea. Ajinkya described an epidemic then blazing through the Indian poultry industry killing thousands of chickens. He had identified the virus and named it using, in part, his own initials—SMAM-1. Upon necropsy, Ajinkya explained, the chickens were found to have shrunken thymuses, enlarged kidneys and livers, and fat deposited in the abdomen. Dhurandhar thought this was unusual because typically viruses cause weight loss, not gain. Ajinkya was about to go on, but Dhurandhar stopped him: “You just said something that doesn’t sound right to me. You said that the chickens had a lot of fat in their abdomen. Is it possible that the virus was making them fat?”
Ajinkya answered honestly, “I don’t know,” and urged Dhurandhar to study the question. That fateful conversation set Dhurandhar on a path to investigate as part of his PhD project whether a virus could cause fat.
Dhurandhar pushed ahead and arranged an experiment using 20 healthy chickens. He infected half of them with SMAM-1 and left the other half uninfected. During the experiment, both groups of chickens consumed the same amount of food. By the end of the experiment, only the chickens infected with the SMAM-1 virus had become fat. However, even though the infected chickens were fatter, they had lower cholesterol and triglyceride levels in their blood than the uninfected birds. “It was quite paradoxical,” Dhurandhar remembers, “because if you have a fatter chicken, you would expect them to have greater cholesterol and circulating triglycerides, but instead those levels went in the wrong direction.”
To confirm the results, he set up a repeat experiment, this time using 100 chickens. Again, only the chickens with the SMAM-1 virus in their blood became fat. Dhurandhar was intrigued. A virus, it seemed, was causing obesity. Dhurandhar thought of a way to test this. He arranged three groups of chickens in separate cages: one group that was not infected, a second group that was infected with the virus, and a third group that caged infected and uninfected chickens together. Within three weeks, the uninfected chickens that shared a cage with infected ones had caught the virus and gained a significant amount of body fat compared to the isolated uninfected birds.
Fat, it seemed, could indeed be contagious.
Now, Dhurandhar is a man of science. He is rational and calm. But even he had to admit that the idea was startling. Does this mean that sneezing on somebody can transmit obesity? This now seemed possible in animals, but what about humans? Injecting the virus into people would be unethical, but Dhurandhar did have a way to test patients to see if they had contracted the virus in the past.
Dhurandhar says, “At that time I had my obesity clinic, and I was doing blood tests for patients for their treatment. I thought I might just as well take a little bit of blood and test for antibodies to SMAM-1. Antibodies would indicate whether the patient was infected in the past with SMAM-1. The conventional wisdom is that an adenovirus for chickens does not infect humans, but I decided to check anyway. It turned out that 20 percent of the people we tested were positive for antibodies for SMAM-1. And those 20 percent were heavier, had greater body mass index and lower cholesterol and lower triglycerides compared to the antibody-negative individuals, just as the chickens had.” Dhurandhar observed that people who had been infected with SMAM-1 were on average 33 pounds heavier than those who weren’t infected.
While Nikhil Dhurandhar was in India pursuing his curiosity about fat, Randy was looking for solutions of his own. After a brief stint as a teacher he moved back to the family land in 1977 because he loved farming.
Randy married and had four children. At family dinners and holiday gatherings, he ate alongside everyone else, but tried eating less than the others. Still, his weight ballooned; by his late 30s he had topped 300 pounds. He remembers feeling hungry all the time, though even when he abstained it didn’t help him lose weight. “I could have several good weeks of eating stringently, much less than others around me, but if I went off my diet for just one meal—boom, the weight would come back.”
The effort to control his eating, even when it was successful, made Randy miserable: “I can’t tell you what it is like to be hungry all the time. It is an ongoing stress. Try it. Most people who give advice don’t have to feel it.”
In the fall of 1989, Randy applied for a commercial driver’s license. The application required a medical exam. After his urine test, the nurse asked Randy if he felt all right. “Normal for the day,” he replied. But the nurse told Randy he would have to give a blood sample because she thought the lab had spilled glucose solution into his urine sample. The blood work showed that Randy’s glucose level was near 500 mg/dL (a normal reading is 100). The lab hadn’t made a mistake with the urine sample after all; Randy’s numbers were just off the charts. Alarmed, the nurse notified Randy’s doctor, who then tested him for fasting blood sugar levels. The results showed that Randy had insulin resistance and severe diabetes.
At 40 years old and 350 pounds, Randy was in trouble. If he didn’t fix this problem soon, he would start to develop serious complications of diabetes, including cardiovascular disease and nerve damage.
Having tried and failed multiple diets, Randy and his doctor decided the best hope was a hospital program for severe diabetics. The staff tested Randy’s blood frequently to determine the optimal dosage and timing of insulin injections to regulate his blood sugar. Randy learned about the Diabetic Exchange diet, which allots patients a specific number of servings of meat, carbohydrates, vegetables, and fat. He cut out all refined carbohydrates, including bread. He says, “I haven’t had a slice of bread or piece of pizza in years.”
But would even this program be enough? Randy had always had a difficult time controlling his weight, though not for lack of trying. He had been fighting fat since his childhood by controlling portions, exercising, and avoiding social eating. But his discipline was no match for his own fat. Randy had to get his weight under control permanently. The hospital environment was helpful. However, despite strictly adhering to the diet, he only dropped a few pounds.
After taking on a postdoctoral fellowship at the University of Wisconsin, Madison under Dr. Richard Atkinson, Dhurandhar was excited to finally be at liberty to pursue what he loved. He had an intense curiosity about viruses and was eager to get started finding answers. However, when he tried to get samples of the SMAM-1 virus that he had worked with in India, the U.S. Department of Agriculture refused to grant him an import license. He was deeply disappointed.
Unable to get SMAM-1, Dhurandhar approached a company that sells viruses for research. Their catalog listed some fifty human adenoviruses. He says, “I was going to order the human adenovirus, but there was no the adenovirus—there were 50 different human adenoviruses! So I was stuck again. I wondered how do I go about this? Should we start number one, number two, number three, number 50, 49, 48? So [with] a little bit of guesswork and mostly luck, we decided to work with number 36. We liked number 36 because it was antigenically unique—meaning it did not cross react with other viruses in the group, and antibodies to other viruses would not neutralize it.”
That was a serendipitous choice. It turned out that Ad-36 had similar qualities to SMAM-1 in chickens. Atkinson thought Ad-36 might very well be a mutated form of SMAM-1. When Dhurandhar infected chickens with Ad-36, their fat increased and their cholesterol and triglycerides decreased, just as had happened with SMAM-1. Dhurandhar wanted to make sure he was not getting a false positive, so he injected another group of chickens with a virus called CELO to ensure that other viruses were not also producing fat in chickens. Additionally, he maintained a group of chickens who had not been injected with anything. When he compared the three groups, only the Ad-36 group became fatter. Dhurandhar then tried the experiments in mice and marmosets. In every case, Ad-36 made animals fatter. Marmosets gained about three times as much weight as the uninfected animals, their body fat increasing by almost 60 percent!
Now came the big question: would Ad-36 have any effect on humans? Dhurandhar and Atkinson tested over 500 human subjects to see if they had antibodies to the Ad-36 virus, indicating they had been infected with it at some point in their lives. His team found that 30 percent of subjects who were obese tested positive for Ad-36, but only 11 percent of nonobese individuals did—a 3 to 1 ratio. In addition, nonobese individuals who tested positive for Ad-36 were significantly heavier than those who had never been exposed to the virus. Once again, the virus was correlated with fat.
Next, Dhurandhar devised an even more stringent experiment. He tested pairs of twins for presence of Ad-36. He explains, “It turned out exactly the way we hypothesized—the Ad-36 positive co-twins were significantly fatter compared to their Ad-36 negative counterparts.”
Of course, it’s unethical to infect human subjects with viruses for research, so the study can’t be perfectly confirmed. But, Dhurandhar says, “This is the closest you can come to showing the role of the virus in humans, short of infecting them.”
Randy’s physician had been treating him for years and knew that his patient’s struggle was difficult and ongoing. The physician referred Randy to an endocrinologist—Richard Atkinson at the University of Wisconsin—who was having some success with difficult obesity cases.
Randy went to see Atkinson, knowing that if he didn’t get his fat under control, it was going to kill him. The first thing Randy noticed about Atkinson was that he was kind. He didn’t make Randy feel guilty about his weight. “Other places put the blame on you,” Randy says. “They go back into your past, what did you do to get here. It is very judgmental. Atkinson did none of that. He said okay we are here now, how do we fix it? He was very future oriented.”
Atkinson had designed a long-term program to treat obesity. He explained to his patients that obesity is a chronic disease and they would be in treatment “forever.” In the first three months of the program, patients would meet several days per week and attend a lecture explaining obesity and the underpinnings of fat. After that, visits decreased to one every one to two weeks, then one every one to two months. Those who started regaining weight were asked to resume more frequent visits. Subjects had to commit to the full program in order to enroll.
Atkinson also introduced Randy to his new postdoctoral assistant, a young scientist from India, Dr. Nikhil Dhurandhar. Dhurandhar examined Randy and studied his blood samples. Randy tested positive for antibodies to Ad-36, meaning he had likely been infected with the virus at some point in the past. Randy remembered being scratched by that rooster as a child, and that afterward his appetite exploded and he started gaining weight quickly. His troubles with food and rapid fat accumulation—he understood it all now. If he was like the chickens, the marmosets, the twins, and the other humans in the study, then his infection with Ad-36 was helping his body to accumulate fat. He says, “What Atkinson and Dhurandhar did for me changed my life. They made everything make sense. It was very liberating and very empowering.”
How would a virus like Ad-36 cause fat? Atkinson explains, “There are three ways that we think Ad-36 makes people fatter:(1) It increases the uptake of glucose from the blood and converts it to fat; (2) it increases the creation of fat molecules through fatty acid synthase, an enzyme that creates fat; and (3) it enables the creation of more fat cells to hold all the fat by committing stem cells, which can turn into either bone or fat, into fat. So the fat cells that exist are getting bigger, and the body is creating more of them.”
The researchers acknowledge that the rooster scratch may have been the start of Randy’s infection. But they are cautious—the transmissibility of Ad-36 from chickens to humans has never directly been studied.
Though Dhurandhar and Atkinson have conducted several strong studies showing the contribution of Ad-36 to fatness, skepticism remains. Atkinson says, “I remember giving a talk at a conference where I presented 15 different studies in which Ad-36 either caused or was correlated to fatness. At the end of it, a good friend said to me, ‘I just don’t believe it.’ He didn’t give a reason; he just didn’t believe it. People are really stuck on eating and exercise as the only contributors to fatness. But there is more to it.”
Dhurandhar adds, “There’s a difference between science and faith. What you believe belongs in faith and not in science. In science you have to go by data. I have faced people who are skeptical, but when I ask them why, they can’t pinpoint a specific reason. Science is not about belief, it is about fact. There is a saying—‘In God we trust, all others bring data.’”
Excerpted from The Secret Life of Fat by Sylvia Tara, PhD. Copyright © 2017 by Sylvia Tara. With permission of the publisher, W. W. Norton & Company, Inc. All rights reserved.
2016 held some wonderful physics moments—hello gravitational waves! Other moments were experimentally impressive, like shining a laser beam through antimatter, but don't have the same oomph as colliding black holes. And some were just downright deflating: Dark matter still won’t show itself. Still, every experimental let-down opens up new avenues for inquiry. The things physicists did, and did not, find in 2016 are clues about what to expect from the science in the coming years.
In late December 2015 CERN, the European center for high energy physics research, released data showing that there might be a new particle afoot. Was it a sister to the Higgs? A kind of neutrino? Though scientists said it was possibly (even probably) a statistical fluke, excitement spread like a shockwave. Within a month, scientists had posted 500 theoretical articles related to the particle on the preprint arXiv server.
But dreams of post-Standard Model physics were dashed as days lengthened into summer. Nope. No real evidence of a particle. And, as the Large Hadron Collider went to bed this month for the rest of the year, the machine had failed to show the way to a new particle.
A century after Albert Einstein’s prediction, physicists confirmed the existence of gravitational waves by detecting ripples in spacetime created when two black holes crashed together 1.4 billion years ago. Einstein’s theory of general relativity predicted that when anything with mass accelerates, it should create a wave in spacetime, like a rock thrown into a pond creating ripples on the water. He thought such signals would be so weak humans would never be able to detect them. Scientists at the Laser Interferometer Gravitational-Wave Observatory were pleased to prove him both wrong and right.
After announcing their find in February, team LIGO has spent much of 2016 upgrading its observatories in Washington and Louisiana. And in late November, LIGO started listening again, straining to hear new ripples. With the recent launch of their citizen science program “Gravity Spy,” you can help them tune in.
The Large Underground Xenon dark matter experiment spent nearly two years beneath a mile of rock in the Black Hills of South Dakota hoping to hear the faint ping of dark matter—specifically, the signal of a weakly interacting massive particle, one of the favored contenders to constitute dark matter. With a third-of-a-ton of cooled liquid xenon surrounded by powerful sensors, LUX was designed to emit a tiny flash of light and an electric charge if a WIMP collided with a xenon atom in the tank, making it the most sensitive dark matter detector to date.
LUX wrapped up its observations in May. But in July, it announced that it had not found any telltale signals of WIMPs. And while, yes, this is a little bit of a bummer, physicists aren't giving up on the search. Coming up next, the LUX-ZEPLIN experiment will replace LUX at the Sanford Underground Research Facility in South Dakota. It should have 70 times the sensitivity of LUX and is expected to be up and running in 2020.
Back in 2009 we told you about the start of the Baryon Oscillation Spectroscopic Survey, an ambitious project to map the 3D structure of the early universe. This summer the BOSS program released its map—the largest ever, containing more than a million galaxies, allowing physicists to make the best estimates yet of the poorly understood “dark energy” that is accelerating the expansion of the universe. What does a map of a million galaxies look like? Kind of like Jackson Pollock married a pointillist.
Matter and antimatter are clearly different—matter dominates the universe, while scientists can only catch snippets of antimatter. But why this is so is a mystery. The Standard Model says the two should be essentially the same, so any indications that they can break so-called charge-parity symmetry can offer clues to why the universe favored matter over antimatter.
In summer, the T2K Collaboration, based in Japan, presented one such clue. They aimed a neutrino beam at the Super-Kamiokande underground detector in Kamioka—and when they measured them, they saw more electron neutrinos and fewer electron antineutrinos than would be expected. What this means is still not clear, but neutrinos could light the path toward understanding the difference between matter and antimatter.
Late-breaking news from CERN rounded out the year in physics. The ALPHA collaboration saw, for the first time, the color of antimatter. By comparing the optical spectrum of an antihydrogen atom to normal hydrogen they found (within limits of the experiment) that they seem to look exactly the same.
Simply managing to make such a comparison is a feat of experimental engineering. It took 20 years for the the CERN antimatter community to get this far, but now it opens up the field to higher precision comparisons between matter and antimatter ... and the hope that someday, scientists will spot a key difference to help explain why matter dominates the universe and antimatter is so hard to find.
For the first time, scientists have confirmed detection of gravitational waves. The finding not only validates Einstein's theory of relativity but also opens a new window on our knowledge of the universe.
I don’t know how your year went, but somewhere out in the world, a bird called a kingfisher jumped off a branch, pierced into a river, and stabbed a fish. Pretty solid if you ask me. A peanut head bug grew a peanut for a head. Also good. And male rhinoceros beetles jousted with their faces. I can think of worse ways to spend a year.
These, ladies and gentlemen, are my three favorite Absurd Creatures of the year. So I went to the California Academy of Sciences in San Francisco to see them in person. Yeah alright, fine, they’re dead, but it still counts. Check out the video above to see more!
Find every episode of Absurd Creatures here. And I’m happy to hear from you with suggestions on what to cover next. If it’s weird and we can find footage of it, it’s fair game. You can get me at matthew_simon@wired.com or on Twitter at @mrMattSimon.
You don't have to visit a galaxy far, far away to see gorgeous images of space. Leave the interstellar antics to Jyn Erso, and check out WIRED's selection of the most stunning photos in the universe, from burping black holes to exploding supernovas.
From the launch of Juno's 20-month orbit of Jupiter to the SpaceX landing (and explosion) to the possible discovery of an exoplanet orbiting Proxima Centauri, 2016 has been full of accomplishments in space. But this year, NASA also lost one of its greatest stars: John Glenn, fearless as the first American to orbit Earth, and, later, the oldest.
Space photos allow everyone to feel a tiny bit of the wonder Glenn experienced looking down on our planet. It's the kind of awe that keeps you coming back. So enjoy these incredible images from the great beyond. May there be many more to come.
America needs to find a new way to talk about drugs. Clearly, not all illegal drugs are bad—more than half of US states have bucked federal rules banning marijuana. And the prescription opioid epidemic proves that the regulated pharmaceutical system is wide open for abuse. 2016 told a tale of these two drugs, and how people using them circumvented the way America thinks about getting high.
It took longer than a year to get to this point, of course. President Nixon and his staff crafted the 1970 Drug Abuse and Control Act to rein in the excesses of the previous decade's counterculture. It established five classes of drugs, called Schedules, ranked according to potential for abuse and medical value. Marijuana and heroin were especially targeted, and listed as Schedule I, the most restrictive category. That means they both had high potential for abuse, and no redeeming medical value. Perps busted buying, selling, using, or transporting these substances could get multi-year jail sentences.
The 1970 law also created the modern pharmaceutical system. Compounds with less addictive potential and greater medical value were placed in lower schedule categories, where doctors could prescribed them. Prescription opioids, like oxycontin, met this more regulated capacity.
Clearly, the system isn't working great. Marijuana is the most widely used drug in the country, and its annual deaths are in the low zeroes (although people have died because of stupid decisions they've made while high). Meanwhile, prescription opioids like oxycontin kill about 20,000 people each year.
How America Is Battling Its Horrific Opioid Epidemic
Legal Weed Has Arrived. Get Ready for the Budweiser of Bud
Kratom: The Bitter Plant That Could Help Opioid Addicts—if the DEA Doesn’t Ban It
In a legal sense, the backlash against federal weed prohibition began in 1996, when California legalized medical marijuana. This year, the state also voted to allow recreational cannabis use. Four days before that vote, on November 4, President Obama told Bill Maher that California's full spectrum weed legalization could make federal enforcement against weed untenable. And indeed, as of this election, 28 states (plus Washington, DC) now have laws legalizing weed medically, recreationally, or both. Those states contain nearly two thirds of the US population.
Donald Trump's election changes things a bit. His pick for attorney general, Jeff Sessions, is a fervent anti-drug hawk. Under his rule, the DEA, FBI, and other federal agencies could prosecute cannabusinesses and citizen tokers in post-prohibition states. The medical marijuana movement weathered these kinds of attacks for decades, and as a result has accumulated a lot of legal precedent in state and federal courts. However, recreational use—first legalized by Colorado and Washington in 2012—hasn't really been tested like that. And if Sessions, or other anti-drug advocates do go on the attack, they'll be doing so with the possibility that their cases could reach the Supreme Court, where Trump has vowed to fill Antonin Scalia's vacant seat with someone equally conservative.
The prescription opioid problem is a bit more complicated. It has hit particularly hard in economically-stressed rural areas, places where Republican lawmakers can't easily demonize inner city foibles. It began as a result of pharmaceutical companies gaming the FDA's rules for prescription drugs. Purdue Pharmaceuticals, maker of Oxycontin, is the epidemic's easiest villain. In the 1990s, the company started a marketing campaign targeting a so-called epidemic of chronic pain. As a result, doctors started prescribing Oxycontin, and other opioids like it, in droves.
Purdue made billions on this strategy. And in the process, got millions of people hooked on drugs, which led to hundreds of thousands of deaths. (As proof that federal drug policy isn't complete FUBAR, Purdue paid $600 million to the federal government for misleading the public about its drug.)
The bright side to the opioid epidemic, if there is such a thing, is that it has changed the way people think about drug addiction. "This current form of opioid addiciton is more relatable than the past stereotype of heroin junkies lying in the street," says Katharine Neill, a drug policy expert at Rice University. "Not that that stereotype was ever accurate, but now that it's suburban and rural kids getting hooked, they aren't getting demonized in the same way."
That's led to a changing attitude in how to deal with the addiction. "Big trends to watch is how states are treating drug use as a medical, or public health problem, rather than something criminal," says Neill. This attitude is still catching on, but moves like Ithaca, NY's proposed safe space for heroin users shows that parts of the country are moving towards a health-focused, rather than criminal, mentality.
It's also led to strange situations, like the kratom uprising earlier this year. In late August, the DEA announced it was putting this herb—related to coffee, but triggers a mild opiate-like response—on the Emergency Schedule 1 listing. The kratom community, purportedly in the millions, responded in droves. A lot of former opioid addicts use kratom—which is really difficult to overdose on—to treat their pain and the effects of coming down off harder drugs. They even got congressional allies involved. The DEA backed off, momentarily, and opened up a public comment period (which ended December 1). The federal enforcement agency's ultimate decision is still pending.
If the DEA's reaction tells you anything, it's that the public's attitudes towards use and addiction are changing. Simple messages don't work anymore—but states and their constituents are ready to see the nuance in their neighbors' stories of drug use and addiction. Whether the country's new political regime adopts that changing mentality is a blind guess.
In August the DEA announced plans to ban Kratom, a herbal substance used to treat pain, anxiety and in some cases opioid addiction. A group of tenacious users got the agency to back down and extend public comment until December 1st. Now a tough decision lies before the DEA.
This should be an officially labeled time of the year. I suggest we call it Maker Time—it's that time after kids get out of school, but before all of the holiday festivities begin. This is the perfect time for kids (and adults) to make something. This is what I tell my own children (I probably heard it from someone else).
Don't just be a consumer, be a creator (or the rhyming version—be a maker, not a taker). A consumer buys things but also watches videos, plays video games, reads books, plays with toys. A maker creates stuff—it can be anything, a video, a video game, a short story, or even a toy.
I think that all too often people focus on the consuming side of being a human and not enough on the making side. But now is the time to change that. Kids are likely out of school and maybe they don't have too much to do. Now is the perfect time to make something.
It can be difficult to get started, so I am going to give you some of my favorite ideas. I'm not going to give too much in terms of details. The idea of making something is to be creative and not just follow a set of instructions. It's not wrong to have instructions, but I don't want you to feel constrained in anyway.
This one is great. It's easy to do and very creative. Really you just need a phone or a tablet with a camera. There are plenty of apps out there that will work for both iOS and Android—just do a quick search. Oh, you might also want a tripod or at least something to hold the camera stationary while you move things around. Some of these apps also give a sort of ghost view of the last frame so you can line things up better. They are actually quite nice.
Here is a quick shot from a video my son made a while ago.

Using Lego pieces with the mini-figs works great since you can position them in many different ways, but kids can also use their favorite toys and narration over the video once it's finished.
There is a video camera with a portable studio right in the hands of most people—a smart phone. Try making a video. It could be a short scene or show how to cook something. How about this? Go interview some older humans and ask them interesting questions. What was it like before the internet? How did you get your first car? What is an experience you remember that kids today might not be familiar with? That would be fun.
This is the part that I like. Have kids actually make something. No, it doesn't have to be super complicated like an internet-enabled robot (but that would be cool). Let me recommend two projects that are very flexible, fun, and safe.
First, kids could build a buzz bot. These are basically just small constructions with an off-balanced electric motor. When the motor turns, it makes the whole device (the "robot") shake and thus it moves. It's kind of cool and very simple to make. Everyone loves it.

Here you can see these buzz bots use golf tees and a CD for a base. Duct tape and hot glue work great. If you want to make multiple ones, add some magnets on the side and watch them interact with each other.
Second, I can recommend an LED flash light.

This is the flashlight my daughter made. She was super pumped up. It was created from a cardboard tube, wire, and a couple of LEDs. She decorated it the way she liked it and it was awesome.
You might not think of this as "making something", and maybe you are correct. However, this is still fun and very instructive. But what could you take apart? Here are some suggestions:
There are probably some things you should avoid taking apart. Stuff with big capacitors are of course dangerous (like an old CRT type TV). Also be careful of objects with super sharp parts like a robotic knife thrower.
Who hasn't played with a cardboard box? I remember using larger boxes to make a submarine (no, not a real one). But you can do much better. You can even get help. Try doing an internet search for "cardboard armor". Yes, this is pretty cool. With just some cardboard and tape you can make something great.

OK, technically in this example my son used some foam board too. Still, this could have been created with just cardboard. Or maybe this is just another great example that there are no rules. Anything goes when you are building stuff.
If you don't want to get off the computer, then stay on the computer—but make something. You might be surprised at how accessible computer programming has become. Kids of all ages can easily start and make stuff. Here is a random moving soccer ball I made—just for you.
All the code is done with a graphical interface by dragging "code blocks." You can really do a bunch of stuff with this, just take a look at the examples other users have posted.
Oh, but maybe you want to learn even more? Check out code.org where you can find some really excellent coding tutorials for all ages.
The most important part of making things is to find something that you enjoy—but this could be anything as long as you are making stuff. Create stuff, and share the stuff you love. It's great and there are no rules. Don't go buy expensive supplies, just get by with the things you have around you (at least at first).
Here are some other resources for you.
How was science's year, you ask? Oh, not too shabby. A robot rocket fought a robot barge and won. Humans detected gravitational waves from space for Pete's sake. And a certain company that rhymes with Chairanos brought serious drama the biomedical world. The biomedical world.
So yeah, it was an eventful year for science—thanks for asking. From space on down to the future of genetic manipulation, may we present to you WIRED’s nine biggest science stories of 2016.
Space, the old maxim goes, is hard. Engineers have to somehow keep their rockets from immediately exploding on the launch pad, then they have to deposit humans in orbit, and then they have to land those humans safely back on Earth.
The more automation, the better, and they don’t come more automated than SpaceX’s Falcon 9 robo-rocket. After several explosive attempts to get the rocket to land by itself on a robot barge, Musk and Co. finally stuck the landing on April 8. On top of that being a beautiful thing to behold, it was a huge moment for the future of spaceflight. Why? Because cash rules everything around aerospace. A dollar saved is an extra rocket launch earned.
That's not to say SpaceX has it all figured out quite yet. Because in September one of its rockets exploded on the launch pad. Again, space is hard.
Much farther out in space, black holes collide and supernovas go boom, sending ripples through the universe. That is, they did theoretically until this January, when scientists announced that they had finally detected so-called gravitational waves. That’s thanks to the Laser Interferometer Gravitational-Wave Observatory. Here, in a pair of 2.5-mile-long tubes, lasers bounce between mirrors. When a wave hits Earth, it tweaks the path of the lasers ever so slightly.
And boom—a discovery that has profound implications for understanding how the universe is built. As theoretical astrophysicist Chiara Mingarelli told WIRED in January: “The direct detection of gravitational waves will open new avenues to explore the universe, and as such, will almost certainly be revolutionary.”
America's opioid crisis has reached staggering proportions. Every day, 3,900 people start using prescription opioids for non-medical purposes. Every. Day. Tens of thousands are dying of overdoses every year. One promising development is the increased availability of naloxone, which reverses the effects of an overdose. Though that means hospitals need to stock the drug and staff enough doctors—two tall orders in a broken health care system.
And no single drug can fight an epidemic—which is why it's important to pay attention to the voices of addicts who are finding a plant called kratom can help wean them off of opioids. The problem? The DEA wants to make the stuff very, very illegal. Just maybe, with enough weapons at its disposal, America can really begin to fight this war against opioid abuse.
Getting almost 200 nations to agree on anything is a feat in and of itself, but it’s all the more impressive when that thing is giving fossil fuels the finger. Such was the miracle of the Paris Climate Agreement, which officially entered into force in November.
Here’s the thing about global warming: It’s very real. And while it’s great and all for individual countries to pledge to cut back on the emissions that are causing global warming, the world won’t make any progress unless a whole mess of countries hop on board. So here’s to hoping they collectively ensure global temperatures don’t exceed 2 degrees Celsius above pre-Industrial Revolution levels—and stay as close to 1.5 degrees Celsius as possible—like the Paris Agreement demands.
As the world banded together to fight climate change in 2016, the Americas were grappling with a more local crisis: the Zika virus. Scientists reckoned it was what was causing some babies to be born with undersized heads, known as microcephaly. And indeed in April the CDC confirmed the connection. That may not seem like any kind of victory to you, but this was big for science. With better understanding of Zika and microcephaly, scientists can go into 2017 with that much more ammunition to fight the scourge.
California continued to reel from a historic drought in 2016. Sure, things are a bit better this year, thanks to a particularly generous El Niño. But in a state of 40 million people and 27 million acres of crops, a little water doesn't go a long way. In fact, California's drought isn't just sticking around for a while: It'll probably last forever.
Alright, maybe politics creeping into science is nothing new. But 2016 was the year when the two really, really got to know each other—though more in an enemy sense than a romantic one.
First it was Brexit. The UK sealing itself up from the rest of Europe doesn't exactly encourage the movement of ideas. And it could jeopardize the nation's involvement in physics experiments and space exploration in particular.
Across the pond, Donald Trump is coming into power. Yes, the same Donald Trump who claimed global warming is a hoax (it's not, as mentioned earlier) and that vaccines cause autism (they don't), and whose cabinet is all tied up in anti-gay pseudoscience. So American scientists are a bit worried.
No slow-motion-train wreck science story was more riveting this year than the implosion of Theranos, once the darling of the biomedical world. It promised accurate blood tests using just a drop of blood, which didn't turn out to be strictly speaking "true." So came the fraud investigations. The lawsuits. The inevitable pivot (a Silicon Valley word meaning "wow what we had in mind at first definitely isn't a thing"). So where's Theranos headed in 2017? Eh, probably not super great places, if we're being real.
We’re stuck with the genes we’ve got, I’m afraid. My genes, for instance, coded for a slight bow-leggedness, which I’m totally fine with why do you ask. But thanks to the immensely powerful tool that is Crispr, scientists can now edit genes in living organisms. For instance, they can modify mosquitoes to resist the parasite that causes malaria.
Increasingly, and more controversially, scientists are trying to edit human genes in embryos. This year, a second team of scientists in China announced it had used Crispr on human embryos, this time to stave off HIV infection. (The first team used it last year to edit a gene tied to blood disease.) That, my friends, is fraught with ethical conundrums. Prepare to hear a whole lot more from the Crispr front in 2017.
Throughout Earth’s 4.37 billion year history, water has been a constant—and in constant flux. During some periods, it's covered the planet in glaciers. During other, warmer periods, it saturates the atmosphere into a planetary greenhouse. These days, the planet is thawing off from it's last ice cycle—and being warmed at an unprecedented rate by industrialized civilization. Commercial satellite company DigitalGlobe wanted to know what the state of Earth's water currently looked like, so they deployed an army of high-definition drones to snap pictures over four billion square kilometers of water-formed landscapes. The result of the satellite data is a stunning photo book, Water.
“During its history, our Earth is a planet that has already undergone many changes, some of which were dramatic,” says Stefan Rahmstorf, one of the book's contributors. “Modern climate change that is caused by humans will again significantly redesign the face of the Earth.”
The book aims to demonstrates humans relationship with water. This vulnerable blue stuff, as the book calls it, has taken a beating: Humans use the source of all life as a garbage dump. They overfish it, reroute it for hydropower, and clog it with container ships. Industrialization has filled the oceans with 140 million tons of microplastics, which kill birds, fish, and negatively impact entire ecosystems.
Climate change is hurting the oceans worse than anything. The oceans absorb 50 times more greenhouse gases than the atmosphere, and about 30 percent of the carbon dioxide humans produce each year. This disrupts their ability to stabilize the planet's climate.
Water is an aerial tour of the how climate change, pollution, and human activity are endangering Earth's most precious resource.
Maybe your ungrateful family doesn’t value all the thought and care you put into wrapping gifts, but that Scotch tape you’re using is the true underappreciated workhorse: It’s ubiquitous, immensely useful, and largely unrecognized as the modern chemical engineering marvel it is. The magic all started with Richard Drew, a scrappy banjo-playing researcher at 3M. In 1929 he was struggling to create a clear tape for meat-packers and candymakers—the cellophane kept ripping and warping near heat, and the adhesive wouldn’t stick evenly. His work eventually led to today’s Scotch Magic tape, and 3M has kept a tight seal on the recipe for its sandwich of polymers and carefully engineered chemicals ever since. So we did our own research on what likely makes 3M’s tape stick.
Cellulose Acetate
The “invisible” film that we know as tape. It starts as cellulose, a long, tough, glucose-laced polymer that gives plants their structure. Typically it’s extracted from cotton or wood and treated with acetic acid, the chemical that makes vinegar vinegary. The process swaps out hydrogens in the cellulose for acetyl groups, which allows the hardy polymer to be dissolved and extruded into a translucent strip that’s strong and water-­resistant yet able to be torn off by hand while you’re lovingly (or hastily) wrapping those holiday gifts. Cellulose acetate goes way back: It’s been used for a century as film for photographs and movies.
__Acrylics __
Although 3M has kept the specific ingredients of its adhesive under wraps for decades, it’s undoubtedly a soup of monomers like butyl acrylate, methyl acrylate, and methyl methacrylate. The acrylic mixture flows onto whatever surface you press the tape on and stays put, thanks to sticky molecular interactions called van der Waals forces.
__Deionized Water __
There’s no water in the tape itself, but water is likely used during manufacturing: The acrylics need to link into chains to form the adhesive, and that often happens in water or another solvent. Tape makers coat the cellulose film with the ­adhesive-water cocktail, and when the H2O evaporates, it leaves a layer of gluey goo that’s usually 20 microns thick.
Polydimethyl-siloxane
Silicone, in other words. To prevent the tape from sticking to itself when it’s rolled up, companies apply a release coating to the nonsticky side. Of course, 3M won’t say what’s in it, but silicone is a usual suspect: It doesn’t really attract other materials, thanks to the stable methyl groups that give it low molecular surface energy. Regardless, the formula is engineered so that the tape unrolls smoothly and quietly—none of those skreeeck noises that packing tape makes.
Styrene Acrylic or Polyurethane
One of these coatings probably helps keep the adhesive stuck to the film so the two don’t just separate when your kid is yanking off that wrapping paper. Styrene acrylic is like the acrylate adhesive but stickier; polyurethane is the main ingredient in some wood finishes and latex-free condoms. No, you should not craft your own mummylike barrier of Scotch tape to use as birth control.
This article appears in the December 2016 issue. Subscribe now.
Of course I'm not the first to look at the physics in Super Mario Bros—there was this interesting paper looking at the optimal jump to get to the highest point on the flag at the end of the level. There is also a nice page looking at the acceleration of jumping Mario in the different games. Good stuff.
But there's a new game out—Super Mario Run on iOS and Android. This is a great chance to take another look at the physics of Mario.
The best way to get data from a video game is to first capture the action and then use video analysis. With video analysis, I can get position-time data by looking at the location of the object in each frame. There are enough important details that I could actually write a book on video analysis (which I did), so I will just include some notes.
That's your crash course in video analysis.
Now on to the data. I am first going to look at the position of Mario as he runs (before he jumps). Here's what I get.

I'm not sure what was going on during that first part. I think I captured part of the motion in which Mario was still in the air. But anyway, the rest looks pretty linear. Since the horizontal velocity is the rate of change of position, the slope of this line would be the x-velocity. I get a value of 7.21 coins per second.
Also, check this out. Here is Mario's y-position as he runs.

It looks like he takes about 0.2 seconds per stride. I'm not sure if that's important or useful, but I've said it so I will now move on.
After Mario leaves the ground, he should just be like projectile motion. For projectile motion, the following should apply:
But is the horizontal velocity constant? Here is a plot of x-position as a function of time during the jump.

Excellent. A constant horizontal velocity, and fairly similar to the running speed of Mario. What about the vertical motion?

This does not show a constant acceleration. For the case of constant acceleration, a plot of position vs. time should be a parabola. This is not a parabola. Instead, it looks like a constant vertical velocity going up, then constant acceleration at the top followed by constant velocity going down. The vertical velocities are around 15 coins per second (I got 12.8 c/s up and 16.3 c/s down).
Since the top of the jump looks like constant acceleration, I fit a quadratic equation. Looking at the fit parameters, this would give a vertical acceleration of -6.3 coins/s2.
Now for some fun. Suppose that Mario lives on Earth and the acceleration at the top of the jump should indeed be -9.8 m/s2. I can use this to find the size of 1 coin and then find the size of other stuff. Let me just set these two accelerations equal to each other.

The units can be treated just like a variable so that I can solve for the relationship between coins and meters.

The diameter of 1 coin would be 1.56 meters (5.12 feet). Wow. Looking at Mario, he is 1.26 coins tall or 1.97 meters (6.5 feet). His height doesn't really bother me, that seems reasonable—it's the size of his head that is crazy big.
Clearly, there are some unanswered questions. Here are some for your homework.
For hundreds of thousands of years, the Siberian permafrost has been a giant freezer for everything buried within it. But global warming has put the frozen ground in defrost mode, and the tundra is now heating up twice as fast as the rest of the planet. “Permafrost is a silent ticking time bomb,” says Robert Spencer, an environmental scientist at Florida State University. As it thaws, the dirt could release a litany of horrors. Beware: The ice-beasts cometh.
Threat level: Sporadically scary
Seventy-five years ago an anthrax outbreak in West Siberia felled herds of reindeer. In July those carcasses thawed and infected 23 humans, killing one.
Threat level: Civilization-imperiling
Taken together, the remains of ancient grass, moss, and animals buried in permafrost add up to some 1,500 billion tons of carbon. Microbes munch on that reheated mulch, exhaling carbon dioxide that could further warm the atmosphere, which would melt even more permafrost and spur a runaway cycle of warming.
Threat level: Extra-civilization-imperiling
In 2014 scientists began noticing massive craters popping up across Russia’s remote Yamal peninsula. The most likely explanation? As the ground gets warmer and thus wetter, bacteria produce more methane (which, by the way, is 30 times more potent than carbon dioxide at trapping heat in the atmosphere). Those pockets of gas build up underground and—boom!—go up with a bang.
Threat level: Haunting
In 1993 a Russian researcher discovered the extremely well-preserved 2,500-year-old body of a woman covered in swirling tattoos and buried with gilt figurines, pots of spices, and six horses. Archaeologists suspect she was a member of the nobility, and she was exhumed for further study. Corpses at other, more plebeian burial sites, meanwhile, will likely just rot away.
Threat level: High—if you’re an elephant
Climate change unearths prehistoric bones by speeding up soil erosion rates. When that happens, profiteers swoop in for mammoth tusks. Since they’re legal and elephant ivory is not, sellers just tell buyers it’s all mammoth. The ivory market stays strong, elephant populations stay threatened.
Threat level: Oh you’ll be fiiiiine
In the past two years, French microbiologists have discovered two kinds of giant viruses buried deep within the Siberian permafrost. Those viruses were still infectious … to amoebas. The researchers warn that there could be other, nastier viruses, like smallpox, lurking beneath the surface, but they probably won’t get you sick. Probably.
*This article appears in the December 2016 issue. Subscribe now.
This post is Chad Orzel’s fault. It started with this tweet:
Hey, @rjallain , is this consistent with your regression analysis of the Lego brick cost a while back? pic.twitter.com/RaTcP07xI1
— Chad Orzel (@orzelc) December 13, 2016

Yes, it’s true that I have pondered the price of Lego bricks before, by looking up the cost and number of pieces in various sets. Here is the data, and a link to my original analysis.
I arrived at a price of about 10.4 cents per piece. Ah, but what about the price per mass, at $8.99 per quarter-pound? This of course requires more data.
It’s not difficult to find the mass of a Lego. Just plop that sucker on a balance and boom—there is your mass. Determining the volume is tougher, because Lego pieces are not solid. Sure, I could drop the pieces into water and find the volume by displacement, but that wouldn’t be very interesting. The mass to volume ratio (something I like call density) likely would be constant since the pieces are probably made from the same material.
Instead, I will look at two different kinds of volume. First, the exterior volume. Imagine a simple Lego brick:
I can measure the height, length, and width with a normal lengthometer (some might call this a ruler). What about the bumps on top (I believe Lego calls them studs.)? I will ignore them—this isn’t the real size anyway. Using these measurements, I can calculate the volume in cubic centimeters.
But there is another volume measurement in units of Lego—the studs. For the brick above, it seems clear that the length and width are two studs long. What about the height? If you take the thinnest Lego piece, which resembles a slice, you find that three of them is the same height as one “normal” brick.
And so I will assign this “normal” brick a height of three slices, although I am sure Lego has specific term for this dimension. To find the volume, I simply multiply length times width times height. However, a height unit isn’t the same as a length or width unit, so this might seem meaningless. I think I will stick with the volume measurement in centimeters.
Lego offers many kinds of pieces. I am going to stick with the basic shapes—no specialized pieces. Here’s a look at the stuff I measured:
With the mass and size data, I can plot mass versus volume:

The data looks fairly linear—well, linear enough for me. The slope of this fit is also a great way to estimate the density of a Lego piece. Slope is defined as the rate that the vertical variable (mass) changes with respect to the horizontal variable (volume). This is just like mass divided by volume—the density. Looking at the slope of this fitting line, the average Lego density is 0.565 g/cm3. This seems plausible. I always like to compare densities of object to that of water which is 1 g/cm3. The Lego density is lower than water so that they would float—assuming that the air stays inside of the piece. Again, this is reasonable.
But why does the data look like it has two linear functions mixed together? Probably because there are two Lego densities. In this study, I only examined single-height blocks or the three-height blocks. The shorter ones offer less empty space—this would yield a higher density than the taller blocks. That’s just my guess—you can examine Lego density further as homework.
How do I link the Lego density to the price per pound? First, I can determine the price per gram with a simple unit conversion. This would be 7.9 cents per gram. Now using the 10.4 cents per piece, I can do the following unit trick:

If I invert that, I get 1.35 grams per piece. This could be interpreted as the average mass of a piece in a kit—if the $8.99 per quarter pound is realistic. What would a 1.35 gram Lego piece look like? Based on my data it would fall somewhere between the tall 2×2 and the tall 4×1 brick.
How about just one homework question. Find a ratio of Lego pieces such that the average mass is exactly 1.35 grams. You should be able to see my mass data if you click on the plotly graph above. You will need that or you will need to determine your own Lego mass data.
An unexpected explosive eruption rocked Bogoslof in the Aleutian Islands of Alaska according to the Alaska Volcano Observatory. The eruption was noticed by pilots flying near the remote volcano along with satellite imagery (see below) that revealed an ash plume reaching up 10 kilometers (32,000 feet). However impressive that plume might be, it appears that the eruption didn’t last too long, as pilots passing the volcano less than an hour later noted much diminished activity.
Update 10:30 am EST on 12/22: Bogoslof had a second big explosive eruption during the night of 12/21. The plume might have been slightly taller than the first explosion, topping out at 10.6 kilometers (35,000 feet). Just like the first eruption, the activity was short-lived, with the explosions ending within 30 minutes. These eruptions have been full of volcanic lightning (see below), which is characteristic of vulcanian eruptions that eject mostly old material in the vent area (Note: after some discussion with folks in the know, these eruptions likely have a lot of interaction with seawater, creating fine ash that is prone to lightning.This Surtseyan style of eruption is a mix of new magma and water to create some of the dramatic explosions). Vulcanian eruptions can be isolated events or the opening salvo of longer periods of activity, so Bogoslof will be very interesting to watch over the next few weeks. The volcano currently sits at Orange/Watch status.
Lots of lightning happening during the #Bogoslof eruption right now in the Alaskan Aleutians @alaska_avo: https://t.co/iq0YFReAfw pic.twitter.com/4vC2VgfFes
— Alexa Van Eaton (@volcaniclastic) December 22, 2016
Bogoslof on satellite images from 12/20:
Yep, looks like a little poof. Data from Himawari-8 via @UWSSEC in the 7.3um Band (sensitive to SO2) confirms. pic.twitter.com/VPpwrq1XRy
— James Hyde (@wxmeddler) December 21, 2016

If you look at a map, Bogoslof doesn’t look like much, just a small island in the Bering Sea. However, when you consider the fact that the volcano rises 1,500 meters (~4,900 feet) from the seafloor, you can appreciate that it isn’t a tiny feature. The last known eruption of Bogoslof was in 1992 when it produced a VEI 3 eruption and a new dome. Since 1796, it has produced a half dozen VEI 2-3 explosive events from different vents on the island. A number of small islands have come and gone over the last 400 years as lava domes have formed during eruptions of Bogoslof, only to be eroded or destroyed. Today, only about 300 meters (~1,000 feet) of the volcano sticks above the ocean waters in the form of impressive spires of lava. It will be interesting to see if the new eruption is the start of a new dome-building period at Bogoslof like those that occurred in 1883 and 1796.
There is nothing in the way of realtime monitoring (like seismometers) for a volcano like Bogoslof, all alone in the Bering Sea north of Unalaska. Only about 4,300 people live within 100 kilometers (62 miles) of the volcano, but as the reports suggest, there is an awful lot of air traffic that crosses the Aleutian Islands, heading from North America to Asia. The quick notices from the Volcano Ash Advisory Centers (VAAC) about eruptions like this one from an unexpected source like Bogoslof are vital for air traffic controllers and airlines to change routes and avoid volcanic ash hazards for aircraft.
As of the morning of December 21, AVO has Bogoslof on an Orange/Watch status, lowering it from Red/Warning after no signs of continuing eruptions were seen overnight.
Something was wrong with the Jackson Pollock. For one thing, 3-D images from a stereomicroscope revealed that the signature was traced with a needle—forged. And, working with a hyper-precise Raman microscope, a tool capable of analyzing sample areas as small as a thousandth of a millimeter across, Jamie Martin identified the presence of Red 170, a pigment that wasn’t widely available until decades after Pollock’s death. Yep. The painting was a fake.
When art historians, museum curators, or law enforcement officials suspect that a work of art isn’t genuine, they call Orion Analytical, Martin’s one-man “microniche materials analysis and consulting firm.” Over the years, he has examined everything from Egyptian artifacts to rare bottles of wine, searching for the tiniest flaws.
“We’re analyzing samples so small they’re invisible to the naked eye,” Martin says.
In his investigations, he relies on research, his vast knowledge of art history, and a collection of highly specialized tools—microscopes, cameras, spectrometers—to answer questions like: Did the forger paint over another painting? Are the materials consistent with the era? Were any elements added later? Is the signature real?
As it turns out, that fake Pollock was one of nearly 40 forgeries created by a Chinese artist in Queens, New York, and sold or consigned by Manhattan’s prestigious Knoedler Gallery between 1994 and 2008. Martin examined 16 of the paintings himself, discovering flaws like anachronistic materials and marks from an electric sander. The scam, totaling some $80 million, is the biggest in American history. Earlier this year, Martin’s detective work landed him in court as an expert witness in a $25 million case over a fake Mark Rothko. (A lawsuit involving the Pollock is ongoing.)
When Martin began his career as a conservation scientist in the early '90s, he didn’t exactly picture himself working cases with the FBI. Back then, he was mostly analyzing works of art so that curators could choose the right materials for repairs and restoration. But that keen eye for detail eventually made Martin into one of the best fakery spotters in the world. Today he consults with the Feds and has taught classes like Infrared Spectrometry for Trace Analysis at the FBI Academy in Quantico, Virginia.
“It’s my job to use technology and research to tell the story of a work of art,” Martin says. “As a scientist, I feel a responsibility to preserve art history so that future generations have an accurate, rich understanding of who these artists were and what they created.”
But while Martin feels an obligation to help find the truth, that’s not what he loves most about his job. He prefers projects like determining the exact materials that New York’s American Museum of Natural History should use to repaint its massive blue whale or figuring out why the surface of a historic brownstone in New York has turned black and begun to flake off. "When I can help conservators preserve cultural property, that’s a really good day for me," he says.
Because Martin is always looking for connections between his data and the objects he examines—and because whatever he’s working on could lead to a legal dispute—he prefers to work alone. Every sample, every test, every element passes through his hands, and his hands only. Well, with one exception. Once, he allowed his daughter, Elizabeth, to use an infrared microscope to test those paint samples from the famous blue whale for her high school science project. But then he redid the tests himself, just to be sure.
Every case is unique; sometimes Martin has to invent whole new methods to solve mysteries. One time, a painting was destroyed while in transit, despite the fact that it had been professionally packed in a crate and stored in the hold of an airplane. The only clue was an oddly shaped hole in the crate. Martin had a hunch about what made it thanks to a former job driving a forklift for an airfreight company. (He's a man of many talents.) He made a scale model of the artwork and the crate, then tested how much force it would take to knock the painting loose. His experiments revealed that the screws used to attach the painting to the crate were too small—and not recommended by the manufacturer.
“If they had spent 89 cents more on screws, a $3 million painting would not have been a total loss,” he says.
Martin encounters sloppiness all the time. Even the craftiest fraudsters leave clues, he says. In another case, a forger was clever enough to buy the same kind of paint that would have been used by an artist in 1932. But Martin one-upped him: After analyzing the entire 12-square-foot surface, he found what he refers to as “accidental material” stuck in the paint. It was a single polypropylene fiber, a material that wasn’t introduced until 1958.
Pro tip for con artists: If you’re trying to fool Martin, wear cotton.
This article appears in the December issue. Subscribe now.
President Obama’s science advisor, John Holdren, isn’t kidding when he praises his boss. “I sincerely think [he] is the most science-savvy president since Thomas Jefferson,” he says. “And there’s a lot more science to be savvy about.”
Holdren has his biases, of course. But there is no denying the scientific know-how of the outgoing administration. Obama has called himself a “science geek,” and made science and technology remarkably central to his tenure—from his appointments to lead agencies to his climate change initiatives, from his emphasis on STEM education to the unbridled nerdery he’s displayed at the White House Science Fairs. But what comes next has scientists worried.
Thomas Jefferson was followed in office by James Madison, one of the authors of the Federalist Papers and the so-called “Father of the Constitution.” Obama will cede his seat to a real estate developer who thinks hairspray is “not like it used to be” because it no longer contains banned ozone-depleting substances.
Still, Donald Trump could appoint a real, honest-to-god scientist to Holdren’s position as advisor and director of the Office of Science and Technology Policy. Holdren, trained as an engineer and theoretical plasma physicist and with decades of scientific policy experience, has been offering up that substantial expertise for two terms now. Few people know as much about the potential—and limitations—of the president’s right-hand lab coat.
Holdren sat down with WIRED last week in his office at the Eisenhower Executive Office Building, next door to the White House. Down the hall from a somewhat subdued going-away party, we sat across from a cluttered bookcase topped with NASA memorabilia. Outside on Pennsylvania Avenue, construction continued on the Presidential reviewing stand, where the new administration will take in the inaugural parade. Though he won’t speculate on specifics related to the incoming president, Holdren offered plenty of thoughts on the advisor position in general, its history, and what sorts of qualities in a president might make the advisor job easier—or impossible.
“You certainly want a president who is open-minded, in the sense of being willing to hear conclusions that conflict with his preferences,” says Holdren. Richard Nixon had two science advisors, both of whom resigned after a couple of years; after that, Nixon simply abolished the position, largely because he didn’t like some of the advice he had gotten (in particular on supersonic transport and on anti-ballistic missile systems).
A president is more likely to listen to opinions that challenge his preconceived notions, Holdren says, when there is an actual, trusting relationship between advisor and president. With such a relationship in place, “when the science advisor has something to say that the president doesn’t want to hear, the president doesn’t assume that it’s wrong, doesn’t assume that the science advisor made it up to make his life more difficult.”
Holdren got to know Obama before he was president. “He has a mode of deepening his understanding [of a topic he is interested in] which is a very effective one,” Holdren says. “He arranges for his staff to find eight or ten of what they regard as the most interesting and best informed people in the country on a given topic, but from different perspectives, and he assembles them for a three-hour private dinner. And he conducts a dinner conversation like an orchestra conductor with these folks sitting around the table.”
Holdren sat next to Senator Obama at such a dinner on climate change in 2007—which he says included an oil exec, some NGO leaders, and academics—and their relationship grew from there. He went on to offer comments on some energy and environment-related speeches for Obama, advised him during his presidential campaign, and was then offered the advisor/OSTP director job. “I took it in a microsecond.”
To the public, the science advisor role can appear toothless, if it appears at all—but that’s largely by design. Conversations between the president and his advisors are confidential, and Holdren says those who take the role tend to be content to operate in the background. He was mentored early in his career by Kennedy’s and Eisenhower’s advisors: “They all told me, you get more done if you don’t care who gets the credit.”
When some tidbit of advisor influence does leak out, it can shape the narrative. Jerome Wiesner, Kennedy’s advisor, became known as one in particular that “had the president’s ear,” thanks in large part to the tale of the Minuteman missiles. He helped convince JFK that the US only needed 1,000 intercontinental ballistic missiles—rather than the much larger buildup the Joint Chiefs of Staff were gunning for. Wiesner didn’t ask for credit for helping tamp down the arms race, but he got it nonetheless.
That sort of story is the exception, however, and not the rule. Holdren lists off other advisors—Frank Press with Carter, John Gibbons and Neal Lane with Clinton, among others—who came and went from the White House without much notice, but carried substantial influence in helping shape policy.
And that influence doesn’t manifest solely through conversations with the president—Holdren says Clinton handed off a lot of the science and technology issues to his VP. “Al Gore was noted among OSTP staff for calling them up in the middle of the night and saying, ‘I’m reading this paper in Nature and there’s one point I’d like you to clarify for me.’ We’ve rarely had a VP who reads Nature late at night.”
Holdren himself doesn’t trumpet his accomplishments. But his influence can be spotted in many areas: Notably, he was said to have been “right at the heart” of deliberations surrounding the Clean Power Plan, the administration’s signature climate change initiative.
Even if President Trump had a well-established scientific confidante, he doesn’t even need to replace Holdren if he doesn’t want to. Congress established OSTP by statute in 1976, and its director is a Senate-confirmed position. But the conjoined position of “assistant to the president for science and technology,” is not confirmed, and is not mandated by Congress. There is recent precedent for separating the two: John H. Marburger III served as OSTP director for almost all of George W. Bush’s tenure, but never was granted the advisory title. In case it has faded from memory, that administration’s record on science wasn’t stellar.
“As innocuous as it sounds, the assistant to the president title is what gives you direct access to the president,” Holdren says. “The assistants to the president can write memos to the president, they can get appointments with the president. If you’re not an assistant to the president the only way you’ll get to the president is with the help of an assistant to the president.”
Of course, Obama’s focus on science and the advice coming from Holdren’s office still couldn’t crack through some of the barriers outside of the White House. The president promised in his inaugural address to “restore science to its rightful place”—but we still have a House Committee on Science, Space, and Technology tweeting out links to absurd global cooling stories. He couldn’t control that part,” Holdren says. “He couldn’t put science in its rightful place in the Congress. He could put science in its rightful place in the executive branch, and he did.”
That may sound distressingly like the final nail in science’s coffin in government. If Congress never came around (with some notable exceptions, such as a largely bipartisan recognition that funding biomedical research is a good idea), and the executive branch is about to fall down some pretty medieval-sounding holes where science is concerned, what’s left?
“We don’t know what’s going to happen,” says Holdren. He noted that many promising initiatives from OSTP and the Obama administration maintain bipartisan support, and could easily continue in the background without ever attracting the Trump Tower Eye’s attention. For example, the 100Kin10 program is a broad effort to train 100,000 new STEM teachers over the course of a decade, which would likely improve science literacy around the country; cutting something like that wouldn’t be a particularly popular move. There are plenty of under-the-radar programs like this, aimed at pushing science forward.
“I will hope that the next administration will recognize the value to the country and to the world that those investments and those priorities have produced, and I hope that most if not all of those initiatives will go forward,” Holdren says. And if not? “On January 20, I will become a private citizen, as the president will be, and if those initiatives do not go forward in the way I think will benefit the nation—I’ll be talking about it.”
*Dave Levitan is a journalist, and author of the book *Not A Scientist: How politicians mistake, misrepresent, and utterly mangle science.