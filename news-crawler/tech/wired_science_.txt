
On Monday, SpaceX is poised to end a 40-day launch drought in the United States with a cargo run to the International Space Station. The commercial space outfit will fire off its 12th resupply service mission for NASA from Kennedy Space Center using a fresh Falcon 9 rocket. On top, a Dragon capsule with over 6,400 pounds of supplies—a typical haul could include a variety of toilet paper (the Russians prefer a rougher texture while the Americans opt for a softer touch), fresh socks, and most importantly, tortillas. Mexican food has been a staple in low-Earth orbit since the 80s, so regular reups of Picante sauce are a must.
Last week, SpaceX performed a hold-down test fire of the CRS-12 Falcon 9 rocket and is targeting a 12:31 pm Eastern liftoff on Monday. Minutes after the booster delivers SpaceX’s Dragon to a preliminary orbit, it will attempt a return flight for touchdown at Landing Zone 1 on Florida’s space coast.
Along with the groceries, ISS cargo drops have included hardware like an espresso machine, a handy 3-D printer, and even a relatively huge inflatable module currently being accessed and tested by the crew. In addition to the essentials, Dragon will be carrying enough resources to aid in over 250 research projects. This includes a NASA-funded experiment to study cosmic rays dubbed CREAM (for Cosmic Ray Energetics and Mass), mice to study the effects of long-duration spaceflight on vision and joints, and seeds to continue growing plants in microgravity. The agency also partnered with Hewlett Packard to send up a supercomputer to determine if off-the-shelf computer hardware can properly operate in space.
About 10 minutes after Monday’s liftoff, the Dragon will begin firing a series of carefully orchestrated thrusts and navigate toward its football field-sized destination. The spacecraft will rendezvous with the space station early Wednesday morning to be captured by NASA astronaut Jack Fischer and ESA astronaut Paolo Nespoli with Canadarm—a robotic grappling mechanism and Canadian folk hero that has served human operations in orbit for 30 years. Dragon will stay mated to the station’s Harmony module for a month until it flies home for a splashdown in the ocean off the coast of Baja, California.
This launch is notable for SpaceX: It will be the last time the company launches a factory-fresh spacecraft from its current line of Dragons on a mission. For future cargo runs with this version of the Dragon, the company plans to only use recovered and refurbished vehicles. SpaceX successfully launched its first refurbished Dragon in June and unceremoniously became the first to fly a reusable spacecraft to the station from American soil since the retirement of the space shuttle.
The shift will help SpaceX focus development efforts on its next generation Dragon V2, which is expected to carry both crew and cargo to low Earth orbit for NASA beginning in late 2018.
Before that, though, SpaceX still has some catching up to do. Though it has launched more times in 2017 than any year since its founding, SpaceX is still chipping away at a customer manifest that piled up after two separate explosions grounded the company for months at a time. SpaceX has been on a short launch hiatus after a dramatic double-header weekend of bi-coastal launches (and sea landings) in late June followed by an an expendable booster launch from Kennedy Space Center just days later in early July. Monday’s cargo run will be the 11th Falcon 9 launch this year and CEO Elon Musk hopes to get in a handful more before 2018.
Watch Two Astronauts Repair the ISS in the Dark
Watch NASA Launch 38 Itty Bitty Satellites to the ISS
NASA Is Finally Sending a Hotel Magnate's Inflatable Habitat to the ISS
After the Falcon 9 booster places Dragon in orbit, it’ll come roaring back to Earth using a new set of titanium-forged fins that improve aerodynamic control and can better resist reentry heat. SpaceX can perform a ground landing this time due to the relatively short orbital distance of Dragon’s destination, which will leave enough fuel in the booster tank to navigate back to the coast. The space station is an average distance of 250 miles above Earth’s surface, while some of the commercial payloads SpaceX delivers can go beyond 20,000 miles—requiring enormous thrust and the use of a drone ship to catch the depleted booster at sea.
If the landing is successful, it'll mark SpaceX’s 14th successful rocket recovery since its first landing in December 2015. Since then, the company has launched two previously-flown Falcon 9 rockets on commercial missions, and is aiming toward the ultimate goal of a 24-hour turnaround window for a single booster.
Another big goal on the horizon: SpaceX will attempt to launch the maiden voyage of the Falcon Heavy—a triple-booster rocket that Musk claims will be the most powerful in operation—from Pad 39A at Kennedy. SpaceX leased the historic site (it hosted the Apollo 11 liftoff and over 80 Space Shuttle flights) from NASA for 20 years, and continues to renovate the pad for Falcon Heavy. The company hopes to launch the test flight in November, though Musk concedes that it will be “risky” and failure is likely. The billionaire went as far as saying that it will be a “win” if Pad 39A isn’t damaged or destroyed after the enormous rocket takes off. Musk’s landlords at NASA must be very excited.
The Astrobee is a brilliant little robot that'll use puffs of air to autonomously float around the ISS.
The human brain is routinely described as the most complex object in the known universe. It might therefore seem unlikely that pea-size blobs of brain cells growing in laboratory dishes could be more than fleetingly useful to neuroscientists. Nevertheless, many investigators are now excitedly cultivating these curious biological systems, formally called cerebral organoids and less formally known as mini-brains. With organoids, researchers can run experiments on how living human brains develop—experiments that would be impossible (or unthinkable) with the real thing.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
The cerebral organoids in existence today fall far short of earning the “brain” label, mini or otherwise. But a trio of recent publications suggests that cerebral-organoid science may be turning a corner—and that the future of such brain studies may depend less on trying to create tiny perfect replicas of whole brains and more on creating highly replicable modules of developing brain parts that can be snapped together like building blocks. Just as interchangeable parts helped make mass production and the Industrial Revolution possible, organoids that have consistent qualities and can be combined as needed may help to speed a revolution in understanding how the human brain develops.
In 2013 Madeline Lancaster, then of the Austrian Academy of Sciences, created the first true cerebral organoids when she discovered that stem cells growing in a supportive gel could form small spherical masses of organized, functioning brain tissue. Veritable colleges of mini-brains were soon thriving under various protocols in laboratories around the world.
Much to the frustration of impatient experimentalists, however, the mini-brains’ similarity to the real thing only went so far. Their shrunken anatomies were distorted; they lacked blood vessels and layers of tissue; neurons were present but important glial cells that make up the supportive white matter of the brain were often missing.
Worst of all was the organoids’ inconsistency: They differed too much from one another. According to Arnold Kriegstein, director of the developmental and stem cell biology program at the University of California, San Francisco, it was difficult to get organoids to turn out uniformly even when scientists used the same growth protocol and the same starting materials. “And this makes it very difficult to have a properly controlled experiment or to even make valid conclusions,” he explained.
Researchers could reduce the troublesome variability by treating early-stage organoids with growth factors that would make them differentiate more consistently as a less varied set of neurons. But that consistency would come at the expense of relevance, because real brain networks are a functional quilt of cell types—some of which arise in place while others migrate from other brain regions.
For example, in the human cortex, about 20 percent of the neurons—the ones called interneurons, which have inhibitory effects—migrate there from a center deeper down in the brain called the medial ganglionic eminence (MGE). An oversimplified organoid model for the cortex would be missing all those interneurons and would therefore be useless for studying how the developing brain balances its excitatory and inhibitory signals.
Deliverance from those problems may have arrived with recent results from three groups. They point toward the possibility of an almost modular approach to building mini-brains, which involves growing relatively simple organoids representative of different developing brain regions and then allowing them to connect with one another.
The most recent of those results was announced two weeks ago in Cell Stem Cell by a group based at the Yale Stem Cell Center. In the first stage of their experiments, they used human pluripotent stem cells (some derived from blood, others from embryos) to create separate organoid replicas of the cortex and MGE. The researchers then let mixed pairs of the ball-shaped organoids grow side by side. Over several weeks, the pairs of organoids fused. Most important, the Yale team saw that, in keeping with proper brain development, inhibitory interneurons from the MGE organoid migrated into the cortical organoid mass and began to integrate themselves into the neural networks there, exactly as they do in the developing fetal brain.
Earlier this year, teams from the Stanford University School of Medicine and the Austrian Academy of Sciences published reports on similar experiments in which they too developed cortical and MGE organoids and then fused them. The three studies differ significantly in their details—such as how the researchers coaxed stem cells to become organoids, how they nurtured the growing organoids, and what tests they ran on the derived cells. But they all found that the fused organoids yielded neural networks with a lifelike mix of excitatory neurons, inhibitory neurons and supporting cells, and that they could be developed more reliably than the older types of mini-brain organoids.
To Kriegstein, all three experiments beautifully illustrate that the cells in organoids will readily transform into mature, healthy tissue if given the opportunity. “Once you coax the tissue down a particular developmental trajectory, it actually manages to get there very well on its own with minimal instruction,” he said. He believes that specialized organoids could bring a new level of experimental control to neuroscientists’ explorations: Scientists could probe different brain organoids for information about development within subregions of the brain “and then use that combined or fused platform to study how these cells interact once they start migrating and encountering each other.”
In-Hyun Park, an associate professor of genetics who led the Yale study, is hopeful that organoids might already be useful in preliminary investigations of the developmental roots of certain neuropsychiatric conditions, such as autism and schizophrenia. Evidence suggests that in these conditions, Park said, “there seems to be an imbalance between excitatory and inhibitory neural activity. So those diseases can be studied using the current model that we’ve developed.”
A Map of the Brain Could Teach Machines to See Like You
New Clues to the Mystery of How Our Brains Keep Time
New Math Untangles the Mysterious Nature of Causality
Kriegstein cautions, however, that no one should rush to find clinical significance in organoid experiments. “What we really lack is a gold standard of human brain development to calibrate how well these organoids are mimicking the normal condition,” he said.
Whatever applications organoid research may eventually find, the essential next steps will consist of learning how to produce organoids that are even more true to life, according to Park. He has also not given up hope that it will eventually be possible to create a mini-brain in the laboratory that is a more complete and accurate stand-in for what grows in our head. Maybe doing so will involve a more complex fusion of organoid subunits, or maybe it will demand a more sophisticated use of growth media and chemicals for directing the organoid through its embryonic stages. “There should be an approach to generating a human brain organoid that is composed of forebrain plus midbrain plus hindbrain all together,” Park said.
Jordana Cepelewicz contributed reporting to this article.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
With optogenetics, the ability to restore and enhance brain function is becoming a reality.  In this World Economic Forum discussion, Nature magazine neuroscience editor I-han Chou explains how the radical method works and the ethical issues it could cause.
It’d be hard to invent a more Star Wars-esque spacecraft than Dawn. It’s 65 feet from tip to tip. It’s exploring the asteroid belt. And it’s got an ion drive, for Pete’s sake.
But Dawn also has a serious job to do. Launched in 2007, it’s been investigating Ceres and Vesta, two mysterious protoplanets in the asteroid belt between Mars and Jupiter. These are smallish, truly ancient bodies, remnants of the early solar system (protoplanets being bodies that formed early on, some of which turned into actual planets like the one you're sitting on) with plenty of secrets to tell―secrets that Dawn has been handedly unraveling.
Why pick these objects, of all the objects in the solar system? Because figuring out what makes them tick will give scientists insights into how asteroids and protoplanets and actual planets form. And what they're finding is exhilarating. They've explored a massive impact on Vesta, which flung heaps of material into space. And on Ceres, they eyeballed a cryovolcano that spews brine instead of lava. Water, of course, is a prime ingredient for life.
Dawn only got where it is because of its ion drive, which unlike a conventional engine, actually sends electricity through xenon fuel, breaking out ions. These propel the spacecraft, but only very weakly. No matter, though―Dawn has been accelerating for five and a half years in the vacuum of space. That's allowed the spacecraft to change its velocity by 25,000 miles per hour.
So ten years after it left the comfort of Earth, we’re checking in on Dawn. Peep the video above to see where Dawn’s been, where it’s going, and the tantalizing discoveries it’s made along the way.
The most formal manifestation of the scientific consensus on climate change is an organization called the Intergovernmental Panel on Climate Change. Headquartered in Geneva, under the aegis of the United Nations, it coordinates the volunteer efforts of several thousand scientists, industry experts, nonprofit researchers, and government representatives into reports issued every five to seven years. These reports underpin virtually every climate-based decision on Earth, from the US military’s threat assessments to the Paris climate agreement itself.
So it’s maybe surprising that the IPCC is a shoestring operation, running on just over $4.3 million a year. It gets that money from about 25 different countries, plus a few UN groups. Historically, the biggest chunk of that money comes from the US. Or rather, it used to.
Congress and the Trump administration effectively zeroed out America’s nearly $2 million contribution for 2017, and the 2018 budget explicitly bars the State Department from giving the IPCC money. Congress, remember, has the power of the purse. But the budget starts and ends with Trump: first as a proposal, and finally as a bill he signs into law. Removing the IPCC from the budget doesn’t necessarily put the organization in an immediate bind; it has savings. But it could leave US scientists out of many important scientific discussions—and leave the US underprepared as climate change progresses.
The UN established the IPCC as an independent research organization in 1988, because member nations were worried about the rising chorus of alarming climate science. They wanted a group to review the research and deliver actionable recommendations to the UN. The IPCC’s fifth assessment came out in 2014, and the sixth is due in 2022.
Every country that wants to participate can do so, and the IPCC’s executive committee selects delegates based on the needs of the working groups—not just scientists, but industry representatives, nonprofit experts, and other climate-interested professions. “The US people that work in these groups are generally selected by the DOE or by the EPA," says Daniel Kammen, a UC Berkeley energy physicist who has been working with the IPCC since 1999. "They get a letter saying you are requested, and the US will cover your travel with the understanding that all the work you do is volunteer.”
Those volunteers don’t conduct any new research. Rather, they review the existing literature in order to present a consensus on climate change, its impacts, and how the world can prepare for the worst of them. Numerous subgroups investigate the nuances in renewable energy, agriculture, sea level rise, and so on. Most of that work happens remotely. The only thing the IPCC pays for is flying the delegates to working group meetings once or twice a year—flights that eat up the bulk of the organization’s budget.
The working group meetups are the meat and potatoes of the IPCC. They decide the focus of the big reports. Which is why the US pulling out its funds could bite back. “The topics we are mostly concerned about, like climate change and drying soils, and the impacts of that on US farmers, will get less attention,” says Kammen. The US only accounts for 2 percent of the Earth’s surface, so it makes a big difference when US scientists are present to stand up for domestic interests.
Despite the IPCC’s relative low cost and undeniably outsized scientific importance, Republican lawmakers have been trying to zero out the US’s contribution for years. It's a familiar dance: Early every year, the State Department sends a budget request to the appropriations committees in both the Senate and House of Representatives. And every year, the respective committees write back, detailing their thoughts on which programs do and don't deserve money. For most of Barack Obama’s presidency, the notes from the House of Representatives have trashed environmental programs, including the payouts to the IPCC. But then the bill would make its way to the Senate, and some politician or aide would work these programs back into the budget. And because the amount was so small ($2 million is less than a percent of a percent of the total US budget), the quibble apparently wasn't worth expending political capital over.
Climate Change Is Killing Us Right Now
Tech Billionaires Team Up to Take On Climate Change
Climate Change Is So Bad That the US and China Agree on It
Until 2017. Last December, right before leaving office, Obama did in fact sign a funding resolution into law that included an IPCC payout. However, it wasn’t an actual budget, and only funded the government for a few months. In May of 2017, a new appropriations bill made it through both halves of Congress, and Trump signed it into law. The $2 million IPCC funding—along with nearly $12 million for the UN Framework Convention on Climate Change, which oversees the Paris agreement—was gone.
But here’s a huge caveat: The 2017 funding resolution's legalese had a loophole. “Although the Congressional Appropriations Act eliminates the line item, it doesn’t prohibit contributions to the IPCC or the UNFCCC,” says Maria Belenky, the director of policy and research at the climate policy research nonprofit Climate Advisors. This is unlike, say, the $3 billion Green Climate Fund, to which contributions were explicitly prohibited. “So, technically, the State Department could find a way to contribute if there's enough will to do so,” says Belenky. For instance, Secretary of State could pull money from the discretionary Economic Support Fund. The former CEO of Exxon Mobil could also reveal that he's a lifetime member of the Sierra Club. Possible, but not likely.
The bigger battle is still to come. The fight for the 2018 budget is coming in September. The House Appropriations Committee has already made explicit its goal to bar the State Department from using any funds, discretionary or otherwise, to pay out the IPCC. The White House is on board as well. Which is sort of ironic, given that Tillerson and Trump have both stated that the US should have a seat at the table in the UN’s ongoing climate discussions. They should know, the best way to make sure have a chance to play, is to pay.
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
In August 2015, scientists from the University of Notre Dame went west, the disassembled pieces of a particle accelerator secured in the back of their U-haul. Over 1,000 miles later and nearly a mile down, they started installing the machine in a new home: deep within an old mine in the town of Lead, South Dakota.
Miners first excavated the Homestake gold mine in the 1880s. But in 2006, the mining company donated the industrial site to the South Dakota Science and Technology Authority, and researchers repurposed its protective layers of rock to search for things like dark matter and neutrinos. Today, the Sanford Underground Research Facility looms over the old west town just as it did during the gold rush, cables spooling out of its tallest building and into a shaft that goes down 8,000 feet. And there, the scientists from Notre Dame planned to install their accelerator—called Caspar, the Compact Accelerator System for Performing Astrophysical Research.
It, too, is a repurposed relic. Since 1958, physicists have used Caspar's core in one form or another: to fuel larger accelerators, to gain insight into radiocarbon dating, to learn about how atoms grow larger. In its most recent incarnation, which will start taking data this fall, Caspar will mimic the fusion that goes on inside stars to learn how they make heavy elements—like the ones that people dug out of the Homestake mine, and that make up solar systems.
Sorry, Folks. The LHC Didn't Find a New Particle After All
Mini Particle Accelerators Make Cancer Treatment Safer For Everyone
Watch Your Back, CERN! Designer Builds Particle Accelerator in His Garage
That particle kind of physics is often the purview of Big Science—expensive endeavors like the Large Hadron Collider and the Stanford Linear Accelerator. Hundreds of humans compose one science team; budgets run into billions. But this little old accelerator and its little team, from Notre Dame and the South Dakota School of Mines and Technology, investigates the universe on a different scale. It leads world-class research into the roiling, collision-filled insides of stars while squeezing into a regular room in the belly of a mountain.
The machine that would become Caspar began nearly 60 years ago as a sort of helper accelerator. It sped up a beam of helium atoms toward another accelerator, which would speed them up even more. But in the '60s, researchers stopped needing that boost. So as Caspar lead Michael Wiescher wrote in a scientific biography of the instrument, the accelerator ”sat unloved and unused near the ion source.” The accelerator moved to the University of Toronto, where it helped people get information they need for radiocarbon dating. But then its caretaker moved on to newer, shinier accelerators, and once again, the machine “became superfluous,” wrote Wiescher.
Until Wiescher himself overhauled it, moved it to the University of Notre Dame in Indiana, and re-animated it with software.
There, he used Caspar to study a kind of reaction that happens inside stars, in which protons slam into alpha particles—two neutrons bound to two protons—and stay there, making more massive objects. And before long, Wiescher saw how to up the team's astrophysical game: Put their accelerator underground. Thousands of feet down, rock blocks the cosmic radiation that can swamp the small signals from the accelerator.
And so in 2015, piece by piece, vacuum pump by magnet by tube, the scientists loaded the accelerator equipment into the old-school elevator that still shifts up and down the mine shaft, lined with lumber. It took more than the 10 usual minutes to get down, the elevator slowed to a crawl to protect Caspar's delicate, antique belt and pulley as it descended from the ground floor to the “4850 Level”—4,850 feet underground, where dirt floors are studded with metal tracks and a light breeze blows.
They moved their equipment to the room (with more modern floors) where they would work and, soon enough, start to smash particles into particles. Specifically, the Caspar team wants to learn how stars a little older than the sun synthesize heavy elements. At its front end, Caspar bathes hydrogen or helium gas in radio-frequency energy, which makes the gas produce a beam of protons. That beam shoots into the vacuumed accelerating tube and streams toward the target at the end of the tube. Particles with the right amount of energy slam into the target—often a neon gas that releases the same neutrons that snowball small elements into bigger ones in star cores.
Since then, the team has been reassembling, commissioning, and calibrating. But this fall, Caspar will finally begin its real work. In June, Wiescher and a few colleagues and students from Notre Dame and the South Dakota School of Mines and Technology converged on SURF and talked over beers in the nearby town of Deadwood, famous for hangers-out like Calamity Jane and Wild Bill Hickock. They discussed the accelerator’s cowboy goal: to understand how stars combine atoms and atomic parts to make larger elements, which go on to form planets, people, and gold.
The morning after, in hard hats and protective eyewear, they gathered around computer screens and racks of electronics in the control room. Behind a closed door, Caspar sat bolted to lab benches, metal tubes leading the beam of particles first straight back, then around a 25-degree bend toward the target. When the particles collide with the target, the ensuing reactions mimic those guttural star reactions.
The knocked-around particles inside Caspar may only get up to a million volts—compared to the LHC’s 7 trillion—but that’s the point. Some of the universe's mysteries lie in the crosshairs of a low-energy, low-cost, little accelerator.
But the team isn’t quite there yet. “Right now we are really futzing around, because we have to learn,” said Wiescher. They'll start, for real, in the fall. He pointed to a wooden board between some controls, as the students kept tapping and staring away. The board is for knocking, commented astrophysicist Dan Robertson, on wood. You know. And although they likely believe more in humor than superstition, a team member nevertheless leaned over and banged his knuckles against it.
Inside a simulation of the universe's particle accelerator with WIRED Science writer Nick Stockton.
This story originally appeared on Grist and is part of the Climate Desk collaboration.
When people ask Luke Evslin why he decided to live off the grid, he starts with the time he almost died.
Evslin grew up on Kauai, a nub of a former volcano at the oldest end of the Hawaiian archipelago, but he was living on nearby Oahu at the time of the accident, working and competing in races with an outrigger canoe club.
The biggest race of the year is a daylong ocean crossing from the island of Moloka’i to Oahu’s Waikiki Beach, which can take between five and eight hours. Exhausted paddlers rotate out of the canoe during the race, jumping into the water to be scooped up by a waiting motorboat. During the first switch, Evslin was getting ready to heave himself into the canoe when the motorboat struck him.
The propellor sliced across his back in five places, severing muscle and bone along his spine and pelvis, each cut a potential death blow. His teammates pulled him out of the ocean and rushed him to shore. Judging from the looks on everyone’s faces, Evslin wasn’t sure he would survive the hour-long trip to land.
“I wasn’t scared to die,” he wrote a month later from his hospital bed, “but I was sad to die. I realized how much I love our beautiful world and everyone that is a part of it … and I was sad that I’d only just noticed.”
Soon after, still recovering from his wounds, “I made the terrible choice to read Walden Pond,” Evslin recalls. He came across these famous words from Henry David Thoreau: “I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I could not learn what it had to teach, and not, when I came to die, discover that I had not lived.”
Evslin began dreaming of a self-sufficient life, in touch with nature and free of the careless consumption of modern society. He convinced his then-fiancee, Sokchea, to move to a rainy acre on his native Kauai, where they built an off-grid yurt powered by six solar panels and a bank of batteries.
They planned to use only their own energy, eat what they grew, and eliminate their carbon footprint. Luke even planted a few coffee trees, imagining he would keep up his caffeine habit guilt-free.
“I had this grand plan of being an example for people,” he says, “showing how easy it was going to be.”
He had good reason to think that. Bathed in Pacific sunlight year-round, Kauai has all the hallmarks of a renewable energy paradise. Others thought so, too. In 2008, the member-owned electricity cooperative set an ambitious goal to run the entire island on 50 percent renewable energy by 2023.
At the time, Kauai had no utility-scale solar at all. But by the final day of 2015, the island’s main power plant—a rusty sugar plantation-era diesel generator—shut down for the first time since firing up the 1960s. For a few hours in the middle of the afternoon, two large solar farms did the heavy lifting on the island of 65,000, and the diesel plant sat dormant.
It was a good omen. By the end of 2016, the utility was on track to hit its 50 percent renewable goal five years ahead of schedule.
This February, the co-op board voted to move the goalposts again: 70 percent renewable energy by 2030. It will probably clear that mark early, too.
But, as Evslin quickly learned, the path to a low-carbon future can be tougher than it seems. Even in Hawaii, the sun doesn’t always shine—and when it does, sometimes you end up with more power than you can use in the moment.
How to collect that solar energy, predict it, get it to the right places at the right time, save it up for a rainy day—those are the kind of challenges our massive, spread-out, and unevenly populated country faces as we make the switch to clean energy. It’s one of the reasons that Tesla is making a major investment on Kauai, hoping to get it right.
And it all comes down to a lesson that the Evslins learned the hard way: It’s not about getting off the grid. It’s about building a better one.
“I imagine that there will be a lot more failures than successes to report,” Luke Evslin wrote in the first post of a blog he started to document his life off the grid, on January 1, 2011. “But that’s the point of it.”
Evslin didn’t know just how much he would come to reconsider what counts as failure and what constitutes success. On a visit with the family this summer, I walk the property with Luke as he points out trees he had planted. He’s tailed by a handsome dog named Asher and a mismatched set of terrier mixes, Peanut and Pico. A calico cat appears and settles on the railing with a view of the yard, where ducks and wild chickens peck hopefully.
“I’ve failed at most things I’ve grown,” Luke says with a shrug. Other than the fruit trees dotting the property—supplying all the banana, papaya, breadfruit, and lychee the Evslins could want—little else has taken root. His attempts at arugula and tomatoes fell prey to the chickens, and the ducks discovered a taste for sweet potato; other crops didn’t take to the damp.
“The only real success I’ve had is taro,” Luke says. An easygoing, water-loving crop that can be regrown from its own stem, taro makes up the bulk of the calories the Evslins get from the land. Their one-year-old daughter, Finley, subsists largely on homegrown poi. For Luke and Sokchea, the grocery store remains a necessity.
Then there’s the water. Their water tank, which collects rain from the hill above the yurt, also provides a welcoming home for mosquito larvae. The tank’s lining recently sprung a leak, so the family has been living on jugs of municipal water hauled from Luke’s sister’s house. At one time, Luke might have thought of this as a betrayal of principle; now it’s mostly just inconvenient.
But the biggest problem for Luke, like the utility that serves his island, has been the sun itself. He and Sokchea scaled back their lives to live within their solar-powered means—ditching their toaster and microwave, giving up laundry on cloudy days when their batteries wouldn’t be able to recharge. But they still have rainy weeks where they run out of power and have to run their gas-powered generator to keep the refrigerator from spoiling.
Most days, however, produced more solar power than they could use or successfully store in their batteries. If they were connected to the grid, Luke thought, that power could be used by his neighbors.
It took about a year for Luke to regret his move off the grid. “It’s not that it wasn’t what we expected,” he explains. “We wanted the difficulty of it.” But he also wanted to show people it was possible to live with a smaller carbon footprint; instead, he was burning gasoline and watching the island’s electric utility outpace him, installing solar power and cutting carbon all over the island.
“That was all happening, not because of me,” he remembers thinking, “but despite me and my efforts.”
Just after 10 am, the sun comes down hot on Kauai’s biggest solar field. Rows of darkly gleaming panels ripple toward a horizon of jungle-green mountaintops and whipped-cream clouds.
By high noon on the sunniest days, the Kauai Island Utility Cooperative generates 97 percent of its energy needs from a combination of three large solar fields, residential rooftop solar, biomass, and hydropower. Last year, 42 percent of the electricity used on island came from renewable sources.
In fact, Kauai is capable of generating so much energy from sunlight that any additional solar power the utility installs would likely go unused much of the time. Unlike the mainland United States, where a massive power grid connects far-flung regions, Kauai has nowhere to send the power it doesn’t use—and right now, it’s got about as much solar power in the middle of a day as it needs.
Yet even on the brightest day, the utility’s diesel-fired power plants start chugging back to full speed as the sun sets. It’s the solar version of feast or famine. And it’s why, despite all its advantages, Kauai is still a long way from complete clean-energy conversion.
That’s where the ranks of industrial, refrigerator–sized boxes lined up beside the solar field come in. Grouped together on neat concrete pads, only the occasional Tesla logo hints at what lies inside: batteries.
In March, Tesla cut the ribbon on this groundbreaking grid-scale battery installation, a key test of the viability of energy storage in making renewable energy a more reliable part of the grid. With 50,000 solar panels and 272 batteries, the combined solar-and-storage plant provides enough energy to power 4,500 homes for four hours.
If Tesla can help keep Kauai solar-powered around the clock with its batteries, then it can apply what it has learned elsewhere in the country, and around the world.
On this particular sunny day, Tesla engineers are doing some final tests before signing off on the plant. The site manager unlocks the front panel and swings the door open to reveal lithium-ion battery cells stacked like cafeteria trays.
Much of this hardware was borrowed directly from the electric cars that Elon Musk built his company on. (The coolant reservoir fastened to the door looks especially automotive.) Decades of research and development into smartphones and electric cars make lithium-ion batteries the most reliable and cheap battery on the market today.
“We designed the Tesla plant to be like a conventional power plant,” Brad Rockwell tells me. He is the head of power supply for Kauai’s utility cooperative, the one in charge of balancing supply and demand.
“I can say, ‘OK, give me 5 megawatts on the grid,’” Rockwell says. “And the plant looks around and says, ‘Am I getting any solar? What do you know, I’m getting 7 megawatts of solar—the grid only needs 5, so I’m going to give them a solid 5, and 2 will go to the battery.’”
He moves a pen across a sheet of paper to underline the shifting arithmetic. “Then when a cloud comes over and the [solar panels are] only putting out 2 megawatts, now I need 2 from the solar and 3 from the battery. And it just does that all day long.”
Rockwell is a former US Navy engineer, familiar with photovoltaic and battery systems because he studied them in the early 1990s. “It turns out that most remote islands are powered like ships are,” he says. Neither can rely on copious cheap fuel, and they can’t afford to waste what power they do have.
Most places, including Kauai, see two big, predictable peaks in energy use every day: one in the morning, when most people are waking up and getting ready for work, and a bigger one at night, when they return home. Both of those peaks occur outside the period when most solar power can be generated.
That’s why “there’s a finite limit” to how much solar power Kauai can consume right now, Rockwell says, showing me a graph of energy use over the course of a day. Between 10 a.m. and 4 p.m. on most days, Kauai nearly reaches its 100 percent renewable goal. Rockwell points out a gap of only a few megawatts between solar supply and the total electricity demand during the daytime hours, represented on the graph as a slim gray wiggle of conventional power under a heap of solar power.
“We’re already adding that much in rooftop solar every year. But,” he goes on, “if we can keep adding projects that don’t have to deliver here,” he taps the sunny yellow hill, “then we can start to erase this stuff,” he says, gesturing to the twin peaks of dark gray conventional power book-ending the day. “And that’s how we get to 100 percent renewable.”
Now that the Tesla battery plant is up and running, the utility will be able to cut 1.6 million gallons of fuel use per year. That power will come right off the top of the morning and evening peak demand. Because those peaks are also the most expensive times to generate power, Kauai’s customers should see a drop in their electric bills, too.
The co-op is already looking to its next solar-plus-storage installation, this one in partnership with the energy company AES. Announced in January, the AES plant will be about twice as big as the Tesla plant, and will supply 11 percent of the island’s annual electricity needs by the end of 2018.
By 2025—three years ahead of their latest goal—the utility expects to get 70 percent of its annual energy from renewables, much of it stored in those battery packs for use during the evening and morning peaks.
In June, Hawaii became the first state to formally adopt the Paris Climate Accord, in the wake of President Trump’s announcement that he planned to pull out. The mayor of Kauai, Bernard Carvalho, also threw his support behind the agreement.
“Although Kauai is a small island,” Carvalho said, “we believe it is our responsibility to take a leadership position on climate change mitigation. And we are strongly committed to staying on course to build a more sustainable and resilient future.”
But what will that future look like? It’s increasingly clear that it won’t be the off-the-grid Eden that folks like Evslin once imagined. Personal solar panels and other attempts to live the virtuous life look outdated in a place like Kauai, where the utility is committed to cutting carbon and costs at the same time.
The economies of scale are such that Kauai’s utility cooperative can install a solar-and-storage unit for about half what it would cost a family to install the same amount on a house. Even when it comes to the island’s fossil fuel–generated power, the utility can produce more from a gallon of gasoline than someone with a $100 generator in their basement.
Relying on personal power, Rockwell says, is no way to power a community, let alone an island.
This became obvious to Evslin midway through his yurt experiment: Inefficiency is the ultimate downfall of any individual effort to address climate change.
“Either you’re wasting electricity in a closed system, because it’s sunny and your batteries are full, or you don’t have enough power and you gotta run your generator,” Evslin says. “That’s not a bug in my system. That’s a feature of any off-grid system.”
These trends mean incentive programs set up to encourage homeowners to install solar panels are now out of whack. Hawaii’s public utility commission still requires Kauai’s utility to pay early solar adopters for power they generate, based on “avoided cost of fuel.” But these days, the power that’s being avoided doesn’t come from fossil fuels—it’s being provided by the island’s solar farms.
So although the utility is offsetting some panel owners’ bills for their (less efficient) solar power, the rest of the utilities’ costs (like batteries) are divided among members who don’t have access to rooftop solar power. These are the kinds of policy disincentives that Hawaii and the rest of the country will need to take into account as renewable power scales up.
As the island around them goes solar, Luke and Sokchea are looking at houses—they’ve tentatively picked one out—that would put them back on the grid, and back in a community they could feel a part of. If they lived in town, they could cut down on a huge chunk of their remaining energy use by walking or biking to work, or to run errands.
Still, they both admit they are reluctant to leave the yurt. Settling onto the couch with their dogs in the evening, Finley sleeping in a crib on the other side of their single large room, Luke and Sokchea weigh the pros and cons. They could shell out several thousand dollars to the utility company to hook them up to the grid out here, sure, but they’d still be left with many unanswered questions.
A Crucial Climate Mystery Hides Just Beneath Your Feet
Silicon Valley’s Mission to Save California Ag From Dying of Thirst
Inside the Race to Build the Battery of Tomorrow
What about the benefits of neighbors, a little lacking out here at the end of the road? What about walls, which might come in handy as Finley gets older? It’s still beautiful here, but it’s no longer the dream it was when they moved in.
The experience taught Luke a lot. He learned first-hand the challenges of solar power—how cheap it seems when he needs to run a fan in the middle of the day, how expensive when he’s rationing out the last watts in his batteries.
By retreating to his hideaway, Luke came to understand the power of civic participation. He’s pursuing a masters in public policy online, and it’s not hard to imagine him—wry, self-deprecating, easy to talk to—running for a seat in county government, or maybe even on the utility board.
“The solutions to all of this can’t be individual,” he says—and by “all of this,” it’s clear he’s thinking about the challenges facing society as a whole, not just Kauai, not just energy.
Walking me out past the taro patch, back across the swinging bridge that spans the creek surrounding his property, Luke points out one last thing. “It’s funny,” he says, “it was only recently I learned that Thoreau had his mom bring him food out in the woods.”
Reporting for this story was supported in part by Longreads and the Fund for Environmental Journalism of the Society of Environmental Journalists.
Take a look inside the first commercial-scale solar energy plant to use nothing more than the sun, molten salt, and a whole lot of mirrors to send power to the people. If the Crescent Dunes Solar Energy facility works as promised, it could be a model for the future of renewable energy.
Inside a red-bricked building on the north side of Washington DC, internist Shantanu Nundy rushes from one examining room to the next, trying to see all 30 patients on his schedule. Most days, five of them will need to follow up with some kind of specialist. And odds are they never will. Year-long waits, hundred-mile drives, and huge out of pocket costs mean 90 percent of America’s most needy citizens can’t follow through on a specialist referral from their primary care doc.
But Nundy’s patients are different. They have access to something most people don’t: a digital braintrust of more than 6,000 doctors, with expert insights neatly collected, curated, and delivered back to Nundy through an artificial intelligence platform. The online system, known as the Human Diagnosis Project, allows primary care doctors to plug into a collective medical superintelligence, helping them order tests or prescribe medications they’d otherwise have to outsource. Which means most of the time, Nundy’s patients wait days, not months, to get answers and get on with their lives.
In the not-too-distant future, that could be the standard of care for all 30 million people currently uninsured or on Medicaid. On Thursday, Human Dx announced a partnership with seven of the country’s top medical institutions to scale up the project, aiming to recruit 100,000 specialists—and their expert assessments—in the next five years. Their goal: close the specialty care gap for 3 million Americans by 2022.
In January, a single mom in her 30s came to see Nundy about pain and joint stiffness in her hands. It had gotten so bad that she had to stop working as a housekeeper, and she was growing desperate. When Nundy pulled up her chart, he realized she had seen another doctor at his clinic a few months prior who referred her to a specialist. But once the patient realized she’d have to pay a few hundred dollars out of pocket for the visit, she didn’t go. Instead, she tried get on a wait list at the public hospital, where she couldn’t navigate the paperwork—English wasn’t her first language.
Now, back where she started, Nundy examined the patient’s hands, which were angrily inflamed. He thought it was probably rheumatoid arthritis, but because the standard treatment can be pretty toxic, he was hesitant to prescribe drugs on his own. So he opened up the Human Dx portal and created a new case description: “35F with pain and joint stiffness in L/R hands x 6 months, suspected AR.” Then he uploaded a picture of her hands and sent out the query.
Within a few hours a few rheumatologists had weighed in, and by the next day they’d confirmed his diagnosis. They’d even suggested a few follow-up tests just to be sure and advice about a course of treatment. “I wouldn’t have had the expertise or confidence to be able to do that on my own,” he says.
Nundy joined Human Dx in 2015, after founder Jayanth Komarneni recruited him to pilot the platform’s core technologies. But the goal was always to go big. Komarneni likens the network to Wikipedia and Linux, but instead of contributors donating encyclopedia entries or code, they donate medical expertise. When a primary care doc gets a perplexing patient, they describe their background, medical history, and presenting symptoms—maybe adding an image of an X-ray, a photo of a rash, or an audio recording of lung sounds. Human Dx’s natural language processing algorithms will mine each case entry for keywords to funnel it to specialists who can create a list of likely diagnoses and recommend treatment.
Now, getting back 10 or 20 different doctors’ takes on a single patient is about as useful as having 20 friends respond individually via email to a potluck invitation. So Human Dx’s machine learning algorithms comb through all the responses to check them against all the project’s previously stored case reports. The network uses them to validate each specialist's finding, weight each one according to confidence level, and combine it with others into a single suggested diagnosis. And with every solved case, Human Dx gets a little bit smarter. “With other online tools if you help one patient you help one patient,” Komarneni says. “What’s different here is that the insights gained for one patient can help so many others. Instead of using AI to replace jobs or make things cheaper we’re using it to provide capacity where none exists.”
Komarneni estimates that those electronic consults can handle 35 to 40 percent of specialist visits, leaving more time for people who really need to get into the office. That’s based on other models implemented around the country at places such as San Francisco General Hospital, UCLA Health System, and Brigham and Women’s Hospital. SFGH’s eReferral system cut the average waiting time for an initial consult from 112 days to 49 within its first year.
That system, which is now the default for every SFGH specialty, relies on dedicated reviewers who get paid to respond to cases in a timely way. But Human Dx doesn’t have those financial incentives—its service is free. Today, though, by partnering with the American Board of Medical Specialities, Human Dx can now offer continuing education and improvement credits to satisfy at least some of the 200 hours doctors are required to complete every four years. And the American Medical Association, the nation’s largest physician group, has committed to getting its members to volunteer, as well as supporting program integrity by verifying physicians on the platform.
Veritas Genetics Scoops Up an AI Company to Sort Out Its DNA
Thanks to AI, Computers Can Now See Your Health Problems
The Chatbot Therapist Will See You Now
It’s a big deal to have the AMA on board. Physicians have historically been wary of attempts to supplant or complement their jobs with AI-enabled tools. But it’s important to not mistake the organization’s participation in the alliance for a formal pro-artificial intelligence stance. The AMA doesn’t yet have an official AI policy, and it doesn’t endorse any specific companies, products, or technologies, including Human Dx’s proprietary algorithms. The medical AI field is still young, with plenty of potential for unintended consequences.
Like discrepancies in quality of care. Alice Chen, the chief medical officer for the San Francisco Health Network and co-director of SFGH’s Center for Innovation in Access and Quality, worries that something like Human Dx might create a two-tiered medical system, where some people get to actually see specialists and some people just get a computerized composite of specialist opinions. “This is the edge of medicine right now,” Chen says. “You just have to find the sweet spot where you can leverage expertise and experience beyond traditional channels and at the same time ensure quality care.”
Researchers at Johns Hopkins, Harvard, and UCSF have been assessing the platform for accuracy and recently submitted results for peer review. The next big hurdle is money. The project is currently one of eight organizations in contention for a $100 million John D. and Catherine T. MacArthur Foundation grant. If Human Dx wins, they’ll spend the money to roll out nationwide. The alliance isn’t contingent on the $100 million award, but it would certainly be a nice way to kickstart the process—especially with specialty visits accounting for more than half of all trips to the doctor’s office.
So it’s possible that the next time you go in for something that stumps your regular physician, instead of seeing a specialist across town, you’ll see five or 10 from around the country. All it takes is a few minutes over lunch or in an elevator to put on a Sherlock Holmes hat, hop into the cloud, and sleuth through your case.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?
SoftBank, the Japanese telecommunications giant, has been on a multibillion-dollar acquisition and investment spree for the past year or so. It isn’t over. Today pharmaceutical company Roivant is announcing a $1.1 billion investment, mostly from the SoftBank Vision Fund. It's perhaps the largest single biotech investment ever. And after the biggest biotech IPOs of 2015 and 2016, this new money might also be another validation of Roivant’s development approach, grabbing up promising-but-abandoned drug candidates that other companies couldn’t get across the finish line.
A quick tally: SoftBank bought robot maker Boston Dynamics from Google parent company Alphabet. It bought chipmaker ARM for $32 billion. Through its $100-billion investment fund (with contributions from Apple and Oracle, among others) SoftBank put $4 billion into chipmaker Nvidia, $130 million into the gene-modifying biotech Zymergen, $1 billion into sports paraphernalia company Fanatics, and at least $300 million into coworking developer WeWork. It is a portfolio diverse to the point of weirdness.
And now this firehose of money is spraying onto a new world. Most of the $1.1 billion is coming from SoftBank; other than confirming the investment, a representative from SoftBank declined to identify who was fronting the remainder. A Roivant spokesperson said the bulk of the rest was coming from existing Roivant investors.
Just three years old, Roivant primarily works through a collection of subsidiaries developing different drugs, all originating from carefully negotiated raids of other pharma companies’ cupboards. Myovant is working on a drug for endometriosis and uterine fibroid pain. Urovant is going after incontinence and overactive bladder. Axovant is chasing dementia and Alzheimer’s. And so on—drugs rescued from abandonment and then put through fast-moving tests to get them commercialized.
Several of the drugs are in Phase III trials, which means they’re being tested on humans for efficacy, one of the final steps before government approval. Several more are in Phase II. “It’s the pipeline of a large pharmaceutical company at about a fiftieth of the burn that any of those companies would spend to get to that same point,” says Barry Selick, a longtime biotech executive now vice chancellor for business development, innovation, and partnerships at UC San Francisco. “It’s pretty remarkable.”
Roivant’s CEO, Vivek Ramaswamy, studied molecular biology as an undergraduate but then headed for the finance business, focusing on biotech. That’s where he started to think the process of turning interesting chemicals into drugs didn’t really work—it took decades and cost billions, and even after all that a drug might still fail out. Worse, the ways other industries have learned to use data to optimize their processes were still largely foreign.
Ramaswamy suspected he could cut through that by starting where other companies had decided to quit. Roivant goes looking for drugs stuck in turnaround—not because of problems with the science, but because of corporate changes-of-plan. “Once we take over those drugs, the same cultural attributes that allowed us to focus on specific drugs rather than general therapeutic categories allow us to focus on the process by which those drugs can be accelerated to the finish line,” Ramaswamy says.
The hard part, you’d think, would be knocking on some company’s door and saying, hi, can we have your stuff? But Ramaswamy says companies with drugs languishing on shelves are open to the idea. “They’re not spending any more money on it. They don’t have to dilute corporate focus. And they can participate in the upside with us in the form of royalties or other arrangements,” he says. “Many of them are motivated by the idea that the work they put in has the potential to help patients. It’d be a shame for it to go to waste.”
To find the drugs, Roivant has relied on the kind of intelligence-gathering you might expect—public databases and networks of relationships. But there’s even more data out there. Using a strategy they call “mapping the drugome,” Roivant’s business development team digs into that data. “You can start seeing patents. You can identify companies where they were running clinical trials in a therapeutic area at a pretty steady state, and now all of a sudden that’s gone down to five, then three. That’s a trend,” says Dan Rothman, Roivant’s chief information officer.
The company’s analysts further chart those drugs against existing categories of drugs, methods of action, and endpoints that others in the business are pursuing. Roivant also looks at whether a given drug might have a patient population that needs it. That’s commercial viability. Rothman says Roivant has mapped 30,000 potential drugs, 2,000 mechanisms of action, and 10,000 endpoints this way—all from publicly available, mostly free databases.
In fact, that’s where some of the SoftBank money will go: to purchasing more expensive, presumably more valuable data on who’s developing what.
Roivant will also spend some of the new money on spinning up another subsidiary, Datavant, designed to improve the overall process of running clinical trials through data analysis. See, companies run a lot of trials and collect a lot of data, but they don’t publish and share it all. So researchers sometimes unknowingly replicate ideas that have already failed. “If all data was pooled, you could cut the cost of clinical trials roughly in half, you could reduce time to market by 50 percent, and significantly increase the odds of success of any given clinical trial,” says Travis May, president of the connectivity division of Acxiom and a member of Datavant’s board of directors.
Big Pharma Buys Into Crowdsourcing for Drug Discovery
23andMe Teams With Big Pharma to Find Treatments Hidden in Our DNA
How Big Pharma Can Save Antibiotics From Superbugs
It seems a little crazy, perhaps—get companies to share their darknets with each other? But Datavant isn’t the only place trying to do it. The US Department of Energy, National Cancer Institute, and pharma giant GlaxoSmithKline are trying, too, as part of the Accelerating Therapeutics for Opportunities in Medicine project. “GSK is looking for other pharmaceutical companies to share their data,” Selick says. Lawrence Livermore National Lab has the supercomputers that can then, perhaps, build models that can say what molecules, antibodies, and so on might be worth testing—useful not just for GSK but for everyone. “And the companies that are nimble, they’ll turn around and apply those predictive models to their own internal pipeline and programs.”
It’ll take time and the right people to build those models, of course. At Roivant, that’s why this latest round of investment is so welcome: more ‘vants, more engineers, more data. For its part, SoftBank has been largely mum on what that company gets out of disruptively investing in yet another sector—“we are impressed with the ambition and track record of the Roivant team and look forward to supporting them in the next step of their journey,” SoftBank Group International managing director Akshay Naheta said in a press release.
Here's a clue, though: The company’s CEO, Masayoshi Son, sometimes says he's preparing for the arrival of digital superintelligence. Well, if the singularity speeds up the creation of new drugs people need, bring on the robot overlords.
The rise of ADHD diagnoses over the years has generated $13 billion in revenue for the companies that produce ADHD medication in the United States. Find out how the increase in attention deficit hyperactivity disorders is resulting in a big payday for big pharma.
Look down on Buenos Aires from the sky, and you can learn a fair bit about the city. It's got a lot of concrete. Also a lot of trees. There's a bright green river delta to the north, which probably explains the ruddy-brown bay to the east. But with the right camera—a hyperspectral one—you can pick up a whole lot more. New colors emerge, hidden hues your eyes and mine aren't wired to see. And these colors reflect even more detail about the scene: the gases coming out of the city, the health of the plants surrounding it, the species of algae coloring the water offshore.
Scientists love pointing hyperspectral cameras at the Earth to analyze things like crop health, or the mineral content of exposed soil. But there aren't many spectroscopic satellites in orbit: The US decommissioned one of the best, called Hyperion, earlier this year. So a private company called Satellogic wants to give scientists its data for free—the company plans to have 300 spectroscopic satellites in orbit by the early 2020s.
Hyperspectral imagery lets scientists see the world for what it is: molecules. Every rock, every crop, every building, and every one of you is made out of them, and every molecule reflects a different brand of photons. Pick up the signals from enough different kinds of light—Satellogic's orbiting imagers use 30 kinds, with wavelengths from 450 to 850 nanometers—and you can get a pretty good idea of a landscape's molecular composition.
Let's say you point your imager at field of corn. The light hitting the nitrogen, magnesium, carbon, hydrogen, and other molecules in their leaves can reveal the species, its health, and whether it's getting enough water. "This has many interesting use cases in agriculture," says Emiliano Kargieman, Satellogic's founder. "It can help monitor and identify crops, show levels of herbicide use, whether disease is spreading, water quality, and overall biomass."
That kind of data is super valuable for megafarmers. And hyperspectral data can even read the gas content in the air—important if you're a pipeline company keen to track possible gas leaks. Kargieman won't name any specific customers, but he says Satellogic serves ag and oil pretty heavily.
Obviously, that sort of information would also come in handy if you're a scientists trying to make sense of the Earth's capacity for sustaining life. But why would Satellogic give it away for free? "Well, for one, we see some trends in defunding Earth science research in the public sector, so we have a certain sense of responsibility to open up our data," says Kargieman. And then there's the fact that Satellogic's dataset is fairly new—they only have three birds in the air at the moment, with two more set to launch in September. The more people they have looking at their data, playing with it, and inventing new applications for it, the better it is for their overall business.
https://twitter.com/earlkman/status/893528111886721024
The competition in this area is heating up. Last year Planetary Resources announced $21.7 million in Series A funding to launch 10 hyperspectral imagers aboard the company's low Earth-orbiting Arkyd satellites.
Satellogic's operation has several advantages. First, its satellites are already in orbit. Second, the company has strength in numbers. The satellite constellation—collectively called Aleph, after a spacetime vortex in a Jorge Luis Borges story—will remap the same areas in shorter and shorter intervals, as more satellites join its fleet. "By this time next year we will have better than daily remaps of the planet with the hyperspectral camera," says Kargieman.
That's if the company can keep to its ambitious launch schedule. Next year it plans to launch 12 to 18 satellites, and another 60 in 2019. Each satellite has a nickname: The first two were Fresco and Batata, after a popular Argentinian dessert, and the third was Milanesat—another food reference. Numbers four and five are named after famous mathematicians Ada Lovelace and recently deceased Fields Medal laureate Maryam Mirzakhani.
But no matter how dense that constellation gets, it won't be enough for Earth scientists tracking some of the planet's most critical changes. "At their coverage range, they will be missing the spectral signatures of the absorption of snow and ice," says Robert Green, an Earth scientist at NASA's Jet Propulsion Laboratory. That's pretty important these days, as scientists are scrambling to figure out how polar feedback contributes to climate change. Melting ice and warming water both emit energy in the near infrared spectrum, which extends from about 700 to 2,500 nanometers.
Asteroid Mining Sounds Hard, Right? You Don’t Know the Half of It
The Journey of NASA’s Smartest Satellite Finally Comes to an End
An Algorithm That Decodes the Surface of the Earth
Green points to the late, great Hyperion as a model for space-based, science-oriented imaging spectrometer: a souped-up hyperspectral imager. Hyperion collected more than 200 spectral bands, in wavelengths ranging from 400 to 2,500 nanometers—that's all of the visible light spectrum, and a nice chunk of infrared. Hyperion was recently decommissioned after 17 years of service (it was only supposed to last 12 months). It will spend the next 40 or so years slowly spiraling into the upper atmosphere, where friction will set it aflame in a brief, but colorful, spectral display of its own.
To shift Hyperion's scientific responsibilities to a private company, scientists like Green will need to know the data is good. When it was operational, Hyperion would recalibrate once a month by shooting images of the moon—with no atmosphere, its reflectance values stay pretty much the same—and comparing those to pre-launch calibration data. "We found that the far blue, and some of the other bands were changing at a rate of over 5 percent over the life of the mission," says Betsy Middleton, a mission scientist at NASA's Goddard Space Flight Center. Most of the other bands varied a bit less, about 2 percent. But nonetheless, that's an important thing to know for anyone relying on the data.
For any scientists who are intrigued, Satellogic invites them to reach out via their website.
The GOES-16 atmospheric satellite is a super-powered machine that can predict the future. At least as far as weather is concerned.
For years, the total solar eclipse of August 21—the first to be visible in the mainland United States since 1979—has been in stealth mode. With the exception of the enthusiasts who’ve been snatching up all the hotel rooms along the eclipse’s 70-mile-wide, 2,800-mile-long path from Oregon to South Carolina, nobody really knew it was coming. But in the past few months, word of this spectacular natural phenomenon has spread like wildfire, and communities across the country that will be graced by the moon’s shadow are on high alert, anticipating millions of visitors.
But this ain’t no nerds-only field trip. In addition to astronomy enthusiasts, the eclipse will draw all kinds, from the curious to the bemused to the any-excuse-for-a-party crowds. There’ll be families, busloads of schoolkids, retirees, young thrill-seekers, and spontaneous road-trippers. If you’re just waking up now to the prospect of being a FOMO poster child, don’t panic. You can still make this happen. And we can tell you how.
If you do, your reward will be a legitimately otherworldly moment. “It’s absolutely unlike any other experience you’ve ever had,” says Fred Espenak, a retired NASA astrophysicist and the preeminent authority on eclipses and eclipse prediction. (Really: People call him Mr. Eclipse.) He describes it best: “It’s the closest thing to a surreal experience, with the sun vanishing from the midday sky. You’re suddenly plunged into an eerie twilight—you can see sunrise colors at the horizon, the black disc of the moon with the sun’s corona around it. It looks like a sci-fi movie, and it hits you quickly. Your hair stands on end and you get goosebumps on your arm, along with a feeling in the pit of your stomach telling you that something is dreadfully wrong. It just shouldn’t look like this. It never gets old.” C’mon, how can you turn down that pitch?
So, logistics. The fact is that most of the nation is within a half-day’s drive of the path of totality, and in spite of the hotel shortage you can camp, crash with friends or family, or just make your car your home for a night or two. The question, then, is where should you go? Make no mistake: Officials in local, state, and national government offices are preparing for massive influxes of people in certain popular observing regions, with many authorities even using the opportunity as a way to test mass-evacuation disaster scenarios. But while there is plenty of concern about traffic crushes or small towns running out of gas and food, much of this is worst-case-scenario speculation.
How to Get the Perfect Eclipse Shot Using Your Smartphone
NASA Unleashes Two Vintage Warplanes to Chase the Eclipse
Meet the Woman Trying to Prepare Your Town for the Total Eclipse
The reality is that nobody actually knows how many people will turn up. Both South Carolina and Oregon—the geographic bookends of the eclipse—are preparing for population surges of 1 million each, and even little towns like Idaho Falls, Idaho, and Jackson, Wyoming, have been told to expect hundreds of thousands of visitors. (The latter particularly so given its proximity to Grand Teton National Park, considered one of the most beautiful spots to view the eclipse.) Then you have Nashville, Tennessee, which is the only city that sits entirely inside the path of totality, outside of which observers will only see far less impressive partial eclipses, and St. Louis, which straddles the line between partial and total. Those cities will have built-in populations jockeying for the best observing areas—and moving out to find clear skies if clouds become a problem.
Michael Zeiler, a cartographer and eclipse chaser who’s created a variety of maps around the event, has analyzed likely traffic patterns to determine who will most likely be observing where. Using census and road network data, including average speeds and road capacities, he created a “driveshed” map to ballpark likely visitation patterns. “I estimated the chance of an average person making a drive of 200 miles to see the eclipse to be between 0.5 percent and 2 percent for the Denver population, for instance,” he says. “For locations further away, of course, the chance of driving to the path of totality diminishes. This is my basic formula. My result was that I expect between 1.85 million and 7.4 million people will make the trip.”
He notes that weather patterns will shift people from one location to another, certain eclipse festivals will be people magnets, and scenic locations will pull in larger numbers. The very worst spot on eclipse morning, he estimates, will be where I-95 meets the eclipse path, near Santee, South Carolina, because that is the closest destination for 75 million Americans, from Maine to Florida. Other potential hotspots: Salem and Madras, Oregon; Idaho Falls, Idaho; Jackson and Glendo, Wyoming; Grand Island, Nebraska; St. Joseph, Missouri; the area south of St. Louis; Carbondale, Illinois; Hopkinsville, Kentucky; the area north of Nashville, Tennessee; Great Smoky Mountains National Park, in North Carolina; and Columbia and Santee, South Carolina.
So one solid strategy is simply to head where these places aren’t. But even then, prepare for every possibility. Plan to be in place at least three hours before the eclipse, show up the day before if you can, have a backup destination in mind if weather or traffic deter you from your primary target. NOAA's eclipse weather site will start posting forecasts on August 15, but you should also check with local meteorologists along the path of totality for predictions—especially in Oregon, Idaho, and Wyoming, which are dealing with smoke from nearby wildfires.
Beyond that, fuel up your car at every opportunity, have food and supplies in your vehicle for at least two days, and assume that cell phone service will be overwhelmed and disrupted wherever you go. (Bring a paper map, or save an offline one in your phone!)
If you do end up squarely in the path of totality, and with clear skies to boot, buckle up for an amazing experience. Espenak advises first-timers to prep beforehand about what to expect (he created a guide) and, of course, to have eclipse glasses for the partial phases of the eclipse. (Go here for astronomer-vetted places to buy ones that aren't a scam that'll burn your eyes out.) Take photos like a pro, if you must.
During the two to three minutes of totality (longer for NASA!) you can take the glasses off. Use binoculars to see the event up close, or just savor the spectacle with your naked eye. “This isn’t just for science geeks,” Espenak says. “It’s the most spectacular astronomical phenomenon, and the closest any of us will get to going into space. You’ll be telling your children and grandchildren about it.”
This story originally appeared on The Guardian and is part of the Climate Desk collaboration.
Staff at the US Department of Agriculture have been told to avoid using the term climate change in their work, with the officials instructed to reference “weather extremes” instead.
A series of emails obtained by the Guardian between staff at the Natural Resources Conservation Service, a USDA unit that oversees farmers’ land conservation, show that the incoming Trump administration has had a stark impact on the language used by some federal employees around climate change.
A missive from Bianca Moebius-Clune, director of soil health, lists terms that should be avoided by staff and those that should replace them. “Climate change” is in the “avoid” category, to be replaced by “weather extremes.” Instead of “climate change adaption,” staff are asked to use “resilience to weather extremes.”
The primary cause of human-driven climate change is also targeted, with the term “reduce greenhouse gases” blacklisted in favor of “build soil organic matter, increase nutrient use efficiency.” Meanwhile, “sequester carbon” is ruled out and replaced by “build soil organic matter.”
In her email to staff, dated February 16 this year, Moebius-Clune said the new language was given to her staff and suggests it be passed on. She writes that “we won’t change the modeling, just how we talk about it—there are a lot of benefits to putting carbon back in the sail [sic], climate mitigation is just one of them,” and that a colleague from USDA’s public affairs team gave advice to “tamp down on discretionary messaging right now.”
In contrast to these newly contentious climate terms, Moebius-Clune wrote that references to economic growth, emerging business opportunities in the rural US, agro-tourism and “improved aesthetics” should be “tolerated if not appreciated by all.”
In a separate email to senior employees on January 24, just days after Trump’s inauguration, Jimmy Bramblett, deputy chief for programs at the NRCS, said: “It has become clear one of the previous administration’s priority is not consistent with that of the incoming administration. Namely, that priority is climate change. Please visit with your staff and make them aware of this shift in perspective within the executive branch.”
Bramblett added that “prudence” should be used when discussing greenhouse gases and said the agency’s work on air quality regarding these gases could be discontinued.
Other emails show the often agonized discussions between staff unsure of what is forbidden. On February 16, a staffer named Tim Hafner write to Bramblett: “I would like to know correct terms I should use instead of climate changes and anything to do with carbon ... I want to ensure to incorporate correct terminology that the agency has approved to use.”
On April 5, Suzanne Baker, a New York-based NRCS employee, emailed a query as to whether staff are “allowed to publish work from outside the USDA that use ‘climate change’.” A colleague advises that the issue be determined in a phone call.
Some staff weren’t enamored with the new regime, with one employee stating on an email on July 5 that “we would prefer to keep the language as is” and stressing the need to maintain the “scientific integrity of the work.”
In a statement, USDA said that on January 23 it had issued “interim operating procedures outlining procedures to ensure the new policy team has an opportunity to review policy-related statements, legislation, budgets and regulations prior to issuance.”
The statement added: “This guidance, similar to procedures issued by previous administrations, was misinterpreted by some to cover data and scientific publications. This was never the case and USDA interim procedures will allow complete, objective information for the new policy staff reviewing policy decisions.”
Trump has repeatedly questioned the veracity of climate change research, infamously suggesting that it is part of an elaborate Chinese hoax. The president has started the process of withdrawing the US from the Paris climate agreement, has instructed the Environmental Protection Agency (EPA) to scrap or amend various regulations aimed at cutting greenhouse gases, and has moved to open up more public land and waters to fossil fuel activity.
The nomenclature of the federal government has also shifted as these new priorities have taken hold. Mentions of the dangers of climate change have been removed from the websites of the White House and the Department of the Interior, while the EPA scrapped its entire online climate section in April pending a review that will be “updating language to reflect the approach of new leadership”.
“These records reveal Trump’s active censorship of science in the name of his political agenda,” said Meg Townsend, open government attorney at the Center for Biological Diversity.
“To think that federal agency staff who report about the air, water and soil that sustains the health of our nation must conform their reporting with the Trump administration’s anti-science rhetoric is appalling and dangerous for America and the greater global community.”
The Center for Biological Diversity is currently suing several government agencies, including the EPA and state department, to force them to release information on the “censoring” of climate change verbiage.
While some of the changes to government websites may have occurred anyway, the emails from within the USDA are the clearest indication yet that staff have been instructed to steer clear of acknowledging climate change or its myriad consequences.
US agriculture is a major source of heat-trapping gases, with 15 percent of the country’s emissions deriving from farming practices. A USDA plan to address the “far reaching” impacts of climate change is still online.
However, Sam Clovis, Trump’s nomination to be the USDA’s chief scientist, has labeled climate research “junk science.”
Last week it was revealed that Clovis, who is not a scientist, once ran a blog where he called progressives “race traders and race ‘traitors’” and likened Barack Obama to a “communist.”
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
Bruce Sherwood, the author of Matter and Interactions, had a question for me when I saw him at the American Association of Physics Teachers conference not long ago: "What calculator do you use?"
If this seems odd, well, it was a conference of physics teachers. I responded with something along the lines of "I don't actually use a calculator." Of course, Bruce probably knew I'd say that.  He absolutely agrees with me.
I don't remember the last time I use a traditional calculator. When students ask to borrow one, I show them this:
Yes, that is a classic HP 11C from the 1980s. It cost $135 back then—a fortune. It's still a great calculator, but I'm not about to let a student borrow it. Not because I'm mean, but because  older scientific calculators use RPN and I doubt many students know RPN. Now, you may ask, "Why don't you have a modern calculator, Rhett?" I have many reasons, not the least of which is I never use a calculator. I'd bet most scientists don't use a calculator.
Neither should you.
So how do I calculate stuff? A slide rule? No. Although slide rules are cool, most scientists don't use those, either. For simple things like finding a square root or converting, say, Fahrenheit to Celcius, I use a web browser. You can just type stuff right into the Google search box. Try it. Type "sqrt(4.55) meters in feet." Not only does Google determine the answer, it converts it from meters to feet (although I have no idea why you would want to do that).
For longer calculations, I use the programming language Python, usually a web-based version like you find at trinket.io. Let me show you how with a fairly typical introductory physics problem.
*Suppose you launch a ball off a table 1.3 meters above the floor. If the ball launches at a 35 degree angle above the horizontal with a velocity of 3.3 m/s, how far from the base of the table will it hit the floor?
We won't work through a full solution, only a short version. Because the ball has only gravitational force acting on it after it leaves the launcher, it will exhibit constant acceleration in the y-direction and constant velocity in the x-direction. You can treat y-motion and the x-motion independently except for the time it takes to travel. This results in two kinematic equations:
To find the final x-position (labeled x2 in the equation), I must first solve for the time from the y-motion equation. Unfortunately, this means using the quadratic equation but, oh well, that's how it goes. We'll switch to Python and finish the problem. Just click the play button to run the program and get the answer.
As you can see, I get two values for the final position. Clearly the positive value is the one I want.
So why is python better than a calculator? A few reasons.
First, I can save this for future reference. If I want to refer back to it, boom, there it is.
Second, the quadratic formula kind of sucks. But in this case, I can assign values to "a,b,c" and  write out the quadratic formula as I usually see it.
Third, I can print the intermediate steps to make sure I'm on the right track. In this case, I printed "t" before using it to make sure it wasn't some super high crazy value.
Fourth, Python (with VPython) features built in vectors. It can add vectors, find the magnitude, cross products, scalar products. That's awesome.
Finally, if I decide to launch the ball with a speed of 3.8 m/s instead of 3.3 m/s, I don't have to redo all the values. Just change one line and get the new answer.
Notice that in this case, I used Python as a calculator. This isn't what I call a numerical calculation. For those, I use Python to break a problem into smaller steps (which is still cool, but just different). Also, in this example I used Python in trinket.io because it shows the output window alongside the code window and that works very well for a calculator. Of course, you could use glowscript.org or even offline python (using Jupyter notebooks or something).
Now, if scientists often shun calculators, should students use them? No. I believe that students should always use the best method of doing something, and in this case, the best method is a tool like Python. I find the graphing calculators that most students use underpowered and overpriced.
If students ditch calculators, how are they supposed to take tests? Simple: They can use their phones to run Python on glowscript.org or trinket.io.  In fact, I am making that the rule in my class this semester. Now, I concede that students might use their phones to Google an answer or text a classmate for help, but I'm not too worried about it.
There's no reason to continue using clunky old calculators when much better tools exist. Ditch that calculator. You'll be better off without it.
At the Seeing Eye guide dog facility in Morristown, New Jersey, puppies spend the first three weeks of their life in plastic kiddie pools—the blue kind, speckled with colorful undersea cartoons. Lined with fleece and towels, the wading pools are perfectly sized for a litter of soft, tiny puppies and their mama to lounge and nurse. And some of those floppy-eared, tiny-tailed puppies will eventually grow into responsible, diligent working dogs.
But to don that Seeing Eye guide dog vest, a dog needs to be exceptional. Guide dogs need to be healthy, unfazed by jackhammers, and smart enough to lead a human across the street only when the coast is clear. Breeders can select for health and to a certain extent temperament, but the combination of all three can be difficult to nail. And if a dog isn’t quite right for the job, the charities that breed and train these dogs would like to know as soon as possible, before they've spent $65,000 on training. So researchers are looking for ways to predict which young dogs will make the cut—by watching hours and hours of guide dog puppy bowl.
Dogs are typically tested for health and temperament when they’re a little over a year old. At that point, any sign of joint or vision trouble will get them pulled out of training. But if they’ve got the wrong personality they can also be let go. “They might just be too timid, for example, because the dog needs to have a good deal of confidence to be suited for the work,” says Dolores Holle, director of canine medicine and surgery at The Seeing Eye.
Those personality traits are hard to identify until a dog is a bit older. In their first few weeks of life, balls of puppy fluff with barely open eyes don’t have temperaments yet—but they do have moms. And in a paper published today in the Proceedings of the National Academies of Science, researchers suggest the way mothers interact with their pups could be an early indication of success. In their 98-puppy study, pups who got the canine version of helicopter parenting were more likely to be released from the training program.
When Emily Bray first started looking at ways to tell these well-pedigreed moms and pups apart, she wasn’t sure if the differences would be clear enough to make any helpful predictions. Seeing Eye pups are bred from top performing dogs—German shepherds, golden retrievers, and labradors—selected for impulse control, obedience, and health. She was a bit worried that such selective breeding might produce canine Stepford Wives, with similar mothering styles, and puppies with uniform, robotic responses to toys, treats, and obstacles.
So Bray tracked 21 litters of puppies and recorded how they interacted with their moms in their first three weeks. She and a team of undergraduate researchers watched an incredible 115 hours of puppy video. Although moms were generally consistent in their own behavior, different litters got different parenting. Some moms spent more time licking and snuggling their pups, while others sometimes faced away from their babies or left the kiddie pool altogether for some alone time. And depending on if a mom laid down to nurse or stood up, some puppies had to work a bit harder to eat.
It’s worth noting that all the moms at the guide dog breeding facility are pretty good—only dogs with the calmest personalities and best behavior become breeders. But the litters whose moms babied them the most, measured by the amount of time they spent in the nest and ease of nursing, were least likely to be placed as successful guides nearly two years later.
They also tested those same dogs’ temperaments and problem-solving abilities about a year and a half later, before they had started intensive guide dog training. To do that, they presented puppies with strange new situations—creepy robotic cats or spontaneously opening umbrellas—or had them puzzle their way through different obstacles. Faster problem solvers and dogs who were less anxious in the face of new objects became guides in greater numbers.
Why Do Dogs Love Yoga Mats So Much?
If You're Happy and You Know It — So Does Your Dog
Meet the Poop-Sniffing Dogs That Are Saving Endangered Wildlife
That lines up with the way that guide dogs are trained now, Holle says. The program of training dogs includes exposing them to new experiences when they’re only a month old so they can learn to be unfazed. “There’s a CD that plays all kinds of sounds from the teapot boiling to ambulances and gunshots and thunderstorms, and that plays in the background while the puppies are having happy experiences. So the sounds aren't likely to stun them,” Holle says.
The dogs that don’t make the cut are hardly failures. They’re bred to have fewer health problems and are rarely aggressive or difficult. They’re just a little less serious: They bark more easily at new objects, and they have a harder time staying focused when a hallway is littered with toys. But those are qualities that can be great in a pet dog. “If they’re identified earlier they can be re-homed earlier, so they can actually go and live their life as the pet they were meant to be,” says Naomi Harvey, a dog behavior researcher at the University of Nottingham.
Linking maternal behavior with these pups' guide dog graduation rate doesn't mean breeders can select dogs based on this information alone quite yet. Most immediately, looking at mothering behaviors could help facilities choose which moms are likely to have career-leaning puppies. But since litters get their mom's parenting style as well as her genetics, it's hard to untangle the two factors. “An animal’s temperament is so multifactorial that you need to know an awful lot about its life, its pedigree, its genetics, its experience, in order to be able to judge what it will be like in the future,” Harvey says.
So for now, The Seeing Eye will continue to pick its trainees based on behavior tests at the 15-month mark. And the researchers will keep their eyes glued to the puppy cam.
Google is getting serious about self-driving cars. So serious that it put a legally blind man in one that drove him around safely on his own. The successful trip means that the tech giant can now launch its own self-driving car company, which it's calling Waymo.
Have you ever witnessed a total solar eclipse? Usually when I give a lecture, only a couple of people in an audience of several hundred people raise their hands when I ask that question. A few others respond tentatively, saying, “I think I saw one.” That’s like a woman saying, “I think I once gave birth.”
What these people are remembering is some long-ago partial solar eclipse. These are quite common. They occur every few years in various places across the globe. But believe me, if you’ve seen a total solar eclipse—when the moon passes directly between the sun and the earth—you’ll never forget it.
Part of what makes a total eclipse so breathtaking has to do with invisible light. During the “moment of totality”—the minutes when sun is completely blocked—observers experience the exquisitely odd and wondrous sensation of solar emissions, both visible and invisible, vanishing right in the middle of the day.
You have a chance to experience this firsthand. The United States has reached the end of the longest total-solar-eclipse drought in its history. A total solar eclipse—or totality—has not been observed from anywhere in the mainland United States since February 26, 1979. This bizarre thirty-eight-year hiatus ends on August 21, 2017, when a coast-to-coast totality sweeps across the continent, ramping up an eclipse fever that is already highly publicized.
For those who do not live in or travel to the narrow, ribbon- like path of totality—the area from which the sun will appear to be in total eclipse, which stretches from the Pacific Northwest to the Carolina coast but is just 150 miles wide—a second totality will unfold on April 8, 2024. Two in a mere seven-year period.
Bob Berman is one of America’s top astronomy writers, and the author of Zoom: How Everything Moves and The Sun’s Heartbeat. He is a columnist for Astronomy and the science editor of The Old Farmer’s Almanac.
Then, as if to compensate for the scarcity of these events (even the 1979 eclipse was a mostly cloudy, far-northern event only observable in a few places such as Helena, Montana), the middle and late parts of the twenty-first century will offer a second sudden flurry of them.
In any given place on earth, a totality appears just once every 375 years. If it’s cloudy, you have to wait another 375 years. So a totality is a very rare event for any location. But that interval of time is just the average. Here and there, a few places will enjoy two totalities in a single decade: Carbondale, Illinois1, for example, sits at the intersection of both eclipse tracks—2017’s and 2024’s. Yet residents of other cities, including Los Angeles, must cool their heels for more than a millennium.
In the United States, no major urban center has seen a total solar eclipse since the dual events of Southern California in 1923 and the now-famous New York City totality of 1924. Boston was scheduled for a sunrise totality in October of 1925, but it was cloudy.
Every eclipse path—a map of the places on earth from which the sun is completely blocked and where stars are seen during the day—is long and narrow. It’s usually around 150 miles wide, but its length extends for thousands of miles. During that Roaring Twenties Big Apple eclipse, for example, the totality ran from central Canada southeast to Albany, in upstate New York, then down through the Bronx and Harlem, and ended unceremoniously at 86th Street in Manhattan, near an eatery that would someday be famous for hot dogs and papaya drinks. People south of the subway stop there stood in daylight: no stars out, no mind-numbing glimpse of the solar corona, no hot-pink flares shooting from the sun’s edge. Volunteers were dispatched to each street so scientists could later know the precise location of the edge of the moon’s shadow. The next day, a newspaper writer, watching the disappearing sun’s final dazzling pinpoint, described it as a diamond ring—a term that has since been fully incorporated into eclipse-speak.
The event has an indescribable effect on observers. While most experienced astronomers would concede that a total solar eclipse is the most powerful, gorgeous, and even life-altering of all celestial phenomena, they’d rate a vivid display of the northern lights as not too shabby, either. A big gap separates those two from the rest of what I call the top four natural spectacles, including a rare brilliant comet and a meteor storm, in which more than a dozen shooting stars flash across the sky each minute. Like the aurora borealis, a solar totality often invokes involuntary gasps and cries of wonder. You’ll often hear that some kind of “feeling” accompanies the visual spectacle. Perhaps this has to do with the fact that both these events are indeed accompanied by large changes in the amount of incoming electromagnetic radiation. It should also be noted that lunar eclipses, even total ones, do not make this top-four list. Those fairly commonplace eclipses, which unfold every few years and are never limited to a narrow section of our planet but instead are visible to half the world, are certainly pretty and worth watching. But they are not life-altering.
In just another few hundred million years, total solar eclipses will be over forever.
During a solar totality, animals usually fall silent. People howl and weep. Flames of nuclear fire visibly erupt like geysers from the sun’s edge. Shimmering dark lines cover the ground. In both the 2017 and the 2024 events, the entirety of the United States and Canada will experience a partial eclipse, so that anyone using protective eyewear will be able to see it by standing outside or by looking out a window (provided that it’s not cloudy, of course). In contrast, less than 1 percent of the continent will experience totality. To most people, it might seem that seeing a partial eclipse ought to be almost as good as seeing a total eclipse, and it’s certainly a lot more convenient. Why travel? The sun being 99.9 percent eclipsed doesn’t sound too different from its being 100 percent eclipsed, right?
Actually, seeing an almost total eclipse is no better than almost falling in love or almost visiting the Grand Canyon. Only full totality produces the astonishing and absolutely singular phenomenon that resembles nothing else in our lives, on our planet, or in the known universe.
No discussion of totality should omit the strange science lurking behind it. It starts with a bizarre coincidence: the moon is four hundred times smaller than the sun, but it also floats four hundred times nearer to us. This makes the two disks in our sky appear to be the same size. Now, if the moon appeared larger than the sun, it could still occasionally stand in front of it, but it would also blot out the dramatic prominences along the sun’s edge, those geysers of pink nuclear flame. So for maximum amazingness, these bodies must have identical angular diameters—i.e., they must appear to be the same size. And they do.
The moon wasn’t always where it is now, which makes the coincidence even more special. The moon has really just arrived at the “sweet spot.” It’s been departing from us ever since its creation four billion years ago, after we were whacked by a Mars-size body that sent white-hot debris arcing into the sky. Spiraling away at the rate of one and a half inches per year, the moon is only now at the correct distance from our planet to make total solar eclipses possible. In just another few hundred  million years, total solar eclipses will be over forever.
For early cultures that regarded celestial phenomena as magical to begin with, eclipses occupied a spot entirely off the weirdness scale. Some, such as the Aztecs and the Babylonians, were obsessive enough to make astoundingly accurate observations that ultimately gave their priests the power to predict astronomical events.
The ancient Babylonians noticed that although some sort of eclipse happens every year, the exact same type of eclipse returns after precisely eighteen years and eleven and one-third days. The accuracy of this observation remains very impressive, especially because that one-third-of-a-day business means that the next eclipse can be best seen (or maybe only seen) in an entirely different region of the world. Babylonians called this eighteen-plus-year period a Saros. The ancient Greeks loved that word and concept so much that they embraced it without even translating the word into their own language.
The Saros’s third-of-a-day feature means that the earth turns through 120 degrees of longitude before the next eclipse in that particular Saros takes place. Therefore, for an eclipse with specific properties (such as total versus partial, long versus short, and tropical versus arctic) to make a repeat appearance in any particular region, one has to wait while eclipses work their way around the world like a set of gears, which requires three Saroses—a length of time equal to fifty-four years and around one month, or, more precisely, thirty-three days. Because this surpasses human life expectancy in that era four thousand years ago, it’s astonishing that the cycle was noticed at all. This three-Saros interval is called the exeligmos, which is Greek for “turning of the wheel.” Using the exeligmos, we can calculate that there must have been a total solar eclipse in the United States fifty-four years and one month before the 2017 event and fifty-four years and one month  before the 2024 event. Sure enough, a total eclipse in Maine unfolded in 1963, and another one amazed onlookers when it raced up the East Coast and covered Virginia Beach and Nantucket on March 7, 1970.
That three-and-a-half-minute March 1970 totality over Virginia Beach belongs to a series of Saroses given the number 139. This series consists of total (not partial) eclipses with paths that always move northeastward. In 1988, this Saros presented its next event a third of the world west of Virginia—a three-and- three-quarter-minute totality over Indonesia. Yet another Saros later, in March of 2006, the same northeastward totality swept from Libya to Turkey. Saros 139’s next return, another third of the world west, will show residents of Cleveland, Rochester, Buffalo, and Burlington, Vermont, a totality in 2024.
So now our stage is set for the next eclipses over North America. After the two-and-a-half-minute coast-to-coast 2017 spectacle, the eclipse on April 8, 2024, will appear longest over central Mexico, at well over four minutes; then the moon’s shadow will move northeastward like a tornado to the north-eastern United States.
After 2017, a solar totality will happen once, somewhere in the world, during most years. None will occur in 2018, but we’ll get a sunset totality over central Chile and Argentina on July 2, 2019, then another in those same countries on December 14, 2020.
Ignoring a strictly Antarctic totality in 2021 and the eclipse-less year 2022 takes us to a marginal one-minute event in steamy equatorial Indonesia in 2023. But then things pick up, convenience-wise.
The 2024 US totality will be followed by the totality of the longest duration between 2017 and the end of the century—six and a half minutes—which will occur in Egypt and Gibraltar on August 2, 2027. That decade will be rounded out by a wonderful five-minute Australian totality on July 22, 2028.
If you want to limit your eclipse tourism to the United States, Canada, and Europe, note that the United States will see its longest-ever solar eclipse on August 12, 2045, a six-minute totality running from Northern California to Florida. Florida gets another eclipse just seven years later, on March 30, 2052. Then the United States will enjoy two within a twelve-month span, on May 11, 2078, and May 1, 2079, while France and Italy will experience their only totality of the century on September 3, 2081.
I have had the good fortune to see eight totalities; please allow me to share the experience. The fully eclipsed sun is always a breathtaking surprise.
First off, no one is really prepared for a total eclipse. Pictures one may have seen don’t do the event justice, because cameras never capture its true visual appearance. The reason has to do with the difference between human retinal sensitivity and the vagaries of a camera’s exposure, whether using digital imaging or film. The inner corona is bright; the outer corona faint and delicate. The correct exposure for one part of the eclipsed sun either underexposes the other so that it’s invisible or overexposes it so that it looks like a huge burned-out area ringed by wide white flares. So a real eclipse does not resemble the ones you see on nature documentaries or in magazines, even when the images are taken by professionals. To get an accurate image, you would have to Photoshop multiple images together.
The magic really starts around ten minutes before totality, when the sun is still partially blocked but almost gone. You need eye protection at this point; I prefer welding goggles fitted with shade 12 filters if the sun is low and shade 14 if the sun is high. These display a clearer, higher-quality image than cheap plastic eclipse glasses do. (Get the goggles from a welding supply store, which is absolutely never located in the mall but rather in the worst part of town, usually adjacent to a fenced-in yard protected by snarling dogs.)
Solar ultra-violet energy drops to zero. So does infrared radiation, whose absence starts to be felt long before totality arrives.
At this stage the sun resembles a crescent moon, but the best thing to do is look at the surrounding countryside. Colors are saturated; shadows are stark; contrast is boosted; the shadows of trees and bushes contain innumerable strange crescent shapes. Ordinary objects such as trees and houses seem unfamiliar, as if illuminated by a star other than the sun. Everyday scenery has been transformed into something extraordinary.
Expectation fills the air. Then a minute or two before totality, shimmering dark lines suddenly wiggle over all white surfaces, such as sand or a sheet spread on the ground. These are called shadow bands, and they can’t be photographed! If you try, your video or still images will show the white substance or object without any wavy bands at all. The rather anticlimactic reason for this is simply that shadow bands have extremely low contrast. Because they shimmer, the eye readily picks them out. But they lie below the contrast required to show up in a photographic image.
Meet the Woman Trying to Prepare Your Town for the Total Eclipse
NASA Unleashes Two Vintage Warplanes to Chase the Eclipse
Guess How Many People Can Actually Watch the Total Eclipse
Then comes totality, which can last anywhere between one second and around seven minutes. Now you take off your welding goggles and look at the sun directly. The bright stars come out. The sun’s corona leaps across the sky, much farther than you expected. Its delicate wispy structure, following the sun’s  normally invisible magnetic-field lines, depends on the part of the solar cycle you’re in. At a glance you’ll know if you’re at sunspot minimum or maximum: during the latter period the corona is round and symmetrical, as if the sun’s springs have been wound up tightly and all the power held in place is ready to pop. But a quiet sun, paradoxically, lets go with long, irregular coronal streamers. Whenever it’s seen, the glow is obviously that of a light different from anything nature normally offers. There is a logical reason for this, too: the sun’s corona is by far the hottest thing the human eye can observe. It’s made of plasma—broken fragments of atoms—rather than the whole atoms that comprise the solar surface and everything else around us on earth.
It’s an experience that does not seem of this life or this world. “The home of my soul” is how one eclipse watcher described it to me. But why? What has really happened? It’s obviously not sim- ply a matter of the sun’s visible light being blocked. Its invisible rays are extinguished, too. (As Victor Hess discovered during a 1912 near-total eclipse, when he went up in a balloon to measure the sun’s radiation, cosmic rays do not decrease when the sun is blocked. But many other energies do indeed vanish.) Solar ultra-violet energy drops to zero. So does infrared radiation, whose absence starts to be felt long before totality arrives. With the drop in infrared energy, clouds, rocks, and the air just above the ground are suddenly cooled. This chill creates a pressure difference that manifests itself as a haunting eclipse wind. Moreover, the decreasing temperature as the sun is steadily blocked can shrink the gap between the temperature and the dew point, allowing clouds to suddenly form. That’s what happened during the Siberian eclipse of the 1980s, with exasperating consequences, as the large international party of professional astronomers who had gathered to observe the event saw nothing when thick clouds materialized. They had meticulously planned for what the sun’s  visible rays would do—but they’d neglected its invisible rays!
When the eclipse is over, observers immediately start thinking about how they can get to the next one. So don’t even think of being anywhere else but in the narrow ribbon of totality on August 21, 2017, and on April 8, 2024. Be sure to factor in likely cloud cover. For example, eastern Idaho is a safer bet than the Pacific Northwest for the 2017 event, whereas in 2024, you’d be better off staying in the dry parts of southern Texas than in the Buffalo, New York, area. Also know that, in most places, midmornings tend to be clearer than midafternoons. When an eclipse offers a long track, as the one in 2017 does, one could choose a late morning event in Idaho or a midafternoon totality in Nashville; the odds somewhat favor the former.
I know someone who went to seven total eclipses but was clouded out of four of them. There are even several people who, on August 11, 1999, inexplicably chose to view the eclipse from Cornwall, England (overcast and drizzling), instead of from Turkey (crystal clear). This is a case in which considerations of convenience—or, perhaps, having friends or relatives in a particular location—can steer us wrong.
Excerpted from Zapped: From Infrared to X-rays, the Curious History of Invisible Light. Copyright © 2017 by Bob Berman. Used with permission of Little, Brown and Company, New York.  All rights reserved.
1Correction appended 8:26 ET 08/07/17: This story has been updated to reflect the correct state.
On August 21, darkness will wash over America. But in a good way, we promise. Here's everything you need to know if you want to catch a glimpse of the solar eclipse.
The symptoms of post-traumatic stress disorder include uncontrolled memories of a traumatic event, anxiety and panic—“hyperarousal” is the technical term—depression, avoiding anything that’s a reminder of the event, self-destructive behavior, and more. It’s the only psychiatric disorder where people are pretty sure of the cause: emotionally traumatic events, from the death of a loved one to an experience of fear or violence. By some estimates 5 to 10 percent of all US adults have PTSD, more women than men. Wars in Afghanistan, Iraq, and other places US troops are deployed put those numbers even higher among people in the military and veterans.
But the biology of PTSD—neurological changes, elevated or depressed levels of something a blood test could pick up, genetic vulnerabilities—is … multifactorial. Difficult to correlate. Under investigation.
Which is to say, nobody’s really sure. So this week, a long-in-the-works research study called Aurora is starting to recruit human subjects. Working with 19 hospitals and collaborators1 around the country, Aurora will ask 5,000 people to become part of the study. The goal is to try to figure out what biomarkers connect a traumatic event to the development and eventual diagnosis of PTSD.
The researchers at the University of North Carolina and Harvard who are running the study have turned to Verily, the health-focused Google spinoff, for help with data collection and management. That means Aurora will test more than how and why people get PTSD. It’ll also test how the methods used to ask scientific questions invariably constrain the answers.
Aurora’s design is, without doubt, slick. When eligible people come to one of Aurora’s 19 partners,1 caregivers will try to sign them up. People who agree will, within hours or days of the inciting event, get a baseline biologic assessment,2 and they’ll return for more tests every month. They’ll also get a Verily Study Watch, a wearable that captures data like heart rate, skin electrical conductivity, and movement.
If all goes as planned, participants will also get an experimental app on their smartphones from the health startup Mindstrong. By monitoring things like keystroke speed and pressure, speed of word choice while texting, and even simple time on-screen—invisibly, in the background—the Mindstrong algorithms are supposed to be able to pick up early signs and symptoms of psychiatric disorders. It’s the “digital phenotype” of an illness.
Right now, no one knows which people who experience trauma are going to develop PTSD, much less how to prevent it or what the underlying biology is. “So we’ll get people right after the traumatic event, and then see if six months later they develop one of these outcomes, and then we’ll see what’s changed,” says Sam McLean, director of the Institute for Trauma Recovery at UNC-Chapel Hill and Aurora’s principal investigator. “Developing better treatments depends on getting a much deeper, more accurate understanding of what’s going on.”
A “biomarker” used to be something that only a blood test or imaging like an MRI would find—a physical manifestation of an illness or injury. Aurora expands that definition. “Historically in studies you’d just get snippets of data. Come in for an assessment, fill out forms, take some physiological data. This is getting data in real time,” McLean says. That means they can look for patterns. Does a person panic every time they leave the house? In that case, “the biomarker is some combination of GPS and, from the watch, seeing your heart rate spike.”
With 5,000 people spewing 24/7 data, you can see why partnering with Verily makes sense. Aurora got $21 million from the National Institutes of Health to spin up; that’s not a lot for a big, multicenter, prospective study. So they can take advantage of Verily’s Google-spawned data-handling heritage, “keeping track of the data and ensuring it’s intact, secure, and well-organized,” says Menachem Fromer, Verily’s mental health science lead, and using Verily’s skill with analytics and machine learning. Aurora also gets to take advantage of Mindstrong’s clever data collection and interpretation algorithms.
Meanwhile Verily and Mindstrong both get to play with the Aurora database to validate and improve their own work. (Verily also has a separate effort, Project Baseline, that will use its watch to monitor 10,000 otherwise healthy people.) “Validation, while it seems straightforward, is actually a really hairy problem,” says Tom Insel, a neuroscientist, ex-director of the National Institute of Mental Health, and currently a partner at Mindstrong. “We’ve got a novel, objective measure of mood and cognition. So how do we prove that?”
In fact, that’s exactly what Insel tried to codify when he was head of NIMH, where he championed finding those more objective measures. As Insel himself acknowledged, it didn’t quite work—and the attempt polarized the mental health community. Insel left NIMH in 2015 for what was then called Google Life Sciences ... now known as Verily. Then in May he left Verily for Mindstrong.
Both companies hope to help people, and also sell products—not that there’s anything wrong with that. But explicitly looking for PTSD biomarkers that might provide therapeutic hooks could also pose a forest/trees problem, because mental health in general doesn’t have clearly-defined objective measures of its disorders. “That has made the hunt for biomarkers particularly difficult,” says epidemiologist Sandro Galea, dean of the Boston University School of Public Health and chair of a committee that produced an Institute of Medicine report on PTSD in 2014, “You can have PTSD in many different ways,” Galea says, “because the diagnostic criteria are essentially a mix of different symptoms and categories.”
Big data approaches like what Verily and Mindstrong bring to Aurora have a chance to unify the belts, as it were, collecting huge amounts of information and then sorting through it to try to quantify the qualitative standards. It could be a powerful approach, says Galea, and may well add to knowledge about and the potential to treat PTSD, “but it also introduces a distorting force. Not the data itself—data is value-neutral. But the data applications have what the companies hope are clear commercial approaches.”
In other words, if you go looking for biomarkers, you are actually looking for therapeutic targets, and while those targets might be hittable by a drug, they might also oversimplify the actual disorder. (An update: After this story's publication, McLean emailed me to emphasize that the study's primary goal is to understand the overall pathogenesis of PTSD, not simply to find biomarkers. That's an important distinction.) It’s a step away from holistic theories that try to encompass socioeconomic or public health-style approaches—because, fundamentally, it’s easier to sell a drug.
Google's Verily Is Spinning Off 'Verb,' a Secretive Robot-Surgery Startup
Star Neuroscientist Tom Insel Leaves the Google-Spawned Verily for ... a Startup?
That Google Spinoff’s Scary, Important, Invasive, Deep New Health Study
Verily’s people understand this potential pitfall, of course. “The best information, to a person living in a social environment that is not that healthy, may not be that useful,” former FDA commissioner Robert Califf, now at Verily, said at a recent biotech conference—not talking about PTSD specifically, but health in general. “Social determinants far outweigh the biological determinants.”
Aurora’s investigators know that working with big-data companies means they have to strike a balance, too. And it's worth it. "They have been and are being terrific," McLean says. "Personally, as PI, if some other entity maybe wouldn't do direct patient assessment but had another way to analyze data or to bring discoveries faster, then hey, I'm totally game."  Right now, Aurora, Verily, and Mindstrong have aligned but not equivalent reasons for wanting to bridge the cause-to-symptom gap for PTSD. For people with the disorder, maybe those reasons don’t matter, as long as they get results.
1 UPDATE 8/10/17 12:00 pm Changed to reflect the correct number of hospitals and other collaborators
2 Removed an incorrect reference to a psychiatric assessment, which isn't part of intake
Load up your devices with these mediation, mindfulness, and relaxation apps. Hopefully, they’ll help you find some inner peace. Namaste!
Genes carry the information that make you you. So it's fitting that, when sequenced and stored in a computer, your genome takes up gobs of memory—up to 150 gigabytes. Multiply that across all the people who have gotten sequenced, and you're looking at some serious storage issues. If that's not enough, mining those genomes for useful insight means comparing them all to each other, to medical histories, and to the millions of scientific papers about genetics.
Sorting all that out is a perfect task for artificial intelligence. And plenty of AI startups have bent their efforts in that direction. On August 3, sequencing company Veritas Genetics bought one of the most influential: seven-year old Curoverse. Veritas thinks AI will help interpret the genetic risk of certain diseases and scour the ever-growing databases of genomic, medical, and scientific research. In a step forward, the company also hopes to use things like natural language processing and deep learning to help customers query their genetic data on demand.
It's not totally surprising that Veritas bought up Curoverse. Both companies spun out of George Church's prolific Harvard lab. Several years ago, Church started something called the Personal Genomics Project, with the goal of sequencing 100,000 human genomes—and linking each one to participants' health information. Veritas' founders helped lead the sequencing part—starting as a prenatal testing service and launching a $1,000 full genome product in 2015—while Curoverse worked on academic strategies to store and sort through all the data.
But more broadly, genomics and AI practically call out for one another. As a raw data format, a single person's genome takes up about 150 gigabytes. How!?! OK so, yes, storing a single base pair only takes up around two bits. Multiply that by roughly 3 billion—the total number of base pairs in your 23 chromosome pairs—and you wind up with around 750 megabytes. But genetic sequencing isn't perfect. Mirza Cifric, Veritas Genetics’ cofounder and CEO, says his company reads each part of the genome at least 30 times in order to make sure their results are statistically significant. "And you gotta keep all that data, so you can refer back to it over time," says Cifric.
That's just storage. "Everything after that is going to specific areas and asking questions: There’s a variant at this location, a substitution of this base, a deletion here, or multiple copies of this same gene here, here, and here," says Cifric. Now, interpret all that. Oh, and do it across a thousand, hundred thousand, or million genomes. Querying all those genetic variations is how scientists get leads to find new drugs, or figure out how existing drugs work differently on different people.
But cross-referencing all those genomes is just the beginning. Curoverse, which was focusing on projects to store and sort genomic data, also has its work cut out for it in searching through the 6 million—and counting—jargon-filled academic papers detailing gene behavior, including visual information found in charts, graphs, and illustrations.
That's pretty ambitious. Natural language processing is one of the stickiest problems in AI. "Look, I am a computer scientist, I love AI and machine learning, and no amount of coding makes sense to solve this," says Atul Butte, the director of UCSF's Institute of Computational Health Sciences. At his former job at Stanford University, Butte actually tried to do the same thing—use AI to dig through genetics research. He says in the end, it was way cheaper to hire people to read the papers and input the findings into his database manually.
Artificial Intelligence Could Dig Up Cures Buried Online
Artificial Intelligence Is Learning to Predict and Prevent Suicide
AI Could Target Autism Before It Even Emerges—But It's No Cure-All
But hey, never say never, right? However they accomplish it, Veritas wants to move past what companies like 23andMe and Color offer: genetic risk based on single-variant diseases. Some of America's biggest dangers come from diseases like diabetes and heart disease, which are activated by interactions between multiple genes—in addition to environmental factors like diet and exercise. With AI, Cifric believes Veritas will be able to not only dig up these various genetic contributors, but also assign each a statistical score showing how much it contributes to the overall risk.
Again, Butte hates to be a spoilsport, but ... there's all sorts of problems with doing predictive diagnostics with genetic data. He points to a 2013 study that used polygenic testing to predict heart disease using the Framingham Heart Study data—about as good as you can get, when it comes to health data and heart disease. "They authors showed that yes, given polygenic risk score, and blood levels, and lipid levels, and family history, you can predict within 10 years if someone will develop heart disease," says Butte. "But doctors could do the same thing without using the genome!"
He says the problems come down to just how messy it is trying to square up all the different research on each gene alongside the environmental risks, and all the other compounding factors that come up when you try to peer into the future. "It’s been the holy grail for a long time, structured genome reporting," says Butte. Even attempts to get researchers to write and report data in a standard, machine-readable way, have fallen flat. "You get into questions that never go away. One researcher defines autism different from another one, or high blood pressure, or any number of things," he says.
Butte isn't a total naysayer. He says partnerships like the one between Veritas and Curoverse are becoming more common—like the data processing deal between genetic sequencing giant Illumina and IBM Watson—because there's a clear need for new computing methods in this area. "You want to get to a point where you are developing stuff that improves clinical care," he says.
Or how about directly to the owners of the genomes? Cifric hopes the merger will improve the consumer experience of using genetic data, even seamlessly integrating it into daily life. For instance, linking your genome and health records to your digital assistant. Alexa, should I eat this last piece of pizza? Maybe you should skip it, depending on your baseline genetic risk for cholesterol and latest blood test results. Diet isn't the only area where genomics could help improve your day to day life. Some people are more or less sensitive to over the counter drugs. A quick query might tell you whether you should take a little less Tylenol than is recommended.
Cifric thinks this acquisition could position Veritas as a global powerhouse of genomic data. "Apple recently announced that they had shipped 41 million iPhones in a quarter, right? I think in not too distant future, we’ll be doing 41 million genomes in a quarter," he says. That might seem ambitious, given that the cost to consumers is nearly $1,000. But that cost is bound to come down. And artificial intelligence will make paying for the genome a matter of common sense.
This story has been updated to reflect that the company is named Veritas Genetics, not Veritas Genomics.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?
Take an onion. Slice it very thin. Thinner than paper thin: single-cell thin. Then dip a slice in a succession of chemical baths cooked up to stain DNA. The dyed strands should appear in radiant magenta—­the fingerprints of life’s instructions as vivid as rose petals on a marital bed. Now you can count how much DNA there is in each cell. It’s simply a matter of volume and density. A computer can flash the answer in seconds: 17 picograms. That’s about 16 billion base pairs—the molecular links of a DNA chain.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Maybe that number doesn’t mean much to you. Or maybe you’re scratching your head, recalling that your own hereditary blueprint weighs in at only 3 billion base pairs. “Huh?” joked Ilia Leitch, an evolutionary biologist at the Royal Botanic Gardens, Kew, in England. Her reaction mimicked the befuddlement of countless anthropocentric minds who have puzzled over this discrepancy since scientists began comparing species’ genomes more than 70 years ago. “Why would an onion have five times more DNA than we have? Are they five times more clever?”
Of course, it wasn’t just the onion that upended assumptions about a link between an organism’s complexity and the heft of its genetic code. In the first broad survey of animal genome sizes, published in 1951, Arthur Mirsky and Hans Ris—pioneers in molecular biology and electron microscopy, respectively—reported with disbelief that the snakelike salamander Amphiuma contains 70 times as much DNA as a chicken, “a far more highly developed animal.” The decades that followed brought more surprises: flying birds with smaller genomes than grasshoppers; primitive lungfish with bigger genomes than mammals; flowering plants with 50 times less DNA than humans, and flowering plants with 50 times more; single-celled protozoans with some of the largest known genomes of all.
Even setting aside the genetic miniatures of viruses, cellular genome sizes measured to date vary more than a millionfold. Think pebbles versus Mount Everest. “It’s just crazy,” Leitch said. “Why would that be?”
By the 1980s, biologists had a partial answer: Most DNA does not consist of genes—those functional lines of code that translate into the molecules carrying out the business of a cell. “Large genomes have vast amounts of noncoding DNA,” Leitch said. “That’s what’s driving the difference.”
But although this explanation solved the paradox of the clever onion, it wasn’t particularly satisfying. “It just opened up more cans of worms,” said Ryan Gregory, a biologist at the University of Guelph who runs the online Animal Genome Size Database. Why, for instance, do some genomes contain very little noncoding DNA—also, controversially, often called “junk DNA”—while others hoard it? Does all this clutter—or lack of it—serve a purpose?
This past February, a tantalizing clue arose from research led by Aurélie Kapusta while she was a postdoctoral fellow working with Cedric Feschotte, a geneticist then at the University of Utah, along with Alexander Suh, an evolutionary biologist at Uppsala University in Sweden. The study, one of the first of its kind, compared genome sequences across diverse lineages of mammals and birds. It showed that as species evolved, they gained and shed astonishing amounts of DNA, although the average size of their genomes stayed relatively constant. “We see the genome is very dynamic, very elastic,” said Feschotte, who is now at Cornell University.
To explain this tremendous DNA turnover, Feschotte proposes an “accordion model” of evolution, whereby genomes expand and contract, forever gathering up new base pairs and dumping old ones. These molecular gymnastics represent more than a curiosity. They hint at hidden forces shaping the genome—and the organisms that genomes beget.
The first signs that inheritance involves the transmission of more than just genes emerged around the time that Mirsky and Ris were marveling at the enormousness of the salamander genome. In the 1940s, a Swedish geneticist named Gunnar Östergren became fascinated with odd hereditary structures found in some plants. Östergren wrote that the structures, known as B chromosomes, appeared to have “no useful function at all to the species carrying them.” He concluded that these extraneous sequences were “genetic parasites”—hijackers of the “host” genome’s reproductive machinery. Three decades later, the evolutionary biologist Richard Dawkins solidified this idea in his popular 1976 book The Selfish Gene; the theory was quickly adapted to explain genome size.
By then, scientists had learned that B chromosomes are only a tiny fraction of the molecular parasites making genomes fat. The most prolific freeloaders are mobile strings of DNA called transposons, identified in 1944 by Barbara McClintock, the groundbreaking cytogeneticist who was honored with a Nobel Prize for that discovery. Transposons are popularly known as “jumping genes,” although they are rarely in fact true genes. They can get passed down from one generation to the next or transmitted between species, like viruses, and they come in several flavors. Some encode enzymes that snip a transposon out of its place in a genome and paste it elsewhere. Others copy themselves by manufacturing RNA templates or stealing enzymes from other transposons. (“You can get parasites within parasites,” Gregory said.)
It’s not hard to see how these copies could quickly multiply, eventually taking over large portions of a genome. (More than 100 can pop up in a single generation of flies; they make up 85 percent of the maize genome and almost half of our own.) Proponents of the “selfish DNA” theory saw this pileup as the driving force of genome evolution: Within the ecosystem of a cell’s nucleus, natural selection would favor fast-multiplying transposons. But only up to a point. Once a genome reached a certain size, its bulk would start to interfere with an organism’s well-being—for example, by slowing the division of cells and thus the rate of the organism’s growth. Selection would kick in again, preventing further expansion. The limit would depend on the organism’s biology.
New evidence soon complicated this picture. In the late 1990s, Dmitri Petrov, then a doctoral student at Harvard University, began tracking small mutations in insects—random genetic changes of up to a few hundred base pairs that resulted from DNA damage, copying mistakes and poor strand repair. He started with flies. Analyzing defunct transposons, he showed that old code was being scrapped more quickly than new lines were being written (because random mutations are more likely to delete existing base pairs than to insert new ones). He wondered if this “deletion bias” might explain the fly’s relatively compact genome. He repeated the experiment in crickets and grasshoppers, whose genomes are, respectively, 10 and 100 times as large as the fly’s. This time, the deletion rates, although still dominant, were indeed considerably slower. Were some genomes bulkier than others simply because they weren’t as quick to clear out debris?
Based on these and similar observations, Petrov laid out a new model of genome size. Transposons, he argued, would always accumulate, sometimes very quickly. (Maize, for example, has doubled its genome in only 3 million years.) But over eons, small excisions would slowly chip away at a genome’s bulk. Eventually, the pace of expunction would match the pace of creation, and the genome would settle into equilibrium. Any number of forces in the chaotic nucleus might set—or reset—this balance.
Not everyone was convinced. Gregory, for one, maintained that spontaneous change happened too slowly to account for the dramatic morphing of genome size in many lineages. But no one could deny that loss was a powerful transformative force. As Gregory wrote in The Evolution of the Genome, “there are more complex interactions between [transposons] and their hosts than strict parasitism.” The tricky part was finding them.
For Feschotte, the tip-off came from a bat. By the early 2000s, following advances in DNA sequencing, labs had begun decoding whole genomes and sharing the data online. At the time, Feschotte’s group was not particularly interested in the evolutionary dynamics of genome size, but they were extremely curious about what transposons could reveal about the history of life. So when the genome of the common little brown bat (Myotis lucifugus), the first genome sequence from a bat, appeared in 2006, Feschotte was thrilled. Bats have strikingly small genomes for a mammal—they’re more like those of birds—and it seemed likely they would hold surprises.
Parsing the creature’s 2 billion base pairs, Feschotte and his colleagues did stumble on something strange. “We found some very weird transposons,” he said. Because these oddball parasite sequences didn’t appear in other mammals, they were likely to have invaded after bats diverged from other lineages, perhaps picked up from an insect snack some 30 to 40 million years ago. What’s more, they were incredibly active. “Probably 20 percent or more of the bat’s genome is derived from this fairly recent wave of transposons,” Feschotte said. “It raised a paradox because when we see an explosion of transposon activity, we’d predict an increase in size.” Instead, the bat genome had shrunk. “So we were puzzled.”
There was only one likely explanation: Bats must have jettisoned a lot of DNA. When Kapusta joined Feschotte’s lab in 2011, her first project was to find out how much. By comparing transposons in bats and nine other mammals, she could see which pieces many lineages shared. These, she determined, must have come from a common ancestor. “It’s really like looking at fossils,” she said. Researchers had previously assembled a rough reconstruction of the ancient mammalian genome as it might have existed 100 million years ago. At 2.8 billion base pairs, it was nearly human-size.
Next, Kapusta calculated how much ancestral DNA each lineage had lost and how much new material it had gained. As she and Feschotte suspected, the bat lineages had churned through base pairs, dumping more than 1 billion while accruing only another few hundred million. Yet it was the other mammals that made their jaws drop.
Mammals are not especially diverse when it comes to genome size. In many animal groups, such as insects and amphibians, genomes vary more than a hundredfold. By contrast, the largest genome in mammals (in the red viscacha rat) is only five times as big as the smallest (in the bent-wing bat). Many researchers took this to mean that mammalian genomes just don’t have much going on. As Susumu Ohno, the noted geneticist and expert in molecular evolution, put it in 1969: “In this respect, evolution of mammals is not very interesting.”
But Kapusta’s data revealed that mammalian genomes are far from monotonous, having reaped and purged vast quantities of DNA. Take the mouse. Its genome is roughly the same size it was 100 million years ago. And yet very little of the original remains. “This was a big surprise: In the end, only one-third of the mouse genome is the same,” said Kapusta, who is now a research associate in human genetics at the University of Utah and at the USTAR Center for Genetic Discovery. Applying the same analysis to 24 bird species, whose genomes are even less varied than those of mammals, she showed that they too have a lively genetic history.
“No one predicted this,” said J. Spencer Johnston, a professor of entomology at Texas A&M University. “Even those genomes that didn’t change size over a huge period of time—they didn’t just sit there. Somehow they decided what size they wanted to be, and despite mobile elements trying to bloat them, they didn’t bloat. So then the next obvious question is: Why the heck not?”
Feschotte’s best guess points at transposons themselves. “They provide a very natural mechanism by which gain provides the template to facilitate loss,” he said. Here’s how: As transposons multiply, they create long strings of nearly identical code. Parts of the genome become like a book that repeats the same few words. If you rip out a page, you might glue it back in the wrong place because everything looks pretty much the same. You might even decide the book reads just fine as is and toss the page in the trash. This happens with DNA too. When it’s broken and rejoined, as routinely happens when DNA is damaged but also during the recombination of genes in sexual reproduction, large numbers of transposons make it easy for strands to misalign, and that slippage can result in deletions. “The whole array can collapse at once,” Feschotte said.
This hypothesis hasn’t been tested in animals, but there is evidence from other organisms. “It’s not so different from what we’re seeing in plants with small genomes,” Leitch said. “DNA in these species is often dominated by just one or two types of transposons that amplify and then get eliminated. The turnover is very dynamic: in 3 to 5 million years, half of any new repeats will be gone.”
That’s not the case for larger genomes. “What we see in big plant genomes—and also in salamanders and lungfish—is a much more heterogeneous set of repeats, none of which are present in [large numbers],” Leitch said. She thinks these genomes must have replaced the ability to knock out transposons with a novel and effective way of silencing them. “What they do is, they stick labels onto the DNA that signal to it to become very tightly condensed—sort of squished—so it can’t be read easily.” That alteration stops the repeats from copying themselves, but it also breaks the mechanism for eliminating them. So over time, Leitch explained, “any new repeats get stuck and then slowly diverge through normal mutation to produce a genome full of ancient degenerative repeats.”
Meanwhile, other forces may be at play. Large genomes, for instance, can be costly. “They’re energetically expensive, like running a big house,” Leitch said. They also take up more space, which requires a bigger nucleus, which requires a bigger cell, which can slow processes like metabolism and growth. It’s possible that in some populations, under some conditions, natural selection may constrain genome size. For example, female bow-winged grasshoppers, for mysterious reasons, prefer the songs of males with small genomes. Maize plants growing at higher latitudes likewise self-select for smaller genomes, seemingly so they can generate seed before winter sets in.
Some experts speculate that a similar process is going on in birds and bats, which may need small genomes to maintain the high metabolisms needed for flight. But proof is lacking. Did small genomes really give birds an advantage in taking to the skies? Or had the genomes of birds’ flightless dinosaur ancestors already begun to contract for some other reason, and did the physiological demands of flight then shrink the genomes of modern birds even more? “We can’t say what’s cause and effect,” Suh said.
Genetic Architects Untwist DNA’s Turns With Crispr
Biologists Are Figuring Out How Cells Tell Left From Right
The Hunt for the Algorithms That Drive Life on Earth
It’s also possible that genome size is largely a result of chance. “My feeling is there’s one underlying mechanism that drives all this variability,” said Mike Lynch, a biologist at Indiana University. “And that’s random genetic drift.” It’s a principle of population genetics that drift—whereby a genetic variant becomes more or less common just by sheer luck—is stronger in small groups, where there’s less variation. So when populations decline, such as when new species diverge, the odds increase that lineages will drift toward larger genomes, even if organisms become slightly less fit. As populations grow, selection is more likely to quash this trait, causing genomes to slim.
None of these models, however, fully explain the great diversity of genome forms. “The way I think of it, you’ve got a bunch of different forces on different levels pushing in different directions,” Gregory said. Untangling them will require new kinds of experiments, which may soon be within reach. “We’re just at the cusp of being able to write genomes,” said Chris Organ, an evolutionary biologist at Montana State University. “We’ll be able to actually manipulate genome size in the lab and study its effects.” Those results may help to disentangle the features of genomes that are purely products of chance from those with functional significance.
Many experts would also like to see more analyses like Kapusta’s. (“Let’s do the same thing in insects!” Johnston said.) As more genomes come online, researchers can begin to compare larger numbers of lineages. “Four to five years from now, every mammal will be sequenced,” Lynch said, “and we’ll be able to see what’s happening on a finer scale.” Do genomes undergo rapid expansion followed by prolonged contraction as populations spread, as Lynch suspects? Or do changes happen smoothly, untouched by population dynamics, as Petrov’s and Feschotte’s models predict and recent work in flies supports?
Or perhaps genomes are unpredictable in the same way life is unpredictable—with exceptions to every rule. “Biological systems are like Rube Goldberg machines,” said Jeff Bennetzen, a plant geneticist at the University of Georgia. “If something works, it will be done, but it can be done in the most absurd, complicated, multistep way. This creates novelty. It also creates the potential for that novelty to change in a million different ways.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
There are thousands of strains of weed. Cracking their genetic codes may be the key to transforming pot from a budding business to a high-flying industry and a cannabis analytics lab is trying to unlock the true potential of weed. Pictures by Preston Gannaway.
Since causing major outbreaks in Saudi Arabia in 2014 and in Korea a year later, the virus that causes Middle East Respiratory Syndrome is laying low. But it hasn't disappeared: A cluster of 34 cases cropped up in July in Riyadh, Saudi Arabia’s capital. The dromedary camels who harbored the virus for more than 20 years aren’t going anywhere, and neither is MERS.
But for many researchers, now—between major outbreaks—is the perfect time to develop treatments and vaccines for a virus that kills a third of the people it infects. The initial stigma of MERS, much like SARS, has made it difficult to study the survivors whose blood carries valuable information about virus-fighting strategies. But in a downturn, more survivors have agreed to donate blood samples—21 of whom contributed to a paper published on Friday in Science Immunology.
That might not seem like a lot, but only 2,040 MERS cases have ever been diagnosed and reported, in part because the starting symptoms—coughing, fever, trouble breathing—are so general. “So obtaining the specimens that we did amounted to something like 2 to 3 percent of the total [known] survivors of MERS,” says Stanley Perlman, a microbiologist at the University of Iowa and a coauthor of the study.
When a virus barges into your body, your immune system can dish out two broad attacks, led by white blood cells. B cells pump out antibodies to neutralize and tag invaders, and T cells identify and kill cells housing infectious viruses. Individual immune systems wield the weapons in very different ways. Take antibodies. They’re important, but they’re actually not the only way to fight an infection—some don’t hang around to protect you, and the weakest antibodies are just duds that don’t do anything at all. But other antibodies hang around for years, keeping a historical record and guarding against reexposure to an infection.
In MERS patients, the first surprise was that virus survivors who made it out of the hospital fastest didn’t stand out for their antibody activity. Instead, they were mostly identifiable by their stronger killer T cell responses. That’s interesting, because it means it’s possible that lots of patients—those with fewer symptoms and minuscule antibody counts that don't show up in blood serum tests—might have flown under the radar. More people may have had MERS than we thought. “But then if you’re not able to detect those patients of those cases using serology, then you kind of have an inflated case fatality rate,” says Vincent Munster, chief of the Virus Ecology Unit at NIH’s Rocky Mountain Labs. “Which actually makes this virus look worse than it really is.”
It also means that MERS is super tricky to contain. Because the symptoms are so general, the virus often spreads to health care workers and other patients before doctors diagnose the carrier. According to the risk assessment the World Health Organization put out this month, 30 percent of MERS cases recently have resulted from this kind of human-to-human contact in hospitals. “It really points out that these health care settings are at risk for these kinds of viruses, and in that sense the western world is not any different,” Munster says. Containing the virus within hospitals will mean getting an accurate count of who has really seen the virus—from T cells as well as antibodies—and working to protect those exposed.
MERS Isn't an Epidemic. That Makes It Harder to Find a Cure
How Ebola Can Teach Us to Prepare for the Next Great Pandemic
Toppling Cranes Aren't the Only Things Threatening Mecca
That leaves health care workers at exceptional risk—and a vaccine would be the best way to protect them. “MERS was just identified in 2012, so that’s not a lot of time to develop a new vaccine,” says Natalie Thornburg, a virologist at the Centers for Disease Control and Prevention. But they’re working on it. Most efforts target the protein the MERS virus uses to sneak into cells—it’s called spike. At NIH’s Rocky Mountain Labs, Munster and his team are exploring recombinant chimpanzee virus vectors to train the immune system against spike. And another company, Inovio, is running clinical trials for a vaccine using spike’s DNA sequence. For any vaccine to succeed at delivering long-term immunity, they’ll need to provoke T cells, and not just antibodies. And those T cells will likely be the best indication of protection.
But vaccines won’t help someone who’s already sick. In this study, MERS survivors who had the most severe cases also developed the most antibodies for recovery—maybe because they couldn’t initially clear the virus with killer T cells. Giving patients extra intravenous antibodies to bolster their fight against an infection is a short-term strategy that’s sometimes used in dire cases, like Ebola. So by testing the antibody amounts required to reduce viral loads in MERS-infected mice, Perlman and his team could estimate how much antibody serum a person would need to fight MERS. It's a short-term strategy, but it's something.
The issue of cost and profitability for MERS therapeutics is a separate issue, but this work matters for more than one infection. While SARS and MERS have made headlines for jumping from the animal population into humans recently, other coronaviruses have quietly infected other animals—such as pigs, bats, and cats—for much longer. And structural similarities within this virus family mean that dismantling one virus could help with the next. Hopefully the immunology research will outpace the viruses.
Each year the World Health Organization determines which of the thousands of influenza variants are most likely to circulate. Flu vaccines then use these variants at the starting point when developing the annual flu shot. Find out what’s inside the influenza-warding shot.
There’s the Marvel Universe, and then there’s the Larval Universe, and the star of the latter is the black soldier fly. In its squirmy maggot phase, it feasts on our messes—and can in turn be feasted on by farm-raised fish and fowl. Lucky grubs that reach winged adulthood live only a week, spending that time so frantically looking for sex that they don’t eat, sting, or bite. A true superhero, in other words, that sustainability-­conscious organizations are now eager to exploit.
Aiming to reinvent the toilet, sanitation company The BioCycle  is using black soldier maggots to convert waste into products like biodiesel. Meanwhile, ­EnviroFlight feeds leftovers from brewing and ethanol production to larvae, whose poop makes a lovely food for prawns.
Composting is great, but stuff left to rot can give off greenhouse gases. Ravenous maggots curtail that process.
GrubTubs will pick up food waste from restaurants and cafeterias in Austin, Texas, and sell it, along with larvae, to local chicken farms.
They’re more than 40 percent protein and packed with calcium, amino acids, and lipids. Raise your own grub-grade grubs in maggot farms such as the BioPod Plus. Scraps go in, snacks come out.
Once farmers can buy cheap insect-­based animal feed, they can stop relying on fishmeal and soy. That’s why companies such as AgriProtein are building fly farms around the world. A swarm of locusts might be a plague, but a global infestation of maggots? Blessing.
This article appears in the August issue. Subscribe now.
Fair warning. This video is about the botfly's horrific larvae, which grow and feed in human flesh. Don't say we didn't warn you.
Animals are living color. Wasps buzz with painted warnings. Birds shimmer their iridescent desires. Fish hide from predators with body colors that dapple like light across a rippling pond. And all this color on all these creatures happened because other creatures could see it.
The natural world is so showy, it’s no wonder scientists have been fascinated with animal color for centuries. Even today, the questions  how animals see, create, and use color are among the most compelling in biology.
Until the last few years, they were also at least partially unanswerable—because color researchers are only human, which means they can’t see the rich, vivid colors that other animals do. But now new technologies, like portable hyperspectral scanners and cameras small enough to fit on a bird’s head, are helping biologists see the unseen. And as described in a new Science paper, it's a whole new world.
The basics: Photons strike a surface—a rock, a plant, another animal—and that surface absorbs some photons, reflects others, refracts still others, all according to the molecular arrangement of pigments and structures. Some of those photons find their way into an animal’s eye, where specialized cells transmit the signals of those photons to the animal’s brain, which decodes them as colors and shapes.
It's the brain that determines whether the colorful thing is a distinct and interesting form, different from the photons from the trees, sand, sky, lake, and so on it received at the same time. If it’s successful, it has to decide whether this colorful thing is food, a potential mate, or maybe a predator. “The biology of color is all about these complex cascades of events,” says Richard Prum, an ornithologist at Yale University and co-author of the paper.
In the beginning, there was light and there was dark. That is, basic greyscale vision most likely evolved first, because animals that could anticipate the dawn or skitter away from a shadow are animals that live to breed. And the first eye-like structures—flat patches of photosensitive cells—probably didn't resolve much more than that. It wasn't enough. “The problem with using just light and dark is that the information is quite noisy, and one problem that comes up is determining where one object stops and another one starts. ” says Innes Cuthill, a behavioral ecologist at the University of Bristol and coauthor of the new review.
Color adds context. And context on a scene is an evolutionary advantage. So, just like with smart phones, better resolution and brighter colors became competitive enterprises. For the resolution bit, the patch light-sensing cells evolved over millions of years into a proper eye—first by recessing into a cup, then a cavity, and eventually a fluid-filled spheroid capped with a lens. For color, look deeper at those light-sensing cells. Wedged into their surfaces are proteins called opsins. Every time they get hit with a photon—a quantum piece of light itself—they transduce that signal into an electrical zap to the rudimentary animal's rudimentary brain. The original light/dark opsin mutated into spin-offs that could detect specific ranges of wavelengths. Color vision was so important that it evolved independently multiple times in the animal kingdom—in mollusks, arthropods, and vertebrates.
In fact, primitive fish had four different opsins, to sense four spectra—red, green, blue, and ultraviolet light. That four-fold ability is called tetrachromacy, and the dinosaurs probably had it. Since they're the ancestors of today’s birds, many of them are tetrachromats, too.
But modern mammals don't see things that way. That's probably because early mammals were small, nocturnal things that spent their first 100 million years running around in the dark, trying to keep from being eaten by tetrachromatic dinosaurs. “During that period the complicated visual system they inherited from their ancestors degraded,” says Prum. “We have a clumsy, retrofitted version of color vision. Fishes, and birds, and many lizards see a much richer world than we do."
In fact, most monkeys and apes are dichromats, and see the world as greyish and slightly red-hued. Scientists believe that early primates regained three-color vision because spotting fresh fruit and immature leaves led to a more nutritious diet. But no matter how much you enjoy springtime of fall colors, the wildly varicolored world we humans live in now isn't putting on a show for us. It's mostly for bugs and birds. “Flowering plants of course have evolved to signal pollinators,” says Prum. “The fact that we find them beautiful is incidental, and the fact that we can see them at all is because of an overlap in the spectrums insects and birds can see and the ones we can see.”
And as animals gained the ability to sense color, evolution kickstarted an arms race in displays—hues and patterns that aided in survival became signifiers of ace baby-making skills. Almost every expression of color in the natural world came about to signal, or obscure, a creature to something else.
For instance, "aposematism" is color used as a warning—the butterfly’s bright colors say “don’t eat me, you'll get sick.” "Crypsis" is color used as camouflage. Color serves social purposes, too. Like, in mating. Did you know that female lions prefer brunets? Or that paper wasps can recognize each others’ faces? “Some wasps even have little black spots that act like karate belts, telling other wasps not to try and fight them,” says Elizabeth Tibbetts, an entomologist at the University of Michigan.
The Science of Why No One Agrees on the Color of This Dress
3-D Map Shows the Colors You See But Can't Name
No, These Biohackers Can't Give Themselves Infrared Vision
But animals display colors using two very different methods. The first is with pigments, colored substances created by cells called chromatophores (in reptiles, fish, and cephalopods), and melanocytes (in mammals and birds). They absorb most wavelengths of light and reflect just a few, limiting both their range and brilliance. For instance, most animals cannot naturally produce red; they synthesize it from plant chemicals called carotenoids.
The other way animals make color is with nanoscale structures. Insects, and, to a lesser degree, birds, are the masters of color-based structure. And compared to pigment, structure is fabulous. Structural coloration scatters light into vibrant, shimmering colors, like the shimmering iridescent bib on a Broad-tailed hummingbird, or the metallic carapace of a Golden scarab beetle. And scientists aren't quite sure why iridescence evolved. Probably to signal mates, but still: Why?
The question of iridescence is similar to most questions scientists have about animal coloration. They understand what the colors do in broad strokes, but there's till a lot of nuance to tease out. This is mostly because, until recently, they were limited to seeing the natural world through human eyes. “If you ask the question, what’s this color for, you should approach it the way animals see those colors,” says Tim Caro, a wildlife biologist at UC Davis and the organizing force behind the new paper. (Speaking of mysteries, Caro recently figured out why zebras have stripes.)
Take the peacock. “The male’s tail is beautiful, and it evolved to impress the female. But the female may be impressed in a different way than you or I,” Caro says. Humans tend to gaze at the shimmering eyes at the tip of each tail feather; peahens typically look at the base of the feathers, where they attach to the peacock’s rump. Why does the peahen find the base of the feathers sexy? No one knows. But until scientists strapped to the birds' heads tiny cameras spun off from the mobile phone industry, they couldn't even track the peahens' gaze.
Another new tech: Advanced nanomaterials give scientists the ability to recreate the structures animals use to bend light into iridescent displays. By recreating those structures, scientists can figure out how genetically expensive they are to make.
Likewise, new magnification techniques have allowed scientists to look into an animal’s eye structure. You might have read about how mantis shrimp have not three or four but a whopping 12 different color receptors, and how they see the world in psychedelic hyperspectral saturation. This isn’t quite true. Those color channels aren’t linked together—not like they are in other animals. The shrimp probably aren’t seeing 12 different, overlapping color spectra. “We are thinking maybe those color receptors are being turned on or off by some other, non-color, signal,” says Caro.
But perhaps the most important modern innovation in biological color research is getting all the different people from different disciplines together. “There are a lot of different sorts of people working on color,” says Caro. “Some behavioral biologists, some neurophysiologists, some anthropologists, some structural biologists, and so on.”
And these scientists are scattered all over the globe. He says the reason he brought everyone to Berlin is so they could finally synthesize all these sub-disciplines together, and move into a broader understanding of color in the world. The most important technology in understanding animal color vision isn't a camera or a nanotech surface. It's an airplane. Or the internet.
The morpho butterfly appears blue but it isn't actually. It looks blue not because of pigment but because of some very fancy scales on its wings.
Every second of every day, trillions of tiny particles called neutrinos are raining down on your head. But unlike raindrops, hailstones, or bird poop, these elementary particles go right through your body—and through Earth’s crust, mantle, and core—at nearly the speed of light. After they sail through the entire planet, they fly silently back into the cosmos with scarcely a hello. It’s almost as if they never existed. “They’re the most mysterious type of particle we know of,” says Juan Collar, a physicist at the University of Chicago.
But neutrinos do leave fingerprints—if you know what to look for. In a study published Thursday in Science, Collar’s group observed a new type of neutrino interaction: a neutrino bumping into an atomic nucleus, a process known as coherent elastic scattering. At Oak Ridge National Laboratory in Tennessee, they fired a beam of neutrinos at a toaster-sized detector made of cesium iodide crystals. When the neutrino interacted with a cesium or iodine nucleus, the crystal would emit about 10 photons’ worth of dim light, cracking a window into the personality of the shyest particle. Understanding this collision could help physicists study weirder properties of neutrinos—and complicate their search for dark matter.
Neutrinos and nuclei are quantum mechanical particles, which means they don’t knock into each other quite like marbles on a sidewalk. They just get close, and then the neutrino transfers a tiny bit of energy to a neutral particle called a Z boson. “The neutrino kind of tosses the Z boson to the nucleus,” says physicist Hirohisa Tanaka of the University of Toronto, who wasn’t involved in the research. When the nucleus “catches” the Z boson, it recoils slightly, like the feeling after you catch a medicine ball, and then it emits the photons.
And it’s really difficult to detect these gentle interactions. Collar’s group bombarded their detector with trillions of neutrinos per second, but over 15 months, they only caught a neutrino bumping against an atomic nucleus 134 times. To block stray particles, they put 20 feet of steel and a hundred feet of concrete and gravel between the detector and the neutrino source. The odds that the signal was random noise is less than 1 in 3.5 million—surpassing particle physicists’ usual gold standard for announcing a discovery. For the first time, they saw a neutrino nudge an entire atomic nucleus.
Neutrino experiments like this one are a piece of a much larger puzzle. Physicists want to fix the current formulation of the laws of physics known as the Standard Model—a model that describes everyday phenomena perfectly well. But physicists have found that it gets some things wrong, especially about neutrinos. For example, in 1998, physicists found that neutrinos have mass—but the Standard Model predicted that they wouldn’t. By studying neutrinos in more detail, researchers like Collar and Tanaka hope to uncover unusual behavior that might illuminate what exactly is wrong in the Standard Model.
Alas, this measurement won’t help them rewrite any physics textbooks. The neutron-nucleus collision occurred exactly as the Standard Model predicted, says physicist Gerald Garvey of Los Alamos National Laboratory, who wasn’t involved in the research. In fact, physicists first predicted that a neutrino should interact with a nucleus in this way over 40 years ago. This time, the Standard Model will live to see another neutrino experiment.
The Search for a New Type of Neutrino Turns Up Empty
New Neutri­no Anomaly Hints at a Matter-Antimat­ter Rift
Inside the Hunt for a Ghost Particle
But Garvey points out that Oak Ridge could adapt the experiment to study some exotic Standard Model-defying neutrino phenomena. Like, say, the sterile neutrino. Physicists have observed three types of neutrinos—electron, tau, and muon—but some hypothesize that there’s a fourth, the sterile neutrino, which is even more invisible than its cousins. It’s possible that regular neutrinos change into sterile ones as they fly through space. Researchers could move the detector such that the neutrinos have to fly a longer distance through the air, then count whether the same number of collisions occur. If they count fewer collisions, that could be evidence of sterile neutrinos.
Counterintuitively, though, scientists will find this new information useful by tuning it out. Neutrino collisions aren’t the only weak signal in the universe—far from it. Dark matter, the elusive particles that physicists think make up a quarter of the universe’s mass and energy, also barely interact with detectors. Nobody knows what it actually is yet, but some proposed dark matter particles should cause nuclei to emit light just like a neutrino-nucleus collision. In those cases, neutrino collisions would be the background noise that needs to get filtered out. And though current dark matter detectors aren’t sensitive enough to be swamped by neutrino collision signals, they should be in the near future. “The saying goes, yesterday’s signal becomes today’s background,” Tanaka says. Current technology can’t filter out the neutrino noise, and they may have to develop an entirely new type of dark matter detector. Even the quietest particle sometimes needs a mute button.
A look inside the EBEX project, an experiment designed to detect a faint signal generated just after the birth of the universe. If successful, this signal could be a huge step toward achieving the "holy grail" of physics: a grand unified theory.
My daughter has spent the past week telling me, "Did you know that every time you click a mouse, you burn one calorie?"
OK, this sounds pretty cool, but I'm a bit skeptical. One calorie seems like quite a bit for one mouse click. Rather than dismiss it out of hand, I will attack this problem with a rough estimation. This may not provide an exact answer, but I can at least get a sense of whether my daughter is even close to being right.
So what exactly is a calorie? It is a unit of energy, but of course things aren't quite that simple. There are two kinds of calories: the small calorie and the big calorie. One small calorie is defined as the energy required to raise 1 gram of water 1 degree Celsius. One big calorie is 1,000 little calories—technically this would be called a kilocalorie, but everyone calls it a calorie because some people find ambiguity cool.
When you look at packaged food, it typically lists the energy equivalent in big calories. So a 100 calorie candy bar is actually 100,000 calories. Yikes. Maybe that's why people use big calories. In this case, let's assume a mouse click burns one big calorie, because that's usually what people mean when they say calorie.
Oh, you need to know about one more unit of energy: the joule. One calorie equals 4.2 joules.  While the calorie might be a common unit for energy with food, the joule is what we use in physics.
Now then. How do you calculate the energy in a mouse click?  You start with the work-energy principle. It states that the work done on an object (which is an energy) is equal to the product of force and displacement. Strictly speaking, you include an angle thingy in there too, but I left it out for simplicity.
If I can estimate the force and displacement for a single mouse click, I can calculate the work done (in joules). Ah, but what about the click force (not to be confused with clickbait)? Let's go with a value of 0.5 newtons. This is the same force a human exerts supporting a 50-gram mass. I assume this is greater than the force needed to click a mouse, but it provides a nice start.
Now, what about the click distance? This depends on your actual mouse, of course, so I will estimate the distance your finger moves when you click that button. How about 1 millimeter? That works for me. Multiplying that force and distance together, I get 0.0005 joules. This is equivalent to 10-7 kilocalories (big calories)—or less than a calorie, and even smaller than a small calorie. Even if I exert a bigger force over a greater distance, I'm not going to burn one big calorie clicking a mouse.
Exercise? Meh. Here's a Better Way to Burn Calories
Calculating Calories by Burning Gummy Bears to Death
Why Fitness Tracker Calorie Counts Are All Over the Map
You want to keep clicking that mouse until you burn one kilocalorie? Fine. Be my guest. You'll have to click it 10 million times. Clicking 10 times a second (which seems pretty fast), you'll be click-click-clicking away for 1 million seconds. That's 11.5 days. I'm pretty sure you would burn more energy breathing. Ultimately, this value of one big calorie per click simply isn't going to work.
But what would burn one big calorie of energy? Let's work backward. One big calorie is 4,184 Joules. I can think of two ways of burning that.  First, I could lift something 1 meter. That means that the force to lift it must be 4,184 Newtons such that it would have a mass of 426 kilograms. This is the mass of something like five to seven people. Yes, lifting them burns just one stinking calorie (big calorie). Let's try my second idea: lifting my own body. Assume I weigh 700 Newtons. To burn one big calorie, I must lift myself 6 meters. That's roughly two flights of stairs. Yes, this probably burns more than one kilocalorie since I also would my my arms and legs and stuff, but you get the idea. And it's a whole lot easier than clicking a mouse for 11 days.
Through motion tracking, data driven exercise, and eventually DNA analysis, SI and WIRED explore how scientific advances in training may help create the perfect NFL athlete.
Last week, when a British reporter broke the news that American scientists had used Crispr to edit the first human embryos on US soil, he wasted no time in cutting to the big, juicy, highly controversial chase. “One Giant Step For Designer Babies” ran the headline on Steve Connor’s world exclusive in the i, a London-based online newspaper. A similar report appeared at the same time in MIT Technology Review, though with a far more subdued title. But in both stories, details about the exact experiments were scarce, because the academic paper summarizing the work was still under peer review. Now the study is out, published online Wednesday morning in the journal Nature. And there’s a lot more to talk about.
In the past two years years, boundary-pushing reproductive biologist Shoukhrat Mitalipov led researchers at Oregon Health and Science University, the Salk Institute, and Korea’s Institute for Basic Science through a series of experiments designed to correct a genetic defect in viable embryos. A mutation in MYBPC3 causes a heart condition known as hypertrophic cardiomyopathy that affects one in 500 people—the most common cause of sudden death among young athletes. Using Crispr-Cas9, they successfully replaced the flawed gene with a normal one for 42 out of 58 embryos, the most successful demonstration of the technique’s gene editing prowess in the human germline. And while the mutation-correcting mechanism was highly efficient, it wasn’t the one that Mitalipov, or anyone, was expecting.
Before Mitalipov's team could edit the first embryos in the US, they had to make them. So they took sperm from a guy with a mutation in his MYBPC3 gene and used it to fertilize eggs from 12 healthy females. In addition to the sperm, they also injected each egg with Crispr-Cas9 protein, a guide RNA directing it toward the mutant copy of MYBPC3, and a piece of template DNA, modeled after the normal gene but with a few tags so scientists could find it again later. The idea was for Crispr to cut out the mutant copy and the embryo’s repair machinery to use the supplied template to build a normal gene in its place.
And it worked—surprisingly well. Past Crispr experiments in China have run into issues; sometimes, not every cell in the embryo gets repaired, or Crispr cuts things it shouldn’t. Even earlier attempts by Mitalipov’s team to edit MYBPC3 in stem cells with Crispr ran into similar issues. But when it came to the embryos they injected at the exact time of fertilization, they saw very low rates of either of these failures.
But one thing didn’t work at all the way the scientists expected. Of the 42 successfully corrected embryos, only one of them used the supplied template to make a normal strand of DNA. When Crispr cut out the paternal copy—the mutant one—it left behind a gap, ready to be rebuilt by the cell’s repair machinery. But instead of grabbing the normal template DNA that had been injected with the sperm and Crispr protein, 41 embryos borrowed the normal  maternal copy of MYBPC3  to rebuild its gene.
Which is why Mitalipov insisted on the title given to their paper: “Correction of a Pathogenic Gene Mutation in Human Embryos.” “Everyone always talks about gene editing. I don’t like the word editing. We didn’t edit or modify anything,” Mitalipov says. “All we did was unmodify a mutant gene using the existing wild type maternal gene.”
The next step will be to see if they can replicate this “unmodifying” effect in different mutations. The MYBPC3 gene had four messed up base pairs, so it was pretty easy for Crispr-Cas9 to find and replace. But other mutations might be off by just a single letter, which would be harder to fix. There’s always a chance that MYBPC3 will have been a case of beginner’s luck, so they want to make sure the effects are generalizable to other common mutations, like the BRCA genes associated with increased risk for breast and ovarian cancers.
Crispr experts around the globe were quick to celebrate the work while pointing out its many limitations. “This is a remarkable paper that shows how much the field has progressed in just the last year or two,” says Gaetan Burgio, a geneticist at the Australian National University. “But I think for now everyone needs to chill down a bit. The scope is very limited, and it’s unlikely to me that Crispr would be a substitute for preimplantation genetic diagnosis, whatever the authors say.” Burgio is referring to the genetic profiling of embryos prior to implantation via IVF—it’s a way to screen for mutated genes like MYBPC3 and only select the 50 percent of embryos that are normal.
Scientists Crispr the First Human Embryos in the US (Maybe)
Scientists Upload a Galloping Horse GIF Into Bacteria With Crispr
Crispr's Next Big Debate: How Messy Is Too Messy?
Mitalipov and his coauthors argue that their Crispr technique can get that number up to around 75 percent, maybe even 100. Which would prevent prospective moms, especially older ones, from having to go through multiple rounds of costly, unpleasant egg harvesting.
But validating that kind of treatment would require lengthy clinical trials—something a rider in the current Congressional Appropriations Act has explicitly forbidden the Food and Drug Administration from even considering. Mitalipov said he’d have no problem going elsewhere to run the tests, as he did previously with his three-person IVF work. Before that, he’d need to re-run these experiments in animals, and implant the embryos to assess them at different developmental stages for any abnormalities. Collaborators like Jun Wu at the Salk Institute will likely follow up in another way, with more stem cell studies, to see if the Crispr corrections follow cells through all their different lineages—into neurons and liver cells and heart cells.
If there’s anything Wu and Mitalipov and the rest of their team have learned through all this, though, it’s that stem cells and embryos are not re-created equal. Because the early days of embryonic development are so tumultuous, with lots of dividing and recombining, those cells might have special ways to avoid genetic mishaps—like, say, copying a random piece of DNA that a scientist stuck into a cell. Evolution may have made it harder than anyone thought to subvert its will with superbaby genes.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
This story originally appeared on Mother Jones and is part of the Climate Desk collaboration.
There’s something different about the crop of Democrats running for Congress in 2018. As in previous years, the party has recruited a small army of veterans in high-profile races and in Republican-held districts. There are loads of state legislators, business owners, and government officials.
But the candidates also include a volcanologist who’s worried that her favorite research spot will be opened up for development; an aerospace engineer who’s running against the climate-denying head of the House Science Committee; a pediatrician who spends part of the year treating leprosy patients in Vietnam; and a physicist who worries what budget cuts would mean to the federal research facility where she spent her career.
All told, more than a dozen Democratic candidates with science backgrounds have announced their candidacies for Congress or are expected to in the coming months. The boomlet of STEM-based candidates amounts to a minor seismic event in a community where politics and research have traditionally gone together like sodium and water. Trump has been in office just six months, but he’s already done something remarkable—he’s gotten scientists to run for office.
The surge of science-based candidates has been aided by a new political outfit called 314 Action, launched last summer by Shaughnessy Naughton, a breast cancer researcher from Pennsylvania who ran for Congress in 2014 and 2016 . The group, named for the first three digits of Pi, aims to do for candidates with scientific backgrounds what EMILY’s List has done for pro-choice women—funding, recruiting, and training candidates at every level of government. So far 6,000 scientists have reached out to the group about running for federal, state, and local offices; and 314 plans to also back candidates in three dozen school board races this fall. Washington has plenty of lawyers; maybe it’s time for a fresh experiment.
“Traditionally the attitude has been that science is above politics, and therefore scientists shouldn’t get involved in politics, and what that ignores is the fact that politicians are unashamed to meddle in science,” says Naughton. “The way we push back against that is to hold a seat at the table.”
The ranks of scientists in Congress have been thin in recent years. Rep. Bill Foster (D-Ill.) was a high-energy particle physicist at Fermi National Laboratory in the district he now represents. Until recently the dean of the bunch was Democratic Rep. Rush Holt of New Jersey, an astrophysicist who retired in 2014 and now serves as CEO of the American Association for the Advancement of Science. The only STEM field that’s well represented in Congress is medicine; there are 14 physicians between the House and the Senate, but most are Republicans who have shown more of a commitment to conservative dogma than scientific best practices. (Former Georgia Republican Rep. Paul Broun, a doctor, infamously referred to evolutionary biology as a lie “from the pit of hell.”)
“Traditionally the attitude has been that science is above politics, and therefore scientists shouldn’t get involved in politics, and what that ignores is the fact that politicians are unashamed to meddle in science.”
One result of the dearth of scientists has been a Congress that is often ignorant of the scientific perspective, not just on obvious issues like climate change—Texas Rep. Lamar Smith, the chair of the House Science Committee has called it a myth propagated by “so-called, self-professed climate scientists” and subpoenaed emails from government-funded climatologists—but on virtually every subject that comes up.
“When the Help America Vote Act was passed after the 2000 election, nobody thought that was a science issue—who thought anybody would hack election computers?” Holt says. “Right from the start, I said, ‘Hey, wait a minute, you passed a bill that encouraged jurisdictions all over the country to move to electronic voting machines that are simple, easy to use, and completely unverifiable. If you had cleared that with some computer scientists before writing the bill, you would have realized that having unauditable elections is not smart.’”
In some sense scientists were victims of their own success. The growth of government-funded science over the last half century through everything from the National Institutes of Health to the Defense Advanced Research Projects Agency (DARPA) has helped thousands of researchers carve out careers. But it has also incentivized scientists to put their heads down and keep quiet, lest they jeopardize that funding.
“On average, scientists are not particularly outgoing and are psychologically not conditioned for this sort of thing,” Holt says. But just as importantly, “the entire rewards system of science doesn’t encourage social or political involvement.” Getting more scientists in the House requires knocking down their preconceptions about how people in STEM should approach public life.
One reason for the political awakening is Trump himself. Even before taking office his transition staff roiled the scientific community when it asked the Department of Energy for a list of staffers who had worked on global warming; the anticipated purge never materialized, but the Trump DOE has issued guidelines instructing employees not to use terms like “emissions reductions” and even brags on its agency Twitter account that Secretary Rick Perry is winning the “fight” with climate scientists. Environmental Protection Agency director Scott Pruitt has jettisoned dozens of members of his agency’s scientific advisory board.
Trump has defied scientific consensus on anthropogenic climate change and placed unqualified friends and allies in charge of departments responsible for doling out billions in funding. His proposed travel ban would bring the hammer down on international researchers. And he has called for steep budget cuts that would more than decimate research budgets and send scientists looking for new sources of funding or risk abandoning their projects. And all that is just six months in.
Jess Phoenix, a volcanologist seeking the Democratic nomination in the Southern California district represented by Republican Steve Knight, decided to run when she saw that the public lands where she’s done much of her research were at risk of losing their protections in the Trump era.
Phoenix has traveled around the world studying lava flows, but “it’s fair to say the Mojave is where I fell in love with science,” she says. Her first research project was in Death Valley National Park, and she runs an educational nonprofit for grade-school students that’s based in the Mojave National Preserve. The newly created Mojave Desert National Monument was among several dozen sites being reviewed by Interior Secretary Ryan Zinke for delisting or possible downsizing.
Like Phoenix, many of the science candidates are running in districts with a high percentage of voters with college degrees. Elaine DiMasi, a physicist who is on leave from Brookhaven National Laboratory, is preparing to run against Long Island Republican Lee Zeldin. Jason Westin, an oncologist and researcher at Houston’s MD Anderson Cancer Center, is running against Texas Republican Rep. John Culberson in part because he’s worried about what NIH cuts would mean for him and his colleagues. Stem-cell scientist Hans Keirstead is the leading challenger to take on longtime Orange County Rep. Dana Rohrabacher, whose district was carried by Hillary Clinton last fall. Joseph Kosper, an aerospace engineer and Army veteran, is one of eight Democrats running against Lamar Smith in a district that includes the University of Texas. All four of those challengers have been in talks with 314 PAC.
In June, not long after her Republican congressman, Ed Royce, voted to repeal the Affordable Care Act, California pediatrician Mai-Khanh Tran switched her office hours to part-time and announced she was running for his Orange County seat.
US Scientists, Please Run for Office. The Planet Needs You
Trump’s Budget Forgets That Science Is Insurance for America
The Creeping, Quiet Gaslighting of the EPA
“I felt like my heart was gripped by this overwhelming pain,” says Tran, who spends part of her year treating lepers in her native Vietnam. “But I went to work and one of the first patients I saw in the office was a patient with a very severe illness—she had a brain tumor.” The girl’s mother, who worked at a nail salon, had been able to get health insurance through a subsidy provided by the Affordable Care Act. “We were hugging each other, crying—we really thought that our lives and a lot of our patients would be affected very soon. I didn’t realize how soon.”
Trump’s election was an energizing moment for Tran not just because of her place in the health care system, but because in addition to being a pediatrician and leprosy researcher, Tran is also a refugee.
She left Vietnam when she was nine on one of last “Orphan airlift” flights out of the country before the United States evacuated Saigon. Her father had dropped Tran and two siblings off at an orphanage because it offered the best chance of survival. (They would later reunite in Oregon.) “I kept thinking, ‘What on Earth is he wearing sunglasses for?’” she said of their parting. “’He’s such a proper man, why is he wearing sunglasses?’ And it dawned on me years later that he didn’t want us to see him cry.”
Tran’s flight was filled with orphans and handicapped children. When they finally landed she remembers being carried off the plane by a Marine; the nature of her arrival in the country was formative in her decision to get into medicine but also in her political outlook.
“When I see that picture of that little Syrian boy, I remember thinking I was just as scared as he was once,” she said, referring to the now-iconic photo of Omran Daqneesh sitting dazed and bloodied in the back of an Aleppo ambulance. “I don’t know why I was any more deserving of being in this country.”
Tran has a head of steam in Royce’s Southern California district. In July, she picked up the endorsement of EMILY’s List. But in a sign of the changing currents in her field, she isn’t even the only scientist in the Democratic primary to take on Royce. To get to the general election, she first has to get past a group of challengers that includes Phil Janowicz, a former Cal State Fullerton chemistry professor who left his job at the education company McGraw-Hill the morning after the election to begin planning for his campaign. Like Tran, Janowicz has been in touch with 314; he flew to DC in April for the group’s first candidate training. His slogan: “Solutions for Congress.”
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
While millions of astronomy enthusiasts chase the moon’s shadow on the ground during the August 21 total solar eclipse, four NASA personnel are going to have front row seats. Two pilots and two technicians will race the big black spot at 50,000 feet, well above any cloud cover. And at 460 mph, they’ll be able to stretch their time in the shadow to four minutes, compared to less than three on the ground.
They’re not just up there for the view. The aircraft, two 1960s vintage WB-57F jets that NASA frequently dispatches for high-altitude research, will carry instruments to help scientists study the solar atmosphere. At cruising altitude, the sky will be 20 to 30 times darker than on the ground, enhancing the details in the sun's atmosphere—and allowing for pictures in the greatest detail yet.
Typically, solar scientists simulate eclipses with special equipment designed to block the sun, called a coronagraph. But the small disk, usually mounted in front of a telescope, can't fully overwhelm the sun's brightness. The moon can, though—which is why people have flown telescopes during eclipses before. "But this will be the highest quality observation of this type made to date,” says lead researcher Amir Caspi, a senior scientist at Southwest Research Institute in Boulder, Colorado. “The resolution and the frame-rate we’ll be able to achieve will provide new insights into how the sun works."
Flying in tandem on either edge of the shadow's 70-mile girth, the planes' paired data will effectively stretch totality to a full seven and a half minutes. They could have stretched it further, but with diminishing returns: “You can fly at supersonic speeds to extend totality," says Caspi. "The Concorde managed to stretch it to 74 minutes when they did their eclipse flight, but that also generates a lot of turbulence in the airflow around the airplane."
This flight's relatively low speed will ensure minimal distortion for the instruments, which are set on moveable gimbals to track the sun as the jets zoom. They’ll use cameras called AIRS/DyNAMITEs, initially developed to track the Space Shuttle during launches in the wake of the Columbia disaster. Data will be recorded onboard the aircraft but also transmitted through ViaSat satellite downlink for live broadcast on NASA’s site.
That footage will help answer a few solar conundrums. The outer atmosphere of the sun, strangely, is dramatically hotter than its surface—like, millions of degrees hotter. There are two competing theories why: One says that the magnetic field can have so-called Alfvén waves moving through it, transmitting the sun’s energy into the outer atmosphere, while another holds that small explosions called nanoflares might be releasing heat. The flight's high-resolution video should be able to detect slight wave motion in the corona—the sun’s atmosphere, which is usually overwhelmed by the star’s surface brightness—allowing the scientists to determine the strength and sizes of the waves, along with their potential influence on temperature.
The equipment may also reveal the nanoflares, which nobody's technically ever seen. If they do finally appear, they could help explain another solar quirk: the surprisingly uniform pattern of the corona's arches and streaks. “It has loops and fans and smooth, well-organized structures, but our images of the surface and our computer models, suggest a turbulent, boiling environment, so the corona should look like a tangled, snarled mess,” Caspi says. “The corona looks like it’s been combed, so we want to know why it’s combed corona and not bedhead corona.”
Guess How Many People Can Actually Watch the Total Eclipse
Meet the Woman Trying to Prepare Your Town for the Total Eclipse
Videos Capture Total Solar Eclipse From Space and Earth
That's all well and good for the solar scientists. But understanding the sun has applications back on Earth, too. “Studying things like these two factors and the solar flares and the coronal mass ejections we can also see during eclipses teaches us about weather hazards," says Caspi—you know, like the possibility of a solar flare disrupting the power grid or messing with satellites. And any solar energy system that impacts Earth will impact other planets around other stars—which means these discoveries could influence the search for habitable planets. "We may find a planet where the temperatures are right for life, Caspi says, "but if it’s getting blasted by X-rays all the time, you’re not going to find life—at least, not on the surface.”
Now, that's just what the flights will focus on for three minutes. In total, they'll be airborne for several hours as they cut a path from Houston's Ellington Field near NASA’s Johnson Space Center through Missouri, Illinois, and Tennessee. So for 30 minutes before and after totality, during the partial phases of the eclipse, the instruments will zoom in on sun-adjacent Mercury, which is typically tough to image because it gets washed out by sunlight.
In that hour, they'll be looking for information about how Mercury's temperature varies across its surface. The little planet rotates much slower than Earth, resulting in a dramatic fluctuation in surface temperatures—from 800 degrees Fahrenheit in the middle of the day to 300 below zero at night. (One day on Mercury equals 59 Earth days.) Infrared cameras will assess the surface temperature across the surface to see how quickly it cools, which will reveal its composition in greater detail than previous attempts using X-ray imaging.
This is just one of 11 research projects—mostly ground-based—that NASA is funding in conjunction with the eclipse, but it promises to generate perhaps the most dramatic results. It’s just a pity that the airplanes only hold two people. “It’s my first eclipse, but I’ll be watching it from a monitor in Houston,” Caspi laughs.
On August 21, darkness will wash over America. But in a good way, we promise. Here's everything you need to know if you want to catch a glimpse of the solar eclipse.
The collapse of the diagnostic company Theranos shook the world of venture-funded biotechnology. Theranos was a “unicorn,” valued by its investors at $9 billion at one point. That made its founder, Elizabeth Holmes, the youngest woman billionaire in the world for inventing a technology that, she said, allowed pharmacies and doctors’ offices to do a bunch of medical diagnostic tests on a single drop of a person’s blood.
Crushing investigative journalism, led by the Wall Street Journal’s John Carreyou, showed that Theranos’ enabling technology didn’t work. The company’s value zeroed out and the Centers for Medicare and Medicaid Services banned Holmes from laboratory work. The rise-fall narrative rang loudly in the worlds of health care, medical device technology, and the investment firms increasingly funding hunts for nominally disruptive, Silicon Valley-style solutions to longstanding medical problems.
Like the Wizard said: What did you learn, Dorothy? At the Innovation Pipeline conference on Tuesday—put on by The Atlantic in San Francisco—a panel of biotech investors and executives sounded like they were still stunned, but hellbent on making sure they don’t get burned again.
There, one of the panelists was Cary Gunn, the president, CEO, and founder of Genalyte, a company that says it can do 128 diagnostic blood tests on just one drop of blood in just a few minutes. That’s an elevator pitch that you might think would raise some red flags in light of well, you know. And it did. But it also garnered a $36 million investment from, among other places, Khosla Ventures.
So Gunn bravely set out to defend Theranos—to a point. “Decentralizing diagnosis and getting the data into doctors’ hands a lot faster” is still a good idea, he said. “We’re just going to do it right.” Genalyte researchers have presented initial data at conferences and announced plans to carry out more human studies and publish the results. Theranos was notoriously cagey with its data.
That seems to have been at least one takeaway: VCs are asking to see results first. “We haven’t done a single financing round without having to show data on our assay,” said Gabe Otte, CEO and co-founder of Freenome, another diagnostics company. “That’s largely been because of Theranos. They were able to raise hundreds of millions of dollars without showing data.” The company didn’t have to, regulatorily speaking. And if future companies do, it’ll be because of economic incentives like funding, unless the laws change.
(Otte has run into his own speed bumps; a Buzzfeed investigation earlier this year suggested either he or his company had at times claimed Otte had a PhD when in fact he left his program before getting a degree.)
Everything You Need to Know About the Theranos Saga So Far
Fixing the Laws That Let Theranos Hide Data Won't Be Easy
Theranos Blew It. But It Didn't Ruin Biotech Startups for Everyone
Lynn Seely, president and CEO of Myovant Sciences—currently in phase III trials of a drug for heavy menstrual bleeding and uterine fibroid pain, with what Forbes called the biggest biotech IPO of 2016—suggested that whatever changed in venture capital and biotech, Theranos had tarred the field. “Two out of three times on a plane, when I tell someone what I do, the next comment is, ‘Theranos’,” Seely said. She added that it was “sad” that the scandal happened to a woman. The implication was that Holmes’ gender might make it even less likely for woman-led companies to get funding.
Absent data, maybe investors and the media should’ve showed more skepticism from the start. People have touted the concept of a single-drop blood test for years. “The idea that maybe some people working on a problem in a garage have been able to do it overnight may not be true,” said Sean Harper, the executive VP of research and development at the drug giant Amgen.
If due diligence wasn’t the norm at venture capital companies, maybe now it will be—if there's a return on that investment. Diagnostics, as a field, is a money-saver in terms of public health, but looks a lot less profitable without the kind of business model Theranos promised. Expensive, high-tech drugs, though? “There’s no multi-billion-dollar exits in diagnostics,” Gunn said. “There are in therapeutics.” Maybe what Theranos really revealed was a structural flaw in the way medical tech gets funded. Self-awareness about that company’s collapse was on display at the conference, but whether Silicon Valley will change is still in play.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?
Satellites do an incredible job of mapping algal blooms, the green mats that spread over lakes and oceans during warm, nutrient-rich summers. But the hypnotic, swirling images from space can't tell if toxins are lurking in a carpet of cyanobacteria, threatening the safety of water.
Ecologists and hydrologists can test water's drinkability by boating through the blooms—though collecting samples off the side of a power boat is tricky and inconvenient. So this year, scientists are monitoring Lake Erie with a robot, 18 feet below the water’s surface.
The so-called Environmental Sample Processor, ESPniagara, sits on the floor of Lake Erie’s western basin. It collects algae from the surrounding water, analyzes microcystin (a small, circular liver-toxic protein), and uploads results for researchers at the end of every test. They're watching this toxin closely, because elevated levels of it could swiftly poison the water supply for humans and wildlife in the surrounding area.
A no-frills charm dominates the ESPniagara's aesthetic. “It kind of looks like a trash can,” says Tim Davis, a molecular ecologist at NOAA in Ann Arbor. Tentacles of clear plastic tubing for sample processing swirl around the lab-in-a-can’s lower half, while circuits and wiring snake between the components above. Those electronics and the machine’s batteries—400 D cell batteries power the unit—understandably need some protection to sit at the bottom of the lake. “The metal trashcan is essentially a pressure case that can withstand very, very high pressures, and essentially keeps it dry,” Davis says.
Staying dry isn’t the only requirement for the lab capsule. ESPniagara also needs to stay put, remain upright, and avoid sinking too deeply into the gunky mud. So NOAA recruited applied physicists at the University of Washington to design the 1,000-pound frame encasing the unit. By their calculations, even if Hurricane Sandy-level winds hit Michigan, the water sampling could continue. And at the lake’s surface, a round orange data buoy relays information from its tests via a cellular modem, like the one in your phone.
So far ESPniagara has been testing the water every other day. But as of August 1, with the risk of harmful blooms steadily rising, it began testing on a daily schedule. It pulls lake water in, concentrating algae cells onto a filter. When the filter is clogged with plenty of algae to measure, the biology begins.
While full-scale labs use temperature-controlled water baths, freezers, and centrifuges to run these kinds of experiments, the ESPniagara accomplishes the same tests with a few carefully formulated protocols. Each toxicity measurement happens within a quarter-sized puck that’s about an inch and a half tall.
To measure the algae’s microcystins, it’s important to know that the cyanobacteria hold most of their toxins inside their cells. “So in order to get accurate concentrations, you need to be able to break the cells open,” Davis says. A bit of methanol-Tween-20 (basically dish detergent) does the trick, along with some heat and pressure. And once the cells are cracked open to reveal all the toxins, the ESPniagara dots samples into a four-by-five grid for quantification.
Climate Change-Fueled Storms Could Leave Less Water for Drinking
Incredible Images of Algal Blooms Taken From Space
Don't Drink the Water ... In Lake Erie
The toxin detection relies on antibodies that fluoresce when they’ve bound a specific substance. In this case, the antibodies that don’t bind a toxin light up, so brighter dots mean safer water. An internal camera photographs the test array, and at the end of this whole process—it takes roughly four hours—the data buoy sends off that photo. The results end up with collaborators all the way across the country, at the Monterey Bay Aquarium Research Institute servers. Then Davis and his team download them for their own analysis.
Once they've got toxicity data, they combine it with satellite measurements for algal biomass and hydrodynamic models of windspeed and current. That full picture tells them how toxic the bloom is, and where the toxins will end up next. Knowing that strong winds are about to send more toxins into the water supply, for example, helps treatment plants decide how to act. When more microcystins arrive, they’ll know to roll out extra filtration steps—like particle activated charcoal neutralization—to keep drinking water safe. It’s almost like the ESPniagara gives water treatment plants ... extrasensory perception.
You can't see it, but there's a lot going on inside a glass of tap water. The faucet aqua is essentially an invisible cocktail of sulfates, resins, varying levels of lead, and so much more. Find out what else is inside.
The new GOP regime looks to be catastrophic for science: The first Trump budget proposed slashing funding on everything from ocean research to satellites. And work on climate science? “We consider that to be a waste of your money,” Trump’s budget director said. Science is under attack; there’s no other way to put it. Apart from marching in the streets and waving signs, what can scientists do?
Run for office. The country desperately needs more egghead lawmakers. Right now, Capitol Hill has almost none. The House and Senate are acrawl with lawyers, bankers, and businesspeople but somehow manage to repel people trained in the process of gathering data and testing hypotheses in order to better understand reality.
Trump's Trying to Chainsaw Nearly Every Environmental Program
Brian Greene on How Science Became a Political Prisoner
Marching Brought Scientists Together—But What Do They Do Now?
Yet, my God—aren’t those precisely the skills you want in today’s post-fact Washington? Particularly given how many critical policy questions pivot on gnarly scientific questions. And I don’t mean only things like atmospheric science or energy—you want scientists grinding on the nonobvious issues too, like crime and agriculture and mine safety. Or how about plain old voting? As Rush Holt, a physicist who served for 16 years in the House of Representatives, points out, in 2002 Congress doled out billions for dodgy, hackable touchscreen voting machines, without first checking with computer scientists who’d have instantly warned them away.
I often assume that many right-wing congressfolk are hostile to science, and some are. But the bigger problem is that lawmakers are intimidated by it. “No member of Congress knows every area, so we want them to be quick learners,” Holt says. They’ll read up on foreign affairs and economics. “But when it comes to science, they say, ‘Oh, I can’t do that.’” If some elected representatives will always be scared of microscopes, they need colleagues who aren’t.
Right now, Capitol Hill is—to borrow a term from agriculture, that oldest form of science—a monocrop. The profusion of lawyers in the House and Senate is understandable: We are a nation of laws. But that legalistic heritage has cultural side effects. Lawyers are trained to be adversarial and to massage facts, if need be, to help their client. “Lawyers argue their case; scientists follow the facts,” says Josh Morrow, executive director of 314, a group pushing scientists to run for office.
We need more politicos who think differently from lawyers and CEOs. (Forget scientists for a minute: How about more artists, health care aides, and elementary school teachers in Congress too?) For all the hand-wringing over the “replication crisis,” scientists really do have an ethical and professional North Star—the scientific method. I’d love to see that culture grafted onto the Hill’s default heuristic of negotiation and argument.
And hey, I dare to hope. In the past, scientists shied away from public office for a bunch of reasons. They’re not plugged into money networks; their reliance on nuance makes them uncomfortable with party politics. They do not talk in sound bites. But Trumpism has so unsettled scientists that they’re throwing their safety glasses into the ring. At least seven have already launched state or federal campaigns. Holt, who still keeps up with politics, has heard from a dozen more.
“I’m thinking about my daughter growing up in a world without clean water or clean air,” says Molly Sheehan, a University of Pennsylvania biophysicist who’s running for the House. If Trump’s administration isn’t going to consult with scientists, scientists will have to muscle their way in. As the old Washington koan goes, if you’re not at the table, you’re probably on the menu. “We need a seat,” Sheehan says. Preferably one that’s the right height for a good microscope.
This article appears in the August issue. Subscribe now.
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
There's something sort of cool in the next version of Apple's iOS. It's called ARKit—basically, it's a part of Apple's developer package to help programmers create awesome augmented reality apps. Like, maybe a program that adds dancing hotdogs to your screen so that they look like they are there in real life. Or better yet, something useful—like an app that measures distances by just looking at stuff through your phone's camera.
But how does this work? What magic does Apple use so that your phone can turn a 2-D image into something that looks like it is there in real life? The answer is parallax.
Let's start with a super simple demo to demonstrate the effect. Here's what you do: Take your arm and hold it out in front of you with your thumb sticking up. Now close your left eye and look at your thumb. In particular, look at some object that is past your thumb (something in the room or something outside—it doesn't matter). Now open your left eye and close your right eye. Notice the apparent motion of your thumb with respect to background objects. It looks as though your thumb is moving. Now switch your viewing eye back and forth—left, right, left, right. Cool winky face, dude.
The moving thumb is an example of parallax. It's the apparent motion of an object with respect to background objects when the viewing point moves. The closer the object is to an eye (or a camera), the more it appears to move. In the case of your eyes, it's as though your view moves from the left to the right eye. You could also do this with an actual camera.
But here is the cool and useful part. If you know the actual distance between two viewing points and the angular change in position of an object, you can calculate the distance to the object. Parallax isn't just a cool party trick, it can also be used to find real stuff. This calculation isn't even that difficult. If you measure the angular displacement of the object and the horizontal shift, then the distance to the object would be:
Of course, the angle should be in units of radians in this cases—but your phone can determine this value by comparing two views from the camera (and using some object recognition algorithm). What about the shift distance? Yes, your phone can do this too. Using sensors like the accelerometer, it can calculate the distance the phone moves from one vantage point to another. So there you have it. That is how the ARKit works—well, probably.
As long as I am talking about parallax, I should include the most important and interesting example from astronomy: stellar parallax.
An early model of the solar system, the geocentric model, held that the Earth was in the center of the solar system—the other planets and the sun revolved around it. At first glance, this model seems to make sense. We can see that the planets do indeed move relative to the background stars. And it doesn't really feel like the Earth is moving, so why not put the Earth in the center?
After collecting more data on the motions of the planets, scientists started leaning toward a heliocentric model of the solar system, with the planets instead orbiting around the sun. But there was one big stumbling block to the adoption of this model—the lack of stellar parallax.
Just imagine that you are on the Earth (because you probably are). In the month of December you are on one side of the sun. Six months later, you are on the other side. The view from these two locations are like a ginormous pair of eyes looking at the stars from different view points. By knowing the size of the Earth's orbit and the angular shift of the stars, you can use parallax to determine the distance to these stars. This is called stellar parallax.
However, there was a big problem. Early astronomers attempted to measure the angular shift of stars as the Earth went around the sun, but they couldn't see any shift. Maybe there was no stellar parallax because the Earth wasn't moving around the sun.  Maybe the geocentric model was the better model.
Measure the Height of a Building With an ... Accelerometer?
How to Measure the Height of a Building With a ... Barometer?
Physics Proves No One Can Safely Text and Drive
So, what happened? Why couldn't they detect stellar parallax?
The answer lies in the distance to the stars. If the Earth orbits the sun (and it does), then we can work backwards and calculate the angular motion of the nearest star. Alpha Centauri is about 4 light years away (or about 250 thousand times further than the Earth is from the sun).  Using the parallax formula above, this should give an angular shift of 1.6 x 10-5 radians or just 3.3 arc-seconds. This tiny angle was just too small to detect with the instruments of the time.  But it is there. Traditionally, the angular shift is reported as half of the total angle (just a note).
What if an object was a at a distance such that it had a half-angle shift of 1 arc-second? This would have the distance of 1 parsec. Yes, a parsec is a unit of distance, not time. The "par" is for parallax and the "sec" is for angle.
Side note: Yes, that means Han Solo was wrong. The Millenium Falcon couldn't have made the Kessel Run in under 12 parsecs—that's not even a time, even though it sounds cool. Really, if Star Wars had a science advisor, they should have recommended some other time unit. How about "megasec"? Or even better you could use "quantsec"—which doesn't really mean anything but sounds cool. Oh, don't worry—I still love Star Wars and this Kessel Run thing doesn't even bother me.
The age of virtual realty is here but augmented reality and its cousin mixed reality are making strides. WIRED senior editor Peter Rubin breaks down the new platforms.
In October 2016, the organizers behind a microbiome conference sent promo materials to some prominent scientists. Elisabeth Bik was one of them. With nearly 12,000 followers, her tweets could help publicize their upcoming event in San Diego. But when she scanned the lineup, she noticed that almost every speaker was a man. Add more women, she suggested—or the conference should expect backlash.
She was right: Biologist Jonathan Eisen—“Guardian of Microbial Diversity,” his Twitter bio says—brought the biased list to the attention of his 46,000 followers with a blog post called "The White Men's Microbiome Congress." The organizers, Kisaco Research, added more female speakers before the conference convened and issued a penitent statement.
Bik, who runs the widely read Microbiome Digest, didn’t raise the alarm at the time. “They looked like they were going to do better," she says, "so I didn't want to make a big stink."
But last week she saw the latest speaker list for the Kisaco-organized European Microbiome Congress happening this November: same story. Eisen did, too.
https://twitter.com/phylogenomics/status/889493638002102272
In the past few months, two other high profile science conferences—Starmus and the World Science Festival—have also ignited internet ire for their lack of representation. And websites exist specifically to point out the most egregious examples: There’s Bias Watch Neuro, an All Male Panels Tumblr, and the hashtag #manel. The Gender Avengers, a community dedicated to hearing women's voices in public conversation, asks professionals to pledge not to serve on such panels.
Yes, it's 2017. Yes, this is still happening. “Women tell themselves, ‘Our generation is going to do better. When I'm in my 40s, I'll be the speaker,’” Bik says. “I thought that. It hasn't happened.”
Women have experienced underrepresentation over decades and in different departments of study. And it has real-world repercussions: Who enters science and who rises to the top of a field both have a dramatic impact on the type of research that gets done. But people like Bik, and the online communities around them, are working to make it better next time. Really.
Gender representation has been pretty imbalanced for the VIP-laden Starmus conference's six-year history, in part because organizers pride themselves on inviting fancy people of a specific sort. Nobel laureates, astronauts, Stephen Hawkings—all designations sooted with historical and cultural biases of their own.
Although Starmus still doesn’t have speaker stats to boast about—less than a quarter of main-stage speakers have been women—more female scientific stars appeared at the June meeting in Trondheim, Norway, than in the past. Still, “there got to be an undercurrent in the audience, when you see this stage full of men with a token woman or no one at all," says astronomer Jill Tarter, who has been the only woman on Starmus' board. "It just festered.”
The festering reached a fever pitch during a panel with seven men and zero women, after economist Christopher Pissarides confessed that he had changed Siri to a male voice. You know, because he trusts it more.
When question time came, Tarter commandeered the mic. “I’m wondering,” she said, “why after a beautiful, inspiring lecture by Jeffrey Sachs this morning about [how] we have to solve our problems globally—everybody needs to be in the game—why our very wise, knighted Nobel laureate found two opportunities on the stage of this conference to piss off half the world’s population?” After the session, young women mobbed Tarter with gratitude.
It's one thing to be a well-known scientist like Tarter, demanding attention at a microphone. But participants have stepped up too, as one audience member did at the World Science Festival in New York in June. There, theoretical physicist Veronika Hubeny found herself surrounded by six men, not given much opportunity to speak for the first hour. "We haven't heard enough from you," the moderator said, and started to ask her a question. But he then repeatedly talked over her to explain string theory (her field) instead of allowing her to answer.
After about three minutes of intermittent interruptions from the moderator, audience member Marilee Talkington shouted: “Let her speak, please!”
The room erupted into clapping and cheers—support that continued after Talkington recalled the account on Facebook. Thanks directed to Tarter multiplied online as well.
But so did the thousand discriminatory papercuts from speakers and organizers. Today, the public record of sexism, at Starmus and the World Science Festival and beyond, reaches past the physical conferences and their chronology—to the postdoc watching the livestream on lunch break, to the student who searches YouTube five years from now to learn about astronauts. Instead of inspiration, they can find a demonstration of just how steep the uphill battle is.
The problems on display at science conferences aren't new. To some extent, they reflect the fundamental gender imbalance in science: The tenured scientific elite has higher male-to-female ratios than the ranks of postdocs and assistant professors. But speaker imbalance still often outstrips that within a field. Self-promotion may amplify the divide: Men on average are more likely to see and sell themselves as important figures (a tendency that shows up on paper, with men citing their own work 56 percent more than women).
So how do you get those numbers to change? If you talk to conference organizers, especially ones with a surfeit of men, they’ll often exclaim (as both Starmus and Microbiome Congress organizers did) that they invited more women. Those women just declined the opportunity! Here's why: They're busy. Conference organizers often have, in their heads, a list of Rock Star Female Scientists to scan through when they need some women. But those rock stars are already attending 55,000 conferences. “You have to invite more women than men because they're being stretched thin,” Bik says.
The good news is that pseudocelebrity scientists aren't the only ones who do robust research and speak comprehensibly. Finding other contributors isn't hard—it just requires looking to different sources.
Bik, for example, maintains a list of women in microbiology who would be happy to give a great keynote speech at your conference. The American Astronomical Society has a similar database. Organizers can also check out this Diversity Distribution Calculator to see how their meetings measure up.
The Battle to Get Gender Identity Into Your Health Records
Women in Data Science Are Invisible. We Can Change That
Trans Researchers Are Struggling to Stay in Science. That Has to Change
The field of microbiology also offers some hope. In 2011, women made up just 27 percent of the speakers at the American Society of Microbiology general meeting. By 2015, the society had bumped that up to nearly 50 percent. How? Researchers from Johns Hopkins University showed organizers numbers from their own meetings: When the committee in charge of speaker selection included at least one woman, sessions had 72 percent more female speakers and were 70 percent less likely to be only male. In response to this and other past-meeting data—and then a call to be better about avoiding all-male panels—conference conveners brought more women into the decision-making, and soon the number of women speaking nearly matched the number of men speaking.
Some conferences set out with the goal of gender parity, and then choose their speaker list accordingly. It requires planning, sure, but Twitter is here to keep scientists from stalling out in their search. The key? Just ask, like neurobiologist Leslie Voshall did a few days ago as she began to schedule talks for 2019.
If an organizer doesn't have enough reach of their own, they can solicit suggestions using hashtags such as #WomenInSTEM or search for lists of science-internet influencers such as the WomenTweetScienceToo rolodex, which popped up after Science put only four female scientists on its top 50 tweeters list. It has 316 badass, smart women who can write 140 informative characters or, you know, wow a weary audience at the 8 am plenary session.
Coordinators can also ping #BlackInSTEM and #QueerInSTEM for speaker suggestions. Because diversity isn't just about women. “Anybody who is a minority will feel the same,” Bik says. “They will look at the podium and wish there was someone there who looked like them.”
Thanks to the internet, there's really no excuse for that wish to go unfulfilled.
We’ve all heard about the Gender Pay Gap, but how prevalent is it? Find out how women compare to men in workplace compensation and what we can do about the glass ceiling.

Directed/Produced by Jared Neumark
Animated by Yoriko Murakami and Kim Blanchette
Shot by Mika Levin
Written by Roya Rastegar
Edited by Mike Russell
In October 2016, the organizers behind a microbiome conference sent promo materials to some prominent scientists. Elisabeth Bik was one of them. With nearly 12,000 followers, her tweets could help publicize their upcoming event in San Diego. But when she scanned the lineup, she noticed that almost every speaker was a man. Add more women, she suggested—or the conference should expect backlash.
She was right: Biologist Jonathan Eisen—“Guardian of Microbial Diversity,” his Twitter bio says—brought the biased list to the attention of his 46,000 followers with a blog post called "The White Men's Microbiome Congress." The organizers, Kisaco Research, added more female speakers before the conference convened and issued a penitent statement.
Bik, who runs the widely read Microbiome Digest, didn’t raise the alarm at the time. “They looked like they were going to do better," she says, "so I didn't want to make a big stink."
But last week she saw the latest speaker list for the Kisaco-organized European Microbiome Congress happening this November: same story. Eisen did, too.
https://twitter.com/phylogenomics/status/889493638002102272
In the past few months, two other high profile science conferences—Starmus and the World Science Festival—have also ignited internet ire for their lack of representation. And websites exist specifically to point out the most egregious examples: There’s Bias Watch Neuro, an All Male Panels Tumblr, and the hashtag #manel. The Gender Avengers, a community dedicated to hearing women's voices in public conversation, asks professionals to pledge not to serve on such panels.
Yes, it's 2017. Yes, this is still happening. “Women tell themselves, ‘Our generation is going to do better. When I'm in my 40s, I'll be the speaker,’” Bik says. “I thought that. It hasn't happened.”
Women have experienced underrepresentation over decades and in different departments of study. And it has real-world repercussions: Who enters science and who rises to the top of a field both have a dramatic impact on the type of research that gets done. But people like Bik, and the online communities around them, are working to make it better next time. Really.
Gender representation has been pretty imbalanced for the VIP-laden Starmus conference's six-year history, in part because organizers pride themselves on inviting fancy people of a specific sort. Nobel laureates, astronauts, Stephen Hawkings—all designations sooted with historical and cultural biases of their own.
Although Starmus still doesn’t have speaker stats to boast about—less than a quarter of main-stage speakers have been women—more female scientific stars appeared at the June meeting in Trondheim, Norway, than in the past. Still, “there got to be an undercurrent in the audience, when you see this stage full of men with a token woman or no one at all," says astronomer Jill Tarter, who has been the only woman on Starmus' board. "It just festered.”
The festering reached a fever pitch during a panel with seven men and zero women, after economist Christopher Pissarides confessed that he had changed Siri to a male voice. You know, because he trusts it more.
When question time came, Tarter commandeered the mic. “I’m wondering,” she said, “why after a beautiful, inspiring lecture by Jeffrey Sachs this morning about [how] we have to solve our problems globally—everybody needs to be in the game—why our very wise, knighted Nobel laureate found two opportunities on the stage of this conference to piss off half the world’s population?” After the session, young women mobbed Tarter with gratitude.
It's one thing to be a well-known scientist like Tarter, demanding attention at a microphone. But participants have stepped up too, as one audience member did at the World Science Festival in New York in June. There, theoretical physicist Veronika Hubeny found herself surrounded by six men, not given much opportunity to speak for the first hour. "We haven't heard enough from you," the moderator said, and started to ask her a question. But he then repeatedly talked over her to explain string theory (her field) instead of allowing her to answer.
After about three minutes of intermittent interruptions from the moderator, audience member Marilee Talkington shouted: “Let her speak, please!”
The room erupted into clapping and cheers—support that continued after Talkington recalled the account on Facebook. Thanks directed to Tarter multiplied online as well.
But so did the thousand discriminatory papercuts from speakers and organizers. Today, the public record of sexism, at Starmus and the World Science Festival and beyond, reaches past the physical conferences and their chronology—to the postdoc watching the livestream on lunch break, to the student who searches YouTube five years from now to learn about astronauts. Instead of inspiration, they can find a demonstration of just how steep the uphill battle is.
The problems on display at science conferences aren't new. To some extent, they reflect the fundamental gender imbalance in science: The tenured scientific elite has higher male-to-female ratios than the ranks of postdocs and assistant professors. But speaker imbalance still often outstrips that within a field. Self-promotion may amplify the divide: Men on average are more likely to see and sell themselves as important figures (a tendency that shows up on paper, with men citing their own work 56 percent more than women).
So how do you get those numbers to change? If you talk to conference organizers, especially ones with a surfeit of men, they’ll often exclaim (as both Starmus and Microbiome Congress organizers did) that they invited more women. Those women just declined the opportunity! Here's why: They're busy. Conference organizers often have, in their heads, a list of Rock Star Female Scientists to scan through when they need some women. But those rock stars are already attending 55,000 conferences. “You have to invite more women than men because they're being stretched thin,” Bik says.
The good news is that pseudocelebrity scientists aren't the only ones who do robust research and speak comprehensibly. Finding other contributors isn't hard—it just requires looking to different sources.
Bik, for example, maintains a list of women in microbiology who would be happy to give a great keynote speech at your conference. The American Astronomical Society has a similar database. Organizers can also check out this Diversity Distribution Calculator to see how their meetings measure up.
The Battle to Get Gender Identity Into Your Health Records
Women in Data Science Are Invisible. We Can Change That
Trans Researchers Are Struggling to Stay in Science. That Has to Change
The field of microbiology also offers some hope. In 2011, women made up just 27 percent of the speakers at the American Society of Microbiology general meeting. By 2015, the society had bumped that up to nearly 50 percent. How? Researchers from Johns Hopkins University showed organizers numbers from their own meetings: When the committee in charge of speaker selection included at least one woman, sessions had 72 percent more female speakers and were 70 percent less likely to be only male. In response to this and other past-meeting data—and then a call to be better about avoiding all-male panels—conference conveners brought more women into the decision-making, and soon the number of women speaking nearly matched the number of men speaking.
Some conferences set out with the goal of gender parity, and then choose their speaker list accordingly. It requires planning, sure, but Twitter is here to keep scientists from stalling out in their search. The key? Just ask, like neurobiologist Leslie Voshall did a few days ago as she began to schedule talks for 2019.
If an organizer doesn't have enough reach of their own, they can solicit suggestions using hashtags such as #WomenInSTEM or search for lists of science-internet influencers such as the WomenTweetScienceToo rolodex, which popped up after Science put only four female scientists on its top 50 tweeters list. It has 316 badass, smart women who can write 140 informative characters or, you know, wow a weary audience at the 8 am plenary session.
Coordinators can also ping #BlackInSTEM and #QueerInSTEM for speaker suggestions. Because diversity isn't just about women. “Anybody who is a minority will feel the same,” Bik says. “They will look at the podium and wish there was someone there who looked like them.”
Thanks to the internet, there's really no excuse for that wish to go unfulfilled.
We’ve all heard about the Gender Pay Gap, but how prevalent is it? Find out how women compare to men in workplace compensation and what we can do about the glass ceiling.

Directed/Produced by Jared Neumark
Animated by Yoriko Murakami and Kim Blanchette
Shot by Mika Levin
Written by Roya Rastegar
Edited by Mike Russell
In March, when Netflix began streaming its original teen suicide mystery series 13 Reasons Why, it took a few days for people to start freaking out. But soon, schools started sending home notes warning parents about the show’s graphic depictions of suicide and rape. Psychologists wrote op-eds denouncing its disregard for the World Health Organization’s suicide portrayal guidelines. News outlets published more than 600,000 stories about it. And then there was Twitter.
People tweeted about the show more than 11 million times in the three weeks following its premiere (more than any other show so far in 2017). They wanted to know why the show never touched on the subject of mental illness and why it chose to turn teenage turmoil into a glorified scavenger hunt. And they worried that the show would spread a suicide contagion, inspiring copycats and nudging those close to the edge.
Into this fray entered John W. Ayers, a computational epidemiologist who studies public health at San Diego State University. He saw a chance to inject some empiricism into the controversy.
“There was a tremendous amount of debate going on all based on deeply personal experiences that wasn’t going anywhere,” he said. “We saw a need for real data.”
So on Monday, Ayers and his collaborators at Johns Hopkins, the University of Washington, and the University of Southern California published the first data on whether 13 Reasons Why is bad for public health. And according to them, the answer is yes. But that’s not necessarily cause to cancel your Netflix subscription.
Ayers leads a multidisciplinary team of scientists who take big data not traditionally used in public health and turn them into rapid insights. They’ve done things like use Twitter data to call attention to a spike in distracted driving incidents thanks to Pokémon Go players behind the wheel. They realized they could help settle the debate over 13 Reasons Why by looking at how the show influenced what people were searching for on Google. For one, it was easier data to get than death certificates (the CDC maintains a free database of deaths by cause, but it only has records through 2015). And previous studies have found correlations between suicide search trends and suicide rates.
So Ayers and his colleagues grabbed search queries from the US between March 31, 2017, the series’ release date, and April 18, a date the team selected as a cutoff because news of former NFL player Aaron Hernandez’s prison suicide might have contaminated the results otherwise.
They looked at all searches containing the word “suicide,” except for those accompanied by the word “squad,” for obvious reasons. And they also analyzed common queries related to but not mentioning suicide. Then they compared the results to a hypothetical scenario over the same time frame had 13 Reasons Why never been released, based on forecasts using historical search trends.
What they found was kind of shocking. Suicide queries went up nearly 20 percent in the 19 days following the show’s release—between 900,000 and 1.5 million more suicide related searches than expected. At first glance, it looked like exactly what the show’s creators wanted—a massive increase in awareness and people engaging with the subject. But, in this case, the details seem to matter as much as the overall impression: of the 16 queries with the biggest spikes, about a third of them were for specific methods for ending one’s life.
“If it was truly raising awareness, we’d see a very different outcome,” Ayers said. While they observed an uptick in queries related to getting help (hotlines, prevention services), it was accompanied by search terms like “quick suicide,” “painless suicide,” and “how to kill yourself.” “That’s troubling,” Ayers said. “Even with the best of intentions, it’s clear this show has real world consequences.”
The Stranger Things That Turned 13 Reasons Why Into Netflix's Biggest Sleeper Hit
Social Networks May One Day Diagnose Disease—But at a Cost
Artificial Intelligence Is Learning to Predict and Prevent Suicide
Not everyone is so sure. The obvious flaw with search data is you can’t attribute intention to every query, said Sean D. Young, who heads up both the UCLA Center for Digital Behavior and the University of California’s Institute for Prediction Technology. Because Google starts automatically populating search boxes, anyone who takes to the search engine out of pure curiosity might wind up spiraling toward suicide voyeurism. Is that fascination morbid? Sure. But it doesn’t necessarily equal suicidal ideation. “It helps you pick up what people have learned about suicide from the show,” Young said. “But it’s important to not get freaked out that the show is causing people to go kill themselves. I don’t see the evidence for that.”
Especially because you could make the same case for anything that draws attention to 13 Reasons Why, like, say, this article, or Ayers’ study. Will suicide-related search terms get a little bump along with the study's publication? Probably. But it’s pretty hard to go from that to saying it will have caused the loss of human life.
Now, that’s not to say data from search engines, social media platforms, or even wearables can’t be useful for public health surveillance. Both Young and Ayers are building tools that accurately monitor and predict health issues. Both researchers are already collaborating with federal agencies like the Centers for Disease Control and the Bureau of Alcohol, Tobacco and Firearms to validate these models. In a few years, you could imagine the CDC spotting a blip in the matrix—a movie or a show or a video game triggering some kind of public health scare—and stepping in before the damage is done.
For now though, the show will go on. It’s scheduled for a second season starting in 2018, and began shooting in California this summer despite calls from people like Ayers to put production on hold. There’s no word from Netflix on whether or not it plans to implement any changes, either to the series’ trigger warnings, helpline callouts, or the on-camera content itself. (The company declined to comment for this story). In the meantime, 13 Reasons Why is still available on demand to anyone with a Netflix login. And that final scene, yeah, you can rewind it and watch it as many times as you want, even if you probably shouldn’t.
For a few months in the fall of 1957, citizens of Earth could look up and see the first artificial star. It shone as bright as Spica, but moved across the sky at a much faster clip. Lots of people thought they were seeing Sputnik—Russia’s antennaed, spherical satellite, and the first thing humans had flung into orbit. But it wasn’t: It was the body of the rocket that bore Sputnik to space—and Earth’s first piece of space junk.
Space junk is the colloquial name for orbital bits that do nothing useful: spent rockets, fragments splayed by collisions and degradation, old satellites no one cares about anymore. In total, they amount to millions of pieces of debris, many of which are large enough to seriously ding satellites and the International Space Station. And then there's Kessler Syndrome: a space sickness in which low-Earth orbit is so overpopulated that collisions cascade into more collisions, which create more debris that causes more collisions that cascade into more collisions. It's all very bad for Sandra Bullock. And it's about to get worse: Thousands and thousands of satellites are set to launch to low-Earth orbit before 2025.
This is not a new problem: Since Sputnik, Russians and non-Russians have already launched thousands of satellites: orbiters that send you Game of Thrones, track global climate, and even track you. Nation-states have developed systems to know where they are and where they're going, along with all their leftover launch paraphernalia. But it will get much more complicated when the Smallsat Revolution fully arrives.
In the US, two governmental offices share that daunting task: a NASA group and the military’s US Strategic Command—USSTRATCOM, if you feel like yelling an acronym that will automatically make you sound like a sergeant—keeps track of 24,000 objects, down to about 10 centimeters in size. Some 18,700 of these are publicly listed at Space Track (the rest are for the Department of Defense to know and you never to find out). Stratcom gets intel from its Space Surveillance Network—a murder of optical and radar instruments that senses space objects. All that data sluices into uber-bunker Cheyenne Mountain, which processes their streams.
Those teams know even more data is coming, as the number of active satellites in orbit grows by at least tenfold by the mid 2020s. So they are working on new sensors, says the command's Brian Maguire. A radar system called the "S-Band Space Fence," located in the Kwajalein Atoll in the Marshall Islands, will operate at high, microwave frequencies (the S-band part), allowing it to detect smaller objects. Microwaves for microsats (and micro trash), in other words.
The US and Australia also recently spooled up a C-band (even higher-frequency) radar in March of this year, which sees space objects in even finer detail. The military plans to do more of that: share, team up with companies and other countries that run their own "space situational awareness" operations. And that's a good thing: Last year, the 18th Space Control Squadron had to deliver 3,995,874 pieces of potentially bad news: “conjunction data messages” that let satellite owners know some other sat or sizable junk was threatening theirs.
The Department of Defense lets NASA sweat the small stuff, though. The space agency formed its Orbital Debris Program Office in 1979, and that office is responsible for characterizing objects too small to be tracked by the Air Force but big enough to cause problems. NASA uses two radar systems operated by MIT’s Lincoln Laboratory for about 1,000 hours a year to understand the population of millimeter-to-centimeter-sized objects in low-Earth orbit. That schedule that will likely get busier, along with the number of close encounters.
Just how much bigger will the problem get? SpaceX alone plans to send up nearly 12,000 small internet-beaming objects over time. OneWeb has designs on some 700 similar sats. Planet just launched around 100 that take pictures of the Earth’s entire landmass every day. And those are just the heaviest hitters. Little orbiters—especially the smallest types, CubeSats and NanoSats—are within reach of research scientists, government agency experiments, smaller companies, and even individual humans. Take the private Breakthrough Starshot project, which eventually plans to send diminutive spacecraft to Alpha Centauri star system (really). It just launched six "Sprites": the world's smallest satellites, measuring 3.5 centimeters on a side.
All of those satellite operators are in charge of making sure what they sent up comes back down, in a timely way. Bigsat operators can just use the last of their fuel to plunge their darlings toward the ocean. But many smallsats, especially the smallest kinds, don’t have propulsion systems. To naturally “de-orbit” fast, they have to be in an orbit that naturally decays quickly—an ellipse in which atmospheric drag drags them back to Earth fast.
The Mad Plan to Clean Up Space Junk With a Laser Cannon
Gecko-Inspired Gripper May Soon Snag Space Junk
The 12 Greatest Challenges for Space Exploration
Some smallsat operators are planning to put propulsion systems aboard. Great! But that poses another problem: explosions. If there’s pressurized fuel, there’s always the possibility for paroxysm. Which means more debris. And problematically, most members of a given smallsat constellation have the same exact specs—and so the same flaws. If one sat's propulsion system has critical personality defects, so too do its identical siblings. When that happens with cars, automakers recall them. When that happens in space, a bunch of satellites can explode. And that's not just true for their movement-making parts: It's also true of satellite batteries. Just ask Samsung.
The international timeline for self-destruction, originally set by NASA’s Orbital Debris Program Office, is that 25 years after the operational life of a satellite ends, it should burn up in the atmosphere. That's the goal for new launchers, to limit how much bigger they make the pile of space trash. NASA calls it "mitigation."
But you can't admonish all those bolts and rocket cores and paint chips and, you know, junk that's already out there. And you can't guilt non-operational satellites into getting down from there (you can take them down, but more on that later). The idea, at this point, is just to make what's bad only as worse as necessary.
On top of that, countries don't have to enforce the 25-year guideline. In 2015, 35 percent of satellites were out of compliance. Of all CubeSats, specifically, launched between 2003 and 2014, a fifth didn’t meet the criteria. Know why? It's hard. It costs money. And, for the most part, no one makes them do it.
There's good news, though, for at least the heftiest of the coming smallsat herds: "Both SpaceX and OneWeb have indicated at that end of the operation of their spacecraft, they plan to lower the orbits so the orbits will naturally decay in [less than] five years rather than 25 years,” says the program's J.C. Liou. But that still leaves unregulated rogues up there, endlessly circling, dead and dangerous.
Slacker satellites—ones that haven't prepared for mitigation—get a bit of a reprieve, thanks to some quirky orbital properties. “Like a river, like the atmosphere, space can clean itself,” says Lisa Ruth Rand, a science historian writing a book about space junk. During solar maximum, which happens every 11 years, the thermosphere heats up and expands, pressing its particles against objects in near-space, where many small satellites operate. That creates extra friction. “It drags them back to the atmosphere,” says Rand.
But there's always gotta be equal-and-opposite bad news, right?
Recent research suggests that human-added CO2 is cooling the thermosphere, and so contracting it. “This contraction, in turn, will reduce atmospheric drag on satellites and may have adverse consequences for the orbital debris environment that is already unstable,” wrote the authors of a 2012 Nature Geoscience paper.
Since Earth is having a harder time taking satellites down, governments and companies may instead opt for more “remediation”: forcing satellites to fall. It's something experts from space agencies in the Inter-Agency Space Debris Coordination Committee have said will need to happen regardless of how well newsat makers behave.
NASA did not want to talk about this option. But a 2006 study by Liou himself showed that if no one launched anything else, which Elon would never agree to, collisions would increase the number of debris pieces 10-centimeter and larger, even considering objects that de-orbit, starting around 2055. And that was using the satellite population 11 years ago.
But remediation remains politically fraught, which is likely why NASA stayed mum. In 2007, for instance, China decided to de-orbit one of its defunct weather satellites ... by firing a missile at it. That certainly took the sat out of its path—but it also created a flume of debris that flung toward the Space Station in 2011.
In February 2008, the US Navy launched its own projectile at a spy satellite toward its own satellite. The government claimed to worry that if it let the satellite fall back intact, its hydrazine fuel could release toxic vapors at breathing level. But some, at the time and still, interpret the action militarily. “That exchange right there was a ‘Hey, look at us,’” says Rand. “China is saying, ‘We can take down a satellite,’ and the US is saying, ‘We can do it, too.’”
That, of course, is gun-flexing: It implies, “…and we can also take down yours.”
Not all remediation has to be so violent. Scientists have proposed focusing the sun's radiation onto small junk, and vaporizing it, in the adultspace version of the magnifying-glass-and-ant gig. The European Space Agency's e.Deorbit program would like to grab junk—with a net or robotic arm—and then send it to its fiery end in the atmosphere. One could also use a space harpoon.
However the spacefarers of the world choose to bring down their satellites—naturally, or with lethal force—one thing is clear: You should keep an eye out for the synthetic meteor showers of the future, as the CubeSats and paint chips of near-space rain back down onto the atmosphere.
NASA's DSCOVR satellite is sending back some of the most amazing images of earth ever seen, including the moon passing over our planet, beamed back from a million miles away. DSCOVR isn't just out there making pretty pictures, it's also an advanced warning system for potentially dangerous space weather.
Woe be to the Environmental Protection Agency. If President Trump gets his way, the federal agency will lose 31 percent of its annual budget—about $3 billion. Supporters of Trump’s 2018 budget proposal call it a “back to basics” approach, carving away what they see as the agency’s regulatory overreach. Opponents are similarly pithy: The EPA’s former director labeled Trump’s proposal a “scorched Earth budget.”
Disagreements over the utility of the EPA's programs typically fall along the partisan lines you might expect. But not all of them.
At least one of Trump's proposed budget cuts targets a program that some scientists have been questioning for decades: residential radon risk. That program, which sets the standards for risk and protection from radon gas, would be completely zeroed out. Now, according to the EPA, the WHO, and many other big public health organizations, radon is second only to cigarette smoking as a leading cause for lung cancer. The EPA says radon gas causes 21,000 deaths every year. Yet some critics in the health care profession say that’s all baloney.
Trump's budget hasn't passed. It still has to survive the congressional gauntlets, full of many senators and representatives who have already voiced opposition to many of the EPA cuts, including to the radon program. But some scientists might actually want the EPA to back off on this one.
The issue here isn't whether radon causes cancer. It does, certainly. But scientists can only prove that radon is carcinogenic at high doses. Like, rates typically found in mine shafts, not suburban homes. Critics say the EPA's method for establishing low-dose radon risk assessments is simply unscientific. Some even go so far as to say low doses of radiation might help your body fight cancer. Follow that logic through, and they say the EPA's low dose radon risk assessment could actually be harming people.
Radon is radioactive gas emitted when uranium decays. And because most rocks contain traces of uranium, it leaks from the soil virtually everywhere on Earth. Like uranium, radon decays, too. If it does this in your lungs, the alpha particles it releases can harm the DNA in your cells. "When we talk about radon risk, we really mean the risk from radon and its decay products," says Phillip Price, a physicist at Lawrence Berkeley National Laboratory. If enough DNA gets damaged, the mutations pile up and … lung cancer.
Radon is everywhere, but it doesn't usually pose much of a risk because it disperses into the open air. "What’s unnatural are the radon levels that accumulate indoors,” says James McLaughlin, president of the European Radon Association.
Scientists began to recognize the threat in the 1970s, when epidemiologists found that miners were getting lung cancer at higher-than-average rates. The tunnels they worked in were full of radon gas. But the agency's real work started after 1984, when an engineer at the Limerick Nuclear Power Plant in Pennsylvania set off radiation alarms as he was coming in to work. After some sleuthing, public health experts figured out that his house was filled with radon. Like, uranium mine levels of radon. This set off a national panic—homeowners feared their home might have similarly high levels.
By 1986, the EPA established an indoor radon risk exposure level—4 picocuries per liter of air. It's similar to that recommended by the World Health Organization and the European Union. That number isn't tied to any regulations: Instead, the EPA recommends that people test their homes (which can cost between $20 and $200) and take action if necessary—say, by installing fans and vents to improve airflow. Hiring a contractor to ventilate your basement can run into the thousands.
That number, 4 picocuries, is a funny one. Because here's the thing: The scientific model used to come up with that limit says there's no acceptable level of radon exposure. None.
It's called the linear no-threshold model, and it's used to estimate risk for low doses of carcinogenic materials. See, it's not hugely controversial to link lung cancer to high radon levels, like those found in a mineshaft. But it's impossible to directly establish a link between low levels of radon and cancer. There are just too many other possible carcinogens, like air pollution, second hand smoke, and even random DNA mutations.
So, the linear no-threshold model takes the cancer rates in highly-exposed populations—those uranium miners—and extends them to lower doses of radiation. It then multiplies that very small risk across a huge number of people in order to estimate the total number of future cancers. That's how the EPA estimated that radon causes 21,000 annual lung cancer deaths.
Radiation risk professionals use the linear no-threshold model to determine the exposure risk from all sorts of other radioactive and carcinogenic material. "The idea is that twice as much radiation means twice as many cells with damaged DNA, and if each cell with damaged DNA has a certain chance of initiating a cancer, then ceteris paribus you have the linear no-threshold model," says Price. "That's not crazy."
But if there's no threshold below which radon doesn't increase cancer risk, how did the EPA settle on 4 picocuries per liter of indoor air as its recommended exposure limit? For one thing, radon becomes both more difficult to detect and remove at lower doses. That translates into dollars. When the EPA established the 4 picocurie action level in 1986, it estimated each saved life cost around $700,000. Lower that to 3 picocuries, and the cost per life more than doubles, to $1.7 million. At 2 picocuries, $2.4 million.
Critics contend that money probably isn't saving anyone. "I personally think the linear no-threshold model is over conservative, and does waste a lot of money that could be better spent," says Cynthia McCollough, a radiologist. "Using the linear no-threshold model to predict future cancers is very bad science, and all radiation protection agencies agree on that." In fact, the Health Physics Society (a large scientific organization for radiation safety specialists) has officially stated that radon produces no statistically measurable health effects until levels reach about 27 billion picocuries. Even the EPA itself, in its 2003 assessment of household radon risk, seemed to agree: "The [committee tasked with establishing radon risk] adopted the linear no-threshold assumption based on our current understanding of the mechanisms of radon-induced lung cancer, but recognized that this understanding is incomplete and that therefore the evidence for this assumption is not conclusive." (Emphasis added.)
Despite that admission, the EPA went ahead with the linear no-threshold model for radon out of an abundance of caution. But some critics say the agency's overprotective action might actually be harming people. "The linear no-threshold model is based on the concept that increased radiation and increased mutations cause cancer. That assumption is fundamentally wrong," says Mohan Doss, a physician at the Fox Chase Cancer Center in Philadelphia. That's because, Doss says, it ignores biology.
Cancer Rates Spiked After Fukushima. But Don't Blame Radiation
How Hiroshima Survivors Are Leaving a Legacy For Science
Space Radiation Remains Major Hazard for Humans Going to Mars
Humans develop DNA mutations all the time. Most of them do not turn into cancer. One theory why, says Doss, is that the immune system moves in and gets rid of those afflicted cells early on. He goes so far as to say that low-dose radiation might prevent lung cancer. Doss points to animal studies showing that low doses of radiation might help the body fight cancer by activating the immune system to clear away mutated DNA. "These boosted defenses actually reduce the DNA damage that would have occurred in absence of radon exposure," he says. If he's right, that means the EPA's guidance levels aren't preventing cancer—they're causing it.
So, why doesn't the EPA listen to these low limit soldiers? Probably because organizations like it tend to be pretty risk-averse. Under the precautionary principle, it’s better to overprotect than under-protect. And the total indoor radon program in the EPA's 2017 budget called for just $3.5 million. Also, remember how hard it is to establish radiation risk at low levels? That sword cuts both ways. "There’s some shaky empirical evidence for a weak protective effect at 2 picocuries per liter compared to 0," says Price. But whether you're looking at health benefits or risks, all the evidence is pretty shaky below an exposure of about 8 picocuries.
Finally, consider inertia. The linear no-threshold model underpins not just the EPA's radon recommendation, but its risk exposure estimates for other radiation sources.
Most of the experts I spoke with didn't have a good idea about what effect, if any, Trump's promise to nix the EPA's radon program might have. McCollough said the real hit happened under Obama's watch, when the Department of Energy's Office of Science killed a program studying the biological effects of low dose radiation.
At least one, though, is rooting for the program's demise. "If the Trump administration nixes the EPA radon program, there would be little adverse impact on public health," says Doss. "But in fact there would be improvement of public health, by reducing lung cancers." Talk about scorched Earth.
Your grandparents might have warned you that cellphones cause brain cancer. Well, that's not at all what science says.
The biophysicist Jeremy England made waves in 2013 with a new theory that cast the origin of life as an inevitable outcome of thermodynamics. His equations suggested that under certain conditions, groups of atoms will naturally restructure themselves so as to burn more and more energy, facilitating the incessant dispersal of energy and the rise of “entropy” or disorder in the universe. England said this restructuring effect, which he calls dissipation-driven adaptation, fosters the growth of complex structures, including living things. The existence of life is no mystery or lucky break, he told Quanta in 2014, but rather follows from general physical principles and “should be as unsurprising as rocks rolling downhill.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Since then, England, a 35-year-old associate professor at the Massachusetts Institute of Technology, has been testing aspects of his idea in computer simulations. The two most significant of these studies were published this month—the more striking result in the Proceedings of the National Academy of Sciences and the other in Physical Review Letters. The outcomes of both computer experiments appear to back England’s general thesis about dissipation-driven adaptation, though the implications for real life remain speculative.
“This is obviously a pioneering study,” Michael Lässig, a statistical physicist and quantitative biologist at the University of Cologne in Germany, said of the PNAS paper written by England and an MIT postdoctoral fellow, Jordan Horowitz. It’s “a case study about a given set of rules on a relatively small system, so it’s maybe a bit early to say whether it generalizes,” Lässig said. “But the obvious interest is to ask what this means for life.”
The paper strips away the nitty-gritty details of cells and biology and describes a simpler, simulated system of chemicals in which it is nonetheless possible for exceptional structure to spontaneously arise—the phenomenon that England sees as the driving force behind the origin of life. “That doesn’t mean you’re guaranteed to acquire that structure,” England explained. The dynamics of the system are too complicated and nonlinear to predict what will happen.
The simulation involved a soup of 25 chemicals that react with one another in myriad ways. Energy sources in the soup’s environment facilitate or “force” some of these chemical reactions, just as sunlight triggers the production of ozone in the atmosphere and the chemical fuel ATP drives processes in the cell. Starting with random initial chemical concentrations, reaction rates and “forcing landscapes”—rules that dictate which reactions get a boost from outside forces and by how much—the simulated chemical reaction network evolves until it reaches its final, steady state, or “fixed point.”
Often, the system settles into an equilibrium state, where it has a balanced concentration of chemicals and reactions that just as often go one way as the reverse. This tendency to equilibrate, like a cup of coffee cooling to room temperature, is the most familiar outcome of the second law of thermodynamics, which says that energy constantly spreads and the entropy of the universe always increases. (The second law is true because there are more ways for energy to be spread out among particles than to be concentrated, so as particles move around and interact, the odds favor their energy becoming increasingly shared.)
But for some initial settings, the chemical reaction network in the simulation goes in a wildly different direction: In these cases, it evolves to fixed points far from equilibrium, where it vigorously cycles through reactions by harvesting the maximum energy possible from the environment. These cases “might be recognized as examples of apparent fine-tuning” between the system and its environment, Horowitz and England write, in which the system finds “rare states of extremal thermodynamic forcing.”
Living creatures also maintain steady states of extreme forcing: We are super-consumers who burn through enormous amounts of chemical energy, degrading it and increasing the entropy of the universe, as we power the reactions in our cells. The simulation emulates this steady-state behavior in a simpler, more abstract chemical system and shows that it can arise “basically right away, without enormous wait times,” Lässig said—indicating that such fixed points can be easily reached in practice.
Many biophysicists think something like what England is suggesting may well be at least part of life’s story. But whether England has identified the most crucial step in the origin of life depends to some extent on the question: What’s the essence of life? Opinions differ.
England, a prodigy by many accounts who spent time at Harvard, Oxford, Stanford and Princeton universities before landing on the faculty at MIT at 29, sees the essence of living things as the exceptional arrangement of their component atoms. “If I imagine randomly rearranging the atoms of the bacterium—so I just take them, I label them all, I permute them in space—I’m presumably going to get something that is garbage,” he said earlier this month. “Most arrangements [of atomic building blocks] are not going to be the metabolic powerhouses that a bacterium is.”
It’s not easy for a group of atoms to unlock and burn chemical energy. To perform this function, the atoms must be arranged in a highly unusual form. According to England, the very existence of a form-function relationship “implies that there’s a challenge presented by the environment that we see the structure of the system as meeting.”
But how and why do atoms acquire the particular form and function of a bacterium, with its optimal configuration for consuming chemical energy? England hypothesizes that it’s a natural outcome of thermodynamics in far-from-equilibrium systems.
The Nobel-Prize-winning physical chemist Ilya Prigogine pursued similar ideas in the 1960s, but his methods were limited. Traditional thermodynamic equations work well only for studying near-equilibrium systems like a gas that is slowly being heated or cooled. Systems driven by powerful external energy sources have much more complicated dynamics and are far harder to study.
The situation changed in the late 1990s, when the physicists Gavin Crooks and Chris Jarzynski derived “fluctuation theorems” that can be used to quantify how much more often certain physical processes happen than reverse processes. These theorems allow researchers to study how systems evolve—even far from equilibrium. England’s “novel angle,” said Sara Walker, a theoretical physicist and origins-of-life specialist at Arizona State University, has been to apply the fluctuation theorems “to problems relevant to the origins of life. I think he’s probably the only person doing that in any kind of rigorous way.”
Coffee cools down because nothing is heating it up, but England’s calculations suggested that groups of atoms that are driven by external energy sources can behave differently: They tend to start tapping into those energy sources, aligning and rearranging so as to better absorb the energy and dissipate it as heat. He further showed that this statistical tendency to dissipate energy might foster self-replication. (As he explained it in 2014, “A great way of dissipating more is to make more copies of yourself.”) England sees life, and its extraordinary confluence of form and function, as the ultimate outcome of dissipation-driven adaptation and self-replication.
However, even with the fluctuation theorems in hand, the conditions on early Earth or inside a cell are far too complex to predict from first principles. That’s why the ideas have to be tested in simplified, computer-simulated environments that aim to capture the flavor of reality.
In the Physical Review Letters paper, England and his coauthors Tal Kachman and Jeremy Owen of MIT simulated a system of interacting particles. They found that the system increases its energy absorption over time by forming and breaking bonds in order to better resonate with a driving frequency. “This is in some sense a little bit more basic as a result” than the PNAS findings involving the chemical reaction network, England said.
We need chemical reaction networks that can get up and walk away from the environment where they originated.
Crucially, in the latter work, he and Horowitz created a challenging environment where special configurations would be required to tap into the available energy sources, just as the special atomic arrangement of a bacterium enables it to metabolize energy. In the simulated environment, external energy sources boosted (or “forced”) certain chemical reactions in the reaction network. The extent of this forcing depended on the concentrations of the different chemical species. As the reactions progressed and the concentrations evolved, the amount of forcing would change abruptly. Such a rugged forcing landscape made it difficult for the system “to find combinations of reactions which are capable of extracting free energy optimally,” explained Jeremy Gunawardena, a mathematician and systems biologist at Harvard Medical School.
Yet when the researchers let the chemical reaction networks play out in such an environment, the networks seemed to become fine-tuned to the landscape. A randomized set of starting points went on to achieve rare states of vigorous chemical activity and extreme forcing four times more often than would be expected. And when these outcomes happened, they happened dramatically: These chemical networks ended up in the 99th percentile in terms of how much forcing they experienced compared with all possible outcomes. As these systems churned through reaction cycles and dissipated energy in the process, the basic form-function relationship that England sees as essential to life set in.
Experts said an important next step for England and his collaborators would be to scale up their chemical reaction network and to see if it still dynamically evolves to rare fixed points of extreme forcing. They might also try to make the simulation less abstract by basing the chemical concentrations, reaction rates and forcing landscapes on conditions that might have existed in tidal pools or near volcanic vents in early Earth’s primordial soup (but replicating the conditions that actually gave rise to life is guesswork). Rahul Sarpeshkar, a professor of engineering, physics and microbiology at Dartmouth College, said, “It would be nice to have some concrete physical instantiation of these abstract constructs.” He hopes to see the simulations re-created in real experiments, perhaps using biologically relevant chemicals and energy sources such as glucose.
But even if the fine-tuned fixed points can be observed in settings that are increasingly evocative of life and its putative beginnings, some researchers see England’s overarching thesis as “necessary but not sufficient” to explain life, as Walker put it, because it cannot account for what many see as the true hallmark of biological systems: their information-processing capacity. From simple chemotaxis (the ability of bacteria to move toward nutrient concentrations or away from poisons) to human communication, life-forms take in and respond to information about their environment.
To Walker’s mind, this distinguishes us from other systems that fall under the umbrella of England’s dissipation-driven adaptation theory, such as Jupiter’s Great Red Spot. “That’s a highly non-equilibrium dissipative structure that’s existed for at least 300 years, and it’s quite different from the non-equilibrium dissipative structures that are existing on Earth right now that have been evolving for billions of years,” she said. Understanding what distinguishes life, she added, “requires some explicit notion of information that takes it beyond the non-equilibrium dissipative structures-type process.” In her view, the ability to respond to information is key: “We need chemical reaction networks that can get up and walk away from the environment where they originated.”
Gunawardena noted that aside from the thermodynamic properties and information-processing abilities of life-forms, they also store and pass down genetic information about themselves to their progeny. The origin of life, Gunawardena said, “is not just emergence of structure, it’s the emergence of a particular kind of dynamics, which is Darwinian. It’s the emergence of structures that reproduce. And the ability for the properties of those objects to influence their reproductive rates. Once you have those two conditions, you’re basically in a situation where Darwinian evolution kicks in, and to biologists, that’s what it’s all about.”
Eugene Shakhnovich, a professor of chemistry and chemical biology at Harvard who supervised England’s undergraduate research, sharply emphasized the divide between his former student’s work and questions in biology. “He started his scientific career in my lab and I really know how capable he is,” Shakhnovich said, but “Jeremy’s work represents potentially interesting exercises in non-equilibrium statistical mechanics of simple abstract systems.” Any claims that it has to do with biology or the origins of life, he added, are “pure and shameless speculations.”
Even if England is on the right track about the physics, biologists want more particulars—such as a theory of what the primitive “protocells” were that evolved into the first living cells, and how the genetic code arose. England completely agrees that his findings are mute on such topics. “In the short term, I’m not saying this tells me a lot about what’s going in a biological system, nor even claiming that this is necessarily telling us where life as we know it came from,” he said. Both questions are “a fraught mess” based on “fragmentary evidence,” that, he said, “I am inclined to steer clear of for now.” He is rather suggesting that in the tool kit of the first life- or proto-life-forms, “maybe there’s more that you can get for free, and then you can optimize it using the Darwinian mechanism.”
Sarpeshkar seemed to see dissipation-driven adaptation as the opening act of life’s origin story. “What Jeremy is showing is that as long as you can harvest energy from your environment, order will spontaneously arise and self-tune,” he said. Living things have gone on to do a lot more than England and Horowitz’s chemical reaction network does, he noted. “But this is about how did life first arise, perhaps—how do you get order from nothing.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Adam Russell, an anthropologist and program manager at the Department of Defense’s mad-science division Darpa, laughs at the suggestion that he is trying to build a real, live, bullshit detector. But he doesn’t really seem to think it’s funny. The quite serious call for proposals Russell just sent out on Darpa stationery asks people—anyone! Even you!—for ways to determine what findings from the social and behavioral sciences are actually, you know, true. Or in his construction: “credible.”
Even for Darpa, that’s a big ask. The DoD has plenty of good reasons to want to know what social science to believe. But plenty more is at stake here. Darpa’s asking for a system that can solve one of the most urgent philosophical problems of our time: How do you know what’s true when science, the news, and social media all struggle with errors, advertising, propaganda, and lies?
Take a scientific claim. Do some kind of operation on it. Determine whether the claim is right enough to act on. So ... a bullshit detector?
“I wouldn’t characterize it that way, and I think it’s important not to,” Russell says. He doesn’t want to contribute to cynicism that lets people think if scientists admit uncertainty, that means they can’t be trusted. “I have a deep faith that there is real science. It’s not that we know nothing about the world.” Science is still the best way of knowing stuff. Darpa just wants to know what stuff science is really sure about, and how it knows it. And how it knows it knows it.
You can imagine why Darpa and the DoD might want to shore up the social sciences. They want to understand how collective identity works, or why some groups (and nations) are stable and some fall apart. The military would like to get a better handle on how humans team up with machines before the machines get smarter and more get deployed. How does radicalization work, especially online? Why do people cooperate sometimes and compete at others? All these questions have two things in common: They are super-important to national security, and no one knows the answer.
The people who are supposed to figure out those knotty issues out have their own problems. You might have heard about the “reproducibility crisis,” the concern that many scientific findings, particularly in psychology and sociology, don’t pass a fundamental test of validity—that subsequent researchers can do the same experiment and get the same results as the first ones. Or you might be familiar with “P-hacking” and other ways some researchers, under pressure to publish and get grants, cherry-pick their experimental results to ensure the appearance of statistical significance.
This is not about whether any one particular claim can be replicated, right? It’s that collectively the claims don’t make sense.
Duncan Watts, Microsoft Research
Those issues come up in Darpa’s call for proposals, but researchers acknowledge that the concerns don’t end there. “If you ask a bunch of social scientists how organizations work, you’re not just going to get 20 different answers. You’re going to get answers not even comparable to each other,” says Duncan Watts, a sociologist at Microsoft Research who wrote a blistering critique of the social sciences’ (as he terms it) incoherency problem in the January 2017 issue of Nature Human Behavior. “You read one paper and then another paper, and it’s got the same words in the title but different units of analysis, different theoretical constructs, entirely different notions of causality. By the time you’ve done a literature review, you’re completely confused about what on Earth you even think. This is not about whether any one particular claim can be replicated, right? It’s that collectively the claims don’t make sense.”
But … Darpa, though, right? Impossible problems! Here’s an internet we made you! Darpa! The agency has an overarching program called Next Generation Social Science, set up in 2016 to use economics, sociology, anthropology, and so on to better understand anything from terrorism to the spread of propaganda online. And, yes, it’s an impossible problem. “In emerging fields you begin to see the development of standards as a good signal that something’s happening there,” Russell says. “We certainly don’t have those standards in social sciences.”
So Darpa wants to build them. “Confidence Levels for the Social and Behavioral Sciences” is the formal title of the agency’s “request for information,” the bureaucratic talk for “we have some grant money to award; send us your pitches.” But this RFI is capacious in its ambition, going well beyond reproducibility. It name-checks other bulwarks of scientific validation—peer review, meta-analyses, statistical techniques, and even more modern approaches like impact factors, citation webs, and expert prediction markets. But only to say, incorporate these and surpass them. Great stuff, guys, really great stuff. Anything else to pitch?
From the document: “There may be new ways to create automated or semi-automated capabilities to rapidly, accurately, and dynamically assign Confidence Levels to specific SBS results or claims.” (“SBS” = “social and behavioral sciences”) Help experts and non-experts separate scientific wheat from wrongheaded chaff using “machine reading, natural language processing, automated meta-analyses, statistics-checking algorithms, sentiment analytics, crowdsourcing tools, data sharing and archiving platforms, network analytics, etc.”
Clearly what we need here is some sort of machine with, like, a slot for feeding in journal articles. And two lights on the front: red and green. Ping or bzzzt.
Yeah, but no. “I think we’re many years from that,” says Matthew Salganik, a sociologist at Princeton who isn’t planning to submit an idea to Darpa but works on related validation issues. Though he’ll allow: “Something that might be more possible would be warning lights for papers from a relatively small number of journals.” Maybe just restrict the corpus to the Big Three—Science, Nature, and Proceedings of the National Academy of Sciences.
Really, though, no one knows what an answer will look like. In fact, one of the first people to submit a response to the Darpa RFI is asking for money to sketch one out. “The key challenge of doing this is there isn’t a gold standard for credibility. We don’t have a benchmark,” says University of Virginia psychologist Brian Nosek, head of the Center for Open Science and one of the main players in the fight for reproducibility. Lots of people say they have ways to validate scientific results, Nosek says. “So you have to play them against each other. We think all of these ideas say something about credibility, so let’s start comparing them.”
The key challenge of doing this is there isn’t a gold standard for credibility. We don’t have a benchmark.
Brian Nosek, Center for Open Science
Nosek’s pitch is titled “Path to Iterative Confidence Level Evaluation,” or, charmingly, “Pickle.” It proposes that Darpa set up, in classic fashion, a competition: Let people with credibility-assessing models test them against specific corpuses, like replicated studies. Then he’d build what’s called a nomological network, pitting the ideas against each other. “The only way to develop confidence in the evidence is to look at the problem in lots of different ways and see where you start to get convergence,” Nosek says. In other words, establish a framework for establishing the credibility of establishing credibility. “It’s very meta,” he says.
Other fields of science have their own problems with replicability and reliability. (The cancer people are freaking right out.) But the social sciences have their own particular epistemological problem. Darpa’s big obstacle here might not be social science answers but social science questions. “Computer scientists are more accustomed to asking questions where they can easily verify the answer,” Salganik says. What spam filter best filters spam? Here’s 900,000 emails labeled “spam” and “not-spam.” Now, here’s another 100,000 emails. Let 10 systems label them, and we’ll see which one gets the most right. Result: spam filter. “The goal is explicitly about prediction, and prediction problems are easy to express quantifiably,” Salganik says. “But a lot of social science questions are different. They’re more about asking why something is happening.”
Basic descriptive questions might be useful to Darpa and its military clients. But they’re tough to quantify. “This really does go beyond just statistical significance,” Salganik says. Even if you adjust the acceptable P value, a test of statistical significance, from 0.05 to 0.005—the lower it is, the more significant your data—that won’t deal with, let’s say, bias resulting from corporate funding. (Particle physicists demand a P value below 0.0000003! And you gotta get below 0.00000005 for a genome-wide association study.)
The Debunkers of a Gay Marriage Study Just Re-bunked It, Sort Of
Social Science Is Busted. But the NIH Has a Plan that Could Fix It
Psychology Is in Crisis Over Whether It's in Crisis
So what’s the answer? One approach might be going beyond statistics and reproducibility to add new confidence tools to the kit. Nosek’s Center for Open Science awards “badges” to articles for things like pre-registering a research plan (to ward off accusations of P hacking) and making full sets of data and the code used to analyze it available. It’s like LEED certification for environmentally-designed buildings.
Social networks might also play a positive role—metrics can show not just how many people cited or linked to a study but how they talked about it. Blog posts and tweets about a new finding in, like, astronomy, could almost constitute a kind of post-publication peer review in which an entire scientific community digs into a paper. In other words, you know who’s gonna save science? Trolls.
Russell seems like he’d be totally open to that. He doesn’t know how many ideas he’ll fund—the RFI is open until mid-August—and he doesn’t know how much money he’ll be able to dole out. “The great thing about Darpa is, if you have a lot of great ideas and budgets are required to do that, you can make that argument,” Russell says. “In the long run, we’re all in this together. The better our science, the better decisions we can make.”
It’s a tricky philosophy-of-science problem, with a tricky philosophy-of-science answer. “It’s a little shocking in some ways that we’re even having this conversation now, that so many of us are waking up and realizing that we’re not as confident in our methods as we thought we were,” Watts says. “It’s going to be a big collective effort to just improve our ability to say we actually believe this result versus we shouldn’t yet.” You have to believe that science can fix science—with some help from Darpa.
DARPA, the Department of Defense research arm, is trying to make its biggest hacking challenge into a visually exciting competition, complete with color commentary.
Humans can do things. Awesome things. Case in point: this human, named Aaron Cook. Cook can do some serious tumbling (just check out his YouTube channel), but he can also do something extremely difficult—a standing double back tuck (backflip).
I have two daughters and both are gymnasts, so I know just how difficult this move really is. A back tuck is a flip in which the feet move forward and the head moves back (a front tuck is the exact opposite). The "tuck" part means that Cook pulls his legs into his body as he rotates. If he stayed in a straight position, it would be called a layout—and it would be super impossible. Last, we have the "standing" part.  This means that he does the flip from a standing position. It's actually much easier to do a double back tuck if it's part of another tumbling pass where the gymnast builds both energy and rotation over the course of several moves.
Why is the standing double back so difficult? Well, you have to do two really hard things to not crash on your head.
First, you have to jump high enough so that you are in the air long enough to rotate. Notice how my daughter's center of mass rises and then moves back down.
Second, you need to rotate—twice. There are two things to helps with this rotation. In order to start the rotation, Cook swings his arms. This motion gives him an initial angular momentum. Angular momentum is very similar to normal momentum, but instead of being the product of mass and velocity, angular momentum is the product of "rotational mass" and angular velocity.  The rotational mass (usually called the moment of inertia) depends on both the mass of the object (or human) and how this mass is distributed. Once he is in the air, he pulls his legs in close to his body. This decreases his rotational mass so that his angular velocity increases.
Here is another way to do a back tuck that is a little bit easier (according to my daughter). In this version, she starts in a handstand. As she moves back to a standing position, she increases her angular momentum so that she will rotate faster once she does the flipping part.
The fact that Aaron Cook can make this standing double back tuck kind of makes him a superhero. Looking at the video, I can measure the time he is in the air with a hang time of approximately 0.867 seconds. Since the vertical acceleration is known (-9.8 m/s2), I can use the normal kinematic equations to make a relationship between time and maximum height.  If you want all the details of this calculation, you can find them in this older post.
With this hang time, Aaron Cook jumps a height of 0.92 meters. That might not seem very high, but remember that is the change in position of his center of mass and not his feet. During that time he rotates twice. So, I can calculate his average angular velocity with a rotation of 4π radians. This gives a rotation rate of 14.5 radians per second or 138 rpm. Yes, that's fast. I don't recommend you try this on your own.
Charlotte Drury, Maggie Nichols, and Aly Raisman talk to WIRED about the skill, precision, and control they employ when performing various Gymnastic moves and when training for the Olympics.
Silicon Valley runs on two things: obscene amounts of cash and the tales people tell about who they are. Which is perhaps why the Bay Area has rapidly become ground zero for people pursuing one of the oldest mythologies in human history—the legend of everlasting life.
Well, maybe not ever lasting life exactly, but vastly-expanded-and-improved life. Call it healthspan extension, call it geroprotection: Silicon Valley wants to find a way to keep humans healthier for like, way longer. What was once a fringe science is rapidly becoming one of the Valley’s hottest investments, thanks to high profile endeavors like Alphabet’s $1.5 billion bet on Calico and Bezos and Thiel-backed Unity Biotechnology.
Most of the excitement around these ventures rests in the drugs they’re developing—pills to prevent damaged proteins and treatments to flush out toxic cells. But a new company launching today called BioAge, is instead selling a process: a way to predict mortality. And it’s doing so with advanced machine learning, a horde of lab mice, and the blood of 600 especially long-lived Estonians.
See, aging isn’t one disease. It’s the dysfunction of many different organ systems, first little by little, then all at once. It will take more than one molecule to reverse that sput-sput-sputtering out. But immortality research isn’t exactly a high priority for federally-funded research: To date, the National Institute for Aging has tested only 30 compounds, compared to the thousands trialed in cancer research at the NIH. One of those, a drug called rapamycin, has a future in combatting the decline of the immune system. Metformin, a diabetes drug, is currently in trials testing its anti-aging properties. But testing longevity drugs, of course, takes a long time.
So BioAge, based in Berkeley and run by a Stanford bioinformatician, is building a platform that doesn’t require waiting for its subjects to actually age. Instead, it wants to measure biological age using signals floating in a drop of blood. Biomarkers aren’t a new concept; it’s standard practice to use protein signals to guide drug discovery for conditions like cancer and heart disease. But up until now, no one outside academic research has really done it for anti-aging.
There are two good reasons for that: One, anti-aging drugs don’t have a clear path to making money, because the FDA doesn’t treat aging as a disease in its own right. Two, it’s really freaking hard. Whoever figures it out first will have a serious leg up in the fight to plumb the Valley’s fountain of youth.
“These studies take so long to complete; you have to give drugs to a mouse for four years before you get results,” says BioAge CEO Kristen Fortney, who crossed over into the private sector two years ago from Stanford’s Center on Longevity. Her company launched from stealth with a $10.9 million Series A today, with investments from Andreessen Horowitz and AME Cloud Ventures, run by Yahoo founder Jerry Yang. “We’re trying to get that down to just a few months,” says Fortney.
The idea is to find mortality predictors whose signals are just as strong in humans as they are in mice. Studies at Stanford and elsewhere in the past few years have shown that transfusions from young mice can restore liver and brain function in older mice, suggesting that there’s something in the blood that promotes aging, that it can be measured, and that it can be counteracted. (This is where that rumor you heard about Peter Thiel feeding on the blood of young entrepreneurs probably came from. While he’s not a vampire, he is funding start-up where you can pay $8,000 for a transfusion from the sub-25-year-old set.)
Anyway. To find that signal, Fortney’s team needed lots of data. And Estonia, with its socialized health care system and relaxed views about health data privacy, was the perfect place to start.  About 10 years ago, the nation’s largest biobank began gathering data on its population at a massive scale: genomics, proteomics, metabolomics, you name it, all combined with medical health records. BioAge picked a cohort of 600 elderly people, and its scientists started to mine their data for longevity secrets.
First, they sorted out the people who died right away or came down with illnesses—cancer, Alzheimer’s, heart disease—from those that went on to live another 10 years or more. Then, using machine learning, they identified molecular differences in their blood, focusing on molecules that humans and mice have in common (so their findings could translate into a mouse-based lab test). At the end, Fortney’s team had found a number of promising proxies for the onset of age-related diseases.
The next step is checking whether those biomarkers will respond to known anti-aging compounds. So they’re about to take a bunch of mice and shoot them full of things like rapamycin and metformin, then see if the biomarkers respond. If all goes well, they can use those compounds to screen against whatever promising chemicals BioAge’s data-mining algorithms turn up—kicking out a thumbs-up, thumbs-down ruling on a new compound  in months rather than years.
Forget the Blood of Teens. This Pill Promises to Extend Life for a Nickel a Pop
The Weird Business Behind a Trendy "Anti-Aging" Pill
Why Testing Drugs on Our Dogs Is Good for Our Health
“Reliable biomarkers would greatly accelerate this whole field,” says Tom Rando, who directs Stanford’s Glenn Center for the Biology of Aging and first discovered the phenomenon of parabiosis—young blood reversing the effects of aging in old mice. He says that while there have been indications that certain molecules might be strong predictors of biological age, there hasn’t been enough data to establish any of them. “If there were good biomarkers out there we’d all be using them,” he says, “but there’s not.”
It’s possible this is all a wild goose chase, and that we already have a powerful biomarker for aging. Steve Cummings, a longtime aging researcher at UCSF, pulled together NIH study data from over 30,000 people over 30 years to look for markers of disease-free survival. The strongest, most consistent signal he found isn’t in blood at all: It’s walking speed. Another good one is how fast you can connect dots of letters and numbers, a measure of cognitive function.
They identified a few molecules as well, like cystatin-C, which tells you how healthy your kidneys are (can’t live without your kidneys), and a few other measurements that reflect inflammation. But even with lots more molecules to work with, Cummings still thinks that blood-based tests are more hype than anything else. “Even 20 to 30 predictors together adds only a little to prediction of survival by age,” Cummings says. “And I’m confident that walking speed alone will outperform any new biological marker.”
That isn’t stopping Fortney, though—or her competitors. Calico is reportedly tracking a thousand mice from birth to death to sift out a few good chemical predictors of morbidity. Given that they launched in 2013, and mice live about four years, the secretive Alphabet offshoot is due for some results. (The company declined to comment on any specifics for this story.) BioAge should be finished with its first round of validation studies before the year is out, and then it can move on to testing novel drugs. Which should give Silicon Valley’s tech titans plenty of new candidates on which to hang their hopes of 100-year-old retirement.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?
You and your favorite cheese—whether it's cheddar, Wensleydale, or a good aged goat brie—have something in common: You’re both home to a constantly evolving menagerie of microbes. The bacteria inside you and your fermented dairy live together in a community called a biome, growing and changing in response to their environments. And they adapt to their homes—a cow’s hide, a chunk of Swiss, or your gut—by stealing their neighbors’ genes.
That genetic transfer has the ability to dramatically change a microbe. “You take this whole gene that you didn’t have before, that has totally new functions that you’ve never had, and you just plop it into this bacterium and suddenly it can do this completely new and different thing,” says Miriam Barlow, an antimicrobial resistance researcher at UC Merced. In humans, that's how antibiotic resistance can emerge—one bug evolves a mutation that helps it survive the onslaught of a drug, and it makes its way into the rest of the community. But to fully understand how resistance evolves, studying superbugs isn’t enough: You need large, diverse bacterial boroughs to understand how bugs siphon off new genes.
The search for lively bacterial communities led Rachel Dutton, a microbiologist at UC San Diego, to cave-aged cheese wheels—the kind you can only pick up with both hands. She wanted to find an environment that would kill some bacteria, but let other interesting microbes survive. Microbes don’t love cheese like humans do—it’s acidic, salty, and dry for their tastes—but it’s passable housing for some. If surviving antibiotics is like winning the lottery for a bacterium, inhabiting gruyere is like winning at bingo. “We have basically, in our freezer in lab, a few hundred vials of cheese,” Dutton says.
That frozen cheese stockpile—which came from 10 different countries—provides plenty of microbes to survey. Dutton and her students isolate bacteria from a smidge of cheese rind, grow communities in petri dishes, then send samples off for genetic sequencing. “Each of these sequences is about four to five megabases, in other words, about 4 million A’s, T’s, G’s and C’s,” says Kevin Bonham, a postdoc in Dutton’s group. Bonham wrote code that lines up hundreds of bacterial species’ genomes, plucks out each of their genes, and finds similarities between samples.
Scrutinizing sequences alone can’t tell you exactly how a gene showed up, but it can give you a good idea of which genes are most mobile in a population of bacteria. The exact same sequence in two different species, say, a proteobacterium and a firmicutes bacterium, stands out as a transferred gene, because normally there would be some variation between their genetic material. And more than one gene can move at once, so if one identical gene showed up next to another, they considered that chunk to have migrated all together, Bonham says.
Your Office Has a Microbiome, and It Might Make You Sick
Obesity Surgery May Work by Remaking Your Gut Microbiome
Antibiotics Have Turned Our Bodies From Gardens Into Battlefields
The most frequently shared genes were the ones that help bacteria grab and use nutrients around them. Iron uptake genes were by far the most commonly transferred. Some bacterial proteins need to bind metal to function, but milk and cheese are low in iron, so bacteria who could unlock the mineral have better odds of surviving. Those who survived also shared bacterial versions of Lactaid pills—genes for breaking down the abundant lactose in cheese.
That kind of information could help researchers figure out which genes are most prone to transferring within the human microbiome. While not all pathogens transfer genes readily, some certainly do. Gonorrhea—which comes in horridly resistant varieties now—really likes to spread its genes around, Barlow says. So understanding how these genes move in a community could mean a better grasp of how to battle antibiotic resistance.
Ideally, it would be possible to pinpoint exactly how certain sequences mobilize. Armed with instructions for spreading bacterial genes, scientists could introduce useful ones to influence how pathogenic foes, or friendly microbes, behave. But transferred traits are only one piece of the bacterial genome—so it will be a while before anyone has precise control of bacteria.
But keeping cheese popsicles in the freezer means Dutton and her students can run plenty more experiments. Now that it’s clear which genes bacteria pass around, they want to narrow in on how often those genes move, and how that transfer changes how microbes compete or cooperate. Maybe the way cheese bacteria steal traits will shed light on how their disease-causing relatives survive. For a humble crumb of cheese, that’s not just gouda–that’s grate.
Scientists are looking to an unlikely source for new ways to fight bacteria. Could the skin of a Galapagos shark hold the key to warding off hospital-born bacteria and superbugs?
As powerful as the gene-editing technique Crispr is turning out to be—researchers are using it to make malaria-proof mosquitoes, disease-resistant tomatoes, live bacteria thumb drives, and all kinds of other crazy stuff—so far US scientists have had one bright line: no heritable modifications of human beings.
On Wednesday, the bright line got dimmer. MIT Technology Review reported that, for the first time in the US, a scientist had used Crispr on human embryos.
Behind this milestone is reproductive biologist Shoukhrat Mitalipov, the same guy who first cloned embryonic stem cells in humans. And came up with three-parent in-vitro fertilization. And moved his research on replacing defective mitochondria in human eggs to China when the NIH declined to fund his work. Throughout his career, Mitalipov has gleefully played the role of mad scientist, courting controversy all along the way.
Yesterday’s news was no different. Editing viable human embryos is, if not exactly a no-no, at least controversial. Mitalipov and his colleagues at Oregon Health and Science University fertilized dozens of donated human eggs with sperm known to carry inherited disease-related mutations, according to the Tech Review report. At the same time, they used Crispr to correct those mutations. The team allowed the embryos to develop for a few days, and according to the original and subsequent reports a battery of tests revealed that the resulting embryos took up the desired genetic changes in the majority of their cells with few errors. Mitalipov declined to comment, saying the results were pending publication next month in a prominent scientific journal.
Big if true, as the saying goes. Mitalipov’s group never intended to implant the eggs into a womb, but the embryos were “clinical quality” and probably could have survived implantation. That makes this only the second time scientists anywhere have edited viable embryos—if that’s indeed what Mitalipov did. Maybe this news is important enough to make it to the popular press without a peer-reviewed, published paper, but without one it’s impossible to be definitive on what Mitalipov actually did versus what he’s claiming to have done.
Let’s say it’s all real. Is it creepy? Maybe. But it’s also legal—at least in Oregon, where embryo research is kosher as long as it doesn’t involve federal funding. Officials at OHSU confirmed that the work took place there, and that it met the university’s Institutional Review Board criteria for safeguarding the rights and welfare of subjects involved in human research—presumably the donors of the eggs and sperm, in this case. No one on the outside knows which exact genetic tweaks the researchers actually made or how safe the procedure was. Tech Review was light on details.
That lack of transparency could turn into a real problem. “These are special cells and they should have special considerations given to them if you’re going to Crispr them,” says Paul Knoepfler, a stem cell researcher at UC Davis who wrote a book on designer babies called GMO Sapiens. Knoepfler worries that incautious work like this could lead to political backlash against Crispr more broadly, like what happened to stem cell research in the 2000s under George W. Bush. “We don’t have an unlimited amount of time to talk about these things and figure them out,” Knoepfler says. “This stuff is moving at warp speed and we need to get our act together on establishing guidelines that are much clearer about what is OK and what isn’t.”
Not that scientists haven’t tried. In February the National Academy of Sciences produced a report with its first real guidelines for Crispr research. It did not go so far as to place a moratorium on gene editing of the human germline—modifications that a person’s offspring could inherit—though it did suggest limitations. Scientists are only supposed to edit embryos to prevent a baby from inheriting a serious genetic disease, and only if the doctors meet specific safety and ethical criteria, and if the parents have no other options.
Those obstacles aren’t insurmountable, and a particularly slippery slope winds between them. At the Aspen Ideas Festival last month, UC Berkeley biologist Jennifer Doudna, one of the people who discovered Crispr, stressed the need for a unified policy on germline editing before scientists really start doing it. “Once that begins, I think it will be very hard to stop,” she said. “It’ll be very hard to say, ‘I’ll do this thing but not that thing.’ And at that point, who decides?”
In the US, it’ll probably be the federal government. Congress has already banned federal funding for the human testing of gene-editing techniques that could produce modified babies. That provision is tucked into an appropriations rider that has to be renewed each year, so it’s an annually moving target. Congress has also barred the US Food and Drug Administration from even considering clinical trials of embryo editing. But even if those laws did change, the FDA’s approval process for these kinds of technologies is among the strictest in the world. They would require years and years of animal studies before the first test embryo could conceivably be conceived.
Crispr Is Getting Better. Now It's Time to Ask the Hard Ethical Questions
Read This Before You Freak Out Over Gene-Edited Superbabies
Human Embryo Editing Gets the OK—But No Superbabies
“For this to be something other than just a reckless person doing something crazy, we’re looking at least a decade and maybe more of safety testing,” says Hank Greely, a law professor and bioethicist at Stanford. In countries with laxer laws, it could happen sooner—like, say, China, where scientists have reported three attempts at using Crispr to modify human embryos.
The first two studies used genetically defective embryos that could never come to term, but the most recent, published in March, used viable embryos. And while all three studies produced mixed results, Crispr was most successful at repairing faulty genes in the normal embryos. Experiments are also moving forward in Sweden and the UK that use Crispr to knock out different genes in viable embryos to study effects on development.
Still, don’t panic. “Modifying embryos that are never going to be implanted is not close to the boundary,” Greely says. “Doing it in embryos you might want to implant is real close to the boundary and shouldn’t be done without any discussion. But that’s not what Mitalipov did.” Maybe. All the institutions apparently involved with the research refused to comment citing an embargo, which would make sense if there were an embargo to break. There wasn't, according to Antonio Regalado, who covers genetics for Tech Review but didn't write this story. Consider it instead just a good new-fashioned leak.
If you think of viable-embryo Crispr research as a journey and not a destination, right now scientists all over the world are on the same path. But at some point the road will fork: Someone will implant an engineered embryo into a human womb. “The work coming out of China and Mitalipov's lab has this implied assumption that someday it will wind up being used heritably in humans,” Knoepfler says. “And I think that requires a unique obligation for being more open about it.” Mitalipov's research is not a good start.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
Last summer, southern Florida nearly ran out of water. It wasn’t drought—actually, the opposite. The state got way too much rain, which flushed nutrients from over-fertilized farms into its canals and reservoirs. All the extra food led to a massive algal bloom, a skim of blue-green slime that smells like rotten eggs and poisons humans. Governor Rick Scott declared a state of emergency in four counties.
Algal blooms aren’t new. In fact, they happen naturally. Florida’s was dangerous because of humanity's industrial-strength thirst and hunger, leading to big, fertilizer-based agriculture. Add climate change to those pre-existing conditions, and you've got a water system primed to snap. More rain in already-wet agricultural areas will leach away even more nutrients, causing more blooms, leading to more water shortages—impacting fisheries, agriculture, and public health.
A new study published today in Science shows just how bad these algal blooms will be, based on three different carbon emissions scenarios. On the bright side, this means policymakers can start planning all sorts of ingenious new ways to curb fertilizer use. And that’s what they’ll do—right guys?
Scientists have known about the links between rain, nutrient runoff, and disaster for years. “Nutrients are essentially food to phytoplankton,” says Anna Michalak, a climate scientist at Stanford University’s Earth Systems Institute. “When a bunch of it leaches into the water, you stimulate these algae to grow. And when they eventually die, they decompose, and use up oxygen, creating dead zones.” One of the biggest dead zones occurs pretty much annually off the coast of Louisiana, fed by the Mississippi River’s runoff.
Climate Change Is Killing Us Right Now
Utah Lake's Poop-Driven Algal Bloom Is a Crappy Situation
Incredible Images of Algal Blooms Taken From Space
Likewise, scientists have long suspected that algal blooms would get worse as climate change brings more rain to already wet regions. “For close to 10 years we’ve been looking at how to better understand recurrence of hypoxic dead zones and algal blooms around the US,” says Michalak, who co-authored the new paper. During that time, she worked with her lab to develop a model linking rainfall to nutrient runoff, which she published last fall. This latest research links that model to the results of a massive database of climate change scenarios.
Those scenarios fall into one of three possible futures. In the first, humans continue with business as usual, emitting fossil fuels at a rate that follows the historical trajectory. This gets bad. The worst runoff will happen in the Mississippi-Atchafalaya watershed, and the Northeast region, from Maryland to Maine. This megaregion—encompassing much of the nation’s agricultural land, and most of its major population centers—could be hit with an 18 percent increase in nitrogen loading by the end of the century. The other scenarios are incremental versions of humanity cleaning up its act, leading to slightly less nitrogen runoff.
Even the rosiest scenario predicts runoff increases ranging around 5 to 10 percent, depending on the region and how far you project into the future. That means humans are going to have to make up the deficit by reducing how much fertilizer they use. Not an easy ask, even before you consider that the Earth's population—and therefore its appetite—is growing. Remember that 18 percent increase Michalak's group predicted for the Mississippi-Atchafalaya watershed? In order to counteract all that runoff, the region needs to dump 30 percent less nitrogen into its soil.
Impossible, you say? Well, before your reflex climate pessimism kicks in, consider this: The US EPA has already imposed a region-wide nitrogen reduction. The ultimate goal calls for a 62 percent cut. Currently, farmers are working towards 20 percent. They've done this by things like applying fertilizer directly to crops through precision agriculture, composting, or by sustainably tossing out livestock manure. "This is all the low hanging fruit. To really get to the next step, we need to talk about some of the more blue sky, high technology agriculture," says Sybil Seitzinger, an environmental scientist, and director of the Pacific Institute for Climate Solutions at the University of Victoria.
For instance, cattle are a huge greenhouse gas emitter, plus their manure is a major source of nitrogen. Lab grown beef would take care of both problems. Another tech solution would be genetically modifying crops like corn or cereal grains, so they are capable of fixing their own nitrogen. This means they would be more efficient at taking up soil nutrients—farmers would need less fertilizer. The technological challenges are tough enough. These solutions would also be up against the monied meat industry, which isn't so excited to cull its herds and leave the burgermaking to pencil-necked lab jockeys. And in case you haven't been paying attention, genetically modified organisms don't have the rosiest reputation in the US. Then again, the prospect of running out of clean drinking water might just inspire people to expand their palates.
You can't see it, but there's a lot going on inside a glass of tap water. The faucet aqua is essentially an invisible cocktail of sulfates, resins, varying levels of lead, and so much more. Find out what else is inside.
There's a new video game in development for all you science nerds, and it has an advisor you might recognize. No, not me—just another one of your favorite physicists-turned-science educators, Neil deGrasse Tyson. The game is Space Odyssey, a space exploration caper that takes you on a journey to explore and settle new planets. Currently, the game is a Kickstarter project with an estimated completion date of December 2018.
Now, we all know that sometimes crowdfunded games just don't work out. But even if Space Odyssey doesn't see the light of day—which, depending on which side of your tidally-locked exoplanet you choose to colonize, might be the case!—I still wanted to talk to Tyson about the game, science education, and how video games can be a great way to explore concepts in physics.
Space Odyssey is about space exploration, but how is it different than something like Kerbal Space Program, where you get to build and test-fly your own spacecraft?
Kerbal has precise, like over the top precise orbital mechanics in it. So that's kind of fun to see: what it takes to launch something, get a right trajectory to intersect with destinations. The beautiful physics in Kerbal is orbital physics.
In Space Odyssey, our goal is basically world building. Initially the target is known exoplanets. We know enough to know where they are and what their approximate masses are, what kind of orbits they have. And beyond that we, we also know if they are in the Goldilocks zone. Beyond that, we don't know. You will continue to use laws of physics to build on that planet, to explore it, to establish geology, atmospherics. You can even build out the materials there—it will build on accurate material sciences. It's the physics of living and exploring.
Why did you decide to promote these physics ideas in a video game?
Video games are such a huge source of interest for so many people. It's a place that is largely devoid of meaningful science, and I think we could possibly make an important mark here. We spend time on video games when we should be spending time doing something else, and that ratio is different for different people. At the end of the day, it would be nice know that if you should have been doing something else, at least playing this video game you will have boosted your level of science literacy. There's something you can claim for having participated.
So how much real physics is in the game? How does the game handle faster-than-light travel and communications, or is it more realistic in that you can’t do that?
That's a great question. I'm still on the fence. So, the question is should the user be allowed to tweak laws of physics? It’s like, holy shit, that could have unintended consequences. If you want to change the constant of gravity, for example, well that has secondary impact on the luminosity of stars. This is a level of power that I don’t think is necessary to wield in this universe.
It could be fun, however, if instead of saying we’re going faster than the speed of light, we could change the speed of light to be something much lower. Let's say 100 mph. So now as you approach 100 mph, all these relativistic effects start taking place. So you don't have to build some super atomic machine that nobody knows how to build yet to go that fast. And then you get to see all the effects of relativity—and that would be really fun to notice.
If we didn’t go there, if you just keep the speed of light as it is, and then we find planets that might be orbiting black holes. Objects that were once stars became black holes but they still have planets orbiting around them. That would be possibly dangerous, you would need an energy source, but you would see amazing relativistic effects. Akin to what you may remember was portrayed in the movie Interstellar.
Astrophysicist Neil deGrasse Tyson Fact-Checks Gravity on Twitter
Neil DeGrasse Tyson Takes on the Cosmos
How Fast Would the Earth Have to Spin to Fling People Off?
There was a game or an educational app that does greatly reduce the speed of light. I love that idea.
What I do know is the famous physicist from the 20th century George Gamow, he authored a series of books called Mr. Tompkins in Wonderland, and it was for adults and kids. What it did was it imagined worlds with the various constants of nature changed so that phenomena would take place in everyday life that would otherwise require exotic physics situations to experience. So, for example, if you change Planck’s constant to something large, then all of a sudden things that go on in your life would be a match for what currently goes on in the world of particles. You would walk through a doorway and you would diffract through the doorway the way light does through a slit in quantum physics.
It’s a fascinating way to learn. Because when you put exotic physics in everyday life then you get to see what’s actually going on with objects and phenomenon that are otherwise familiar to you. So yeah, that’s a way to take the game. But I think the anchoring in the real physics and then having the creativity based on in the end will pay more dividends in terms of people’s enjoyment.
So maybe there should be two games: Space Odyssey based on real physics, and let’s mess up physics bad and see what happens, as another game.
Space Odyssey: Rogue edition. How about that?
Astrophysicist Neil deGrasse Tyson sits down to talk about the cultural importance of Carl Sagan’s Cosmos: A Personal Journey, and how he plans to carry on the legacy with his new version of the transformative science exploration show.
The mountain of evidence connecting professional football and long-term brain damage grew this week with publication of a new study that examined the brains of former NFL players. Boston University scientists found 110 of the 111 post-mortem brains showed signs of chronic traumatic encephalopathy (CTE), a degenerative brain disease linked to repeated hits to the head. Linemen had it the worst, while punters seemed to escape relatively unharmed.
This kind of brain damage and the NFL’s response—or lack thereof—has dogged the league for the past decade. Football has seen class action lawsuits, congressional hearings, and efforts to make the game safer by banning certain hits and designing new helmets. But neurologists involved in this new study, as well as other experts, say another sport may rival football’s impact on the brain: soccer.
Soccer isn’t usually considered a contact sport, although gameplay hits can result in concussions. “Soccer has repetitive impacts, from player to player and players heading the ball,” says Boston University’s Ann McKee, an author of this week’s study of NFL players. “It doesn’t matter how you do it, just that you do it and do it repetitively.” And with 265 million players worldwide, soccer represents a potentially huge pool of head injury patients.
One US pro soccer team, DC United, lost six players to retirement over the past decade due to concussions, while another two players missed playing time this season with head injuries. In a recent lawsuit against the team and its coach, former DC United goalkeeper Charlie Horton said one of his teammates elbowed his head on purpose in 2016, giving him a concussion and ending his professional career. And in 2015, the US Soccer Federation, the sport’s governing body, settled a proposed class action lawsuit by limiting heading by youth soccer players.
Lawsuits aren’t scientific evidence, but an increase indicates that players are concerned about how their sport responds to concussions and the possible long-term risk of brain disease. Case studies have shown that retired professional English and Brazilian soccer players with a history of concussions can later show signs of dementia; autopsies revealed CTE-riddled brains. In April, BU researchers reported on the case of a former American high school soccer player who had 19 soccer-related concussions, a history of depression and mental health problems who died aged 24 of a drug overdose. An autopsy revealed that he also had CTE, according to a report in the journal Neurology.
Those case studies have limited scientific value, of course; to nail down the connection between soccer and brain damage, the sport will need bigger sample sizes. The big difference between soccer and football is the number of former NFL players and their families who have come forward asking for help from the medical community. McKee’s group in Boston has set up a “brain bank” where families of ex-football players (pro and college athletes) who are worried about their mental state can donate their brains for research. So far they have received 425 brains; CTE has been found in 270 of them. That kind of focused research effort hasn’t yet been developed for soccer.
Even in the case of football-related brain injuries, there’s no absolute proof that concussions cause CTE. In fact, scientists say that it may be the less powerful “sub-concussive” hits that both football and soccer players receive all the time that could trigger the disease. But at New York’s Albert Einstein College of Medicine, neuroscientist Michael Lipton has been working to identify the trigger. “In soccer, where you have people repeatedly hitting their head over time,” says Lipton, “the question is how much does it take to lead to a pathology that rises to a level where there are functional effects.”
To answer that question, Lipton has been following a group of recreational soccer players in New York City for the past few years. About 400 active players participate in his Einstein Soccer Study: They come in to the lab to get a scan of their brain and some blood work, and then they are asked to perform brain games on a tablet to test their cognitive abilities. Lipton uses diffusion tensor magnetic resonance imaging, which allows him to map changes in the brain’s white matter.
Accelerometers Could Finally Fix the NFL's Concussion Crisis
How a Mere Prick of the Finger Can Diagnose a Concussion
Neurologists Call for Strict Sports-Concussion Guidelines
In 2013, Lipton reported in the journal Neuroradiology that repeated heading the ball—even without getting a concussion—is associated with cognitive problems and physical changes to the structure of the brain. Players head the ball, on average, six to 12 times per game, trying to deflect balls that travel up to 50 miles per hour in recreational games. In practice, players head the ball up to 30 or more times in a row during drills. Lipton’s study suggests that initial problems with memory began at 1,800 headers.
Now, that study only examined 37 players—a small sample that isn’t big enough to establish a clear-cut connection. But with a larger sample size of several hundred participants, Lipton is looking to identify some kind of biological change in the brain over time. “There is clearly something going on, but what it means for the long term requires more work,” he says.
Finding that tipping point—beyond which a professional or recreational player should probably retire or take a break from heading the ball—would be a huge relief for everyone who loves the game.
McKee notes that the only way to determine CTE is through an autopsy. But she and other medical researchers are hoping to find some kind of biomarker, a protein in blood or urine perhaps, that signals the beginning stages of CTE. That kind of information would give the player a yellow or red warning card that maybe it’s time to pick up checkers or croquet. Until then, the only thing that can quell the damage is taking a break from repetitive hits—whether they’re from a defensive linebacker or a soccer ball to the head.
As the NFL begins to address the existential problems that brain trauma pose, see how innovations in protection, impact monitoring, and training techniques are being developed to protect players.
The only way to know exactly what’s in a wildfire’s smoke is to sample straight from the haze. So during the Rim Fire in Yosemite—which emitted so much smoke it formed its own clouds—a NASA DC-8 passenger plane and an Alpha fighter jet each crisscrossed through the plume. On both planes, scientists had created an in-flight lab to measure exactly what the fire was producing.
The answer seems obvious: Fire makes smoke. But smoke isn’t a uniform entity. It’s a variable portfolio of gases, invisible but for the particles they ferry along. “That’s what you’re actually seeing when you see a smoke plume, you know the big white smoke plume. That’s sunlight bouncing off the little particles,” says Bob Yokelson, an atmospheric chemist at the University of Montana. The composition of that smoke matters for human lungs and the climate—which is why Yokelson’s team and NASA’s Alpha jet crew are busy planning their next flights for late summer.
There are a lot of ways to study those pollutants—from the ozone that makes it hard for humans and crops to breathe to the light-absorbing particulate matter that raises atmospheric temperatures. The US Forest Service runs a Fire Sciences Lab in Missoula, where Yokelson has compared burning manzanita to ponderosa pine to see how fires in different ecosystems might burn. But it’s incredibly difficult to capture every component of a burning forest—with variable light, temperature, and fuel conditions—in a lab. So the truest measurements come straight from the airspace above a burning forest.
Sending a lab down a runway and into the sky isn’t easy. Prep can take a year or more, as teams of scientists design and assemble custom gas and particle measurement systems. In research labs, these machines are finicky, sprawling combinations of pumps and tangled wire. For field flights, they’ve got to work at a range of temperatures and pressures, and neatly replace a row of plane seats—or get even smaller.
The Alpha jet was converted from a fighter jet, taking off as a science plane for the first time in 2010. Before that, it had to be quieted down for civilian airspace, and equipped with sensors to measure trace gases in the atmosphere: ozone, carbon dioxide, methane, and formaldehyde. As its two pilots follow a fire’s smoke, the sensors continuously measure the air, according to Laura Iraci, the NASA chemist who runs the experiments. After a two or three hour flight—the next will likely be in late August—they land back at the airstrip with data cards full of numbers to analyze.
When Yokelson and his team outfit a jetliner like the DC-8 that flew to the Rim Fire, they get to renovate the plane’s interior. “We'll take out every other row of seats, and bolt down instruments in their place, so now you have the scientist sitting in front of an instrument and they can monitor the data as we're sampling the atmosphere,” he says. This summer, their team is getting a C-130 jet ready for its close-up–test flights, set for September.
Photo of the Week: Firefighters Save a Flag From California's Raging Wildfires
The West Is on Fire. Blame the Housing Crisis
An Epic Fire Season Is Coming. These Firefighters Are Ready
In flight, the scientists on a larger plane like the DC-8, or a C-130, monitor the same trace gases as the Alpha jet. But a bigger plane means more room for equipment. So they can also measure the size of smoke particulates, plus a whole range of volatile organic compounds and nitrogen oxides. Those nitrogen oxides react with with volatile organic compounds in sunlight to make smog—ozone and particles—so measuring all the ingredients of the reaction is ideal. Larger planes can also collect samples, sucking air into two liter stainless steel cylinders. They sometimes ship hundreds of these canisters back to lab overnight for analysis of dozens more chemicals.
So far, airborne studies like these have highlighted that wildfires burn dirtier than their indoor and prescribed cousins, carefully lit and contained in the forest. Bigger logs and wetter material create even more particulate matter. And as fires smolder longer, they can actually start to release a serious amount of methane, which traps more heat in the atmosphere than carbon dioxide.
Both Yokelson and Iraci have lots more questions about what else fires dump into the atmosphere, and how the airspace changes throughout the course of a fire. So as soon as the planes are ready, they’ll head back towards the smoke. Their accurate field measurements are the key to good air quality and climate change models—and the EPA would love to predict how wildfire pollutants might descend on neighboring cities and states. “We're really optimistic that our data can provide sort of truth, so they can continue improving their models,” says Iraci. It may take a season or two to pump new data in, but predicting air quality around wildfires could get a lot better in the next few years.
NASA started a blaze aboard the unmanned Orbital ATK Cygnus cargo vehicle. It’s the Spacecraft Fire Experiment. Seriously, that’s exactly what NASA is calling it.
When Etienne Schneider became Luxembourg's minister of the economy in 2012, one of his first trips abroad was to NASA’s Ames Research Center. It might have seemed strange for the tiny state's money man to solicit meetings with cosmic researchers, but Luxembourg is always on the lookout for its next big investment. So when the center’s director, Pete Worden, began to talk about space mining—extracting water, ore, precious metals, alien time capsules, and whatever else from the likes of asteroids—Schneider listened.
“I thought this was all science fiction,” says Schneider. But Worden convinced him there was a whole cosmic economy to build, one that could extend from the moon to Mars. That same year, two guys founded a space prospecting company called Planetary Resources. And in January 2013, another, named Deep Space Industries—headquartered inside NASA's Ames campus—was born. Soon, Schneider saw the same future they did. “The question was not if that all would happen, but when,” he says. “And there I saw a huge opportunity for Luxembourg.”
That’s why, on August 1, Luxembourg plans to adopt a new law that gives empyrean mining companies the rights to whatever they pull from asteroids—making itself an attractive place for those companies to settle and distribute their harvested riches.
Here are two things you should know about Luxembourg: One, its population is less than Milwaukee’s. Two, its per-capita GDP is second-highest in the world, according to currently available World Bank figures. In other words, the Grand Duchy is small, but the Grand Duchy is mighty.
That’s not an accident. “Luxembourg is such a small country that we always have to reinvent ourselves and take on a certain risk to succeed,” says Schneider. Back in the 1980s, the government gave SES—Europe's first private satellite operator—the legal and budgetary boost it needed to grow into a dominant satcom provider. The state didn't just throw money at SES: It actually owns a significant chunk of the company. Other space-centric companies sprouted up around SES, and together, they now account for 1.8 percent of the country’s nearly $60 billion GDP.
To do the same with the embryonic asteroid mining economy, Schneider needed to create incentives for mining companies to set up shop in the landlocked country. After all, Luxembourg is a few years behind the US’s own space act, which says a citizen who commercially acquires a space resource then "shall be entitled to it...to possess, own, transport, use, and sell." Planetary Resources and Deep Space Industries, as well as space-habitat-maker Bigelow Aerospace, lobbied in favor of the law.
So Luxembourg lured them eastward with money (know your strengths)—200 million euros of initial support in grants, R&D money, and direct investment. In May 2016, Luxembourg committed to funding some of Deep Space Industries' R&D, and that same month, Planetary Resources signed its own deal, too—pledging to develop some tech exclusively within the country's borders. Luxembourg went on to invest 25 million euros in Planetary Resources, making the country a key shareholder. On top of direct investment, companies that relocate to Luxembourg or add a district office can apply not just for the country's grants but also for those from the European Space Agency. Soon, there may even be a public-private venture capital fund.
In other words, Luxembourg is making it rain. And more than 60 companies have either benefited from its cash or hope to.
But all that investment does no good if a space mining company has no rights to its plunder. The United Nations’s Outer Space Treaty, created in 1967, suggests that rights to resources may not be rights anyone can have, legally. Its Article 2 says, “Outer space, including the moon and other celestial bodies, is not subject to national appropriation by claim of sovereignty, by means of use or occupation, or by any other means.”
So Luxembourg—and the US, in that 2015 move—crafted loophole laws. They both say, essentially, companies are not staking claim over asteroids but merely the minerals they dig out. They're not appropriating any bodies—just severed limbs!
It’s complicated. Perhaps the UN agreement applies only to nations, not individual citizens. And it does say that "there shall be free access to all celestial bodies," and that space is "free for exploration and use by all States without discrimination of any kind." Wouldn't, then, a prohibition against asteroid mining limit those freedoms? Plus, officials wrote the treaty back when mining, and even the private space industry, was science fiction.
Luxembourg's Bid to Become the Silicon Valley of Space Mining
Asteroid Mining Sounds Hard, Right? You Don’t Know the Half of It
Congress Says Yes to Space Mining, No to Rocket Regulations
Luxembourg and the US are confident that anti-mining interpretations of the old agreement won't stand in the space-faring future. So after August 1, if you get permission from Luxembourg to mine asteroids, the subsequent riches are yours. To get that OK-to-go, companies must have written permission from Luxembourg, an office there, a solid risk assessment, and major shareholders or members who have not skimmed money off the top or from terrorist groups, among other strictures. Even Luxembourg doesn't want to let just anybody take a crack at a space rock.
But no one’s going to be doing that for a while, anyway. Schneider himself estimates it will be 20 years before private companies actually reach into asteroids. That means Luxembourg, even in its own optimistic view, is just BoyScouting—being prepared for the 2030s. Then, the miners will live (or at least occasionally work) along the banks of the country's Alzette River and riches will rain down from orbit. And if Luxembourg is the primary backer (and, in some cases, part owner) of those asteroid-stuff-extractors? One of the planet’s smallest nations will be the solar system’s biggest player—pulling down tax revenue, return on investments, and the satisfaction that comes from controlling that future space supply chain.
Schneider isn’t shy about the country’s ambitions. “Ten years from now,” he says, even before anyone is mining anything out of the not-ground, “Luxembourgish will be the official language in space.”
When humans touch down on an exoplanet they'll need some new tools—like NASA's prototype mining robot and the Z-2 suit made for Martian hiking trips.
Conventional pharmaceuticals aren't always the best way to treat an ailment. Drugs are often imprecise, unpredictable, or come along with tricky side effects. Medicine is always trying to move on to more targeted treatments. And soon, robots will be one of those options: small and mobile, they could theoretically deliver pharmaceuticals right where they’re needed, tear through tumors, or rebuild broken bits of your body. Of course, these kinds of treatments are decades away—which might not be a problem, depending on how you feel about maggots.
See, the hitch with robots is getting them to move. For obvious reasons, it’s especially crucial that the borgs creeping through your body aren’t complete klutzes. And as far as modes of transportation go, few are as gentle as the scootch. That’s why mechanical engineer Franck Vernerey started modeling his machines after maggots' squirming movement.
These aren’t your typical robots. Really, at this point, Vernerey's creations aren't even really robots at all: They're just tiny cylinders of hydrogel, a synthetic material that sucks up or spits out water, depending on its temperature. But Vernerey, whose lab is at the University of Colorado Boulder, was able to induce these makeshift medicinal maggots to creep through tubes by cycling them through warm, then cool water.
Maggots move through a combination of two mechanical processes—extending and contracting their bodies. "For motion to happen however, sliding has to be easier in one direction than another," says Vernerey. So, larvae have tiny, rearward facing spikes on their body. When the maggot extends its body, the spikes’ backside brushes over whatever piece of rotten food the maggot is trying to squirm through. When it contracts, the sharp end of the spikes bite into the surface, creating friction, and the creature’s body moves forward.
Vernerey thought to reproduce this motion with hydrogels. They're sort of like rubbery sponges, and his expand or contract depending on their temperature. "The specific hydrogel used in this research displays a dramatic and reversible phase transition at a temperature around 32 ˚C," he says. Below that temperature, the hydrogel becomes hydrophilic, absorbing water. Above that temperature, the reverse happens. So, Vernerey made little cylinders of the stuff, about 3 centimeters long by 1.3 centimeters in diameter. I measured, and that’s about the length and width of my thumb from the tip to the first knuckle. So, imagine a thumb, except blue, somewhat translucent, and gelatinous.
But no spikes. It’s pretty tough to put those on a hydrogel, so Vernerey cheated a little by printing out 3-D tubes with scaly textures on the inside. He made four of these, the largest about two-thirds the diameter of his hydrogel at normal temperature, and the other three smaller by one centimeter increments. He stuffed the hydrogel inside each, and submerged them in a fluid that he could heat and then cool.
If this were a real medical robot, it'd be crawling through flesh, aiming to deliver its medicine, shred apart a tumor, or help heal some tissue. But before it can crawl, it has to ... crawl. So, Vernerey went about testing his proof of concept in tubes of various sizes to see which mechanical forces optimized the hydrogel's ability to squirm forth. At play here are several factors. First, they had to figure out the best angle of the scales inside the tube. Then they found out that longer bits of hydrogel were more efficient crawlers than shorter ones. "This might explain why maggots are longer than they are wide," says Vernerey.
And finally, the shape of the tube itself presents competing forces for movement. First is confinement: In a smaller tube, the hydrogel would have more surface area to grasp against. However, this also inhibits the hydrogel’s ability to extend itself. After repeated trials, he found that a tube that is between 20 and 50 percent of the hydrogel’s diameter was most efficient. Then he used a computer model to generalize his results to smaller scales—after all, a thumb-size robot is much too big to be putting inside anybody’s body.
This all might sound exciting, but remember that Verneney is still just learning how to crawl. Astute readers might point out that the inside of our bodies are not lined with scales, so a hydrogel might have trouble finding a grip. Fabricating hydrogels with scales of their own is still an open question, says Vernerey. He also has to teach these things how to home in on their target, and figure out locomotion itself. He envisions embedding the hydrogels with magnetic nanomaterials, and then exciting them through warm and cool cycles through an EMP field. But maybe you’re still grossed out by the thought of maggot-like robots creeping through your body. Well, the medical grade hydrogels will be micron scale. Out of sight, out of mind, perhaps?
A robotic heart points the way to a future where soft robots help us heal.
This story originally appeared on New Republic and is part of the Climate Desk collaboration.
A young, fit US soldier is marching in a Middle Eastern desert, under a blazing summer sun. He’s wearing insulated clothing and lugging more than 100 pounds of gear, and thus sweating profusely as his body attempts to regulate the heat. But it’s 108 degrees out and humid, too much for him bear. The brain is one of the first organs affected by heat, so his judgment becomes impaired; he does not recognize the severity of his situation. Just as his organs begin to fail, he passes out. His internal temperature is in excess of 106 degrees when he dies.
An elderly woman with cardiovascular disease is sitting alone in her Chicago apartment on the second day of a massive heatwave. She has an air conditioner, but she’s on a fixed income and can’t afford to turn it on again—or maybe it broke and she can’t afford to fix it. Either way, she attempts to sleep through the heat again, and her core temperature rises.  To cool off, her body’s response is to work the heart harder, pumping more blood to her skin. But the strain on her heart is too much; it triggers cardiac arrest, and she dies.
Such scenarios could surely happen today, if they haven’t already. But as the world warms due to climate change, they’ll become all too common in just a few decades—and that’s according to modest projections.
This is not meant to scare you quite like this month’s cover story in New York magazine, “The Uninhabitable Earth.” That story was both a sensation and quite literally sensational, attracting more than two million readers with its depiction of “where the planet is heading absent aggressive action.” In this future world, humans in many places won’t be able to adapt to rising temperatures. “In the jungles of Costa Rica, where humidity routinely tops 90 percent, simply moving around outside when it’s over 105 degrees Fahrenheit would be lethal. And the effect would be fast: Within a few hours, a human body would be cooked to death from both inside and out,” David Wallace-Wells writes. “[H]eat stress in New York City would exceed that of present-day Bahrain, one of the planet’s hottest spots, and the temperature in Bahrain ‘would induce hyperthermia in even sleeping humans.’”
These scenarios are supported by the science. “For heat waves, our options are now between bad or terrible,” Camilo Mora, a geography professor at University of Hawaii at Manoa, told CNN last month. Mora was the lead author of a recent study, published in the journal Nature, showing that deadly heat days are expected to increase across the world. Around 30 percent of the world’s population today is exposed to so-called “lethal heat” conditions for at least 20 days a year. If we don’t reduce fossil-fuel emissions, the percentage will skyrocket to 74 percent by the year 2100. Put another way, by the end of the century nearly three-quarters of the Earth’s population will face a high risk of dying from heat exposure for more than three weeks every year.
Even the best-case scenario shows that nearly half of humanity will be exposed regularly to deadly heat by the year 2100.
This is the worst-case scenario. Even the study’s best-case scenario—a drastic reduction in greenhouse gases across the world—shows that 48 percent of humanity will be exposed regularly to deadly heat by the year 2100. That’s because even small increases in temperature can have a devastating impact. A study published in Science Advances in June, for instance, found that an increase of less than one degree Fahrenheit in India between 1960 and 2009 increased the probability of mass heat-related deaths by nearly 150 percent.
And make no mistake: Temperatures are rising, in multiple ways. “We’ve got a new normal,” said Howard Frumkin, a professor at the School of Public Health at the University of Washington. “I think all of the studies of trends to date show that we’re having more extreme heat, and we’ve having higher average temperatures. Superimposed on that, we’re seeing more short-term periods of extreme heat. Those are two different trends, and they’re both moving in the wrong direction.” Based on those trends, the US Global Change Research Program predicts “an increase of thousands to tens of thousands of premature heat-related deaths in the summer ... each year as a result of climate change by the end of the century.” And that’s along with the deaths we’ve already seen: In 2015, Scientific American noted that nine out of the ten deadliest heat waves ever have occurred since 2000; together, they’ve killed 128,885 people.
In other words, to understand how global warming wreaks havoc on the human body, we don’t need to be transported to some imagined dystopia. Extreme heat isn’t a doomsday scenario but an existing, deadly phenomenon—and it’s getting worse by the day. The question is whether we’ll act and adapt, thereby saving countless lives.
There are two ways a human body can fail from heat. One is a direct heat stroke. “Your ability to cool yourself down through sweating isn’t infinite,” said Georges Benjamin, executive director of the American Public Health Association. “At some point, your body begins to heat up just like any other object. You go through a variety of problems. You become dehydrated. Your skin dries out. Your various organs begin to shut down. Your kidneys, your liver, your brain. As gross as this may sound, you in effect, cook.” (So maybe Wallace-Wells wasn’t being hyperbolic after all.)
Heat death can also be happen due to a pre-existing condition, the fatal effects of which were triggered by high temperature. “Heat stress provokes huge amounts of cardiovascular strain,” said Matthew Cramer of the Institute of Exercise and Environmental Medicine. “For these people, it’s not necessarily that they’ve cooked, but the strain on their cardiovascular system has led to death.” This is much more common than death by heat stroke, but is harder to quantify since death certificates cite the explicit cause of death—“cardiac arrest,” for instance, rather than “heat-related cardiac arrest.”
In both scenarios, the body’s natural ability to cool itself off through sweating has either reached its capacity or has been compromised through illness, injury, or medication. There are many people who have reduced capacity for sweating, such as those who have suffered severe burns over large parts of their bodies. Cramer, who studies heat impacts on burned people, says 50,000 people suffer severe burn injuries per year in America, and the World Health Organization considers burns “a global public health problem,” with the majority of severe burn cases occurring in low- and middle-income countries.
Bodies that are battling illness or on medication may also struggle with heat regulation. Diuretics tend to dehydrate people; anticholinergics and antipsychotics reduce sweating and inhibit heat dissipation. An analysis of the 2003 heat wave in France that killed 15,000 people suggested that many of these deaths could have been avoided had people been made aware of the side effects of their drugs. As for illnesses, “Anything that impairs the respiratory or circulatory system will increase risk,” said Mike McGheehin, who spent 33 years as an environmental epidemiologist at the Centers for Disease Control and Prevention. “Obesity, diabetes, COPD, heart disease, and renal disease.” Kidney disease, mental illness, and multiple sclerosis. The list goes on and on.
This summer has presented many opportunities for bodies to break down from heat. Temperature records, some more than a century old, have been broken across California, Nevada, Utah, Idaho and Arizona. (Speaking of Arizona, it’s been so hot there that planes can’t fly.) And it’s not just America. Last month, Iran nearly set the world record for highest temperature ever recorded. The May heatwave that hit India and Pakistan set new world records as well, including what the New York Times called “potentially the hottest temperature ever recorded in Asia”: 129.2 degrees Fahrenheit. Worldwide, 2017 is widely expected to be the second-hottest year, after 2016, since we began keeping global average temperature records in 1880.
These trends have public health professionals concerned about how people are going to deal with the heat when it comes their way. “Clearly this is one of the most important problems we’re going to see from a public health perspective,” Benjamin said. “This is not a tomorrow problem. It’s a significant public health problem that we need to address today.”
Thanks, Climate Change: Heat Waves Will Keep on Grounding Planes
The West Is on Fire. Blame the Housing Crisis
How Climate Change Denial Threatens National Security
It’s a public health problem especially in cities, says Brian Stone, a professor at Georgia Tech’s City and Regional Planning Program. “Our fundamental work shows that larger cities are warming at twice the rate of the planet,” he said, describing a phenomenon known as urban heat islands, where built-up areas tend to be hotter than surrounding rural areas, mainly because plants have been replaced by heat-absorbing concrete. Global warming is making that phenomenon worse. “We’re really worried about the rate of how quickly we’re starting to see cities heat up,” Stone said.
According to Stone’s analysis, the most rapidly warming city is Louisville, Kentucky, followed by Phoenix, Arizona, and Atlanta, Georgia. But he’s less concerned about cities like Phoenix, which already have infrastructure to deal with brutally high temperatures, than he is about Chicago, Buffalo, and other cities in the northern United States that have really never had to deal with extreme heat. That is precisely why the Chicago heat wave of 1995 that killed 759 people was so deadly. According to the Chicago Tribune, the city was “caught off guard,” and had “a power grid that couldn’t meet demand and a lack of awareness on the perils of brutal heat.”
In other words, Stone and others say, excessive death rates are not always due to just extreme temperatures, but unusual temperatures. People are more likely to die when they are confronted with temperatures they don’t expect and thus aren’t prepared for. That’s why officials in cities not experiencing heat-related extremes need to improve emergency response systems, now. “Those people have got to start thinking in term of, ‘two years ago we had four hot days, the year after we had eight hot days,’” Benjamin said. “Public health systems should be put in place to respond to prolonged heat waves. Emergency cooling centers where people can go should be built. Identify where the people who are most socially isolated live.” Absent preventative action, heat-related deaths in New York City could quintuple by the year 2080, according to recent research.
Some cities have already started to prepare. Stone recently completed a heat adaptation study for Louisville that includes not only emergency management planning but also ways the city can prevent itself from getting so hot (by improving energy efficiency and installing green roofs, for instance). But as for now, he said, it’s rare to see a city actually adopt policies supportive of heat management. “We do see flooding adaptation plans—New York City has one, and New Orleans has one—but heat adaptation planning is a very new idea, in the US and really around the world,” he said. “It takes a lot to convince a mayor that a city can actually cool itself down. It’s not intuitive.”
The good news is that humans adapt to heat, both physiologically (through acclimatization) and socially (with air conditioning, for instance). That will continue, according to the US Global Change Research Program, which states with very high confidence that adaptation efforts in humans “will reduce the projected increase in deaths from heat.”
“It’s a quintessential public health problem in that it impacts the most disenfranchised of our society.”
But there’s a limit to this. “There’s no way to adapt to heat that’s more than a certain amount,” Frumkin said. “And socially, there’s always going to be people we miss, who don’t have access to air conditioning.” McGeehin noted those people will likely be poor, elderly, and minority populations. “It’s a quintessential public health problem in that it impacts the most disenfranchised of our society. Young, healthy, middle-class people will largely be left alone,” he said.
Air conditioners also have limits, especially in cities where blackouts can occur. “It is inevitable,” Stone said, that large cities will see blackouts during future heat waves. “The number of blackouts we see year over year is increasing dramatically,” he said. “Whether that’s caused by the heatwave or just happens during the heatwave doesn’t really matter.... The likelihood of an extensive blackout during a heatwave is high, and getting higher as we add more devices and stressors to the grid.”
It’s a “cruel irony,” Frumkin said, that as the world gets hotter, we need more air conditioning, and thus consume more electricity. And if that electricity comes from fossil fuel sources, it will create more global warming, which in turn will increase the demand for air conditioning. The answer, he said, is to “decarbonize the electric grid.” But that’s easier said than done, especially when the Trump administration is devoted to increasing the use of fossil fuels to support the country’s electrical grid.
As with many other efforts to fight climate change, though, cities don’t need Washington’s help to take action on heat adaptation. “Cities can manage their own heat islands on their own, and that’s where we most need to be focused,” Stone said. But that will require convincing elected leaders that extreme heat is big a threat as, say, rising seas—and one that can’t be addressed with something as obvious as a sea wall. That’s the challenge, says McGeehin: “Heat as a major natural disaster is mostly overlooked in this country.” It’s a quiet killer, and perhaps more lethal because of it.
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
During a recent Home Run Derby, Aaron Judge did something that no one thought was possible. He took a swing and hit a ball so hard that it collided with the ceiling at Marlins Park. The ball hit the ceiling about 170 feet above the ground. The height of the ceiling had been designed by engineers so that balls wouldn't hit it—but clearly, they can.
OK, I don't really want to talk about sports. I want to talk about physics. Just how would you even calculate the height of a baseball's trajectory? I'm not just going to show you how to do it, I'm going to let you do it too.
I'm going to start with the most important physics idea needed for the trajectory of a baseball: the momentum principle. This says that the total force on an object is equal to the time rate of change of the momentum. Momentum is the product of mass and velocity; both it and the force are vectors.
If you know the forces on an object, you can find its change in momentum.  With the momentum, you get the velocity and then can find the new position. That's basically how it works.
After a baseball is hit by the bat, it only has two forces on it (OK, approximately two forces. The first is the gravitational force, a downward force that depends on the mass of the object and the value of the gravitational field (g = 9.8 N/kg).  The second force on the ball is a little more complicated: It's the air resistance force.
Although you don't think about it much, you've felt this air resistance force before. When you stick your hand out of a moving window or when you ride on a bike you can feel the force as you move through the air. One of the simplest models for this force uses the following equation:
That might look complicated, but it's not too bad. The ρ is the density of air (about 1.2 kg/m3 in most cases). The cross sectional area of the object is A and C is the drag coefficient that depends on the shape of the object. Finally, there is the velocity. This model says that as the velocity increases, the air resistance also increases.
But you might notice one little problem with the above expression: It's not a vector. I left that part off for simplicity, but yes—air resistance is a vector. The direction of this force is always in the direction opposite of the velocity vector.
I can find the values of all of these parameters for air drag, and the mass and size of the ball are easily found online. For this calculation, I will use a drag coefficient of 0.3.
Isn't this a projectile motion problem? Couldn't you just use the kinematic equations to find the range of a ball after it was hit? Actually, no. This isn't projectile motion because we are including the drag force. Projectile motion problems have an object with the only force being the gravitational force—and this would be approximately true for baseballs at low speeds. We are clearly not dealing with low-speed balls.
You can't use the kinematic equations because those assume the acceleration is constant. However, as the ball slows down or changes direction the air resistance force also changes. With this non-constant acceleration, there is really only one option: Create a numerical solution.
In a numerical solution, we essentially cheat. Since the problem is that forces are not constant, we can pretend they are constant if we take just a tiny time interval (say 0.01 seconds). During this short time, the velocity and thus the air resistance won't change too much, so I could use the kinematic equations (for constant acceleration). This constant force approximation works—but it leaves us with another problem. If I want to calculate where the ball is after 1 second, I would need to do this calculation 100 times (100 x 0.01 = 1).  And this is where the computer becomes useful (but not required).
If you want to go over the details of creating a numerical calculation, take a look at this post that models the motion of a spring. Otherwise, let's just jump right into the code. Notice that you can indeed change things in the code and run it again—that's the fun part. Just click the "play" to run it and the "pencil" to edit.
This code is written in Python. That means that the number sign (or as my kids call it, the hashtag) at the beginning of line makes it a comment that is ignored by the program. I added a bunch of comments to point out things that you might want to change (like the initial velocity and the launch angle). Go ahead, change something. You won't break it.
Since I gave you the numerical calculation, I also have to give you homework.
A physics professor re-imagines the size and shape of an NFL football in an effort to optimize it for flight. It turns out a couple of small changes can add a whole lot of distance, but how would that change the way the game is played?
In a feat of swagger only NASA could muster, three sunglassed men haul their payload across a lakebed in a 1963 Pontiac convertible. They’re towing what appears to be an enormous bathtub, but is in fact one of the strangest planes ever conceived: the M2-F1. It's a "lifting body," able to take flight without wings. The idea being that an astronaut could pilot a similarly designed reentry vehicle back to Earth (indeed, data gathered with the M2-F1 made it into the space shuttle program). And sure enough, burning across the Mojave, the men in the convertible grin as the brute gets ever-so-barely airborne.
If you’re not hours deep already into the NASA Armstrong Flight Research Center’s YouTube channel, you ain’t living. Especially now that the agency has uploaded a slew of new archival footage that was previously tucked away in a remote part of the internet. It’s a fascinating glimpse into the wild west of aerospace experimentation and the history of humanity’s excursions into space.
Armstrong Flight Research Center is based at Edwards Air Force Base, the desert home of the sometimes mysterious, sometimes bizarre, and almost always dangerous X-plane program. The aerospace achievements that unfolded here are now legend. It was here in 1947 in the X-1 rocket plane, for instance, that Chuck Yeager broke the sound barrier for the first time. And things only picked up from there: X pilots were the first to reach three, four, five, even six times the speed of sound.
What exactly happens when you reach those kinds of speeds? Well, general melty-ness, as NASA learned. One video tours the seared, crumbling body of a X-15A-2 hypersonic jet after it hit Mach 6.7.
It was daring tests like this that helped NASA come to dominate space. And speaking of, the Armstrong Flight Research Center’s feed features some more familiar faces. You can watch Neil Armstrong pilot a Lunar Landing Research Vehicle which, as you might have guessed from the name, prepared him for his trip to the surface of the moon.
And of course there’s the iconic space shuttle. Watch the mating process as it piggybacks on NASA’s 747 transporter. Or watch one come in for a landing through the eye of a long-range infrared camera. Or see what happens when crews respond to a (fake) space shuttle emergency.
So do yourself a favor and check out our highlight reel above, then head on over to the Armstrong Flight Research Center’s YouTube channel for more fascinating windows into NASA's legendary history.
Every day you make thousands of decisions, from the imperceptibly quotidian to those that will change your life forever. But what if instead of listening to the little voice inside your head, you listened to your genes? Your DNA makes you who you are, so theoretically, it could help dictate your daily workout or pick the most palatable bottle of wine. That’s the world that personal genomics company Helix envisions—a genetically optimized existence where every decision can benefit from a peek inside the double-stranded magic eight-ball.
In 2015, Helix spun out of San Diego-based Illumina, the $20 billion genomics giant whose supercomputers sequence about 90 percent of the world’s DNA data. Its goal? To take the sequencing tech used by researchers and doctors and open it up to consumers in a digital marketplace built on DNA. On Monday they launched that platform—a smorgasbord of 18 products designed to turn a one-time genetic donation into a lifetime of insights.
Other companies have offered consumer genetic tests before, of course, from the genealogical insights offered by AncestryDNA and 23andMe to the cancer risk panels promoted by Color Genomics. But each of those tests require a new donation of DNA. Every product on Helix’s marketplace will draw from the same DNA sample, which the company will sequence, store securely, and offer up through an API. Helix’s CEO Robin Thurston says by doing all that dirty work, the marketplace will democratize DNA, making it possible for developers to make new personalized genetics products without huge amounts of up-front capital.
For consumers, that means they can sequence once with the potential to purchase a lifetime of dynamic discoveries. The key word there is dynamic: The marketplace’s offerings will expand as genetic insights improve. Its initial products, like other consumer tests, are constrained by the current state of research. Right now, if you’re generally pretty healthy, there are real limitations to what your genes can tell you—which means you might not want to turn over your decision-making controls to your DNA quite yet.
Here’s how the Helix platform works: Eighty bucks and a spit sample gets you a spot on one of Helix’s sequencing machines, and a chunk of cloud storage for the protein-coding region of your genome—about 22,000 genes, otherwise known as the exome. Normally that would cost somewhere between $500 and $1,000. More commonly, consumer genetic tests use a technology called genotyping to report on certain genetic sites, but even those cost $100 to $250.
So comparatively, Helix’s base sequencing fee is a real bargain. But once you’ve gained access to the marketplace, it’s a pay-as-you-go buffet of personal genomics lifestyle products. Want to see what your genes say about the best exercise for your body? Then you might add one of DNAFit’s fitness apps to your shopping cart. Want to know which foods your body has a harder time metabolizing? Then maybe you want the EmbodyDNA app from Lose It!.
Or, you could just save your money. Eric Topol, a geneticist at Scripps Research Institute and a leader in the US’s most ambitious public sequencing project—the Precision Medicine Initiative—says Helix’s low cost of whole exome sequencing is indeed an impressive achievement. But because you can’t download all your raw data, the price is essentially an illusion, meant to hook people more than return meaningful results. The question consumers should be asking is: What do you really get for it? Today, only about 1 percent of all sequencing tests yield a result that could help users treat or prevent the onset of a disease. And that’s in the oldest field of genomics: medicine. Drawing useful links between genes and diet fads and skincare routines will prove even more challenging. “Sequencing is great for sick people, but for healthy people there really isn’t any proof that it’s informative,” Topol says. “Someday the science might be good enough, but at this moment it seems like it’s not going to yield much for people.”
Which could actually have the opposite of Helix’s intended effect, he cautions. If people get back results that don’t wind up being that relevant or interesting, they could become disenchanted with the whole idea. Or users could over-engage with results that are intended only to educate and inform—not diagnose. As a physician, Topol worries that incidental findings from direct-to-consumer tests could trigger patients to ask for unnecessary tests from their doctors, increasing health care costs and straining resources. Researchers call this phenomenon “raiding the medical commons."
Helix tries to avoid these consequences by carefully evaluating the genetic interpretations and recommendations made by each product, based on the most up-to-date scientific literature. And product developers do their part by setting clear expectations up front. Take Sema4, a company spun out of New York’s Mount Sinai Health System by geneticist and big data evangelist Eric Schadt. It sells tests that tell expectant mothers if they carry a gene that might pass on an inherited disease to their children. Its flagship product uses DNA sequencing to screen for disease-causing mutations for 281 different genetic conditions, and is ordered only at the recommendation of your physician. But Monday, Sema4 announced a new, less expensive test on Helix’s marketplace that identifies genetic indications for only 67 diseases, which you can order yourself from the comfort of your home—with a small caveat. After you select the test, Sema4 requires that you submit some additional health information to a doctor online, who then reviews it before giving the final okay. That's a precaution the company takes to stay on the right side of current regulations.
Technically, the FDA has the authority to regulate genetic test kits, though the exact scope of the agency's jurisdiction is still a matter of legal dispute. As it stands now, the degree of FDA oversight of a test depends on its intended use and the risk posed by an inaccurate result. To date, the agency has mostly turned a blind eye to direct-to-consumer products (with the notable exception of 23andMe). So as long as companies don’t overpromise on the clinical validity of results, they can pretty much escape regulatory scrutiny.
Which is why Schadt says the product isn’t for people actively trying to get pregnant “We view it as an educational guide in the long term family planning journey, much further upstream than a clinical care scenario,” says Schadt. The idea is that if women see anything of concern show up in their simple screen, they’ll come back to Sema4 for the clinically comprehensive test when it comes to actually making a baby. But they shouldn’t use the product on the Helix marketplace to make any serious family planning decisions.
From a business perspective, the appeal of Helix’s platform is obvious. But owning DNA also presents some thorny ethical issues. Bioethicists have pointed out that when companies have a financial incentive to keep genetic data private, information that could otherwise be used for the common good stays behind closed doors. Helix’s parent company, Illumina, has a pretty good reason to keep those exomes for itself. In January, it spun out another company, called Grail, to leverage big DNA data to commercialize a technology called liquid cancer biopsy. It detects cancer in its very earliest stages by finding tiny amounts of tumor DNA in a drop of blood. A massive genetic database would give Grail a boost on validating that technology. While Helix wouldn’t comment on how the company plans to use its genetic information internally, it did say it doesn’t have any plans to share data with any third parties, to support external research efforts or otherwise.
Illumina, the Google of Genetic Testing, Has Plans for World Domination
The Cure for Cancer Is Data—Mountains of Data
A Single $249 Test Analyzes 30 Cancer Genes. But Do You Need It?
These are the kinds of concerns Sema4’s Schadt had previously tried to confront, by opening up his company’s own genetic library to academic and nonprofit medical researchers for free. And while that vision is at odds with Helix’s own vice-grip policies, he still found something in the partnership that he couldn’t get on his own: volume. While Sema4 doesn’t receive any raw sequencing data from its Helix-based customers—just a report on the 67 diseases in the screen—it’s still worth it to get more long-term users. “It’s not perfect,” says Schadt, who spent many months working with Helix to come to a data-sharing agreement. “But it’s not bad either. I would love to have every single variant that Helix is generating, but this still gives us a far larger pool to go after to for studying diseases.”
Last year, Schadt published a paper in Nature Biotechnology describing something he calls the Resilience Project. The idea is to find people with mutations that should result in terrible disorders, but for some reason don’t. Something in their DNA protects them, and could offer clues to potential treatments. For that paper, Schadt analyzed the genomes of nearly 600,000 people—an incredibly time and cost-intensive project. He’s betting that Sema4, and Helix, can help speed up the treasure hunt. Some of the genes that harbor these disease-causing mutations are on the smaller Sema4 screening test, so Schadt is still hoping he can use that data to recruit individuals for further study. “Helix provides us this really broad net,” he says. “It’s a cool way to identify promising people.”
Other health researchers are also interested in that broad net. Though not on the list of marketplace products announced Monday, two big hospital networks with hundreds of thousands of patients—Geisinger Health and the Mayo Clinic—have plans to launch products there later this year. Being able to tie even limited genetic data with information from those patients’ electronic health records is the kind of thing that gets even skeptics like Topol excited. But even so, he thinks it will be at least five years before the science really catches up to the product platform’s promises. If Helix can ride the wave that long, you might actually be able to stake some everyday decisions on your DNA.
Dr. Euan Ashley and his team are gathering DNA samples from the most elite endurance athletes on the planet to find the genetic reasons that they are so fit.
Matthieu Komorowski wanted to be an astronaut. Still does. The French-born anesthesiologist, currently getting a PhD at Imperial College London, applied to the European Space Agency in 2008. But he knows his chances are limited. “Being basically a medical resident I didn’t get very far in the selection,” Komorowski says. “But I’ve been working on building up my skills.”
Among those skills: administering anesthesia for surgery. And as Komorowski found when he started looking at the literature on space medicine, that might be more helpful than it sounds. Of all the concerns about astronaut safety and health, traumatic injury is the one that worries people the most. It has the biggest potential impact on a mission and, worse, it’s the one people know least about.
In part that’s because it has never happened. Over decades of Apollo, Mir, Skylab, space shuttle, and International Space Station missions, astronauts have had medical concerns and problems—and, of course, there have been deadly catastrophes. But no astronaut has ever had a major injury or needed surgery in space. If humans ever again venture past low Earth orbit and outward toward, say, Mars, someone is going to get hurt. A 2002 ESA report1 put the chances of a bad medical problem on a space mission at 0.06 per person-year. As Komorowski wrote in a journal article last year, for a crew of six on a 900-day mission to Mars, that’s pretty much one major emergency all but guaranteed.
Worst case: Someone goes outside the spacecraft to fix something heavy and it gets away from them, crushing an arm or a leg. The astronaut gets exposed to vacuum, but makes it back inside the vehicle—dehydrated, partially frozen, bleeding heavily, in shock. What happens next will depend on whether the crew is in orbit around Earth, or in interplanetary space—and on what kind of gear is on board.
NASA doesn’t seem headed for Mars any time soon, but people like Elon Musk are making noises about missions as early as the end of this decade. At the International Astronomical Conference in Guadalajara last September, Musk described plans for a Mars mission that seem to now be delayed or scaled back. But he still says SpaceX is going. Speaking to the ISS Research and Development Conference in Washington DC on July 19, Musk also said: “If safety is your top goal, I would not go to Mars.”
Yes, sure, space is unsafe. Even if you manage to stave off killer radiation, you still have to worry about muscles atrophying and bones getting less dense—and more breakable—in weightlessness. Not to mention the ever-present danger, thanks to long-term isolation in a confined space, of “psychiatric decompensation.” That’s NASA-talk for catastrophic marbles-losing.
Spend a long time in space, though, and your body starts to change in all sorts of other ways, too, and they all make traumatic injuries even worse. Your total amount of circulating blood and red cell mass goes down. Your blood vessels don’t constrict and dilate as well. That suite of cardiovascular problems adds up to what on Earth would look like the result of significant blood loss—and this is before you get injured. Your hormones go kind of wonky, and your immune system and wound healing get sluggish. Your bones break more easily and heal more slowly, if at all. Meanwhile, infectious bacteria become more resistant to antibiotics, and, oh, hey, you know how you always get sick after a “long” airplane flight? Imagine if the flight lasted two years.
Thanks to a freedom-of-information request from Vice, the medical gear on board the ISS is public knowledge. The crew has access to a small but professional pharmacy, including some serious drugs and EpiPens. They have an automated emergency defibrillator, gear to administer intravenous fluids, and diagnostic equipment like blood pressure cuffs. The ISS also carries an ultrasound device, for example—the only sophisticated imaging device on board, but one that’s great at finding internal bleeding and monitoring fluid levels in eyeballs, a thing astronauts have to worry about so they don’t go blind. It might also have therapeutic uses. Oh, and they have some dental equipment, which, nope, hard pass. “When it boils down to it, there’s a few things we train to handle right away,” says Steve Swanson, who commanded the ISS for six months in 2014. “Anything besides that, we were going to be calling the ground.”
Swanson learned to insert a chest tube and do a tracheotomy on a goat during training, and spent some time assisting in an emergency room. But even with that experience behind him, he and his fellow astronauts wondered how a real emergency would actually play out. “We always think about worst-case scenarios. What would you do if there was a little hole? A big hole? What would you really do?” he says. “If someone is really bad, we’ll throw them in a Soyuz and come down. But that’s not an easy trip.”
Essentially, ISS crews learn to mostly stabilize and restrain an injured astronaut, and then call the ground to talk to a flight surgeon. Anil Menon, one of about 20 NASA flight surgeons, wouldn’t tell any specific stories about astronauts’ medical problems—doctor-patient confidentiality applies even in space. But over the years he has done everything from answering a slightly worried email from the ISS to a full-blown team meeting with specialists teleconferencing in.
That’s all possible if you’re in low Earth orbit, where the ISS is. The communications delay from Johnson Space Center to the floating lab is basically nil, and in the event of a serious injury, an astronaut could nominally get into a Soyuz capsule docked at the ISS and come home.
On the other hand, “de-orbiting” is the kind of decision that goes all the way to the flight director and head of NASA—and it might not even work. “If someone breaks a leg, how would you get them in the suit?” Swanson asks. The Soyuz capsule is a cramped fit. “They’re really bent up in there.” If the patient is intubated, on a ventilator with oxygen tanks, they won’t fit into the Soyuz at all, much less into a pressure suit.
So NASA is sponsoring all kinds of research to try to figure this stuff out. Researchers on parabolic “vomit comet” airplane flights with brief periods of weightlessness have performed intubations, opened and closed wounds, repaired blood vessels, and done all kinds of other gory stuff in animals. One team even cut a benign tumor off a human man’s arm.
Even administering drugs gets harder in space. “Once you pop a blister tab, a pill is exposed to air and becomes oxidizable, so it decays in terms of usefulness,” says Menon. IVs rely on gravity on Earth; in space, you need a pump, and bubbles that would otherwise float to the top stay in solution, potentially posing the threat of embolism. Peggy Whitson, on the ISS right now, has been experimenting with those procedures. “You need a lot of fluid, but that’s a lot of mass and volume that we don’t have up there,” Menon says. “And bubbles float around in weird places. She had a lot of problems with that.”
Some of the biggest challenges remain the messiest. In space, blood can splatter even more than it usually does on Earth, unconstrained by gravity. Or it can pool into a kind of dome around a wound or incision, making it hard to see the actual trauma. (Fun fact: If you are bleeding more than 100 milliliters per minute, you are probably doomed. An amazing 2009 paper in the Journal of Trauma Management & Outcomes called “Severe Traumatic Injury During Long Duration Spaceflight” suggests that an onboard computer monitoring hemorrhage rate could see that and ping the Chief Medical Officer to say, yeah, don’t use any more fancy anti-coagulant bandages on that guy—he’s a goner.)
One cool idea for dealing with the spurting/pooling blood problem in space is to seal a wound or incision site in a kind of bubble filled with fluid, like saline, and then operate laparoscopically, with tiny instruments on extended arms. A team led by James Antaki, a biomedical engineer at Carnegie Mellon, actually tried it on a simulated bleeding arm on a vomit comet mission four years ago. “I wimped out on going,” Antaki says. His first version had a flexible collar with gaskets for instruments and a transparent top, almost like a diver’s mask. “I’ve evolved it into a flexible, blister-like enclosure that’s puncturable,” he says. “It’s transparent so you can see what’s bleeding, the vessels and vasculature, and you poke through with an instrument, make stitches or retract and resect, cauterize, and go.” It’s made of a thick elastomer reinforced with a fiber mesh that stays closed almost like a self-sealing tire; Antaki hopes to send the latest version on a SpaceX mission to the ISS this autumn for testing—on a simulator, not an astronaut.
And Komorowski, the would-be astronaut anesthesiologist? It turns out all that cardiovascular “reconditioning”—the loss of blood volume and overall slowdown—can be catastrophic for anesthesia. “The drugs we use to put people to sleep during general anesthesia are actually quite dangerous. They lower blood pressure. They dilate blood vessels,” he says. Administering them requires really finicky training to tailor dosages to different people’s metabolisms even on Earth, and that ignores the problem of how to get complicated, often flammable gases on board a spacecraft.
Komorowski suggests adding something new to the space exploration pharmacopeia: the hardcore dissociative anesthetic ketamine. “It’s used throughout the world in hostile environments,” he says. “It doesn’t impair hemodynamic systems. The cardiovascular system is preserved, so it’s suitable for patients after blood loss, in shock, or severely dehydrated.” And it’s safe. “Even if you get it wrong and give five times too much, most likely not much is going to happen.” (Except, you know, a certain kind of party might break out.)
NASA, meanwhile, has awarded dozens of grants to researchers trying to better understand the physiology of space travel and possible medical interventions. Menon says they might be able to get around the signal-delay problem with interplanetary telemedicine by sending multimedia tutorials on a long duration mission, or by having procedures with hard stops built in after certain steps. That way, astronauts doing the meatball surgery could stop, stabilize their colleague, and await evaluation and further instructions.
If people are going to leave orbit, though, that research is going to have to head for the final frontier, too. “I think something that would be ethically acceptable would be to try a sedation in space, because the risk is really moderate, and we could learn a lot,” Komorowski says. “It’d have to be done by an anesthesiologist to start with, so I volunteer to go.” See, now he’s working the angles.
1 UPDATE 7/25/17 4:30 PM Added a link to the report
Many people know that work on nuclear weapons enabled the development of the first electronic computers. But it’s no less true that the humble refrigerator, in a roundabout way, enabled the development of the first atom bomb.
While reading the newspaper one morning in 1926, Albert Einstein nearly choked on his eggs. An entire family in Berlin, including several children, had suffocated a few nights before when a seal on their refrigerator broke and toxic gas flooded their apartment. Anguished, the forty-seven-year-old physicist called up a young friend of his, the inventor and scientist Leo Szilard. “There must be a better way,” Einstein pleaded.
Szilard, a stocky man of 28, had first impressed Einstein six years earlier by proving him wrong on a certain scientific point. (That didn’t happen often.) Szilard also had a knack for turning esoteric ideas into useful gadgets. In later years he became a sort of Thomas Alva Edison of high-energy physics, sketching out the first electron microscope and particle accelerator; he and Einstein had bonded in part over their love of such mechanical devices. (Although a theorist and somewhat flighty, Einstein came from a family of tinkerers—his uncle Jakob and father Hermann had invented new types of arc lamps and electricity meters—and he’d worked in the Swiss patent office for seven years.) So when Einstein called Szilard that morning, the two men agreed to collaborate and build a better, safer refrigerator.
This wasn’t as odd as it might sound: in the previous half century, refrigeration had become serious science. The study of thermodynamics and heat had led to the concept of absolute zero—the coldest possible temperature—and several labs around the world were racing to reach the bottom of the thermometer. Some of the best science revolved around attempts to liquefy certain gases: nitrogen, oxygen, hydrogen, methane, carbon monoxide, and nitric oxide. Throughout the 1800s this sextet—the so-called permanent gases—had resisted all efforts to liquefy them. This stubbornness had led some scientists to declare that these six gases could never be liquefied, that they somehow stood apart from the rest of matter. Other scientists said baloney—that powerful new cooling methods would eventually condense them. In particular, the latter group pinned their hopes on a clever, cyclical cooling process that involved removing heat from substances in several stages.
Stage one involved filling a chamber with a gas that was easy to liquefy. Call it A. Scientists first compressed A with a piston, then cooled down the compression chamber with an external jacket of cold water. As soon as A had chilled down, a valve opened. This dropped the pressure on A and allowed it to expand into a larger volume. The key point is that expanding into a larger volume takes energy, takes work. (It’s similar to how a litter of puppies, if locked in a broom closet, would suddenly expend a lot more energy if you opened the door and let them run free inside the house.) And in this situation, the only energy A can draw on to expand and spread is its own internal store of heat energy. But depleting its internal store of heat energy inevitably cooled A down even more, and it eventually condensed into a liquid at around –100°F.
Now came the clever part. The next stage involved a chamber of gas B, which was tougher to liquefy. Scientists once again compressed B with a piston to start. But for the cooling jacket this time, instead of cold water they ran liquid A through the jacket. This dropped gas B’s temperature to –100°F. Opening a valve then caused B to expand, which forced B to deplete its internal store of heat energy. Its temperature plunged to around –180°F, whereupon it also liquefied.
Liquid B could now be used in another cooling jacket to liquefy a more stubborn gas, C, and so on alphabetically. This bootstrapping process finally reached temperatures so low (circa –420°F) that not even “permanent” gases could resist, and all six were eventually liquefied. Especially beautiful was liquid oxygen, which glowed faintly blue, like liquid sky.
Gas refrigeration remained a mere curiosity, however, until the Guinness Brewing company invested in the technology around 1895. Before this, breweries generally brewed beer only in the winter and stored it. (Lager means “storage” in German.) Refrigerators let Guinness make beer year-round, thank goodness. As a knock-on technology, the rest of the world got commercial refrigerators, like the one in your home right now. All modern fridges rely on the same general principles of gaseous cooling.
Sam Kean is the New York Times bestselling author of The Tale of the Dueling Neurosurgeons, The Disappearing Spoon, and The Violinist’s Thumb.
If you tore out the inner panels on your fridge, you’d see a series of tubes. Inside the tubes you’d find a liquid (call it Z) with a low boiling point. As the casseroles and other leftovers inside your fridge emit heat, Z absorbs the heat through the fridge walls and warms to a boil. The resulting gaseous Z then floats away through other tubes, carrying the heat with it.
Next, Z enters a compression chamber, which compacts the gas with a piston. (The motor that runs the compressor causes the characteristic hum of refrigerators.) The compressor now pushes warm gas Z through still more tubes behind the fridge, which allows Z to jettison heat to the outside world. At this point the gas has successfully removed heat from inside the unit and dumped it out back. And after Z dumps enough heat, it condenses back into liquid. Now Z passes through an expansion device that lowers its pressure, cools it further, and completes the cycle. Liquid Z reenters the tubes inside the fridge panels, reboils, and resumes sucking out heat.
Now, one detail here might sound suspicious. You’re boiling a liquid (Z), so shouldn’t everything heat up? Not quite. The liquid heats up, yes. But in an enclosed unit like a refrigerator, the liquid can warm itself up only by stealing heat from your casserole: warming the one necessarily cools the other. And the boiling is indeed crucial. Remember James Watt’s old bête noire, latent heat? This principle says that liquids changing into gases absorb ridiculous amounts of energy. In Watt’s engines this was a bug, but fridges make it a benefit: absorbing heat and whisking it away is exactly what refrigerators aim to do, and nothing does that better than liquids changing into gases. (This same general process explains why liquid sweat, when it evaporates, cools you on a summer day.)
By the 1920s gas-compression refrigerators had replaced iceboxes all across Europe and North America. There was only one problem.  All  three  gases  commonly  used  as  coolants  then—ammonia, methyl chloride, and sulfur dioxide—were toxic and occasionally killed whole families. (Methyl chloride sometimes exploded, too, just for fun.) Hence Einstein’s vow to find “a better way.” He knew the weak point in home refrigerators was the compressor, whose seals often cracked under pressure. So he and Szilard designed a fridge without a compressor, a so-called absorption fridge.
In the simplest type of absorption fridge you start with two liquids mixed together in a chamber, the absorbent and the refrigerant. The key to the design is that, at low temperatures, these substances mix readily. But if you raise the temperature—usually by warming the chamber with a small methane flame—the refrigerant boils out as gas, leaving the absorbent behind.
The refrigerant gas now goes on a long and tortuous journey. It first flows into tubes behind the fridge and dumps the heat it absorbed from the flame; this step simultaneously cools the refrigerant back into liquid. This liquid flows via gravity into the panels inside the fridge, where it sucks the heat out of yet another casserole. Absorbing this heat causes the liquid to reboil, and the resulting gas whisks the latent heat away, removing it from the unit’s interior. (In some designs the gas then heads to still more tubes behind the fridge, to jettison heat one last time.)
Meanwhile, back in the original chamber, the methane flame has switched off, allowing the absorbent there to cool down. A jacket of cold water then cools the absorbent further. The absorbent cools so much, in fact, that when the refrigerant gas finally wends its way back into the chamber, the absorbent condenses it into liquid again and reabsorbs it. You therefore end up back where you started, with a mix of two liquids that you can separate with a flame. Overall, absorption fridges and regular fridges cool things down the same way, by boiling gases. But they use a different process to recycle the refrigerant.
Again, though, this probably sounds like cheating: a flame can cool my beer? But that’s the magic of gases. Really, the flame here isn’t so much adding heat as doing physical work—separating the refrigerant from the absorbent by turning the refrigerant into gas. And once you have a free gas in the system, you have oodles of options. Indeed, the art of refrigeration consists of manipulating gases to absorb heat energy here, carry it there, and dump it somewhere else. Hearkening back to Thomas Savery, you could call the Einstein-Szilard refrigerator an engine for freezing water by fire.
The Einstein-Szilard fridge actually used three liquids and gases, not two, making it a tad more complicated than the scheme above. But their design did have several advantages over regular fridges. With no motor, it made no noise and rarely broke down. It also used no electricity (just methane), and it avoided the seals that all too often broke and leaked toxic gas.
In looking back on this episode, some historians have assumed that Einstein merely offered advice on the patent applications or used his celebrity to lure investors, leaving the real work to Szilard. In truth Einstein labored over the project, and the duo ended up receiving dozens of patents in six countries on different fridge components. (An American patent attorney reviewing applications did a double take, as well he might, when he noticed Einstein’s signature.) The duo ended up selling several patents and collecting a nice check for $750 (around $10,000 today); they subsequently opened a joint checking account, like a married couple. Szilard collected an additional $3,000 per year in consulting fees.
Like any married couple, though, they clashed sometimes. Szilard had an engineer’s appetite for complexity and kept adding new valves and cooling lines to the fridge. Einstein, meanwhile, longed for simplicity and elegance — no less in his home appliances than in his physics. (He would have hated working with James Watt.) The need for simplicity eventually drove Einstein and Szilard to invent two other cooling units, each of which worked on a different physical principle. In one they replaced the piston in a standard fridge with molten sodium, which magnets pumped up and down to compress gases. The other device used water pressure from a kitchen faucet to power a small vacuum pump; the pump then cooled things by evaporating methanol. Einstein called the latter device Der Volks-Kühlschrank, the people’s fridge.
In the end, sadly, none of the three Einstein-Szilard fridges ever made it into anyone’s home. Not surprisingly, the molten-sodium pump proved a wee bit impractical for your average kitchen (though it later found use in nuclear power plants). The faucet cooler failed because German apartment buildings had lousy water pressure, which hindered the vacuum pump. And absorption fridges simply burned too much fuel to compete with compression fridges; the Einstein-Szilard design seemed like a Newcomen engine in comparison.
Yes, I Found Einstein’s Brain
Physicists Find Another Gravitational Wave to Suggest Einstein Was Right
Even Einstein Didn't Think Gravitational Waves Existed
Even the biggest objection to conventional fridges, the lethal gases, became moot in 1930 with the debut of a new and nontoxic cooling gas, Freon. Within a decade, virtually all home units had switched to this chlorofluorocarbon, and the Einstein-Szilard fridge was rendered a historical relic. Of course, Freon did have one pesky drawback. When old refrigerators went to the junkyard, the Freon leaked out and climbed into the stratosphere. There, ultraviolet light cleaved the chlorine atoms off, creating free radicals that chewed through ozone molecules with sickening efficiency: each chlorine radical can destroy 100,000 O3 molecules over its lifetime. This destruction eventually opened up a hole in the ozone layer that still exists and that won’t recover for decades, if ever. Humanity might have saved itself a lot of trouble in the long run by investing in the Einstein-Szilard approach to cooling water with fire.
So was the Einstein-Szilard fridge a waste of these men’s time and talent? Not entirely. Einstein found the work a refreshing break from his futile search for a Theory of Everything. With two families to support and a crumbling German economy, Einstein also enjoyed the extra cash. Szilard needed the money even more, especially after he fled Nazi Germany for London in 1933. (He was part Jewish.) He spent the next few years living off his fridge proceeds, and he used his sudden freedom to take long walks and ponder what the next big thing in physics might be. The answer came to him one afternoon in September 1933, as he stepped off a curb near the British Museum. He’d been hearing about some experiments involving the release of subatomic particles called neutrons. He started wondering what would happen if, say, a uranium atom split and released multiple neutrons. Other nearby uranium atoms might absorb them, become unstable, and release neutrons themselves when they split. These secondary neutrons would destabilize more atoms, which would release tertiary neutrons, and so on. Each atom that split would also—according to his patent partner’s famous equation, E = mc2—release energy in an ever-growing cascade . . .
By the time he crossed the street, Szilard had worked out the principle behind the first nuclear chain reaction. And unlike his clever fridges, this invention became all too pervasive in the turbulent decades to follow—decades that would shatter not only the public’s belief in benevolent science, but scientists’ belief in a neat, tidy, predictable universe.
CAESAR’S LAST BREATH by Sam Kean. Copyright © 2017 by Sam Kean. Reprinted with permission of Little, Brown and Company.
LG's Signature Fridge and Samsung's Family Hub Refrigerator concepts are a glimpse of the future of connected, touchscreen and app-controlled refrigerators that might even order their own groceries.
More than a quarter billion people today are infected with the hepatitis B virus (HBV), the World Health Organization estimates, and more than 850,000 of them die every year as a result. Although an effective and inexpensive vaccine can prevent infections, the virus, a major culprit in liver disease, is still easily passed from infected mothers to their newborns at birth, and the medical community remains strongly interested in finding better ways to combat HBV and its chronic effects. It was therefore notable last month when Reidun Twarock, a mathematician at the University of York in England, together with Peter Stockley, a professor of biological chemistry at the University of Leeds, and their respective colleagues, published their insights into how HBV assembles itself. That knowledge, they hoped, might eventually be turned against the virus.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Their accomplishment has gained further attention because only this past February the teams also announced a similar discovery about the self-assembly of a virus related to the common cold. In fact, in recent years, Twarock, Stockley and other mathematicians have helped reveal the assembly secrets of a variety of viruses, even though that problem had seemed forbiddingly difficult not long before.
Their success represents a triumph in applying mathematical principles to the understanding of biological entities. It may also eventually help to revolutionize the prevention and treatment of viral diseases in general by opening up a new, potentially safer way to develop vaccines and antivirals.
In 1962, the biologist-chemist duo Donald Caspar and Aaron Klug published a seminal paper on the structural organization of viruses. Among a series of sketches, models and X-ray diffraction patterns that the paper featured was a photograph of a building designed by Richard Buckminster Fuller, the inventor and architect: It was a geodesic dome, the design for which Fuller would become famous. And it was, in part, the lattice structure of the geodesic dome, a convex polyhedron assembled from hexagons and pentagons, themselves divided into triangles, that would inspire Caspar and Klug’s theory.
At the same time that Fuller was promoting the advantages of his domes—namely, that their structure made them more stable and efficient than other shapes—Caspar and Klug were trying to solve a structural problem in virology that had already attracted some of the field’s greats, not least among them James Watson, Francis Crick and Rosalind Franklin. Viruses consist of a short string of DNA or RNA packaged in a protein shell called a capsid, which protects the genomic material and facilitates its insertion into a host cell. Of course, the genomic material has to encode for the formation of such a capsid, and longer strands of DNA or RNA require larger capsids to shield them. It didn’t seem possible that strands as short as those found in viruses could achieve this.
Then, in 1956, three years after their work on DNA’s double helix, Watson and Crick came up with a plausible explanation. A viral genome could include instructions for only a limited number of distinct capsid proteins, which meant that in all likelihood viral capsids were symmetric: The genomic material needed to describe only some small subsection of the capsid and then give orders for it to be repeated in a symmetric pattern. Experiments using X-ray diffraction and electron microscopes revealed that this was indeed the case, making it apparent that viruses were predominantly either helical or icosahedral in shape. The former were rod-shaped structures that resembled an ear of corn, the latter polyhedra that approximated the sphere, consisting of 20 triangular faces glued together.
This 20-sided shape, one of the Platonic solids, can be rotated in 60 different ways without seeming to change in appearance. It also allows for the placement of 60 identical subunits, three on each triangular face, that are equally related to the symmetry axes—a setup that works perfectly for smaller viruses with capsids that consist of 60 proteins.
But most icosahedral viral capsids comprise a much larger number of subunits, and placing the proteins in this way never allows for more than 60. Clearly, a new theory was necessary to model larger viral capsids. That’s where Caspar and Klug entered the picture. Having recently read about Buckminster Fuller’s architectural creations, the pair realized it might have relevance to the structures of the viruses they were studying, which in turn sparked an idea. Dividing the icosahedron further into triangles (or, more formally, applying a hexagonal lattice to the icosahedron and then replacing each hexagon with six triangles) and positioning proteins in the corners of those triangles provided a more general and accurate picture of what these kinds of viruses looked like. This partitioning allowed for “quasi-equivalence,” in which subunits differ minimally in how they bond with their neighbors, forming either five-fold or six-fold positions on the lattice.
Such microscopic geodesic domes quickly became the standard way to represent icosahedral viruses, and, for a while, it seemed that Caspar and Klug had solved the problem. A handful of experiments conducted in the 1980s and ’90s, however, revealed some exceptions to the rule, most notably among groups of cancer-causing viruses called polyomaviridae and papillomaviridae.
It became necessary once more for an outside approach—made possible by theories in pure mathematics—to provide insights into the biology of viruses.
About 15 years ago, Twarock came across a lecture about the different ways in which viruses realize their symmetrical structures. She thought she might be able to extend to these viruses some of the symmetry techniques she had been working on with spheres. “That snowballed,” Twarock said. She and her colleagues realized that with knowledge of structures, “we could make an impact on understanding how viruses function, how they assemble, how they infect, how they evolve.” She didn’t look back: She has spent her time since then working as a mathematical biologist, using tools from group theory and discrete math to continue where Caspar and Klug left off. “We really developed this integrative, interdisciplinary approach,” she said, “where the math drives the biology and the biology drives the math.”
Twarock first wanted to generalize the lattices that could be used so she could identify the positions of capsid subunits that Caspar and Klug’s work failed to explain. The proteins of the human papilloma viruses, for instance, were arranged in five-fold pentagonal structures, rather than hexagonal ones. Unlike hexagons, however, regular pentagons cannot be built from equilateral triangles, nor can they tessellate a plane: When slid next to each other to tile a surface, gaps and overlaps inevitably arise.
So Twarock turned to Penrose tilings, a mathematical technique developed in the 1970s to tile a plane with five-fold symmetry by fitting together four-sided figures called kites and darts. The patterns generated by Penrose tilings do not repeat periodically, making it possible to piece together its two component shapes without leaving any gaps. Twarock applied this concept by importing symmetry from a higher-dimensional space—in this case, from a lattice in six dimensions—into a three-dimensional subspace. This projection does not retain the periodicity of the lattice, but it does produce long-range order, like a Penrose tiling. It also encompasses the surface lattices used by Caspar and Klug. Twarock’s tilings therefore applied to a wider range of viruses, including the polyomaviruses and papillomaviruses that had evaded Caspar and Klug’s classification.
Moreover, Twarock’s constructions not only informed the locations and orientations of the capsid’s protein subunits, but they also provided a framework for how the subunits interacted with each other and with the genomic material inside. “I think this is where we made a very big contribution,” Twarock said. “By knowing about the symmetry of the container, you can understand better determinants of the asymmetric organization of the genomic material [and] constraints on how it must be organized. We were the first to actually float the idea that there should be order, or remnants of that order, in the genome.”
Twarock has been pursuing that line of research ever since.
Caspar and Klug’s theory applied only to the surfaces of capsids, not to their interiors. To know what was happening there, researchers had to turn to cryo-electron microscopy and other imaging techniques. Not so for Twarock’s tiling model, she said. She and her team set out hunting for combinatorial constraints on viral assembly pathways, this time using graph theory. In the process, they showed that in RNA viruses, the genomic material played a much more active role in the formation of the capsid than previously thought.
Specific positions along the RNA strand, called packaging signals, make contact with the capsid from inside its walls and help it form. Locating these signals with bioinformatics alone proves an incredibly difficult task, but Twarock realized she could simplify it by applying a classification based on a type of graph called a Hamiltonian path. Imagine the packaging signals as sticky pieces along the RNA string. One of them is stickier than the others; a protein will adhere to it first. From there, new proteins come into contact with other sticky pieces, forming an ordered pathway that never doubles back on itself. In other words, a Hamiltonian path.
Coupled with the geometry of the capsid, which places certain constraints on the local configurations in which the RNA can contact neighboring RNA-capsid binding sites, Twarock and her team mapped subsets of Hamiltonian paths to describe potential positions of the packaging signals. Weeding out the unpromising ones, Twarock said, was “a matter of taking care of dead ends.” Placements that would be both plausible and efficient, enabling effective and rapid assembly, were more limited than expected. The researchers concluded that a number of RNA-capsid binding sites must occur in every viral particle and are probably conserved features of genome organization. If so, the sites might be good novel targets for antiviral therapies.
Twarock and her colleagues, in collaboration with Stockley’s team in Leeds, have employed this model to delineate the packaging mechanism for several different viruses, starting with the bacteriophage MS2 and the satellite tobacco mosaic virus. They predicted the presence of packaging signals in MS2 in 2013 using Twarock’s mathematical tools, then provided experimental evidence to back up those claims in 2015. This past February, the researchers identified sequence-specific packaging signals in the human parechovirus, part of the picornavirus family, which includes the common cold. And last month, they published their insights into the assembly of the hepatitis B virus. They plan on doing similar work on several other types of viruses, including alphaviruses, and hope to apply their findings to gain a better understanding of how such viruses evolve.
When Twarock’s team announced their finding on the parechovirus in February, headlines claimed they were closing in on a cure for the common cold. That’s not quite right, but it is a goal they’ve kept in mind in their partnership with Stockley.
The most immediate application would be to find a way to disrupt these packaging signals, creating antivirals that interfere with capsid formation and leave the virus vulnerable. But Stockley hopes to go a different route, focusing on prevention before treatment. Vaccine development has come a long way, he acknowledged, but the number of available vaccines pales in comparison to the number of infections that pose threats. “We’d like to vaccinate people against several hundred infections,” Stockley said, whereas only dozens of vaccines have been approved. Creating a stable, noninfectious immunogen to prepare the immune system for the real thing has its limitations. Right now, approved strategies for vaccines rely on either chemically inactivated viruses (killed viruses that the immune system can still recognize) or attenuated live viruses (live viruses that have been made to lose much of their potency). The former often provide only short-lived immunity, while the latter carry the risk of being converted from attenuated viruses to virulent forms. Stockley wants to open up a third route. “Why not make something that can sort of replicate but doesn’t have pathological features to it?” he asked.
In a poster presented at the Microbiology Society Annual Conference in April, Stockley, Twarock and other researchers describe one of their current areas of focus: using the research on packaging signals and self-assembly to probe a world of synthetic viruses. By understanding capsid formation, it may be possible to engineer viruslike particles (VLPs) with synthetic RNA. These particles would not be able to replicate, but they would allow the immune system to recognize viral protein structures. Theoretically, VLPs could be safer than attenuated live viruses and might provide greater protection for longer periods than do chemically inactivated viruses.
Swirling Bacteria Illuminate the Strange Physics of Swarms
A 'Digital Alchemist' Unravels the Mysteries of Complexity
Evolution Is Slower Than It Looks and Faster Than You Think
Twarock’s mathematical work also has applications beyond viruses. Govind Menon, a mathematician at Brown University, is exploring self-assembling micro- and nanotechnologies. “The mathematical literature on synthetic self-assembly is quite thin,” Menon said. “However, there were many models to study the self-assembly of viruses. I began to study these models to see if they were flexible enough to model synthetic self-assembly. I soon found that models rooted in discrete geometry were better suited to [our research]. Reidun’s work is in this vein.”
Miranda Holmes-Cerfon, a mathematician at the Courant Institute of Mathematical Sciences at New York University, sees connections between Twarock’s virus studies and her own research into how tiny particles floating in solutions can self-organize. That relevance speaks to what she regards as one of the valuable aspects of Twarock’s investigations: the mathematician’s ability to apply her expertise to problems in biology.
“If you talk to biologists,” Holmes-Cerfon said, “the language they use is so different than the language they use in physics and math. The questions are different, too.” The challenge for mathematicians is tied to their willingness to seek out questions with answers that inform the biology. One of Twarock’s real talents, she said, “is doing that interdisciplinary work.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
It’s been less than a year since Elon Musk announced his plans to settle humans on Mars during a talk in Guadalajara, Mexico. On stage at the International Astronautical Congress, the billionaire invoked the lore of Hitchhikers Guide to the Galaxy and Battlestar Galactica while describing a massive passenger ship loaded with the essentials—you know, like a movie theater and a restaurant. SpaceX hoped to launch these breezy cruises to the red planet in the early 2030s.
Plot twist: Musk's original vision is no longer canon in his universe. On Wednesday, Musk took questions during a keynote discussion at the International Space Station R&D conference in Washington, DC. In between dad jokes about tunnel digging, a staple artificial intelligence threat assessment, and a spirited attempt to unpack the potential for interplanetary war, he candidly revealed a series of obstacles for SpaceX and its plan to build a city on Mars. SpaceX is rebooting its colonization plan, and may pivot to focus on a moon base that would aid that effort.
The Hawthorne, California-based spaceflight company has spent years touting propulsive landing technology for the next version of its Dragon spacecraft. SpaceX expected to equip the Dragon V2, rated for crew and cargo, with four small SuperDraco engines and deployable landing legs to allow for a guided surface touchdown—first on the Earth’s surface, and then, maybe, on Mars. SpaceX was confident enough in the design to propose a variant of the vehicle Musk claimed would be able to land anywhere in the solar system.
The pitch for those uncrewed Red Dragon missions to Mars included a collaboration with NASA to gather landing data, test communications, and plan for potential contamination from Earth-based microbes. The space agency, of course, has its own boots-on-Mars ambitions, and hopes to send astronauts to the red planet aboard the Orion spacecraft by 2040. Musk would later compare Red Dragon launches to a “train leaving the station,” delivering cargo and science to Mars in preparation for a human mission.
But now, SpaceX has pulled the plug on its prologue to an interplanetary future.
Musk explained that Red Dragon was no longer in line with the evolving vision SpaceX has for getting to Mars—specifically, the part where you have to land on Mars. The company is hitting pause on the development of its propulsive landing technology on the Dragon V2 spacecraft. Musk argued that while the technology works, SpaceX would be put through the wringer trying to meet NASA’s safety standards for landing a human crew on the ground. “It doesn’t seem like the right way to apply resources right now,” Musk said. “I’m pretty confident that is not the right way, and that there’s a far better approach.” He later tweeted that SpaceX would still land with propulsive thrusters on Mars, but with a larger spacecraft.
SpaceX has had a busy year adding to its growing arsenal of recovered rockets while launching more times than any other year since its founding. The company also managed to re-fly both its Falcon 9 rocket and Dragon cargo capsule. In the flurry of praise surrounding rocket landings and Mars concepts, the fact that SpaceX has yet to attempt or complete a deep space mission of any kind still weighs on the company’s future. Red Dragon would have been SpaceX’s first toe into the deep end of the pool.
Its journey would have begun atop the triple-booster Falcon Heavy rocket, the famously-delayed launch vehicle that Musk claims has over twice the payload capability of a single Falcon 9 rocket, able to easily deliver 100,000 pounds to low-Earth orbit. At the ISS R&D conference, Musk invited the audience and those watching the livestream to witness the launch of the vehicle—currently projected for this fall—from Kennedy Space Center. But he followed with an uneasy disclaimer: “Real good chance that vehicle doesn’t make it to orbit.”
That uncertainty doesn’t bode well for Musk’s original Mars ambitions. Musk argued that the Falcon Heavy was impossible to test on the ground due to the machine’s complexity. And he said that development was far more difficult than SpaceX expected, admitting that the company was naive in its original projections. The simultaneous firing 27 orbital engines notwithstanding, launching a Falcon Heavy includes changing aerodynamics, heightened vibration, and an enormous thrust that pushes qualification levels of the flight hardware to the limit. Musk admitted on Wednesday that limited damage to former Apollo 11 Pad 39A would be a “win” in the aftermath of the Falcon Heavy test flight. Along with Musk, the audience laughed nervously.
According to Musk’s keynote this week, SpaceX is planning to scale down its Mars-bound spacecraft to a size suitable for a wider range of missions—missions that would help pay for its development costs. A size reduction would certainly have a large economic impact on manufacturing, but savings could be augmented by focusing all efforts on a single reusable vehicle that could serve both low-Earth orbit and deep space. And Musk also offered that building a base on the moon is essential to getting the public excited about space again.
Those Veggies Grown on the ISS Get Humans Closer to Mars
SpaceX's Plan to Reach Mars by 2018 Is ... Actually Not That Crazy
Tiny, Laser-Beaming Satellites Could Communicate With Mars
But is that a suggestion to another company? To NASA? Or is SpaceX going to unveil plans for a moon base as part of their updated Mars architecture?
Elon Musk has said that he would offer priority seating to NASA for missions to lunar orbit. SpaceX was the first private company to dock with the space station and the success between the federal space agency and the spaceflight company could point to a continuing partnership that expands beyond low Earth orbit. The ISS won’t be around forever, and with NASA shifting toward deep space exploration, the opportunity to give the agency a lift is there. Especially if NASA wants to return to the moon.
But that doesn’t mean SpaceX is abandoning its Mars ambitions; far from it. SpaceX owes much of its financial and development success to its partnership with NASA, and there’s no doubt Musk will pursue that partnership beyond low-Earth orbit. That means that NASA astronauts could one day be flying on these deep space missions under lucrative taxpayer-funded contracts. Before then, SpaceX will have to fully prove its technology, along with life support systems and radiation protection for crewed missions.
Just a week ago, Musk dispatched SpaceX VP Tim Hughes to make the case for deep space in front of the Senate Subcommittee on Space, Science & Technology. Hughes used the success of SpaceX and NASA’s commercial resupply missions and the governing Commercial Orbital Transportation Services program to make a case for partnership in deep space exploration. "To this day, America’s achievement of landing men on the moon and returning them safely to Earth likely represents humankind’s greatest and most inspirational technological achievement,” he said. “Now, other nations like China seek to replicate an achievement America first accomplished 48 years ago.“ Maybe SpaceX can add private companies to the roster.
The Mars Reconnaissance Orbiter has seen many places on the planet. One of the most interesting is one of the great canyon systems on Mars.
In a room at Northwestern Medicine Chicago Proton Center, Robert Johnson keeps a small collection of plastic heads. At first glance, they look like they’ve been lopped off the top of department store mannequins. But they’re more lifelike than that—made of materials that mimic bone, flesh, and brain. “One of them even has a gold filling,” he says.
For the last six years, Johnson, a physicist at the University of California, Santa Cruz, has been working on a machine that shoots protons through the human skull. His goal: to use protons instead of conventional X-rays to take 3-D images inside cancer patients. But first, he has to perfect the technology on his model skulls.
His prototype can map the dummy’s head in about six minutes. It can find the gold filling inside the dummy’s mouth. And most importantly, it can recognize a tumor. While his machine isn’t yet good enough to make a diagnosis—X-ray images still have better resolution—that’s not the point. Johnson thinks that a proton-based image, even a blurry one, can guide a cancer treatment known as proton therapy better than a conventional X-ray.
Proton therapy fights cancer by bombarding tumors with, well, protons. But before doctors send in the protons, they have to design a treatment plan based on a 3-D image of the tumor. Right now, these images are CT scans, which see inside a patient with X-rays. From that scan, doctors calculate how much energy the protons need to hit the tumor—a complicated sequence of conversions and estimates to translate an image into a treatment.
That’s where Johnson’s prototype comes in. If you have a proton-based image, you can skip those conversions and design a more precise, more effective treatment plan, Johnson says.
Advocates of proton therapy say that it’s the most advanced form of radiation therapy today. In many ways, it’s safer and more effective than chemotherapy and conventional X-ray-based radiation therapy. Protons don’t really damage healthy tissue, because doctors can target them to release most of their energy at a specific depth inside the patient. “You don’t get any damage beyond the tumor itself,” says Bill Hansen, the director of proton therapy marketing at Varian, a company that makes cancer therapy machines for hospitals. X-rays, on the other hand, damage tissue wherever they go, sometimes causing serious side effects. Breast cancer treatment with X-rays increases the risk of a heart attack, for example, because of the left breast’s proximity to the heart.
Critics of proton therapy, however, say it’s highway robbery. Proton therapy machines are behemoths that require a circular particle accelerator the size of a room and expensive superconducting magnets. All together, they can cost $20 million or more—about 10 times the cost of a conventional X-ray radiation machine. While Medicare does cover proton therapy, some patients have trouble getting insurance companies to cover it because of its cost.
Cancer Rates Spiked After Fukushima. But Don't Blame Radiation
How Hiroshima Survivors Are Leaving a Legacy For Science
Mini Particle Accelerators Make Cancer Treatment Safer For Everyone
That’s why researchers like Johnson are tweaking the technology in the hopes of making the therapy more mainstream. Johnson’s prototype was a long time coming—his collaborator, oncologist Reinhard Schulte of Loma Linda University, began working on this prototype all the way in 1998. Back then, the US only had one hospital-based proton therapy machine, installed at Loma Linda in 1990.
Proton therapy has since gotten more affordable, Hansen says. In recent years, companies have cut costs five times by shrinking the machines. Loma Linda’s first proton machine, still in use, accelerates protons around a circular track with a diameter the length of a tennis court. More recent models are almost 10 times smaller. And because protons are more precise, a patient may not have to schedule as many appointments on a proton treatment plan compared to a conventional radiation one.
As prices dropped, demand for the therapy rose a bit. In the US, only two medical centers offered proton therapy in 2003. Now more than 25 do. Because radiation damage of healthy tissue in growing children is especially harmful, doctors often recommend proton therapy to kids with cancer. “It’s now the gold standard for treating children,” Hansen says. But most cancer patients aren’t kids, and the technology still hasn’t really taken off.
Johnson and Schulte’s prototype doesn’t fix the cost barrier—it actually makes proton therapy more expensive. But their hope is that an even more precise proton therapy machine, aided by their proton imaging, will make it more attractive to hospitals. Proton therapy is capable of extremely powerful performance, but no one knows how to make it shine yet. “It’s kind of like driving an airline jet on the ground instead of flying it,” Hansen says. “To a certain extent, it’s a misuse of the technology.” Hard to think of a better use than on mannequin heads, though.
Researchers have loaded a robot with AI that lets it scan an object and determine how best to grab it.
If you're not already excited about the total solar eclipse that will be visible in the United States on August 21, maybe you haven't been paying attention. During a solar eclipse, the shadow of the moon falls on the Earth's surface—an event that hasn't been visible in the lower 48 states since 1979. And yes, it should be visible to just about everyone (wearing appropriate glasses), though you might not notice that big of a difference. Even with a little bit of sunlight going past the moon, the sky will appear to be pretty darn bright.
But if you want to experience the full power of the solar eclipse, you need to be in the region of total blackout (the umbra). The rest of the nation will just be in a partial shadow of the moon (the penumbra). When the moon completely covers the sun in this region, you will know it. It will be as dark as night. That's where I want to be—and where most of the country's eclipse nerds will, too. Which raises a question: Is it possible for everyone in the US to see the full solar eclipse? Could everyone get in their cars and drive to a place inside the path of the moon's shadow? Would there be enough room for all?
Let's begin with some crazy approximations. My general rule is that I don't want to look up things unless I absolutely have no choice. For everything else, I will just use my best judgement to get values.
There are two kinds of solar eclipses—annular and total. For an annular eclipse, the apparent angular size of the moon is a little bit smaller than the angular size of the sun. This means that in the best position, there is still a ring of fire (not really fire) around the shadow of the moon. For a total eclipse, the moon will completely cover the sun. This is much cooler to see.
The size of the moon's shadow depends on both the Earth-moon distance and the Earth-sun distance. Both of these distances change over the course of a year since neither the Earth's orbit nor the moon's are perfectly circular. OK, so say that the moon and the sun are at certain distances. How big would the moon shadow be? Let me start with a simplified diagram of the sun-moon-Earth system.
Of course this isn't drawn to scale—that's pretty much impossible. But for this calculation, focus on the white right triangle that I have included. This will have one side as the radius of the sun—but I will also need the radius of the moon and the distance from the sun to the moon. I am going to now just focus on the triangle (without showing the sun and moon).
I'll gloss over most of the details, but the key is to focus on the red and the green triangles in this diagram. These have to be similar triangles (based on the angles and stuff). Remember that for two similar triangles, the ratios of corresponding sides are the same.  For the red triangle, the vertical side is equal to the difference between the radius of the moon and the sun. For the green triangle, the vertical side is the difference in the spot size (on the Earth) and the radius of the moon. It looks complicated, but once you have those two triangles, it's not too difficult to solve for the size of the spot. Note that I am assuming the Earth-moon distance includes the radius of the Earth (it doesn't). I am also assuming that the surface of the Earth is perpendicular to the spot shadow (which would be true near the equator). If the spot was near the North or South pole, it would be stretched out and not circular.
How to Get the Perfect Eclipse Shot Using Your Smartphone
There's Still Time to Prepare Yourself for the Apoceclipse
NASA Unleashes Two Vintage Warplanes to Chase the Eclipse
Now I just need the exact distances for the sun and moon. I will use this site for the Earth-sun distance and this site for the moon distance on a date very close to August 21.  Plugging these values in (with the radius of the sun and moon), I get a spot radius of 53.7 km (that's a spot diameter of about 66 miles). I'm sure that's not the exact spot size, but it's close enough for me.
The question is not how many people can fit in the shadow, but how many people could view the total eclipse. Since this spot travels all the way across the US, it's like a really long and skinny rectangle. The width of this will be 107 km (I will just go with 100 km) and a length of perhaps 5,000 km. That gives a viewing area of 0.5 million km2.
How does this viewing area compare to the total area of the lower 48 states? Let me just approximate the US (lower 48—that's the last time I'm going to say it) as a giant rectangle that is 5,000 km by 2,000 km for a total area of 10 million km2. That means the viewing area is 5 percent of the total area. This is important for the next calculation.
This one is tough. It's not just about the available viewing area, it's also about parking. All of these people have to get into the viewing area somehow. They are going to have to drive—really, there's no other way. But how many parking spaces are there? I am just going to have to make three assumptions. First, the parking space density is uniform.  This is obviously not true. There are probably more parking spaces in Nashville than there are in the middle of Oregon (although maybe there aren't many open parking spaces in Nashville). Second, there is one car for every two people in the USA.  If there are 300 million humans, that would make 150 million cars. Third, there are four parking spaces for every car (yes, I just totally guessed). Obviously there are more spaces than cars—just look at all the empty parking lots on the weekend. That puts the total parking spaces at 600 million.
Now I can estimate the parking spaces in the viewing area. Since this is 5 percent of the total area, it would be OK to estimate that 5 percent of the parking spaces are in the viewing area. That would be 30 million spots for cars. If each car can carry five humans, that makes for 150 million viewers. That's a bummer for about half of the country's population. Who gets to be in the viewing area? How about a lottery system where everyone flips a coin? You get "heads," and congratulations, you get to see the total solar eclipse!
OK, this is just an estimation. I think it's possible I'm off by a factor of two, which means it's possible everyone could fit in this viewing area. But what would that be like? It would probably be hell on Earth. Traffic would completely suck. Think of the giant lines at Starbucks. What about the strain on the power grid and sewer systems? What about internet access? I think I would prefer to see the total eclipse from home and have the whole state to myself.
On August 21, darkness will wash over America. But in a good way, we promise. Here's everything you need to know if you want to catch a glimpse of the solar eclipse.
During a solar eclipse, the Moon slides in front of the Sun, blocking it perfectly, and a swath of the world goes dark. And on August 21, that darkness will pass from west to east, from Oregon to South Carolina.
For the millions watching, it will be an awe-inspiring event, as the moon blocks an entire star from view, casting an umbra across the continent. But for the towns hosting those millions, it’s also a logistical nightmare. Places in the path of "totality"—where a full eclipse occurs—will have more visitors than perhaps ever before. The website Great American Eclipse estimates that 12.25 million people live within the path, and between 1.85 and 7.4 million will travel to it. Those numbers make it hard to plan for just how taxed their roads, gas reserves, watering holes, bathrooms, and food services will be.
So to make their eclipse memorable and safe, towns have turned to expert outsiders. And there's one in particular who can help: psychologist, author, and eclipse consultant Kate Russo. Based in Belfast, Ireland, she's the world expert in eclipse-specific community planning, and she's committed to helping the largely small towns across America prepare for the experience.
One doesn't become the preeminent community eclipse guru overnight. Russo began with years of obsessive eclipse-chasing, traveling far from Northern Ireland to see totalities in action. She had been watching for more than a decade when a solar eclipse finally visited her native continent of Australia. This time, she went back home to work with local officials, launch her first book on eclipse-chasing, and do psychological research. She interviewed first-time eclipse viewers, surveying them before and after the event to see how the actual experience compared with their anticipation.
But as she spoke to people around town, she realized that for many locals, that "before" period was dominated by worry—mostly about tourist traffic. Some people even planned to duck out and avoid the whole hassle. “They really couldn't see that it was for them,” she says.
After the eclipse, Russo interviewed the local coordinators, to find out what had gone well and what they would do differently. That work, and preparatory outreach visits to their archipelago starting in 2013, caught the notice of the Faroe Islands near Denmark, which was getting ready for its own solar eclipse in 2015. The organization Visit Faroe Islands appointed Russo as their official consultant, where she educated local leaders on what happens during eclipses, helped create public-oriented brochures, advised on ill weather, and established safety standards for viewing filters. Afterward, she wrote a white paper on community eclipse planning and revealed it at a 2015 eclipse-planning meeting of the American Astronomical Society in Portland.
On August 21, 2017, the Earth will cross the shadow of the moon, creating a total solar eclipse. For the first time in almost 40 years, the path of the moon's shadow passes through the continental United States.
Since then, it's become the go-to guide for umbra-encumbered regions around the world, distributed by, among others, the American Astronomical Society. It's a digestible 14-page guidebook for locals who hope to coordinate eclipse efforts in their communities.
Earlier this year, Russo began consulting remotely, videoconferencing with interested communities along the August eclipse's path. For each place, she helped people consider how to deal with all those other humans, educate residents and tourists, disseminate day-of information, and delegate to local leaders. "Imagine you are tasked with having to prepare your community for the event of a lifetime—except you have no personal experience of this event, no idea what to expect or even how many will be coming," the pitch on her site goes. "This is the reality for every community that finds themselves along the path of totality."
Many of the precepts laid out in Russo's white paper are reflected in the US's internal prep team. The country has been readying itself for this eclipse for a while, an effort led by the American Astronomical Society’s “Eclipse Task Force." The Force aims to educate people on what eclipses are and how to stare at them safely. Members of the team want people to see the eclipse, rather than staying home to stay out of traffic, and so help coordinate government (national, state, local) agencies. Those agencies, in turn, can themselves coordinate official viewing areas, traffic reroutes, and extensive Porta-Pottie networks.
Astronomer Angela Speck is on that task force, and is a driving force in her own community of Columbia, Missouri, one of few big-ish cities in the eclipse’s path this year. She’s been talking to libraries, emergency management organizations, the chamber of commerce, the state’s science-teacher organization—everybody. And that’s because this eclipse is different from most eclipses: Instead of humans having to go to the eclipse, the eclipse is coming to them.
“Eclipses are usually in places that are hard to get to,” Speck says, “just because most of the planet is places that are hard to get to.” When the solar system makes an eclipse easy to get to, the planet’s inhabitants have a lot more to worry about. Imagine your town’s biggest event, Speck suggests. In Columbia, that event would be graduation, or a big ballgame. “You can't go out to eat; you can't book a hotel room,” she says. “This is going to be much, much worse.”
Solar Eclipse Images Show Dazzling Corona Detail
How to Watch the Total Solar Eclipse if You're on the Wrong Side of the Planet
Videos Capture Total Solar Eclipse From Space and Earth
So much worse, in fact, that she then compares it to prepping for a zombie apocalypse. “We're not going to have anybody eating brains, but zombies don't need to eat and sleep,” she says. Eclipse hounds do.
Communities need to know that, and get ready. Many—like Jackson Hole, Wyoming, where lots of people will go because it’s is beautiful like a different planet and even better when you cover it up with darkness—have their own websites. There, visitors and residents can find special events, designated viewing areas, safety tips for how not to go blind, pleas to please get gas ahead of time and know that your cell phone probably won’t work, and requests to avoid flash photography during the dark time.
Towns and businesses also have physical coordination to do. They have to stock up on food, and request that businesses not price gouge. They have to disseminate weather forecasts so viewers can decide whether to bail for a sunnier spot. They have to reroute traffic and shut down the streetlights near viewing areas. They have to control crowds. “Peace officers must understand the emotion that will be released during the event," says the website Eclipse2017.org, "and maintain professionalism at all times to ensure an enjoyable experience for all participants."
The whole Department of Transportation has even gotten involved: “Why:” its website explains, “a planned special event for which there has been no recent precedent in the United States.”
Still, not every town in the United States has equal and adequate support or knowledge. Russo knew that the thousand-ish communities along the eclipse path would need help—her help. After all, many of them didn't even have the benefit of a big ballgame's worth of experience with big crowds. "The plan was to follow up on these communities that got in contact [remotely], to give much more guided and tailored input, and to be a resource on the ground to as many communities as possible," she says.
For that, she needed to be mobile, so she started planning that most American of activities: a camper-van road trip, along the path of the totality. She planned to stop and consult with interested locales on the trail, and settle in Oregon at the end, where she herself would watch the eclipse. To that end, in January 2017, she put out an "Expressions of Interest" call, which netted 90 days of work and 180 associated events that hinged on Russo's in-person guidance, to start in April.
But she ran into trouble, thanks to that other most American of enterprises: bureaucracy. “I can't get a labor certificate,” she says, “because there's no such job as eclipse consultant.” Official immigration was the only option that would allow her to perform work and apply for grants in the US, so in summer 2016, she applied for a visa under the category of “alien of exceptional ability.” And though US officials granted her petition in November, she still hasn't received her visa—even after six requests to expedite the rest of the process.
As a result of the processing time, Russo has had to forgo nearly all of her boots-on-the-ground consultations. Although she was able to visit Nebraska for 10 days, most of the towns that were counting on her have missed her.
It wasn't until June 30 that Russo received a letter from the National Visa Center (NVC). "The applicant is now in the queue awaiting an interview appointment overseas, where a consular officer will adjudicate the applicant’s visa application," it read. "Most appointments are set within three months of NVC’s receipt of all requested documentation." After the interview, in London, officials require 10 more business days to return passports and finish processing. That puts her visa approval sometime around October—when, note, there are no eclipses in North America.
Russo has a visa waiver to come as a mere tourist, with a few engagements, for the eclipse itself. She wants to see it: After all, before she was a consultant, she was a chaser. And she will be one of the many millions, watching the sun disappear for a while—an event that hopefully no one will miss because of an avoidable traffic jam.
On August 21, darkness will wash over America. But in a good way, we promise. Here's everything you need to know if you want to catch a glimpse of the solar eclipse.
Let's pretend that the US didn't recently pull out of the Paris Climate Agreement. Let's also pretend that all the other countries that scolded it for withdrawing also met their Paris pledges on deadline. Heck, let's pretend that that everyone in the whole world did their very best to cut emissions, starting today. Even if all that make-believing came true, the world would still get very hot.
Fact is, if you add up all the emissions cuts every country promised in their Paris pledges, it still wouldn't keep the planet's temperature from rising beyond the agreement's goals—to keep global temperatures from rising more than 2˚ C higher than they were before the Industrial Revolution, and as close to 1.5˚ C as possible. If Earthlings want to avoid a heat-soaked, tide-swamped, and war-clouded future, they need to do more. This raises the specter of geoengineering: things like seeding the stratosphere with sulfur, or using ice crystals to dissolve heat-trapping clouds. But geoengineering is a dirty word many climate scientists and climate policy experts avoid, because humans meddling with nature doesn't have the best track record. Which is why they say world leaders need to come up with some rules about geoengineering ASAP, before desperation over the coming climate catastrophe forces humanity to do something it might well regret.
Geoengineering strategies generally fall into two categories: removing carbon dioxide and reducing heat. The former problem has vexed researchers for years. Sure, they can do it on small scales—carbon scrubbers are essential life support aboard closed systems like the International Space Station and submarines. But installing systems large enough make a dent in all those parts per million is functionally impossible. It would be expensive, energy-intensive, and also nobody really knows how to do it. Doing the same with reforestation would require covering nearly half of all world’s landmass with trees. Not likely to happen. And despite the hype, carbon capture and storage—sucking the stuff up before it leaves the smokestack and pumping it underground—is still in its infancy.
Heat reduction is currently more practical. You can do it many ways, and all of them involve either blocking the sun's heat from coming into Earth’s atmosphere, or allowing more of Earth's heat to radiate into space. For blocking heat, sulfur injections are probably the most likely to work. "It scatters and reflects solar radiation back into space," says Ulrike Niemeier, a climate scientist at the Max Planck Institute for Meteorology, and co-author of a new paper in Science discussing that geoengineering technology and its risks. The concept comes from volcanic eruptions. Large ones send gobs of sulfur dioxide into the stratosphere, triggering temporary global cooling events. After Mount Pinatubo erupted in 1991, scientists measured 17 million tons1 of additional sulfur dioxide in the atmosphere. The Northern Hemisphere cooled by about 0.5˚ to 0.6° C in the aftermath.
Compared to carbon removal, sulfur injection isn't so hard. The basic technology already exists: High-flying jets capable of carrying tanks of sulfur into the stratosphere. The difficulty arises when you consider the scale at which you'd have to deploy those jets in order to get meaningful cooling. Niemeier and her co-author estimate that 1˚ C of cooling would require 6,700 flights a day. Over the course of a year, that would cost around $20 billion.
Can a Million Tons of Sulfur Dioxide Combat Climate Change?
Climate Change and the Terrible, No-Good Odds of Bad Weather
Weaponizing the Climate: Geoengineering's Military Potential
OK, how about that other tack, letting Earth shed more heat into space? Similarly, this strategy would involve high-flying jets. Their targets would be cirrus clouds, those wispy strokes of white common on pleasant days. Cirrus clouds form high in the atmosphere, and are made from particles of ice. "The cirrus clouds that form at high altitudes absorb some of the radiation that would otherwise be emitted to space. In that sense they act similar to greenhouse gases," says Ulrike Lohmann. That’s a different Ulrike than the author of the previous paper; Ulrike was just a popular name for girls in north Germany for a while, and this one is a climate scientist at ETH Zurich's Institute of Atmospheric and Climate Science and co-author of a separate Science paper describing how eliminating these high altitude cirrus clouds could cool the planet.
The concept is sort of counterintuitive. Cirrus clouds are made of ice. In order to prevent them from forming, jets would have to seed the atmosphere with tiny particles like desert dust or pollen. These act as nuclei for ice to form around. The idea is that these particles will cause fewer, but larger ice crystals to form than would in a typical high altitude cirrus cloud. "This reduces both the amount of scattered sunlight and allows more longwave radiation to escape to space," says Lohmann.
Of course, sending a fleet of planes to wage war on high altitude clouds would be a similarly expensive endeavor. But Lohmann points out that it would probably be a better option. In addition to allowing radiation to escape, the large ice crystals would take up more of the water vapor present in the upper atmosphere. "Since water vapor is also a greenhouse gas, reducing water vapor in the upper troposphere also contributes to reducing the warming effect," she says.
That said, neither she nor Niemeier advocates employing either technology at the moment. Too many unknowns. Suddenly cooling the planet could cause freaky weather all over the world. It could interrupt India's annual monsoon. The globe's wind patterns could change completely. Plus, you'd have to keep flying planes into the stratosphere for a very long time—remember, all that carbon dioxide is still in the atmosphere, releasing trapped heat. Also, poisoning the ocean.
But the technical challenges of geoengineering are minimal compared to the challenges governments face in deciding when, if, and how to deploy these technologies. The biggest worry of all is that some desperate country, or group of countries, might decide to do some geoengineering all on their own. "Imagine if somebody starts flying planes in the atmosphere full of sulfur dust, and then India's monsoons are late. This would be a geopolitical crisis," says Janos Pasztor, the executive director of the Carnegie Climate Geoengineering Governance Initiative, and co-author of yet another Science paper, this one specifically addressing the policy implications raised by the former two.
So, first step is starting some kind of international dialogue involving as many countries as possible. Hmm... wonder what that would look like. And this dialogue would begin from virtual ignorance on geoengineering. Tweaking the global thermostat is going to require a lot of control, and nobody really knows how much will be too much.
There is also the question of local and regional risks—what happens if one part of the world bears  more of the geoengineering side effects than everyone else? Then there's a thing called the termination effect: Once you start geoengineering, you can't stop. "If you stop quickly, the temperature will jump up to what it would have been, and that will be catastrophic," says Pasztor. And most urgent of all is how to conduct research on geoengineering technologies. Because you basically have to use Earth as a laboratory.
Obviously, humans are already using Earth as a laboratory—and the current experiment is akin to cranking up every the Bunsen burner in the building before leaving for a holiday weekend. The best course of action would be to calmly, purposefully turn them all off, rather than wait for the building to catch fire and douse it with flame retardant foam. Same with planet Earth. "I want to highlight that we aren't promoting geoengineering, we are promoting dialogue," says Pasztor. Emissions reductions are far cheaper, and more effective, at curbing climate change than technological fixes. Carbon sequestration, difficult as it is, should come next. Geoengineering ought to be deployed only as a last resort, and only with a good plan in place.
1 UPDATE 7/24/2017 4:30pm ET — This originally stated that Pinatubo injected 17 Kilotons of sulfur into the stratosphere. That's several orders of magnitude less than the actual figure.
Underwater robots do a lot of neat things—take photos of underwater volcanoes, track leopard sharks, and explore shipwrecks—but they could still learn a few things from fish. Especially the rocket-fast, insanely agile tuna.
Tuna are built to cruise across oceans, usually around 2 mph. But they can crank up to 45 mph at the drop of a snack (Michael Phelps races at around 5 or 6 mph, for comparison). And tuna are able to whip after fast-turning squids or sardines. They owe their agility, in part, to a newfound hydraulic system that allows them to raise and lower some specialized fins. These sickle-shaped fins on the top and bottom of its body aren’t for thrust. At full extension, they stabilize the fish’s body at high speeds. When the fins are lowered, a tuna can turn on a dime. And as researchers report in a study published today in Science, the fact that these fins are controlled with the help of hydraulics—and not muscles alone—could teach them a lot about bringing underwater robots up to speed.
In the early days of bioinspired robotics, engineers came up with the robotuna, a machine intended to cruise as well as its scaly role model. Robots have gotten better at being in the ocean since then, but aquatic life still beats them in the power category. Fish don’t run out of batteries. They’re also never held back by fiber-optic tethers. Your average wild tuna, with help from a myriad of specialized features, maneuvers and cruises continuously for more than a decade. But roboticists may be in luck—biologists are still discovering new ways that fish function underwater.
Finding a totally new piece of the tuna’s anatomy intrigued Barbara Block, who has been studying fish at Stanford’s Hopkins Marine Station for decades. A biomimetics researcher in her lab, Vadim Pavlov, was studying tuna fins to develop tracking tags when he dissected his way to terra incognita. “We found this strange system of channels, muscles and bones," Pavlov says. "They looked very strange, and they looked like disconnected pieces of a big puzzle. We had no idea what they did."
Like any good dissector, he injected a dyed silicone gel through the network, mapping the channels in bright blue. The dye highlighted a large chamber at the base of the sickle-shaped median fins, and smaller channels nestled between the fish’s back muscles and the fin also turned blue. Color spread into the fin too, darkening separate channels around the bony rays that hold the fin up like tent poles. It looked like the tuna could contract muscles at the base of the chamber to pressurize it, squeezing fluid out into the smaller channels and elevating its fin.
There was only one way to be sure. Pavlov targeted a spot just behind the fin—probably the least leaky route to the chamber beneath—and used a syringe of saline solution to artificially pressurize the main chamber. Bingo. The fin popped right up.
That was exciting, but it still left lots of questions about how the hydraulic system actually aids tuna maneuvers. Proving something on a dead fish doesn't quite cut it.
So Block and her team analyzed high-speed video of tuna schools in 20,000 gallon research tanks, looking to see if swimming tuna indeed raise and lower their fins at the angles that Pavlov experimentally observed. Pavlov translated those measurements into a computerized fluid dynamics model. By looking at the lift and drag on a fish in 120 simulations of swimming, he found that raising those median fins stabilizes a tuna like a yacht keel—preventing rollover. In quick turns, backing off on stability sped up maneuvering.
One Man's Quest to CT Scan All the World's Fishes
Print an Army of Giant, Articulated Fish From This 3-D Database
The Biologist Trying to Make the First Pregnancy Test for Sharks
The origins of the pressurized fluid within the channels were still a mystery. If you think of other, er, biological hydraulics, blood comes to mind, but that didn’t seem quite right in this case. “First of all the liquid itself, it’s not red and thick like blood, it’s a pinkish pale color,” says Benyamin Rosental, a Stanford immunologist that Pavlov enlisted for help. Because a pinker color probably meant more white blood cells than red ones, Rosental suspected the stuff came from the lymphatic system—known more for immune function and circulation than biomechanics. And sure enough, when he sorted and checked the hydraulic fluid’s components, he found cell types typical of the lymphatic system.
It’s not totally clear why the hydraulics and musculature work together, but it could be that using fluid is energy efficient. After all, biology is all about return on caloric investment. “Muscle is expensive to grow, expensive to maintain, and expensive to use, energetically,” says Brooke Flammang, a biomechanics researcher at Rutgers University-Newark. Fluid, on the other hand, takes less effort for mechanical returns.
It’s also possible that the lymphatic system gives fish better fin control. Engineers leverage hydraulics in situations where they need precise control and large forces. With snacks at stake, it makes sense that a tuna would want to steer well. And although autonomous undersea vehicles don’t usually hunt squid, they do have similar energy constraints. Batteries could last longer with efficient fluid-driven boosts for propulsion or steering. Robots might do well to follow those tuna into the hydraulics game.
Adam Summers is on a mission to scan all 33,000 species of ray-finned fish—and upload all of that data for anyone to make amazing 3D images, just like we did.
In December of 2016, a team of researchers showed up at Romy Camargo's house with a better-than-average holiday gift. The front of the nondescript silver box lowered—like one of those spaceship doors from Star Wars, minus the dramatic clouds of vapor—to reveal a fetching robot, with cameras for eyes and a flatscreen for a hat. With the assistance of its human handlers, the Human Support Robot, as Toyota calls it, wheeled into Camargo's home on a mission: to support the quadriplegic veteran and in the process pave the way for truly useful care robots.
First, though, the HSR had to surmount a host of obstacles.
Robots already work well in uniform environments. That's why self-driving cars are so promising: Urban planners have certain rules for signage, for instance, that the car can read. And industrial bots have already taken over factory floors. “Robotics works very well in a more manufacturing situation, where you can make the environment very static,” says Allison Thackston, a roboticist at the Toyota Research Institute who's helping develop the HSR. “You can structure it in a very specific way and the robot can do the same repeatable action over and over again very quickly and very accurately.”
But a human home is anarchy. Even if you’re working with a cookie-cutter floor plan in a McMansion development, what’s inside the home is changing day by day or hour by hour. So the HSR, a wheeled robot with a single arm outfitted with a gripper to snag objects like bottles and even a vacuum to suction-grasp pieces of paper, has to adapt to the chaos. To find its way around, it uses 3-D cameras and lasers—just a like a self-driving car, only it’s necessarily far more cautious with its speed. And for the time being, it has to identify objects in Camargo's home using QR codes.
For Camargo—who was shot in the neck during combat operations in Afghanistan in 2008—that means he can use a mouth stick (think of it like one of those pens that work on touchscreens, only longer and held in the mouth) to navigate a special interface and command the robot to, say, fetch a bottle of water. The operators have already given the robot a hint as to what room it would find the bottle in, so then it's a matter of the HSR getting there and recognizing the QR code it's after. After getting good grasp, the robot makes its way back to Camargo.
The bot can pull off other tricks like opening doors. “I would be there by the window outside my door, and it would come up to me and do facial recognition,” Camargo says. “And then it would just go backwards and open the door, which is another thing I didn't have to have my nurse around for.” That means more freedom both for Camargo and for his nurse, who can instead concentrate on more complex tasks. The HSR, after all, isn’t meant to be a replacement for caregivers, but a complement to their services.
So the HSR requires humans to order it around. But you can imagine a day when robots will, for instance, pick up clutter on their own so the elderly don’t trip. Really, just like the HSR has to adapt to unique environments, it also has to adapt to unique users. For now, that's a matter of its handlers programming it for different situations. But in the future, expect bots to more actively adapt to our various needs.
Meet Salto, the One-Legged Robot With an Incredible Leap
Gecko-Inspired Gripper May Soon Snag Space Junk
Google.org's Giving $20 Million to Engineer a Better World for the Disabled
Human needs won't just be dictated by age or health, because not everyone is going to be comfortable with a robot autonomously wandering their home. “If we have the robot that has the capability to both identify, manipulate, and navigate to a high level of autonomy, then we might pair that with what the user is comfortable with,” says Doug Moore, who's also working on the HSR. “An elderly person might be less or more comfortable with more autonomy versus a younger person that's more comfortable with technology, right.”
For now, Toyota will keep testing the HSR, experimenting with different hardware. On the software side, it'll keep training the robot to better recognize facial cues so it can tell, for example, exactly when its subject is ready to sip some water. And Camargo, for his part, will keep inviting the robot back. Sadly, though, the HSR won't be roaming your halls anytime soon—it'll take some time to get to the market.
These are early days in robotics, after all. This is all about experimenting with the limits and potential of a flexible platform, because the machines of the future won’t be one-size-fits-all. As much as they’ll have to adapt to us, we’ll have to adapt to them. You might, for instance, take a test to determine what kind of driver you are and how your self-driving car should mimic that. And one day, it might be something like an HSR that you customize to fit your housekeeping needs. Jetsons, here we come.
This week, a car-sized scale model of the International Space Station is hanging from the ceiling of the Regency Ballroom at the Omni Shoreham Hotel in Washington. It’s positioned toward the back of the room, so it doesn’t block the view of a bank of six TV cameras. Bathed in purple lights, the grayish behemoth lurks out of sight and nearly out of mind.
On the ballroom floor, more than 1,000 space industry contractors, researchers, students and policy types are gathered for the annual ISS Research and Development Conference, a four-day celebration of the $3 billion-a-year orbiting lab and outpost. And, well, they’re all exhibiting symptoms of a bit of a mid-life crisis.
The ISS runs out of congressional money and authorization in 2024, and NASA policymakers are trying to figure out what comes next. Officials from the space agency are writing a final report on the station’s future, to deliver to Congress by December. Among the options: renovate the solar panels and keep it flying until 2028, turn the whole thing over to a private buyer, break it up into pieces and auction them off to various commercial firms, or let it slowly descend into the Earth’s atmosphere and leave a fiery trail in the sky.
The uncertainty over the ISS’s future was no more obvious when rock-star entrepreneur Elon Musk, CEO of SpaceX, took to the stage for an hour long Q&A with NASA’s ISS program manager Kirk Shireman on Wednesday. Musk has revolutionized the space industry by building reusable rockets that will soon make it cheaper and easier to get cargo and people to the ISS and beyond.
But asked about the ISS’s future, Musk replied meekly: “We have to educate the public about the awesomeness of the space station.” Musk said private firms could one day use the station to beam internet signals to remote parts of the world or keep tabs on dangerous storms, droughts or crop failures—something satellites already do now.
Musk’s enthusiasm kicked up a notch when he described what he really wants to do in space. “If you want to get the public really fired up, you need to get a base on the moon, and then Mars,” Musk said to lusty cheers from the space-partisan crowd.
Another space industry titan, Robert Bigelow—CEO of Bigelow Aerospace—agrees that the US needs to go back to the moon because China might get there first, and then start making money from the mineral resources on the lunar surface. “From a China and a competition standpoint, going to moon makes more sense,” said Bigelow. “If we are going to execute something grand in a reasonable amount of time, we need to do it in a single administration.”
Ted Cruz Asks Space Capitalists How to Make Orbit Great Again
The Future of the International Space Station Is Up to a Weird Little Florida Nonprofit
Somebody Just Buy the ISS Already
Bigelow’s Las Vegas-based firm has developed an inflatable habitat called BEAM that is attached to the station to test if it can protect astronauts from both debris and radiation. But both Bigelow and Musk really want to use the ISS to get somewhere else—like the moon or Mars. Congress and federal auditors said in 2014 that NASA needs to figure out how to get the ISS to generate more private income if it’s going to survive past 2024.
But the commercialization game still seems stuck in startup stage. Presenters talked about space station deals to develop new Tupperware food containers or 3-D printed medical devices for astronauts. And the big announcement before Musk’s talk was a $1 million contest/partnership with Target to research how to grow sustainable cotton plants in space—small potatoes for a massively expensive orbiting bus that even NASA officials admit is underutilized.
“We are looking for emerging markets in space,” says Robyn Gatens, [deputy director of the ISS division] (http://www.womeninaerospace.org/forms/events/conf2016/Robyn-Gatens.pdf). at NASA headquarters in Washington. “Today the demand is pretty small.” Gatens says the limiting resource is crew time: Astronauts are too busy keeping the station running, fixing stuff, maintaining power supplies and life support systems, and conducting an occasional science experiment. The Russians can only afford to put two of their three people up in space, so everyone else has to work hard to keep things tidy and flying.
There should be more time for commercial projects when NASA puts a third American astronaut on the ISS in 2019, Gatens says. By then, perhaps SpaceX will have made going to the space station as exciting for private companies as Musk’s dream of a moon base.
On April 8, SpaceX will launch an inflatable, inhabitable bouncy castle to the International Space Station and it may be the start of the first hotel chain in Space.
Recall your favorite memory: the big game you won; the moment you first saw your child's face; the day you realized you had fallen in love. It's not a single memory, though, is it? Reconstructing it, you remember the smells, the colors, the funny thing some other person said, and the way it all made you feel.
Your brain's ability to collect, connect, and create mosaics from these milliseconds-long impressions is the basis of every memory. By extension, it is the basis of you. This isn't just metaphysical poetics. Every sensory experience triggers changes in the molecules of your neurons, reshaping the way they connect to one another. That means your brain is literally made of memories, and memories constantly remake your brain. This framework for memory dates back decades. And a sprawling new review published today in Neuron adds an even finer point: Memory exists because your brain’s molecules, cells, and synapses can tell time.
Defining memory is about as difficult as defining time. In general terms, memory is a change to a system that alters the way that system works in the future. "A typical memory is really just a reactivation of connections between different parts of your brain that were active at some previous time," says neuroscientist Nikolay Kukushkin, coauthor of this paper. And all animals—along with many single-celled organisms—possess some sort of ability to learn from the past.
Like the sea slug. From an evolutionary perspective, you'd have a hard time drawing a straight line from a sea slug to a human. Yet they both have neurons, and sea slugs form something similar to memories. If you pinch a sea slug on its gills, it will retract them faster the next time your cruel little fingers come close. Researchers found synapse connections that strengthen when the sea slug learns to suck in its gills, and molecules that cause this change. Remarkably, human neurons have similar molecules.
Time Might Only Exist in Your Head. And Everyone Else's
What Can Novelists Learn From Neuroscience?
Genetic Switch Could Restore Memory
So what's that got to do with your favorite memory? "What is unique about neurons is they can connect to thousands of other neurons, each very specifically," says Kukushkin. And what makes those connections a network is the fact that those specific connections, those synapses, can be adjusted with stronger or weaker signals. So every experience—every pinch to the gills—has the potential to reroute the relative strengths of all those neuronal connections.
But it would be a mistake to believe that those molecules, or even the synapses they control, are memories. "When you dig into molecules, and the states of ion channels, enzymes, transcription programs, cells, synapses, and whole networks of neurons, you come to realize that there is no one place in the brain where memories are stored," says Kukushkin. This is because of a property called plasticity, the feature of neurons that memorize. The memory is the system itself.
And there's evidence of memory-making throughout the tree of life, even in creatures with no nervous system—scientists have trained bacteria to anticipate a flash of a light. Kukushkin explains that primitive memories, like the sea slug's response, are advantageous on an evolutionary scale. "It allows an organism to integrate something from its past into its future and respond to new challenges," he says.
Human memories—even the most precious—begin at a very granular scale. Your mother's face began as a barrage of photons on your retina, which sent a signal to your visual cortex. You hear her voice, and your auditory cortex transforms the sound waves into electrical signals. Hormones layer the experience with with context—this person makes you feel good. These and a virtually infinite number of other inputs cascade across your brain. Kukushkin says your neurons, their attendant molecules, and resultant synapses encode all these related perturbations in terms of the relative time they occurred. More, they package the whole experience within a so-called time window.
Obviously, no memory exists all by itself. Brains break down experience into multiple timescales experienced simultaneously, like sound is broken down into different frequencies perceived simultaneously. This is a nested system, with individual memories existing within multiple time windows of varying lengths. And time windows include every part of the memory, including molecular exchanges of information that are invisible at the scale you actually perceive the event you are remembering.
Yes, this is very hard for neuroscientists to understand too. Which means it's going to be a long time before they understand the nuts and bolts of memory formation. "In an ideal world, we would be able to trace the behavior of each individual neuron in time," says Kukushkin. At the moment, however, projects like the Human Connectome represent the cutting edge, and they are still working on a complete picture of the brain at a standstill. Like memory itself, putting that project into motion is all a matter of time.
Joshua Foer can remember anything, including the first 100 digits of Pi. The former U.S.A. Memory Champion explains how he—and you—can memorize anything using the major system technique, which converts numbers into words and images.
Macular degeneration is the most common cause of vision loss among the elderly. But for some people with the disease, a shot of stem cells to the peeper was all they needed to see again. For others, treatment left them permanently blind. What gives? Stem cell treatments like the one described above—happening every day in 600 clinics across the US—are not approved by the FDA, and in fact have never been tested in a clinical trial.
Eyes aren’t the only organ getting stabbed full of stem cells. Around the country, more doctors are marketing the therapy to treat everything from diabetes to asthma to erectile dysfunction. The procedure usually involves sucking out some of a patient's fat tissue with a liposuction needle, isolating their stem cells, and reinjecting them back into the place in their body that needs most healing. But because these are living tissues unique to every individual, results may vary. Advocates of the therapy say that’s just the cost of doing cutting-edge medicine. Except, any proof they have that it is effective comes from data collected on patients who pay thousands of dollars for the treatment. Usually people pay money for medicine after there’s proof it works. In the last few years, some of these stem cell clinicians have begun posting large-scale studies on a government-run website called ClinicalTrials.gov, even though they're often not up to medical research standards or even in compliance with federal regulations. This allows them to masquerade their pay-to-participate studies as legit science.
According to a paper published today by the University of Minnesota, US companies have successfully registered 18 “patient-sponsored” stem cell studies on that publicly funded website. Only 7 of them disclose that patients pay their own way. None of them list the costs, which can range from $5,000 to $15,000 a treatment, outright. And none of them are actual clinical trials in the randomized, blinded, gold-standard sense of the phrase. Instead, they’re observational studies, based mostly on quality of life questionnaires that ask if you’ve had any adverse reactions to the procedure.
Leigh Turner, the bioethicist who penned today’s perspective and who has become somewhat of a watchdog in the ballooning stem cell clinic industry, says this amounts to hijacking a public good and repurposing it into a free marketing tool. “They don’t have to pay for ads on television, people just come to them because it’s this trusted national resource,” he says. “It’s all meant to suggest a seal of approval from the federal government. And that’s what’s so dangerously misleading, because it doesn’t mean that all. It just means someone filled out a form and pressed a button.”
Following the passing of the Food and Drug Administration Modernization Act of 1997, the National Library of Medicine established ClinicalTrials.gov to serve as a source of information about publicly and privately supported clinical trials for patients, their families and caregivers, doctors and nurses, and the public. But the NIH doesn’t independently verify the scientific validity of any trials posted to the site beyond a limited quality control review. That doesn’t really include things like sound study design, compliance with current regulations, or ethical guidelines. In fact, the process is largely automated, and relies almost entirely on the honor system.
With 21st Century Cures Act, the Future of Regenerative Medicine Is "Inject and See"
The Feds Are Spending Millions to Help You Survive Nuclear War
Scientists Brew Up the Creepiest Batches of Brain Balls Yet
NIH said in a statement that it is continuing to evaluate ways to improve its outreach to make sure that trial participants understand potential risks and benefits. That included adding a prominent disclaimer on the ClinicalTrials.gov homepage in March 2017, stating: “Listing of a study on this site does not reflect endorsement by the National Institutes of Health. Talk with a trusted healthcare professional before volunteering for a study.”
But that hasn’t stopped stem cell therapy slingers from touting their clinical bonafides. Cell Surgical Network, an umbrella organization for more than 50 clinics that market the treatment directly to consumers, first registered on ClinicalTrials.gov in October of 2013. At the time, the organization put out a press release noting that the NIH had registered its approved safety study, and that it was now cleared to enroll 3,000 people to study the adverse effects of stem cell treatments on arthritis, cardiomyopathy, Parkinson’s, ALS, and a host of other inflammatory and neurological diseases. The ClinicalTrials.gov listing of this study does not mention that research subjects are charged an average of $6000 to participate.
Elliot Lander, a urologist and co-founder of Cell Surgical Network says those costs are necessary because stem cell therapies don’t make money like pharmaceuticals do—they can't be packaged up and mass-produced. Which means pharma companies and research institutions aren’t interested in footing the bill for clinical trials. And he says that while it might be worth noting the costs upfront, the NIH doesn’t have a mandatory policy about including fees. “Stem cell treatments have no business model,” he says. “So it’s left to physicians like myself to do the right thing by my patients, and get them regenerative medicine now, not 10-15 years from now. People don’t need to be protected by Leigh Turner, they can do their own due diligence.”
So far, at least 6,000 people have followed their due diligence to a treatment at one of Cell Surgical Network’s clinics. The network keeps a database of all its patients and their outcomes, hoping for the day when the FDA decides to go from merely ignoring the proliferation of un-approved treatments to giving them a green light based on post-market data (a regulatory “middle path” floated by advocates like Lander.) In the meantime though, Cell Surgical Network is working with the FDA on an application for a real clinical trial, limited to just knee pain treatments—that will even include a randomized placebo. “It’s going to take us years and a few million dollars, says Lander. “The walls are really high for this kind of thing.” As they should be.
Back in May, when Jennifer Hettema first saw the Trump administration’s proposed budget, it took her a while to find the bad news. But buried in a Health and Human Services appendix, there it was: a $100 million line through the nation’s teen pregnancy prevention program. A psychologist and public health researcher at the University of New Mexico Health Sciences Center, Hettema is studying how doctors can best talk to Native American and Latino teens about avoiding unwanted pregnancies. Her work is funded by the Obama-era program, which gives out five-year grants for evidence-based intervention evaluations.
That left Hettema with three years left on her grant, but she figured it wasn’t too soon to worry about the future. She started to talking to her local representatives in Congress, who assured her there was bipartisan support for science and reproductive health—the budget was just a proposal, after all. She went back to work recruiting patients for her trial study.
But last week, when her annual grant award letter arrived from the Office of Adolescent Health, the HHS arm that administers the teen pregnancy prevention program, she found one gut-sinking sentence: “This award also shortens the project period to end June 30, 2018, and the end of this budget year.” The grants were supposed to last through 2020.
As recipients at 81 institutions around the country found out last week, the Trump administration decided to cancel them early—cutting off $213.6 million in promised funds and disrupting ambitious research projects aimed at unwanted pregnancies in teens. The unusual move, circumventing the traditional congressional budgetary process, has scientists and public health officials scrambling to figure out how to save work already in progress. But for most, the outlook is bleak.
“Our study can’t be salvaged,” says Lisa Masinter, who leads a Chicago Department of Public Health project to test the efficacy of a school-based education and STI screening program. Started as a pilot in 2009, the program had so much demand that CDPH wanted to make sure it actually worked. So they applied for a federal grant and began collecting baseline data last year. They were planning to start testing the intervention on ninth graders next year, and follow them through their entire high school career. Now, they can maybe collect six months of follow-up data. Which, if you know anything about human gestation, isn’t long enough to evaluate the most relevant metric: births. “Even if we find other funding, the framework of the evaluation has been totally altered,” says Masinter.
Many grantees WIRED spoke with indicated that their project officers at the Office of Adolescent Health were just as surprised by the grant disruptions as they were. According to a Reveal report that broke the news last week, the decision to eliminate funding likely came from the office of the assistant secretary of health. Last month, President Trump appointed Valerie Huber, an outspoken advocate of abstinence-only education, as the office’s new chief of staff. On Monday, a spokesperson from the office of the assistant secretary for health confirmed eliminating the final two years of funding, but when asked where the directive came from, responded in an email that “the President’s FY 2018 Budget eliminated funding for the Teen Pregnancy Prevention Program, so our grants office informed the grantees of their June 30, 2018 end date, to give them an opportunity to adjust their programs and plan for an orderly closeout.”
It should be pointed out here that the President’s proposed 2018 budget is not a legally binding document meant to guide any immediate agency funding decisions. Until Congress approves the budget, it should only be a White House wish list.
Before coming to HHS, Huber was the president of Ascend, formerly known as the National Abstinence Education Association. In a March editorial in The Hill, Huber wrote that the time had arrived for evidence-based pregnancy prevention programs to yield to an abstinence-only sex-ed stance. And indeed, evidence of that thinking was on display in the Trump administration’s rationale for eliminating the program. In the HHS appendix, which outlines budget justifications, the stated reason was that while the teen pregnancy rate has declined significantly over recent years, “it does not appear this program has been a major driver in that reduction.”
Trump’s budget did leave in place $277 million for abstinence-only sex-ed, stating that the program exclusively “supports an evidence-based approach defined as voluntarily refraining from non-marital sexual activity.” Not having heterosexual sex, whether you’re married or not, is the best way to not get pregnant in your teen years. But researchers know this, of course, which is why the programs being evaluated with these grants include abstinence education in addition to information about contraception and sexual decision making.
Pat Paluzzi, a public health researcher at the Healthy Teen Network in Baltimore, says one of the biggest determinants for whether or not people get pregnant during their teen years is the age at which they start having sex. That’s why the smartphone apps she has designed, which give teens information about having healthy sexual relationships, focus largely on helping them decide to hold off on having sex. But she’s also a realist. “Forty percent of teens, no matter what kind of education they receive, are having sex,” she says. “And ignoring the 40 percent who are either making that choice willingly or unwillingly is harmful to these young people. We have to also teach them how to stay safe.”
Paluzzi was using her grant to conduct a randomized clinical trial—the gold standard for testing whether or not an intervention works—on an app tailored to 18 and 19-year old African American and Latino girls. But without the promised funds, she’s going to have to cut recruitment short, from 1,500 down to only 800 participants. And without enough people to make her results statistically significant, she won’t be able to actually prove whether or not the app is effective. She’s started to look around for additional funds, either from non-profit or commercial partners, but she’s not overly optimistic.
Fixing America’s Lousy Sex Ed—With Sociology
Even for the GOP, Parts of Obamacare Should Be Worth Saving
Scientists Found Sperm’s Power Switch—And a Way to Turn It Off
When Paluzzi first started working with the Healthy Teen Network, it was under President Bush’s abstinence-only administration. Back then, there was reliable non-profit funding from sources like the Ford Foundation and the Edna McConnell Clark Foundation to support research for evidence-based interventions. But these days, private endowments are still recovering from the Great Recession. And they’re stretched thinner, as they support other fields whose government dollars are drying up, like climate change research and opioid addiction treatments. “Foundations aren’t spending as they once were,” says Paluzzi. “I’m not sure we’re going to be able to rebound in the same way.”
So far, there doesn’t seem to be one white knight emerging to fill in the fiduciary gap. Some researchers, like Masinter in Chicago, are working with local authorities and citywide initiatives to make up for the loss of federal dollars. Others, like Hettema, are even entertaining the idea of partnerships in the pharmaceutical and medical device industry. All of them hold out a sliver of hope that the decision may yet get reversed, or the money reinstated by Congress. But researchers can’t just push the pause button while they wait and see. Kids have already been recruited, baselines already measured. Which means they’re all forging ahead with whatever they think they can cobble together.
The biggest loser in all of this, researchers say, is the American taxpayer. “To put $3 million into a powerful, randomized clinical study with more than 1,000 people and then stop it before we can really deliver?” says Hettema. “It’s so wasteful.” She points out that these grants in particular had a unique requirement. They specifically set aside time and money for dissemination of results in year five, so that researchers actually put effort into making their findings widely available to educators, physicians, and teens. Now, even if anything valuable can still come out of these 81 projects, it’s unlikely that information will get where it most needs to go.
The teen birth rate in the US has indeed been declining over the past two decades, but not as quickly for minorities and low-income communities (or in states that only only offer abstinence education). And it’s still the highest among other developed nations: 57 births for every 1,000 teenagers between 15 and 19. Switzerland, where only eight out of 1,000 teens give birth every year, starts sex-ed in kindergarten. And it covers a lot more than abstinence.
California is on fire again. CalFire, one of the agencies charged with putting those fires out, is tracking upward of two dozen conflagrations up and down the state at the moment—Detwiller, Grade, Bridge, Wall, Alamo, Garza, on and on—ranging in size from a couple hundred acres to nearly 50,000.
And it’s not just the Golden State. Across the North American West, from Wanblee, South Dakota, to the Rogue River-Siskiyou National Forest in Oregon; from the Coronado National Forest near the Mexican border in Arizona to Fort Fraser in British Columbia and even farther north, grasses, chaparral, and forest are all ablaze. The continent is deep into the seasonal cycle of wet-winter-grows-plants/dry-summer-turns-them-to-fuel/they burn.
It’s tempting to see each fire season as worse than the last, and to further see that as evidence of the kind of apocalypse that a changing climate will visit on civilization. If it ain’t rising water at the coasts it’s a “firenado” in the hills. But researchers have identified an even more pernicious problem: us. Specifically, where we build houses.
Whether the sheer number of fires in a season or the amount of land they burn has increased over years (or decades or centuries) is hard to tell. It depends on the dataset. “When you smear out fires over a continental scale, from the edge of the boreal forest to the tropics, you’re hiding a lot of regional variation,” says Mark Finney, a researcher at the US Forest Service’s Missoula Fire Sciences Laboratory. In fact, Finney says, Native Americans burned a lot more territory before Europeans arrived than after.
A changing, warmer climate does make wildfires more likely. And human beings have attempted to manage them in myriad ways over the last century or so. They’ve cut down trees that seemed likely to burn, set intentional fires to try to mimic the periodic fires ecosystems rely on, and even favored some kinds of plants—“fuel,” in the parlance of fire management, which should give you some sense of where fire managers’ heads have been about the nature of nature.
Even that’s a more complicated story than I just made it sound, though. As a paper in Science in June of 2017 put it, savanna ecosystems need frequent fires to stay healthy, but different kinds of forests have different kinds of needs. Those authors say that as human uses of land have altered landscapes and ecosystems—more agriculture, more cities—the total amount of area burned on Earth went down by about 24 percent over the past 18 years. They think that’s because with a more built-on environment comes more fire suppression to protect the buildings, farms, domestic animals, and everything else that comes with “civilization.”
It's not the number of fires or how bad they are, but where they are.
The key to how we think about fire’s severity, then, is not the number of fires, or even their magnitude, but where they are—and what’s near them. Wildfires tend to occur at what’s called the Wildland-Urban Interface. That’s where houses or other human-built stuff butt up against nature, what a less sophisticated academic might once have called the frontier. Humans set most wildfires—95 percent of them, according to CalFire. That’s a real problem when, as of 2004 in the continental US, the WUI was almost 278,000 square miles and contained 44.8 million housing units. In other words, 39 percent of all the houses in the country are in the WUI. People built 10 million new housing units in WUIs in the decade leading to 2010.
This situation is worst in California. According to a report from the Center for Insurance Policy and Research, 2 million homes in the state are in wildfire-prone areas—14.5 percent of all the houses in California. (Texas and Colorado fill the number two and three slots, respectively.) The Western promise of open land has a combustible side. “What has happened over time is that development has become less dense in the US,” says Volker Radeloff, a forestry professor at the University of Wisconsin, Madison and lead author on that 2005 WUI study. “People like to move to a 5-acre ranch, and that creates this volatile mix of houses and flammable vegetation.”
So just to be clear: At a time when cities all over the country are experiencing housing crises, unable or unwilling to build enough units to accommodate growing populations, the greatest expansion of houses is on previously unbuilt spaces at the edges of cities. In other words: sprawl. The “intermix” version of the WUI, with development cropping up amid nature—which is to say, sprawl's exurban edge—is even more fiery than interface WUI, with construction slammed right up against greenbelt.
Sprawl in a climatically challenged world isn’t the only path to a wildfire, of course. The fires that killed more than 60 people in Portugal in June, for example, happened in part because in Mediterranean Europe the WUI is contracting rather than expanding. People are abandoning marginal agricultural lands and moving to cities. Those areas had been forest, cleared for human use. Now they are being abandoned. “Wildland vegetation re-invaded, and we’re now having a lot of fires in places where they didn’t have much history of it,” Finney says.
An Epic Fire Season Is Coming. These Firefighters Are Ready
How's America Tackling Its Epic Fires? With an Epic Scheme
All the Trees Will Die, and Then So Will You
In Portugal, some of that land got converted to commercial timberlands, with exotic tree species useful for structural wood and paper pulp. Those trees—pine and eucalyptus, primarily—also burn really well. Fine, loose pine needles burn fast and hot; left unmanaged, blue gum Eucalyptus trees are basically bombs. “It has these big strands or bark that exfoliate, and that can carry embers and provide ladder fuel so that when fire is on the ground it can climb up the strings and into the canopy,” Finney says.
But wait! Can’t we still blame climate change for wildfires? Sure, kind of. In the American southwest, maybe not—it’s already a Mediterranean climate, with six months of summer drought and heat and then dry, fast Santa Ana winds to push chaparral fires around. But climate change means the Mediterranean climate is going to move northward and upward in elevation, turning more places into the kind of tinderboxes that SoCal can turn into—but without SoCal’s experience in managing land, restricting building materials, and regulating defensible spaces around houses. “Is this climate change?” asks LeRoy Westerling, a management professor who studies fire at UC Merced. “The appropriate response is, all the weather we experience now is the result of a changing climate.”
The worst part of all this, then, is the feedback loop. Sprawl itself is a driver of climate change, particularly through increased greenhouse gas emissions from commuters, but also because of energy use, infrastructure inefficiencies, and other secondary effects. “Land use planning is the root of the whole problem, but it could also be the source of the most effective solution,” says Alexandra Syphard, an ecologist at the Conservation Biology Institute. Build more places for people to live in cities, and the number of fires in the wild—and the challenge of fighting them—goes down.
We break down the science of strike-anywhere matches to see what makes sparks fly.
This story originally appeared on Project Earth and is part of the Climate Desk collaboration.
A controversial study of the electric grid, requested by Energy Secretary Rick Perry in April, is finally expected to be released this month. And to some experts, its exact purpose remains, well, questionable at best.
In an April memo to energy department staff, Perry called for a study investigating whether certain federal energy policies, such as subsidies for renewables like wind and solar, were prematurely forcing coal, nuclear and other baseload power plants into retirement—and whether this is a problem for the grid’s performance. These issues, he writes, are “central to protecting the long-term reliability of the electric grid.”
But the request has sparked alarm among renewable energy advocates, some of whom are bracing themselves for a report they fear will become a vehicle for the Trump administration to attack wind and solar energy. These fears are compounded by the fact that the person Perry has appointed to lead the study is Energy Department political appointee Travis Fisher, a former economist from the right-leaning Institute for Energy Research, who has previously criticized the existence of federal tax credits for renewable energy.
And while the report’s findings are still to be seen, other energy experts say the motivation for the study—and what it will actually accomplish once released—remains murky. The study seems to rely on several inaccurate assumptions, they say, and its purported goal of protecting grid reliability may actually be threatened by the Trump administration itself.
In its most recent 2018 budget proposal, the White House has proposed significant budget reductions for the Department of Energy, including cuts to the Office of Energy Efficiency and Renewable Energy, the Office of Nuclear Energy and the Office of Electricity Delivery and Energy Reliability. The latter two programs, in particular, would seem to support research that’s directly related to Perry’s interest in supporting baseload power plants and safeguarding grid performance.
“...This is a report being created by the political leadership of an agency that doesn’t have a vision for what they’re going to do, and they don’t have control over their budget, it seems.”
“There are probably multiple motivations [for the study],” said David Victor, an energy policy expert at the University of California San Diego. “The most uncharitable one is that this is a report being created by the political leadership of an agency that doesn’t have a vision for what they’re going to do, and they don’t have control over their budget, it seems — and so they’re kind of hunching around for something to do.”
A more optimistic perspective, he suggested, is that the forthcoming report is intended to be used as a kind of “road map” for the federal government and interested states to explore and prepare for the challenges that may face the grid as the US energy landscape continues to evolve. But its contents, once they’re revealed, will speak volumes about whether this is actually the case.
Perry’s request is based around the idea that “baseload power is necessary to a well-functioning grid,” as he writes in his memo—and that recent closures of baseload power plants are threatening the grid’s reliability.
By “baseload,” he’s referring to power plants, such as coal and nuclear plants—both energy sources favored by the Trump administration—that are able to produce a constant supply of electricity to satisfy minimum demand. Wind and solar, on the other hand, are what’s known as “intermittent” energy sources because they can’t be harnessed in the same location all the time—they only provide energy when the sun is shining or the wind is blowing.
While small communities with their own microgrids and battery storage systems may be able to power themselves entirely on renewable energy, for the US as a whole it’s true that intermittent sources alone would be unable to meet energy demands around the clock without significant improvements in energy storage technology, which would enable utilities to save up energy as it’s generated and deploy it later as needed.
But much of the controversy surrounding the grid study stems from Perry’s implication that federal policies, including subsidies for renewables, are responsible for edging baseload power plants out of the picture—and that they’re doing so to an extent that they “create acute and chronic problems for maintaining adequate baseload generation and have impacted reliable generators of all types,” as Perry writes.
These suggestions rely on inaccurate assumptions about both the energy market and the grid, said Ryan Fitzpatrick, deputy director of the clean energy program at centrist think tank Third Way.
For one thing, the challenges facing baseload power plants are hardly being driven by federal policies alone. In fact, the greatest single factor in the decline of coal has actually been the rise of cheap natural gas, Victor noted — not renewables. Coal has just been unable to compete.
That’s not to say that the continued expansion of renewables like wind and solar are not helping to edge coal out of the picture, but they’re hardly the primary culprit. According to Victor, it seems unusual that the new report doesn’t call for a greater focus on natural gas — although, he noted, this could be another indication of the administration’s interest in simply “bashing renewables.”
Renewable industry trade groups have already expressed their worries about these potential biases in an open letter to Perry, submitted in May, which called for a transparent review that’s open to public input and voiced concern about what they see as the study’s “faulty premise.”
“What we’re actually seeing is that the markets are doing exactly what they’re programmed to do, which is provide reliable electricity at low prices.”
Another problem is the idea “that we have this supposed reliability crisis that we need fixed, and that the markets aren’t solving that on their own,” Fitzpatrick told Project Earth. “What we’re actually seeing is that the markets are doing exactly what they’re programmed to do, which is provide reliable electricity at low prices.”
That’s not to say there are never any problems with the US grid system. Just last weekend, a blackout in California’s San Fernando Valley, which officials say was likely caused by an intense heat wave and excessive energy demand, left 140,000 people without power.
For the time being, Fitzpatrick said, it’s unlikely that any reliability issues have much to do with the growth of renewables, which currently only account for about 15 percent of US electricity generation. The San Fernando Valley incident actually speaks both to the need to replace aging infrastructure around the country—the outage was reportedly sparked by an explosion at a transformer more than 40 years old—and the strains that future climate change may place on the grid in the form of increased energy demand.
Fitzpatrick also suggested that other issues with grid reliability are more likely to stem from the nation’s increasing reliance on natural gas, which recently surpassed coal as the country’s leading source of electricity generation.
“The more you rely on any one fuel, the more challenging it can become to manage unexpected or unanticipated or rare obstructions,” he said.
Indeed, some experts suggest that expanding renewable energy sources and investing in the increased flexibility of the grid, which was originally designed to be supported mainly by baseload power sources, is the way to go. A report recently commissioned by the Natural Resources Defense Council, for instance, suggests that grid planners should be “technology neutral” instead of focusing on designing electricity systems around baseload power sources.
As market forces continue to phase out more expensive power sources like coal and support the expansion of alternatives, the report suggests a variety of strategies to deliver lower-cost mixes of energy, including taking advantage of resources like energy storage technology, utilizing hydro or natural gas-fired plants that can more easily ramp up and down in response to demand, and providing incentives for consumers to reduce their energy consumption at certain times to reduce pressure on the grid.
That said, as intermittent energy sources continue to expand, there are some legitimate questions about their effect on grid performance, Victor pointed out. Continued research on energy storage technology and more efficient electric power transmission will be key in the continued integration of renewables on the grid.
“There’s a really big technical debate about how much renewables, at what cost, you can bring onto the grid and still keep reliability high,” he noted.
However, these questions have already been addressed by a number of recent studies, including a federal report released just last year by the National Renewable Energy Laboratory. It found that renewable technologies have the potential to supply up to 80 percent of US electricity generation in the year 2050 “while meeting electricity demand on an hourly basis in every region of the country.”
There may still be some room for debate about this exact value, and several academic studies in the past few years have come to different conclusions. The point, though, is that these questions have been asked over and over already—and research from both the federal government and independent scientists suggests that renewables can expand a lot further before they begin to threaten the grid’s reliability.
Despite so much controversy surrounding the new report, though, experts suggest it’s unlikely to have any major policy significance. Indeed, Victor noted that he’s “very skeptical that this report is going to have any relevance at all.”
“People are very suspicious of what this administration is doing, and of what Rick Perry in particular is doing.”
These days, energy markets are influenced by state-level policies, such as renewable energy portfolios, far more than federal ones—and the report will almost certainly have no authority to change them. It’s true that federal tax credits for wind and solar have provided significant benefits for the renewables industry, but these have already been extended by Congress through the year 2021 and are unlikely to change before that time.
Fitzpatrick noted that this doesn’t mean the report’s contents couldn’t still create difficulties for the renewable energy industry in certain ways.
“A lot of the states that have renewable portfolio standards, they’re run by Republican legislatures,” he pointed out. Depending on its contents, he said, the report “could make it kind of difficult if supporting an increase for renewables on a given grid puts you in direct contradiction—even if it is completely rhetorical contradiction—with what the White House has says it cares about and believes. Not to say that that’s the deciding factor, but it doesn’t make it any easier to support or expand things like renewables.”
According to Victor, there are many legitimate questions that remain about the future of grid reliability, the integration of renewables and the influence of power markets on these issues—and a reasonable, unbiased report from the Department of Energy could help to further some of these discussions.
Should the final report be viewed as an unfair attack on renewable energy, though, the hope of any productive discussions coming out of it may be lost.
“People are very suspicious of what this administration is doing, and of what Rick Perry in particular is doing,” he said. “And they are going to look for evidence that their suspicions are right in things like this report.”
Take a look inside the first commercial-scale solar energy plant to use nothing more than the sun, molten salt, and a whole lot of mirrors to send power to the people. If the Crescent Dunes Solar Energy facility works as promised, it could be a model for the future of renewable energy.
Huntington’s disease is brutal in its simplicity. The disorder, which slowly bulldozes your ability to control your body, starts with just a single mutation, in the gene for huntingtin protein. That tweak tacks an unwelcome glob of glutamines—extra amino acids—onto the protein, turning it into a destroyer that attacks neurons.
Huntington’s simplicity is exciting, because theoretically, it means you could treat it with a single drug targeted at that errant protein. But in the 24 years since scientists discovered it the gene for huntingtin, the search for suitable drugs has come up empty. This century’s riches of genetic and chemical data seem like it should have sped up research, but so far, the drug pipeline is more faucet than fire hydrant.
Part of the problem is simply that drug design is hard. But many researchers point to the systems of paywalls and patents that lock up data, slowing the flow of information. So a nonprofit called the Structural Genomics Consortium is countering with a strategy of extreme openness. They’re partnering with nine pharmaceutical companies and labs at six universities, including Oxford, the University of Toronto, and UNC Chapel Hill. They’re pledging to share everything with each other—drug wish lists, results in open access journals, and experimental samples—hoping to speed up the long, expensive drug design process for tough diseases like Huntington’s.
Rachel Harding, a postdoc at the University of Toronto arm of the collaboration, joined up to study the Huntington’s protein after she finished her PhD at Oxford. In a recent round of experiments, her lab grew insect cells in stacks of lab flasks fed with pink media. After slipping the cells a DNA vector that directed them to produce huntingtin, Rachel purified and stabilized the protein—and once it hangs out in a deep freezer for a while, she’ll map it with an electron microscope at Oxford.
Harding’s approach deviates from the norm in one major way: She doesn’t wait to publish a paper before sharing her results. After each of her experiments, “we’ll just put that into the public domain so that more people can use our stuff for free,” she says: protocols, the genetic sequences that worked for making proteins, experimental data. She’d even like to share protein samples with interested researchers, as she’s offered on Twitter. All this work is to create a map of huntingtin, “how all the atoms are connected to each other in three-dimensional space,” Harding says, including potential binding sites for drugs.
The next step is to ping that protein structure with thousands of molecules–chemical probes–to see if any bind in a helpful way. That’s what Kilian Huber, a medicinal chemistry researcher at Oxford University’s arm of the Structural Genomics Consortium, spends his days working on. Given a certain protein, he develops a way to measure its activity in cells, and then tests it against chemicals from pharmaceutical companies’ compound libraries, full of thousands of potential drug molecules.
If they score a hit, Huber and his consortium collaborators have pledged not to patent any of these chemicals. To the contrary, they want to share any chemical probe that works so it can quickly get more replication and testing. Many times, at other researchers’ requests, he has “put these compounds in an envelope, and sent them over,” he says. Recipient researchers generally cover shipping costs, and the organization as a whole has shipped off more than 10,000 samples since it started in 2004.
Under the umbrella of the SGC, about 200 scientists like Kilian and Rachel have agreed to never file any patents, and to publish only open access papers. CEO Aled Edwards beams when he talks about the group’s “metastatic openness.” Asking researchers to agree to share their work hasn’t been a problem. “There’s a willingness to be open,” he says, “you just have to show the way.”
There are a few challenges to such a high degree of openness. The academic labs are involved in which projects they tackle first—but it’s their funders that ultimately decide which tricky proteins everyone will work on. Each government, pharmaceutical company, or nonprofit that gifts $8 million to the organization can nominate proteins to a master to-do list, which researchers at these companies and affiliate universities tackle together.
That list could be a risk for the pharma companies at the table: While it doesn’t specify which company nominated which protein, the entire group can see that somebody is interested in a Huntington’s strategy, for example. But they’re hedging their bets on a selective reveal of their priorities. For several million dollars—a fraction of most of these companies’ R&D budgets—companies including Pfizer, Novartis, and Bayer buy into the scientific expertise of this group and stand to get results a bit faster. And since no one is patenting any of the genes, protein structures, or experimental chemicals they produce, the companies can still file their own patents for whatever drugs they create as a result of this research.
That might seem like a bum deal for the scientists doing all the work of discovery. But mostly, scientists at the SGC seem thrilled that collaborating can accelerate their research.
Why Pharma Wants to Put Sensors in This Blockbuster Drug
Fixing a Broken Drug Business by Spreading the Wealth
Drug Test Cowboys: The Secret World of Pharmaceutical Trial Subjects
“Rather than trying to do everything yourself, I can just share whatever I'm generating, and give it to the people that I think are experts in that area,” says Huber. “Then they will share the information back with us, and that, to me, is the key, from a personal point of view, on top of hopefully being able to support the development of new medicines,” says Huber. Because all the work is published open access, technically anyone in the world could benefit.
Edwards has pushed the SGC to slowly open up new steps of the drug discovery process. They started out working on genes, which is why they’re named a ‘genomics consortium’, then eked their way to sharing protein structures like the ones Harding works on. Creating and sharing tool compounds like Huber’s is their latest advance. “We’re trying to create a parallel universe where we can invent medicines in the open, where we can share our data,” Edwards says.
He hopes their approach will expand into a wider movement, so that other life science researchers get on board with data sharing, and open-source science improves repeatability and speeds up research findings. The Montreal Neurological Institute stopped filing patents on any of its discoveries last year. And there are other groups, like the Open Source Malaria Project, that have made a point of keeping all of their science in the open.
Sharing data won’t necessarily solve the inflating price of certain drugs. But it could certainly speed up understanding of new compounds, and shore up their chances of getting through clinical trials. The drug-making process is so complicated that if data sharing shaved just a bit of time off each step, it could save people years of waiting. The Huntington’s patients are waiting.
Medicine has an expiration stamp—but Is it actually, you know, serious? Or are those sell-by dates just a Big Pharma racket? Mr. Know-It-All gives you a healthy dose of the truth.
In a cramped meeting room Wednesday on Capitol Hill, House Democrats hosted a roundtable to discuss climate change with several national security experts. In attendance were two former admirals, a retired general, a once-ambassador to Nigeria, and the former undersecretary to the Secretary of Defense.
Over several hours of questioning, they described how climate change would escalate instability across the globe and make it harder for the US military to conduct its operations. Nothing they said, however, was all that new. In fact, the Department of Defense has known about, and sometimes planned for, the security threats created by climate change for well over a decade. Congressional Democrats—minority members of the House Science Committee—called the roundtable as a plea to the Republican-led Congress to stop standing in the way of the military's preparations for the heightened dangers of a warming world.
One of the key phrases here is "threat multiplier." Coined about a decade ago by panelist Sherri Goodman, a former Deputy Under Secretary of Defense, it means climate change will raise the stakes for existing conflicts, and push unstable communities toward catastrophe. Case study: the Syrian Civil War, rise of ISIS, and Syrian refugee crisis began in part because of a climate change-linked drought that began in 2006. "Droughts affected the Syrian harvests, compounded by historically poor governance and water management," says Marcus King, a professor of international affairs at George Washington University. This caused migrations of farmers into the cities, where they had neither jobs nor food. The violent protests for both became rallying cry against repressive president Bashar Assad. The protests became riots, then insurgency, and eventually full-blown chaos.
The threat multiplier paradigm is appearing in other places. Guatemala already has problems with food security, and many regions are still left ungoverned after that country's not-so-distant civil war. Rising seas are bringing saltwater incursion to Egypt's Nile Delta, adding food insecurity to that country's already tense political situation. And in Nigeria's capital city of Lagos, nearly half of the 22 million residents live below sea level and will eventually have to relocate—unlikely to be easy or conflict-free. "This isn’t a political issue for the defense community," says Ann Phillips, a retired admiral and an advisor for the Center for Climate and Security. "We in this community are pragmatic and mission-focused."
Without preparation, responding to those threats will stretch both the US military's defense and humanitarian capabilities. "If you take Yemen as an example, what if we sent forces there, how do we deal with their famine situation?" asks Phillips. She says coming in with that dual capability leads to mission creep, the military doing jobs better suited for other agencies—and encroaching on its primary job of defense.
How Global Warming Helped Cause the Syrian War
United Nations Says Climate Change Threatens Global Security
Climate Change Is So Bad That the US and China Agree on It
This worrisome trend is ever more likely, given President Trump and the Republican majority's climate-cutting spree throughout the federal government. This doesn't just affect future threats. Climate change is already hindering the military's operational readiness. A 2015 Rolling Stone article illustrated how higher tides—driven by sea level rise—regularly flood the world's largest naval base in Hampton Roads, VA. President Obama had directed multiple agencies to attack the problem together. The Department of Transportation would elevate the roads, Department of Energy would shore up the grid, etc. "Current Secretary of Defense General James Mattis has testified that he understands climate change should be addressed where it impacts his mission," says Phillips. "But some of other agencies have directors who are less ... interested, you could say?" That could mean the DoD is left to mop up these problems all on its own.
There's an even broader danger to leaving the military to address the threats from climate change. "The problem with leaving it to military, is that they are trained to deal with threats. So their solutions to climate crises tend to frame the victims of climate change as threats," says Nick Buxton, co-editor of The Secure and the Dispossessed, a collection of academic articles exploring how the military and corporations are responding to climate change. Just look at how the US, EU, and other western democracies have responded to Syria's refugee crisis. And refugees don't always live overseas—Buxton points to how many New Orleans residents experienced violations of their personal liberties in the chaos following Hurricane Katrina.
Buxton says continuing to ignore these climate catastrophes until they boil over could lead to ecological apartheid. Societies with resources will protect themselves from those without. "The US will not be most vulnerable, it has resources, but how other nations are defended and protected will have implications for us," he says. Again, look at Syrian refugee crisis: It was one of the major issues that drove UK voters to vote for Brexit, and threatens to further fracture the EU.
In the backdrop of the climate security roundtable, the US House of Representatives was debating the National Defense Appropriations Act, which authorizes the military's annual budget. In it is an amendment that recognizes climate change as a threat to national security. Every panelist mentioned this amendment as a favorable step towards greater security. Buxton, however, points out that this is a short sighted, and inconsistent goal, as it only addresses the threats, and not the causes of climate change. "I can understand, given nature of Republican party, why Democrats believe this is only avenue for advancing the conversation on climate change in Washington, but it has a lot of dangers," he says. "If you’ve only got a hammer as your tool, then everything in your shed starts to look like a nail."
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.
Space could be capitalism's next frontier, but not without the government's help.
Luckily, the government is all ears. Last Thursday, a bunch of space capitalists sat across from a bunch of senators to talk policy. The panel included everyone from SpaceX's senior VP to the CEO of a small launch startup. Topics ranged from removing orbital debris to enabling deep space commerce; from eliminating bureaucratic speed bumps to dealing with space pirates. Yes, the discussion ranged wide, and at times weird—again, space pirates?—but overall one point was clear: The riches of space are America's to lose.
Congressional hearings like these often reveal their organizers' political motives. Texas Republican Ted Cruz, chair of the Senate Science, Space, and Commerce committee, convened this meeting, and it's not unreasonable to think the themes these speakers touched upon were his, and its attendees chosen by his staff to give voice to his vision. "Ted Cruz is a very smart man, no matter what anyone thinks of his politics, and he's been on track lately to do commerce the old fashioned way," says Keith Cowing, editor of NASAWatch. If this meeting is really an indication of what's to come, the future of space will continue in that good ol' fashioned American spirit of free markets with a heaping side of government help.
The US has always looked at space through the Lewis and Clark paradigm—exploration leads to commerce. It's even written into NASA's 1958 founding charter. (The same document requires NASA to study the Earth's atmosphere.) And space commerce has exploded. Globally, it's a $330 billion a year industry, with commercial activities making up more than three quarters of that total value. Every starry-eyed space entrepreneur owes some measure of their success to technology, property, or expertise that NASA has given away at cut-rate prices, or occasionally, for free. The rest of the federal government is generally just as accommodating. Hence, this meeting.
But still, the government is by definition a bureaucracy, and its concerns for things like safety, security, and process can slow down the flow of space bucks. Hence, this meeting.
Rocket launches get more publicity than satellite operations, but the latter are a bigger business, representing more than 60 percent of the space economy. And like in any big business, pollution can become a problem. Moriba Jah, an astrodynamicist and professor at the University of Texas, Austin, compares Earth's orbital environment to the Wild West: NASA's work providing infrastructure and expertise to commercial interests was like the Transcontinental Railroad, which brought Eastern American businesses to everything left of the Mississippi. "The environmental impact of runaway mining and prospecting was harsh and detrimental in many instances," says Jah. Except instead of mercury poisoning, the proliferation of commercial space operations has left Earth's orbital environment littered with junk.
And as anyone who saw Gravity knows, even small pieces of space junk can cause catastrophic damage to expensive operational pieces of equipment like the International Space Station. Jah points out how woefully behind the US's space junk tracking (let alone clean-up) efforts are. The Air Force's US Strategic Command tracks over 24,000 objects in its space situational awareness database. Jah says the actual number of potential flight hazards—any orbital object bigger than a centimeter in diameter—is at least 100 times that.
There's another good reason for keeping an eye on all the objects in the sky. "Space piracy has likely already happened, is happening, and will happen so long as we lack the ability to comprehensively monitor all space activities," he says. "This unfortunate human behavior has happened in all other domains, and to expect the space domain to be an exception is naïve at best." Yargh! Jah recommends that the Air Force let civilian operators take over monitoring orbital space, modeled after civil air traffic management.
Rockets may be economic small potatoes, but their obvious importance—can't get to space without 'em—explains the presence of two rocket company reps at Thursday's meeting. SpaceX wants to go to Mars, and its VP of global business Tim Hughes made clear how the US government could support that goal: by contracting with commercial launch companies for its deep space missions.
Beyond subsidizing Elon Musk's plans to colonize the Red Planet, deep space launches are necessary to grow the space economy overall. Remember, satellites are the space economy's biggest payout. But that can't last forever. Earth can only hold so many humans, which means there's a finite limit to this species'  telecommunications needs. If space capitalism is going to grow, it needs to open up new markets.
And yet, Earth's crowded orbital space still has some niches, particularly for smaller payloads. Relativity Space, Inc. is a space launch startup that wants to cater to small satellite operators, those that don't require massive rockets to reach low Earth orbit. In explaining his company's needs, Relativity's CEO Tim Ellis extended Jah's historical manifest destiny analogy by a hundred years. "We firmly believe that opening and strategically building up specialized government infrastructure could act as an “accelerator” of space startups, in much the same way that President Eisenhower created the highway system and catalyzed the automobile industry," he says. This would free up small companies like his from using their limited startup capital on infrastructure like launchpads and engine stands, and instead letting them build the rockets they need to survive until their next round of funding.
Somebody Just Buy the ISS Already
SpaceX's Plan to Reach Mars by 2018 Is ... Actually Not That Crazy
Congress Says Yes to Space Mining, No to Rocket Regulations
The US government provides a third, more specialized space market: The International Space Station. And while the ISS needs no introduction, it might soon require a eulogy. At present, Congress plans to stop funding the $3 to $4 billion a year orbital behemoth in 2024. This is mostly so NASA can focus on other big money projects, like sending humans to Mars. Many smart, hopeful experts in and outside of the government have suggested extending this lifespan by courting commercial activity.
Notably, Casis, the nonprofit that is ostensibly in charge of commercializing the ISS, wasn't at the meeting. Jeffrey Manber, CEO of NanoRacks—the only really successful business operating on board the ISS—was. His request was simple: Be clear about when the ISS will close down. "No matter what the end of operations date, the private sector needs to hear what that date is, rather than keeping it ambiguous," he says. For him, this isn't about getting an extended government handout—Manber is outspoken about the fact that he doesn't ask for NASA funding. He says that time will be necessary for him and other space capitalists to plan ahead so foreign governments don't encroach on all the robust services the ISS offered.
In a nutshell, that is space capitalism, the American way.
The private American companies battling it out for a $3.5 billion NASA contract have one last chance to successfully launch their spacecraft before a decision is made in January.
For a species whose numbers show no signs of collapsing, humans have a shockingly high mutation rate. Each of us is born with about 70 new genetic errors that our parents did not have. That’s much more than a slime mold, say, or a bacterium. Mutations are likely to decrease an organism’s fitness, and an avalanche like this every generation could be deadly to our species. The fact that we haven’t gone extinct suggests that over the long term, we have some way of taking out our genetic garbage. And a new paper, recently published in Science, provides evidence that the answer may be linked to another fascinating procedure: sex.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
For about three decades, one of the senior authors of that paper, Alexey Kondrashov, a biologist at University of Michigan, has explored how populations might shed such mutations. The question poses more of a conundrum than you might think. One model of natural selection is that it acts on mutations one by one: letting this one stay, forcing that one out. Another, though, is that the fates of mutations can be linked—an effect that population geneticists call synergistic, or narrowing, epistasis. This might happen if having one mutation can compound the effects of another: for instance, a system that’s able to limp along with one defective piece will fail with the loss of a second or a third. In this way of thinking, for an individual, having more mutations is not just additively worse, but closer to exponentially worse.
To Kondrashov and others, that prediction suggests an escape route from the trap of rapidly accumulating mistakes, both for humans and other multicellular organisms prone to mutations: As the number of nasty genetic errors in a population rises, natural selection will sweep large rafts of them out of the genome together. And in sexual organisms, because of the ways that mutations from each parent can recombine randomly onto the same chromosomes, the synergistic elimination of bad mutations can happen even faster.
Kondrashov has investigated the implications of synergistic epistasis with theoretical studies. Other researchers have taken the experimental route, trying to detect whether, in real life, mutations can interact with each other this way. Those tests yielded mixed results, though, perhaps because the effect would not have to be very large to keep a population from succumbing.
Now, however, Kondrashov and his co-authors have put together a statistical case, pulled from the genomes of about 2,000 people and about 300 wild fruit flies, that the effect has been quietly acting on us and other organisms all along. Drawing on knowledge of the species’ mutation rates and other factors, the scientists began by calculating what the distribution of mutations in populations of humans and flies ought to be in the absence of this purging effect. Certain numbers of individuals in the group, for example, ought to show 100, 50 or 30 mutations. Then the group of researchers turned to the genomic data, looking for the distribution of mutations in real-world populations.
What they found was that significantly fewer individuals than expected had large numbers of dangerous mutations. They are missing from the population, “suggesting that at the high end, at the end where people have many deleterious mutations, there’s stronger selection against these people,” said Arjan de Visser, an evolutionary geneticist at University of Wageningen who was not involved in the work. This observation fits well with what should happen if mutations are not acting independently.
That finding comes with some caveats. There does not seem to be any shrinkage in the number of individuals with less-than-devastating mutations, cautioned both Kondrashov and Shamil Sunyaev, a computational geneticist at Harvard Medical School and another senior author of the paper. “We don’t see it for the whole genome,” Sunyaev said, although the decrease is there “at least for mutations that are undoubtedly deleterious in effect.” The team would also like to get better data on the consequences of mutations in parts of the genome that don’t make proteins. That would let them run their statistical tests again with more confidence that the interactions are occurring more broadly.
Still, the evidence is provocative, and the idea elegant. “I always found it quite attractive, biologically,” said Brian Charlesworth, an evolutionary geneticist at University of Edinburgh who was not involved in the study. “If you think about someone getting hit on the head with a hammer, the first few blows might not do you too much harm, but after a while it will finish you off.” Of the new work, he said, “It’s really the first study which comes up with evidence from what’s going on actually out there in natural populations.”
Perhaps the most interesting corollary of this finding, however, is that it might help explain the persistence of sex. Among population geneticists, sexual reproduction is notoriously difficult to justify as an evolutionary strategy. As a sexual organism, even if everything goes well—if you manage to find a mate who accepts you, if you manage to conceive—you will still be passing on only half of your genes. An asexually reproducing organism, having daughters by making perfect copies of itself, gets double the benefit, none of the hassle. Yet clearly, sex continues.
The redeeming feature of sex, when it comes to evolution, seems to be that it shuffles the parents’ genes together in endlessly new combinations. Unless you have an identical twin, none of your siblings are just like you. And each of your sperm or egg cells carries a mish-mash of your own genes, so none of your children will get the same thing. Sex leads to greater variety for natural selection to work with, a wider palate of quirks, abilities, shapes and sizes that might be fitted to the situation at hand.
The benefits of this arrangement may exceed the costs, though, when there is some efficient way to get rid of the real genetic disasters. And that’s where this new work comes in. Dangerous mutations can be wiped out from the population en masse only if they happen to get shuffled together, thanks to sex, into the same individual. That unlucky “individual” loaded with bad mutations could be a sperm cell that’s not fit enough to ever reach an egg, or an organism that is not healthy enough to ever reproduce. Either way, that combination of mutations would drop out of the population, never to be passed on.
At one stroke, then, a large mass of worrisome problems—brought together by sex, then doomed by their associations with one another—would be culled from the gene pool.
Nearly 30 years ago, Kondrashov, then a scientist in the Soviet Union, wrote a paper for Nature that pointed out this process, now called the deterministic mutation hypothesis, could help to justify sex. “The [genetic profiles] that are eliminated can contain many mutations, which may give a sexual population an enormous advantage,” he mused in the paper. In an asexual population, because the members are genetically identical, natural selection can’t purge bad mutations rapidly without killing everyone.
Speaking from his summer research base near Moscow, Kondrashov said he hopes to see more experimental verification of the interactions between mutations. “Before it’s replicated on a number of species, I’m reluctant to say that we made a discovery,” he said dryly. “But I can’t think of any other explanation.” Next he plans to raise a carefully controlled population of fruit flies in which the genetic variation among individuals is known from the beginning, and then to run selection experiments to see in more precise detail exactly how it changes over time.
Helpful Mutations Didn't Sweep Through Early Humans
Signals of Natural Selection Found in Recent Human Evolution
Human Genome Still Chock-Full of Mysteries
Furthermore, the statistical test the group uses should be applicable to any population where researchers have some basic information to plug in, de Visser noted. It would be relatively straightforward for other scientists to apply it and see if they can uncover similar interactions in other human or animal populations.
It is easy to assume that, in an era with modern medicine and agriculture, we humans have somehow escaped the grasp of natural selection. But this glimpse into the mutational landscape of the human genome shows selection may still be acting on us without our noticing it, even as our numbers boom. These absences in the population, these empty places at the high end of the mutational distribution—these may be selection’s fingerprints on our DNA.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Suppose someone builds a wall. A great and tall wall that is both impenetrable and beautiful. Who knows—maybe it's even solar powered. This wall stands 10 meters tall and goes on and on and on.
Now suppose someone wants to toss a bag of stuff over that wall. A big bag with a mass of, oh, 60 pounds. (I will say 27 kilograms, because kilograms are better.) How much force must be applied to get this bag over that wall? And what happens if the bag bonks someone on the other side?
Yes, this is a two-part question. We do that a lot in physics.
I find that the best way to start just about any physics problem is to diagram it. Visualizing the problem helps determine just what you need to solve for and what you already know. Plus, I like making diagrams.
Yes, I know the diagram is not to scale. Don't worry about that. The key thing to consider is that the guy (or hombre—good or bad, you decide) must push this bag of stuff some distance s to get it up to speed. The sack then moves up to a height h so that it goes over that tall, glorious wall. Right here we should be able to see a path to the solution. We can solve this problem using the work-energy principle because we don't care about time, only distance. This principle says that the work done on a system is equal to the change in energy of that system.
If I choose a system consisting of the sack and the Earth, we will see two types of energy in this system: kinetic energy and gravitational potential energy.
Here g represents the gravitational field with a value of 9.8 N/kg. But what performs the work on this system? The man (or hombre), of course. As the fellow pushes on the sack of stuff, he applies a force (F) over some distance (s). The angle between this force and displacement would be zero degrees such that the cosine would be 1. And so the total work is:
Now for the change in energy. If I consider the sack at the moment just before it's thrown the starting point, it starts with a kinetic energy of zero joules. For the second point, I will use the highest point in the path at the sack clears the great and awesome wall. Here, too, the sack has a kinetic energy of zero joules. Therefore, the change in kinetic energy is zero joules.
For the gravitational potential energy, let's make the starting point y = 0 meters such that the initial potential is zero joules. At the highest point, this gives the sack a potential of mgh.  Making this work equal to the change in energy, I can solve for the require throwing force:
Now we plug and chug. Using a sack mass of 27 kg and a beautiful bodacious wall height of 10 meters, I just need a value for the throw distance. Let's be generous and give it a value of 1 meter—using both arms and legs to increase the distance of the throw. This would require an average throwing force of 2,646 newtons, or almost 600 pounds. That makes this one tough sack-throwing hombre, someone not to be messed with.
But wait! But what if that hombre hurls that sack over that mighty wall and bonks someone on the head? How do we find out what happens. Well, the physics is exactly the same, but backward. If you caught this sack on the other side with a catching distance of 1 meter, you would need an average force of 2,646 Newtons.  If it hit you in the head, it might stop over a much shorter distance of about 0.25 meters.  In this case, there would be an impact force of 10,584 newtons. Bam. That would hurt. Hopefully someone gives this massive magnificent wall some windows so you can see the sacks being tossed over.
Among the many reasons humans are bizarre among mammals (the dearth of body hair, the bipedalism, the fact that someone invented the turducken) is a sad shortcoming: You and I don’t have sensory whiskers. Cats, dogs, raccoons, sea lions—you name a mammal and it’s probably got special hairs sprouting out of its face. After all, whiskers are immensely useful. Rats use them to navigate the darkness, for instance, while a seal's whiskers detect the movements of fishy prey.
Whiskers are all the rage in nature, so why not give them to robots? Mechanical engineer Mitra Hartmann of Northwestern University is doing just that. In a new paper published in the journal Soft Robotics, Hartmann and her team detail how they’re one step closer to a rat-like machine that can feel an object and pinpoint it in 3-D space—meaning robots of all kinds could soon get a powerful new sense.
In animals, the whisker doesn’t have sensors running along its length—they're packed into the follicle instead. When the whisker hits an object, those sensors trip and relay information to the brain, acting as a supplement to the critter’s other senses.
If you wanted to give a robot whiskers, you could use something called a six-axis load cell. Connect a whisker to this device and it can relay lots of information when the appendage hits an object: lateral force, how much the whisker twists in the follicle, how much it pushes into the follicle, in what direction the whisker bends and how much it bends. That's a whole lot of information, and that kind of cell is bulky and expensive.
Working in a simulation, Hartmann and her team discovered that you don't actually need all of this information to pinpoint objects with a whisker. “You just need how much the whisker bent, what direction the whisker bent, and how much it got pushed into the follicle,” Hartmann says. She's currently working on two versions of a sensor that can measure those three signals. At .8 mm cubed, the first is about half the size of a macro-scale six-axis load cell. The other is 1 mm long and 0.5 mm in diameter—about the same size of a real rat's follicle.
Meet Salto, the One-Legged Robot With an Incredible Leap
Finally, the Robot Bat We Deserve and the Robot Bat We Need
Soft Sensors Might Make Wearables Actually Wearable
The team also looked at the effectiveness of two different whisker designs: tapered and plain old cylindrical. Tapered is what you'd find in nature—thicker at the base and ending with a pointy tip—and the team found that they could pinpoint objects with tapered whiskers but not cylindrical ones. Why, exactly, Hartmann can't say. But she ventures a guess. "If the whisker were cylindrical, then at every point along the whisker's length the stiffness would be the same," Hartmann says. "The tapered whisker is more flexible in some regions than other regions—it's more flexible at the tip."
So, now Hartmann knows what signals and what whisker shape she needs to build a rat-like robot. The question then becomes: Why whiskers? Robots can already map their surroundings in incredible detail using lasers. Why complicate matters with funny-looking face hairs?
The idea is that whiskers aren’t a replacement for machine vision—they’re a complement. Once night falls, traditional vision is no longer an option. And when a dust storm moves in, lasers are right out. So in these trying times, the robot could feel its way around instead. Whiskers could even detect currents like a seal, making for a powerful underwater robot.
Another upside: Whiskers are a sneaky way to map your environment. “What if you wanted to avoid detection?” Hartmann asks. “You wouldn't want to go blazing light all over the place, right? You'd want to be sneakier than that.”
So does that mean one day you'll get a whiskered robot cat instead of a real one, à la Blade Runner? Maybe. If it comes without the attitude problem, count me in.
Robots can learn to do tasks just fine. Getting different kinds of robots to share knowledge, though, is another challenge entirely.
Early this morning, a white Mercedes Sprinter van began a delivery route along the streets of Fancher Creek, a residential neighborhood on the southeastern edge of Fresno, California. Its cargo? 100,000 live mosquitoes, all male, all incapable of producing offspring. As it crisscrossed Fancher Creek’s 200 acres, it released its payload, piping out swarms of sterile Aedes aegypti into the air. It’ll do the same thing tomorrow, and the next day, from now until the end of December.
Though counterintuitive, the goal of this daily mosquito dump is actually to get …  fewer mosquitoes. Specifically, fewer female Aedes aegypti, the ones that bite and lay eggs and transmit diseases, including the United States’ newest scourge: Zika.
That mission, to “Debug Fresno,” is emblazoned across the van’s driver side. Behind its wheel is an employee from Verily—the secretive Alphabet spinout formerly known as Google Life Sciences. Verily partnered with MosquitoMate, a Kentucky-based sterile mosquito breeder, and Fresno’s local authority, the Consolidated Mosquito Abatement District, to release a million insects a week through the end of 2017. It’s the largest sterile mosquito trial in US history.
The scale of the project isn’t the only thing Googlian about it. Lest you think pest control is a step down for the company behind the ambitious Baseline Study and operating rooms staffed by surgical robots, Verily has turned one of the buildings on its South San Francisco campus into an autonomous skeeter factory capable of producing 150,000 bugs daily. Raising mosquitoes isn’t easy. You need precise conditions—the right levels of heat, humidity, and light—and because they grow so fast they need to be watched 24/7. It’s a laborious process for humans. But perfectly suited to robots.
Verily’s larval-rearing robots live inside a self-contained factory system a few thousand square feet in size. The system mostly runs on its own, syncing up feeding cycles and monitoring growth. Then, every day, humans take out a batch of pupae that are ready to be sorted into males and females. This is the most critical step: In order for eradication to work, Verily has to be 100 percent sure to release males and only males.
That’s because every mosquito here was bred to carry a bizarre bacterium called Wolbachia. When a Wolbachia-carrying male mates with a wild, local female, none of their offspring will be able to hatch. If the sterile males can outcompete the wild ones long enough, eventually, no more mosquitoes. But—and it’s a big but—if a bacterially-infected male mates with a bacterially-infected female, the two wrongs make a right, and the eggs will hatch into healthy mosquitoes. So, you gotta get the sex-sorting right.
Historically, the sex-sorting has fallen to humans. University of Kentucky entomologist Stephen Dobson, who patented how to make Wolbachia-carrying mosquitoes back in 2005, has gotten really good at spotting the differences between male and female. But it’s still a total pain in the ass. Dobson founded MosquitoMate in 2013, and the company ran a much smaller trial here in Fresno County last summer. His team used a sieve-like device to sort the sexes by size in their pupal stage, and then gave each mosquito another look under a microscope before shipping them from Kentucky to California, where district officials released the bugs by hand, shaking them out of cardboard tubes. He knew that scaling up was going to take some serious engineering, so when he met Verily’s Jacob Crawford at an entomology symposium, it seemed like a natural partnership.
A California City Is Fending Off Zika by Releasing 40,000 Mosquitoes Every Week
These Scientists Saw Zika Coming. Now They're Fighting Back
Florida Votes to Release Millions of Zika-Fighting Mosquitos
While Crawford was very light on details about its custom-designed autonomous sex-sorting set-up, he did say that it’s a two-step process that leverages the company’s computer vision technologies. “Achieving sex sorting at any scale has been an enormous hurdle,” says Crawford. “This is the stuff entomologists dream about.”
Dobson agrees. This year’s trial will put 25 times more mosquitoes on the streets of Fresno than MosquitoMate was able to do on its own last year. That level of coverage could actually kill mosquitoes in a wide enough range to protect people as they move around between their homes and work and school. “At this scale you’re starting to get into the level where you can actually have an epidemiological impact on disease transmission,” says Dobson. And while Fresno doesn’t currently have any of the diseases that Aedes aegypti carries around with it—Zika, dengue, yellow fever, chikunguya—all it takes is one infected visitor for the disease to establish itself in the local mosquito population.
Aedes aegypti arrived in California in 2013, and since then, it’s already spread from Fresno to places like Los Angeles and San Francisco counties. “This invasive species has really changed everything about mosquitoes in California,” says abatement district director Steve Mulligan. “They’re expanding ranges, and they don’t respond well to conventional control methods. When you add that along with emerging diseases it’s a real challenge.” If this summer’s trial proves effective, Mulligan and Dobson hope they could expand to other places in California and wipe out the existing pockets of Aedes aegypti before they become a permanent feature of the landscape.
Verily, on the other hand, is already thinking beyond California, and even the US. Its ultimate goal is to be able to make more mosquito factories, ready to ship all over the world whenever a new mosquito-borne disease strikes. They would provide a steady supply of sterile males to local public health officials fighting the next big outbreak. Verily’s Debug the World Tour might start in Fresno, but there’s no saying where it will lead next.
Scientists in California are breeding and releasing mosquitos into Zika hotspots. While it may seem like they're making matters worse, they are actually releasing a kind of biological trojan horse.
Update: Google's Transparency Project has posted an addendum to its Academics Inc. report.
Earlier this week the Wall Street Journal published a detailed investigation showing that Google has been systematically paying academics to publish research favorable to the company’s policy and business positions—often without disclosure of the financial relationship. Concurrently, an organization called the Campaign for Accountability published a report from its Google Transparency Project showing the same thing, and naming many more as the recipients of direct or indirect Google dollars.
It’s the kind of thing that would seem to contravene Google’s one-time exhortation “don’t be evil,” and the Journal article seems to have the company nailed. But in the case of the Google Transparency Project, things aren’t that simple. Several academics named in GTP’s database say that they’ve never received money from Google—that they’re innocent dolphins caught up in what was, best case, a methodologically sloppy tuna net.
In at least one instance, the GTP database captured a scholar with no financial ties to Google. Annemarie Bridy, a professor of law at the University of Idaho, got named because of her status as an affiliate scholar at Stanford’s Center for Internet and Society. Stanford CIS does list Google as one of its funders, but the center’s director, law professor Barbara van Schewick, confirms that it has no financial relationship with its affiliate scholars. “The idea that because I have those affiliations I’m somehow tainted by a relationship to the donors to those centers, I think is ludicrous,” Bridy says. “They said I receive indirect funding. That is a verifiably false factual claim harmful to my reputation, which is pretty much the definition of libel.”
Bridy isn't the only one. Van Schewick herself shows up in the database, owing to the Google contribution to CIS. Like Bridy, she has requested the Google Transparency Project remove her name—CIS doesn’t pay her salary. Stanford Law does.
Aaron Perzanowski, a law professor at Case Western Reserve University, seems to have been slurped into the database only by (dubiously-interpreted) association. Perzanowski says he has never taken money from Google, but his frequent co-author Jason Schultz once worked for the Electronic Frontier Foundation and then for the Samuelson Law, Technology, and Public Policy Clinic at UC Berkeley—both of which have indeed gotten Google dollars. But it was in the form of cy pres payments, basically a judge taking money from a class action settlement and directing it to some other recipient. “I suppose in some sense you could say that Google funded that center, perhaps unwillingly, right?” says Perzanowski. “That’s where this idea of indirect funding gets really messy. I’m not suggesting there’s an easy answer one way or the other, but this is saying individuals who at one point two jobs ago worked for an organization like the University of California that received funding from Google are forever tainted.”
It’s not clear how far back and how deep such connections should be to register as compromising. What warrants disclosure? Casey Fiesler, of the Department of Information Science at the University of Colorado Boulder, is in the database because of a Google Policy Fellowship she had in 2011, which paid some of her living expenses while she was working for Creative Commons (another recipient of Google funds) after law school. Fiesler says she doesn’t even remember if Google or CC determined she’d get the money. “I never worked for Google, and as far as I know they had nothing to do with the work I was doing at Creative Commons,” Fiesler says. “And the work I did that summer had nothing to do with the research referenced in the database.”
Here’s where “indirect” and the lack of clarity on what constitutes a disclosure-worthy association become complicated. Professionals are terrible at judging what kind of influence even a small gift or exchange can have on later behavior. And disclosure is a poor metric for potentially unethical behavior.
Still, though, a grad school fellowship is a long way from prospective pay-for-play. “The only way I can think of that would in any way relate to me is that Google is finding impressionable graduate students and giving them money so that maybe when they do policy work later they’ll favor Google,” Fiesler says. It does sound unlikely.
The Campaign for Accountability buys some but not all of that. “A lot of the emails I’ve received sort of don’t change my mind. I think we’ll add what they say about it or their defense,” says Dan Stevens, the Campaign’s executive director. “We’re working on a post or something we might add to allow these folks to voice what they say.” Stevens says that should happen in the next few days.
Bridy, he says, received what might be described as an indirect benefit through her association with the Stanford Center for Internet and Society, but “given that she’s not receiving any payments, it seems like something we can update in the database.” Perzanowski’s connection to Schultz, he says, is harder to figure out. “I think this is one where we’re going to have to think about how we disclose accurately.” And as for Fiesler? “Yeah, I think that’s fine,” Stevens says. “There’s a specific relationship between her and Google. She received direct funding from them.”
The Wall Street Journal article didn’t include Bridy, van Schewick, Perzanowski, or Fiesler. A person familiar with the article’s reporting (who wasn’t authorized to speak for the paper and so asked to go unnamed) says the writers, Brody Mullins and Jack Nicas, went through GTP’s list and retained entries, adding their own academics and throwing out many of GTP’s. Mullins and Nicas did their own investigation and analysis out of due diligence, but also because they were aware that Campaign for Accountability had a history of going after Google specifically.
Stevens says as much. “Our point is to say, look at Google. They’re funding people to extract this result,” he says. “Building this database to show their influence campaign, that’s the point. We don’t want to impugn anybody specifically. The larger issue is Google’s attempt to influence all these academics.”
Even the people who don’t think they should have been in the database agree that the pay-for-play the Journal uncovered and GTP wants to talk about is a pernicious, serious problem. “I think the issue is so important that I don’t take any money and I wouldn’t take any money that could be seen as compromising my independence,” Bridy says.
Perzanowski says he had the same reaction to the initial reports as any casual reader. “I saw the headlines and thought, ‘This is bullshit, people shouldn’t be out there taking money from Google and not disclosing. Who are these bad colleagues of mine?’” he says. “And I pull up the database and, like, that’s my name.” He adds: “Because of the sloppy way this was done, it’s distracting from a really important issue.”
“Sloppy” is, in fact, a word that came up again and again in my reporting. But so did a question: Who funds the Campaign for Accountability? I should have checked before I wrote my initial story, and didn’t—which I'm embarrassed about, because it turns out the organization, dedicated to transparency in financial affiliations, does not disclose its financial affiliations. “It’s just always been our policy since the beginning not to disclose our funding sources,” Stevens says.
Isn’t it ... ironic? “Of course I’ve heard that before. I would just say that we’re not a major company trying to extract things from government and policymakers. We have a different purpose,” he says. “Our statement from the beginning has been, let the work speak for itself.”
As a 501(c)(3) nonprofit, the Campaign for Accountability should have an Internal Revenue Service form 990 detailing its funding. It doesn’t. “We started in May of 2015 as a project of the New Venture Fund. They spun off some of their projects into the Hopewell Fund, which we were part of through 2016,” Stevens says. “We’ve been a standalone project through 2017, so because we are still new and getting spun up, we don’t have a 990 yet.”
Prior reporting has said that at least one funder of the organization is Oracle, which—perhaps unrelatedly—has been locked in a bitter legal battle with Google, the outcome of which could bring one side or the other billions of dollars.
That all makes it hard to draw conclusions about whether the Campaign for Accountability made the best-possible decisions with messy data, made bad decisions with messy data, or had some other motive. “Maybe they don’t care about the academics. I think what they probably care about is making Google look bad by whatever means they can,” Perzanowski says. “We’re not the target. We’re just sort of the ammunition.”
The first semester of an undergraduate physics course invariably spends a lot of time on two big ideas: The momentum principle and the work energy principle. Both deal with forces acting on an object, which often leads students to think they are similar. In a way, they are, and they play a huge role in almost everything you learn during an introduction to physics.
Before I give you a great physics question that uses these ideas, I will go over them in a super-brief physics lesson. First, the momentum principle says that a net force changes the momentum of an object where the momentum is the product of mass and velocity. Working in one dimension to avoid dealing with vectors, I can write it like this:
If you consult your introductory physics textbook, you'll see that this is essentially the same as Newton's Second Law, which states that the net force is equal to the product of mass and acceleration (where acceleration represents the change in velocity). You can rewrite the momentum principle to solve for the change in momentum (which is useful).  It looks like this:
Trust me, you'll find this equation useful in just a little bit.
OK, now for the second big idea, the work energy principle. It states that, for a single particle, the work done on an object is equal to the change in kinetic energy. Work is defined as the product of a force in the direction of a displacement. I can write this as:
Just to be clear, Δr represents the displacement (how far the force pushes something) and θ represents the angle between the force and the direction the object moves. As with the momentum principle, I can rewrite this so it looks a bit more useful:
Let's take a second and look at these two ideas. Two things differentiate the momentum principle from the work energy. First, it is technically a vector equation because the momentum of an object depends upon its direction of movement. Second, the momentum principle depends upon the change in time (this is important). The work energy principle depends only on displacement, not time.
OK. Now to my great physics question. Suppose a heavy truck and a light car start with the same momentum (if it makes you happy, we can say the truck has a mass three times that of the car). Both vehicles have the same force acting on them to bring them to a stop. Which one stops first?
If you want to take a moment to think about this, I'll wait.
I'm still waiting.
OK, hopefully you have an answer by now. If you like, you can check with friends to see what they think. However, since I'm not there and you aren't here, I will just share two common answers people provide.
Answer number 1: The light car stops first. Since it has lower mass, the force acting on it results in larger acceleration. This, in turn, causes the car to slow down more quickly because the truck has a large mass and a small acceleration.
Answer number 2: They stop in the same amount of time. Yes, it's true that the car has a lower mass and a higher acceleration. However, it starts with a much larger velocity since the two vehicles have the same starting momentum. In the end, both vehicles will have the same force with the same change in momentum. According to the momentum principle, they must have the same change in time.
Clearly, answer number 2 is correct. The cars stop at the same time because they start with the same momentum. Just for fun, let's create a numerical calculation for this. Of course, that requires some actual values for the mass of the two vehicles, the starting momentums, and the stopping force. We'll say the car has a mass of 10 kg (it's a really small car) and the truck has a mass of 30 kg (three times the mass of the tiny car). The initial momentum is 20 kg*m/s and the stopping force is 2 newtons.
A plot of the x-velocity for the car and the truck looks like this:
You can see that the car does indeed start with a higher velocity, but both cars stop at the same time. Yes, this is a plot of velocity vs. time instead of distance vs. time for a very particular reason.
Now for the next (and more interesting) question. Using the same situation we examined above, which vehicle stops in the shortest distance and why? Figure it out and explain your answer. I'll wait.
Really, you should answer this one.  Take your time.
I'll enjoy this picture of a horse while I wait.
Do you have an answer? Are you sure about it? I really ought to just stop here, but I can't leave this question unanswered. I enjoy talking about it too much to do that.
Instead of explaining the answer, I will show you the answer. Here is a numerical calculation of the two vehicles stopping. It's basically the graph above, except you can see the motion of the two objects.
Just press play to run it and the pencil to see (and edit) the code. The big red box represents the truck and the small blue box is the car. You'll notice the two vehicles leave a trail of dots. I did that so you see how fast they are moving. An arrow represents the velocity of the car.
Clearly, the red truck stops first. Let me explain why. When determining the time required to stop an object, it makes sense to use the momentum principle since it deals with time. To find the distance it takes an object to stop, I must use the work energy principle. Since the two vehicles will have the same acting force on them, I can compare stopping distances by looking at the change in kinetic energy. If the vehicles started with the same kinetic energy, it would take the same amount of work to stop them. With the same force, this would be the same stopping distance.
The fact the two vehicles have the same starting momentum doesn't mean they have the same starting kinetic energy. The car has a lower mass, so it must have a higher velocity in order to have the same momentum as the truck.  But since kinetic energy depends upon the square of the velocity, the higher car velocity matters much more than the lower mass. The car starts with a higher kinetic energy and thus requires more work to stop it. With a greater work, the force has to be applied over a larger distance.  That's the explanation.
But wait! I have one more question for you as homework. What kind of starting velocity would the car require to stop in the same distance as the truck? No, I am not revealing the answer. You're on your own. If you change the starting speed of the car, which vehicle stops in the shortest time? It's your turn to do the physics.
Picture a rectangle of fabric cut from a standard grey t-shirt. It’s stretchier than most tees, because it’s made from a mix of nylon and spandex, not cotton. And it stands out in another way, too: If you flip back a corner of the cloth, one side has an unexpected metallic sheen.
This textile isn’t the creation of a sci-fi costume director. It’s called shieldex, and it was exactly what textile engineer Asli Atalay and her team at Harvard needed to develop a soft, stretchable, motion-measuring sensor. The metallic shine comes from silver coating the flexible fibers, so the fabric can stretch and conduct electrons at the same time. Rather than slapping silicon chips into bracelets, these electronics could give wearables more of the stretchability and comfort of the best sweatpants.
While the roboticist’s arsenal of metal components and silicon chips accomplishes a lot, softer robotic wearables could be friendlier for injuries, or older users, driving down the risks to humans while still providing help with, say, opening a jar. Think gloves that boost grip, or sleeves that act as assistive exoskeletons. “You put on a t-shirt, a sweater, a pair of socks—you could have these types of sensors embedded in them,” says bioengineer Conor Walsh, a co-author of the paper.
To make the sensors, Atalay first sandwiches two layers of souped-up fabric around a film of soft, electrically insulating silicone. Then, a trusty laser cutter slices the sandwich into whatever shape she wants. She runs a hot iron over an adhesive to attach the electrical leads—like attaching an iron-on patch to your jean jacket, except she’s sticking a tiny wire to each layer of silver spandex.
Technically, what she's building is a parallel plate capacitor—each side of the metal-plated fabric is an electrode, holding equal but opposite charges. As the fabric stretches, the insulating silicone between the electrodes thins out and the electrodes get bigger and closer together, changing the sensor's capacitance (that’s the the charge on each conducting plate divided by the voltage difference between them). That capacitance change is used to measure how far the fabric stretches. And voila: a batch of stretchy, flexible motion sensors.
When Atalay and her collaborators attached these sensors to the fingers of a glove, they registered capacitance changes between different hand positions. Walsh imagines that a sensor integrated into a t-shirt would measure heart rate. Though it's not something you should expect to see on shelves soon: “We’re not quite at the put-it-in-the-machine and wash it for 20 cycles stage yet,” Walsh says.
The Robots Are Coming for Your Heart
Soft Robot Exosuits Will Give You Springier Steps
MIT Prof Invents a Squishy Material for Shape-Shifting Robots
Full-on roboclothes will also need other infrastructure to support these stretch-tracking sensors. A gripper would need actuators to provide oomph (Walsh’s lab has some in the works), and then chips for “wireless communication, data storage, and power, so that your glove is truly a fully integrated wearable system,” says Sheng Xu, a soft electronics researcher at UC San Diego. Xu has worked on stretchable lithium ion batteries, and other groups continue to make new types of optical fibers, Bluetooth antennas, and processing chips that are smaller and more flexible.
Other groups have made stabs at stretchy sensors before: They've tried carbon nanotubes, graphene, and liquid metals as the conducting electrodes in similar devices. But Walsh is excited that their process is capable of forming many sensors at once, rather than building just one sensor at a time.
Mass production is exciting, because stretchable electronics are geared to alter other human-machine interfaces, too. In Xu’s view, “the virtual world is also basically electronics,” so more sensors like these could crop up in VR gear. And inflatable robots, or the inflatable space dwellings that NASA is testing, would benefit from neatly integrated sensors in their fabric structures. Now that's metal.
A robotic heart points the way to a future where soft robots help us heal.
I went to Antarctica 20 years ago, and I didn’t care about ice shelves. I noticed one at last when the blinding white of the ice, struck up against an abidingly black ocean, made me understand at last why the penguins all around me and the orcas occasionally surfacing a few dozen feet away had the same basic color scheme. Evolution ain’t stupid.
But evolution isn’t smart, either, or we humans would be much better at perceiving patterns without such obvious visual clues. Like, when a 1.1 trillion-ton, 2,200-square-mile piece of ice breaks off of the Antarctic Peninsula—the fiddly spit-curl in the upper left1 of most maps of the continent—we might be able to see it not just as megasized glaciological action but as yet another piece of the global weirdness, increasing in magnitude and frequency, that tells us Earth is getting hotter, the seas are rising, and we are all in trouble.
Alas, no.
On its own, a massive iceberg unconsciously uncoupling from the Larsen C ice shelf won’t raise sea levels along the world’s coastlines—the newly-calved iceberg was already floating. Researchers from the UK-based Project Midas have been watching the region for decades and expected the break-up; they were there to be supportive just as they were when Larsen A split in 1995 and Larsen B collapsed in 2002. (Some peninsulas just fear commitment.)
I don’t think people are stupid. I think they recognize there’s a broad, systemic change happening. What used to be an esoteric concept is now something that hits home.
Michael Brune, Sierra Club
Hey, the break-up might not have even been due to climate change. “Although this is a natural event, and we’re not aware of any link to human-induced climate change, this puts the ice shelf in a very vulnerable position,” said Martin O’Leary, a glaciologist at Swansea University, in a Project Midas statement. But even if climate change didn’t make Larsen C fall off, it will make the potential consequences that much worse. “This is the furthest back that the ice front has been in recorded history,” O’Leary continued. “We’re going to be watching very carefully for signs that the rest of the shelf is becoming unstable.”
In 1997, over two weeks in Antarctica—a few days at McMurdo Station, a couple days at the old South Pole Base, and a few more days in the McMurdo Dry Valleys—I was much more interested in reporting on the place as an analog for an alien landscape. I went looking for microbes that could live without water for months at a time. I watched technicians bore into ice to install strings of glass balls like Christmas ornaments that could detect tiny blue flashes of Cherenkov radiation caused by subatomic neutrinos passing through the continent. Talk of the Western Antarctic Ice Sheet and various glacial rivers bored the hell out of me, to be honest. As usual I chased things that sounded like science fiction—only true.
So it’s appropriate, I guess, that it took science fiction to explain why I was being a dope back then. Kim Stanley Robinson’s latest novel New York 2140 is set in a flooded, Venetian Manhattan—catastrophic sea level rise having been induced by the failure of all the various ice shelves and sheets in Antarctica, which in turn allowed all that other ice that carapaces the continent to slide into the warming seas. Oh, says me! That’s why we’re supposed to care about ice shelves.
Yet we still kind of don’t. Even if climate change didn’t send Larsen C packing, the air and oceans on Earth are incontrovertibly warmer than they used to be. That makes it less likely that the Larsen ice will ever bulk up again, and the newly exposed shelf even more vulnerable to the lapping sea. Might this calving galvanize action to fight climate change? “The only appropriate answer is, who knows? It’s been less than a day,” says Michael Brune, executive director of the Sierra Club. “I don’t think people are stupid. I think they recognize there’s a broad, systemic change happening. What used to be an esoteric concept is now something that hits home, literally.”
The question is, can an event like Larsen C move a policy needle? What makes something into a focusing event that opens a policy window, or even just a teachable moment that might shift the positions of the 25 percent of Americans whose opinion on climate change ranges from “meh” to “conspiracy?” “This particular event is very important from a climate science perspective,” says Tony Leiserowitz, director of the Yale Project on Climate Change. "But it’s happening within a very complicated political-economic-cultural landscape where there are already well-entrenched positions, where different audiences exist, and where people will either hear about or not hear about this because of their different media sources or networks.”
Scientists, activists, and journalists all tend to race for the existentially dreadful bottom at times like this. It’s a little ironic, considering that just a few days ago a New York magazine article charting Earth’s impending climatic doomsday took heavy fire from climateers themselves, who, like apologetic wingmen and -women for a drunken friend at a party, quickly tried to minimize the damage. It probably won’t be that bad, it might never be that bad, there’s still time to fix this. In this field, tradition demands a certain restraint when you’re pitching doomsdays—as Elizabeth Kolbert named it in the New Yorker, erring on the side of least drama.
Map Shows Where Sea Level Rise Will Drown American Cities
Britain's Antarctic Research Station Looks Like a Spaceship
The Break in the Larsen Ice Shelf Is Bad for the Planet, But Huge for Science
Is banging a climatic-disaster drum about Larsen C’s viking funeral “dramatic?” Maybe. I can handle the cognitive dissonance of simultaneous apocalyptic despair and hope because of upward spikes in hybrid automobile sales, wind and solar energy, and lots of civilized countries agreeing to cut greenhouse gas emissions. “All responsible scientists are saying, look, there are some very serious impacts coming our way, but we do still have choices, and let’s act on those,” says Rachel Cleetus, lead economist and climate policy manager at the Union of Concerned Scientists. “This is not just gloom and doom. This is a moment that should galvanize us to action, and we should push our policymakers to take those actions.”
Inland from the ice shelves, in the McMurdo Dry Valleys of Antarctica, the Austral summer turns bits of glaciers into transient meltwater ponds. In these rocky, shallow pools, tiny bubbles of microbes lurch, temporarily, back to life. No matter how frozen some Earthly biome may seem—a pond near Seuss Glacier or a Capitol—a little sunshine always has a chance to spark a miracle.
1 UPDATE 7/13/17 3:40 PM Corrected to reflect the correct map placement
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.
Last month, Phoenix enduring a blistering heat wave, with temperatures so high that airport officials had to cancel dozens of flights. The reason was two-fold. First off, some jet engines risk catching on fire in extreme heat. And when air gets hot, it expands and becomes less dense—so an airplane’s wings can’t generate enough lift to get off the ground. Planes either need to speed up during take-off or use a longer runway.
But Phoenix’s flight delays aren’t a one-off event. As the Earth’s climate undergoes a 1 to 3 degree Celsius warming over the next half-century, extreme heat waves will hit more frequently. Some of these heat waves will hit airports with short runways. Forget about rain delays or missing flight crews. At places like Washington’s Reagan National Airport, New York’s LaGuardia Airport, and Dubai International, the real trouble will come during heat waves.
During the hottest part of the day, 10 to 30 percent of planes will have to offload cargo or people, according to a new study by graduate student Ethan Coffell and climate scientist Radley Horton at Columbia University. “This study shined a light on a potential vulnerability,” Horton says. “A lot of airplanes at full capacity are ill-equipped to take off on some of the world’s runways when temperatures get really high.”
The scientists looked at five common commercial airplane models—the Boeing 737-800, Airbus A320, Boeing 787-8, Boeing 777-300, and Airbus A380—and calculated how their takeoffs would be affected at 19 airports around the world, based on projected temperatures from 27 different global climate models. The runways represented the most common climates, elevations, and runway conditions at busy airports around the world—including US airports in Denver, Phoenix, Chicago, Atlanta, New York, Washington, DC, Los Angeles, Houston and Miami.
The good news for air travelers is that London, Paris, and JFK in New York will be able to shrug off the worst of future heat waves. But a Boeing 777-300 departing from Dubai at the time of the daily highest temperature may be weight-restricted about 55 percent of the time according to the study, which appears today in the journal Climactic Science. Because of the short runways, airplanes at Washington’s Reagan-National (7,170 feet) and New York’s LaGuardia (7,000 feet) will also have to lighten their load to get off the ground. Expanding the runways probably won’t work, given that they are either sandwiched along the Potomac River or Jamaica Bay.
Why Phoenix's Airplanes Can't Take Off in Extreme Heat
It's Cheaper for Airlines to Cut Emissions Than You Think
Oh Great. Climate Change Will Make Flying Worse, Too
Heat waves aren’t the only problem facing the aviation industry as the Earth’s climate changes rapidly. Other scientists have calculated that severe turbulence—the kind that sends drink carts flying and sometimes even unbuckled passengers—will increase over certain transatlantic routes that follow the meandering jet streams.
Paul D. Williams, an atmospheric scientist at the University of Reading who published on the connection between climate change and turbulence this past year, thinks the industry needs to start dealing with climate change more aggressively. They could develop engines that produce fewer greenhouse gases, for one, as well as adapt its planes to the future world. “I’ve yet to see a benefit of climate change to aviation,” Williams says. “All the published studies have been about thing getting worse.”
There are plenty of solutions out there, some more complicated than others. Flights may have to leave earlier in the day when its cooler, Horton suggests, or aircraft manufacturers may have to make planes lighter. Or engineers could step in, building some kind of special new wings that generate additional lift. There’s one other option that probably won’t happen: leaving three or four paying customers back at the terminal in order to make the takeoff weight.
Last year was the hottest year since scientists started keeping records in the 19th century. It's no fluke---because it's humanity's fault.
Picture Jupiter. Even if you're a total space junkie, your mental image is probably an orange and white-striped planet with a big red dot in the southern hemisphere. Jupiter's red spot—a storm with a diameter larger than Earth's—has been the planet's most conspicuous feature for centuries, and was definitely the answer to a question on your fifth grade astronomy test.
But the spot itself has always been kind of mysterious. Scientists don't fully understand what created the storm, or how it's been swirling around for so long. And while they haven't figured that part out yet, NASA's Juno spacecraft has brought them closer than they've ever been before—literally. On Monday, Juno skimmed just 5,600 miles above the storm clouds, and snapped some pictures as it went. It's taken the data a few days to get back to Juno's Earthbound science team, but the images are finally here.
The Juno team posted these raw, unprocessed images on the webpage dedicated to images taken by the mission's onboard camera, JunoCam. NASA encourages Jupiter fans to edit the images themselves as a kind of audience participation gimmick, but that doesn't mean these images are the final product.
Still, the snapshots reveal hints of the curiosities to come. The big red spot seems to cause pockets of turbulence in other bands of Jupiter's atmosphere as they pass by the behemoth, though scientists are no closer to knowing how the storm maintains its energy and cohesion. Future images will include data from Juno's other instruments, which should tell scientists more about the atmospheric processes boiling away underneath the storm's brick red surface. For now, though, that surface is still a pretty good view.
Find out how NASA’s Juno Mission will help unlock the mysteries of our planet and our solar system.
E. coli might best be known for giving street food connoisseurs occasional bouts of gastric regret. But the humble microbial workhorse, with its easy-to-edit genome, has given humankind so much more—insulin, antibiotics, cancer drugs, biofuels, synthetic rubber, and now: a place to keep your selfies safe for the next millennium.
Scientists have already used plain old DNA to encode and store all 587,287 words of War and Peace, a list of all the plant material archived in the Svalbard Seed Vault, and an OK Go music video. But now, researchers have created for the first time a living library, embedded within, you guessed it: E. coli. In a paper published today in Nature, Harvard researchers1 describe using a Crispr system to insert bits of DNA encoded with photos and a GIF of a galloping horse into live bacteria. When the scientists retrieved and reconstructed the images by sequencing the bacterial genomes, they got back the same images they put in with about 90 percent accuracy.
The study is an interesting—if slightly gimmicky—way to show off Crispr's power to turn living cells into digital data warehouses. (As if E. coli didn’t already have enough on its plate, what with securing global insulin supplies and weaning the world off fossil fuels.) But the real question is, why would anyone want to do this?
To the left are a series of frames from Eadweard Muybridge’s Human and Animal Locomotion. To the right are the frames after multiple generations of bacterial growth, recovered by sequencing bacterial genomes.
If you’re Jeff Nivala, it’s not to preserve visual messages for people in the far-off future. It’s so he can turn human cells like neurons into biological recording devices. “The E. coli is just a proof of concept to show what cool things you can do with this Crispr system,” says Nivala, a co-author on the paper and geneticist at Harvard. “Our real goal is to enable cells to gather information about themselves and to store it in their genome for us to look at later.” That concept is called the “molecular ticker tape.” It’s something George Church thought up before Nivala, a post-doc, arrived in his lab. But it’s a challenge Nivala thinks is uniquely suited to Crispr.
In case you’ve been living in a bunker, Crispr-Cas9 is a revolutionary molecular tool that combines special proteins and RNA molecules to precisely cut and edit DNA. It was discovered in bacteria, which use it as a sort of ancient immune system to fend off viral attackers. Cas9 is the protein that does all the cutting, i.e. gene editing’s heavy lifting. Lesser known are Cas1 and Cas2. They’re the ones that tell Cas9 where to do the cutting.
Church's lab plans to leverage that system to get human brain cells to show how exactly they develop into neurons. Nivala thinks they’ll be able to do that because of how Cas1 and Cas2 work. During a viral invasion, the proteins go out and grab a piece of the attacker’s DNA, which they slip into the bacterial genome for another enzyme to turn into a matching guide RNA. That’s what helps Cas9 find (and then chop up) copies of the virus in the cell. The really cool bit is that Cas1 and Cas2 don’t just insert viral DNA into the genome at random. As they encounter new threats, they add DNA in the order in which it arrives. That turns a cell’s genome into a temporal record—think ice cores for molecular history—of whatever the cell encounters.
To the left is an image of a human hand, which was encoded into nucleotides and captured by the CRISPR-Cas adaptation system in living bacteria. To the right is the image after multiple generations of bacterial growth, recovered by sequencing bacterial genomes.
One day, Nivala thinks scientists will be able to use that system to record synaptic activity. Like a guest book at a wedding, embedded signals in the genome could tell researchers exactly which neurons were talking to each other at different times, in response to different stimuli.
“If you think of a cell as a processor, this adds a thumb drive, which stores information for later processing,” says Karin Strauss, lead researcher on Microsoft's own DNA storage project. Last year the company set a new record—200 megabytes—and has plans to get a DNA storage system up and running by the end of this decade. “As for DNA data storage in the IT industry, it is more well served by standard DNA synthesis and sequencing at the moment, because they are easier to control and a lot denser than whole cells,” says Strauss, who is unconnected to the Harvard research.
Companies that make custom DNA, like Twist Biosciences, are already selling to customers using it for storage purposes. But it’s still only a small piece of their business—about 5 percent. Costs have to come down by a factor of about 10,000 before DNA becomes competitive with traditional storage methods. But the long-term benefits will be huge; properly stored in a cold, dry place, DNA can keep data intact for at least 100,000 years.
Crispr Creator Jennifer Doudna on the Promises—and Pitfalls—of Easy Genetic Modification
Scientists Capture Crispr's Gene-Cutting in Action
Easy DNA Editing Will Remake the World. Buckle Up.
That’s why scientists like Ewan Birney, director of the European Bioinformatics Institute, are working on better tools and methods to make DNA storage truly scalable. In that endeavor he doesn’t see a place for live cells, which start out at less than 100 percent accuracy and are susceptible to mutations over time that could further degrade data integrity. “It’s cute, and I wish I’d done it,” said Birney of the Nature paper. “But it doesn’t add much on the DNA storage side of things. What did impress me was the amount of edits they achieved with high fidelity. It’s a real tour-de-force of Crispr.”
So, at least for now, there’s no reason to think your family photo albums will one day be backed up on an E. coli drive. More likely, the memories cells store will be their own.
1Disclosure: One of these researchers is married to a WIRED editor.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
When ferrets get a rabies shot in a neurobiology lab, they don't get infected with the virus—or even inoculated against it. They get a brain hack that might just explain how your brain handles vision, and maybe even your other senses, too.
In a lab at Dartmouth, scientists are experimenting with targeted injections of a modified rabies virus into the brains of ferrets—essentially allowing them to control how the animal responds to simple visual patterns. The goal is to understand the brain's enormously complex visual processing system. But really? Rabies? Ferrets? Are these guys just screwing around?
Lots of visual research depends on lab mice—the most popular of model organisms in biology. But Dartmouth neuroscientist and lead author Farran Briggs wanted to study an animal that uses its vision the same way humans do, in an evolutionary sense: to prey on tasty snacks. Mice aren’t predators, and their vision falls solidly in the ‘legally blind’ range. So these vision researchers turned to the notoriously vicious ferret and its front-facing eyes. They're color blind, but at the neural level, ferrets’ visual systems have “remarkable similarities to a primate, and a human,” says Briggs. (Ferrets also help avoid the ethical issues of experimenting on primates.)
The classic approach to understanding visual circuits would be to find a way to switch off part of the system, and see what changes. That’s one reason why mouse models are great: Scientists have figured out how to breed mice with light-sensitive instructions inserted next to specific genes—a relatively new field called optogenetics—so that neurons with that specific gene will turn on when exposed to light. Biologists don't have the same systems for genetic control in ferrets, though, so Briggs and her team figured out a different way to engineer light-controlled brain bits.
In this case, they used rabies—not so much a a virulent pathogen here as an on-off switch for neurons. When an animal gets rabies, the virus sneaks into nerve cells thanks to special protein coating that acts like a secret, neuronal password, and Briggs and her team capitalized on that entry system. With help from the Salk Institute scientists who invented this technique, they plunked a light-sensitivity gene into the rabies virus and stripped off its coat-protein genes. Then they covered their modified virus in just enough protein to enter one cell, and injected it into the part of the ferret’s brain they were interested in. “Once those viruses get into those neurons, they’re stuck,” says Briggs. The virus makes the neurons light-sensitive, but otherwise keeps to itself.
The Unexpected Science of Manipulating Neurons With Light
A Cure for Blindness Just Might Come From Algae
‘Brain Balls’ Grown From Skin Cells Spark With Electricity
Specifically, Briggs and her team wanted to look at the place in the brain where the cortex whispers to another brain region called the thalamus. One of the first steps of visual processing is a feedback loop between these two regions, which starts when light bounces into your eye. Your retina signals the thalamus, which calls out to the cortex, which in turn sends a message to the thalamus via long, neuronal tendrils. “For decades, basically, we haven't really had a great handle on the role of these kind of projections,” says Ben Scholl, a researcher at the Max Planck Florida Institute for Neuroscience.
Once those projections were inoculated with rabies, Briggs and Co. could switch them on by flashing a blue LED. Tracking the cortex and thalamus' activity with electrodes, they showed anesthetized ferrets statistically controlled TV noise and watched how visual processing changed. They saw that cortex feedback made the thalamus respond more quickly and precisely than when those feedback neurons were disengaged.
Briggs had expected an amplification in the signal, but instead saw a change in the speed and quality of signaling. And although this is just one circuit in the pathway of processing sensory information, it’s possible that interactions like this one might point to design rules that apply to other feedback loops, as well, says Scholl.
So, no, these guys aren't just messing around. Understand the way the brain works is so complicated that researchers have had to come up with complicated, highly specialized methods like these to isolate pieces of the puzzle. Maybe one day, armed with more rules for these types of circuits, researchers will be able to nail down an idea of how your brain lets some information fade, while other signals get cranked to 11. Until then, you can expect lots more weird neuroscience dispatches.
With optogenetics, the ability to restore and enhance brain function is becoming a reality.  In this World Economic Forum discussion, Nature magazine neuroscience editor I-han Chou explains how the radical method works and the ethical issues it could cause.
Officially, the online search giant Google’s mission is to “organize the world’s information and make it universally accessible and useful.” According to two new reports—one from The Wall Street Journal and one from the nonprofit, nonpartisan Campaign for Accountability’s Google Transparency Project, the company doesn’t just organize. When Google wishes it had information that’d maybe help further its policy and regulatory goals, it just pays academics under the table to gin it up.
That’s pretty evil, y’all.
The assertions in both—the Journal reporters had access to an early draft of the Google Transparency Project’s report and did even more reporting—are astounding. The Journal article contends that Google financed hundreds of papers at anywhere from $5,000 to $400,000 a pop, even at times participating in the editing process. And the researchers the company worked with often didn’t disclose the relationship.
Guys. Guys. Do computer science departments not mention the thing about not subverting academic freedom with bribes in an attempt to influence legal and regulatory frameworks?
Oh, but you’re all like, “come on, don’t be so uptight! Google funds research! It’s practically an R&D institution. Machine-learning cars that search their own balloon-powered books database at gmail dot com!”
Google has a tremendous amount of power and wealth, and they really try to leverage that to get what they want from policymakers.
Dan Stevens, Campaign for Accountability
The Google Transparency Project says no. The papers Google funded expressly supported the business, covering “a wide range of policy and legal issues of critical importance to Google’s bottom line, including antitrust, privacy, net neutrality, search neutrality, patents and copyright.”
When European and US regulators started looking hard at Google for potential antitrust violations between 2011 and 2013, the number of Google-funded papers with titles like “Google? A Monopoly? Don’t Make Me, a Credentialed Academic, Laugh” spiked. The same thing happened again in 2015. In 2013, when regulators and media companies wanted to know if Google could be held responsible for linking to pirated materials, it was all papers like “Just Because You Found Something Copyrighted on Google, That’s Just, Like, Your Opinion, Man.” OK, I made those specific titles up, but still.
Then those articles and papers themselves got linked or referenced elsewhere, further muddying the trail of money, and those entire networks of pseudo-knowledge became fodder to lobby regulators and elected officials.
You know what company is very, very good at understanding network effects?
“Google is a company with a tremendous amount of power and wealth,” says Dan Stevens, executive director of the Campaign for Accountability. “They really try to leverage that to get what they want from policymakers.” At one point, his organization’s report says, Google CEO Eric Schmidt even cited to Congress a paper saying his company wasn’t a monopoly—without disclosing that Google had paid for the paper.
Here’s the Journal again:
Google has paid professors whose papers, for instance, declared that
the collection of consumer data was a fair exchange for its free
services; that the company didn’t use its market dominance to
improperly steer users to Google’s commercial sites or its
advertisers; and that it hasn’t unfairly quashed competitors. Several
papers argued that Google’s search engine should be allowed to link to
books and other intellectual property that authors and publishers say
should be paid for.
You might remember this tactic from such betrayals of the public trust as Big Tobacco covering up the link between cigarettes and cancer, or oil companies obfuscating the link between greenhouse gas emissions and climate change. And, to be fair, Google gave up on the injunction “don’t be evil” a couple years ago.
The Google website posted a response to the Campaign for Accountability report. The company took issue with the idea that any amount of funding at any time represented an ongoing influence on a person or organization. “Our support for the principles underlying an open internet is shared by many academics and institutions who have a long history of undertaking research on these topics—across important areas like copyright, patents, and free expression. We provide support to help them undertake further research, and to raise awareness of their ideas,” the statement says. Furthermore, the company says the researchers it funds have complete editorial and intellectual independence, and they’re supposed to disclose their own financial relationships to journals, conferences, and whoever else is reading their work.
That’s tricksy. First of all, while Google both funded people who already supported the company’s positions and solicited favorable research, people who take money or gifts—of almost any size—almost always end up showing favoritism to the giver’s positions. (Physicians who take gifts from pharmaceutical companies are a great example. Members of presidents’ families who meet with Russian government lawyers might be, too.)
But the second thing—the part about disclosing financial relationships—is even sketchier. In the world of banking and finance, forcing people to admit if they’re being paid to make certain claims, or getting money from the people whom those claims support, applies a coat of teflon to a transaction. Everyone having information about everyone’s priors removes some friction from the transaction.
Outside that world, in let’s say science or public policy, it’s less clear what good a disclosure does. “In an ideal world, you’d have a lot of academics studying this stuff, being funded by a university,” Stevens says. “You want untainted research. But if that’s impossible, at least disclose.” Many academic and trade associations require disclosures of financial connections from their members. Government usually does, too, unless the heads of ethics agencies deployed to watchdog such stuff quit in a fog of mindboggle.
Google's Big EU Fine Isn't Just About the Money
Digital Privacy Is Making Antitrust Exciting Again
Google Takes on Rare Fight Against National Security Letters
But even if the researchers and academics who took Google money did disclose, nothing about that disclosure guarantees fair work, ethically conducted. If a disclosure is supposed to make a reader regard work with more skepticism, how much more? And however high a disclosure of a financial interest makes you raise an eyebrow, does it stay that high for a subsequent paper that cites the first one?
It’s hard to get academic research funded. Google has a lot of money, and funds a lot of good research. It’s a company with a lot of goodwill in the world—but imagine how this would look if, as my colleague Megan Molteni suggests, this same story global-replaced “Google” with “Monsanto.”
Google owns massive amounts of data on you and all of its other users that it does not share. And if capitalism’s answer to potential corporate malfeasance is that customers are free to take their business elsewhere, well, it’s hard to find a viable option to the world’s biggest internet search engine. If The Wall Street Journal and the Google Transparency Project are right, the company’s doing everything it can to make sure things stay that way.
Publicly traded U.S. tech companies have stashed as much as $530 billion in offshore tax havens. How’d they do it? Find out what magic tricks companies like Google, Apple, and Microsoft have up their sleeves.
Every year, 5 million people die from causes associated with one of the most mundane scourges of the modern era: sitting around. That’s like losing one Norway-sized country every 365 days to the likes of heart disease, diabetes, and bowel cancer—illnesses linked to a lack of exercise. Norwegians though, aren’t falling victim to inactivity nearly as much as elsewhere in the world. At least according to the largest human movement study ever undertaken, brought to science by the ubiquitous smartphone.
In a paper published Monday in Nature, researchers at Stanford analyzed the minute-by-minute habits of 717,527 people from 111 countries to understand how things like activity levels, gender, and location impact their weight. By dissecting data from a physical activity-tracking app, the researchers found that in countries with low obesity rates, people walked a similar amount each day. The bigger the gap between those who took steps and those who didn’t, the fatter the country—a phenomenon they call "activity inequality."
“Up until now we’ve had a very limited picture of how active people are,” said Tim Althoff, a doctoral candidate in computer science and first author on the paper. “Smartphones give us this unprecedented opportunity to better understand what people are doing all day and how that relates to their health and wellbeing.” That’s the same promise digital health devotees at Stanford and elsewhere have been making ever since the iPhone debuted. But using smartphones to study public health requires reliable data—and researchers, even at well-connected universities like Stanford, still have a hard time getting their hands on the truly good stuff.
Yesterday's study came out of Stanford’s Mobilize Center—an institution dedicated to translating America's oodles of smartphone and wearable data. It was made possible by a $12 million grant from the National Institutes of Health, as part of a 2014 initiative to form 12 top data-crunching centers around the country. Althoff and his collaborators there started with data donated by Palo Alto-based Azumio, makers of the Argus app. The company anonymized the step-counting data but provided a few key demographics: age, gender, height, and weight. The last two enabled the researchers to calculate each user’s body mass index, and from there they correlated activity levels with obesity rates.
They found some interesting results. Take the US and Mexico, for example. Americans and Mexicans take roughly the same number of steps each day—about 4,500. But in the US, those steps are distributed much more widely across the population. And that gap between the activity-rich and the activity-poor corresponds with a much higher rate of obesity. “It’s not just about individuals," says Abby King, a public health researcher at Stanford who contributed to the study. "It’s about where they live.”
King leads the center’s efforts to help people manage weight via mobile health apps, and she sees a huge opportunity to use that kind of continuous data to provide more targeted, dynamic interventions to people who are headed down a wellness dead end. “We can catch people on their way toward obesity, and provide them feedback through smartphone apps, so they can actually do something about it in the moment.”
For now, though, using smartphone-based data to build public health research and guidance is still problematic. Reason number one: Step-tracking data is actually pretty unreliable.
Wearables Could Soon Know You're Sick Before You Do
Social Networks May One Day Diagnose Disease—But at a Cost
Can Apple's ResearchKit Really Change Medical Research?
“In particular, steps that come out of commercial devices like the Apple built-in step counters are not very accurate,” says Bruce Schatz, head of Medical Information Science at the University of Illinois-Urbana Champaign. “They’re tuned for making physically active people feel good.” The issue, he says, isn’t with the measurement device. Smartphones are equipped with accelerometers that measure tiny variations in location, and they do it well.
But the handful of algorithms that Apple and other phone manufacturers and app developers employ to package that raw data into easy-to-use step counts can't accurately capture the huge variety in people's walking mechanics. They don’t have enough flexibility to account for, say, old people who shuffle instead of stride. And not all steps are created equal. Strolling in the park burns fewer calories than sprinting up stairs. Which matters for people trying to manage their weight (though not as much as what people eat). Detecting those distinctions requires raw, not pre-packaged accelerometer data. That's why Schatz, who has worked with the NIH and NSF on their population-scale mobile health initiatives, says raw is the way to go if data is going to be used for health interventions.
The downside is it’s a lot harder to work with. Most app developers don’t keep raw data themselves because the storage costs would be huge. And constantly pulling that data from your phone (think 60 times every second instead of 60 times every hour) would knock out its battery in about an hour or two. Algorithms that store inferences about what you’re doing—walking, biking, sitting—cut down all that data and save battery power. That’s the kind of information Althoff and his Stanford collaborators got from Azumio: 1,440 data points per person per day as opposed to 5 million.
That data was constrained in a less technical way, too. By only looking at the steps of people who bought iPhones and downloaded Azumio's app, the researchers limited themselves to a self-selected group—more likely to be wealthier and healthier than average. Azumio doesn't collect data on things like income and race, and while some app users do keep track of daily food logs and calorie intakes, the company didn't share those for this study. So researchers couldn't test any other hypotheses about lifestyle variations that could impact obesity other than steps. Building accurate models with which to detect, monitor, and predict obesity will require more information than most smartphones readily give up.
Getting population-scale raw accelerometer data from phone manufacturers like Apple and Google isn't impossible. It's just wildly impractical. Researchers who wanted to do it would need to either partner with a developer or build an app themselves, then get loads of people to download it despite the battery drain. Neither Apple nor Google are just giving away data pulls on the billions of phones they have circulating the globe because of its value to paying customers, like online advertisers. And that makes the best information for building accurate predictive models for public health issues like obesity, for all intents and purposes, beyond the reach of most scientists.
“Mobile data really is good enough now to be actionable,” says Schatz. “But nobody has done it except for targeted ads.” Which means that for smartphone data to be able to tackle public health problems, it may first have to become a public good.
When you're trying to meet a Fitbit threshold, but you just can't cut the mustard, we've got several cheats that will bring your count up to 10,000 steps with minimal effort.
Elon Musk is the closest thing this world has to a real-life Tony Stark. Think about it. He builds cool cars. He builds cool rockets. He builds cool tunneling machines. He wants to fire people through pneumatic tubes. He built a ginormous battery factory in the desert, and now he's building the world's largest battery.
OK, technically, Elon Musk isn't building it. Tesla is. But same difference, because Tesla is his company. And Tesla plans to build a lithium-ion battery array capable of storing 129 megawatt-hours of energy.
Wait... 129 mega-whats? What is a megawatt hour, and just what could you do with all that energy?
The most common unit for energy is the joule. If you pick a textbook up off the floor and place it on a table, you've expended about 10 joules of energy. Yes, that's an approximation. The precise figure would depend upon the mass of the book and the height you raised it.
OK, so what about a megawatt-hour? That's also a unit of energy. To understand it, let's first look at power. We define power as the rate at which you use energy.
Measuring the change in energy (ΔE) in Joules and the time interval (Δt) in seconds yields a power measured in watts. That means that power-time is a unit of energy and a watt-second is equivalent to a joule. And a watt-hour? Just do a simple unit conversion. Remember, the key to converting units is to multiply by the number one.
So a 129 megawatt-hour battery represents 4.6 x 1011 joules (where a megawatt equals one million watts). Whoa. What can you do with 460 billion joules?
Let me return to my example of lifting a book. What could I lift with 460 billion joules? Lifting something, increases its gravitational potential energy. This change in potential energy is equal to the product of the mass, the height, and the gravitational field (9.8 N/kg here on Earth).
With that in mind, how much mass could you lift to a height of 10 meters using the energy in Musk's super-battery? With a change in height of 10 meters and a total energy of 460 billion joules, I can solve for the mass—4.7 billion kilograms. Or roughly the mass of the pyramid of Giza. Imagine lifting that 10 meters. Pretty cool.
Want to work with a known mass and solve for lifting height? OK, how about a Nimitz class aircraft carrier like the Carl Vinson? Such ships have a mass of about 108 kg. Using the same energy but solving for the change in height, I get a value of 469 meters. Now, I should note that this assumes 100 percent efficiency in the battery and lifting mechanism. Reduce the efficiency to, say, 50 percent and you reduce these values by half.
Another common form of energy is the energy of motion, something we call kinetic energy.  The faster something moves, the more kinetic energy it possesses. And the greater the mass, the greater the kinetic energy. The equation looks like this:
Once again, I can pick the mass of an object and calculate the speed that I could get that object up to using the energy in the battery. How about a locomotive? They're pretty heavy, right? Let's use a locomotive mass of 100,000 kg. Given 460 billion joules, this locomotive could achieve 3,033 m/s—way faster than a bullet (assuming no air resistance). Want something a little more typical? How about a baseball with a mass of 145 g? That same amount of energy would propel it to 2.5 x 106 m/s. Whoa. Crazy fast. But nothing compared to the speed of light (3 x 108 m/s). The real question is: Why would you want to get a baseball going this fast? Who knows. Maybe you're Iron Man.
Well, not stuff. Ice. How much ice could you melt with that much energy? First, let's assume  the ice is at 0oC so we don't consume any energy heating the ice, only melting it. You need 334 joules to melt 1 gram of ice. Scientists call this the latent heat of fusion for water. So I we must do is divide the battery energy by the latent heat of fusion. That gives us an ice mass of 1.4 million kilograms. Can't picture that? Imagine a cube that measures 459 meters on each side. That is one giant ice cube.
By now, you know how to calculate this stuff, which means it's time for some homework questions. But first check out a short python program with my calculations. It will help.
Determine the mass of this giant battery. Don't just Google it. Use the density of lithium-ion batteries to calculate it.
How high could a fully charged super-battery lift itself?
How fast could the battery move itself?
You want to make coffee. Using the super-battery to boil room-temperature water, determine how many cups you can make for your friends.
What is the biggest object you could place in orbit using 460 billion joules?
How long could you use this battery to power your house, assuming your house uses 1,000 watts daily.
A human can pedal a bicycle at about 100 watts of power output. How many humans on bicycles would it take to charge the super-battery 24 hours?
How long would your phone last using this battery? How many times could you watch Iron Man on your phone?
Tesla's Gigafactory, under construction in Sparks, Nevada, will be the largest building in the world, by footprint, when it's finished. The batteries it produces are crucial to Tesla's plan to make affordable electric vehicles.
As I walk through life, I often look down at people’s shoes. No disrespect intended. I’m not trying to avoid eye contact with you. I’m paying attention to the imperfections in the world and looking for ways to help fix a few of them.
You see; shoelaces are not just shoelaces when you view them through the filter of everything all at once. They are the raw material of knots, and knots are the embodiment of mathematical beauty; mathematical beauty is a fabulously useful tool for rational problem solving; and rational problem solving is, of course, the most powerful tool for changing the world. In my Nye’s-eye view of the world, tying a well-crafted knot is like a personal promise to engage in that whole glorious process. I often have three such knots with me: two on my shoes and one around my neck in the form of my beloved bow tie.
But when I look at the knots all around me—well, it’s troubling. There’s a lot of work to be done.
Try looking down yourself, and what do you see? Around half of the people I meet tie their shoes with bow knots that are prone to coming untied from the day-to-day flexing inherent in walking. These bowknotters often compensate by tying their laces with doubled knots, piling one asymmetrical knot upon the other in a desperate bid to keep it together—or worse, they repeatedly walk with loose laces dragging. It doesn’t have to be this way. With a little more thought and attention, you can bring inspirational order to what may seem like one of the most mundane objects in your daily life. Plus, your shoes will fit better and stay tied.
Let’s start with a simple experiment we can do together, right here and right now, using only the loosened laces on your shoe. Begin by tying one of the most useful of all knots, the square knot. It’s also called a “reef knot,” as it was and is, from time to time, used to reduce the sail area of a sail on a boat, to reef the sail in a storm or strong wind.
Wrap one lace over the other, then the second lace over the first one. You may have heard the expression “right over left, left over right.” Look at that knot. It’s beautiful, symmetrical; it’s the marriage of two curves. This square or reef knot is square; I mean it’s symmetrical. It’s the basis for the knot we call a “bow.” Now, untie the second of the two wraps. You might go, “right over right, right over right” again. Please examine this knot. I hope you notice it’s not as good looking as the reef knot described above.
If you’re like me, you might at this point exclaim, “Oh, the asymmetry!” This lack of balance found in about half of all conventional shoelace knots is heartbreaking. What we want in a square or a reef knot is symmetry. Here, mathematical beauty is a means to an end. It’s more than beauty for beauty’s sake, although that ain’t bad. It’s a matter of function: A shoe tied with a reef knot will stay tied long after other, sloppier knots have come unraveled. In shoelaces, as in so much of physics, symmetry is the key to balance and stability.
When you tie a conventional bow on your shoe, check to see if its two loops, or bunny ears, lie perpendicular across your foot, left to right, or lengthwise along your foot, toe to heel. If the loops or ears come to rest in a neat left-to-right position (“athwart,” as we say at sea), that’s the way we want it. That’s symmetrical, and that arrangement will seldom come untied. This is what I call a “square bow.” If one gently pulls the loops so that the loose ends of the laces pop free, the knot that is left there underneath is the beautiful square knot. Even if you perceive your laces to be woven from slippery stuff, the squarebow knot will hold its own once it is gently but snuggly tightened. Or as the saying goes, any knot has to be properly “dressed.” (For you crossword puzzlers out there, the loop of the lace is called a “bight.” It’s pronounced just like our word “bite,” and it works wonderfully in Scrabble.) The unsymmetrical knot, on the other hand, will slip with each step. It will start to lose its shape, its integrity, and its stability the moment you start walking and put stress on it. Oh, the trauma; oh, the suffering.
As you may have inferred, I tie my bows by forming a single bight and wrapping the other end of the lace around the base of the bight. If you are among those who tie laces by finishing the knot with two loops, or “bunny ears,” it all works the same way. The bunny ears are your knot-ty-er bights. Allow me to reassure you bunny-ear, double-bight people: You can create a square bow just fine. If you tie the base overhand knot, then form your two bunny ear bights, and tie them in the opposite direction from your base overhand knot, you will produce a lovely square bow.
Bill Nye Saves the World, the Anti-Anti-Science Show, Hits Netflix in April
Bill Nye Says Climate Change Deniers Have a Bad Case of Cognitive Dissonance
Why Bill Nye Makes the Perfect Leslie Knope
Now, I loved my grandmothers. They were both remarkable people. They raised my parents, after all, and I believe anyone who met either of them would say, “That girl has plenty of common sense.” Nevertheless, the asymmetrical, not-quite-a-proper reef knot is, by long tradition, called a “granny knot.” Sorry, Nana. Sorry, Mini. We seek a square bow rather than a “granny bow.” If you have suffered lo the many years of your life with asymmetrical granny bows, you’ll find it’s a hard habit to break. But it can be done. Try this: Reverse the first wrap of your laces. Instead of going right over left, reverse that and go left over right. Then let your muscle memory take over for finishing the bow, either by wrapping individual laces or by wrapping bunny ear loops.
All this talk of shoe laces may seem like an unimportant detail of everyday life, but it is always underfoot—or literally atop foot. A shoelace knot is a metaphor for the scientific approach to problem-solving. Too many people learned to tie bow knots in their shoes and accepted that imperfect, unsymmetrical, time-consuming route rather than dig deeper for a better long-term approach. So when I wax poetic about the beauty of a square knot, it’s not only because I like showing off my sailor skills; it’s because good design should be good all the way down to the details, even when we’re talking about something fairly straightforward like tying knots. I think we should all make a habit out of expecting the best problem-solving from ourselves, and there’s no better place to start than with design problems we encounter every day. That’s where things like shoelaces work well or . . . not. (Get it? Or knot? Uh . . . sorry.)
There is another big idea in here, masquerading as a small one. Even if you have tied your laces the other way, in granny-bow fashion, for years on end, you still have a chance to change. This ongoing potential for improvement is at the heart of the scientific way of looking at the world. In politics or religion, changing your ideas can be risky or even heretical. In science, abandoning a decades-old habit in response to new information reflects a vital quality of open-mindedness. Such open-mindedness is essential for making a fundamental discovery . . . or for keeping your shoes tied.
Excerpted from Everything All At Once: How to Unleash Your Inner Nerd, Tap into Radical Curiosity and Solve Any Problem by Bill Nye. Copyright © 2017 by Bill Nye. With permission of the publisher, Rodale Books. All rights reserved.
Bill Nye uses the power of Twitter to answer some common science questions.

Check out Bill's new show on Netflix "Bill Nye Saves The World" premiering April 21st!
Planet Earth is getting hotter. One of the more confusing aspects of this global trend is the persistent, undeniable discomfort of winter. Even more confusing is when that chilly weather continues into April, May, or godforbidpleasenot June.
This might clear the confusion (but probably not the frustration): Those colder temperatures in the first half of the year might be due to warmer weather in the Arctic. Authors of a new study, published Monday in Nature Geoscience, found this trend looking at over 100 years of climate data from the Arctic and North America. This warm Arctic/cold North America connection has been particularly noticeable since 1990. And that doesn't just mean you'll be wearing a puffy jacket to Memorial Day cookouts from now on. Spring is an important time for agriculture, and the authors noted that US crop productivity declined by as much as 4 percent following warm Arctic years. Plus, those crops, along every other plant affected by the connected weather cycles, absorb less CO2—Arctic warming begets the potential for even more warming.
The Arctic is warming faster than anywhere else on the planet. This causes problems, because that big gob of cold air messes with large scale atmospheric circulations in the latitudes the US occupies. "The Arctic warming has a remote impact via atmospheric teleconnection," says Jong-Seong Kug, an environmental scientist at Pohang University of Science and Technology in South Korea, and co-author of the new paper. Atmospheric waves, induced by polar forcing, convey signals to the middle latitudes." What kind of signals? Well, the upper atmospheric waves could alter the position of key high or low pressure zones on either side of the North American continent. This would have huge consequences for how large scale weather systems travel, altering not just temperature, but precipitation, cloud cover, and a myriad of other ecosystem changes.
To make sense of how the Arctic was affecting North America, this study pulled data from a set of climate models called CMIP5. "These compile everything going on with Earth’s climate: human emissions of fossil fuels, interactions between the atmosphere and ocean, radiative forcing, clouds, and so on," says Anna Michalak, a climate scientist at the Carnegie Institution for Science in Stanford, and co-author of the new paper. CMIP5 is necessary for this kind of research because it averages out the flaws of each individual climate model—Earth is too complicated for any one of them to replicate perfectly. "Let's say you are trying to get a headcount of all the people who attended a rally, and you ask one person," says Michalak. "That person's count might be a bit off. But if you talk to 10 or 15 different people, you can get a good sense of the range of people who were there." CMIP5's operators ran the multi-model against historical data in order to vet the thing's accuracy. Other researchers analyze smaller chunks of this massive database in order to make sense of how intercontinental climate systems interact.
Climate Change Causes Extreme Weather—But Not All of It
Climate Change Is Going to Be Expensive—For Everybody
Climate Change Is Wicked Bad for New England's Cod
The new research compared observational temperature data from the Bering Sea with temperature and plant growth data from the US from 1990 to 2010. They saw a pronounced trend of generally cooler winters and springtimes in the northern US and Canada, and dry weather around Texas and neighboring states. In terms of plant growth, these weather patterns stunted about 14 percent of the aggregate US ecosystem's ability to uptake carbon dioxide.
And then there are the farms. Using historical state-level crop yield data from the National Agricultural Statistics Service of the United States Department of Agriculture, the researchers found that warmer Arctic years were associated with a 1 to 4 percent overall decline in agricultural yield across the US. But those are just the averages. Some states, like Texas, experienced as much as 20 percent decline. This kind of research could be used by farmers to plan accordingly—they could watch for warm Arctic winters, and plant later in the spring, for instance. But that's no guarantee: The authors point out that their work merely identifies a trend. Weather cycles are notoriously complex, and don’t work in simple cause/effect relationships.
Still, you probably want to know what happens when you project the model forward in time. Wanna guess? Yep, things get worse. Warmer Arctic seems to result in even colder North American, and much more damage to plants. Of course, these scientists are modelers, not oracles, and it's hard for them to model the effects exactly. "The vegetation growth does not linearly respond to the climate factors," says Kug. "For example, when  the temperature decrease about 10%, the vegetation damage does not increase by 10%. It may cause much more damages than 10%. We call this is nonlinear response." Different species will respond to cooler springs, or hotter summers, in different ways. Some might migrate, some might die off, some might kick their production into overdrive. But that's not much to warm the soul.
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.
Last August, Masahide Sasaki and his team instructed a satellite to shoot laser beams at a suburb of Tokyo. No, not like that. The laser beam, made of infrared light, was invisible to the human eye. By the time it had traveled through hundreds of miles of outer space and atmosphere, the light was harmless: It had spread out like a spotlight, about as wide as 10 soccer fields. Some of that light made its way into the end of a telescope, where it bounced off mirrors and flew through lenses and filters onto a photon-measuring detector.
Someday, Sasaki hopes, that light could be more than invisible wavelengths hitting a telescope—it could be encoded with information. Today, the radio waves beamed in satellite communications have limited bandwidth, which means they can’t transmit a lot of data at once. But if you can encode a message in infrared photons, you can transmit a million times more data per second, says Sasaki, a physicist at the National Institute of Information and Communications Technology in Japan.
For years, space scientists have proposed this kind of laser-beaming sat, which could make it possible to communicate with unmanned space rovers on faraway planets faster than radio waves allow. But laser light will die out as it travels 55 million miles to Mars—only a few photons might actually reach a receiver on a rover. So scientists first need to be able to read encoded information from a single quantum of light.
Capturing and reading individual photons from a satellite is a tough experiment that took Sasaki’s group seven years to pull off—and by then, someone else had already done it. Physicists in China published in Science last month that they’d managed an even more difficult version of the experiment, where their satellite beamed two photons to two different cities at the same time. But the Japanese group’s claim to fame, published in Nature Photonics, is that they did their experiment in a tiny satellite known as a microsatellite—a cube that weighs about 100 pounds, somewhere between the size of a microwave and a refrigerator. “The microsatellite weighs less than one-tenth of the Chinese satellite,” Sasaki says.
That weight difference also means it’s a lot cheaper to launch: you can launch a 100-pound satellite for about 2 million dollars, as opposed to hundreds of millions for larger satellites. That price point is appealing to a lot of companies. “Many companies that are not specialists in space technology can enter this new field,” Sasaki says.
Chinese Satellite Relays a Quantum Signal Between Cities
The Bizarre Quantum Test That Could Keep Your Data Secure
Physicists, Lasers, and an Airplane: Taking Aim at Quantum Cryptography
Sasaki’s group is working with a company in Japan that wants to launch a network of small sats. It wants to investigate laser communication as a technique for sending messages within its network, as well as a fringey encryption technique known as quantum cryptography to secure those messages. Sasaki won’t name the company, but it’s definitely not the only game in town: US company Planet launched 88 small satellites in February, though its focus is imaging, not communications. Japanese company Axelspace has also launched a few, with a grand plan of a network of 50. Even Canon has a 110-pounder up there right now, carrying photography system based on one of its DSLR cameras. In 10 years, Sasaki expects 4,000 of these tiny satellites will be in low Earth orbit, many of which might need secure communication technology.
All these companies are interested in launching small satellites because they’re cheap—and now that tech is finally small enough to fit on them (thanks, Moore’s law!) there’s not much holding them back. “You can actually start to do significant things in small satellites that you could only do before in a large satellite,” says Todd Harrison, a space security expert at the Center for Strategic and International Studies.
The US military might, for example, be able to use a laser-beaming sat to communicate with drones, Harrison says. Military drones take lots of high-resolution photographs and need fast, secure data transmission. So you could launch a dedicated microsatellite for downloading and delivering drone data. Laser communication, unlike radio waves beamed from conventional satellites, delivers a targeted beam, which means it’s best used in a one-on-one setting.
These small satellites could also change military satellite networks, which consist of a handful of conventional large satellites. “We’re heavily dependent on each individual satellite,” says Harrison. “To make [the network] more resilient, instead of building a small number of large satellites, you could build a large number of small satellites.” Last week, The New York Times reported that the US government was planning to launch a fleet of small satellites to watch for North Korean missile tests.
Still, Sasaki’s communications tech is far from deployment. To send a message fast, they have to be able to detect as many photons as quickly as possible, and their group could only detect about one in every hundred million photons sent from the satellite. “This time, we decided to widen the laser beam to make the experiment more feasible,” Sasaki says. “But it’s kind of an embarrassing specification.” Right now, they can’t do their experiment in the daytime because the sunlight completely drowns out their tiny signal, even with filters. They’re planning to shrink the size of the laser beam so that more of it goes in the telescope. Then maybe they can send that good morning text to Mars.
Temperature, carbon output, water usage—that's all data that a farmer could use to get better crops. And it's also available, via satellite, to the average smart phone. In this World Economic Forum discussion, Wim Bastiaanssen, a water resource engineer at Delft University, argues that this tech could someday feed the world.
When it comes to detecting new organisms that emerge from exotic places and cause global havoc, the US military is ready. The Pentagon operates infectious disease labs and surveillance networks in places like Kenya, Georgia, and Thailand, as well as a giant research center and vaccine-making unit just outside Washington, DC.
All that effort makes sense, with 200,000 US troops deployed at bases in 171 countries that can encounter a wide range of emerging biological threats. But Pentagon planners are starting to wonder what happens if the next deadly flu bug or hemorrhagic fever doesn't come from a mosquito-infested jungle or bat-crowded cave. With new gene editing tools like Crispr-Cas9, state enemies could, theoretically, create unique organisms by mixing-and-matching bits of genetic information.
As this scenario evolves from sci-fi to real-world possibility, many public health experts, biology researchers, and even the military have begun to examine possible threats, according to Christian Hassell, deputy assistant secretary of defense for chemical and biological defense. “We had people asking us, ‘How is the government responding to this? What is the threat that it poses, if any?’”
So Hassell and his colleagues at the Pentagon funded a year-long review by the National Academies of Sciences of the biodefense vulnerabilities created by synthetic biology. This week, the committee of experts held their fourth of six meetings in Washington, inviting academic scientists, biotech CEOs, and public health experts. A preliminary report outlining the scope and direction of the probe is undergoing “classified review” before being released to the public, and a final report—with recommendations—is due next year.
Those results could have implications for defensive strategies against a new type of bioweapon, potentially more difficult to identify because it resembles its “natural” counterpart. And that defense could start at home—by limiting biological research that has potentially nefarious applications. The final review will have the potential to guide regulations on federally-funded research labs.
Conflict over the need for future regulations spiked during the public portion of the meeting on Thursday—likely continuing on Friday behind closed doors. Some scientists at the meeting felt that the molecular biology community is already doing enough to monitor itself: The academic biology and DIY bio-hacking communities have voluntary codes of ethics to deter experimentation by would-be bad guys. And they fear what might happen to important genetic research if the Pentagon gets too paranoid.
Crispr Is Getting Better. Now It's Time to Ask the Hard Ethical Questions
Science Would Like Some Rules for Genome Editing, Please
Cheap DNA Sequencing Is Here. Writing DNA Is Next
They point to 2014, when the federal government halted 18 studies on so-called “gain of function” research that tinkered with viruses like MERS, SARS, and the flu to make them more likely to transmit in humans. The White House is taking another look at that moratorium to determine whether it still makes sense. Many scientists hope the ban is lifted—they argue understanding how viruses mutate is critical to stop them.
Scientists at the meeting expressed a range of ideas about how the military could best defend against biological threats. Sriram Kosuri runs a synthetic biology lab at UCLA that has developed libraries of DNA sequences that can be developed into new kinds of organisms. While he understands the possibility of a lab-engineered threat, he believes the Pentagon and federal health officials should focus on responding to emerging public health menaces rather than monitoring academic labs that use genetic manipulation tools. “There’s a legitimate threat of emerging viruses and we need to be prepared for those things,” Kosuri said during a break in the meeting. “The tiny threat of engineered viruses is miniscule compared to that.”
The Pentagon could also use the country's surveillance skills and genetic smarts to outwit biological bad guys. Howard Salis at Penn State has developed a computer program to predict what a new organism will do based on its genetic sequence. He thinks the best way to stop bad actors is at the beginning. “How do you stop someone from getting at the testing stage, or at the clinical stage of doing something bad?” Salis told the audience. “If you catch that actor trying to design the system, it's early in the process, it's easy to see what they are designing.”
For now, the threat of a hyper-lethal designer virus remains hypothetical. “This is not a tomorrow threat, it might be a tomorrow-tomorrow threat,” says Daniel Gerstein, an analyst at the Rand Corporation and former science policy advisor for the Obama administration. “I don’t think it's purely science fiction. But we have not seen a lot of terrorists looking to manipulate genome sequences.”
And even if they do, the good news (for now) is that responding to a super-charged human-made virus is pretty much the same as responding to a nasty Ebola- or Zika-like outbreak, according to Cmdr. Franca Jones, chief of global emerging infections surveillance for the Pentagon’s Defense Health Agency.
There are ways to determine whether a flu virus comes from a lab or the jungle. “We should be able to detect newly created organisms using a variety of methodology we have available, DNA sequencing being one,” says Jones. But whether it’s natural or lab-grown, public health officials will still need the resources to respond quickly to a infectious disease outbreak. “When it comes to our infrastructure to respond,” she says, “I don’t think there is much difference.”
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.
In February 2016, the leaders of the Laser Interferometer Gravitational-Wave Observatory (LIGO) announced that they had successfully detected gravitational waves, subtle ripples in the fabric of space-time that had been stirred up by the collision of two black holes. The team held a press conference in Washington to announce the landmark findings.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
They also released their data.
Now a team of independent physicists has sifted through this data, only to find what they describe as strange correlations that shouldn’t be there. The team, led by Andrew Jackson, a physicist at the Niels Bohr Institute in Copenhagen, claims that the troublesome signal could be significant enough to call the entire discovery into question. The potential effects of the unexplained correlations “could range from a minor modification of the extracted wave form to a total rejection of LIGO’s claimed [gravitational wave] discovery,” wrote Jackson in an email to Quanta. LIGO representatives say there may well be some unexplained correlations, but that they should not affect the team’s conclusions.
On June 13, 2017, Jackson and four co-authors published their criticism on the scientific preprint site arxiv.org. The paper generated considerable interest, prompting Ian Harry, a researcher at the Max Planck Institute for Gravitational Physics in Potsdam-Golm and a member of the LIGO Scientific Collaboration, to publish a public rebuttal five days later. Harry argued, in effect, that the independent team missed some subtleties in their data analysis, and that he couldn’t reproduce the claimed correlations. Jackson’s team then replied that they had found errors in Harry’s code, and that their argument stood. In an email to Quanta, Harry responded that he had corrected the typo in his code even before Jackson’s team published, and that in any case the error did not affect his analysis.
The technical issues at stake here have to do with the extreme difficulty of the measurements that LIGO attempts to make.
Gravitational waves are exceedingly faint, so to catch them LIGO was built with the ability to measure a change in distance just one-ten-thousandth the width of a proton. Lots of little bumps and vibrations can mimic a gravitational-wave signal, so LIGO uses two observatories, 3,000 kilometers apart, which operate synchronously, each double-checking the other’s observations. The noise at each detector should be completely uncorrelated—a jackhammer going off in the town near one detector won’t show up as noise in the other. Yet if a gravitational wave swoops through, it should create a similar signal in both instruments nearly simultaneously.
The main claim of Jackson’s team is that there appears to be correlated noise in the detectors at the time of the gravitational-wave signal. This might mean that, at worst, the gravitational-wave signal might not have been a true signal at all, but just louder noise.
A far more likely scenario is that the correlations in the noise, if real, point to something else. Perhaps the LIGO team subtracted the gravitational-wave signal from the raw data in such a way that it left a little correlated noise behind. Or perhaps there’s a small amount of correlation in the noise that caused the LIGO scientists to misinterpret their gravitational-wave signal. Vicky Kalogera, an astrophysicist at Northwestern University and a member of the LIGO team, said that the correlated noise, if significant, could cause a bias in the result that could “tell us potentially wrong information about the black holes” that created the gravitational waves.
But not everyone believes that the correlations are real. Harry, in his rebuttal, points out that Jackson’s team could have misused a common data-processing technique called the Fourier transform. The Fourier transform separates a data signal into a collection of simpler waveforms. The error, Harry writes, has to do with the technical assumption that the input data signal be “cyclical,” repeating itself without any breaks or discontinuities. For example, a cyclical sound wave would be the repetition of a sound clip without a pop in between each repetition. A signal that isn’t cyclical cannot be analyzed through the Fourier transform without introducing subtle errors. Otherwise, the so-called Gibbs phenomenon distorts the input signal’s frequencies, thus decreasing the accuracy of the ensuing analysis.
Since real-life data is almost never cyclical, anyone doing Fourier analysis must first execute an array of cleanup jobs on the raw data. “It looks like some of the results [of Jackson’s team] had to do with not pre-filtering the data before taking the Fourier transform,” said David Shoemaker, a physicist at the Massachusetts Institute of Technology and spokesperson for the LIGO Scientific Collaboration, echoing Harry’s public analysis.
Jackson, who declined to be interviewed for this article, writing in an email that “public polemics tend to harden positions and do not advance the desired end,” disputes this characterization. “We are aware of these issues. We neither agree with nor accept Harry’s views,” he wrote. Jackson’s four co-authors did not respond to Quanta’s requests for comments.
Colliding Black Holes Tell the New Story of Stars
Mining Black Hole Collisions for Hints of the Elusive Axion
This Dark Matter Theory Could Solve a Celestial Conundrum
For now, confidence is high in LIGO’s conclusions. “The only persons qualified to analyze this paper are in the LIGO Scientific Collaboration,” said Robert Wagoner, a theoretical physicist at Stanford University who is not affiliated with LIGO. “They are the only ones who have had access to the raw data.” Steinn Sigurðsson, an astrophysicist at Pennsylvania State University who is also not affiliated with either team, agrees. “For now, I’d definitely go with the LIGO people,” he said. “It is very rare for outsiders to find major errors in a large collaboration.”
Nevertheless, “it’s going to take longer than people would like” to get these issues resolved, said Sigurðsson. “It’s going to take months.”
The LIGO team later reported that they had found gravitational waves from a second black-hole merger, then a third. Jackson and his colleagues have not yet published any analysis of these events.
What of the controversy, then? “There is no drama here,” Kalogera said. “It’s science as usual. … Healthy, positive communication is very much welcome amongst scientists.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
For the first time, scientists have confirmed detection of gravitational waves. The finding not only validates Einstein's theory of relativity but also opens a new window on our knowledge of the universe.
Five years ago, Daniel MacArthur set out to build a massive library of human gene sequences—one of the biggest ever. The 60,706 raw sequences, collected from colleagues all over the globe, took up a petabyte of memory. It was the kind of flashy, blockbuster project that would secure MacArthur a coveted spot in one of science’s top three journals, launching his new lab at the Broad Institute into the scientific spotlight. But before all that happened, he did something that counted as an act of radicalism in the world of biology: He put it on the internet.
Posting scientific papers online before peer review—in so-called preprint archives—isn’t a new idea. Physicists have been publishing their work this way, free to the public, for decades. But for biologists, preprints are uncharted territory. And that territory is rapidly expanding as academia and its big-time funders shift toward a culture of openness. As preprints become more popular, they’re throwing the field into a state of uncertainty.
Science usually goes like this: Researcher runs experiment, researcher analyzes data, researcher writes up results. In high school biology, the process stops there. But in real life, that’s when the real slog starts. Researchers submit their results to the most prestigious journal they think might publish them ... and then they wait. If the paper is rejected, they try another journal. Then they wait again. Once they get accepted, they go through a cycle of peer review, responding to critiques from an anonymous group of colleagues. On average, it takes biomedical researchers eight months to go from submission to publication, but sometimes it takes up to three years. All the while, scientific progress—the sequential building of knowledge, based on the work of others—gets held up.
That slow, rigorous process leaves academic publishing houses—including big names like Springer Nature, Wiley, and Elsevier—with control over the flow of scientific knowledge. By selling that knowledge back to universities, academics, and the public in pricey subscriptions and per-article fees, the global industry brings in more than $24 billion in revenue every year. But since the early 2000s, scientists and powerful funders like the Gates Foundation, the Ford Foundation, and the Wellcome Trust have championed alternatives to subscription publishing. Grant-givers want to stretch the public impact of their research dollars, which means knocking down pricey paywalls. And researchers want to break out of the brand-name journal merry-go-round, whose incentives, they believe, are distorting the quality of modern science.
“This is a tipping point in biology. It’s a cultural choice, not a technological question.”
Stephen Quake
Preprints could solve these issues by decoupling distribution of results from their certification via peer review. But publishers and some scientists worry preprints will only further dilute the research literature and endanger fields already struggling with reproducibility failures. And since preprints also threaten to dilute revenues at academic publishing houses, there’s more than just scientific integrity at stake.
Daniel MacArthur, like most scientists trying out the preprint scheme, didn’t totally abandon the traditional scientific publication track. His human exome reference library was eventually published in Nature, and would go on to be cited more than 800 times. But because he posted both the dataset and the preprint explaining it more than nine months before the peer-reviewed version came out, other scientists didn’t have to wait to start using his data. Between October 2015 and August 2016, scientists viewed his newly compiled exome data 3 million times and downloaded the preprint more than 18,000 times. Together, they helped researchers launch new investigations into the genetic factors underlying diseases like schizophrenia, Alzheimer’s, and cancer.
This, then, is the two-fold promise of preprints: Scientists get to demonstrate their scholarly contributions to potential funders while their manuscripts are being peer-reviewed for publication. And at the same time, the scientific community gets to see that work months or even years before they would otherwise. Just how quickly could preprints speed up scientific discovery? According to Stanford bioengineer Stephen Quake, if one preprint inspired the work of just two other people, biologists would see a five-fold acceleration in scientific progress within a decade.
Quake’s interest in cranking up to quintuple-time isn’t just hypothetical. He’s working on one of the more ambitious biological projects of the 21st century—cataloguing every cell in the human body. In September, Quake was named co-president of the Chan Zuckerberg BioHub, a new $600 million center funded by Silicon Valley’s couple-in-chief. The BioHub’s premier project is its work on a first draft of the Human Cell Atlas, an international effort1 made possible in part by inventions (from Quake and others) that let scientists study individual cells on chips. To keep innovations like those flowing, Quake is requiring all of the BioHub’s 47 investigators to post preprints if they’re going to submit to a peer-reviewed journal. “This is a tipping point in biology,” Quake told a crowd at Stanford’s Big Data in Biomedicine Conference in May. “It’s a cultural choice, not a technological question.”
Whether or not Quake’s estimate is correct, he is right about one thing: Biology is at a tipping point. Depending who you talk to, it’s either in the middle of a populist revolution or an existential crisis. In the past year, the popularity of biology preprints has taken off like a SpaceX Falcon 9. But they still only represent 1 percent of all scholarly work in the biomedical fields. In comparison, the preprint server for physicists and mathematicians today hosts 1,275,427 papers—about 70 percent of their academic canon.
For physicists, preprints have been the default method of sharing new work since the ‘90s, when the high energy particle folks first started bringing mimeographed copies of their submitted articles to physics conferences. That tradition eventually yielded a central repository that lived on the internet: arXiv.org.
Biology’s nascent network is more fractured. There are currently seven active servers for biology preprints, depending on how you define them, with more showing up all the time. Without clear standards or expectations, scientists usually just choose whichever one they’re most familiar with.
Increasingly though, the go-to server is one called bioRxiv (pronounced bio-archive). In April, the Chan Zuckerberg Initiative agreed to a multi-year funding package—terms of which have not been disclosed—to solidify the future of bioRxiv. The money and engineering resources will also be used to bulk up the server’s automated tools for text mining, to make the repository’s content more accessible to researchers and easier to analyze with a machine. In addition to providing a digital home for preprints, scientists can also submit their work directly from bioRxiv to more than 100 peer-reviewed journals with just a few clicks.
Richard Sever, a molecular biologist at Cold Spring Harbor Laboratory, co-founded bioRxiv in 2013. When it first started, scientists submitted about 50 papers each month. Most of them were genomics researchers and bioinformaticians—people who used to be physicists. They had switched professions only to find no preprint server, or culture of sharing, to match their previous experience. They were the earliest adopters of Sever’s new repository.
But lately, they’ve been joined by others. The fastest growing adopters are now neuroscientists, also known for their physics and computational backgrounds. “I suspect we’ll see lots of separate waves as new fields start to catch on,” says Sever. The topics are already a lot more diverse than when Sever started; back then, there were so many papers about Crispr, he joked they should rebrand as a journal devoted to the new gene editing technique. “Crispr is a perfect example of why preprints are needed,” he says. “Things are just happening so fast.” In the last year, bioRxiv has grown exponentially; as of March, it hit 1,000 new papers added per month.
That rapid expansion is creating serious friction. One of the concerns with preprints is that scientists will sacrifice accuracy for speed—that in the rush to be first on the scientific record, they’ll wind up filling the internet with crap. Traditional peer review is supposed to catch mistakes and make sure a paper’s scientific reasoning is sound, and uploading a virgin paper means people will see work that could be wrong. But that’s kind of the whole point of preprints: It allows an entire field to weigh in, in public, instead of an anonymous few talking in a vacuum. Sever thinks it will actually make science more rigorous, allowing peer-reviewers to glean broader insights from a paper’s public trial.
And, as UC Denver research librarian Jeffrey Beall points out, there’s already plenty of crap science out there—much of it a result of another, earlier attempt to fix the problems of academic publishing. In the early '90s, so-called open access journals started to make scientific research free to anyone with working WiFi by shifting costs to scientists, who pay an upfront fee to cover editing. But it’s not a perfect solution: Some “pay-for-play” journals prey on novice scientists, charging exorbitant fees to publish junk papers without careful review. Beall calls these predatory publishers “the biggest threat to science since the Inquisition.” From 2012 to 2017, Beall maintained a blacklist of journals with dubious publishing practices, serving as a resource for scientists, journalists, and hiring committees. But in January, after five years, he was forced to take it down, as his university came under pressure from many of the journals he targeted.
Now, the preprint may succeed where open access journals have floundered. Beall is optimistic: “It’s a little more chaotic, but it doesn’t involve the exchange of money, which is the root of all evil in scholarly publishing,” he says. “I think preprint servers could be a way to make predatory publishers obsolete.”
And so far, the preprint has a pretty good track record. Right now, about 60 percent of the articles on bioRxiv go on to be published in a peer-reviewed journal. Look at someone like George Church, the decorated Harvard geneticist who was an early entrant to bioRxiv (and one of those people populating it with Crispr studies). Starting in 2014, he has posted 28 papers to the preprint server. Of those, 13 went on to be published in peer-reviewed journals like Nature Methods and Science Advances. The other 15 have not, but more than half of those were just added in the first six months of 2017.
Of course, Church is hardly the norm. A well-published—nay, famous—scientist like George Church has a much easier time choosing to preprint than biologists early in their careers. There are risks to publishing online before peer review: Scientists may not acknowledge preprints as establishing priority of discovery. Peer-reviewed journals could reject a manuscript if it has previously appeared as preprint. And opening up the forum could also devolve the quality of the discussion. There’s already an app, developed by biostatisticians at Johns Hopkins, that allows people to swipe right on bioRxiv papers they like. Its creators say the “Tinder for pre-prints” is just for fun, but they do hope to learn from it how scientists value different kinds of work.
It wouldn’t be the first time that people used an online platform in weird ways its creators never intended—Twitter bots pushing fake news and Facebook groups sharing revenge porn. It’s impossible to know how people will use new tools before they actually use them. Are preprints a first step toward publication in a peer-reviewed journal, a working document, or something else entirely? In the absence of consensus, the rules around preprints are as varied as the biologists that publish them.
You may be wondering why scientists would even bother to publish in journals after they’ve posted a preprint—a system intentionally built to subvert the bottleneck of peer-reviewed publication. But the system of academic publishing and all the rewards built into it haven’t disappeared. Which means for now at least, biology careers aren’t made on bioRxiv. Traditional journals still hold the key to postdoc positions, tenure lines, and lab funding.
Some of those publications make no attempt to hide their disdain for preprints. Which forces scientists to choose between sharing their work openly and keeping it offline to give it a shot at a classy publication. The Proceedings of the National Academy of Sciences won’t take papers that appear as preprints if they have a Creative Commons License, which about 70 percent of bioRxiv papers do. On the other end of the spectrum, the open access journal PLoS Genetics actually sends its editors to scour bioRxiv and other preprint servers to look for papers to publish.
But perhaps no publisher better encapsulates the upheaval than Cell Press. Which is appropriate, given that its namesake journal was the first to propagate the idea that where you published mattered more than what you published. In 1974, the Massachusetts Institute of Technology launched Cell to showcase the newly emerging field of molecular biology. At that time, the norm was for scientists to submit to the journal that matched its subject matter best, and for editors to publish any research that could pass peer-review. But Cell’s first editor, a young biologist named Ben Lewin, treated his new journal like an exclusive club, rejecting far more papers than he published. He basically invented prestige publishing.
Within a few years, other titles like Nature and Science followed suit, jumping to the top of the newly established ranking system known as the “impact factor.” It does things like measure how many citations a paper gets and in what kinds of journals those citations appear. Now commonly accepted as the currency of scientific prestige, researchers who publish in “high-impact” journals are more likely to get job offers, grant money, and attention from the mainstream media.
Now with 30 high-impact journals to its name, Cell Press is one of those publishers that can make or break a career. It’s also had a rapidly shifting relationship with the preprint process. Before September of last year, Cell Press’s official policy required scientists who were planning on submitting to a journal to consult an editor before submitting a preprint. Unofficially, some researchers were told they couldn’t post a preprint until after they’d submitted to a Cell journal, some were told it was at the editor’s discretion, and some were discouraged from putting it up at all. This confused scientists and upset advocates for open publishing.
A Rainbow Unicorn Wants to Transform Biology Publishing
Social Science Is Busted. But the NIH Has a Plan that Could Fix It
John Arnold Made a Fortune at Enron. Now He's Declared War on Bad Science
Fighting the blowback, Cell Press updated its language to clarify that any papers previously posted on a preprint server would be considered for publication, and the talking to an editor was just encouraged, not required. But still, scientists weren’t convinced that Cell titles were fully on board with preprints.
In March of this year, things got even murkier when the publisher launched its own early stage platform. Called “Sneak Peek,” it’s not exactly a preprint server (because scientists can only post if Cell Press has accepted their manuscript), and it’s not exactly open access (because you need to register for free to see them). But it does allow scientists to share work ahead of peer review and publication and brag about their high-profile placement at the same time. “Many scientists are concerned about the impact of endorsing and disseminating non-peer reviewed data, others champion speed and decentralization over quality control,” says Emilie Marcus, Cell Press’ CEO and the editor-in-chief of Cell. “Both perspectives hold merit and the challenge is to find a way forward that respects and sustains them both.”
In Valleyspeak, Cell is attempting to disrupt its disruptor. As many scientists have pointed out, it looks like the for-profit company is making moves to undercut a nonprofit model for open information sharing. There’s no real money in preprint servers—Nature Publishing Group tried it a decade ago (before it was Springer Nature)—and shuttered the service after five years. All the other repositories out there are sustained by some combination of grants, donations, and support from academic institutions. But if every name brand journal started hosting their own “sneak peek,” it would further fracture efforts to get all biological preprints in one place with one set of rules.
For more than a year, a scientist-driven initiative to promote preprints in biology, called ASAPbio, has been working with funders like the National Science Foundation, the National Institutes of Health, and the Gates Foundation to develop plans for one central aggregation site—along with bylaws and a community-elected body to govern it. But those conversations were put on hold in April, following the news of the partnership between Chan Zuckerberg Initiative and bioRxiv. Not wanting to duplicate or compete with its efforts, ASAPbio is now re-evaluating its roadmap to a centralized service.
It’s still too early to say whether bioRxiv will emerge as the one preprint server to rule them all, or if it will become just a part of a grander plan. But if it’s up to researchers like Daniel MacArthur, posting your work online won’t always be an act of rebellion. Someday, it’ll just be science as usual. “I don’t think we’re looking at a world where professional journals magically disappear,” he says. “But the publishing business model will no longer depend on being the sole gatekeepers of access to scientific communication.”
For now, MacArthur has a foot in two worlds. In the field of large-scale genetics, preprints have already become the default. But many of his clinical biology colleagues are still wary. So he’s trying to lead the way by example. On a webpage of Massachusetts General Hospital’s Analytic and Translational Genetics Unit, MacArthur and the unit’s other core faculty members have signed a pledge. They promise to deposit every manuscript from their labs to preprint servers like bioRxiv when they submit to a journal. “We believe that it is only a matter of time before the concept of restricted access to the products of scientific research becomes an anachronism,” they wrote. “And we hope that the human genetics and genomics community can play a leading role in the transition to more enlightened models.”
At the bottom, there’s a place where other scientists can add their names and labs, swear their own oaths of openness. Right now the list isn’t very long. But it’s only getting longer.
Correction appended [7:55PM PST 7/11/17]: A previous version of this story incorrectly identified Stephen Quake as the leader of the Human Cell Atlas Project, which is an international effort co-chaired by researchers at the Broad and Wellcome Trust Sanger Institutes. The article has been updated to clarify Quake's involvement with the project.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.