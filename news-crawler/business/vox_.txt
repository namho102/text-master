
It seems like the United States economy is enjoying more innovation than ever before. At the same time, statistics show the economy suffering from its slowest growth in decades.
Analysts often try to resolve this by arguing either that conventional statistics aren’t properly measuring the value of innovation or else that the apparent speed-up in innovation is actually an illusion and progress is slowing down. Another possibility is that things are exactly as they seem: Rapid technological innovation is real, and so is slow economic growth. In fact, in a sense the innovation is causing the slow growth.
Call it the productivity paradox, and recognize that it explains a lot about the current state and the future direction of the American economy.
Most of the progress in recent decades has involved making cheaper and more convenient versions of products that already existed. Phone calls and photographs, for example, have been around for a long time, but over the past 20 years we’ve seen a revolutionary decline in the cost and massive increase in the convenience of snapping photos or making long-distance calls.
As innovation has pushed down the cost of certain types of products (mostly durable goods such as televisions, furniture, and clothing), Americans have used the savings to spend more on other things — especially education, health care, child care, and housing — where productivity growth has been much slower.
Over time, low-productivity sectors have become a larger share of the economy, while high-productivity goods production has become a smaller share. And an economy dominated by industries with low productivity growth is going to grow slowly.
Slow growth sounds bad, but the future implied by the productivity paradox isn’t actually so terrible. It means that in the future a small minority of people will produce the world’s material goods and automated services, while the rest of us are focused on providing personalized services to each other. It’s a future of material abundance and plentiful jobs.
Indeed, one way to think about it is that middle-class Americans are getting close to enjoying as much material comfort and convenience as it’s possible for any society to provide for ordinary people. Accumulating even more stuff isn’t going to make us much happier, so we’re devoting more and more of our incomes to personal services that don’t see rapid productivity growth but do a lot to make our lives better.
Two things can happen when a given industry enjoys soaring productivity — it can expand, as new production techniques lead to a surge in output and consumption, or it can shrink, as a smaller and smaller number of people are needed to serve a fixed market. The history of the textile industry provides examples of both dynamics.
Americans started to mechanize the production of cloth in 1814. Productivity per worker steadily improved, and at first these gains powered a rapid expansion of the textile industry. People in the 19th century owned very few clothes, so as cloth got cheaper, people bought more.
But by the 1950s, this process had reversed. People already had plenty of clothes, so as prices continued to fall, people just spent less on clothing and pocketed the savings. As a result, apparel spending as a share of the typical household’s income has fallen steadily for the past 60 years.
Agriculture tells a similar tale. In 1900, farming was the biggest industry in America, employing about 40 percent of workers. A century later, farms were more productive than ever, but they employed less than 2 percent of the workforce. And because people only need so many calories, spending on food has plummeted.
In the mid-20th century, consumers took money they were saving from cheaper food and clothing and used it to buy a wide range of other manufactured goods that had recently been introduced: telephones, electric lighting, cars, radios, washing machines, refrigerators, televisions, air conditioners, and so forth.
Most of these products experienced a life cycle similar to the one I described for clothing. Cars went from nonexistent to a rare luxury item to a mass-market consumer product that employed a vast army of factory workers. The same happened for vacuum cleaners, telephones, refrigerators, and other household applies. But eventually, we reached a point where almost every home had a refrigerator and there was little room left for quality improvements. At that point, further productivity gains in manufacturing mostly meant that refrigerators got cheaper and consumers spent less on them.
This process can take a few decades, which means the many significant inventions between 1900 and 1940 allowed the manufacturing sector to continue growing through the 1970s.
The past few decades have been different, as economist Robert Gordon has argued. The list of major new inventions in the past 40 years is pretty short, and it’s dominated by computing gadgets — PCs, gaming consoles, DVD players, smartphones. In most other areas of our lives, Americans largely buy the same things we bought 20, 40, and even 60 years ago.
Innovation in manufacturing hasn’t stopped. American factories now produce about twice as much per worker as they did in the 1980s. But we’ve mostly gotten cheaper goods or modest quality improvements — not the invention of major new product categories. As a result, the manufacturing sector is winning a smaller and smaller share of consumer spending.
And as the chart above shows, this isn’t just an American phenomenon — and it’s not primarily about jobs moving overseas. Manufacturing’s share of the global economy has been shrinking, just as it has in the US.
When thinking about manufacturing’s declining share of the economy, it’s easy to think there must be something wrong with the manufacturing sector. But the truth is closer to the opposite: Manufacturing industries are victims of their own success. In recent decades, the manufacturing sector has consistently enjoyed higher productivity growth than the economy as a whole. Manufacturing is shrinking relative to the broader economy precisely because it has continued to get more productive even as demand for manufactured goods plateaued. That’s the productivity paradox.
With few new manufactured goods to spend money on, consumers have devoted more and more of their income to industries where productivity growth is slow or non-existent. These tend to fall into two big categories. Some industries, such as health care, education, and child care, suffer from low productivity growth because they are labor-intensive and difficult to automate. Others, notably housing in affluent areas, have become more and more expensive due to natural and legal limits on supply.
So to deliver high growth rates over the coming decades, one of two things would have to happen. One possibility is a series of major new inventions — perhaps flying cars, space tourism, holodecks, or nanorobots that cure cancer — big enough to entice consumer dollars away from low-productivity service industries. This seems unlikely to me — it would take some really impressive breakthroughs to reverse a 50-year trend — but it’s impossible to rule it out.
The more likely option is to figure out ways to automate industries that are labor-intensive now. An obvious example would be taxi and truck drivers being replaced by self-driving vehicles. If this kind of thing happened across a range of other industries, economic growth would soar. And many people think artificial intelligence software is about to make that happen.
I think that’s unlikely no matter how sophisticated AI software gets. The reason is that the productivity paradox operates within industries as well as among them. To see how this works, consider the case of coffee shops
In 2012, the Wall Street Journal reported that “Starbucks baristas are being told to stop making multiple drinks at the same time” as a result of “customer complaints that the Seattle-based coffee chain has reduced the fine art of coffee making to a mechanized process with all the romance of an assembly line.”
A Starbucks barista in Minnesota griped that the new rules had "doubled the amount of time it takes to make drinks in some cases."
People don't go to Starbucks simply to get a cup of coffee — after all, there are lots of cheaper and faster ways to get a cup of joe. People go to Starbucks because they want a cup of coffee and the “romance” that comes from getting personal service from a human being.
This means that even if Starbucks invented a vending machine or robot that could make and sell coffee as well as a human barista, it wouldn’t make sense for Starbucks to lay off its human workers. Baristas aren’t just an expensive way for people to get the coffee they want; they’re essential to Starbucks’s strategy for distinguishing itself from lower-cost options like making coffee at home or the office or buying it from McDonald’s or Dunkin’ Donuts.
A lot of other service industries work the same way:
Automation treats human labor as a cost to be reduced or eliminated. But this attitude misunderstands the value of the human workers in these industries. The opportunity to interact with other human beings is a big selling point for fancy restaurants, farmers markets, and in-person fitness classes.
If we ever figure out how to automate aspects of education, health care, or other major labor-intensive industries, something similar is likely to happen.
If people develop online educational technology that works better than traditional lectures, tests, and so forth, that could be offered as a separate product for frugal students. But parents who can afford it are likely to prefer traditional universities. Traditional universities can always adopt educational technology for aspects of the college experience where it’s really better. But they can also offer personalized services  — like faculty mentoring and face-to-face interactions with other students — that no online service can offer. So online learning will always be a cut-rate alternative to a four-year university, just as an exercise video or app is a downscale alternative to a fitness class or personal trainer.
Similarly, we may eventually invent software that can diagnose medical problems as well as a human doctor, and that would provide a low-cost option for people who can’t get time with a human doctor. But doctors do a lot more than diagnose diseases — they perform physical examinations and surgeries, answer patient questions, coordinate patient care, and so forth. So people who can afford it will prefer to talk to a human doctor — who may integrate the latest diagnostic software into their practice — just as most people prefer a tax accountant over TurboTax.
That’s what I mean when I say the productivity paradox works within industries as well as among them. As society gets wealthier and manufactured goods get more affordable, people spend a larger and larger share of their income on upscale, labor-intensive alternatives within any given industry.
An important consequence of the productivity paradox is what it does to prices and wages. When a particular industry gets more productive, it tends to benefit both workers in the industry (who may get raises) and customers (who enjoy price cuts).
You might expect the converse to hold for low-productivity industries: that if productivity doesn’t improve, then wages and prices won’t change either. But that’s not how it works.
If you run a small-town restaurant that pays waiters $10 per hour and the factory in town announces it’s giving entry-level workers a raise from $11 to $13 per hour, you’re going to have to pay your waiters more too or risk having them quit for factory jobs. And because they won’t be getting any more work done than before, this likely means you’ll have to raise your prices.
This is a general economic principle: When some industries enjoy high productivity growth, industries with slower productivity growth tend to raise wages and, therefore, prices. A barber today can perform about as many haircuts as his predecessors 100 years ago, but barbers today make a lot more money than barbers did a century ago. As a consequence, haircuts cost a lot more, in inflation-adjusted terms, than they did a century ago.
In the economics world, this is known as Baumol’s cost disease. It’s named after William Baumol, the economist who first described the phenomenon in the 1960s. Baumol was trying to explain why performing arts institutions kept getting more expensive to run (he observed that playing a string quartet took exactly the same amount of labor as it had in the 19th century), but the principle he identified applies quite broadly:
This chart shows how prices in various industries have changed since 1978 — relative to the overall price level and adjusted for quality improvements. You can see that manufactured goods like cars, clothing, furniture, and toys have steadily gotten cheaper.
Meanwhile, medical care and college tuition have been afflicted with Baumol’s cost disease, as hospitals and schools have had to pay more to attract skilled workers to be doctors, nurses, professors, administrators, and so forth. The trend lines look similar for other service industries, including veterinary services and child care — costs have soared in recent decades.
Still, the name “Baumol’s cost disease” is unfortunate, because there are actually two sides to this coin. From the customer’s perspective, rising prices amount to a troubling “cost disease.” But if you work in a service job with low productivity growth, you’ll be happy about the phenomenon Baumol described: that workers in low-productivity-growth industries tend to get raises whenever their peers in high-productivity-growth industries do. We might call it “Baumol’s wage bonus.”
Baumol’s wage bonus is a big reason why we shouldn’t be alarmed by the prospect of more and more of our economy — and, therefore, our jobs — being focused on providing personal services. Many people believe that because services workers like teachers, nurses, barbers, and police officers tend not to become more productive over time, they will inevitably lag further and further behind manufacturing jobs in terms of pay.
But that’s wrong. Baumol’s work showed why it’s wrong in theory, and the data shows that it’s wrong in practice:
Until 2006, workers in the manufacturing sector did make a bit more than workers in the service sector. But in the past decade, the trend has actually reversed, with manufacturing wages lagging a bit behind service sector wages. But regardless of which sector is ahead at any particular point in time, the more important point is that manufacturing wages are unlikely to dramatically diverge from wages in the service sector. The reason is simple: If a persistent gap opened up, young workers entering the workforce would flood into the higher-paying sector until they were brought back into balance.
People often dismiss service sector work as burger flipping, but that’s a mistake for two reasons. The service sector isn’t limited to low-paying restaurant and retail jobs. Doctors, college professors, financial advisors, real estate agents, and the like are all in the personal service sector. The service sector offers work up and down the wage scale, just as the manufacturing sector does.
But the more important point is that factory jobs used to be awful. That changed slowly and painfully over the course of the late 19th and early 20th century as society developed institutions like labor unions that helped ensure ordinary workers were treated fairly.
There’s probably nothing we can do to stop the growth of service sector jobs. What we should be doing instead is taking this shift seriously and thinking more about how to make service sector jobs better. Depending on your politics, you might think that would mean stricter enforcement of labor laws, stronger union organizing, lowering of barriers to entrepreneurship, better worker training, etc. A serious debate about those alternatives would be a lot more productive than complaining about the decline of manufacturing — a long-term trend that can’t and shouldn’t be reversed.
At this point, it should also be clear why I think people are mistaken when they predict that automation will lead to a jobless future. Automation will certainly eliminate many jobs, just as it has done for the past 200 years. And some economists worry that the premature decline of manufacturing in developing countries will stunt their long-term growth. But a wealthy society has a basically unlimited demand for workers to provide personal services.
Most parents would like to send their young kids to day care options with fewer children per adult, and their older children to schools with smaller class sizes. People would like to provide their elderly parents with better elder care services with more human interaction.
People would like their doctors to have more time to talk to them. They’d like to go out to more fancy dinners and take vacations at fancier resorts. They’d like to have personal fitness instructors and life coaches. They’d like to go to more concerts, plays, and comedy clubs. They’d like to have people renovate their kitchens and bathrooms.
Demand for these services will always outstrip supply because each worker only has about 2,000 hours of work to offer to the market each year, and there’s a lot more than 2,000 hours of work each of us would like to have other people do for us. Most of us can’t afford all the human services we’d like to consume, so we buy a Roomba instead of hiring cleaners, buy frozen dinners instead of eating out, and so forth. But if automation made us richer, we’d spend more on these services and employ more people as a result.
In 1930, the economist John Maynard Keynes wrote a famous essay called “Economic Possibilities for our Grandchildren,” in which he speculated that the workweek could continue falling to 15 hours over the next century.
It doesn’t look like that’s going to happen, and our demand for personal services helps to explain why. Americans with above-average incomes could work a lot less and still support their families. A blogger named Mr. Money Mustache brags about how he retired at age 30 after living frugally as a software engineer during his 20s. Lots more people could do this if they really wanted to.
But most of us don’t want to. We’d rather work more and enjoy more luxuries. And while luxuries can take a variety of forms — with expensive housing being a big one in coastal cities — the most expensive ones are increasingly the ones that are the most labor-intensive.
So I think the future will look like the present — but even more so. A tiny minority of the population will produce the world’s clothing, smartphones, cars, household appliances, and other material goods. Almost everyone will work providing each other with personalized services — and these services will consume a growing share of our incomes.
When word leaked on Friday that AT&T, the wireless carrier that also owns Direct TV and the smallish U-Verse wireline fiber optic service, was preparing to purchase Time Warner, shares of the telecom utility immediately plummeted and those of the media conglomerate soared. In subsequent days, analysts have put forward various versions of the story from insiders in both companies as to why this is a good idea.
But it’s worth taking that initial market reaction seriously. The combined company, if it comes together, may well prove to be a well-managed and profitable conglomerate. But in order to purchase Time Warner, AT&T and its shareholders are going to have to pay a premium over the current price of its stock.
But there’s no reason to believe Time Warner’s shares are undervalued. There’s also no reason to believe useful synergies will flow from combining Time Warner’s portfolio of television and movie content with AT&T’s portfolio of cell towers, satellites, and fiber optic cables.
AT&T’s board and management, in other words, appear to be simply wasting AT&T’s shareholders’ money. What’s in it for them isn’t so much the opportunity to build a great new business as to break out of the dreary reality that their current business is boring. As the leaders of larger conglomerate, they’ll be able to pay themselves higher salaries and hang out with movie stars.
People consume video content over various “pipes” that AT&T controls, and Time Warner either producers or owns the rights to a lot of video content. In theory, you certainly could combine these businesses. Right now, for example, many NBA games are exclusively available on Time Warner’s TNT network, which is only available to cable subscribers. A merged company could let cord-cutting AT&T wireless users stream those games without buying cable, enticing basketball fans to switch from Verizon or Sprint. Or HBO Go subscriptions could be sold at a discount as part of a “bundle” with AT&T telecom services.
One problem here is it’s not clear that the economics of this kind of integration pencil out. Nothing is stopping Time Warner’s constituent elements from striking these kinds of deals with today’s wireless companies. The deals don’t get done because there aren’t mutually advantageous terms. Combining the two companies into one could force the component elements to come to terms, but those arrangements wouldn’t necessarily result in higher profits — they would just shift gains from one subsidiary to another.
But even if some of these ideas do make sense, the government probably won’t let them happen.
We have a clear precedent from back in 2010 when Comcast, the gigantic cable company, bought NBC Universal, a media conglomerate similar to Time Warner (and also an investor in Vox Media, which owns this website). When the takeover plan was announced, the merging companies had all kinds of grand plans for bringing content and infrastructure together. That raised huge red flags for regulators who worried that Comcast was seeking to use its strength in the relatively low-competition infrastructure industry to gain leverage in the content industry.
Consequently, the government agreed to approve the merger only on condition that Comcast and NBC Universal disavow making any special deals with each other that were not available to other companies. The approval notice from regulators required the creation of “an improved commercial arbitration process for resolving disputes about prices, terms, and conditions for licensing Comcast-NBCU’s video programming” and “requir[ed] Comcast-NBCU to make available through this process its cable channels in addition to broadcast and regional sports network programming,” among other things.
This was considered, at the time, a relatively lenient settlement, and since then the regulatory environment has gotten more — not less — skeptical of this kind of merger. The Obama administration is widely perceived to have become more skeptical of mergers in the intervening years. Hillary Clinton has promised to step up antitrust enforcement relative to where it is today. And Donald Trump outright promised to block the deal during a Saturday speech.
The AT&T/Time Warner proposed merger will be a really good test to see if Clinton plans to live up to her rhetoric on antitrust.
We don’t know if the deal will ultimately be approved. AT&T badly misjudged the regulatory climate back in 2011 when it tried to buy T-Mobile only to have it disallowed by the Federal Communications Commission and Justice Department. So this could prove to be another overreach. But if regulators do approve the terms, they will surely insist on Comcast-esque terms that vitiate the ostensible business rationale for the deal.
Why did Comcast go through with the deal even after regulators axed the supposed synergies it was supposed to create? The cynical answer is that Comcast is confident it will find a way to cheat. The even more cynical answer is that Comcast’s executives don’t really care. Being a cable company is a great business, but it’s boring and unglamorous, plus everyone hates you.
The executives of a media company, by contrast, get to hang out with A-list celebrities and star athletes.
They also get paid very well. As the Associated Press reported last year, “six of the 10 highest-paid CEOs last year worked in the media industry.”
None of the top 10, by contrast, was the head of a utility company. Well, except for Brian Roberts, the CEO of Comcast, who counts for the purposes of AP’s list-making as a “media industry” CEO because his cable company also owns NBC Universal.
All of which is to say that AT&T CEO Randall Stephenson, who is currently paid less than Time Warner CEO Jeffrey Bewkes, and his team of executives have a perfectly good reason to want to buy Time Warner — whether or not it makes business sense.
Talk of buying Time Warner naturally brings to mind AOL’s disastrous 1999 acquisition of Time Warner, which over 15 years later is still remembered as the most catastrophically failed merger of all time.
This serves, in an odd way, to set the bar unreasonably low for an AT&T takeover. Pieces making the case for the merger frequently invoke the comparison only to debunk it, explaining that the current situation is totally different. And it really is different! This deal, if it goes through, almost certainly will not go down in history as the worst deal of all time.
In fact, the combined company should even be successful. AT&T is a profitable phone company and Time Warner is a profitable media company, and if you slap them together you’ll have a big, profitable company. The merged Comcast-NBC Universal entity isn’t doing anything special, but it’s a well-managed business that makes money. There’s no reason AT&T couldn’t pull off the same thing.
But when you pay a premium to buy another company, the test isn’t supposed to be “will the combined entity avoid being a huge catastrophe that ruins everyone’s lives and careers?” The test is supposed to be “will this be worth the money?” In the case of AT&T/Time Warner, the answer seems to be no. But it’s the shareholders’ money, so who’s counting?
If you open up your wallet, you probably have an assortment of $1, $5, and $10 bills — and maybe a few $20 bills. You almost certainly don’t have a thick stack of $100 bills.
Yet statistics show that $100 bills account for a large majority of the value of cash in circulation. There is $1.38 trillion worth of cash in circulation; $1.08 trillion of this is in the form of $100 bills. If you do the math, that works out to 34 hundreds in circulation for every man, woman, and child in the United States — even though a normal American rarely carries even one.
Of course, this cash isn’t evenly distributed. No one knows exactly who has all those large bills because physical cash, by its nature, is hard to track. Studies suggest that's precisely its appeal, and consequently most C-notes are used for a variety of illicit purposes —  tax evasion, drug dealing, bribery, and so forth — both in the United States and overseas.
Harvard economist Kenneth Rogoff believes that the solution is to get rid of cash. In a new book, he argues that abolishing $100, $50, and perhaps even $20 bills will seriously inhibit crime and tax evasion while doing little to hamper legitimate commerce.
We spoke on the phone in early October. The transcript has been edited for length and clarity.
You call your new book The Curse of Cash. What is the curse of cash and what should we do about it?
There's a lot more cash out there than we really need for the legal economy. A big chunk of the cash that the US, the eurozone, Japan, other advanced countries have printed is floating around in the world underground economy. It’s facilitating drug trafficking, human trafficking, extortion, money laundering. It also plays a role in illegal immigration.
My recommendation is not to get rid of cash. It is not to go to a cashless world. It's to get rid of the big bills, which don't have an important use in normal transactions. And I propose doing this very slowly over a long period. I go as far as to get rid of the $20 bill, though we could debate that.
I'm trying to interfere as little as possible with ordinary uses while making it as hard as possible to hide and launder money, which is why the $100 bill is so popular.
I also have a proposal for financial inclusion: providing debit cards just to protect low-income people from adverse effects. They are not the ones mainly using the $100 bills anyway.
There’s a ton of cash in circulation, and in your book you say that no one knows for sure who has it and how it’s being used. But just to help readers understand this issue, could you give us your best guess about who has it and why?
At least half of $100 bills are held abroad. The Fed used to think they were almost all held abroad, but we now know that's not true. In fact, when you look at cross-country comparisons, it seems that a lot of them must be held at home.
It's a really hard to know to apportion it between tax evasion and crime, but I'm guessing that of the ones that are held domestically, 75 percent or 80 percent are for tax evasion and crime. The Fed has tried to demonstrate that there's a lot in legal usage, and they can't.
Other countries have done studies. The British did this very detailed study and found these massive hordes of cash being used in illegal activity. A lot of $100 bills are used by Mexican drug lords. Columbian FARC rebels use them a lot. Oligarchs in Russia hold them a lot. In China they're used a lot by wealthy people to do transactions off the radar.
Some of that we might say is doing a public good. But it’s hard to say that about Mexican drug lords.
The Bank of Canada produced a report recently showing a very large supply is unaccounted for and they found it in places like construction contractors' basements.
Let’s take construction companies as an example. Why specifically would a construction company hold so much cash, and what are they doing with it?
The construction industry is an area where people do a lot of payments off the books. There’s a lot of hiring undocumented workers. There’s some avoidance of regulation and some avoidance of taxes.
In Boston, it used to be the case that there were sites you could show up and get hired for certain types of daily construction work. They were undocumented workers.
I actually favor a broad amnesty program for existing illegal immigrants. One of the objections to amnesty is that you open the floodgates for more people to come in. Right now the payments from employers in cash are the big magnet, maybe it wouldn't be as extreme as it is now.
What about concerns that phasing out large bills would hurt poor people, many of whom prefer to deal in cash? You’ve proposed giving debit cards to everyone, but low-income consumers haven’t always had good experiences with debit card systems.
Other countries manage to deal with this fairly straightforwardly. Nordic countries give everyone who gets government transfers free debit accounts. It's very inexpensive.
But more broadly, I think the idea that this is bad for poor people has it backward. The tax evaders are at the upper part of the income distribution. Payment recipients, like cleaners, don’t owe taxes. And if they’re paid under the table, then when they reach retirement age and try to get their Social Security, there isn't any. This is not something that favors poor people.
Also, crime falls disproportionately on the poor communities. So it is an important question and a tough question.
In addition to the crime-fighting benefits, you argue that phasing out large bills will make monetary policy more effective by allowing the Fed to boost the economy by setting negative interest rates. You argue that negative rates could be necessary if interest rates stay at their current low level, and you worry that people could defeat negative rates by stockpiling $100 bills in warehouses.
The thing I wonder about here is: Wouldn’t it be simpler to just set a higher inflation target? What really matters for monetary policy is inflation-adjusted interest rates. So if the Fed raised its inflation target from 2 percent to 4 percent, then a nominal interest rate of zero would be equivalent to a real interest rate of -4 percent — which seems like it should provide the Fed with plenty of breathing room. This seems like it would be simpler and less controversial than trying to phase out cash so we can force people to accept negative interest rates.
The biggest argument against changing the inflation target is that it will confuse the heck out of people. If Federal Reserve chair Janet Yellen and European Central Bank President Mario Draghi had a press conference and said, “we told you that 2 percent is nirvana but now it's 4 percent. We're sorry about our mistake,” I think it would be very hard to anchor those expectations.
I think you could literally have a financial crisis out of that. And then who believes the 4 percent target?
A subtle but really important argument that Stan Fisher has made is if you have 4 percent inflation, you’ll have more inflation indexing. People will change wages and prices more often. And in theory if prices and wages change all the time, monetary policy does nothing. So it's perfectly possible that you raise the inflation target to 4 percent and then you need all of it because monetary policy is less effect.
Another problem is that the distortions caused by a higher inflation target are pretty substantial. Let's suppose people don't change prices more often. Then when it comes time to make a change, you're going to make a much bigger change. So there will be larger economic distortions.
In 2012, Narayana Kocherlakota did something that’s rare for a policymaker of his prominence: He changed his mind. Kocherlakota was the president of the Minneapolis Federal Reserve Bank, which gave him a rotating seat on the powerful Federal Open Market Committee. That’s the committee that decides whether — and to what extent — the Fed should use its control over the money supply to boost the economy.
When Kocherlakota took the helm of the Minneapolis Fed in 2009, the Minneapolis Star Tribune described him as “openly suspicious of government's ability to bolster economic growth.” That view was evident in 2011, when Kocherlakota cast a rare dissenting vote against a stronger Fed effort to boost the economy. He argued that the Fed’s dovish policies could create too much inflation.
But the inflation Kocherlakota feared never came, and a year later Kocherlakota’s thinking had changed dramatically. In September 2012, he began calling for the Fed to do more to boost the economy. In 2014, he dissented three times from Fed decisions, each time calling for the Fed to be bolder about growth and less worried about inflation.
Kocherlakota’s term at the Minneapolis Fed ended earlier this year. He now teaches economics at the University of Rochester and writes a column for Bloomberg. But he has continued to argue that the Fed is too cautious.
If he’s right, it could be a really big deal. The current recovery has been the slowest in decades; the economy has fallen trillions of dollars short of its pre-2007 trajectory. Kocherlakota believes inadequate monetary policy is partly to blame for this shortfall.
And his view is becoming increasingly mainstream. Indeed, in a speech last week, Fed Chair Janet Yellen suggested that stronger Fed action might be needed to boost the economy’s growth rate. The comments come at a time when the Fed is widely expected to raise interest rates within months. But Yellen’s comments — which echo Kocherlakota’s arguments — suggest that the Fed might want to keep rates low for much longer than that.
When Kocherlakota took over as the head of the Minneapolis Fed in 2009, he believed there simply wasn’t that much the Fed could do to boost the economy. By 2011, the unemployment rate was still around 8 percent. However, Kocherlakota told me, “I was concerned that a large amount of the unemployment was due to structural forces that would be very difficult for monetary policy to influence.”
For example, workers in areas with high unemployment might find it difficult to move to other cities where jobs were more plentiful. Or it might take time for workers in declining industries to be retrained in industries where demand was rising. If these kinds of factors were responsible for high unemployment rates, Kocherlakota reasoned, pumping more money into the economy wouldn’t boost growth; it would just push up prices.
“The way that would manifest itself is by inflation rising above the committee's 2 percent target,” Kocherlakota said. “That was my concern in 2011.”
But two factors caused him to change his mind. One was that the inflation he and other hawks feared never materialized. Inflation fell in 2012 and has stayed below the Fed’s 2 percent target ever since.
Kocherlakota said he was also influenced by new economic research. In particular, a 2012 paper by economists Edward Lazear and James Spletzer convinced him that structural explanations — like a mismatch between the skills workers have and the skills employers are demanding — couldn’t explain the weakness of the labor market.
“The main conclusion was that the unemployment that was in place in 2011 and 2012 was largely attributable to non-structural influences that could be amenable to monetary policy,” Kocherlakota said. In other words, he became convinced that if the Fed pumped more money into the economy, we’d get lower unemployment and higher growth instead of just more inflation.
In 2012, Kocherlakota’s newfound belief in the power of monetary policy to boost growth ran counter to economic orthodoxy. The conventional view held that monetary policy has the biggest effects for 12 to 18 months after a major shock like the 2008 financial crisis. After that, further efforts to boost the economy would simply produce more inflation.
This conventional view focused on the role of wage and price changes in smoothing out economic fluctuations. For example, if an industry is in decline, a fall in workers’ wages may be necessary to avoid the need for layoffs. The Fed can help this adjustment process along by boosting the inflation rate (or preventing a burst of deflation). That allows workers’ real (inflation-adjusted) wages to fall without the need for morale-destroying reductions in workers’ nominal wages.
“There's been a ton of work done in economics on trying to measure how quickly prices change. And most of the work has found that prices change pretty rapidly,” Kocherlakota told me. This means that unless the Fed totally screws up, this adjustment process should be complete within 18 months of a major downturn.
If that view were true, then it would have been pointless for the Fed to try to boost the economy in 2011, because by that point — more than two years after the 2008 financial crisis — the market’s natural adjustment process would have already run its course.
But Kocherlakota now believes this view is mistaken. He believes that especially in the current environment of low interest rates and low inflation, what really matters is the Fed’s ability to affect the market’s expectations about future growth rates.
This is because businesses take future economic conditions into account when they make today’s investment decisions. “If they see better demand conditions in the future, they're more likely to implement ideas and to engage in innovation, and we'd have faster productivity growth as a result of that,” he argued. This means that if the Fed can make a credible promise to boost growth in the next few years, it can result in a self-fulfilling prophesy where businesses invest more today.
In Kocherlakota’s view, easier money wouldn’t just help reverse the decade-long decline in Americans’ workforce participation rate. He argues that stronger monetary policy could actually increase workers’ productivity by giving companies the confidence they need to make long-term investments in technologies that boost worker productivity.
Instead, the Fed has talked incessantly about raising interest rates, a signal that it’s preparing to withdraw support from the economic recovery.
“There is a growing lack of confidence in central banks' ability or willingness to offset persistent downside shocks,” according to Kocherlakota. “That makes people less willing to spend [and] less willing to invest if you're a business owner, and that creates downward pressure on interest rates. People feel that both fiscal authorities and monetary authorities seem to lack the will or the ability to offset shocks, and that's going to make them very guarded about spending.”
When Kocherlakota first articulated these views in 2012, they were far outside the mainstream of economic thinking. Most people — including Kocherlakota himself just a year earlier — believed that the Fed had done plenty to support the economic recovery and that the slow growth of the post-2009 period was due to other factors.
But as the slow recovery has continued year after year, his views have started to seem more plausible. In a speech last week, the nation’s top monetary policymaker, Fed Chair Janet Yellen, floated some ideas that sound a lot like the ones Kocherlakota has been championing over the past four years.
Yellen suggested that insufficient spending as a result of weak monetary policy could have negative long-term effects on the economy’s productive capacity — for example, by causing discouraged workers to drop out of the labor force. She then asked whether these effects could be reversed by “temporarily running a ‘high-pressure economy,’ with robust aggregate demand and a tight labor market.”
“One can certainly identify plausible ways in which this might occur,” Yellen said. “Increased business sales would almost certainly raise the productive capacity of the economy by encouraging additional capital spending, especially if accompanied by reduced uncertainty about future prospects.”
Meanwhile, she said, “a tight labor market might draw in potential workers who would otherwise sit on the sidelines and encourage job-to-job transitions that could also lead to more efficient — and, hence, more productive — job matches.” It could also boost productivity by “prompting higher levels of research and development spending and increasing the incentives to start new, innovative businesses.”
In short, the Fed might still have a lot of room to boost the economy.
Yellen made clear that she isn’t endorsing these arguments — yet. She said more research was needed, and emphasized that keeping money loose for too long could have significant downsides. Still, her speech makes it clear that these ideas are becoming increasingly mainstream, and Kocherlakota deserves a lot of credit for that shift.
Back in 2014, Apple launched a massive project, code-named Titan, to build a car. But a couple of years later, Apple is drastically scaling back its ambitions. According to Bloomberg, the company has given up on the auto-manufacturing dream entirely. Instead, it’s focusing on writing self-driving car software that could power cars manufactured by traditional automakers.
Silicon Valley moguls have gotten into the habit of jumping into new industries and quickly turning them upside down — as Apple’s iPhone did to the cellphone industry. But Apple’s car struggles are a reminder that not every industry is as ripe for disruption.
Manufacturing a car is really hard. And succeeding at it requires a different kind of corporate DNA than succeeding in the computing sector. Tesla has been slogging through these challenges head on for a decade, and profitability is still years away. Google seems to have decided that car manufacturing isn’t worth it — it’s focused on creating software that will eventually run on cars manufactured by others.
Now Apple seems to be seeing the wisdom of Google’s approach, implicitly conceding that car companies have unique capabilities that Silicon Valley can’t easily duplicate.
The best explanation I’ve seen of Silicon Valley’s struggles to build its own cars comes from industry analyst Edward Niedermeyer, whom I interviewed earlier this year.
Niedermeyer pointed out that cars are complex products with hundreds of moving parts, and customers expect them to work reliably for years, over tens of thousands of miles, and in all kinds of terrain and weather conditions. These challenges are magnified by the massive scale required to turn a profit in the car business — which means that every production mistake costs millions of dollars to fix.
That means that succeeding in the car business requires a degree of regimentation that’s rare in Silicon Valley, where innovation is often valued more than flawless execution. Car companies plan the manufacturing process in great detail and have extensive systems for detecting and fixing flaws. Building up the necessary knowledge, equipment, and organizational structure takes years — even decades — of expensive trial and error.
Niedermeyer offered an example of the kind of challenge Tesla has faced as it has tried to grow from a luxury carmaker into a mainstream one:
A great example is the problem of mold growing from inside the Model S's roof, particularly in Norwegian cars. Because its large panoramic sunroof is difficult to manufacture and install to a precise specification, Model S roofs often leak. A lot of those leaks are so small that customers might not notice. But because Tesla used an organic-fiber pad at the edge of the sunroof, aggressive molds invade at alarming rates in certain climates.
Of course, an iPhone is also a complex product with many components that have to fit together perfectly. But cars are much bigger, have a lot of moving parts, include many different types of material, and are expected to work in punishing conditions ranging from blizzards to tropical rains. That means there are a lot more ways a car can break down, and there’s no substitute for years of experience testing cars in a wide variety of real-world conditions.
Of course, just as technology companies can’t easily duplicate car companies’ manufacturing prowess, the same is true in reverse. Building great software requires a particular culture, structure, and set of skills that technology companies have and car companies mostly don’t. Which is why car companies and technology companies are increasingly rushing into each other’s arms.
And so far, it looks like technology companies have the upper hand. There are only a handful of major technology companies known to be working on self-driving car software — Apple, Google, and Tesla are most prominent — whereas there are lots of car companies looking for tech sector partners. So if Apple can develop self-driving software that rivals Google and Uber’s, it won’t be hard to find automakers eager to incorporate its software into their cars.
Focusing on self-driving software allows Apple to keep its options open. It can potentially work with several different car companies and focus resources on the partnerships that prove the most promising.
This leaves Tesla as the only Silicon Valley company trying to bring Apple’s traditional strategy of selling both hardware and software over to auto manufacturing. Tesla is betting that doing both will allow it to reimagine the way cars are designed and deliver a more elegant, compelling product. But there’s a big risk that CEO Elon Musk has simply bitten off more than he can chew.
Recent breakthroughs in artificial intelligence and machine learning are enabling computers to understand the world and respond intelligently to it. Google is already embracing these technologies for Android, but they’re poised to have bigger implications, touching everything from drones to medical diagnosis.
At least that’s the view of Marc Andreessen, a prominent venture capitalist at the firm Andreessen Horowitz. And he should know. He made his fortune as co-founder of Netscape two decades ago, and more recently his firm has invested in successful companies like Facebook, Twitter, Airbnb, Slack, and Lyft. Andreessen is in constant contact with entrepreneurs and investors trying to build the next great technology company.
Andreessen argues that recent breakthroughs mean artificial intelligence has the potential to spawn a new generation of big, important technology companies. At the same time, he acknowledges that certain industries have proven stubbornly resistant to technological change — and he argues that more work is needed to bring the power of software to every corner of the economy.
We spoke by phone in late September. The transcript has been edited for length and clarity. You can read part two of the conversation here.
Where do you think the next great technology companies will come from? In the 1990s you had Google and Amazon, and in the 2000s you had Facebook and Uber. Obviously there might be a startup I haven’t heard of yet that’s about to get a big break. Still, it’s hard for me to think of any companies founded in the last six years that have a shot at becoming a Google or Facebook or Amazon-sized company.
The traditional way this happens is with new platforms and architectures. New generations of technology that emerge. The last big category of technology was the smartphone and smartphone apps. Smartphones materialized in 2007, many of the app categories were identified in 2010 or 2011. It’s becoming clear that there are some major new smartphone-centric companies that are going to be important companies. But four years ago or even two years ago that wasn't clear as it is today.
So if smartphone architecture was the last one, it feels like artificial intelligence, virtual reality, autonomy, voice, and the internet of things are all candidates for the next wave. The obvious example right now is AI. It sure feels like there are going to be a new set of products and companies that are going to be AI-powered at their core.
Facebook and Google and Amazon have these giant first-class efforts in this space. But we’re also seeing a legion of startups. I think there will be a whole generation of new very important AI companies that come out — many of which are just getting started right now.
People have been talking about AI for a long time, but commercial success has been elusive. What makes you think things are different now?
I was really skeptical at first. It’s not widely known, but there was an AI bubble in the 1980s where there were a whole bunch of venture-backed companies that got funded and they basically all blew up and torched all the capital.
We feel like we're seeing something different now. The really big change was the ImageNet competition in 2012. In 2012, computers became better than people at recognizing objects in images. This is an actual competition where they’ve calibrated how to measure this.
Basically what we've seen in the last four years is breakthrough after breakthrough after breakthrough. First was the breakthrough in recognizing objects in still images. There are corresponding breakthroughs happening right now in recognizing objects in videos — entirely new kinds of video classification. If you can do video recognition you can do realtime video, which means you can do autonomy.
We’ve invested in a company called Skydio that’s doing full autonomous consumer drones. It's such a different product than you can get today, with such different capabilities, it’s almost eerie. It's following you around with no human guidance at all. You run into a forest and it’s navigating and flying between tree branches entirely by itself. And it’ll be at a consumer price point. That's something out of a science fiction movie.
We’re seeing deep learning applied to pre-detection of cardiac events. We have a company called Freenome doing deep learning applied to blood biopsies for cancer diagnoses that seems to be working very well.
There’s a classic tech industry question: “Is this a product or a feature?” You see Google, Facebook, and Amazon all putting a lot of money into artificial intelligence. Siri began as a startup but was quickly acquired by Apple. So is AI going to spawn big, independent companies with new products? Or is it more likely that these innovations will be absorbed by existing big companies to improve their existing products?
Two years ago, I thought the big companies would dominate. The big companies had several big advantages:
What's happened in the last two years is that every single one of those factors has changed to at least some degree. All of a sudden, you have a lot more computer science graduates coming out knowing how to do this because this has become the hot new area of computer science. You also have a lot of the engineers who have been at the big incumbents working on this stuff who are now realizing they can start their own companies.
There's a whole new generation of autonomous vehicle startups that are spinning out of Google. Otto (recently acquired by Uber) was a prominent one, but there are, like, six others that are in flight right now.
Meanwhile, the technology itself is becoming more tractable. A lot of the interesting new projects we’re seeing don't need 1,500 people. They need five. Google open sourced this thing called TensorFlow, which is one of the building blocks of deep learning. We’re seeing startups all over the place picking that up and running with it, which would not have been possible a couple of years ago.
The science itself keeps advancing. People are learning how to do deep learning on small data sets.  We’re seeing startups that either figure out a clever hack to get the big data set, or figure out a way to run the algorithms in a way where they only need a small data set.
A lot of these potential AI applications seem like viable businesses but not necessarily big businesses. And for the really big opportunities — like self-driving cars — it seems like the big companies have advantages that will be hard to match.
I still think you're thinking of this as you'll take an existing product and add some AI to it. That’s not what we’re seeing. What we’re seeing is an entirely new kind of product that wasn't possible before.
Let's talk about drones for a second. You buy a drone today and pilot it yourself and 20 minutes later you crash into a tree. You say “boy that was fun,” and you have to buy a new drone.
The incumbent drone-makers have been talking for some time about adding a feature they call “follow me.” The number of drone companies that are either incumbent drone companies that are deciding they want to add that feature, the number of Kickstarter projects that are promising to add that feature, is dozens or hundreds. But nobody’s been able to make it work.
The reason is because it's not a feature; it's a totally new architecture. The drone has to be built on AI from the ground up. The bet that DJI and other drone makers are making is that it's a feature. The bet that we’re making is that it requires a brand new architecture.
That's an example of fundamental reinvention. If our thesis on that is right, then all the existing drones become obsolete. They just don't matter because they can't do the thing that actually matters.
If you talk to the automakers, they all think that autonomy is a feature they're going to add to their cars. The Silicon Valley companies think it's a brand new architecture. It’s a bottom-up reinvention of the fundamental assumptions about how these things work.
So it sounds like we have a lot of innovations coming out. At the same time, interest rates are very low and growth is slow in the economy overall. The way it’s supposed to work is that when interest rates are low, it’s easy to borrow money and easy to raise money, and we should have this surge of investment. But the statistics seem to show that there’s a lot more money being saved than invested. What do you think is going on?
Right now there are two different kinds of industries. There are the industries that have rapid technological adoption and productivity improvement. Television sets, computer equipment, media, food. Bloomberg had a story that food prices are plummeting because food production is getting much more sophisticated.
So you've got these sectors of the economy where there's rapid productivity growth. Prices are falling fast. Those are the industries where everyone is worried that the jobs are going away — or to China or Japan or Mexico. People say there's too much disruption — too much technological change. The Silicon Valley kids are wreaking havoc on the economy.
Then you have the sectors in which prices are rapidly rising: health care, education, construction, prescription drugs, elder care and child care. Here there’s very little technological innovation. Those are sectors with insufficient productivity growth, innovation, and disruption. You’ve got monopolies, oligopolies, cartels, government-run markets, price-fixing — all the dysfunctional behaviors that lead to rapid increase in prices.
The government injects more subsidies into those markets, but because those are inelastic markets, the subsidies just cause prices to go up further, which is what is happening with higher education.
And so in these sectors, people are irate that there's not enough productivity growth. There’s not enough technological growth and we’re paying too much.
You sum those together, you get this muddle in the middle where it looks like we're puttering along. But this masks what’s actually happening.
You have some sectors falling in prices very fast, some are rising very fast. What happens over time is that the rising-cost sectors eat the entire economy. Consumers see their incomes being eaten by health care and education.
To me the problem is clear: The problem is insufficient technological adoption, innovation, and disruption in these high-escalating price sectors of the economy. My thesis is that we're not in a tech bubble — we’re in a tech bust. Our problem isn't too much technology or people being too excited about technology. The problem is we don't have nearly enough technology. These cartel-like legacy industries are way too hard to disrupt.
One thing that most of these low-growth industries have in common is that they’re very labor-intensive. A big chunk of what you’re paying for is another human being to spend time with you — a nurse, a teacher, a nanny, etc. You’re probably familiar with the concept of Baumol’s cost disease — that as manufactured goods become cheaper, people are going to devote more of their resources to the thing that’s scarce, which is human labor.
So I wonder if this is a problem that’s just inherently unsolvable. There are always going to be some labor-intensive industries with slow productivity growth, and those industries are always going to have costs going up rapidly relative to the others.
On the macro level I agree with that. I think that’s an accurate description of what’s happening, and I do think Baumol's disease plays a big role in how the cost shifts.
The thing that I would point to is just because we think that an industry has to be labor intensive because it always has been, that doesn’t mean it has to be going forward. If you go to the productivity literature of the 1980s, one of the things they all knew for a fact in the 1980s was that you could automate production but you couldn’t automate retail. It was taken as a given that retail was always going to be labor-intensive. Distribution would always be labor-intensive. You’ve got the person who stocks the shelves, you’ve got the checkout person, you’ve got the person who helps carry stuff out to the car.
The big advance at the time was computer-based checkout and laser scanning. But it turned out the laser scan didn’t help productivity. The laser scan took time. Half the time it didn’t work and then you had to do a price check. Maybe it even degraded productivity because with the laser scan you didn’t have a price tag on the object because you didn’t think you needed one.
So there was a lot of disillusionment at that time that you’d never get retail to be more productive. Of course, in the last 20 years, retail has become radically more productive. First there was Wal-Mart with their modern approach to the supply chain. And then there was Amazon. And then I would argue the transition from physical products to software products is a third layer of productivity improvement, delivering music as an MP3 or a stream is a much more productive way of doing it than having a physical CD through stores.
So you have this giant industry of retail that was held to be completely hand-crafted. And now it turns out it can be almost completely automated. There’s a point where everyone is upset that the retail jobs are going away.
Are they though? Retail stores employ almost 5 million people, and the Labor Department has projected that to grow by 7 percent over the next decade.
That’s right. This is the thing where the luddites just keep getting it wrong. It’s an application of what you said, which is that the scarce thing becomes valuable. Retail clerks are growing.
The other thing that's been growing for decades is bank tellers. That one might actually finally begin to decline. But bank teller jobs have continued to grow for the last 30 years as ATMs and online banking were rolled out exactly for the reason you said. Which is all the sudden there’s an opportunity to differentiate by providing a higher level of service by providing a person.
Vinod Khosla has written all these stories about how doctors are going to go away. He thinks computers are going to be so much better at diagnosis that there’s no room for doctors any more. And I just think he's completely wrong. I think the job of a doctor shifts and becomes a higher-level, more important job that pays better as the doctor becomes augmented by smarter computers.
That's why I'm so optimistic about the economy. That’s why I think the Luddites and the slow-growth people are wrong. We can have tremendous amounts of job creation and have huge productivity improvements. They’re not actually in conflict, despite what everyone thinks right now.
Google’s Android dominates the smartphone market overall, but Apple has attracted a disproportionate share of high-end users — and consequently an outsize share of smartphone profits.
At a Tuesday event, Google unveiled a two-pronged strategy to change that. Part one was the Pixel, the first smartphone that will be designed and manufactured by Google. Google is betting that building its own phone will allow it to offer the same kind of seamless user experience Apple provides its own users.
But the second prong of Google’s strategy is more original and received more attention on Tuesday. The company wants to make voice-based artificial intelligence a much bigger part of how people interact with their smartphones. Google envisions a future where you’ll make restaurant reservations, look up photos, and play music by talking to your phone instead of tapping and swiping on its screen.
Obviously, this isn’t a totally new idea, as all the major smartphone platforms have had voice-based personal assistants — Apple’s Siri, Microsoft’s Cortana — for several years. But Google says it’s about to make this technology a lot better — so much better that people will use it a lot more.
If anyone can pull this off, it’s Google. Making AI really good requires a lot of data to “train” sophisticated machine learning algorithms. Wrangling large amounts of data has always been Google’s specialty. But even if the company can build a voice-based AI that can really understand a wide variety of requests, I’m still skeptical it will change the smartphone game as much as Google hopes.
Apple has a designed-focused culture and is known for its polished, elegant, and user-friendly user interfaces. By contrast, Google has a culture that’s focused on delivering fast and reliable online services.
Google’s business model for Android puts it at a further disadvantage in the user interface department. Apple designs both the hardware and software for the iPhone, allowing it to guarantee users a seamless experience. Google, by contrast, licenses Android as open source software to dozens of smartphone manufacturers, many of which customize it themselves, leading to a cacophony of different and often mediocre user interfaces.
The iPhone’s greater polish is a big reason the iPhone is disproportionately popular at the high end of the market, and why Apple is able to charge a healthy premium for the iPhone. Android has the biggest market share in the smartphone business overall, but Google earns much less in profit from the smartphone business than Apple does.
Many iPhone users nevertheless enjoy Google services like search, maps, and Gmail. But the fact that the iPhone’s operating system sits between Google and many of its users gives Apple a lot of leverage. In 2014, Google paid Apple $1 billion to maintain its status as the default search engine on the iPhone.
The executives onstage never said so explicitly, but several of Google’s announcements on Tuesday were clearly aimed at knocking Apple from its high-end smartphone throne. Most obviously, the Android Pixel is Google’s most direct challenge yet to the iPhone.
Google’s earlier line of Nexus phones were designed and manufactured by third parties; by contrast, Google is planning to bring most of this work in house for Pixel. The hope is that by owning the whole “stack” — software, hardware, and online service — Google will be able to match the seamless user experience Apple has long offered to its users.
But Google doesn’t just want to ape the iPhone; it wants a way to differentiate its products from the iPhone. Google believes its real secret weapon is a new user interface based on voice recognition and artificial intelligence.
In a sense, this is just a souped-up version of Google’s existing voice recognition feature, Google Now. Apple and Microsoft have their own competing versions, Siri and Cortana. And these products don’t seem to have had a big impact on the market.
But Google believes that’s just because the technology is not good enough yet. Google has been working to improve its voice feature in three directions:
An example can help to illustrate what Google has in mind here. Right now, if you want to look at photos from a vacation you took last summer, you’d open your photo app and scroll back until you find the date you want. Google envisions a totally different approach. You’d say, “Okay, Google, show me pictures from my vacation last July.” Android would understand the request and call up the photos.
Google’s Photos app demonstrates that Google is already well on its way to developing the image recognition technology to make this work. You can already ask Google Photos to show you photos that contain snow, or a dog, or a particular friend. Google hopes to bring these capabilities — and more — to its personal assistant, so you can ask complex queries like, “Show me pictures from 2014 that have Aunt Lisa and dogs in them.”
It’s easy to be skeptical of this kind of thing, since the existing smartphone “personal assistant” technologies aren’t very good. They get confused often enough that it’s usually easier to just do things the old-fashioned way. But artificial intelligence technology is advancing rapidly, and Google insists that it will soon be good enough that voice-based personal assistants will “just work.”
If voice-based search becomes capable enough, it could reach a tipping point where it’s easier to just ask the voice assistant for the information you need than to open up the appropriate app and find it the old-fashioned way.
This would be particularly good news for Google because it would play to the company’s strengths. It would essentially be putting search back at the center of the user experience.
Making this kind of smart voice assistant work will require oceans of information. Computer scientists have found that tasks like image and voice recognition work best when they have huge numbers of examples they can use to “train” the algorithms. A smart AI system also needs to know lots of facts about the world so that it can respond to complex queries. Collecting and organizing information has always been one of Google’s strengths — after all, Google’s mission statement is to organize the world’s information.
At the same time, it would downgrade Apple’s big strength — the ability to make elegant, user-friendly devices. If the main way people interact with smartphones is by asking them questions, the particular device will become less important — just as the advent of the web made the difference between PCs and Macs much less important.
Google is also hoping to continue the shift toward more and more user data stored online instead of on users’ local devices. Pixel buyers get unlimited, free storage for their photos and videos. That’s a good selling point for the Pixel, but more importantly, it will mean that Google can offer users access to their content from any device. Google envisions a future where users ask their Google Home smart speaker to put photos from their vacation on the TV. That kind of future would play to Google’s strength — managing massive quantities of data online.
It’s easy to understand why Google would like to essentially make voice-based searches a central part of how people interact with their phones and other devices. But even if Google manages to build a sophisticated voice assistant that can respond to a wide variety of requests, I’m skeptical that will give Google a significant leg up in the smartphone wars.
The canned demos in Google’s presentation were impressive, of course — canned demos usually are. But the question to ask is how much of the time we spend interacting with our smartphones would be improved by a smart voice assistant.
One problem is that talking to your phone isn’t always convenient. There are many social settings — at the office, in line at the grocery store, on the bus — where people around you are likely to be annoyed if you’re constantly barking commands at your phone. In those situations, you’re going to want to discreetly type or scroll your way to the information you’re looking for, so people are still going to care how user-friendly the old-fashioned touch-based interface is.
But more importantly, a lot of the time people spend on their phones — perhaps most of it — just wouldn’t be improved by a personal voice assistant. People spend a ton of time scrolling through posts on Facebook, Twitter, or Instagram, reading text messages, swiping through Tinder profiles, and so forth.
Even with a task like finding photos, scrolling quickly through photo thumbnails will often be an easier way to find photos than trying to describe the photo you’re looking for. Often if you’re looking for an old photo, you don’t remember exactly when it was taken or what was in the photo. It’s helpful to scroll back to roughly the right time, look at a few photos at random, and use that to jog your memory about the context of the photo. This kind of browsing will probably always be faster on an old-fashioned touch-based interface than with voice-based queries.
Disclosure: My brother works at Google.
Tucked away in a fact sheet released on the Clinton campaign website Monday is a little-noticed set of bullet points that signals something important about how Hillary Clinton would govern. The campaign outlined an aggressive plan for beefing up the nation’s antitrust laws in order to “address excessive concentration” among major industries and end the “abuse of economic power” by corporations.
The proposals are significant because they don’t require passing legislation in Congress. Simply by choosing officials devoted to more vigorous enforcement of antitrust laws, Clinton can bring about a big shift in the way the nation’s antitrust laws are enforced.
The Obama administration has already taken a stronger stance on antitrust issues than President George W. Bush did. The antitrust principles outlined in Clinton’s fact sheet suggest she could beef up antitrust enforcement even further. The big question is how ambitious she’ll be about it.
Barack Obama came into the White House vowing to be tougher than George W. Bush. Bush was widely seen as weakening antitrust enforcement compared with the vigorous policies of President Bill Clinton. The Clinton administration, in turn, was generally seen as more active in antitrust enforcement than the Reagan and first Bush administrations.
These partisan divisions were best illustrated by the Microsoft antitrust case of the 1990s. The Clinton Justice Department concluded that Microsoft had violated antitrust laws by integrating its web browser, Internet Explorer, with the Windows operating system. It launched an epic legal battle that ultimately led to a victory in court for the government in 1999 and an order to break Microsoft up in 2000.
But the case dragged on into the George W. Bush administration, which decided to settle the case quickly and on terms relatively favorable to Microsoft. Microsoft never faced a punishment anywhere near as drastic as being split in two.
Yet it’s easy to overstate the degree of difference between recent Democratic and Republican administrations. Antitrust policy is a technocratic enterprise, and antitrust technocrats in both parties largely share a common intellectual framework. Democrats have been a bit more aggressive than Republicans about blocking mergers and suing companies for anticompetitive conduct. But as a 2002 article put it, antitrust officials “seem to play the game between the 45 yard lines.”
To find the last big change in antitrust policy, you have to go back to the 1970s — a period of much greater skepticism of concentrated economic power. Back then, antitrust officials were suspicious of large companies in general, whether or not they were engaged in anticompetitive behavior. They were particularly skeptical of national chains, fearing that their concentrated power could drive mom-and-pop stores out of business.
Then economists centered at the University of Chicago launched a sustained critique of this framework. They pointed out that up to a certain point, more concentration can actually be good for consumers. A big chain like Walmart, for example, can use economies of scale and superior supply chain technology to deliver much lower prices than mom-and-pop stores.
The courts started to accept these arguments in the 1970s. And they were embraced with gusto by antitrust officials in the Reagan administration. Under Reagan, federal regulators became much more sympathetic to arguments that mergers would benefit consumers by increasing efficiency and thereby lowering prices.
Republican administrations have embraced this shift most enthusiastically, but Democratic officials were tugged along by the same intellectual currents. The Clinton administration enforced antitrust laws more vigorously than the Reagan administration had done, but they weren’t as reflexively suspicious of mergers as pre-Reagan antitrust officials had been.
The story has been similar for the Obama administration, which has been somewhat more aggressive than the George W. Bush administration but still approved controversial mergers like Comcast’s acquisition of NBC Universal. (NBC Universal is an investor in Vox.com’s parent company, Vox Media.)
So one possibility is that Clinton will follow in the footsteps of her husband and President Obama, closely scrutinizing mergers but still operating within the relatively merger-friendly intellectual framework that has guided antitrust thinking since the 1980s.
The other possibility is that she’ll push further than Obama has gone, developing new rationales for blocking mergers that go beyond Chicago School thinking. If she did choose this option, she’d have some enthusiastic allies among her fellow Democrats.
Leading the charge is Sen. Elizabeth Warren, who has been beating the drum for stricter antitrust scrutiny all year.
One big theme of Warren’s thinking is the need to think more about factors beyond prices as the main criteria in determining whether a merger or company practice is good for consumers.
Warren has trained her fire on tech giants like Google, Apple, and Amazon, arguing that these technology behemoths have acquired too much power and are at risk of squelching competition. Traditional antitrust analysis might focus on the fact that Amazon has among the lowest prices in the retail industry and Google gives away most of its products for free. But Warren argues that’s the wrong way to think about it — the larger issue is that these companies’ dominance of key technological platforms could allow them to shut out smaller firms with innovative ideas.
You can make a similar point about one of the Obama administration’s most famous antitrust decisions. In 2011, the Obama administration rejected AT&T’s bid to acquire T-Mobile, which would have left the nation with just three wireless carriers.
If that merger had been approved, it might have led to higher prices for cellular service. But it also would have deprived the world of T-Mobile CEO John Legere’s “uncarrier” strategy, in which he’s blown up widely disliked wireless provider policies like two-year contracts and early termination fees and experimented with offerings like unlimited music and video streaming.
Similarly, critics of Comcast’s proposed merger with Time Warner — which was rejected in 2015 — weren’t so much worried about higher prices, since Comcast and Time Warner didn’t compete directly, but rather that the combined firm would use its power to stifle innovation in online services and diversity in television content.
If Clinton embraces Warren’s thinking — and appoints antitrust officials who share it — it could lead to reopening questions that have been considered closed for more than 30 years.
It’s far from obvious where this new thinking would lead. Even if antitrust officials conclude that big tech companies’ size is a danger to competition, it’s not obvious how they can best rein them in. But antitrust officials do have a lot of discretion. So if they take office determined to change how we think about antitrust, it could have a big impact on the economy.
Deutsche Bank, Germany’s largest bank, is facing its biggest crisis since the global financial meltdown in 2008. A couple of weeks ago we learned that US regulators were seeking to fine the company $14 billion — not just a large sum of money but actually enough to fundamentally threaten the viability of the bank. And that was only the latest in a series of setbacks that have cost the company’s stock more than half its value over the past year:
And now Deutsche Bank is facing a multibillion-dollar fine.
Things have gotten so bad that people have started to worry about the bank’s solvency. Last week, Bloomberg reported that a few of the bank’s clients had curtailed their business with Deutsche Bank due to fears about its financial health. That led to chatter that the bank’s failure could trigger a broader 2008-style crisis.
At the same time, regulators and banks have made a lot of changes since 2008 to prevent another crisis. European officials say that thanks to these reforms, Deutsche Bank — and other major European banks — is in a better position to weather future financial problems.
But those precautions have never been fully put to the test, and for years critics have worried that they may be insufficient. If Europe experiences an economic downturn, it could threaten the financial health of the continent’s banks — with Deutsche Bank one of the most at risk. And strict new anti-bailout rules passed as part of a backlash could hamper German leaders’ ability to respond effectively to a new crisis.
The 2008 financial crisis occurred because banks (as well as insurance companies and some other financial institutions) were making big, risky bets with borrowed money. Bank shareholders didn’t have very much of their own cash on the line, so when their bets went bad, shareholders could get wiped out quickly.
And because many banks owed money to each other, the failure of one institution threatened the solvency of others. That created the danger of a domino effect that could cripple the global financial system. In the fall of 2008, US and European regulators stepped in to rescue the banks before this could happen.
Since then, regulators have taken a number of steps to prevent this from happening again. One of the most important is to require bank shareholders to put more of their own money on the line. That way, if a bank’s bets don’t pay off, the costs will be eaten by shareholders rather than the bank’s creditors or (ultimately) the government.
European and American regulators have performed a series of “stress tests” to try to predict how banks will fare in the event of another economic downturn. If banks fail these tests, they’re required to beef up their reserves.
Deutsche Bank has been one of the worst performers in these tests, and last year it was forced to suspend dividend payouts to shareholders to allow it to build up its cash reserves.
At the same time, regulators have punished banks for their role in the 2008 crisis. The Obama administration has sought a series of stiff fines against banks that allegedly sold bundles of low-quality mortgages without fully informing customers of the associated risk. Deutsche Bank faces one of the biggest fines — $14 billion. But the bank is widely expected to negotiate a settlement that will require it to pay a fraction of that amount —  as little as $5.4 billion, according to one report.
To a large extent, these two regulatory efforts work at cross purposes. On the one hand, regulators are pushing banks to build up a bigger financial cushion to help them weather future economic downturns. But levying multibillion-dollar fines erodes that cushion, making banks more likely to become insolvent if they hit an economic rough patch.
It’s inevitable that a big, struggling bank will invite comparisons to 2008. And there are some obvious parallels. But the two situations also differ in some important ways.
In a financial crisis, it’s important for banks to have liquidity — cash or assets like government bonds that they can easily sell to raise cash. The Wall Street Journal’s James Mackintosh notes that Deutsche Bank’s liquid assets are about 12 percent of its total assets. For comparison, Lehman’s liquidity was just 7.5 percent of total assets a month before it collapsed.
The 2008 crisis occurred because Lehman Brothers, AIG, and other financial institutions had loaded up with “toxic assets” — complex financial instruments assembled using a lot of low-quality mortgages. Deutsche has about €37 billion of assets on its balance sheet that are not easy to price, which has created concerns that it’s in a similar situation.
Is it? It’s hard to say. As the Financial Times puts it: “The equity stakes and loans could be to thriving companies or businesses in deep trouble. The debt could be ‘distressed’ or in the form of high-grade private placements that are only illiquid because they were sold to family offices and institutions that tend to hold investments to maturity.”
Nobody outside Deutsche Bank knows for sure, which is one reason the bank’s stock price has been battered in recent months. Some traders are assuming the worst. At the same time, these assets represent a small fraction of the bank’s overall €1.8 trillion balance sheet. And the fact that some of these assets could be bad doesn’t mean they are bad.
The 2008 financial crisis happened after US officials refused to organize a bailout of Lehman Brothers, starting a chain reaction that brought down other companies that had been heavily exposed to the mortgage market. Ever since then, policymakers in both the US and Europe have been trying to change the rules to make another bailout unlikely.
In an acute crisis, Deutsche Bank and others could count on getting short-term loans from the European Central Bank. But if a bank winds up insolvent, European rules prohibit national governments from providing a no-strings-attached bailout. Instead, the rules require governments to first “bail in” a failing bank’s creditors — forcing them to accept that they’ll be repaid less than 100 cents on the euro.
This approach seems reasonable in principle, but it can lead to practical problems. In Italy, for example, banks’ creditors are not always large, sophisticated financial institutions. According to Bloomberg, 45 percent of Italian bank debt is held by ordinary Italians. That means complying with the EU rules could mean some Italians lose a big chunk of their life savings.
Italian Prime Minister Matteo Renzi got a taste of the potential backlash back in December, when the Italian government rescued four banks in accordance with EU rules. Creditors took losses in the process, and one of them was an Italian man who lost $110,000 he had invested in bonds issued by one of the failing banks. The man killed himself, leaving a suicide note criticizing his bank.
Earlier this year, Renzi was lobbying other European leaders for permission to inject taxpayer money into Italian banks to prevent a repeat of this fiasco. But German Chancellor Angela Merkel said no, insisting on strict adherence to the eurozone’s no-bailout rules. And she’s been consistent at home, with a German magazine reporting that Merkel has privately vowed not to use German taxpayer money to rescue Deutsche Bank.
That position is good politics, as bank bailouts are unpopular among German voters. But if Deutsche Bank were to actually fail, Merkel’s resolve would be tested. The losses from a Deutsche Bank failure could be felt widely across the German economy. And there’s always a risk that the failure of one big German bank could be the first domino that leads to a larger financial crisis.
“The internet is still at the beginning of its beginning,” writes Wired co-founder and Silicon Valley guru Kevin Kelly in his new book The Inevitable. Kelly argues that adding machine intelligence to everyday objects — a process he calls “cognifying” — “would be hundreds of times more disruptive to our lives than the transformations gained by industrialization.”
Is he right?
Kelly’s extreme optimism represents one pole of this debate. At the opposite pole is economist Robert Gordon, who believes the IT revolution is basically over. In his new book The Rise and Fall of American Growth, Gordon documents the dramatic economic changes of the 20th century — electricity, cars, indoor plumbing, antibiotics — and predicts that nothing of that scale is on the horizon.
“The economic revolution of 1870 to 1970 was unique in human history, unrepeatable because so many of its achievements could happen only once,” Gordon writes. He acknowledges that since 1970 there have been big improvements in televisions, smartphones, and other information technologies. But he argues that many other aspects of the economy — food, clothing, transportation, health care, and so forth — have changed little since the 1970s and are unlikely to change much in the next couple of decades.
Kelly and Gordon don’t just have opposite predictions about the future — they represent opposite approaches to thinking about an uncertain future. Gordon has difficulty imagining how computers could continue to transform our lives, so he assumes they won’t. Kelly’s life has been so transformed by computers that he can’t imagine how anyone doesn’t see their continued potential.
Reality, of course, is likely to be somewhere between these extreme views. It’s hard to believe that the IT revolution will be “hundreds of times more disruptive” than the Industrial Revolution — or even to figure out what that would mean. At the same time, Gordon is too cavalier about dismissing technologies like self-driving cars that really do seem poised to have a big social and economic impact.
But ultimately, I think Gordon gets closer to the mark than Kelly does. Kelly has spent his career in Silicon Valley, the place that has reaped the biggest gains from the exponential improvements in computing power. So it’s natural to assume that any problem can be solved — and any industry can be disrupted, or at least wildly improved — if we just bring enough computing power to bear.
He even has a sort of rallying cry for his perspective. “Who knows? But it will come!” The line is tucked into a chapter where Kelly tries to imagine different goods and services after they’ve been “cognified” by computers. “Cognified knitting” is one of the possibilities he imagines. What does that mean? “Who knows?” Kelly writes. “But it will come!”
Indeed, “Who knows? But it will come!” has become the de facto rallying cry for a lot of recent Silicon Valley innovations with more hype than obvious applications, and it emerges out of a foundational experience: watching the internet be underestimated after it emerged, and then mocked after the tech bubble popped, only to change the world directly thereafter.
But it-will-come-ism has fallen flat in recent years, and I think it’s going to continue failing in the years to come. There are a number of industries — with health care and education being the most important — where there’s an inherent limit on how much value information technology can add. Because in these industries, the main thing you’re buying is relationships to other human beings, and those can’t be automated.
It’s not surprising that Silicon Valley — a place that grew rich and powerful by building the internet economy — is full of technology optimists. Silicon Valley elites aren’t just used to changing the world with software. They’re used to being underestimated as they do it.
When I interviewed venture capitalist Marc Andreessen a couple of years ago, for example, he told me about his experience as a young startup founder in the early 1990s trying to convince big companies to take the internet seriously. Established CEOs laughed at him when he argued that the internet could disrupt industries like music, retail, media, and telecommunications.
Obviously, no one is laughing now.
Many tech elites believe history is about to repeat itself, only on a much larger scale. Andreessen made the case in a 2011 article called “Why software is eating the world.”
Until that point, the internet had mostly disrupted businesses that dealt in information. Now, Andreessen argued, the tech sector was coming for the rest of the economy.
In 2011, it seemed like the signs of this second digital revolution were sprouting up all over. Airbnb was widely seen as a pioneer of a new “sharing economy,” with Uber and Lyft announcing ride-hailing services in 2012. 3D printing seemed poised to render conventional manufacturing obsolete. The “internet of things” promised to embed cheap, tiny wifi-connected computers in everyday objects.
Startups like Khan Academy and Udacity were promising to revolutionize the education market with online classes. Bitcoin seemed to offer a new digital foundation for the financial sector. An IBM supercomputer called Watson beat the world’s best Jeopardy players, and IBM vowed to apply the technology to medical diagnosis.
It all seemed like it could be a really big deal. The industries the internet has disrupted so far — music, news, mapmaking — add up to a relatively small fraction of the overall economy. If digital technology can disrupt sectors like health care, education, manufacturing, finance, and government, the economic benefits could be massive.
Kevin Kelly thinks this future is right around the corner. “70 percent of today’s occupations will be replaced by automation,” he writes. “Most of the important technologies that will dominate life 30 years from now have not yet been invented.”
Until recently, I was very sympathetic to this point of view. But recently I’ve become more skeptical. One thing that started to change my mind was reporting on (and then buying a house using) the real estate startup Redfin.
In its early years, Redfin seemed like exactly the kind of disruptive startup Andreessen and Kelly expected to transform the conventional economy. In a 2007 interview on 60 Minutes, Redfin CEO Glenn Kelman described real estate as “by far the most screwed-up industry in America” and vowed to do for it what Amazon had done for bookselling and eBay had done for the sale of collectibles.
Back then, Redfin was charging homebuyers about a third of what a conventional real estate agent would charge for buying a house. To turn a profit, Redfin had to offer customers much less access to human real estate agents.
But customers hated this early, bare-bones product. For most of them, a personal relationship with a human agent was the main attraction of hiring a real estate firm. Facing the biggest purchasing decision of their lives, customers wanted someone available to answer questions and walk them through the steps of the home-buying process.
So over the past decade, Redfin has hired more agents and dramatically raised its fees. Today, the company looks more like a conventional real estate agency — albeit an unusually tech-savvy one — than the scrappy, disruptive startup Kelman led a decade ago. The real estate business wasn’t as ripe for disruption as Kelman thought.
Even if Kelman’s original vision had been more successful, Redfin would still have represented only an incremental improvement over conventional real estate services. It would have been cheaper and perhaps a bit more convenient, but it wouldn’t have fundamentally changed the process of buying a home.
And the same is true of a lot of recent startups that have aimed to disrupt conventional industries. Food delivery startups make it more convenient to order takeout. Uber and Lyft streamline the process of calling a cab. Zenefits provides a cheaper way for small businesses to manage payroll. Even Amazon mostly provides a cheaper and more convenient alternative to driving to the mall.
These are all perfectly good business ideas. The internet is creating lots of opportunities to squeeze inefficiencies out of the system. But Gordon’s book reminds us that this isn’t what a real technological revolution looks like.
The world inhabited by a typical American family in 1900 looked radically different from today’s world. Automobiles were expensive toys for the wealthy. Traveling from New York to Los Angeles required a train and took several days.
Washing machines, refrigerators, dishwashers, and vacuum cleaners were still in the future, making housework a back-breaking full-time job. Electric lighting was out of reach for most families, so they had to rely on dim and dangerous candles or kerosene lamps — and most simply didn’t try to do very much after dark.
Most homes lacked running water and flush toilets, leading to recurrent sanitation problems. And with no antibiotics and few vaccines, it was common for families to lose young children to infectious diseases.
By 1960, in contrast, a typical American family enjoyed a lifestyle that would be familiar to us today. Running water, flush toilets, electric lighting, cars, refrigerators, and washing machines were all commonplace. Deaths from infectious diseases like influenza, pneumonia, and polio were plunging. Ubiquitous cars and newly developed freeways meant that you could drive across town about as quickly as you can today (maybe faster at rush hour), and newly invented passenger jets could fly from New York to Los Angeles in five hours.
The rapid progress of the early 20th century depended on two factors. One was a series of technical breakthroughs in science, engineering, and medicine. But the other was the fact that in 1900, the human race had a bunch of big problems — dimly lit homes, slow transportation options, deadly diseases, a lot of tedious housework — that could be solved with new technologies.
The situation today is different. America hasn’t completely conquered material wants, of course. There are still tens of millions of people — far too many — who struggle to afford the basics.
But unlike in the early 20th century, these Americans represent a minority of the population. Among the affluent majority, food is cheap and easy to buy, closet space is scarcer than clothes, refrigerators and washing machines are ubiquitous, and there are often as many cars in the driveway as adults in the house.
Instead, these more affluent Americans have a different set of worries. Can I get a house in a good school district? Can I afford to pay for child care? Can I afford health insurance coverage? Will I be able to send my kids to college?
Indeed, the trend can be seen graphically in this chart:
The chart shows how various goods and services have changed in price relative to the overall price level. Over the past four decades, manufactured products like clothing, toys, cars, and furniture have gotten more affordable. At the same time, services like education and medical care have gotten a lot more expensive.
These trends are connected. Technological progress (and increased trade with low-income countries like China) has pushed down the cost of manufactured goods. But families don’t need an infinite number of televisions, cars, or clothes. So they’ve taken the savings and plowed them into other sectors of the economy — sectors where technology can’t easily boost output. With demand often outstripping supply, the result has been rising tuition, rising housing costs in trendy neighborhoods, rising child care costs, and so forth.
At this point, you might object that this just shows the need for disruptive innovation in education and child care. After all, aren’t startups like Udacity and Khan Academy working to create online learning alternatives?
But disrupting the education industry will be hard for the same kind of reasons it was hard for Redfin to disrupt the real estate business. Udacity aims to streamline education by reducing the number of hours staffers spend grading papers, answering student questions, and so forth. But from the student’s perspective, time talking to professors, TAs, and administrators isn’t wasted — it’s an important part of the educational experience.
Much of the value people get from attending a four-year college comes from interaction with other people. People spend their college years forming a circle of friends and a network of acquaintances that often become invaluable later in their careers. They gain value from group study and extracurricular activities. There is real benefit from mentorship by professors.
The social experience of college also serves as a powerful motivator. An early, free, Udacity course, for example, had a dismal 4 percent completion rate. It’s hard to motivate yourself to study hard when you’re only interacting with a computer program. The process of having human instructors regularly checking assignments — and students comparing grades with their peers — is core to students’ success, especially for less disciplined kids. So parents who can afford to send their children to a conventional college are unlikely to choose a cut-rate online university instead.
The same point applies to health care. Even if AI gets better at diagnosing diseases, people are still going to want a human doctor around to answer questions about the diagnosis and possible treatment options, to make sure the patient’s overall treatment process stays on track, and provide a comforting bedside manner. And they’re going to want a human nurse — not a robot — to tend to their needs during their hospital stay.
None of this is to say that technology can’t add value to industries like education and health care. Technology is likely to serve as a complement to these conventional services. Software will continue to help doctors get better at their jobs by providing better software for scheduling, diagnostics, and so forth. And technology may also provide affordable alternatives for people who don’t have access to a traditional university or hospital.
But it’s unlikely that today’s schools and hospitals are headed for the fate of Borders or Kodak.
So as long as technologists are merely finding ways to make it modestly cheaper or more convenient to do things people have been doing for decades, their impact on the overall economy will be necessarily modest. To have a bigger impact, they need to invent broad new product categories — which necessarily means finding big, unmet needs that can be addressed by new inventions.
Self-driving cars could be one example. Robert Gordon is dismissive of this emerging technology, and I think that’s a mistake. Autonomous vehicles will not only make people’s morning commutes more convenient, but they also have the potential to revolutionize the retail sector, change how people plan cities, and more.
Also, of course, in recent years we’ve seen a steady progression of entertainment and communications gadgets: DVD players, video game consoles, smartphones, and now VR headsets.
But outside the worlds of entertainment and communication, it’s hard to think of major new products in the recent past or likely in the near future. And Kelly doesn’t offer any plausible examples of big breakthroughs that could be on the horizon.
The one chapter of his book that focuses on this question is called “Cognifying.” It argues that embedding computer chips into everyday objects could dramatically change our lives. Yet his examples are not impressive:
We’ve see the same faith in the transformational potential of computing power at work in discussions of other recent innovations that have generated a lot of buzz in the media but haven’t been big hits with consumers. For example, there’s been a lot of discussion about the “internet of things” — an effort to embed computer chips in everything from thermostats to light bulbs. These products have been on the market for about five years, yet it’s hard to think of any cases where the addition of computer chips and software has added a lot of value.
You can say the same about home 3D printing. 3D printers have carved out an important but still niche application for prototyping in industrial labs and universities. But early predictions that 3D printers would become standard equipment in people’s homes have not been borne out.
Similarly, when Bitcoin burst into the mainstream in 2013, there was a lot of speculation (including from me) that it could disrupt the financial sector. But three years (and more than $1 billion in venture capital investments) later, we seem to be no closer to finding practical applications for the technology.
Of course, it would be a mistake to say that because these technologies haven’t produced much value yet, they won’t do so in the future. As technologists are quick to point out, people underestimated revolutionary technologies like PCs and the internet in their early years too.
But the fact that so many of these efforts seem to be falling short of expectations makes me skeptical of the view that computing power will inevitably transform every sector of the economy. Computers have proven that they’re great at transforming industries — music, news, maps, phone calls, and so forth — that are fundamentally about collecting, processing, and distributing information.
But software companies are now entering industries — from health care and education to lightbulbs and thermostats — that are primarily about managing physical objects or human relationships rather than information. That’s a bigger challenge, and in many of these industries I think technology companies will discover there just isn’t much room for them to add value.
The American economy in 2016 is full of contradictions.
On the one hand, we’re in an era of rapid technological progress. The internet has disrupted industries from retail to music. Emerging technologies like self-driving cars and virtual reality promise to take the trend a step further, creating whole new industries while rendering many conventional jobs obsolete.
At the same time, the current recovery has delivered the weakest GDP growth in many decades. Incomes and worker productivity have grown slowly. Despite record-low interest rates, business investment has been weak ever since the recession ended.
So what’s really going on? It’s a question that’s far too big to answer in a single article. In fact, we think it’s big and important enough to be worth its own section at Vox.
We think it’s impossible to really understand the changing economy without a deep understanding of technology. The internet and smartphone have already upended a number of conventional industries. Advocates say a wave of new technologies — Bitcoin, self-driving cars, the “internet of things,” and so forth — are poised to be even more disruptive. We’ll take a careful look at these emerging technologies, explaining how these technologies work and exploring what they can — and can’t — do.
And this works in reverse too: The technology sector is strongly affected by larger economic forces. Record-low interest rates have helped fuel Silicon Valley’s investment boom. Crippling housing shortages in the San Francisco Bay Area have limited the growth of technology startups. Decisions made by government regulators will have a big impact on emerging technologies like drones and cryptocurrencies. And broader economic statistics often serve as a much-needed reality check on the relentless hyping of new technologies.
So New Money will explain economics for people who love technology. And we’ll explain technology for people who are fascinated by economics.
Of course, we’ll also track and explain day-to-day business stories that have a bearing on these larger questions. This week, for example, I’ll have my eye on the problems at Deutsche Bank and Wells Fargo and rumors of an impending sale of Twitter.
We’ll cover Apple product introductions, Federal Reserve meetings, and everything in between. But our goal won’t be to break news or provide comprehensive coverage of these topics. Rather, the goal will be to help readers connect the dots between today’s business news and broader technology and economic trends.
These are topics I’ve enjoyed writing about for a number of years, and I’m excited to focus on them full time. I hope you enjoy reading about them!
You can read the first New Money article here. Future articles will appear on the New Money front page.
Correction: This article originally reported that urban households had seen their median incomes rise in 2015, while incomes in rural areas fell. But that appears to have reflected a statistical anomaly in the Current Population Survey. More reliable data released later in the week by the American Community Survey — another program of the Census Bureau — found that median household income in rural areas gained 3.4 percent, while urban and suburban households gained 3.6 percent. The text below has been corrected.
Earlier this week, the Census Bureau released data on income and poverty has good news for almost everyone. The data, the result of the Census Bureau's Current Population Survey, shows the first significant growth in average household incomes in almost a decade — 5.4 percent between 2014 and 2015 — with all races, age groups, and regions of the country enjoying gains.
But it appeared to show a big gap between the gains of urban and rural households. It reported that households outside of metropolitan areas (which I'll slightly imprecisely call rural) saw their incomes drop by 2 percent, while suburban households gained 4 percent and urban households gained 7.3 percent.
It seemed like a big deal, so I wrote an article about it.
Unfortunately for me — but fortunately for people in rural areas — it wasn't true.
What actually happened is that the Census Bureau changed how it defined rural households (technically, households outside of a metropolitan area) between 2014 and 2015. As a result, the 2014 statistics were measuring the incomes of different households than the 2015 statistics. Unsurprisingly, that resulted in a big apparent change. But this didn't reflect changes in anyone's income, it was just a statistical anomaly.
Later in the week, the Census Bureau also released statistics from a separate survey called the American Community Survey. This survey was larger and didn't make a big change in the way it defined urban and rural households, making it more reliable for this purpose. And it found there was hardly any difference between rural and non-rural households. Rural households saw their incomes gain by 3.4 percent, while non-rural households gained 3.6 percent.
I was inclined to believe the original Census numbers because they were consistent with a trend I’ve reported on before: The current recovery is seeing big cities reap the largest economic gains. That was a big change from the economic boom of the 1990s, which saw less populous areas gaining more.
In the past, smaller counties tended to grow faster than larger counties. This made a certain amount of sense — large counties like Los Angeles or Dallas were already expensive and crowded places to live, so it was easier for economic growth to happen in smaller towns or outlying suburbs.
But in the latest recovery, the pattern has reversed. The largest counties saw the fastest growth in jobs, with Los Angeles County, Miami-Dade County, and Kings County (Brooklyn) leading the way. Meanwhile, the least populous counties have been suffering the weakest recovery in decades.
But while major urban areas are seeing larger job gains, that's apparently not translating into big differences in household incomes.
Monsanto. It’s hard to even say the name without triggering a fierce reaction. The company has long been the public face of GMOs, thanks in part to the sheer dominance of its corn, soy, cotton, and other crops engineered to be resistant to the herbicide Roundup.
And pretty soon, Monsanto may no longer exist. At least not in its current form.
On Wednesday, the German chemical conglomerate Bayer offered to buy up Monsanto for $56 billion, in what could prove to be the largest corporate merger of the year. Monsanto has accepted the bid. And if the deal is approved by regulators — which is still an open question — the new company would become the largest agribusiness on the planet, selling 29 percent of the world’s seeds and 24 percent of its pesticides.
That would put the new firm in a commanding position vis-à-vis our food supply. Which is why European Union regulators and the US Department of Justice are likely to scrutinize this deal more closely than usual, to make sure it doesn’t create an all-consuming monopoly that can crank up prices on farmers and shoppers. The deal comes amid a blurry rush of agribusiness consolidation in recent months, with ChemChina-Syngenta and DuPont-Dow Chemical forming their own multibillion-dollar Voltrons.
Some onlookers are fretting that the reduced competition could shrivel up innovation, leading to slower improvements in crop yields. Others worry that these new agricultural giants may have outsize political power. "They’ll have more ability to lobby governments," says Phil Howard of Michigan State University, who studies consolidation in the food industry. "They’ll have a lot more power to shape policies that benefit themselves at the expense of consumers and farmers."
It’s a big story, and not just because Monsanto is such a famous (or infamous, if you prefer) brand. The consolidation of the world’s seed, chemical, and fertilizer industries over the past two decades has been astonishing, with potentially large ripple effects for farms and food systems all over the globe.
Back in 1994, the world’s four biggest seed companies controlled just 21 percent of the market. But in the years since, as crop biotechology advanced, companies like Monsanto, Syngenta, Dow, Bayer, and Dupont went on a feeding frenzy, buying up smaller companies and their patents. Today, the top four seed companies and top four agrochemical firms command more than half their respective markets.
And the pressures to merge have only become even more intense. Due to an economic slowdown in China and a glut of food production over the past few years, the global agricultural economy has been slumping. Commodity prices have fallen sharply, and farmers have less to spend on supplies (as well as on pricier biotech seeds). And the major seed, chemical, and fertilizer companies haven’t been able to churn out enough innovative new products to counteract this trend.
So their only choice at this point is to consolidate further, hoping to convince shareholders that they can slash costs and keep profits high.
Monsanto, the world’s largest seed producer, has found itself in a surprisingly precarious position. For years, the company reaped huge profits from selling its popular weedkiller, glyphosate (known as "Roundup") in tandem with crops genetically engineered to withstand glyphosate (known as "Roundup Ready" crops). But thanks in part to improper use, more and more weeds in the United States are developing resistance to glyphosate — and Monsanto is racing to find a replacement. The company is currently investing $1 billion to develop crops resistant to dicamba, another herbicide, but a merger would help it maintain market share in the meantime.
Last year, Monsanto put in a failed bid to buy up Syngenta, the world’s largest agrochemical producer. After the deal fell through, Syngenta CEO Mike Mack said the bid showed that Monsanto’s "core markets have been saturated" and that the company lacked "fundamentally new innovation" to drive growth. You might say the same about the Bayer-Monsanto merger.
Monsanto’s not alone here. Last year, Dow Chemical and Dupont agreed to combine their crop science divisions, and are waiting on US and EU regulators for approval. This year, the China National Chemical Corporation got the okay from US regulators to buy the Swiss seed company Syngenta in a $43 billion deal. Last week, in Canada, Potash Corporation of Saskatchewan and Agrium joined forces to create a fertilizer giant amid slumping fertilizer prices.
If all these mergers go through, Tom Philpott of Mother Jones points out, the three biggest companies that will emerge (Bayer-Monsanto, ChemChina-Syngenta, and DowDupont) will sell 59 percent of the world’s patented seeds and 64 percent of all pesticides. The behemoths are getting behemoth-ier.
There are a couple of reasons to be concerned about an agricultural landscape dominated by just a handful of giant companies. If firms can corner key markets in seeds and chemicals, they might be able to raise prices of their products on farmers, which in turn could make food more expensive. For this reason, groups like the National Farmers Union have been opposing many of these deals.
The other fear is that if these behemoths face less competition, they may face less pressure to pursue the sorts of innovations needed to improve crop yields and help feed a rapidly growing world. Some worry that these newly merged companies would end up focusing more on their most profitable crops rather than branch into smaller and underserved markets such as Africa.
"As these industries have consolidated, they’ve spent less on research, and what research they do has been steered toward big blockbuster profits with commodity crops such as corn or soy," Howard says. That means they’ve been spending less on smaller crops and even focusing less on smaller markets like the Southeast US.
Last year, when Monsanto was trying to buy up Syngenta, the company argued these fears were unfounded. Among other things, the company contended that innovation might actually be quicker, not slower, if research labs were consolidated.
The big question now is whether regulators will buy these arguments. The US Justice Department’s antitrust division will have to decide whether to approve the Bayer-Monsanto deal, block it, or add conditions before it can go through.
For their part, Bayer and Monsanto are arguing that the two companies have little overlap: Monsanto focuses on seeds and biology, Bayer on chemicals. But, for instance, Jack Kaskey of Bloomberg points out that the newly merged Bayer-Monsanto company would control about 70 percent of cottonseed sales in the United States — so that may be one possible area of focus (and perhaps the new firm will have to divest its cottonseed assets).
In years past, this deal might have been a foregone conclusion, as US regulators regularly waved through similar deals with few changes. But more recently, DOJ has become much more active in scrutinizing agribusiness mergers. As Philpott points out, just two weeks ago, the DOJ halted a deal in which Monsanto would’ve sold its precision planting division to John Deere — because the latter would have had 86 percent of the market in these technologies. Not an auspicious sign for Bayer.
On the other side of the Atlantic, EU regulators tend to be extremely critical of GM crops, so they may put up even more of a fight. "There is a risk of a lot of regulatory and political scrutiny. We put chance of approval at 50 percent," Jeremy Redenius, an analyst at Bernstein bank, told the Financial Times.
Another question is whether Bayer would keep the Monsanto name if the deal goes through.
After all, the name "Monsanto" carries a lot of baggage, much of it negative. When people express fears about corporate control of food or biotechnology, they invariably point to Monsanto. It’s widely viewed as the company that patents seeds and ruthlessly sues farmers who try to misuse them (even if the reality is considerably less sinister than the perception).
People in the company — and many crop scientists outside of it — have long seen that reputation as unfair. To them, the anti-GMO movement has spread a lot of baseless information about genetic engineering and has caricatured onto Monsanto as the face of evil. The company has tried a series of rebranding moves over the years to burnish its reputation. (Witness this Wired story: "Monsanto Is Going Organic in the Quest for the Perfect Veggie.")
Alas, none of it has flown. A telling anecdote in the New Yorker: In 2013, David Friedberg sold his innovative weather data company, the Climate Corporation, to Monsanto for $1 billion. His own father’s first reaction was: "Monsanto? The most evil company in the world? I thought you were trying to make the world a BETTER place?"
Given all that, Bayer may consider going all in and changing the name entirely. "It is too early to speculate about what the name of the company is going to be," Bayer CEO Werner Baumann said in an interview in May. "But let me tell you that Bayer’s name and Bayer’s reputation stand for science, innovation and an utmost level of responsibility for societal needs, and that is what we are going to leverage on, also for the combined company going forward."
— The Wall Street Journal offers more context around flagging biotech seed sales: "Behind the Monsanto Deal, Doubts About the GMO Revolution."
—This is an excellent timeline of Monsanto's history. And a few years ago in Modern Farmer, Lessley Anderson wrote a great piece on how Monsanto became so reviled in certain corners.
Nintendo dropped huge news on Wednesday: It will develop and release a Mario game, called Super Mario Run, for the iPhone by December and other mobile devices later on.
For Nintendo and gamers around the world, this is very big. Since it got into video games in the 1970s, Nintendo has largely restricted its big franchises, and particularly the games in those franchises that it directly develops, to its own hardware — the original Nintendo, Super Nintendo, Nintendo 64, Game Boy, Nintendo DS, and so on. With its announcement, the company is showing a serious commitment to the mobile phone market — and doing so with Apple as a major ally.
But Nintendo didn’t decide to stroll out Shigeru Miyamoto, the legendary creator of games like Super Mario Bros. and The Legend of Zelda, to the stage of Apple’s iPhone 7 event on Wednesday on a whim. This news has been long in the making — not solely the specifics of working with Apple, but Nintendo’s embrace of the mobile market in general.
Although Nintendo is one of the most recognized — and best — game developers, its hardware business is seriously struggling. The Wii U, the successor to the very popular Wii, is an absolute flop. And the Nintendo 3DS, the handheld successor to the also very popular DS, isn’t doing too much better, with 3DS sales consistently lagging the DS.
So Nintendo is looking to shore up in alternative markets — like the iPhone. And it’s not just with Mario — Nintendo in April also announced mobile games for two of its smaller franchises, Fire Emblem and Animal Crossing. This is a significant move for the company — and it tells us a lot about where much of the gaming industry is headed.
The numbers tell the story. Since it launched in 2012, the Wii U had, as of earlier this year, sold below 11 million units. In comparison, the Wii had sold nearly 10 times that since it launched in 2006, and the GameCube sold twice that amount since its release in 2001 (mostly through 2006, when the Wii replaced it).
The 3DS is doing much better, with more than 54 million units sold. That’s a promising number, but far below expectations. Every once in a while, analyst VGChartz looks at how the DS, the 3DS’s predecessor, was doing at the same point in its lifespan as the 3DS. The results are brutal for the newer console:
It’s no surprise, then, that Nintendo’s stocks remain down about 60 percent since the peak of the Wii and DS in late 2007.
This isn’t because the console market is dead. As Eurogamer reported, the Xbox One and PlayStation 4 have so far outsold their predecessors. Nintendo’s struggles are unique — driven by what the company now admits was poor marketing for the Wii U, and at least some hemorrhaging in the 3DS handheld market to the growing mobile gaming market. (The latter reportedly led former Nintendo President Satoru Iwata to call Apple the “enemy of the future.”)
One clear example of the poor marketing is in the name: For the casual consumer, how is the difference between a Wii and a Wii U or a DS and a 3DS entirely clear just based on the name? It’s clear, based on the name alone, that the PlayStation 4 is an upgrade to the PlayStation 3, but not so much for the Wii U or 3DS. Worse, the Wii U and 3DS look very similar to their predecessors, suggesting that they may just be an optional addition to the existing consoles — for example, maybe the Wii U’s tablet is just a new controller for the Wii and the 3DS is just a DS with a 3D add-on.
Now, Nintendo plans to release a new console sometime next year — codenamed the NX. So Nintendo isn’t looking at these numbers and abandoning its hardware business altogether. Instead, it is seemingly trying to hedge its bets.
The mobile market is a natural place for that: As Nintendo’s hardware business lags, the mobile gaming market is booming.
As hardware on mobile devices has become more capable of running more advanced games (not that there’s anything wrong with Snake or Tetris, but come on), developers have begun to take mobile gaming more seriously. And so have consumers: According to Statista, the number of mobile phone gamers in the US doubled to more than 164 million from 2011 to 2015 — and appears set to reach more than 210 million in 2020.
To put that in context, mobile phones hold a bigger platform of gamers than any one Nintendo console — even the massively successful Wii and DS — ever had. Based on the sales numbers for Nintendo hardware above, US mobile gamers make up nearly three times the population of gamers on the current Nintendo consoles, the Wii U and 3DS.
This holds up even when looking at other game developers: The most successful game console of all time — the PlayStation 2 — has sold just shy of 54 million units in the US since 2000, according to VGChartz. That’s still less than a third of the number of total US mobile gamers.
Mobile users aren’t necessarily as profitable per person as console gamers — mobile games tend to cost a few bucks or nothing at all, while console games are typically between $40 to $60. But mobile offers a huge user base nonetheless — and big game developers, from Nintendo to Square Enix (creators of the Final Fantasy series), are taking note of that.
The mobile games market is also particularly big in Japan, where Nintendo is based. Although the Japanese market is smaller in size, users are apparently more willing to spend cash: According to Statista, the average user in Japan is willing to spend nearly twice as much as the average US consumer on mobile games each year — $64 in 2016 in Japan versus about $33 in the US. This is why market analyst SuperData concluded that “Japan is the world’s largest mobile games market” despite its smaller user base.
For Nintendo, getting into these markets is potentially a very profitable venture — and not just because of software sales within that market.
One lesson from the smash hit of this summer’s Pokémon Go is that the success of a popular Nintendo franchise in the mobile market doesn’t have to be exclusive to the mobile space.
After the release of Pokémon Go, Nintendo’s stocks skyrocketed — more than doubling over the course of July. The company later clarified that it did not expect to make too much money from Pokémon Go, as a bulk of the profits will go to Niantic, the creator of the game, and Alphabet (formerly Google), which invested in the product.
But Nintendo’s stock was still 73 percent higher in early September compared to June — meaning it still benefited overall.
Similarly, after Pokémon Go, other Nintendo products massively benefited. The latest Pokémon games for the 3DS, Omega Ruby and Alpha Sapphire, sold 80 percent more this July than they did in July 2015. Nintendo 3DS hardware, meanwhile, had its best-selling month ever in July. Nintendo attributed some of these increases to the release of the popular game Monster Hunter Generations and a price drop for one of its handhelds, but it also said the buzz around Pokémon Go significantly helped.
I can personally vouch for this: After not getting into a Pokémon game for a few years, I bought Omega Ruby in August after getting caught up in the hype surrounding Pokémon Go. (It was well worth the purchase: It’s a good game, as Polygon’s review notes.)
Other successes in the mobile market could play out similarly. Do you like Super Mario Run on the iPhone? Maybe you’ll be more interested in checking out the new, bigger Mario game for the Nintendo NX. Maybe you’ll even buy the Nintendo NX to check it out.
This is the gamble Nintendo is playing here: Not only could the mobile market provide a new source of revenue for a somewhat struggling company, but it could shore up its more traditional businesses too.
Correction: A reference to the Xbox One in this article initially called it the Xbox 360.
Lara Croft is one of the most recognizable female characters in video gaming, having starred in a dozen editions of Tomb Raider since the series debuted in 1996. And that makes her a perfect window into the rapid pace of progress in 3D graphics over the last two decades:

(HalloweenCostumes.com)
At the left is the Lara Croft character as she appeared in the original 1996 game. On the right is Croft as she appeared in 2014's Tomb Raider: The Definitive Edition.
The increasing realism is a reflection of Moore's law, which holds that the amount of computing power per chip doubles every couple of years. Video games represent 3D scenes as a sequence of polygons; the more computing power you have, the more polygons you can render. The original Lara Croft was rendered with a few hundred polygons. The latest models use tens of thousands, producing images that look almost indistinguishable from the real thing.
See more images of Lara Croft's evolution here.

When Uber announced Thursday that it would begin offering rides in self-driving cars to customers in Pittsburgh, it caused a lot of consternation among people worried about job losses. After all, sharing economy companies like Uber are supposed to represent one of the economy’s big sources of job growth. If even Uber is automating its fleet, doesn’t that mean workers are doomed?
But in an interview with Business Insider, Uber CEO Travis Kalanick argues that Uber drivers shouldn’t worry. He expects to continue offering work to drivers for a long time:
If you're talking about a city like San Francisco or the Bay Area generally, we have, like, 30,000 active drivers. We are going to go from 30,000 to, let's say, hypothetically, a million cars, right? But when you go to a million cars, you're still going to need a human-driven parallel, or hybrid. And the reason why is because there are just places that autonomous cars are just not going to be able to go or conditions they're not going to be able to handle. And even though it is going to be a smaller percentage of the whole, I can imagine 50,000 to 100,000 drivers, human drivers, alongside a million-car network. So I don't think the number of human drivers will go down anytime soon.
Obviously, Kalanick has an interest in putting a positive spin on this since he depends on Uber drivers to make the service operate today. But his argument isn’t crazy. Similar things have happened in other industries.
For example, when automated teller machines were developed, many people thought ATMs would put most bank tellers out of work. But that didn’t happen. ATMs made it cheaper to open a bank branch, allowing banks to open many more branches in the 1990s. As a result, teller employment has actually grown slightly over the last 40 years, as this chart from economist James Bessen shows:
The same logic could apply to the car market. If self-driving cars make taxi rides a lot cheaper, people will take a lot more taxi rides. And that could create more jobs even if the number of jobs per ride goes down. In the long run, there won’t be someone sitting in the driver’s seat, but there will be lots of other jobs supporting cars — things like maintaining, repairing, and cleaning the vehicles, handling customer service calls, keeping maps updated, and so forth.
Some jobs will be destroyed; others will be created. The net impact on the job market isn’t obvious.
Correction: I originally said that the number of bank tellers had declined after 2008, but better data shows employment rising.
Gawker.com, a gossip blog that spawned a multimillion-dollar media empire, is shutting down next week. The decision comes just two days after Univision won an auction to acquire Gawker Media, the parent company of Gawker.com as well as websites like Gizmodo and Lifehacker.
It might seem surprising for Univision to spend $130 million for the company and then immediately shutter its flagship brand. But the business logic behind the decision seems pretty clear: Gawker.com’s brand had become so toxic that continuing to operate the site could have posed significant financial risks.
Shuttering Gawker.com will allow Univision to get a fresh start. Other Gawker Media sites will apparently continue operating, and Univision’s management will be able to focus their full energies on them.
Gawker Media’s path to bankruptcy began with a decision by Gawker.com editors to publish a video of Hulk Hogan having sex — without the permission of either Hogan or his partner. Hogan sued, arguing that this was a violation of his privacy. A Florida jury agreed, awarding Hogan $140 million in damages earlier this year.
Gawker Media didn’t have $140 million, so the company was forced to declare bankruptcy. The company was put up for auction, with the proceeds used to pay part of Hogan’s judgment.
We didn’t know it at the time it was filed, but Hogan’s lawsuit was secretly funded by technology billionaire Peter Thiel. Thiel has borne a grudge against Gawker since 2007, when Gawker published a post titled “Peter Thiel is totally gay, people.” Thiel started to look for opportunities to punish Gawker for what he saw as irresponsible journalism.
These were not isolated incidents. Gawker has outed others, including CNN anchor Anderson Cooper and Shepard Smith of Fox News. In one of its most infamous stories, Gawker reported on a New York media executive soliciting the services of a male escort. The man was not a public figure, and the piece was later removed after founder Nick Denton decided that the story had gone over the ethical line.
In court, Hogan's lawyers sought to portray Gawker as an organization without a moral compass. It wasn't a hard argument to make. During one deposition, Hogan's lawyers asked a former Gawker editor if there were any situation in which a celebrity sex tape would not be newsworthy.
"If they were a child," replied the editor, Albert Daulerio.
"Under what age?" the lawyer asked.
"Four," Daulerio replied sarcastically.
As a result, arguments about media freedom fell on deaf ears in the jury box. Jurors didn't buy arguments that the First Amendment protected Gawker's right to humiliate random celebrities by publishing video of their most intimate moments.
Whatever your view of Gawker’s journalism, the case does raise questions about whether billionaire-funded lawsuits could threaten freedom of speech.
Gawker isn't the only publication to be targeted by a disgruntled billionaire. Last year, the liberal magazine Mother Jones defeated a defamation lawsuit filed by Republican donor Frank VanderSloot. Winning the lawsuit cost Mother Jones, a relatively small nonprofit organization, and its insurance company $2.5 million in legal fees.
If VanderSloot's goal was to punish Mother Jones for writing an accurate but unflattering story about him, a loss was almost as good as a victory. His lawsuit sought $74,999 (staying just under the $75,000 threshold that would have allowed Mother Jones to move the case to federal court and away from an Idaho jury that might have favored the hometown plaintiff). So "winning" the lawsuit cost Mother Jones and its insurance company 30 times as much as the amount they would have had to pay if they had lost.
What was really ominous was what happened after VanderSloot's loss. He "announced that he was setting up a $1 million fund to pay the legal expenses of people wanting to sue Mother Jones or other members of the 'liberal press.’"
As far as I know, no one has taken him up on the offer. But the threat to freedom of the press is obvious. Any news organization doing its job is going to make some enemies. If a wealthy third party is willing to bankroll lawsuits by anyone with a grudge, and defending each case costs millions of dollars, the organization could get driven out of business even if it wins every single lawsuit.
Thiel insists that he has no quarrel with news organizations that conform to mainstream journalistic norms. But the key thing about his strategy is that he didn't sue Gawker for outing him — a case he probably would have lost. Instead, he waited for years until he could find other plaintiffs with stronger cases.
That's a tactic that any billionaire could use against any news organization. And because most news organizations cover a wide variety of topics, the story that provoked a billionaire's ire might have nothing to do with the stories that actually trigger a lawsuit funded by that billionaire.
In short, Thiel's war on Gawker could become a template for other extremely wealthy people with personal or ideological scores to settle against news organizations. And that’s something to worry about even if you think Gawker deserves what it’s getting.
With its purchase of Gawker Media, Univision got a number of valuable assets. Aside from Gawker.com, Gawker Media operates six other websites. There’s a sports site called Deadspin, a gadget site called Gizmodo, a car site called Jalopnik, and a video game site called Kotaku. The site also published Lifehacker, which focuses on productivity tips, and Jezebel, which covers culture from a feminist perspective. Univision also gains control of the technical and business infrastructure that allows Gawker to sell ads and turn a profit.
One of Gawker’s big downsides is that the site’s sensationalistic style and cavalier attitude toward personal privacy creates a lot of financial risk. There’s the obvious risk that the site could publish another post that subjects Gawker — and the deep pockets of its parent company — to a multimillion-dollar lawsuit. There would also be a risk that a story could create so much controversy that it could poison relationships with advertisers.
Last year’s controversy over the story on a media executive soliciting a male escort is a good example. Multiple advertisers threatened to pull their ad campaigns if the story wasn’t killed. After Denton decided to pull the story, two Gawker editors resigned in protest.
While editorial independence is an important principle, it’s also important that senior editors exercise good judgment and avoid publishing stories that invade people’s privacy. If Univision had kept Gawker.com running, it might have faced the same kind of management headaches Denton has in the past few years.

For the past several years, Google and other companies have been testing out self-driving car technology on public roads. But if you were a member of the general public, there was no way for you to ride around in a self-driving car.
Uber is about to change that. Later this month in Pittsburgh, the ride-hailing company will offer the first self-driving car service that’s available to the general public. It represents a big step toward transforming self-driving cars from a research prototype into a commercial service.
Yet the service also comes with a big asterisk. While the new program will be open to the general public, it’s still very much a work in progress. Rides will initially be free, and they’ll still have a driver behind the wheel making sure nothing goes wrong.
This means Uber CEO Travis Kalanick is still far from his dream of offering a fully autonomous ride-hailing service that could slash prices and boost Uber’s profits. Rather, the new service reflects the start of a new project to make sure Uber doesn’t fall behind its rivals — especially Google — in self-driving technology. It will be years before you’ll be able to hail an Uber and have it show up with no driver inside.
Kalanick has long seen self-driving cars as the long-term future of his company. In early 2015, he began to make it a priority. And he decided to base Uber’s new self-driving research center in Pittsburgh so he could raid Carnegie Mellon’s top-tier computer science school for self-driving car talent.
In a matter of months, Uber hired about 50 researchers and scientists from CMU. The team began to apply what they had learned in the lab to the real world, building self-driving car prototypes. By May 2016, Uber’s self-driving car prototypes could be seen driving around on Pittsburgh streets.
Extremely accurate maps are essential for a fully self-driving car, so in 2015 Uber acquired a mapping startup to help build its own maps and began building detailed maps of Pittsburgh’s urban landscape. Uber plans to dramatically scale up that project, spending $500 million to build maps around the world.
A big challenge for developing self-driving car technology is to collect massive amounts of real-world data. Collecting millions of hours of real-world data provides the raw material that software engineers use to refine their self-driving car software, finding and weeding out bugs that could lead to a fatal crash.
Google has had to do this the hard way, hiring dozens of test drivers to ride around the San Francisco Bay Area (and more recently Austin and other cities) while watching carefully to make sure the car doesn’t crash.
Uber’s existing ride-hailing business should make this process more cost-effective for the company, since it will be able to ferry passengers around — earning some revenue — while collecting test data.
But Uber evidently doesn’t think its technology is quite mature enough for that yet. According to Bloomberg, Uber’s “self-driving” cars will have not only a safety driver ready to grab the wheel at a moment’s notice but also a “co-pilot” in the front passenger seat monitoring the car’s software to make sure nothing goes wrong. Right now, for example, the human driver has to take over when a car is crossing a bridge. In other words, this is still very much a research project rather than a service that has any hope of making money.
Presumably, Uber’s plan is to gradually change that, replacing the Uber employees who will serve as the initial test drivers with ordinary Uber drivers, and then eventually eliminate the drivers altogether. Uber’s vast database of both customers and drivers means the company will be able to scale up the program quickly once it’s convinced that it’s safe to put ordinary Uber drivers behind the wheel.
Of course, the barriers here might not only be technological. A variety of state and federal regulations could prevent Uber from entirely dispensing with human drivers. Uber — along with other companies working on self-driving technology — will likely have to convince policymakers to modify or reinterpret these regulations before it will be allowed to send anyone a car with no driver at all.
Uber isn’t planning to build cars itself; its plan is to collaborate with conventional carmakers, which will make the actual vehicles. On Thursday it announced a new partnership with Volvo. Uber’s first self-driving cars will be Volvos modified with Uber’s technology, and the two companies plan to spend $300 million to jointly develop a new self-driving car model. The two companies are aiming to have the car on the road by 2021.
That date is significant because Ford also recently vowed to bring fully self-driving cars — cars without steering wheels or pedals — to market in 2021. That’s roughly consistent with statements executives from other companies — including Tesla, Google, and major carmakers — have made about their own plans for self-driving cars.
Of course, overconfidence is common when bringing ambitious new technologies to market, and few efforts are more ambitious than fully autonomous cars. It’s likely that some of these projects will fall behind schedule, and others may wind up being only partly autonomous, with drivers having to stay ready to grab the wheel in case of emergency. (This is how Tesla’s Autopilot feature works now.)
Still, with billions of dollars being poured into self-driving car research, companies have every reason to worry about being left behind. So lots of companies are now making ambitious goals to bring self-driving technology to market by the early 2020s.
And Uber’s position in the market gives it a big advantage: Because it doesn’t make cars itself, it can partner with multiple car companies, play them off each other, and ultimately work with whoever gets self-driving technology to the market first. Uber’s deal with Volvo is nonexclusive, and the company may sign similar pacts with other carmakers in the coming months.
No matter who makes the best self-driving cars of the 2020s, there will be millions of consumers who have grown accustomed to hailing rides using their Uber app. Hence, it will likely be an easy sell for Uber to convince those customers to use the same app to hail a self-driving car.
Big changes are coming for the automobile industry, and everyone in the industry knows it. This week, Ford and the Chinese technology company Baidu announced a $150 million investment in Velodyne, makers of powerful LIDAR sensors that are widely used in self-driving cars.
It's only the latest in a long sequence of deals linking car and tech companies together over the past year:
The proliferation of deals represents a growing realization among car companies that they are going to need help navigating the major changes of the next decade — Ford says it wants to start offering fully self-driving cars by 2021.
The auto industry is facing three big innovations — car sharing, battery-powered electric vehicles, and autonomy. By itself, any one of these shifts would represent a significant but manageable challenge. But the real problem is that all three trends are converging, and they jointly represent an existential threat to today's dominant car companies.
So far, car sharing is the innovation that has had the biggest practical impact on the way people get around. Ride-hailing services are still limited to a narrow elite — just 15 percent of Americans had ever used Uber or Lyft in 2015 — but investors are betting that they will grow quickly.
The big question for the auto industry is whether ride-hailing services will start to displace car ownership as the primary way people get around. Even before Uber and Lyft came along, there were a few areas in America — like Manhattan — where it was common for people to forgo car ownership and get around using mass transit and (for the more affluent) taxis.
Uber and Lyft are making this lifestyle both cheaper and more convenient, which could cause more families to give up their cars. Short-term car rental services like Zipcar are also helping people get by without owning a car, knowing that one will be available to them in a pinch.
By itself, the shift to greater car rental isn't necessarily a problem for the car industry. Someone still has to manufacture and sell the cars that Uber, Lyft, and Zipcar are renting out, and there's no reason Ford, GM, and Toyota couldn't be the companies that do it.
But selling cars to ride-sharing drivers or the corporate fleets of companies like Zipcar is a different business than selling cars directly to consumers. And Detroit benefits from an incestuous relationship with conventional car dealers.
For example, one of the big barriers that has held back Tesla's growth is the network of dealers conventional car companies use to sell their vehicles. Not only do conventional car companies have a lot of experience selling cars through these dealerships, but state laws often require cars to be sold this way, creating a barrier to entry for startups like Tesla.
In a future where on-demand car rental is the default way people get around, the key to success in the auto industry may be winning big orders from companies like Zipcar, Uber, and Google. And that could create openings for companies focused on creating different kinds of vehicles and selling them in new ways.
Still, car sharing alone doesn't pose a big threat to the auto industry for the simple reason that today's ride-hailing services are too expensive for mass adoption. If you live in a suburban area with plentiful parking, it's cheaper and more convenient to buy a car and drive it around yourself. And even many people who live in high-density areas and don't have a car rely on mass transit (and walking and biking), because these options are more affordable than hailing a ride.
Internal combustion engines are marvels of engineering, channeling rapid-fire explosions to power a vehicle as it moves down the road, and modern car designs are deeply influenced by the strengths and weaknesses of internal combustion engines. Conventional car companies have expertise not only in engines but also in transmissions, radiators, gas tanks, and more — and a lot of that would be rendered obsolete if cars were powered by batteries and electric motors.
This is the basic premise behind the creation of Tesla. CEO Elon Musk and his financial backers are betting that a company specifically created to build electric vehicles will be better at it than a conventional car company trying to make the switch from gasoline to electricity.
Yet so far, electric vehicles haven't seemed like much of a threat to conventional vehicles. In 2015, there were 17 million new cars sold in the United States. Of these, just 115,000 — fewer than 1 percent — of them were battery powered or plug-in hybrids.
The basic problem is that electric vehicles don't provide a very compelling value proposition for ordinary consumers. They might save money on fuel, but they tend to cost significantly more than gas-powered equivalents — even after you factor in generous government subsidies. With long charging times and relatively few charging stations in most metropolitan areas, they're significantly less convenient than conventional gasoline-powered cars.
Self-driving capabilities would represent the most radical change in car technology in decades — and the biggest threat to conventional car companies. Making cars drive themselves is mostly a software problem, and software has never been Detroit's strong suit. Google is investing heavily in its self-driving car program, betting that its expertise in software development will allow it to claim a major role in the car industry of the 21st century.
Yet even here, conventional car companies have managed to keep their heads above water by outsourcing the hard technical problems to third parties.
A bunch of car companies — including Tesla and several conventional firms — have begun offering cars with "advanced cruise control" or "autopilot" capabilities. And several of them have purchased their technology from Mobileye, an Israeli startup that  builds car sensors and software that allows cars to stay in their lane and avoid hitting the car in front of them.
Detroit's vision is that this self-driving technology will gradually get more and more sophisticated. Cars will continue to look more or less like they do today, but over time you'll have to grab the steering wheel less and less frequently. Eventually, grabbing the steering wheel will become so rare that it will make sense to sell cars with no steering wheel at all.
If that's how the market will evolve, it's not such a scary prospect for incumbent car companies. They have decades of experience integrating third-party components into their vehicles, and their existing manufacturing and distribution facilities will give them a big advantage over companies that try to start building self-driving cars from scratch.
So incumbent car companies would be able to cope with any one of these three trends taken individually. The problem for Detroit is that these changes are not going to come one at a time. They're happening all at once, and each of them is likely to accelerate and magnify the impact of the others.
For example, it's hard to make electric cars compelling to consumers, but it should be easier to make them attractive for car-sharing services. A ride-hailing car doesn't need the 300-mile range of a conventional gasoline-powered car. It just needs enough power to get through a morning of driving; the driver can then recharge it during his lunch break.
So electric cars purpose-built for sharing use might have smaller batteries, which can reduce vehicle weight and further improve energy efficiency. And of course ride-hailing drivers care a lot about the energy efficiency of the vehicles they drive, since a gas-guzzling car will cut into their earnings.
Self-driving technology will magnify this effect still further. As long as taxis need human drivers, they're going to mostly be sedans with room to carry three or more passengers. Nobody would want to drive a two-seat taxi and miss out on fares because he only has one passenger seat available.
But once you get rid of the driver, much smaller and lighter vehicles become viable. Lots of people take taxi rides alone, and they could take these rides in tiny one-passenger electric vehicles. These lighter vehicles would be more power-efficient and require smaller batteries, which in turn would further lower the cost of taxi service.
The optimal one- or two-seat electric car is likely to look significantly different from the best five-seat sedan — especially if the car doesn't need a driver's seat. So the shift to self-driving, electric ride-hailing cars will create a bigger opening for new companies to disrupt the car market.
And of course, with improved efficiency and no driver, these automated taxis will cost a lot less to run. That will bring them into financial reach for many more customers, dramatically expanding the market for ride-hailing services.
At the same time, the shift to ride hailing will help accelerate the shift to self-driving technology. One big reason for this relates to mapping. Right now it's widely believed that Google has the world's best self-driving car technology, and Google's technology depends on having extremely detailed maps of every street where a Google car drives. Uber has traditionally relied on Google's map data, but it recently announced plans to create its own independent mapping data.
If you're trying to sell cars to consumers, that's a huge problem, because it means you have to map the entire country before you sell your first vehicle — no customer is going to buy a car that refuses to drive in rural areas. On the other hand, if you're renting cars, it's not such a big problem. You can roll out your car rental service one metropolitan area at a time. Nobody minds renting a taxi that will only go to destinations in the same metropolitan area.
Renting self-driving cars will have other advantages too. Companies that write self-driving software will be concerned about safety (and the liability that comes with accidents). Maintaining ownership of the vehicles will allow them to guarantee that they get regular maintenance and that defective parts are replaced promptly.
So there's a good chance that fully self-driving cars will initially be available only for use as part of a ride-sharing service. If that happens, it will undermine the advantage provided by car companies' dealership networks and experience selling cars directly to consumers.
Electric, self-driving cars designed for a rental market are likely to look dramatically different from today's cars. With no driver and less space needed for an engine, we're likely to see companies experimenting with different shapes and configurations — rear-facing seats, built-in desks and mirrors, big touchscreen displays and televisions, perhaps even beds for long nighttime trips.
So conventional car companies are going to face some serious competitive threats over the next two decades. But it would be a mistake to write them off, because they have one big advantage over Silicon Valley upstarts: They know how to manufacture millions of cars.
Tesla is the only Silicon Valley company to even attempt to manufacture cars at a significant scale, and it's been learning the hard way that it's really difficult. Tesla CEO Elon Musk has become notorious for announcing production delays and cost overruns.
All three of Tesla's cars so far came to market later than originally announced, and Tesla recently announced it was raising $2 billion to fund further expansion of its manufacturing facilities. If all goes according to plan — and it probably won't — that will allow Tesla to produce 500,000 cars a year, about 3 percent of US car sales in 2015.
Manufacturing a big, complex object like a car is difficult, and conventional car companies have a 100-year head start on learning how to do it. That explains why Silicon Valley has been as eager to partner with Detroit as Detroit has been to partner with Silicon Valley.
The big question is how these partnerships will work. Will Google, Uber, and other Silicon Valley giants design the cars and rely on their Detroit partners to assemble them, much as Chinese companies like Foxconn assemble smartphones for Apple and Motorola? Will car companies treat Google as just another supplier and Uber as just another distribution channel? Or will a new generation of companies emerge that combines the strengths of both sides? Companies in Detroit and Silicon Valley are spending billions in the hope that it will help them come out on top.
A robot vacuum cleaner sounds like a great idea. I have a Roomba, one of the most popular models, and most of the time it works great. But sometimes there are unexpected problems.
In a recent Facebook post, an Arkansas man described just how bad these problems can be. His dog had an accident on the floor, and then the Roomba started its scheduled cleaning.
"If your Roomba runs over dog poop, stop it immediately and do not let it continue the cleaning cycle," the man wrote. Unfortunately, he happened to be asleep when the Roomba ran. The result: it "spread the dog poop over every conceivable surface within its reach, resulting in a home that closely resembles a Jackson Pollock poop painting."
Silicon Valley optimists like venture capitalist Marc Andreessen have predicted that digital technology would revolutionize every facet of our lives. And of course that's been true for industries like music, news, and maps. But other tasks have proven more resistant to digital transformation.
Earlier this year, I wrote about Nest, whose popular smart thermostat made it a poster child for smart homes. But the company, which was acquired by Google in 2014, has struggled to develop new products, raising questions about whether Google overpaid for the company.
A similar story can be told about iRobot, the company behind the Roomba robotic vacuum cleaner. The company is hardly a failure, having sold 15 million units since it was introduced in 2002. But the Roomba remains a niche product, and iRobot hasn't come up with another hit.
These companies are struggling for similar reasons: Their products demand too much from their users while providing too little value in return.
Last year, iRobot sold 2.4 million Roombas. By any reasonable metric, that's a successful product. But in a nation of 320 million people (not to mention a world with more than 7 billion people), it's still a niche product. The vast majority of American households don't have a Roomba or any other robot vacuum cleaner and seem to be in no hurry to buy one.
And if you talk to Roomba owners, it's not hard to see why. "It gets stuck a lot," my Vox colleague Sarah Kliff told me. "I can't really leave it at home unsupervised."
Sarah has a table with a curved metal bottom that her Roomba finds fiendishly difficult to navigate. Often she'll come home to find that it drove up the table's leg and got stranded, the cleaning job unfinished. The Roomba also terrifies Sarah's dog.
(Sarah Kliff / Vox.com)
Sarah's dog Spencer hiding from Sarah's Roomba
My Roomba also has problems with getting stuck. But I've also found that it just doesn't save me that much time. I still have to tidy up the room before letting the Roomba loose. Then when it's done, I have to empty the dustbin and — often — dig out debris that got caught in the rollers. It's not as much work as using an old-fashioned vacuum cleaner, but it's not that much less work.
And then there are the times when the Roomba wreaks havoc. Asked about poop-related accidents, a spokesman for iRobot told the Guardian that "quite honestly, we see this a lot." Neither Sarah or I have experienced this particular misfortune, but we've had other, less traumatic problems with our Roombas.
"My old roommate had a Roomba that ran into my mirror," Sarah told me. "The mirror toppled over and broke."
One day, my Roomba got ahold of a spool of thread. When I got home, it had unwound the entire spool and wrapped it around the cleaning brush roll. It took several minutes to get it unwound, and I had to throw away the rat's nest of thread that was left.
I have a $399 Roomba 650. iRobot recently introduced a new high-end model, the $899 Roomba 980, which comes with a built-in camera, a longer-lasting battery, and other improvements. But as Fortune's Kif Leswing pointed out in a review last October, these improvements only get you so far. The longer battery life doesn't help if the dustbin gets full or your home has multiple levels. And the latest Roomba seems about as clumsy as its cheaper cousins — Leswing says it "beached itself on the legs of my Ikea Poang chair." And it ate one of his cat toys, damaging one of the robot's wheels.
The Roomba is by far the iRobot's most successful product. Over the years, the company has built a couple of mopping robots, a pool-cleaning robot, and a device for cleaning out your gutters. None of them have been big hits.
Other companies have tried to create internet-connected lawn sprinklers, crock pots, and lightbulbs.
A fundamental problem here is that for many tasks in the physical world, there just isn't that much room for software and complex electronics to add value.
The home appliances that have done the most to improve people's lives are the ones like dishwashers and washing machines that took a really time-consuming and tedious task and made it dramatically faster.
But in many cases, the preinternet devices in our homes are already pretty good. There isn't a ton of room for further improvement. People don't spend a lot of time adjusting their thermostats, so the better interface on a Nest Learning Thermostat doesn't add a ton of value. Smart lightbulbs or robotic gutter cleaners seem even more like a solution in search of a problem.
Machines add the most value when they can be operated at scale in a controlled environment — washing machines and dishwashers are useful because you can wash dozens of dishes or shirts at the same time. And because all the action happens inside the machine, there's less room for unpleasant surprises — like a stray cat toy getting into the gears, or dog poop being spread across the floor.
In contrast, home robots and connected home devices are trying to operate in the chaotic and nonstandardized environment of a modern home. It's an inherently more difficult problem to design a product that will work flawlessly in a wide variety of different home types.
And this is a reason to be skeptical that we'll see rapid progress in household robotics or smart homes in the coming years. It has proven difficult to build a robot vacuum cleaner or a smart thermostat that's a big hit with the public. And other home automation tasks — like iRobot's mopping robots — have been even less popular than that. The concept of smart homes and cleaning robots sounds appealing in theory, but making it useful in practice is surprisingly difficult.
In a recent interview on Fox Business, Donald Trump said that ordinary Americans should not put their retirement savings into the stock market:
"I don't like a lot of things that I see," he said. Trump said that he had personally gotten his money out of the stock market and urged ordinary Americans not to put their 401(k) funds into stocks.
Put simply, this is terrible advice. If you’re under 50 — and have another couple of decades before retirement age — you should put around 80 percent of your retirement savings in the stock market. (Read the Vox guide to retirement for all the details on how to do it.) If you don’t, you’re likely to wind up with a much smaller nest egg when you reach retirement age.
Public attitudes toward the stock market soured in 2008, when the market plunged by 37 percent. A lot of people concluded that the stock market was too risky for ordinary people to invest in.
But the stock market doesn’t look so scary if you take a longer-term perspective. Here’s a chart of the average return of the S&P 500, an index of 500 large American stocks, over 20-year periods for the past century:
For example, if you invested in the stock market between 1994 and 2013, your average rate of return would have been 6.6 percent. The thing to notice about this chart is that there has never been a 20-year period when stock market investors lost money. The worst period, between 1962 and 1981, produced an average return of a little less than 1 percent. The best period, between 1980 and 1999, produced an average return of more than 13 percent — after adjusting for inflation.
If you had the bad fortune to buy stocks in January 1929 — on the eve of the Great Depression — you would have earned your money back and then some by the end of 1948. And that was followed by a massive stock market boom in the 1950s.
Of course, past performance is no guarantee of future results. And the United States has had an unusually successful economy over the past century. But the data shows that stocks in most developed countries have produced positive returns — and returns significantly higher than you could get from government bonds — over the long run. The exceptions are countries that have experienced communist revolutions (Russia and China) or lost major wars (Austria and Germany).
Not every country has seen its stock market rise as quickly or as steadily as the United States over the past century. And there have been a few cases (like Japan between 1990 and 2010) when countries have seen negative returns over multiple decades. But these cases are relatively rare. Around the world, people who invested in stocks over the long run have usually come out ahead of those who invested in bonds or — even worse — invested in gold or put their money under a mattress.
And that's not a coincidence. Some people think the main way to make money in the stock market is to buy stock when it's undervalued and then resell it when the price rises. And it's true that in the short run, stock price fluctuations account for most of the gains investors enjoy.
But these fluctuations don't matter as much for long-term stock returns. Stocks produce positive returns because companies earn profits that they give to their shareholders, either by paying dividends or by buying shares back from shareholders (which itself pushes up stock prices). This steady flow of cash to shareholders helps ensure that stocks produce positive returns over the long run whether stock prices go up or down. Historically, that has produced an average return of 6 to 7 percent per year.
When you buy a broad portfolio of US stocks, you're essentially buying a stake in the long-term success of the American economy. Of course, there's no guarantee that the American economy will continue to prosper — a major war, revolution, asteroid strike, or other calamity could destroy the American economy and with it the value of American companies. But as long as the American economy continues to grow, companies will continue earning profits and patient investors will enjoy a healthy rate of return.
Trump’s most plausible argument against investing in the stock market is that stock prices are propped up by low interest rates.
"If interest rates every seek a natural level, which obviously would be much higher than it is right now, you have some very scary scenarios out there," he said. "The only reason the stock market is where it is is you get offered free money."
Trump is right — sort of. Falling interest rates do tend to push up the value of all assets, including stocks. If interest rates start to rise, it could cause stock prices to decline.
But this issue isn’t specific to the stock market — it applies to almost everything you might invest in. If rising interest rates push down stock prices, they’ll also push down the value of bonds, real estate, or any other income-generating assets. So unless you want to put your money under a mattress — a strategy that’s guaranteed to underperform stocks and bonds over the long run — there’s no way to avoid the risk that higher interest rates could lead to lower asset prices.
But you also shouldn’t worry about this too much. While interest rate hikes could push down stock prices in the short run, they’re associated with higher rates of return over the long run. So if you’re able to hold on to your stocks for a couple of decades, you’re likely to come out well ahead.
The growth of the US economy keeps falling short of expectations. On Friday, we learned that the US economy grew at an inflation-adjusted rate of 1 percent in the first half of 2016. That’s the slowest six-month growth rate since 2012, and it continues the slow growth that has characterized the recovery since 2009.
The weakness of the recovery has been surprising because conventional economic theory says that the bigger an economic downturn is, the bigger the subsequent boom will be. And the 2009 recession was the worst in decades, so post-2009 growth should have been massive.
Instead, the US economy has turned in its weakest performance in decades. Since 2009, inflation-adjusted output has barely grown at 2 percent per year. Business investment has been weak, wages have been stagnant, and worker productivity has improved at its slowest pace since World War II.
We’re not in a recession — the economy has grown and unemployment has fallen to a healthy 4.9 percent. But the economy isn’t delivering the kind of rising prosperity previous generations took for granted.
So what’s going on? It’s one of today’s most important economic questions, and there’s no consensus among economists. Here are eight of the leading theories.
The 20th century saw the development and widespread adoption of a ton of important new inventions: automobiles, airplanes, air conditioning, antibiotics, refrigerators, televisions, PCs, cell phones, and so forth. Yet with the notable exception of the smartphone, it’s hard to think of any major new technologies invented since 2000. Many aspects of our modern lives, from our kitchens to our streets, look little changed from 1996 or even 1976.
In a new book, economist Robert Gordon argues that this slowdown in new inventions is the root cause of the last decade’s economic malaise. He views the IT-driven boom between 1995 and 2005 as a one-time event that’s unlikely to be repeated. Now, he suggests, people are going to have to get used to slow growth of incomes, worker productivity, and the economy as a whole.
Two data points seem to support Gordon’s point of view: the rate of startup formation is at record lows, and so is overall corporate investment. That suggests that — perhaps — entrepreneurs and established CEOs alike are struggling to identify promising new technologies that are worth investing in.
Discussion of this theory tends to quickly get bogged down in technical jargon. But at a basic level the theory is simple: If the government gave people more money, they would spend it. And more spending would create more jobs and higher incomes.
There’s fairly broad agreement among economists that this kind of stimulus can work during a severe economic downturn. But the conventional view holds that it becomes less effective once the economy starts growing again. Indeed, Alex Tabarrok, an economist at George Mason University, argues that it’s "crazy" to believe that a lack of demand explains the slow recovery.
"The time period in which monetary policy would have been effective is long over," he says.
Once an economy reaches full employment, he argues, there’s no way for increased spending to boost economic output — you’d just get more inflation instead.  And the US unemployment rate is currently 4.9 percent, near historic lows, a sign that a shortage of demand might not be a problem right now.
But other economists aren’t so sure. Larry Summers, an economist who led President Obama's National Economic Council during Obama's first two years in office, has championed a theory of "secular stagnation," in which peoples’ desire to save money outstrips opportunities to invest it. This leads to a vicious cycle in which slow spending growth makes companies pessimistic about future growth, causing them to cut investment spending even further.
So some economists believe it’s possible that more government stimulus — either interest rate cuts from the Federal Reserve or more spending by Congress — could set off a more vigorous economic boom that could expand the real output of the economy.
This could work in two ways. First, a boom could entice workers back into the workforce. The labor force participation rate for people aged 25-to-54 plunged after 2008 and is still near its lowest level in 30 years. A booming labor market could reverse that trend.
Second, more stimulus could actually make workers more productive. "We know the same person can be engaged in higher or lower productivity activities," says economist Josh Mason. "We have a lot of evidence that when you have a high-pressure economy, you have people shifting to high-productivity activities."
When labor markets are tight, companies are going to look harder for ways to make their employees more efficient. Companies might invest more in retraining workers for higher productivity jobs or they might invest more in labor-saving devices to economize on rising labor costs.
Tighter labor markets can also help better match firms to workers, as Mike Konczal and Marshall Steinbaum argue in a recent paper. When jobs are plentiful, people are more willing to make risky job moves because they know they can always go back to their old job — or one like it — if it doesn’t work out. In contrast, in a more sluggish economy a lot of workers become locked into jobs they don’t like very much and may not be very good at because they’re worried that a job switch could lead to an even worse situation.
Another possibility: businesses have plenty of investment opportunities, but pressure from Wall Street is discouraging them from making them. This is the "quarterly capitalism" critique Hillary Clinton has talked about on the campaign trail.
Over the last couple of decades, we’ve seen the rise of activist investors that take a substantial stake in a company’s stock — usually 5 to 10 percent — and then use that ownership stake as leverage to try to get companies to change how they do business. One of the most common demands is for the company to pay more cash out to shareholders in the form of dividends or stock buybacks.
As I argued in November, there’s not much evidence for the theory that activist campaigns are hurting shareholders. Stock prices typically go up when activists target a company, and these price increases are sustained over time — suggesting that the activists create lasting value.
But it is possible that having companies increasingly focused on the narrow interests of their shareholders is bad for the broader economy. Investments in new technologies often produce broad social benefits — think about all the non-Apple jobs created by the iPhone — that are not fully captured by a company’s shareholders. So if Wall Street is pressuring CEOs to invest less in new technologies, that could be helping shareholders but hurting the rest of us.
A major factor behind the 2008 financial crisis was the high levels of debt accumulated by both individuals and businesses. When asset prices started to fall, people found themselves underwater, leading to defaults and panic-selling.
The acute phase of the 2008 financial crisis was over quickly. But economist Kenneth Rogoff has argued that the effects of these high debt levels lingered for years after the crisis. Households and businesses realized that they had excessive levels of debt. The result, Rogoff argues, has been a prolonged period of depressed demand as households and businesses focus on paying down their debts instead of buying new goods and services.
This seems like a plausible explanation for the early years of the post-2008 recovery. But at this point, it’s been almost eight years since the financial crisis. That should be more than enough time for households and businesses to get their debts down to manageable levels. So the longer growth remains sluggish, the less plausible this theory becomes.
This is probably the most popular theory on the political right: that burdensome regulations have slowed the pace of economic growth. This theory is catnip for Republicans because it places blame squarely at the feet of President Obama, who has pushed through the Obamacare, the Dodd-Frank financial regulation bill, stricter environmental regulations, and more.
Others point to burdensome regulations beyond Obama’s agenda. The White House itself has pointed to the proliferation of occupational licensing laws — for everything for barbers to florists — as an impediment to economic growth. A recent paper by economist James Bessen finds evidence that regulations have created barriers to entry that boost the profits of incumbents in certain industries while reducing the dynamism of the economy as a whole.
Yet it has proven hard to find evidence that this is a major reason for the slow recovery. "I thought that regulation was probably one of the major causes of declining dynamism," says Tabarrok, who co-authored a study on this topic in 2014. But he and his co-author were unable to find evidence to back up the theory. "The basic reason is pretty simple," he says. "You see declining dynamism in pretty much all industries."
The theory that regulation is the major factor choking off innovation is hard to square with the historical record. The US economy enjoyed its fastest growth in worker productivity in the half century between 1930 and 1970. This happens to be the period when many sectors of the economy were subject to burdensome New Deal-era regulations.
Policymakers relaxed these regulations over the course of the 1970s and early 1980s, deregulating industries like trucking, airlines, telecommunications, energy, and more. And yet, the growth of productivity and wages was slower between 1975 and 1995 than they had been in previous decades.
Tabarrok is quick to point out that this doesn’t mean regulation is harmless — it’s possible that many individual industries would be more productive with less regulation. But growing regulation does not seem to be the primary reason for the productivity slowdown of recent decades.
Housing regulations are, of course, a subset of regulations generally. But there’s reason to think they could be significant.
Big cities and their surrounding suburbs have strict regulations that effectively limit how much housing can be built in the most economically dynamic metropolitan areas. The result has been skyrocketing housing prices and — more importantly — slow population growth in metropolitan areas like San Francisco, New York, and Boston. And that’s a problem because these cities account for a disproportionate share of innovation and job growth in today’s economy:
The chart shows the growth of establishments — that is, business locations like restaurants, car repair shops, or factories — during the first five years of the past three recoveries. In the past, smaller counties tended to grow faster than larger counties. This made a certain amount of sense — large counties like Los Angeles or Dallas were already expensive and crowded places to live, so it was easier for economic growth to happen in smaller towns or outlying suburbs.
But in the latest recovery, the pattern has reversed, with large counties like Los Angeles County, Miami-Dade County, and Kings County (Brooklyn) enjoying a disproportionate share of business expansion. The same pattern holds for job growth.
Obviously, this is bad news for people who live in small towns. But it may also help to explain the slow growth of the economy as a whole. Because it’s possible that big cities would be creating new jobs at a much faster rate if there were more workers available. But big cities’ housing restrictions mean that there’s nowhere for additional workers to live.
The period since 2010 has been bad for ordinary workers, but they’ve been great for corporate America. In the last six years, corporate profits have been at their highest level — as a share of the economy — since the 1960s.
Some commentators have argued that this reflects lax enforcement of antitrust laws, which has allowed many industries to become increasingly concentrated. With larger market shares, these big firms have more power to raise prices, and they’ve also become more effective at locking new firms out of the market.
There’s little doubt that there has been a trend toward industry concentration in many important industries, and it seems plausible that this is a factor in the growing profitability of corporate America. What’s not so clear, however, is whether this is leading to slower innovation.
Two well-known advocates for the view that industry concentration is holding back economic progress are Phillip Longman and James Schmitz. But I found their arguments hard to evaluate because they seemed highly backward-looking.
Longman, for example, argues that deregulation of the railroad, trucking, and airline industries allowed industry consolidation that has, in turn, hurt smaller cities and towns. That might be true, but it’s hard to believe that it’s a major factor driving recent economic trends. After all, the fastest-growing industries of recent decades — like software and finance — are not very dependent on railroads or trucking   to get their goods to market.
For his part, Schmitz writes about the sugar industry between the 1940s and and the 1970s, the cement industry in the 1970s and 1980s, and the construction industry in the 1920s. His stories do a good job of illustrating his theoretical argument, but they don’t shed much light on whether growing industry concentration is a serious problem today.
A final theory suggested to me by Tabarrok is demographics. Americans are having fewer babies than they did in the past, and this has had two related effects: The population as a whole is growing more slowly, and the average age of the population is rising.
There’s reason to think that both trends are bad for economic growth. Younger people are more likely to pursue new ideas, take risks, and start new businesses. So an aging population is likely to lead to a less dynamic economy.
Slower population growth can also be a source of economic stagnation in its own right. A rapidly growing population means rising demand for products of all kinds — new homes, restaurants, shopping malls, and so forth. So more businesses will be started in general, which means more opportunities for experimentation. Successful stores, restaurants, and other businesses can be expanded or franchised to other metropolitan areas, allowing good ideas to spread quickly.
In contrast, in a country with a more stagnant population, starting a new business requires replacing an existing business. Even if a young person has an innovative idea for a new company, the practical difficulties of getting the business started might be too great for putting the idea into practice. And so change can only happen by convincing existing business owners to change their behavior — an inherently slower and more difficult process.
Correction: This article originally misstated Larry Summers' role in the Obama administration.
Internet access is a lot faster in some places than others. Reddit user DMan9797 made this map showing broadband speeds around the world as of 2014:


The data comes from Speedtest.net, a website that lets users test their own internet connections. It indicates that the fastest internet in the world is in Hong Kong, with an average of almost 80 million bits per second (Mbps). Other high-speed countries include Japan, South Korea, Sweden, Romania, the Netherlands, and Switzerland. The United States clocks in at number 30, with average speeds of 24 Mbps.
It's important to note that these figures are based on a self-selected sample: users visiting the Speedtest.net website to test their own broadband speeds. It seems likely that users with fast connections would be most likely to try it. So these data likely overstate average broadband speeds somewhat — other measures, for example, peg the average US broadband speed at more like 12 Mbps than 24.
Still, the map gives a pretty good sense of which countries enjoy fast internet access (mostly in Northern Europe and parts of Eastern Asia) and which ones are relegated to the slow lane (much of the developing world).

One of the most important companies of the first dot-com boom, Yahoo, has reached the end of its life as an independent company. Yahoo’s board approved the sale of Yahoo’s core business to Verizon in a deal valued at $4.8 billion. The company’s shareholders and regulators must still approve the deal — the companies expect it to close in early 2017.
The deal represents a stunning decline for a company that was valued at more than $100 billion at its 2000 peak. Yahoo was never really able to adapt its technology and culture for a post-2000 internet that was focused on social media and mobile devices, and so it steadily fell behind rivals such as Google and Facebook.
After the Verizon acquisition, signature Yahoo properties like its search engine, email service, photo sharing site Flickr, and blogging platform Tumblr will presumably continue operating. But it’s hard to imagine that Yahoo will ever again play the kind of high-profile role online that it did two decades ago.
Ironically, what ultimately forced the hand of Yahoo CEO Marissa Mayer wasn’t the dismal performance of Yahoo’s online properties so much as an investment by Yahoo that worked too well. In 2005, Yahoo invested $1 billion in one of China's hottest technology startups, Alibaba. That bet paid off so spectacularly that by last year Yahoo’s Alibaba shares accounted for the large majority of the company’s value.
Shareholders worried that Yahoo management would eventually squander this windfall to prop up Yahoo’s declining internet businesses. The problem was that if Yahoo sold off the shares and gave the money to shareholders, it would trigger a massive tax bill. So instead of selling the Alibaba shares, Mayer was forced to sell the rest of the company, effectively putting herself out of a job.
It’s a humiliating end for Mayer, a Google veteran who joined Yahoo in 2012. Her turnaround effort didn’t work, and now Yahoo will be folded in with AOL, another struggling internet brand that was acquired by Verizon last year.
The most successful companies in Silicon Valley — including Google, Facebook, and Apple — have an intensely technology-focused culture. These companies are obsessive about hiring the most talented engineers (and in Apple's case, designers) so they can build the best technology products. And this culture tends to be self-perpetuating — very skilled, highly motivated people like to work with other very skilled, highly motivated people. Once you have a critical mass of such people, it becomes easy to recruit more of them.
Yahoo never had the same kind of obsessive focus on recruiting technical talent. Paul Graham, a well-known Silicon Valley investor who sold his company to Yahoo in 1998, has written that even in the late 1990s, Yahoo was ambivalent about its status as a technology company.
"One of the weirdest things about Yahoo when I went to work there was the way they insisted on calling themselves a 'media company,'" Graham wrote. Yahoo employed a lot of programmers and produced a lot of software, of course — and still does. But it never made software as core to its identity as some of its major competitors.
That's probably because at the time Yahoo was founded, in 1994, no one had ever heard of an ad-supported software company. Back then, software companies sold their products in shrink-wrapped boxes at Best Buy. Yahoo had the same business model as CNN and the New York Times — build up a large audience and then make money by selling ads — so it was natural for Yahoo to think of itself as being in the same industry. But one consequence of this was that Yahoo didn't focus as much as it could have on recruiting the best programmers.
Marissa Mayer's roots are as an engineer at Google, and she made an effort to beef up Yahoo's technical talent. She instituted a more rigorous hiring process, and the company worked hard to hire more computer scientists, especially from top universities.
But there's little sign that these moves have changed the culture or improved morale among Yahoo's programmers. "I just try to ship products that I’m not ashamed of," a Yahoo executive told the New York Times in December. This is not an attitude that tends to produce excellent products.
At the same time, Mayer doubled down on the "media company" side of Yahoo's personality. In 2013, she hired television news anchor Katie Couric for Yahoo's news site. Couric's contract was renewed last year in a deal reportedly worth $10 million. Mayer also recruited gadget reviewer David Pogue from the New York Times to anchor Yahoo's relaunched technology news section.
But despite these investments, Yahoo didn’t have nearly the prestige of a New York Times or a CBS. The company was seen as something of an also-ran both in Silicon Valley and in the media world. Yahoo created technology products that people use and media properties that have an audience, but its attempt to be a technology company and a media company simultaneously resulted in an organization that was less than the sum of its parts.
In the past few years, Yahoo's media and tech businesses were overshadowed by a third line of business: venture capital. At the same time that Yahoo's core business was in decline, its Alibaba investment was soaring in value. Indeed, earlier this year if you subtracted the value of Yahoo's major assets from the total market value of the company itself, you got a large negative number.
The uncharitable way to interpret this is that the core Yahoo business was actually destroying value. It's possible Mayer could have increased her stock price by simply announcing that she was shutting down all of Yahoo's websites and laying off all of its employees.
But there's another major factor in Yahoo's depressed share price: taxes. On paper, Yahoo's Alibaba share was worth around $25 billion. However, if Yahoo ever tried to sell its stake and pay out the proceeds to shareholders, it would have owed billions of dollars in taxes to the IRS.
After adjusting for these tax liabilities, it's possible to get a positive number for the value of Yahoo's core business. But it's still a small number. When Bloomberg’s Matt Levine crunched the numbers in December, he concluded that Yahoo's core businesses were worth just $1.7 billion, about 5 percent of Yahoo's overall market value at the time.
So Yahoo's search engine, email service, news site, and other properties might not literally be worth less than nothing. But the stock market didn’t seem very optimistic about the chances.
The big fear of Yahoo's Wall Street critics wasn’t just that Yahoo management would fail to turn a profit; it was that they'd burn up billions of dollars in a futile effort to turn Yahoo around. Yahoo had enough cash in the bank to continue its current losses for a few more years, and after that it could have sold its Alibaba and Yahoo Japan stakes to buy itself more years of money-losing operation.
But while Yahoo's management and employees obviously like to have a big cash cushion, shareholders weren’t interested in endlessly subsidizing a money-losing business. And so last year, Wall Street started to ratchet up the pressure on Mayer to separate Yahoo's core internet business from its stakes in Alibaba and Yahoo Japan.
To mollify Wall Street, Mayer announced a plan last year to spin off Yahoo's Alibaba shares into a new holding company. Under tax law, a company can spin off part of its business tax-free if it's doing so for a legitimate business purpose, but it can't do so merely as a tax dodge. In the past, the IRS hasn't enforced this rule very strictly, but when Yahoo asked the IRS to bless its spinoff proposal, the IRS demurred. That meant Yahoo could face a multibillion-dollar tax bill. So in December, Yahoo announced that it was canceling the spinoff.
In a January letter, the hedge fund Starboard Value was scathing about Mayer's performance. "The management team that was hired to turn around the Core Business has failed to produce acceptable results," the firm wrote.
So Starboard urged Yahoo's board to sell Yahoo's core business to another company. That would provide Yahoo shareholders with several billion dollars in cash while avoiding tax liability for the Alibaba shares.
Starboard wasn't just making a suggestion. Starboard is an activist investment firm that buys a significant stake in company shares and then uses it as leverage to force management to make changes. In 2014, for example, Starboard successfully ousted the management of the Olive Garden after writing an epic 300-page slide deck criticizing the company's management.
Starboard threatened to take that same approach at Yahoo. "If the Board is unwilling to accept the need for significant change," the company wrote on January 6, "then an election contest may very well be needed so that shareholders can replace a majority of the Board with directors who will represent their best interests."
The threats worked. Mayer began shopping Yahoo around to potential buyers, and Verizon emerged as the leading contender.
One reason Verizon was a strong candidate to acquire Yahoo is that the company has done this before. Verizon bought another struggling internet company, AOL, last year. And AOL has a lot in common with Yahoo. So the lessons Verizon learned from its AOL acquisition could prove valuable as Verizon digests Yahoo.
Both AOL and Yahoo are well-known internet brands whose best days are a decade or more in the past. Like AOL, Yahoo makes a lot of its money by creating internet content and selling ads against it.
When Verizon purchased AOL, it emphasized the company's portfolio of media brands, including TechCrunch and the Huffington Post. But as Matt Yglesias wrote for Vox last year, Verizon may have also been interested in AOL's ad technology business — and in particular how Verizon could use data gathered from its vast broadband and mobile networks to help AOL content companies target ads more effectively.
Either way, if Verizon was happy with its AOL acquisition, buying Yahoo, a company with a similar portfolio of technology, media, and advertising products, seems like a logical next step.
In recent years, scale has become increasingly important in the online advertising business. Advertisers prefer to make a few big ad deals rather than many small ones, and larger media companies are often able to command premium prices. With Yahoo and AOL under one roof, Verizon will be able to integrate their ad sales teams and offer advertisers packages that include media brands from both companies.
According to Yahoo's release about the deal, "Yahoo will be integrated with AOL under Marni Walden, EVP and President of the Product Innovation and New Businesses organization at Verizon" — suggesting that Mayer may end up out of a job, with the combined AOL/Yahoo unit reporting to Tim Armstrong, who was AOL's CEO prior to the Verizon sale and was asked to stay on afterward.
Elon Musk, CEO of the electric car startup Tesla, is one of the most ambitious businessmen on the planet. Tesla is the first successful American car startup in decades, and its first two cars — the Roadster and the Model S — have dramatically raised the profile and prestige of electric vehicles.
In a Wednesday evening blog post, Musk signaled that he was just getting started. He wrote that he has an even more ambitious project road map to follow next year’s release of the mid-market Model 3.
Musk wants to merge Tesla — which makes batteries in addition to cars — with the solar panel company SolarCity to offer integrated in-home power systems. He wants to expand into self-driving trucks and self-driving buses. And he signaled that he plans to compete directly with Uber by enabling Tesla owners to rent out their cars to other passengers when they’re not in use.
Taken individually, each of these ideas seems like a reasonable direction for an electric car company to go. But Musk isn’t planning to take them individually. He’s planning to pursue all of them simultaneously — at the same time as he faces growing questions about his ability to deliver the Model 3 by its late 2017 target date.
Maybe Musk will surprise us once again and execute on this ridiculously ambitious plan. But there’s also a big risk that he's bitten off way more than he can chew.
While Tesla is known for its innovative designs, it’s not known for its flawless execution.  All three of the company's previous vehicles faced production delays before they were finally delivered to customers. And there are mounting questions about whether Tesla can meet the Model 3’s 2017 deadline.
The problem, as industry analyst Edward Niedermeyer told me last month, is that the higher volume and lower price of the Model 3 makes it a much bigger production challenge. Tesla has not had a great track record for quality and reliability, but Niedermeyer argues that high-end customers tend to be more forgiving of quality flaws since they typically have another car to fall back on (or can afford to hail a taxi) if one of their vehicles breaks.
Also, producing cars at mass-market scale requires a ton of capital. Earlier this year, Tesla announced that it would raise $2 billion to fund expanded production facilities for the Model 3 — and the company might need all that cash and more to get its Model 3 assembly line up and running by next year.
All of which means that Tesla doesn’t seem like a company with a lot of spare capacity to tackle additional projects. Musk has vowed to put his desk at the end of the assembly line and sleep there while he sorts out the company’s production delays. That seems like a smart move, but a CEO who is focused on fine-tuning the company’s assembly line does not seem like a guy who will have a lot of spare bandwidth for tackling ambitious new initiatives in home power, ride-hailing, and self-driving.
So we definitely shouldn’t expect Musk to deliver on his ambitious vision on time or under budget. There will be setbacks along the way, and there’s a real danger that Wall Street will grow tired of these problems and refuse to give Musk more capital.
That said, the case for optimism here is twofold. First, if anyone can deliver on an agenda this ambitious, it’s Musk. He is already doing something most people would find impossible — running Tesla and another startup, SpaceX, simultaneously — so perhaps his impressive management abilities will stretch to allow him to manage a few additional projects.
Second, the markets are hungry for investment opportunities with high potential returns. One of the biggest problems facing the US economy right now is a shortage of high-return investments for the trillions of dollars Americans (and foreigners wanting to invest in the US) have accumulated. So if Musk can convince the markets that he will eventually deliver on his lofty promises, they might be willing to cut him a lot of slack — and give him a lot more money — along the way.
And Musk has one other advantage: Most of Tesla’s competitors are in an even worse position to deliver on the future of cars than Tesla is. Tesla’s roots in Silicon Valley give the company a leg up over old-fashioned car companies that lack the software expertise that will be required to succeed in self-driving and ride-hailing technology. Meanwhile, technology companies like Apple, Google, and Uber lack practical experience building self-driving cars.
So Tesla’s road map will likely prove slower and harder than anyone thought. But Tesla has unique advantages that could allow it to stay a step ahead of the competition. And as long as Musk is doing that, he should be able to find investors willing to bank on his eventual success.
A key job of the president is to choose the chair and other key officials of the Federal Reserve’s Open Market Committee. The Fed is way more powerful than most people realize: If the Fed raises interest rates at the wrong time, it can trigger a recession that can destroy a president’s career. (George H.W. Bush blamed his 1992 loss on Fed Chair Alan Greenspan, for example.)
Since 2008, there’s been an important debate about whether the Fed has been doing too much or too little to support the economy. Many economists believe that the Fed’s quick action in late 2008 saved the country from a more severe economic downturn. But critics, including many conservatives, argue that low interest rates between 2008 and 2015 have been counterproductive.
Personally, I think the loose-money side of the debate is right — we’re lucky the Fed took decisive action in 2008, and it probably should have done even more. And this is a lot more than an academic debate, because the Fed is going to face the same choice during the next economic downturn. If the Fed does too little to support the economy, it could make the next recession a lot more severe than it needs to be.
As often happens, Donald Trump has had conflicting views on the topic. Back in November, he argued that "Janet Yellen should have raised the rates" — a tight-money policy that could cause the economy to slow down. Then in May, Trump took a seemingly contradictory position, saying that Yellen "is a low interest rate person, she’s always been a low-interest-rate person, and let’s be honest, I’m a low-interest-rate person."
But while Trump’s views on monetary policy are vague and seemingly contradictory, his running mate Gov. Mike Pence’s views are clear and — for fans of monetary stimulus — rather ominous.
"During 2008 and 2009, the Fed pushed well over $1 trillion into the financial system in an attempt to rein in unemployment through more government stimulus," Pence argued in a November 2010 speech. "Yet the national jobless rate has been well above 9 percent for a record-tying 18 straight months."
"Printing money is no substitute for sound fiscal policy. The Fed can print more money, but it can’t print jobs."
Right now, the Fed has a mandate to boost employment while also keeping inflation under control. In his 2010 speech, Pence called on Congress to drop the employment half of the Fed’s mandate. That, presumably, would mean that the Fed would do less to contain the next recession, which could lead to greater job losses.
At the time, many conservative commentators were warning that the Fed’s easy money policies would bring about high inflation rates. Instead, the opposite occurred: Inflation rates have been below the Fed’s 2 percent target for most of the past eight years. That suggests that the Fed’s efforts to boost the economy were not as costly as Pence feared at the time.
In the same speech, Pence also flirted with returning to the gold standard. "The time has come to have a debate over gold and the proper role it should play in our nation’s monetary affairs," he said. Many economists believe the gold standard played a central role in worsening the Great Depression. Even free-market economist Milton Friedman argued that a return to the gold standard would be a mistake.
In his 2010 speech, Pence was echoing what had become the orthodox conservative line. Ever since Fed Chair Paul Volcker brought inflation under control during the administrations of Jimmy Carter and Ronald Reagan, hawkishness against inflation has been a standard conservative position.
But in the wake of the 2008 financial crisis, this position has become more contested.
Hawks kept predicting inflation, and it kept not happening. So some conservative and libertarian thinkers began to rethink the hawkish position on monetary policy. Ramesh Ponnuru, a prominent conservative writer, was one of the first conservatives to argue that money had been too tight in the wake of the 2008 crisis.
His frequent co-author on monetary topics, economist David Beckworth, was recently hired by the libertarian Mercatus Center to write about monetary policy, where he joined economist Scott Sumner, another early advocate of the view that money was too tight in 2008. James Pethokoukis, an economist at the conservative American Enterprise Institute, has made similar arguments.
But while this point of view is gaining support among right-leaning intellectuals, it’s not clear how much traction it has among rank-and-file Republican voters. Many Republicans are old enough to remember the high inflation of the 1970s. And despite decades of falling inflation, they still believe that vigilance against inflation should be the Fed’s highest priority. Trump’s choice of Pence could help to cement that conventional wisdom among Republican elites for another four years.
To make progress in Pokémon Go, a mobile game that’s taken over America in the past week, you can’t just sit on your couch. You have to get out of the house and travel around town. Some Pokémon can only be found in watery locations. Others are only available at night.
When you find a Pokémon, the game shows the creature superimposed on your actual surroundings. Move your phone around, and the animated image moves too, making it appear as though the Pokémon is located in the real world.
This technology of blending the physical world with virtual objects is known as augmented reality, and it’s destined to become a huge deal over the next decade.
But before it can do that, the technology has to get better. That’s partly about better software, but augmented reality won’t really come into its own until companies like Apple and Samsung create better hardware for seamlessly merging the real and virtual worlds.
Pokémon Go is augmented reality in only the most superficial sense. When I first started playing the game yesterday, a Pokémon showed up on top of my desk. Then when I backed up, it slowly slid toward me until it appeared to be bouncing around on the floor under the desk. As I walked over to the window, it appeared to be floating through the air and then bouncing on the ledge outside.
In short, the creature didn’t seem to be very firmly connected to the physical world. And that broke the illusion augmented reality tries to create.
The basic problem here is that my iPhone is only aware of its location and surroundings in the crudest sense. It has a GPS chip, a compass, and an accelerometer that tells it very roughly where I’m located and which way my phone is pointing. But the iPhone’s camera doesn’t have the ability to recognize or precisely track objects — something that human beings take for granted.
Technology companies have been working on technology to fix this problem. Last year, Intel introduced RealSense, a new category of chips that allow the creation of 3D photos. A RealSense-equipped phone would not only capture a 2D image of my desk but would also instantly capture its three-dimensional shape (using either a range-finding sensor or a pair of cameras that work together).
This kind of advanced sensor would enable apps like Pokémon Go to offer much more realistic augmented reality. Pokémon Go would know the exact size, shape, and location of my desk, allowing the game to appropriately scale the Pokémon image and place it in a precise location in 3D space. If an object passed in front of the Pokémon, the augmented reality camera would recognize this and render the nearby object in front of it.
A character in this more sophisticated version of Pokémon Go could also interact with its surroundings. It could leap off my desk onto the floor and start walking down the hall, or hide under the desk.
While games are the most obvious application for augmented reality, they’re far from the only one. For example, if you had a smartphone with a built-in 3D sensor, you could take a photo of a room and have it automatically compute the size of objects in the shot. Ikea might make an app allowing you to visualize how new furniture would look in your living room. Or an architect could show a client proposed changes as they walk through a building.
In the 18 months after Intel’s RealSense chips were announced, there hasn't been a lot of progress in integrating this kind of technology into smartphones. But I think it’s only a matter of time before this technology becomes commonplace. Google has a project called Tango that aims to bring augmented reality to Android phones.
But there still seems to be room for Apple to set the standard for smartphone augmented reality.
Building a high-quality augmented reality platform will require combining hardware and software innovations. Smartphones will need both 3D sensors and a software platform that allows app developers to make effective use of them.
Apple is the only company that makes both a popular smartphone and a mobile operating system (the iPhone and iOS). That means Apple is the only company that can guarantee to app developers that there will soon be millions of phones out there with both hardware and software support for its augmented reality platform.
This kind of hardware-software integration has long given Apple a leg up in pushing new technologies like Apple Pay and TouchID. I’d love to see the company follow the same approach to bring better augmented reality to a future iPhone.
Update: I added information about Google's Project Tango.
The interest rate the US government pays to borrow money for 10 years fell below 1.4 percent last week. Those are the lowest borrowing costs Washington has faced — ever. This chart from Quartz tells the story:
<picture class="c-picture" data-cid="site/picture_element-1502714045_9530_7543" data-cdata='{"picture":true,"dynamic_picture":false,"convert_picture":false}'>

  

  

  <img srcset="https://cdn.vox-cdn.com/thumbor/0a9f45tjE2OWOm9ycHTFM7nIJKU=/0x0:640x360/320x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 320w, https://cdn.vox-cdn.com/thumbor/106Mu5I85lgdfKsMMl-DPHeKz44=/0x0:640x360/520x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 520w, https://cdn.vox-cdn.com/thumbor/gbx4TQYUoMANOAhDqAyIlS3WFJw=/0x0:640x360/720x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 720w, https://cdn.vox-cdn.com/thumbor/YU3lVf2c4mEap9JUz-VXy6q0xIg=/0x0:640x360/920x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 920w, https://cdn.vox-cdn.com/thumbor/fEsf2G2GU6JTUhJdHWRNz7-werU=/0x0:640x360/1120x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 1120w, https://cdn.vox-cdn.com/thumbor/LEg4BaGe_xH3YXAP3cCbaj3DcSM=/0x0:640x360/1320x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 1320w, https://cdn.vox-cdn.com/thumbor/9R-QRCz0tON40VyZDiqUcKdgypI=/0x0:640x360/1520x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 1520w, https://cdn.vox-cdn.com/thumbor/XbDIB_Tp624pnIpyMpTMz3l1ew4=/0x0:640x360/1720x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 1720w, https://cdn.vox-cdn.com/thumbor/finPp5Y_iT6HzPGiKBG89azSltE=/0x0:640x360/1920x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif 1920w" sizes="(min-width: 1221px) 846px, (min-width: 880px) calc(100vw - 334px), 100vw" src="https://cdn.vox-cdn.com/thumbor/SsVsXuu8Jvbe3TQA2JSAlF__WB4=/0x0:640x360/1200x0/filters:focal(0x0:640x360):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/6773427/yield_on_us_10-year_government_bonds_close_chartbuilder.0.gif">

</picture>
The falling interest rates of the past decade have surprised a lot of people. When I bought a house a year ago, people told me I should hurry up and close a deal before interest rates rose again. (Mortgage rates and government bond rates are different, but they tend to move up and down together.) Instead, mortgage rates have fallen further.
For almost a decade, people have been predicting that rates would rise soon — even as they kept falling and falling. Many people still assume it’s only a matter of time before rates start drifting back up toward "normal" rates of 6 to 8 percent (for mortgages) and 4 to 6 percent (for government bonds).
But looking at interest rates over a longer time period suggests a different possibility: Today’s interest rates are normal.
Suppose you showed someone a chart of government 10-year borrowing costs from 1800 to 1960 and asked them to project rates in the 2010s. They would probably observe the fairly steady downward trend and predict that rates in the 2010s would be very low — perhaps around 1.5 percent, where they stand today.
The reason people have such a strong intuition that 6 percent interest rates are normal and 1.5 percent interest rates are abnormal is that the human life span is only about 80 years. Few people alive today remember the ultra-low interest rates of the 1940s, and no one today remembers the steadily declining interest rates of the 1800s. But lots of people remember that interest rates were a lot higher than the 1970s, '80s, or '90s, and they’re expecting those high rates to come back.
But there’s good reason to think that the period from 1960 to 2010 was an anomaly driven by the high inflation rates of the period. Bond markets in the 1970s worried that inflation would erode the value of their investments, so they demanded higher interest rates to compensate. But when inflation rates started to fall in the 1980s, interest rates fell too.
And if you think about the big picture, falling interest rates make perfect sense. Like any price, interest rates are driven by supply and demand. And America is getting richer and richer, driving up the supply of capital. So of course the price of capital is going to fall.
The newspaper publisher formerly known as Tribune Publishing has rebranded itself "Tronc" as part of an ambitious plan to reinvent itself as an online news organization. A key plank of its new strategy: producing 2,000 videos per day.
You might think that sounds unrealistic — the Vox.com video team, for example, has produced only a few hundred videos in its two-year history — but a New York Times article explains how Tronc expects to accomplish the goal: automation. Tronc is going to deemphasize the traditional, labor-intensive process for producing video content in favor of software tools that allow it to churn out hundreds of videos per day with minimal human involvement.
Tronc’s business logic is straightforward: Advertisers pay top dollar to run ads before videos. So if Tronc can produce a lot of videos, it will be able to sell a lot of high-dollars ads and make a lot of money.
"Video is video," Tronc’s chief technology officer, Malcolm CasSelle, told the Times. "We’re producing it because it’s strategic and important."
The strategy Tronc is pursuing here isn’t really new. For at least a decade, some media companies have churned out text-based articles in a process known as "content farming." Tronc appears to be updating this strategy for the video age. The problem is that content farming has been a total disaster for companies that have tried it.
A company called Demand Media provides an object lesson in the dangers of this strategy. Starting in 2006, it produced a huge amount of content optimized for prominent placement in search engine results.
Demand Media paid writers a fraction of what more traditional outlets would pay — one writer reported making between $7.50 and $15 per article in 2010, a rate that hadn’t changed significantly by 2015. Obviously, the only way to make a living at this is to write extremely quickly, churning out one or more articles per hour. These articles would appear on Demand-owned websites like eHow.
Articles written this quickly aren’t very likely to be good. But Demand Media was betting this wouldn’t matter very much. Demand Media’s goal was to rank highly in Google, and Google’s search algorithms can’t tell the difference between a well-reported, in-depth article and one dashed off in an hour.
Traditionalists wrung their hands, but for a while it looked like Demand Content’s model was working. The company even signed a syndication deal with Hearst Newspapers in 2010 to provide some content for the San Francisco Chronicle and the Houston Chronicle. Demand was briefly worth more than $1 billion after its 2011 IPO.
But then disaster struck. Or, more specifically, Google’s search quality engineers struck. Because while Google’s algorithms couldn’t tell the difference between a bad article and a good one, its human programmers sure could. A few weeks after Demand Media’s IPO, those engineers tweaked Google’s algorithm to downgrade spammy sites. Within days, Demand Media’s eHow site lost more than 50 percent of its traffic, and other content farming sites saw even more severe traffic declines.
Demand Media never really recovered. The company is still around, but its stock has plummeted, and nobody thinks its business model represents the future of online content.
The broader lesson here isn’t just that media companies should stay on Google’s good side. It’s that quality actually matters — even if it doesn’t look like it in the short run.
For a little while, you can generate traffic by stuffing garbage articles full of relevant keywords. If you’d looked at Demand Media’s traffic and revenue data around 2009, you could have fooled yourself into believing the company had found a new model for producing content profitably.
But people aren’t stupid. They gradually realized that articles on sites like eHow were no good. Google realized that its users didn’t want to be shown a bunch of garbage articles when they searched, so it changed its algorithm to show people articles they were more likely to actually enjoy and value.
Tronc’s "video is video" strategy appears to be updating the Demand Media business strategy for the video age. And it’s probably true that in the short run, a low-quality video with a catchy title and cover image will generate as many clicks as a high-quality one.
But as with Demand Media’s content, you can’t fool people for very long. If Tronc videos aren’t good, Tronc’s audience will figure it out and stop clicking on them. Facebook will figure it out and stop putting them in people's newsfeeds. Advertisers will figure it out and take their money elsewhere.
And while Demand Media started from scratch, Tronc is starting with high-quality brands like the Los Angeles Times and the Chicago Tribune. Stuffing their articles full of low-quality video isn’t just going to fail to produce a sustainable business model — this spammy video business model is likely to tarnish these newspaper brands for years to come.
In the United States, we view the economic crisis that started in 2008 as something that ended years ago; the story now is the slow pace of recovery. But in Europe, the crisis genuinely hasn’t ended. Greece had a major debt crisis as recently as last year, and Spain is still suffering from an unemployment rate above 20 percent.
Last month’s vote for Brexit has led to a renewed flare-up of anxiety about the European financial system, and it looks like Italy may be the next European country to face a crisis. Italian banks have about $400 billion in bad loans on their balance sheets, and markets have started to panic. One Italian bank has lost 80 percent of its value over the past year, and observers worry that some banks could be on the verge of collapse.
Italy’s prime minister, Matteo Renzi, wants to use about $45 billion in Italian taxpayer funds to shore up Italian banks and head off an economic crisis. But his plan runs afoul of European Union rules, which prohibit countries from bailing out their banks without making the banks’ investors pay first.
This is exactly the kind of rule that bailout critics in the United States and elsewhere have demanded since 2008. They want to make sure that investors, not taxpayers, foot the bill for bank failures. But there’s a danger that strict adherence to this principle could prevent the government from taking measures to contain a financial crisis that could have much larger costs in the long run.
Italian banks are facing a problem that’s broadly similar to the problem some American banks faced in 2008. They made a lot of loans to people who aren’t paying them back — a situation that’s been made worse by years of weak economic growth in Italy.
Renzi is worried that this will lead to a collapse of several major Italian banks, which could trigger a broader financial crisis. So he wants to organize a government bailout of Italian banks, injecting $45 billion into the banks to provide the cushion they need to ride out a wave of loan defaults.
This is the kind of bailout that the United States and many other countries orchestrated in the recent past, but Renzi has a problem doing it for Italy: Relatively new EU rules prohibit governments from doing this kind of no-strings-attached bailout. Under European law, a bank’s own creditors — investors in the banks’ bonds — must take losses before the government can spend taxpayer money shoring up the bank's finances.
That’s exactly what critics of America’s TARP bailouts wanted to happen in 2008. They said it wasn’t fair to make taxpayers pay billions of dollars to bail out a bank while people who made loans to risky banks get 100 cents on the dollar.
They also argued that making creditors pay before taxpayers would create an incentive to do due diligence on a bank’s finances before lending it money. They thought making creditors pay would make them more wary of lending to banks making reckless investments. That, in turn, would force banks to be more prudent, making future crises less likely.
This argument assumes that a bank’s creditors are wealthy, sophisticated financial institutions that understand the risks they’re taking on. But in Italy, that assumption doesn’t necessarily hold. According to Bloomberg, 45 percent of Italian bank debt is held by ordinary Italians. That means complying with the EU rules could mean some Italians lose a big chunk of their life savings.
Renzi got a taste of the potential backlash back in December, when the Italian government rescued four banks in accordance with EU rules. Creditors took losses in the process, and one of them was an Italian man who lost $110,000 he had invested in bonds issued by one of the bailed-out banks. The man killed himself, leaving a suicide note criticizing his bank.
Renzi is understandably reluctant to repeat this experiment on a broader scale. So he’s been lobbying EU leaders for an exemption from the EU’s anti-bailout rules that would allow him to inject cash directly into Italian banks. But European leaders are unconvinced. German Chancellor Angela Merkel, the most powerful EU leader, has refused to budge, insisting that it would set a bad precedent to relax the EU’s anti-bailout rules just two years after they were overhauled in 2014.
If this were just a debate about the solvency of a few random Italian banks, there’d be no reason the rest of us should care. The concern is that an Italian banking crisis could have broader effects in the Italian economy — and potentially the rest of Europe.
The reason is that banks play a central role in modern economies. Making and receiving payments is an essential function for any business. If these payment functions were disrupted by a wave of Italian bank failures, it could have an outsize impact on the Italian economy.
And while the goal of discouraging banks from making too many risky investments seems sensible, it’s important to remember that a financial crisis can transform otherwise sound investments into money losers.
In a crisis, financial institutions tend to sell assets in order to shore up their cash reserves. But that can make things worse by pushing down asset values. Suddenly, banks that were perfectly sound prior to the crisis find their assets are worth less than their liabilities. They might be forced to start selling assets themselves to make sure they have enough cash on hand if the crisis gets worse. The result can be a downward spiral that takes down responsible banks along with irresponsible ones.
So if authorities are too fastidious about refusing to bail out banks that have been irresponsible, it can wind up taking down banks that never did anything wrong.
This is why Renzi wants to rescue banks now — before panic starts to set in, and without worrying too much about making banks’ creditors pay.
At this point, you might be wondering why EU officials care so much about Italian banking policies. If Italy’s elected prime minister wants to bail out Italy’s banks using Italians’ taxpayer money, why not just let him?
The problem is that Italy, Germany, and a bunch of other EU countries share a currency. And the shared currency effectively links their economies in other ways.
Back in 2012, investors lost confidence in bonds issued by the government of Italy and a few other countries in Europe’s periphery. Italy had a big national debt and a slow-growing economy. Financial markets worried their debts would spiral out of control — and the more they worried, the higher Italian interest rates got.
That, of course, made the Italian budget deficit even bigger, making the country’s debt problems even worse. Without the ability to print its own currency, there was a real danger that Italy would be forced to default, which could have led to a crack-up of Europe’s common currency area.
To prevent this from happening, the European Central Bank in 2012 promised to guarantee bonds issued by Italy and other eurozone countries. Because the ECB can print an unlimited number of euros, that quickly calmed markets and brought interest rates down.
But an ECB backstop for Italian debt essentially means that the rest of Europe has a financial stake in keeping Italian government debt under control. If Italy spends beyond its means, other European countries could get stuck with the tab. And so it’s not too surprising that other countries are insisting the Italian government not be too generous with its bank bailouts.
But there’s also an obvious problem with a situation in which German and French leaders get an effective veto over Italian economic policy. It’s easy for Merkel to insist that Italy must strictly adhere to EU rules regardless of their effect on the Italian economy — she doesn’t have to worry about winning reelection in Italy.
And there’s a real danger that this kind of standoff will hamper an effective response to Europe’s next economic crisis — whether it involves Italian banks or problems elsewhere on the continent. Each European leader is accountable only to voters in her own country. In cases where different countries have divergent interests, there’s no good mechanism for resolving the disagreement.
In the long run, this situation doesn’t seem sustainable. If economic policy is going to be made on a Europe-wide basis — and the euro is pushing the continent in that direction — then it should be made by a Europe-wide, democratically elected body like the European Parliament. That would mean shifting functions like taxation and bank regulation up to the EU level.
If, on the other hand, voters want to maintain national control over economic policy, then Europe might be forced to reconsider the wisdom of a shared currency.
The US economy gained 287,000 jobs in June, the strongest monthly result of 2016. The unexpectedly strong result helps put to rest fears of the US economy tipping into recession — a recession that would have been bad for Hillary Clinton’s chances of capturing the White House in November.
In recent years, the economy has gained about 200,000 jobs in a typical month. That’s more than enough to keep up with population growth, which is why the unemployment rate has been gradually declining over the past few years.
But last month, economists got a nasty surprise: The economy gained only 38,000 jobs, far below the level needed to absorb new workers. That created fears that the economy was falling into recession.
But now it looks like those fears are unfounded, as the latest number shows robust job growth:
The weak job numbers in May likely contributed to the Federal Reserve’s June decision to delay further interest rate hikes in an effort to boost the economy. And supporters of Hillary Clinton worried that a weak economy would help Donald Trump in his bid for the White House, since the party in power tends to do better when the economy performs well.
However, a single month of bad jobs data can simply be statistical noise — and that appears to have been the explanation for May’s poor results. In today’s release, the Bureau of Labor Statistics actually revised the May figure down further to 11,000 jobs. But the strong June result suggests that May’s numbers weren’t the start of a larger trend.
The latest report also includes some other moderately good news. Workers’ wages grew at a respectable 2.6 percent over the past year — slightly better than the rate of inflation:
The unemployment rate ticked up slightly, from 4.7 percent to 4.9 percent. But this isn’t necessarily a bad sign, as it partly reflects workers who had previously left the labor force being lured back into job seeking by the relatively strong economic recovery.
Years ago, I heard an argument from a guy who later went on to serve at a high level in the Obama administration that I thought was really smart and persuasive: American politics massively overstates the importance of trade policy in causing the big shock to the American economy that was induced by foreign manufacturing in the early 21st century.
The reason you can tell is that the big change in trade policy regarded Mexico (NAFTA, in other words), but the big change in terms of actual trade regarded China and the United States didn’t do anything to lower tariffs or other barriers to Chinese imports. The result is a kind of nonsensical deadlock in which people talk a lot about NAFTA, which was a real policy change, but are mostly angry about second-order consequences of trade with China, the country with which the United States runs far and away the biggest trade deficit.
The problem, on this account, is that Chinese exports surged primarily because China got better at making stuff, not because of anything that happened in American politics.
I found this convincing and have believed it for years. I’ve repeated it to other people, some of whom may also have found it convincing. And over the weekend I read a piece of research that convinced me I’ve been wrong this whole time. We didn’t lower tariffs on Chinese goods, but we did make a subtle policy change right at the end of the Clinton administration that’s made a big difference.
Here’s the subtle policy shift. Starting in 1980, Chinese-made goods were taxed at the relatively favorable rates applied to countries that enjoyed what was known in legal terms as normal trade relations with the United States. That status had to be affirmatively extended each year by Congress or it would expire, and it was extended each and every year for 20 years.
Starting in 2000 that changed, and China was granted permanent normal trade relations. That meant it would automatically keep the low tariff rates it had enjoyed since 1980 unless Congress affirmatively took them away.
Clearly, that’s a change. But since Congress really had routinely approved NTR for two decades, it never seemed to me like a particularly large change. Certainly not a big enough change to explain the wholesale transformation of the industrial landscape that’s taken place in the 15 years since it happened. Clearly the action must have been things that happened in China rather than this extremely modest shift in US trade policy.
But Justin R. Pierce from the Federal Reserve and Peter Schott from Yale’s School of Management have convinced me this is wrong.
In their paper “The Surprisingly Swift Decline of US Manufacturing Employment,” they deploy two statistical tests that reveal that the modest-sounding change was a really big deal in practice. They say the effect is plausibly large enough to explain the 18 percent decline in manufacturing employment that occurred between March 2001 and March 2007.
Pierce and Schott’s primary strategy is to exploit the fact that the gap between the NTR tariff rate and the non-NTR tariff rate varies from industry to industry. If the switch from NTR to PNTR were a big deal, we would expect to see bigger changes where the gap between the NTR and the non-NTR rates was biggest.
And, indeed, their regression analysis reveals “a negative relationship between the change in U.S. policy and subsequent employment in manufacturing that is both statistically and economically significant.” This change remains significant even when they control for changes in Chinese policy.
They also exploit the fact that the European Union, another large, high-wage economy, gave China its equivalent of PNTR many years earlier and had no significant policy change in 2000. They find “no relationship” between “the US NTR gap and EU manufacturing employment,” confirming that the correlation that exists in the American context isn’t spurious.
So why did American trade policy make such a big difference, even though tariffs didn’t fall? Pierce and Schott can’t say for sure, but they speculate that it has to do with patterns of investment.
Giving Chinese companies confidence that tariffs would stay low encouraged them to invest in production capacity aimed at supplying the American market. And giving American companies confidence that tariffs would stay low encouraged them to build supply chains around Chinese manufacturers.
Shifting production to Asia to save a few bucks on each item sold, in other words, might not make sense if there was a chance that you’d have to shift it back a year or two down the road. The old practice of keeping tariffs low but requiring the low-tariff status to be renewed on an annual basis gave US-based producers a significant amount of hidden protection. Despite the superficial appearance of openness to imports, the temporary nature of the low tariffs discouraged companies from making plans built around that openness.
Twenty years of repeated extensions of normal trade relations had only a relatively modest impact on the American economy, but making NTR status permanent led to a very rapidly displacement of American manufacturing work by Chinese imports in a way few PNTR proponents anticipated or have even acknowledged.
This election season has seen a great deal of negativity about the state of the US economy. And it’s true that economic growth since 2008 has been slower than some earlier periods in US history.
But this chart from the Organization for Economic Cooperation and Development helps to put the performance of the US economy in perspective:
The US economy has grown by a bit more than 10 percent, adjusted for inflation, in the eight years since the financial crisis began. In contrast, the euro area grew by less than 1 percent, and Japan’s economy is essentially the same size it was in early 2008.
This partly reflects a difference in population growth — the US population has grown by about 6 percent since 2008, while the eurozone grew about 2 percent and Japan’s population actually fell. But even adjusting for population, the US economy has been outperforming the economies of Japan and the euro area.
The euro area only includes countries that have adopted the EU’s common currency, the euro. It doesn’t include Great Britain, and so this slow growth doesn’t directly explain why so many Brits were anxious to leave the EU. However, this kind of anemic growth certainly didn’t help British supporters of EU membership make their case.
What explains this anemic growth? Monetary policy, in large part. The European Central Bank has made a series of blunders over the past eight years that have prolonged and deepened the eurozone’s depression. Meanwhile, Japan has been struggling with an aging population. With fewer and fewer working-age people, it’s hard to keep economic growth going.
In a blistering speech whose prepared text Donald Trump's campaign leaked to Politico early Tuesday afternoon, the Republican candidate hammers free trade policies in terms starkly reminiscent of Bernie Sanders' presidential campaign.
"Globalization has made the financial elite who donate to politicians very wealthy," says Trump. "But it has left millions of our workers with nothing but poverty and heartache."
He says he'll bring back manufacturing jobs by renegotiating NAFTA, slapping China with punitive tariffs, and a range of other measures designed to put America First and make the country great again.
American trade policy could certainly be improved upon, but the fact of the matter is that nothing Trump or any other trade skeptic proposes is going to bring back the heyday of American manufacturing jobs, for the simple reason that when you look at the data, the decline of manufacturing employment actually doesn't reflect a broader decline in the state of American manufacturing. In fact, the output — as measured in inflation-adjusted dollars — of the US manufacturing sector is higher than it's ever been, even as manufacturing employment has barely recovered from its recession-era lows:
One reason for these divergent trends is that as you might expect, the segments of the manufacturing supply chain that tend to migrate to Mexico or Asia are the ones that are the most labor-intensive and have the lowest value added in terms of complexity or intellectual property. So when factories go overseas, they tend to be unusually "jobful" factories relative to the ones that stay.
The other reason is that companies involved in manufacturing are working relentlessly to improve the productivity of their operations and do more with less labor. This is, in some respects, a cause of the relatively high wages we associate with the manufacturing sector — workers can get paid more when their work generates a lot of value. And it's in some respects a consequence of relatively high wages. If you have to pay a lot for your workers, it makes sense to invest in figuring out ways to use less of them.
Either way, the very strong implication is that any steps we take to strengthen the manufacturing sector are going to have a fairly marginal impact on manufacturing employment.
For better or for worse, the bulk of employment growth in the future is going to come from health care and other in-person services — and we're going to have to find a way to make a services-oriented economy work, not waste our time pining for the good old days of factory work.
On Thursday, Britain voted to leave the European Union— an option dubbed "Brexit." Almost 52 percent of Britons voted in favor of leaving.
Although the "leave" campaign often focused on emotional arguments about immigration, there are in fact many reasons those in favor of leaving believed it would benefit the UK. They came from across the political spectrum, and some of the arguments even contradict others. Here are seven of the most significant.
This is probably the most common argument among intellectual-minded people on the British right, expressed by Conservative politicians such as former London Mayor Boris Johnson and Justice Minister Michael Gove.
Over the past few decades, a series of EU treaties have shifted a growing amount of power from individual member states to the central EU bureaucracy in Brussels. On subjects where the EU has been granted authority — like competition policy, agriculture, and copyright and patent law — EU rules override national laws.
Euroskeptics emphasize that the EU’s executive branch, called the European Commission, isn’t directly accountable to voters in Britain or anyone else. British leaders have some influence on the selection of the European Commission’s members every five years. But once the body has been chosen, none of its members are accountable to the British government or to Britons’ elected representatives in the European Parliament.
Critics like Johnson say the EU’s regulations have become increasingly onerous:
Sometimes these EU rules sound simply ludicrous, like the rule that you can’t recycle a teabag, or that children under eight cannot blow up balloons, or the limits on the power of vacuum cleaners. Sometimes they can be truly infuriating – like the time I discovered, in 2013, that there was nothing we could do to bring in better-designed cab windows for trucks, to stop cyclists being crushed. It had to be done at a European level, and the French were opposed.
Many British conservatives look at the European bureaucracy in Brussels the same way American conservatives view the Washington bureaucracy. Gove has argued that EU regulations cost the British economy "£600 million every week" ($880 million). (Though this figure is disputed.)
This is the mirror image of the previous two arguments. Whereas many British conservatives see the EU as imposing left-wing, big-government policies on Britain, some on the British left see things the other way around: that the EU’s antidemocratic structure gives too much power to corporate elites and prevents the British left from making significant gains.
"The EU is anti-democratic and beyond reform," said Enrico Tortolano, campaign director for Trade Unionists against the EU, in an interview with Quartz. The EU "provides the most hospitable ecosystem in the developed world for rentier monopoly corporations, tax-dodging elites and organized crime," writes British journalist Paul Mason.
This left-wing critique of the EU is part of a broader critique of elite institutions more generally, including the World Trade Organization, the International Monetary Fund, and the World Bank. Brexit supporters on the left would have a lot in common with Americans who are against trade deals like the Trans-Pacific Partnership.
The United Kingdom has had a significant faction of euroskeptics ever since it joined the EU in 1973. But until recently, this was a minority position.
"There are nearly 130 Conservative MPs who have declared for leaving the EU," economist Andrew Lilico told me last week. "If you went back 10 years, you would have struggled to find more than 20 who even in private would have supported leaving the EU."
So what changed their minds? The global recession that began in 2008 was bad around the world, but it was much worse in countries that had adopted Europe’s common currency, the euro. The unemployment rate shot up above 20 percent in countries like Greece and Spain, triggering a massive debt crisis. Seven years after the recession began, Spain and Greece are still suffering from unemployment rates above 20 percent, and many economists believe the euro was the primary culprit.
Luckily, the UK chose not to join the common currency, so there’s little danger of the euro directly cratering the British economy. But the euro’s dismal performance still provides extra ammunition to Brexit supporters.
Many economists believe that deeper fiscal and political integration will be needed for the eurozone to work properly. Europe needs a common welfare and tax system so that countries facing particularly severe downturns — like Greece and Spain — can get extra help from the center.
But that makes Britain’s continued inclusion in the EU awkward. Britain is unlikely to go along with deeper fiscal integration, but it would also be unwieldy to create a set of new, parallel eurozone-specific institutions that excluded the UK.
So, the argument goes, it might be better for everyone if the UK got out of the EU, clearing the path for the rest of the EU to evolve more quickly into a unified European state.
The intellectual case for Brexit is mostly focused on economics, but the emotional case for Brexit is heavily influenced by immigration. EU law guarantees that citizens of one EU country have the right to travel, live, and take jobs in other EU countries.
British people have increasingly felt the impact of this rule since the 2008 financial crisis. The eurozone has struggled economically, and workers from eurozone countries such as Ireland, Italy, and Lithuania (as well as EU countries like Poland and Romania that have not yet joined the common currency) have flocked to the UK in search of work.
"In recent years, hundreds of thousands of Eastern Europeans have come to Britain to do a job," British journalist and Brexit supporter Douglas Murray told me last week. This, he argues, has "undercut the native working population."
The UK absorbed 333,000 new people, on net, in 2015. That’s a significant number for a country Britain’s size, though according to the CIA the UK still received slightly fewer net migrants, relative to population, than the United States in 2015.
Immigration has become a highly politicized issue in Britain, as it has in the United States and many other places over the past few years. Anti-immigration campaigners like Nigel Farage, the leader of the far-right UK Independence Party, have argued that the flood of immigrants from Southern and Eastern Europe has depressed the wages of native-born British workers. Some voters are also concerned about immigrants using scarce public services.
"One of the causes for the great public disgruntlement," Murray argues, is that Labour governments at the turn of the century "massively understated the numbers [of immigrants] to be expected," creating public distrust of current pledges to keep migration under control.
While many Brexit supporters simply want to reduce the amount of immigration overall, others argue that the UK could have a more sensible immigration system if it didn’t have the straitjacket of the EU.
EU rules require the UK to admit all EU citizens who wants to move to Britain, whether or not they have good job prospects or English skills.
"Leave" advocates argue that the UK should be focused on admitting immigrants who will bring valuable skills to the country and integrate well into British culture. They mention the point-based immigration systems of Canada and Australia, which award potential migrants points based on factors like their language and job skills, education, and age. That, "leave" advocates argue, would allow the UK to admit more doctors and engineers who speak fluent English, and fewer unskilled laborers with limited English skills.
The EU doesn’t have the power to directly collect taxes, but it requires member states to make an annual contribution to the central EU budget. Currently, the UK’s contribution is worth about £13 billion ($19 billion) per year, which is about $300 per person in the UK. ("Leave" supporters have been citing a larger figure, but that figure ignores a rebate that’s automatically subtracted from the UK’s contribution.)
While much of this money is spent on services in the UK, Brexit supporters still argue that it would be better for the UK to simply keep the money and have Parliament decide how to spend it.
The United Kingdom voted to leave the European Union on Thursday. And global financial markets, reacting to the decision, started this morning with sharp drops in London and around the world, meaning that the vote to "Brexit" caused a tremendous loss of wealth not only in the UK but throughout Europe and Asia.
London’s FTSE 100 index opened to an 8.7 percent drop, and the FTSE 250 index, which is considered a better barometer for the British economy, fell 12 percent.
The British pound is now worth just $1.37 against the dollar — an 8 percent drop and its lowest value in 30 years.
And the economic ripples have reached far beyond London. The DAX, the most widely used German stock index, has fallen 7 percent. France’s CAC index has fallen 8.6 percent. The IBEX, representing Spain, is down 11 percent:
It’s not just Europe. Japan’s Nikkei index is down 8 percent, and other Asian markets have fallen, although less dramatically. The United States is likely to be next. Trading hasn’t yet opened, but Dow Jones futures dropped 3 percent last night:
The big British banks were faring even worse. Stock in Lloyd’s and Barclays, two of the UK’s major financial institutions, were down nearly 20 percent.
Before the vote, economists predicted that leaving the European Union would be economically catastrophic for Britain. The British government estimated that the economy could shrink by up to 8 percent by 2030. Some of the biggest British banks said they’d have to shift some of their operations to other financial centers such as Paris or New York.
The governor of the Bank of England has promised to do whatever it takes to shore up financial markets, perhaps through quantitative easing or lower interest rates, the Guardian reported.
But the global impact so far is happening largely because Britain’s vote to leave is creating a tremendous amount of uncertainty — no one can say for sure how long leaving the EU will take, what kind of trade deals the UK will end up negotiating, or what the impact will be on British businesses. The European Union is the United States’ biggest trading partner, and an economic slowdown there would affect Americans as well.
It’s possible that markets will stabilize some as the day goes on. But for now, the news is bad. Bloomberg reported this morning that the investment firm T. Rowe Price estimates that a global recession is now more likely than not.
The Brexit results are in: The British people have voted to leave the European Union in a historic referendum.
This doesn’t mean Britain has actually left the European Union, but it does mean that they will leave soon. Unlike a typical nation-state, the EU does have an exit procedure that’s laid out in law. It’s not an exit procedure that’s ever been actually put to the test, so we don’t have a great sense of how it will work in practice. But the broad outline is that the referendum sets into motion a two-year period for a negotiated divorce during which time the British government and the EU are supposed to unwind their fiscal ties and lay out a new framework for how the country will relate to the trading bloc.
This could, in practice, mean almost anything — ranging from something like Norway’s extreme close relationship to the EU to something like the arms’ length relationship that Australia or the United States has.
Here’s who won and who lost.
An intriguing contrarian case for Brexit was British economist Andrew Lilico’s argument that "Leave" should win precisely in order to make it easier for the rest of the EU to integrate more deeply.
The eurozone, after all, is a bit of an odd beast. Having a bunch of countries that share a currency but don’t share a tax system or a welfare state means that when particular places fall on economic hard times, they have no escape.
A truly independent country facing the kind of loss of external investment funds that struck Spain during the financial crisis would experience rapid depreciation of its currency. The suddenly much cheaper country would become a more attractive tourist destination, more attractive maker of export goods, and more attractive magnet for a new round of foreign investment, thus helping it recover from the financial crisis.
Conversely, a member of an integrated welfare state facing a rapid collapse of its local housing market (think Florida in 2008) would be stabilized by continued federal inflow of dollars to its health care, retirement, and unemployment insurance systems. Not a bailout of debts incurred by the local government authority, but assistance to hard-hit citizens that helps them get back on their feet and keeps local businesses afloat.
Eurozone member states, however, are stuck in a strange limbo between these two poles, neither fully independent nor integrated into a common welfare state. Which means that when disaster strikes, their means of promoting economic recovery are limited.
Moving to that kind of integrated welfare state is a big project, and it clearly isn’t going to happen anytime soon. But with Britain set to leave the EU and become unable to block new measures in Brussels, it’s substantially more likely to happen.
Britain, after all, is richer than the average EU member state, meaning most of its citizens would probably end up paying slightly higher taxes for slightly fewer services under such an arrangement. Britain is also temperamentally skeptical of the case for deeper European integration — and, crucially, not a member of the euro, and thus not particularly desperate to make the euro work. A Europe without Britain will be one that’s set on a more rapid course to even deeper forms of fiscal integration.
In the short term, the vote to leave the European Union is going to commence a period of disruption and uncertainty in which foreign direct investment in the UK takes a temporary pause while investors wait to find out what the long-term situation will look like.
That, in turn, will cause the value of the pound to fall and likely prompt a recession whose severity and duration will have a lot to do with the short-term policy choices made by the UK government and the Bank of England.
This possibility was discussed extensively during the campaign, and evidently Britain’s voters decided that a little short-term turbulence was worth the possible long-term upside of getting out of the EU. The real risks to the British economy, however, concern something else entirely — the long run.
Right now, many large, multinational companies like to put their European headquarters in London. London is a great city to live in, and its English-speaking population and low-by-European-standards taxes make it an attractive base of operations since the European Union’s single market means you can do business throughout the continent from anywhere.
Leading Leave campaigners say that post-Brexit they will negotiate a trade pact with the EU that lets them retain that kind of access, but there’s no guarantee they will succeed. The French government might deliberately try to block them, figuring that if executives can’t station themselves in London they’ll fall back on Paris. The Irish government might see enormous upside for English-speaking Dublin.
Similarly, the UK’s major export industry is the financial services cluster in London. Thanks to the EU, London-based banks currently serve a continent-sized market just like New York-based ones. If the post-Brexit UK can negotiate deep market access with the EU, that will stay the same. But if it can’t, European banking will likely migrate to Frankfurt and Amsterdam. And the governments of Germany and the Netherlands may want to make sure that happens.
If you’re wondering how this entire referendum business came about, the answer is that British Prime Minister David Cameron had a problem heading into the UK’s 2015 general election. He and most of the other Conservative Party leaders, reflecting the preferences of the British business class, wanted to stay in the EU. But a growing number of Conservative Party voters, driven largely by anti-immigration sentiment, wanted to leave and were tempted to vote for the anti-European, anti-immigrant UK Independence Party (UKIP).
Cameron’s solution was to promise a referendum on EU membership if the Conservatives won the election — something the opposition Labour Party wasn’t promising.
Vote UKIP, Cameron argued, and you’d get Labour — and no referendum. The best way to get the UK out of the EU would be to vote Conservative, even though the Conservatives weren’t promising to leave.
This was a neat trick, but one that was stunningly short-sighted. Because though it succeeded in delivering an election victory for the Tories, the referendum campaign itself badly split the Conservative Party between Cameron’s "Remain" faction and a substantial chorus of pro-Leave politicians led by former London Mayor Boris Johnson, Cabinet Minister Michael Gove, and former Minister Iain Duncan Smith.
Cameron remains prime minister but he announced his intention to resign by the fall Friday morning. He’s just been humiliated, and the ambitious pro-Brexit politicians who won the referendum will seek to overthrow him with the support of Tory backbenchers. The argument that leaving the EU ought to be conducted and negotiated by people who actually believe in the policy seems pretty compelling, after all.
Even more clearly than his boss David Cameron, it’s George Osborne, the chancellor of the Exchequer, whose personal political career was on the line in this vote.
Osborne is essentially Britain’s Treasury secretary, except that because Britain isn’t a military superpower, his stature vis-à-vis the Foreign and Defense ministers is higher. Even more than that, Osborne is effectively Cameron’s number two (the UK only sometimes has a formally designated deputy prime minister, and now is not one of those times) as well as his intended successor as leader of the Conservative Party.
Losing the vote badly jeopardizes the ability of the pro-EU faction of the party to retain control. The win sets the party up for a lot more trouble and infighting, but Osborne is likely to come out ahead.
The EU is divided along a number of different lines. But one tension is between the countries whose political classes are deeply enthusiastic about European integration — primarily the six founding members, Germany, France, Italy, Belgium, the Netherlands, and Luxembourg, joined by countries like Spain and Greece that see the EU as guaranteeing the stability of their political systems — and a more diffuse set of countries that prefer a looser union.
The latter group includes Denmark and Sweden, both of which (like the UK) opted out of membership in the eurozone common currency union, as well as many Eastern and Central European countries like Poland and the Czech Republic, whose experience of Communist rule and non-experience of the three-decade social democratic growth miracle in the aftermath of World War II have bequeathed a more right-wing political culture.
The UK is the most significant country in this loose-union bloc, and their departure will significantly weaken its presence in Brussels and ensure that the "ever closer union" school of thought would win future policy battles.
Poll results have started to come in in this week's historic vote on Brexit— British exit from the EU. And it's looking like there's a real possibility that the United Kingdom will vote to leave. Many people view this as a disaster for the European Union.
But Andrew Lilico, a British economist at the consulting firm Europe Economics, argues that it's more complicated than that. He views the creation and expansion of the EU over the past half-century as a great accomplishment with benefits for both Britain and continental Europe. But he now believes it would now be better — for both Britain and the rest of the EU — for Britain to leave.
Why? Lilico believes that British exit from the EU became inevitable as soon as soon as the UK refused to join Europe’s common currency project. The euro has been an economic disaster, creating shockingly high unemployment rates in peripheral EU countries like Greece and Spain.
Lilico argues that the euro can only work well if the eurozone becomes a single integrated superstate. And he argues that the UK’s presence within the EU has become the most important obstacle to deeper European integration.
We spoke by phone last week. The interview has been edited for length and clarity.
Timothy B. Lee: A lot of Brexit supporters are critics of the EU. But you argue that the EU has been good for both Europe and Britain. So why get out now?
Andrew Lilico: We've gained considerably by being in the EU, but the sorts of gains we've made are gains we can't repeat from here.
Some of the key advantages have been geopolitical. Within the EU, we were able to participate in absorbing post-fascist states like Spain, Portugal, and Greece as liberal democracies. The EU united Western Europe against the Warsaw Pact. It absorbed post-communist states into the family of liberal democracies. Those are enormous gains. Setting aside a few remaining details, like absorbing Albania and Serbia, the vast majority of that project is complete.
Along the way Britain converted our European partners to a regulatory and economic philosophy that was aligned with ours. The EU embeds a very British, pro-market-oriented economic philosophy based on privatization, market liberalization, free trade, opposition to state aid, and opposition to protectionism. Those are also gains you can't make twice.
And as long as we remain within the EU, we are a blockage upon the EU achieving its proper destiny. The euro project was supposed to have the institutions to make it work. Because Britain is in the EU — and not part of the common currency — that has meant that the euro has not been able to take control of institutions properly. That has then turned into economic or political catastrophes in countries like Spain and Greece, where we've had 20 to 30 percent unemployment.
Once we withdraw, the EU will be able to become a unified state. It will allow the EU to grow faster, which will be to our advantage as well.
TBL: Walk me through the logic here. How does a British exit help the rest of the EU get where it needs to be?
AL: In order for the eurozone to work properly as an economic unit, it needs a proper system of fiscal transfers.
When a country has economic shocks, it can offset them by the population moving around. This happens within the US and within the UK. But this doesn’t happen very much between eurozone states. That means asymmetric economic shocks get entrenched and you end up with high unemployment in some areas.
If you want to have things function and maintain social cohesion, you don't want vast population movements. You need fiscal transfers. That means regional subsidies, benefits payments, tax breaks to allow individuals to keep going in tough times.
The EU has a system like this, but it's very small: half a percent of GDP. In the UK it's 3 to 8 percent of GDP, something like six to 15 times as big. If you're going to make the euro function, you're going to need much larger transfers from the richer parts of Europe — like Germany and Northern Italy —  to poorer parts — like Greece, Southern Italy, and Spain.
So they need eurozone-wide taxes, a eurozone treasury, an elected eurozone president, a eurozone parliament, and eurozone civil servants to manage expenditures.
It's going to be difficult to introduce those kinds of things with new treaties. But actually, you don't need new treaties, because the EU already has most of these things. We don't need a eurozone parliament, you have the European Parliament. We don't need a eurozone president because we have the president of the European Commission.
The reason this hasn’t happened is the eurozone is connected to a set of non-eurozone countries. Of these countries, all but two — the UK and Denmark — have a commitment to join the euro in the end.
Once the UK is out, there will be enormous pressure on other countries to say when they will be joining. Countries that refuse to join will get rolled together with Norway (which isn’t in the single market but has a free trade arrangement with the EU). Then the eurozone can take full control of the EU's institutions and be able to function properly to make it work as a currency union.
TBL: What makes you confident this will actually happen? Given the level of turmoil in the eurozone lately, will European countries really be prepared to give up even more power to Brussels?
AL: This kind of deep political integration is the official plan, as stated by the EU authorities. There have been public commitments by French President François Hollande, German Chancellor Angela Merkel, and the prime minister of Italy. The Future of Europe Group and all kinds of other bodies have declared that the ambition is much deeper integration.
People sometimes call it the United States of Europe. Ultimately I think that will occur because it's such a cool idea. And European officials understand that the eurozone has enormous problems. People worry that the next downturn could cause the entire system to collapse. So they know they have to do something.
European Council President Donald Tusk said recently that Brexit might bring about the end of Western political civilization. I don’t think Brexit will do that. The thing that could bring about the collapse of the EU — which really could be a threat to Western civilization — is failing to sort out the eurozone's issues. But the issues with the euro can't be addressed while Britain is part of the EU. It would be much better if the UK would get out of the way.
TBL: The recent crisis in Greece suggested to me that there might be a lot of resistance to further eurozone integration. The standoff created a lot of resentment — especially between Greece and Germany. Many Greeks thought the Germans were too stingy; Germans thought the Greeks had been too financially reckless. Can European countries really move past those feelings and continue toward deeper integration?
AL: I don't know whether Greece can survive as a member of the euro. I thought that it couldn't, but if they were going to go, they would have done it last year.
Here's a key thing to grasp. Apart from Britain being in the way, the great threat to eurozone integration is the continued insistence by commentators that the only way to do it is for the Germans to pay everyone else's debts. Europe can only proceed to the fiscal union if it's absolutely understood that that doesn't mean them being responsible for other people's debts. The more people say that, the more fiscal union will be seen as a subterfuge to pay the debts of the Italians. Germans won't do it.
But that isn't what fiscal union means. For example, for much of the past century, regional subsidies have been sent to Liverpool. If you go back 250 years, people in Liverpool were sending money to London. In the future people in Liverpool will probably send money to London again. But that's completely different from saying the Liverpool council will do whatever they want, then if they go bust, London will pay.
It's the first one that makes a currency union work. The second one — saying Germans have to be responsible for everyone else's debts — is what makes the currency union collapse. It’s a really important distinction that a lot of people in Britain and American don't grasp.
TBL: Brexit opponents argue that the UK gets a lot of benefit from being the de facto financial capital of Europe. With English being the world’s most widely spoken language, a lot of companies have established London headquarters to manage their European operations, creating jobs for Britons. Should we be worried that Brexit will cost the UK this kind of economic advantage?
AL: A good example here is the Markets in Financial Instruments Directive. There's a principle being established that if you're a non-EU country and you have regulatory equivalence to the EU, then you can sell into the EU as if you were an EU member.
As globalization extends, many of the EU's external barriers are being reduced. Many global barriers are low already. The advantages of being inside become greatly reduced.
Some people may move some staff. There could be some nontrivial tax impacts depending on where your headquarters is. But the UK is going to remain an attractive location. The vast majority of key activities will stay in the UK where we have comparative advantages in financial services and legal services.
A key reason financial services are in London is you need a lot of smart, highly motivated people. They're not going to want to live in a place with 80 percent tax regime like France. Also, it’s got to be a fun place to spend money. It’s no coincidence that London and New York are fun cities to spend money in.
Frankfurt and Geneva are not fun places for clever, highly remunerated people to be. It's not impossible that there will be some relocation. I won't discount that altogether. But I think it's easy to exaggerate how much of that kind of thing there will be.
The UK will do some different things over the medium term. The UK has been big in financial and legal services since the 1980s. Perhaps in the future it won't be as big as I would expect, but if it didn't and we became more IT-heavy or big in space ports or whatever tomorrow's exciting new thing is, why is that a bad thing?
TBL: I think your pro-EU case for Brexit is really interesting. But this position also seems somewhat marginal within the larger Brexit debate. My impression is that most Brexit supporters hold the opposite view — that the EU is a disaster that has been undermining British institutions, and that the UK should get out before it does any more damage. Am I wrong?
AL: I think there's a variety of points of view. There are a number of people who think the EU has always been bad. But that's not most of the current opposition.
For example, there are nearly 130 Conservative MPs who have declared for leaving the EU. If you went back 10 years, you would have struggled to find more than 20 who even in private would have supported leaving the EU. Until very recently, it was believed that we could renegotiate and find a way to get along with our European partners.
Now that project has been explored and failed. It can't be done. Because there's not a way of making it work. So it's now time to move on.
That doesn't mean that those people have always been opposed. Neither does it mean that they feel the EU has served us badly for most of history. The majority of those who are in favor, in the press, opinion formers, have only come to that view recently. Most have wanted to renegotiate. Only when they've despaired of that as a possibility, that's when they said we'll have to leave.
It's also a post-euro thing. if you go back 10 to 12 years, people argued it would unsustainable if Britain didn’t join the euro. Those people were right. That didn't mean we had to leave immediately. We got an extra 15 or 20 years in the EU. That’s a long time —  almost as long as we’ve been in [the EU] in the first place. But deciding not to join the euro started the clock ticking.
On Thursday, voters in the United Kingdom of Great Britain and Northern Ireland will go to the polls to decide whether to leave the European Union — an option dubbed "Brexit" — or stay in. Polls and betting markets suggest that British voters will probably vote narrowly to remain in the EU, but it’s too close to say for sure. But regardless of how the vote turns out, the fact that Britain is having this debate at all is a sign that the political project to unite Europe isn’t going very well.
The EU was created in 1993 (it was an expansion of the earlier European Economic Community), with "ever closer union" as one of its founding principles. European leaders hoped to gradually transform Europe from a collection of sovereign nations into a single European superstate. But that process has slowed over the past decade, leaving the EU awkwardly straddling the line between being one nation and many.
The EU has become too economically and politically integrated for its member states to function as truly independent nations. Yet it lacks the kind of continent-wide financial and political institutions — like a strong executive and the authority to levy taxes independent of member governments — that would allow it to act as a single integrated country. As a result, the EU has a tendency to become paralyzed by infighting and indecision during crisis moments like last summer's Greek financial meltdown.
If Britain leaves the EU, there's a danger that it will embolden other EU nations to follow suit, leading to the gradual disintegration of the EU. The prime minister of the Czech Republic warned in February that "Czexit" could follow close on the heels of Brexit.
On the other hand, it’s possible that removing Britain’s relatively Euroskeptical population from the EU could make it easier for the EU’s remaining members to move forward with economic and political integration.
On the other other hand, even if UK voters decide to stay in the EU, British voters have made it clear that they don't like European elites' vision of "ever closer union." So a "remain" vote could leave the EU's current dysfunctional institutions in place for the foreseeable future — which could prove even worse for the EU, in the long run, than losing the UK.
It will create some big headaches for European leaders if the UK votes to leave the European Union. They'll have to negotiate a new relationship with the Brits while simultaneously worrying that other countries will follow the UK's lead.
But over the long run, the British vote is more a symptom of the EU's broader challenges than it is a source of problems in its own right.
To see why, it's helpful to think back to last year's Greek crisis.
The Greek economy was suffering from a severe economic downturn caused in part by a Europe-wide monetary policy that was insufficiently stimulative for Greece. The downturn worsened Greece's already large deficit, which meant that Greece needed to ask European leaders for more financial aid. But European leaders were reluctant to provide it, because they worried it would establish a precedent and encourage other countries to engage in reckless spending in the future.

If Greece were an independent country with its own currency, it could have addressed the crisis by devaluing its currency and stabilizing its debt with depreciated cash.
On the other hand, if the EU were a single sovereign nation like the United States, it would have a nationwide, federally funded social safety net, which would have meant that a local economic downturn would not have been as devastating to Greece's budget.
Instead, Greece was stuck with the worst of both worlds: It didn't have the autonomy to deal with the crisis itself, nor were there EU-wide institutions powerful enough to address it effectively.
Every year, the US federal government taxes rich people in Massachusetts and Connecticut and uses the money to fund government benefits for poor people in Mississippi and West Virginia. Few Americans see that as a problem, because we identify as Americans first and citizens of Massachusetts or West Virginia second.
But Europe's political culture hasn't made the same leap. People still see themselves as Brits, Germans, or Greeks more than Europeans. So when circumstances call for shared sacrifices, things tend to get bogged down in arguments about which European nations should bear the biggest burden.
These kinds of problems are likely to crop up every time the continent has an economic or political crisis, generating resentments that will make people even more reluctant to support further integration. British taxpayers weren't on the hook for last year's Greek bailout, but observing the bitter conflict between Greece and the rest of the eurozone can't have made British voters more enthusiastic about British institutions becoming more integrated with the continent.
The Brexit debate, then, is a sign that the European political experiment isn't going very well. Whether or not British voters decide to pull out now, it's clear they don't want to see the integration process go much further. And if further integration isn't on the table, it's not clear how the EU can gain the legitimacy it will need to survive over the long run.
Britain's participation in the European project has always been controversial within the United Kingdom. In recent years, opponents of Britain's membership in the EU, known as Euroskeptics, have been concentrated in the more populist portions of Britain's political right.
The strongest Euroskeptics in Britain have a lot in common with conservative populists in America such as Donald Trump and Pat Buchanan. Just as Trump argues that the US has gotten a bad deal from recent trade deals, so British Euroskeptics contend that Britain is getting a raw deal from its membership in the EU.
In the past few years, the debate over Britain's EU membership has increasingly become a debate over immigration. Britain did not sign on to the Schengen Agreement, which established de facto open borders among many EU members on the continent. But the EU's rules still require Britain to open its labor markets to citizens of other EU nations and to offer them certain government benefits.
The EU's liberal migration rules are unpopular on the British far right.
The United Kingdom Independence Party, founded in the 1990s with an explicit goal of opposing British membership in the EU, has gained supporters in recent years by focusing on an anti-immigrant platform. The party won 12 percent of the vote in the 2015 election — good enough for third place.
The Syrian refugee crisis has created a political environment in which anti-immigrant and anti-EU politics can find broader support. Few Syrians have made it to British shores, but the flood of Syrian refugees into other EU nations and last year's terrorist attacks in Paris have raised fears about border security more generally.
While Cameron has endorsed EU membership and has induced most members of his leadership team to do the same, he still faces significant opposition within his own party. The most prominent critic is former London Mayor Boris Johnson, who announced in February that he favored a "leave" vote.
Johnson's case against EU membership focuses on the growing regulatory burden imposed by the EU and the corresponding erosion of British sovereignty. "EU law is likened to a ratchet, clicking only forwards," he writes. "We are seeing a slow and invisible process of legal colonisation, as the EU infiltrates just about every area of public policy."
"It was one thing when that court contented itself with the single market, and ensuring that there was free and fair trade across the EU," Johnson adds. Now, however, "the European Court of Justice has taken on the ability to vindicate people’s rights under the 55-clause Charter of Fundamental Human Rights, including such peculiar entitlements as the right to found a school, or the right to 'pursue a freely chosen occupation' anywhere in the EU."
Grassroots agitation to leave the EU has given British Prime Minister David Cameron leverage to seek changes to the EU's rules. In February, after months of negotiations, Cameron announced a new deal with EU leaders that confirms Britain's right to remain permanently outside the common currency and gives the UK new rights to control immigration and to limit government benefits to EU citizens who move to the UK.
This deal has been a key part of Cameron's campaign to stay in the EU, as he has argued that it addresses critics' major concerns with EU membership.
And Cameron's allies in the business community argue that EU membership has major economic benefits. The EU operates as a massive free trade area, and its standardization of national regulations make it easier to do business across borders. This makes it easier for British businesses to sell their products in countries from Spain to Poland. The EU also makes it easier for people to move between European countries to find work, allowing British business to draw on a continent-wide labor force.
EU advocates say these benefits are economically significant. One major business group estimates that being part of the EU provides Britain with a net benefit of 4 to 5 percent of British GDP — or roughly $1,500 per British person. And advocates emphasize that Britain's net contribution to the EU's budget, 0.4 percent of GDP, is comparatively small.
Of course, not everyone sees the EU's efforts to harmonize regulations in a positive light. Critics see the EU as foisting unwanted and unnecessary regulations on the British economy, and they argue that leaving the EU — and repealing many of those regulations — could provide economic benefits.
Of course, a lot depends on what kind of relationship would replace EU membership if Britain were to leave the EU. The deep ties between Britain's economy and the rest of the EU means that both sides would have strong incentives to cooperate. Cameron might be able to strike a deal that salvages many of the benefits of EU membership while enhancing Britain's autonomy. Indeed, Boris Johnson has argued that leaving the EU could allow the UK to enjoy most of the benefits of EU membership with fewer costs.
On the other hand, European leaders might decide to drive a hard bargain in order to make an example of Britain and discourage other countries from following its lead. There's a risk that Britain's exit from the EU could lead to higher trade barriers with the continent, putting UK businesses at a disadvantage compared with German and French firms trying to do business in the European market. Some companies have hinted that a British exit could cause them to move their offices from London to continental capitals to keep them within the EU.
The only way to find out for sure would be for voters to endorse Brexit. And Cameron argues that's too big of a risk to take.
There was a time when I had seen but one and a half movies in the Fast & Furious franchise.
I had always been skeptical of the films' powers, perhaps because I had only seen 2 Fast 2 Furious (easily the worst installment) and Fast Five — without sound, while on a plane (still pretty awesome).
Then I watched the first seven of them in a week.
That experience made me not just a convert, but an evangelist. The Fast & Furious movies stand as one of the best film franchises America has going for it right now — and maybe even a gloriously dumb representation of everything good about America.
They boast a healthy affection for old-fashioned, practical effects work, where most of the cars we see crashing are really cars crashing, not sprites in a computer. They have a huge, talented, racially diverse cast. They feature openly sentimental, oddly complex storytelling. They're a testament to the bravery and diversity of this great land, to its boldness of spirit and openness of heart.
And, yes, to its ability to solve pretty much any problem by hurling a car at it.
Fast & Furious is a mutant hybrid horror of a movie franchise. It starts out as a low-key story of illegal street racers and eventually becomes, for all intents and purposes, the Marvel Cinematic Universe (MCU) but with cars. Along the way, it freely appropriates tropes from the Halloween, Terminator, and James Bond franchises.
But underneath it all, Fast & Furious is a grand, glorious soap opera, with motor oil pumping through a heart measured in horsepower.
This is America reimagined as a fireball.
The Fast & Furious franchise consists of seven movies of gradually increasing mayhem, with an eighth hitting theaters on April 14, 2017.
Related
The first film is basically a small-scale character drama with some car chases thrown in here and there. The seventh film features cars parachuting out of airplanes. As you can probably surmise, things escalated over time.
Here’s a brief overview of all the films in the franchise, with quick plot descriptions, international box office totals, and the official Vox rating:
Rating
The Fast and the Furious (2001): LAPD officer Brian O'Conner (Paul Walker) goes undercover among illegal street racers to figure out who's behind a string of truck hijackings. While there, he develops an intense rivalry — and eventual friendship — with the mysterious, mountainous Dom Toretto (Vin Diesel). Worldwide box office: $207 million.
Rating
2 Fast 2 Furious (2003): Brian leaves behind everybody who was in the first movie to track down drug dealers in Miami. Also, there are car chases and street races and Eva Mendes. This movie is bad. Worldwide box office: $236 million.

Rating
The Fast and the Furious: Tokyo Drift (2006): For 99 percent of its running time, this movie has absolutely nothing to do with the previous two. An American misfit teenager named Sean (Lucas Black) goes to live in a version of Japan that seems cribbed from a Carmen Sandiego game. While there, he learns about the mysteries of "drift" racing and takes on the Yakuza. Surprisingly decent! Worldwide box office: $158 million.
Rating
Fast & Furious (2009): The franchise accidentally creates continuity by gathering up major characters from the first three films (which, remember, had very little to do with one another) and sending them after drug dealers yet again in the wake of a tragedy. The plot doesn't make much sense, but the action is better than ever. Worldwide box office: $363 million.

Rating
Fast Five (2011): The franchise's pinnacle so far mostly leaves behind car racing in favor of an elaborate heist plot, with even more characters from the series' history getting together to rip off a Brazilian crime lord. The concluding action sequence is one for the ages. Worldwide box office: $626 million.

Rating
Fast & Furious 6 (2013): The franchise takes a full turn toward melodrama, with double-crosses, amnesia, and brave self-sacrifices turning up as major plot points. It's arguably a little too much, but that's what this franchise is for now. Oh, and the action sequences are stunning. Worldwide box office: $789 million.

Rating
Furious 7 (2015): The brother of the sixth film's main villain seeks revenge on Dom, Brian, and the crew — a scenario that can only end furiously. This is an enormously entertaining movie and would serve as a great finale for the whole series, if necessary. (It won't be the finale, however.) Worldwide box office: $1.516 billion

Rating
The Fate of the Furious (2017): A new chapter begins for the series, in the first film made entirely after the death of Paul Walker in a 2013 car accident. Has Dom turned against his beloved family? Will a new group of cyberterrorists destroy the world? If you don’t already know the answers to those questions, you may not have seen these movies before. A mild step back. Worldwide box office: n/a
No. The first film is actually loosely based on a 1998 magazine article from Vibe magazine about street racers.
The 1955 film — produced by famed low-budget Hollywood maven Roger Corman — was about a criminal who breaks out of prison and has to drive really fast to stay ahead of the law with a beautiful woman at his side. Elements of that idea have been sprinkled throughout the series, but no single film in the franchise is a remake of that movie. However, Universal did buy the rights to use the title for the first film.
You can watch the full 1955 movie here.
The first Fast & Furious film is also not a remake of the 1939 Busby Berkeley comedy Fast and Furious, as the latter is about rare book dealers. Here is its trailer anyway.
The first film was directed by Rob Cohen, and his departure from the sequel was seen as something the franchise might struggle to overcome. (He left with Vin Diesel to make XXX.) Cohen’s visual style — which mostly consists of making the world look all jittery and stretched out as those super-fast cars pass by — defines that first film in a big way.
The second film was directed by John Singleton, a great director who also made Boyz N the Hood but who seemed to fundamentally misunderstand that a Fast & Furious movie should never be too serious.
The next four films were directed by Justin Lin, one of the best action directors working right now. What Lin understands intuitively is that action sequences require long shots that establish geography, so we have a better idea of what's happening to whom, and when. Here's a great example of how he establishes the geography of a major chase in Fast Five:
The seventh film was directed by James Wan, best known for his horror work on movies like Saw and The Conjuring. He's an expert at building tension, a skill he used to great effect throughout Furious 7.
The eighth film is directed by series newcomer F. Gary Gray, who previously directed the 2003 “cool cars” movie The Italian Job.
It's also worth pointing out that films three through eight are all written by Chris Morgan, who has become a kind of steward of the franchise’s characters.
The crazy thing about the Fast & Furious franchise is that no single character has appeared in all seven films. This isn't a conscious choice in the way it might be in, say, the Marvel movies, where all the characters have their own solo adventures before coming together in Avengers films. Many of the actors split off to have more successful careers elsewhere — only to come back when those careers didn't exactly pay off.
Here's a quick look at who appears in which Fast & Furious films.
These characters break down into five main groups.
The main four: This group includes former LAPD officer and FBI agent Brian O'Conner (Paul Walker), who sets the series in motion by trying to get close to street racer and small-time criminal Dom Toretto (Vin Diesel) and his girlfriend Leticia "Letty" Ortiz (Michelle Rodriguez). Naturally, Brian gets in too deep and falls for Dom's sister, Mia (Jordana Brewster). The constant twists and turns in the relationships of these four characters drive every movie but the second and third ones, though Brian is still the main character of 2 Fast.
The rest of the crew: These are the characters who sign up with the main four to pull off impossible schemes. Roman Pearce (Tyrese Gibson) and Tej Parker (Chris "Ludacris" Bridges) joined the franchise in 2 Fast, before becoming its comic relief in Fast Five. HanSeoul-Oh (Sung Kang) is a wryly philosophical drift racer from Tokyo who proved so popular that the franchise literally brought him back from the dead. (More on this in a bit.) Gisele Yashar (Gal Gadot) is a relatively minor character in Fast & Furious who joins the crew more fully in Fast Five and develops a relationship with Han. (Also in this classification but not in our graphic are Tego Leo [Tego Calderón] and Rico Santos [Don Omar].)
Friends in law enforcement: These characters start out as Fast Five antagonists but quickly realize our heroes are the good guys and help them take out supercriminals. They include Luke Hobbs (Dwayne "The Rock" Johnson), who is basically Tommy Lee Jones's relentless pursuer from The Fugitive until he and Dom become best pals, and Elena Neves (Elsa Pataky), a Brazilian police officer who's the only non-corrupt cop in Rio or something.
Wild cards: As the films move further past the Brian and Dom era, more characters are being brought in to test the group’s dynamics. These include villain-then-tenuous friend Deckard Shaw (Jason Statham) and the mysterious Mr. Nobody (Kurt Russell).
Sean: Sean Boswell (Lucas Black) is the protagonist of Tokyo Drift. Because of the series' wacky chronology, he sat out three movies and returned in Furious 7 — but only in a couple of scenes.
The most beautiful thing about the Fast & Furious franchise is that it accidentally invented a prequel trilogy, then stuck it in the middle of the run, and it's all because Vin Diesel wanted to make more Riddick movies.
As mentioned above, Tokyo Drift has nothing to do with either of the two movies preceding it, but audiences sparked to the character of Han, who trains Sean in the ways of drift racing. (Drift racing involves sliding around a curve, seemingly perpendicular to the road, in a way that seems almost magical when done well.) In the course of that film, Han dies after a race. At the film's end, when Sean has taken the title of Drift King, he is challenged by a new racer — who turns out to be Dom, who says Han used to ride with his crew back in the day.
Diesel was hoping to make more films in the Riddick franchise of sci-fi action pictures. To do that, though, he needed to get the rights for the character from Universal, which also produced the Fast & Furious movies. He agreed to cameo at the end of Tokyo Drift — as a promise of his return to the franchise in a fourth film (which eventually happened) — but instead of payment, he asked for the Riddick rights. Universal obliged, and another Riddick movie came out in 2013.
These two things — Han's popularity and Diesel's cameo mentioning Han — combined to create the franchise's twisty timeline. Because wouldn't it be fun to see Dom hanging out with Han? Yes. Yes it would. But for that to work Han had to be alive, leading to films four, five, and six being set before Tokyo Drift instead of after it. Han's death is then repeated at the end of Fast & Furious 6, revealing the villain of Furious 7 to be responsible.
The forthcoming eighth film picks up where Furious 7 left off.
Thus, the chronological order of the films is 1 - 2 - 4 - 5 - 6 - 3 - 7 - 8, and the third film (in which everybody uses flip phones and instant messaging programs) technically takes place sometime in the 2010s, even though it was released in 2006.
Here's a visual representation of that timeline:
Nah. You could watch these movies in literally any order and be fine. The chronology is complex, but it's not incomprehensible or anything.
Your best bet is the same as it is with The Chronicles of Narnia (another series where chronology doesn't reflect release order): Watch the films in the order they were released. You'll be fine! We promise!
And how!
This is "Act a Fool" by Ludacris, from 2 Fast 2 Furious. It is the most memorable thing about that film, if only because you will essentially be forced to now say the film's title in the same cadence as the song.
Make no mistake: Fast & Furious is one of the most important franchises in Hollywood. If you look at the box office totals for the first seven films above, you'll see that — outside of the blip for Tokyo Drift — they keep going up and up and up.
What's more, the franchise has been almost perfectly cultivated by its studio. Really, the only comparable franchise in Hollywood right now is the Marvel Cinematic Universe, over at Disney. And the films reflect how Hollywood has changed as they've been made.
Here are a few ways they reflect those changes.
With Fast Five, the films basically switch genres. For the first four films, this is a franchise about car racing and big chase sequences, with those elements mixing into fairly standard crime thriller plots. In Fast Five, however, the franchise takes a hard right turn into the heist movie genre, with only one (very short) car chase. And Fast & Furious 6 is much closer to a James Bond movie than anything else, with the characters having to take down a villain who could destroy the world. The franchise always prominently features cars, and the solution to every problem is always "more cars," but these aren't "car movies" anymore. It's a neat trick.
With Fast & Furious, the franchise switches templates. Before the fourth film, this is basically the Halloween franchise — three films, where the third has essentially no connection to the first two other than genre. With the fourth, though, the films accidentally become, thanks to the addition of Han to the main cast, a shared universe, where all of the characters are involved in one another's adventures — and they get there a few years before Marvel would. Along the way, they also make a stop in "the bad guys are good guys now!" territory, straight out of the Terminator movies.
The films have a surprisingly deep mythology. Okay, it's not Marvel levels of deep, but the MCU has decades of comic books propping it up. And from the fourth Furious movie on, the story of the franchise has been one story, which slowly builds on previously established character relationships and plot twists. It's kind of like a giant, cinematic TV show.
Here are some great moments from each film in GIF form.
The Fast and the Furious: Here's a quick look at how the first film portrays going really fast in a car like traveling through hyperspace in the Star Wars movies.
2 Fast 2 Furious: Literally the only moment of note in this film is when Brian jumps a car onto a boat. (This movie is bad. Did we mention that?)
The Fast and the Furious: Tokyo Drift: Drift racers have to avoid pedestrians at one of Japan's busiest intersections.
Fast & Furious: The whole opening sequence (which involves stealing gas from a truck as it's barreling down a mountain) is so cool, but here's its best moment.
Fast Five: The closing heist — involving Dom and Brian dragging a vault through the streets of Rio — is the franchise's action sequence pinnacle, but best of all is how Dom uses the vault as a weapon.
Fast & Furious 6: Dom catches Letty in midair. It's true love!
Furious 7: The seventh film has by far the most computer-assisted visual effects in the series, as you can see in this scene where Brian runs up the side of a bus that's falling off a cliff and leaps to grab hold of Letty's car. It's crazy and over-the-top in the series' best sense.
It's impossible to say why, of course, other than the fact that they are hugely entertaining blockbusters, with just the right amount of character work to go with the giant stunts. But here are my five best guesses.
1) The movies feel real. Though there are plenty of computer-assisted visual effects throughout the series, the franchise attempts to actually perform as many stunts as it can. This means that when the cars crash, some stunt driver really crashed a car. It underlines the verisimilitude of much of what happens onscreen, no matter how ridiculous. This is really happening to these characters in our universe, it seems to say, not in some fantastical one.
2) The movies' dramatic stakes build slowly. The stakes of the first film literally hinge on Brian deciding whether to betray Dom. That's it. Even in Fast Five, the stakes involve ripping off one man — albeit the most powerful criminal in all of Rio de Janeiro. It's only in Fast & Furious 6 that the idea of "saving the world" comes into play, and it's almost always an afterthought there, because ...
3) Personal stakes are always more important than plot stakes. The major story of these films is the relationships between the characters and how they shift and change. If you ever watch a Fast & Furious movie, there is at least a half-hour — and often more — smack-dab in the middle of the movie where the characters talk about how they feel about each other. The action sequences become seasoning, sprinkled throughout the rest of the story to heighten it, not to define it. The series defines itself by questions of honor, friendship, family, and redemption. It is the most sensitive major franchise out there, and the most sentimental. (See also: the tear-jerking end of Furious 7.)
4) The cast is very diverse. It seems no coincidence to me that the rise of Fast & Furious — largely on the backs of films made by a Taiwanese-American director — came right as Hollywood was realizing people of color were hungry for stories in which they were prominently featured. Of the series' major actors, only Walker fits the usual white-guy-hero mold of most Hollywood films. (Diesel's full ethnicity is unknown, though he's said he's "definitely a person of color.") That's proved quietly revolutionary.
5) This series isn't afraid to be stupid. From Tej and Roman's comic relief to Hobbs's frequent one-liners to the sheer ridiculousness of many of the series' stunts, there is nothing in Fast & Furious that is above broadly winking at the audience about how silly the entire enterprise is. That lack of self-seriousness is hugely welcome in the current blockbuster climate — and another link between this franchise and the Marvel movies.
The eighth film, The Fate of the Furious, arrives April 14 — and features Charlize Theron and Helen Mirren! — but in the wake of Walker’s 2013 death, the franchise will have to pivot from the relationship that formed its core for most of the first seven films. (Walker had completed enough of Furious 7 to appear in that film, and the filmmakers were able to construct a fitting coda for his character.)
The eighth film won’t be the franchise’s last, either. A ninth and 10th film are planned, and one thing’s for certain: They’ll probably involve Vin Diesel staring at a seemingly unsolvable problem, then smiling and driving a car through it.
When the newspaper publisher formerly known as Tribune Publishing changed its name to "Tronc," people started snickering. When the company released this video touting Tronc’s disruptive new business strategy, it was subjected to merciless mockery from across the internet:
"It’s about meeting in the middle," says Tronc chief digital officer Anne Vasquez in the video. "Having a tech startup culture meet a legacy corporate culture and then evolving and changing."
"That’s really the fun part," she says, with no hint of excitement on her face. "That’s exciting."
Reforming the "legacy corporate culture" at Tronc’s stable of local newspapers — including the Los Angeles Times, the nation’s fourth largest by circulation — doesn’t actually sound all that fun. But Tronc’s effort to reinvent local newspapers for the internet age is pretty important.
People in major cities like Orlando and Chicago rely on the journalism performed by Tronc papers, and those publications have suffered from wave after wave of layoffs over the past decade.
Painfully scripted delivery aside, the video shows Tronc management making a sincere effort to change the company’s culture and business model to adapt to the realities of the internet. The problem is that this task is stupendously difficult, and it doesn’t sound like Tronc has found a strategy that is likely to work.
In the late 20th century, owning a newspaper like the Hartford Courant or the Baltimore Sun was highly profitable. Tronc owns about 10 of these newspapers around the country.
Thanks to decades of industry consolidation, most major cities had only one or two daily newspapers, making them de facto monopolies and allowing them to charge premium rates for advertisements.
Competition from the internet changed all that:
Between 2004 and 2014, newspapers’ print advertising revenues fell by almost two-thirds, from $47 billion to $16 billion. Digital advertising generated $3.5 billion in 2014 — nowhere near enough to make up the difference.
Newspapers’ basic problem is that they were built for a world in which consumers got a wide variety of timely information in one big newspaper bundle — not only local news, weather, and sports coverage but also national and international news, national sports coverage, the comics page, movie reviews, and so forth.
But on the internet, these functions are increasingly unbundled. People can get their political news from Politico, weather reports from Weather.com, sports news from SB Nation (including local SB Nation blogs for their local sports teams), and so forth. So not only do people spend less time on a particular newspaper’s website than they once spent reading the print newspaper, but the increased competition for ad dollars means newspapers can’t charge as much.
The best-known newspapers —  the New York Times, the Wall Street Journal, and the Washington Post — have benefited from an offsetting advantage: Their brands are so strong that they’ve been able to take their product national and attract millions of new readers. This has worked especially well for the Times and the Journal, both of which have convinced huge numbers of readers to sign up for their paywalls.
But this isn’t really a viable option for midsize Tronc newspapers like the Orlando Sentinel or the San Diego Union-Tribune. Many readers outside of Orlando and San Diego haven’t even heard of these newspapers, and they certainly aren’t the kind of iconic brands that can attract paying subscribers.
So for the past decade, the kind of midsize newspapers that make up the bulk of Tronc’s portfolio have been fighting the relentless force of economic gravity. Revenues keep going down and down, forcing more and more layoffs.
The solution to midsize newspapers’ woes seems obvious: The papers need to refocus on areas where they have a comparative advantage — especially local news — and they need to use the internet to become leaner and more efficient. There is a significant audience for local news, and organizations like the Orlando Sentinel and the Hartford Courant have strong local brands that make them ideally positioned to deliver it.
But that’s a lot easier said than done. There’s no easy way to transform a complex organization like a daily newspaper into the radically different structure required to succeed in online news. A daily newspaper’s structure and culture reflect the accumulated wisdom of many decades in the newspaper business. Adapting to the web means tossing out a lot of that and starting over. There’s a risk that this will mean tossing out many of the values and skills that made the newspaper successful in the first place.
Adapting to the web is likely to involve substantial layoffs — both to reduce overall headcount and to make room to hire staff with internet-specific skills. But mass layoffs are terrible for morale. They can cause friction between a newspaper’s editorial and business wings and can scare away a newspaper’s most talented personnel.
Changing a newspaper’s culture and processes to adapt to the web is even harder than reducing its headcount. An organization’s structure and internal processes tend to be extremely durable — once people get used to doing things a particular way, it’s very difficult to get them to change.
That’s especially true because newspapers are still going to have to put out a physical publication for another decade or more. Print ad revenue may have fallen by two-thirds, but newspapers’ remaining print revenues are still worth about five times as much as digital ad revenue. So news organizations have to straddle a line between putting out an adequate print product in the short term while managing a long-term shift to the web.
This brings us to Tronc’s new video. It’s easy to mock (I did it myself), but it’s also important to give Tronc credit for trying. Tronc was absolutely right when it said in a tweet yesterday that "change can be terrifying, but we know what happens if we do nothing."
Tronc’s corporate thesis seems to be that digital technology is the source of its woes, and digital technology can also be its salvation. And once you strip away the buzzwords, most of its ideas seem sound.
Tronc plans to use software (which it calls "machine learning" and "artificial intelligence") to help reporters with the more tedious parts of writing a story — for example, choosing a photograph or video to accompany the story.
Tronc also plans to focus on embedding more videos in Tronc stories. Video ads pay better than old-fashioned banner ads, and so a lot of online news organizations have made video content a priority — here at Vox, we have a team that does exactly what Tronc is talking about, embedding videos in high-traffic posts.
Then there’s this:
It’s not clear why these newspapers are in outer space, but the basic idea seems sensible — Tronc’s corporate headquarters will have a team whose job is to help avoid duplication of effort and maximize the audience for content created by Tronc papers. If the Baltimore Sun’s restaurant critic writes a popular article that’s not Baltimore-specific, the content optimization team will look for ways to drive readers from other papers to the article.
The basic problem the company is trying to solve is that a midsize newspaper like the Hartford Courant doesn’t have either the culture or the scale required to build technology tools that allow its journalism to have the maximum possible impact and generate the maximum possible revenue. So Tronc is trying to build some technology tools that will be available for all of its papers to use.
That’s a fine theory. The problem is that making this actually work is going to be really difficult.
The basic issue is that these kinds of tools and strategies only work if the corporate office gets buy-in from rank-and-file reporters and editors. Tronc’s new content optimization tools won’t do any good if reporters ignore them.
Conversely, reporters are only going to use these tools if they actually help them do their jobs better. Centralizing development creates a danger that developers will build tools that sound like a good idea on a white board but turn out not to actually help reporters do their jobs. The more ambitious the plan is, the longer it will take for them to bring it to fruition and discover that it’s not working as they expect.
So the key to making the Tronc vision work is going to be to find ways to get Tronc’s software developers and its reporters actually working together. This tends to work best by starting with small experiments, then scaling up those experiments if they work well. The Washington Post, where I worked from 2013 to 2014, has had some success with this, launching a series of blogs and expanding the ones that caught on with the public.
And this is why I’m skeptical that Tronc’s ambitious reinvention plan will actually work. The thing the company needs most isn’t a splashy video laying out a grand vision for the future of journalism. It needs concrete examples of times Tronc’s technical talent boosted the profile of specific reporters at specific local newspapers.
Buzzword-laden promotional videos are going to make Tronc’s seasoned reporters roll their eyes and go back to doing their jobs the way they always have. If Tronc wants to actually get reporters and editors excited, it needs to start racking up a bunch of small wins in some papers that it can replicate in others.
Correction: I got the Tribune Company confused with Tribune Publishing.
This afternoon, the Federal Reserve will announce whether it's going to raise interest rates for the second time since 2008. And despite months of chatter from Fed officials about impending rate hikes, it looks like they’re likely to hold off on any increase for now.
When the Fed last raised rates in December 2015, the central bank predicted it would raise rates four more times in 2016. In March, it revised the forecast down to just two interest rate hikes for 2016. Last month, Fed Chair Janet Yellen said that the Fed was likely to raise rates "gradually and cautiously" over the next few months — perhaps as soon as the June meeting that’s happening this week.
But then the Labor Department released the worst jobs report in years, killing chatter about a June rate hike. Higher interest rates slow economic growth, and Fed officials are expected to conclude that the economy can’t afford higher interest rates right now.
This pattern — the Fed talking about imminent interest rate hikes but then delaying them due to disappointing economic performance — has been playing out for a couple of years. A lot of commentators simply treat it as bad luck, with the Fed as a mere spectator. But that misunderstands what's going on.
In reality, the Fed’s constant chatter about raising rates is itself an important cause of the economy’s sluggish performance. Markets and business leaders pay close attention to Fed statements. When Yellen signals that higher interest rates — and, consequently, slower growth — are imminent, companies respond by cutting investment spending. The result is a self-defeating feedback loop.
To understand why Fed statements can have such a big impact, it's helpful to think about things from the perspective of a business considering whether to start building a new factory. One thing the company’s CEO will likely do is ask an economist for a forecast of economic conditions. It makes more sense to open a factory if the economy is on the verge of a major boom than a recession.
Of course, no one — not even someone with a PhD in economics — knows exactly what’s going to happen to the economy. But Fed policy has a big impact on the economy's trajectory.
A perceptive forecaster will notice that the Fed seems determined to raise interest rates (and slow the economy) as soon as doing so won’t be disastrous. This means that the odds of a 1990s-style boom are fairly low, and the odds of a premature rate hike triggering a recession is higher than it would otherwise be.
Fed policy has a big impact on the economy's trajectory
Hence, every time Yellen reiterates that interest rates are likely to happen soon, the nation’s economic forecasters revise their forecasts downward a little bit. That causes the nation’s businesses to spend a little less on new investments. Less investing spending, in turn, means the economy grows a little bit more slowly. And a slowing economy forces the Fed to delay its interest rate hike.
In other words, the fact that the Fed constantly has to delay interest rate hikes in response to bad economic news isn’t just a matter of bad luck — it’s partly a result of the Fed’s own actions. We’re getting a slow economic recovery because the Fed has been signaling that it wants a slow recovery — or at least that it doesn't care enough about faster growth to make that its priority.
If the Fed wants a more robust economic recovery, it needs to convince markets that it wants a faster recovery. To do this, Yellen and other Fed officials need to stop talking about interest rate hikes and instead talk about how they're dissatisfied with the current pace of economic growth. They could say they want to see higher growth and are willing to risk moderately higher inflation in order to get it. They might even want to hint that their next move might be a rate cut rather than a rate hike.
Ironically, this strategy could actually get the Fed back to "normal" interest rate levels more quickly than its current policy of constantly talking about — and then delaying — interest rate hikes. The Fed keeps delaying interest rate hikes because the economy is underperforming, and the economy keeps underperforming because businesses don’t believe the Fed will allow faster growth. But if economic growth and inflation start exceeding expectations, then the Fed will be able to raise rates without triggering fears of more years of poor economic performance.
The larger problem here is the Fed’s ad hoc strategy for setting interest rates and explaining future decisions to the markets. At a fundamental level, the Fed’s job is to set clear and realistic expectations for the economy’s growth. If businesses believe the economy will grow quickly, they’ll invest more and help make that forecast come true.
The economy has gotten stuck in a low-inflation, low-growth, low–interest rate rut
But right now the Fed is doing a terrible job of this. The economy constantly falls short of the Fed’s projections, and the Fed keeps talking about interest rate hikes anyway. As a result, its projections have less and less credibility, and the economy has gotten stuck in a low-inflation, low-growth, low–interest rate rut.
A better approach would be for the Fed to explicitly put economic growth at the center of its decision-making. The best way to do this is with a technique called nominal GDP targeting, which has been endorsed by prominent economists like former Obama adviser Christina Romer. Under this approach, the Fed would commit to keeping the total amount of spending in the economy — in nominal terms, not adjusted for inflation — growing at 5 percent per year. If that means cutting interest rates or even restarting unconventional programs like quantitative easing, that’s what the Fed would do.
If the Fed were able to make this pledge credible, it would reverse the current bad equilibrium in which business pessimism causes the economy to consistently undershoot the Fed’s projections. If businesses believed the economy were really going to grow as fast as the Fed projected — because the Fed was committed to doing whatever it takes to hit the target — then businesses would be more inclined to spend money on new stores and factories. Growing business confidence, in turn, would help the Fed reach its targets. This would be a virtuous cycle, instead of the vicious cycle the Fed is stuck in right now.
In one of the year's biggest technology deals, Microsoft is buying LinkedIn for $26 billion.
If you’re in a profession that makes heavy use of LinkedIn, this deal may make perfect sense to you. But if you're in a profession that doesn't use LinkedIn much, you might be surprised to learn that LinkedIn is not only still in business but is worth $26 billion.
But this is no blunder. LinkedIn isn’t a very cool company, but neither is Microsoft, and LinkedIn has a thriving business focused on helping professionals find jobs and companies find workers.
Beyond the dollars changing hands, the acquisition of LinkedIn cements a broader shift in Microsoft’s corporate strategy that many consumers still haven’t noticed. Microsoft's early success came from dominating the market for PC software, followed by a period of struggle in which it tried to basically copy that business over to the mobile world and failed.
But under the leadership of relatively new CEO Satya Nadella, Microsoft is shifting to become a company that primarily sells online services to business customers. LinkedIn fits this new strategy perfectly, and it will help Microsoft both broaden the set of business customers it can serve and deepen the relationship with those it already serves.
I remember the surprise I felt a couple of years ago when I learned that LinkedIn has been enjoying robust growth. I joined LinkedIn about a decade ago, but I didn’t find it very useful. I gradually became more annoyed by the deluge of emails from distant acquaintances asking to "add you to my professional network on LinkedIn," so I eventually deleted my account, hoping that would make the emails stop. (It didn’t.)
But my experience is not universal. While journalists like me do most of their professional networking on Twitter, there are lots of professions where LinkedIn is considered an essential networking tool. Today the site has more than 100 million monthly active users.
LinkedIn is most valuable in professions where a bulging Rolodex was once a prized professional asset. If you are a sales professional, for example, LinkedIn is both an indispensable way to learn about potential clients and an essential way to find out about new job opportunities.
And while LinkedIn’s audience isn't as big as mainstream social networks like Facebook, Twitter, or Instagram, it's a highly lucrative audience. Companies are willing to spend considerable sums of money to find the best employees, and LinkedIn effectively has the world’s largest Rolodex of skilled professionals. LinkedIn's "talent solutions" business — charging businesses to post job ads and provide other recruiting services — accounted for almost two-thirds of LinkedIn’s $3 billion in revenue.
LinkedIn also makes money selling premium subscriptions to LinkedIn users and helping users in sales occupations find other users who may be interested in buying their products.
And depending on how you measure it, LinkedIn is already profitable. According to generally accepted accounting principles, LinkedIn took a modest $46 million loss in the most recent quarter. But like many technology companies, LinkedIn prefers an alternative method of computing profits that excludes the value of employees’ stock-based compensation. This measure shows the company earning a hefty $99 million in profits.
Buying LinkedIn makes good sense for Microsoft too. It represents one of the most significant steps in Nadella’s effort to reinvent Microsoft from the leading PC software maker to a company that sells business technology services more generally.
Microsoft’s initial success came from selling software for PCs — most notably the Windows operating system and Office productivity suite. During the 1990s and 2000s, Microsoft built on its dominance of the PC market by selling a growing portfolio of licensed software that runs on corporate servers — like the Exchange email server, SQL Server database, and IIS web server.
But over the past decade, Microsoft has faced two big disruptive threats. First was the online app revolution led by Google. People increasingly used online products like Gmail and Google Docs instead of desktop software like Microsoft Outlook and Microsoft Office.
Next came the mobile revolution, led by Apple. People increasingly used smartphones and tablets running Apple’s iOS — or Google’s Android — instead of Microsoft software.
In the waning years of CEO Steve Ballmer’s tenure, Microsoft made a series of increasingly desperate attempts to meet these threats head on. Microsoft responded directly to Google with products like the Bing search engine and Bing maps. It responded directly to Apple with the Windows Phone. But these products didn't catch on, and the efforts cost Microsoft hundreds of millions of dollars in losses.
Under Nadella, Microsoft's new CEO, the company finally seems to be accepting that it’s not going to play the kind of dominant role in the consumer technology market that it did in the 1990s. People are mostly going to perform Google searches on their iPhones, not Bing searches on their Windows Phones.
But while Microsoft has been bleeding market share among consumers, the company continues to be popular with business customers. So Nadella has focused on expanding and modernizing the company’s business products.
Because companies often invest millions in large, multiyear technology projects, they tend to be less fickle than consumers. If your corporate IT system is built on Microsoft software today, you’re going to be very interested in buying additional Microsoft software tomorrow. So Microsoft has been very successful at upselling existing customers on additional software products that work within the Microsoft software ecosystem.
At the moment, one of Microsoft’s fastest-growing products is its Azure cloud computing platform. This is a subscription-based service that allows businesses to run software in Microsoft’s data centers instead of on servers they run themselves. Microsoft has also been shifting customers to Office 365, an online, subscription-based version of its productivity suite.
In short, there are two big themes to Microsoft’s reinvention. First, Microsoft is deemphasizing the consumer market and focusing on business customers. Second, Microsoft is shifting from selling individual copies of software to run on customer-owned hardware to selling online services supported by subscription fees and advertising.
It’s obvious how Microsoft’s acquisition of LinkedIn dovetails with Microsoft’s shifting business strategy. There’s a lot of overlap between LinkedIn’s user base — corporate professionals — and Microsoft’s customers. And LinkedIn is already providing online services rather than selling software.
There are also some specific ways Microsoft and LInkedIn hope to strengthen each other’s products. For example, LinkedIn offers a Facebook-style newsfeed to its users. Microsoft plans to integrate this newsfeed into the Office 365 user interface, allowing users to keep track of developments in their professional network while they’re working on a spreadsheet or presentation.
Microsoft also hopes to integrate data from LinkedIn into Cortana, the personal assistant that is Microsoft's answer to Siri and Google Now. So a future version of Cortana may be able to look up the phone number of a LinkedIn contact or tell you about mutual friends.
Access to LinkedIn's data will be especially useful for Dynamics, Microsoft’s customer relationship management software. You probably haven't heard of CRM software, but it’s considered an essential software tool for any company that maintains long-term relationships with customers. Microsoft hopes to use data from LinkedIn to enhance Dynamics, giving customers access to more data about their customers and helping them find new ones.
Gawker Media, the company behind the Gawker.com website as well as sites like Jezebel and Gizmodo, filed for bankruptcy on Friday. The company was forced to take the drastic action after a jury awarded former pro wrestler Hulk Hogan $140 million in damages — a judgment the media company can’t afford to pay.
The bankruptcy process should protect Gawker from its creditors while the company tries to find a long-term solution to its legal and financial woes. That will likely involve selling the company to another media company. According to the Wall Street Journal, the digital media company Ziff Davis is expected to make an initial $100 million bid for the company, and other potential buyers will be invited to make competing offers.
Gawker's bankruptcy represents a devastating blow not only for Gawker founder Nick Denton and the site's other investors but also to the sensationalistic and privacy-hostile style of journalism Gawker represents.
Gawker has aggressively pursued stories about the private lives of celebrities, repeatedly publishing stories that more traditional news outlets would have declined to publish. That culminated with the 2012 publication of a sex tape that Hogan says was made without his knowledge or consent. If Gawker’s loss is upheld on appeal, it will force other media organizations to tread more carefully with these kinds of stories.
Gawker’s bankruptcy also represents a triumph for technology billionaire Peter Thiel. He has borne a grudge against Gawker ever since it revealed that he was gay in 2007, and in recent years he has been secretly funding the Hogan lawsuit and others against Gawker Media. There’s a real worry that Thiel’s tactics could endanger the free press by giving billionaires a new weapon to use against media outlets they don't like — a weapon that has already been used against at least one non-Gawker outlet.
Gawker is infamous for publishing stories about the private lives of the rich and powerful (and, in some cases, people who aren’t particularly rich or powerful) that more conventional media organizations would not have published. In one of its most infamous stories, Gawker reported on a New York media executive soliciting the services of a male escort. The piece was later removed after founder Nick Denton decided that the story had gone over the ethical line.
Outing gay people is something of a specialty for the digital gossip rag. Gawker was one of the first to report that CNN anchor Anderson Cooper was gay in 2009. In 2013, Gawker reported that Fox News anchor Shepard Smith was romantically involved with a male staffer.
And one of the early targets of Gawker's outing campaign was Peter Thiel. A 2007 post called "Peter Thiel is totally gay, people" apparently marked the start of Thiel's vendetta against the site.
Still, the Hogan video represented a new low for Gawker. The video of Hogan having consensual sex with the wife of a radio shock jock (who arranged the encounter and made the videotape) had no obvious news value, but Gawker decided to publish it anyway.
In court, Hulk Hogan's lawyers sought to portray Gawker as an organization without a moral compass. It wasn't a hard argument to make. During one deposition, Hogan's lawyers asked a former Gawker editor if there were any situation in which a celebrity sex tape would not be newsworthy.
"If they were a child," replied the editor, Albert Daulerio. "Under what age?" the lawyer asked.
"Four," Daulerio replied sarcastically.
As a result, arguments about media freedom fell on deaf ears in the jury box. Jurors didn't buy arguments that the First Amendment protected Gawker's right to humiliate random celebrities by publishing video of their most intimate moments.
Gawker is filing for bankruptcy under Chapter 11 of the bankruptcy code, which provides companies with protection from creditors while they reorganize their finances. The company has secured $22 million in debtor financing to give it some breathing room as it looks for a long-term solution.
That long-term solution will likely be a sale to another media company. Ziff Davis will reportedly offer $100 million, and if Hogan ultimately wins the case on appeal — or settles — a lot of that money will go to him.
If all goes well, Gawker and its sister websites will be able to continue operating under new management — and perhaps different editorial standards. Gawker's underlying business seemed to be thriving until it got into legal trouble. A version of Gawker Media that’s more careful about coloring inside the lines could be profitable, perhaps immensely so.
The larger significance of the Gawker case is the role it will play in shaping how journalists weigh the competing interests of privacy and free speech. In recent years, we've seen a number of cases where news sites have published photos, videos, and other private information about famous and not-so-famous people.
The most extreme form of this, known as "revenge porn" has triggered a backlash from privacy advocates and feminists who argue that publishing sexual content without its subjects' consent is a deep violation of their rights. But until recently, it wasn’t clear how the law viewed these claims.
But in the past couple of years, these issues have seen greater attention in the courts. The proprietors of the most egregious "revenge" porn sites have faced criminal prosecution for violating the privacy rights of their non-famous victims.
The Gawker case was different because it involved a prominent media organization and a famous subject, former pro wrestler Hulk Hogan. Gawker argued that Hogan was a public figure, and that his decision to talk about his sex life on programs like The Howard Stern Show made his sexual acts matters of public concern. If Gawker wins the case on appeal, it could give media organizations broad leeway to pierce the privacy of celebrities.
Whatever your view of Gawker’s journalism — and personally, I think publishing the Hogan tape was indefensible — the case does raise questions about whether billionaire-funded lawsuits could threaten freedom of speech. Hogan didn't fight his case alone; he was aided by a billionaire who was motivated by a broader antipathy toward Gawker’s style of journalism.
And Gawker isn't the only publication to be targeted by a disgruntled billionaire. Last year, the liberal magazine Mother Jones defeated a defamation lawsuit filed by Republican donor Frank VanderSloot. Winning the lawsuit cost Mother Jones, a relatively small nonprofit organization, and its insurance company $2.5 million in legal fees.
If VanderSloot's goal was to punish Mother Jones for writing an accurate but unflattering story about him, a loss was almost as good as a victory. His lawsuit sought $74,999 (staying just under the $75,000 threshold that would have allowed Mother Jones to move the case to federal court and away from an Idaho jury that might have favored the hometown plaintiff). So "winning" the lawsuit cost Mother Jones and its insurance company 30 times as much as the amount they would have had to pay if they had lost.
What was really ominous was what happened after VanderSloot's loss. He "announced that he was setting up a $1 million fund to pay the legal expenses of people wanting to sue Mother Jones or other members of the 'liberal press.'"
As far as I know, no one has taken him up on the offer. But the threat to freedom of the press is obvious. Any news organization doing its job is going to make some enemies. If a wealthy third party is willing to bankroll lawsuits by anyone with a grudge, and defending each case costs millions of dollars, the organization could get driven out of business even if it wins every single lawsuit.
Thiel insists that he has no quarrel with news organizations that conform to mainstream journalistic norms. But the key thing about Thiel’s strategy is that he didn't sue Gawker for outing him — a case he probably would have lost. Instead, he waited for years until he could find other plaintiffs with stronger cases.
That's a tactic that any billionaire could use against any news organization. And because most news organizations cover a wide variety of topics, the story that provoked a billionaire's ire might have nothing to do with the stories that actually trigger a lawsuit funded by that billionaire.
In short, Thiel's war on Gawker could become a template for other extremely wealthy people with personal or ideological scores to settle against news organizations. And that’s something to worry about even if you think Gawker deserves what it’s getting.

Few companies have enjoyed more hype over the past few years than electric carmaker Tesla. And not without reason: Tesla is the most successful automaking startup in decades and has almost singlehandedly made electric cars cool.
Yet the automaker has also been struggling with the quality of its vehicles. On Thursday, the National Highway Traffic Safety Administration revealed it was investigating a possible problem with the suspension of Tesla's flagship Model S sedans. The reported flaw is the latest in a series of quality and reliability problems with Tesla's vehicles.
Industry analyst Edward Niedermeyer, who blogs at the Daily Kanban, argues that Tesla's challenges with vehicle quality are only going to get worse in the coming years. Tesla is preparing to release the Model 3, whose modest $35,000 price tag is designed to appeal to mainstream customers. And Niedermeyer argues that middle-class customers are less forgiving of quality problems than the wealthy customers Tesla has served so far.
Tesla's basic problem, Niedermeyer argues, is culture. Industry leader Toyota conquered the American car market with a rigorous manufacturing process that emphasized quality and reliability above all else. But Tesla has a freewheeling Silicon Valley culture that values innovation and creativity over reliable execution.
We spoke by phone last week. The conversation has been edited for length and clarity.
Timothy B. Lee: You're skeptical that a Silicon Valley company like Tesla can become a major player in the auto business. Why do you think cars are different from software?
Edward Niedermeyer: To boil it down to the most essential issue, it's a question of scale. With software, you have a fixed cost of development that is oftentimes quite high, but once you have a viable product and you pay off that fixed cost, your variable cost to scale beyond that is almost nonexistent. You're literally just copying code.
With automobiles, not only do you have immense fixed costs in research and development, tooling up factories, creating testing, but once you've done all the development work for a car, you still have a process of scaling. Not only are the variable material and labor costs much higher than in software, but you also have a lot of details that can go wrong.
Cars have become so reliable and so easy to use that we think about them less than we ever have in the 100-plus-year history of the automobile. This is one reason we don't appreciate this depth of complexity. Not only are cars different from software in very fundamental ways, they're much more complicated than anything else consumers buy.
Cars use a wide variety of materials, built into components and subassemblies by massive global supply chains. Car companies have to choose and develop the right materials and components, maintain their uniformity and integrity throughout that supply chain, and ensure that they operate reliably in almost every imaginable condition on Earth.
A great example is the problem of mold growing from inside the Model S's roof, particularly in Norwegian cars. Because its large panoramic sunroof is difficult to manufacture and install to a precise specification, Model S roofs often leak. A lot of those leaks are so small that customers might not notice. But because Tesla used an organic-fiber pad at the edge of the sunroof, aggressive molds invade at alarming rates in certain climates. This kind of complex, cascading defect is why automakers value their accumulated institutional knowledge and spend years testing vehicles.
TBL: It seems like Tesla's early cars — the 2008 Roadster and the 2012 Model S — were lauded for their innovative designs and were well-received by customers. But you're skeptical about the Model 3, which is more affordable and aimed at a mass market. What's the difference?
EN: It's a common misperception that the more expensive the car, the more people expect out of it. The opposite is true: the cheaper the car, the more people tend to rely on it, and the more reliability and quality come into play.
The car business is a very risk-averse business by nature. It's capital-intensive and relatively low-margin. The first 50 years or so after the Model T (in 1908) was focused on technology development. People pushed the limits in terms of power, styling, futurism.
Since the 1970s or so, it's reverted to a kind of more pragmatic, utilitarian mode. The market has become more mature. Toyota and Honda have really made their names on quality and reliability, not exciting futuristic values.
In a lot of ways, Tesla is a throwback to an earlier era of the auto industry. They tap into the idea that there is new technological space to be conquered; you get there by focusing on performance, on building a very attractive, appealing car. That's what Ferrari and Lamborghini did between the 1930s and the 1960s.
I think that parallel is worth looking at, because neither Ferrari nor Lamborghini is known for quality. If you operate in the high end of the market, consumers appreciate performance and design. If their Ferrari or Lamborghini breaks down, they have their chauffeur take them in a Mercedes or a Lexus.
It's not the end of the world that Tesla's quality has been bad so far, because they're operating in a luxury space. But as they move down market with the Model 3, reliability and quality are going to be real issues. The level of quality they've achieved in the Model S is not going to be sufficient to succeed in the $30,000-to-$50,000 price range.
TBL: Is it really that hard to improve manufacturing quality? Elon Musk is a smart guy, and he recently put his desk at the end of the Model X assembly line so he can personally keep an eye on the progress there.
EN: Anything is possible. Obviously they've proven doubters wrong before. So I'm not going to say that it's impossible for them to do it again.
But raising quality is very different from the things they've done to gain market position so far.
One parallel that's worth thinking about is General Motors. This was the most successful car company in the world for the better part of a century and the most valuable company for a period of time. Then they got surpassed on quality in the 1970s, and they still haven't caught up.
So the question is what makes the quality of Japanese companies? I think you can trace that back to Toyota, which developed the Toyota production system and — just as importantly — a broader corporate philosophy called the Toyota way. It systematizes everything about the production of automobiles.
For example, the need to keep plants operating at a high rate meant you'd let defects go down the line and fix them at the end. One of the things Toyota did was when the defect came down the line, you stop the line and you trace the defect back to its root and fix it, then you restart production. This is just one example.
To this day, they're still the leaders in quality. Nobody has caught up with them.
TBL: But GM in the 1970s was a big, bureaucratic organization tied down with a lot of union rules. It seems like it should be easier for a young and nimble company like Tesla to pivot and adopt a more Toyota-like production philosophy.
EN: It's certainly more likely because they're not at the point where Ford and GM and Chrysler were when they faced that challenge — they already had tens of thousands of workers and faced much more inertia.
But in the first half of the 20th century, Detroit was the equivalent at the time of Silicon Valley today. Yes, Tesla is a startup culture, but they demonstrate an arrogance that is similar to the arrogance that Detroit demonstrated in the past. When a culture works really well, there's an assumption that it can be universalized. I think that shows in the thinking that Silicon Valley culture will apply to manufacturing.
But what does startup culture emphasize? It emphasizes flexibility, individual effort, and working long hours to reach ambitious goals. What it's not is regimented.
But the only way to make money in cars is at huge scale. And scale creates immense complexity. And as you go up the volume scale, it becomes more challenging. So if your goal is to make your car company a mass-market player, you have to bake in the regimentation and the production system from the get-go.
When I say companies are risk-averse, it's because success in the car business is not about reaching out into the unknown in order to achieve unprecedented things. It's about driving waste, inefficiency, and defects out of your production machine. That is what Toyota's innovations enabled it to do. It systematized every aspect of development and production.
TBL: What do you think about the approach Google has taken to the car business?
EN: Google's strategy is the counterfactual that makes me especially nervous about Tesla. Google's core technology is the autonomous drive capability, and I think they have to be closely watching Tesla and the struggles they've had. So Google has hired some very high-profile people from the car business. They have former Ford CEO Alan Mulally on their board. Lawrence Burns, the former research and development boss for General Motors, is a consultant for them. The head of their autonomous car program is John Krafcik, one of the auto industry's most respected veterans.
It's a dream team of real tier-one automaker experience. With their accumulated knowledge — and looking at Tesla's struggles — they know that building their own car is a fool's mission. They also recognize that Silicon Valley culture is fundamentally different from manufacturing culture.
They realize there are plenty of car companies and car factories in the world. Even before autonomous drive comes out, there's a likelihood that shared mobility will begin to impact demand for cars, leaving spare production capacity for autonomous vehicles.
What fundamentally sets Google apart is that these auto people know how hard building cars is. It is not only an intellectual challenge, it's a discipline challenge. Managing that level of complexity requires a certain amount of accumulated knowledge; building that from scratch is incredibly difficult.
And these systems are already highly automated. It's not like the car guys are doing purchase orders on paper. They already need to be highly software-driven in order to make current levels of complexity work. I'm not sure how much room Silicon Valley has to improve that. If they do, they should develop the capability and sell it to the car companies.
So Tesla is fundamentally an old-school car company. They sell you a desirable, high-performing vehicle that you own. Google is trying to transform mobility without becoming a car company. Their focus is on autonomy. They have the leading position right now in terms of self-driving capability. They're going to continue to build on that.
TBL: It seems like the danger of partnering with existing car companies is that they could be too set in their ways, and too resistant to making the kind of changes that are required for self-driving cars to be really successful.
EN: At first, there was this sense that Google was going to directly take on the car companies and take them out of business. The car companies, without question, have cultural biases that prevent them from wanting to develop autonomous vehicles.
These companies have been around for 100 years. They've only ever sold vehicles to drivers, and the vast majority of their profits came from selling cars to drivers. Because cars are low-margin, you have to find ways to pad that margin, and they do it with things that appeal to a driver, like a more powerful engine or a sport suspension.
Fully autonomous vehicles will be fundamentally different. But Google has singlehandedly pushed autonomy from being a science experiment to something that's going to be viable. They have forced the car industry to accept that things are changing.
So what Google is doing is very pragmatic. Instead of having this existential battle between human drivers versus robot drivers, they've shown that this technology works and argued that they need to work together. It's going to be very disruptive to car companies' business to manage the change from driven vehicles to autonomous vehicles. But they're not fighting it. They're going along with it.
I think this is something that's emerged in the last year. What both sides have realized is that Google can avoid massive investments in very low-margin aspects of the business, while car companies can stay with the times if they work with Google or other startups.
TBL: It seems like we're going to see some big changes in the car industry over the next decade. Which car companies do you see as best positioned to navigate those changes?
EN: You see two responses. You see General Motors and Nissan initially trying to get on this wave of excitement and saying, We're going to set aggressive timelines for autonomous capability. GM made the biggest investment by buying Cruise. I've done a bit of research on Cruise and I'm not super convinced by them. I'm not convinced that GM didn't massively overpay.
Toyota has a much different approach. They are basically making a long-term investment in the research capabilities for autonomous driving. Their deployment strategy is super conservative. They are deploying in bits and pieces. They're starting off by putting low-level semi-autonomous safety functions in all of their vehicles.
That's in part due to their culture and in part due to their experience of the unintended acceleration scandal of 2010, where they were basically accused of having self-driving cars. As far as I can tell — and I spent a lot of time covering it — it was basically bullshit and kind of a witch hunt.
But the legal liability risks are very high. It's very easy for people to make mistakes and blame the car for their mistakes. So in some ways that's an incentive to go full autonomy. But even for a company like Toyota with $80 billion in the bank, there could be liability issues that could challenge the fate of the company. So they are incredibly conservative about deployment, and they will not deploy anything unless it works in 99.9 percent of use cases.
That contrasts with what Tesla is doing, a public beta test. They say they are. They admit it. Frankly, that is an incredibly risky proposition. I think Tesla has a halo right now. I think we have a celebrity news cycle where we tend to build things up and break them down. Once Tesla reaches a critical point in that hype cycle, the public beta test of autopilot software could be part of what destroys them as a company.
I personally tend to like Toyota's approach, because they accept and own the conservative nature of the business. So they don't fool themselves that they're going to do this leapfrog approach.
One of the dangers of expressing opinions on the internet for a living is that you sometimes express opinions that turn out to be totally incorrect. After I published a piece about the failure of home 3D printing on Monday, the American Conservative's Robert VerBruggen reminded me that I had a very different perspective on the topic four years ago:
@RAVerBruggen People in 1975 couldn't think of many programs they'd want to run on a personal computer.
@RAVerBruggen I think you're underestimating hindsight bias. Those are obviously useful on today's powerful computers.
Obviously my thinking on this topic has changed. And I think there's a broader lesson for how we think about technology as it becomes increasingly intertwined with the physical world.
As I say in these tweets, people underestimated the first PCs in the 1970s. They were so underpowered that you could hardly do anything useful with them. So lots of smart, sophisticated, thoughtful people dismissed them as overpriced toys. Then, as everyone now knows, PCs took over the world.
The same thing happened with the internet. In the 1980s it was hard to use and couldn't do very much. People mocked the idea that it could eventually support billion-dollar businesses. Then we got Amazon, Google, and Facebook, and people stopped laughing.
It happened again with mobile phones. People mocked the concept of using phones to check email or take photos. And then ... you get the idea.
By 2010, these stories had become the default way technology pundits like me looked at the world. "New technologies always look overly complex and underpowered at the outset," we'd say. "But they don't stay that way."
But in this decade, we've been seeing more and more examples where the PC analogy doesn't seem to be working.
When Google Glass was introduced in 2012, supporters saw it as the next great computing platform. But normal people weren't actually that enthusiastic about having computers on their faces, and after several years of mockery, Google has put Glass on the back burner.
In 2014, Google spent $3.2 billion to acquire the smart thermostat company Nest. Its CEO, Tony Fadell, quit last week after struggling to expand to other "smart home" products. Other companies have rolled out "internet of things" products like smart lightbulbs and wifi-connected slow cookers, but consumers haven't seemed very interested.
Though iRobot has experienced modest success building its Roomba robotic vacuum cleaners, it has struggled to produce follow-on products, and Roombas remain a niche product 14 years after its introduction.
Home 3D printing was introduced with great fanfare in 2012. But so far there's been no sign that consumers want 3D printers in their homes. Instead, 3D printer companies have pivoted to selling their wares to commercial customers.
In each of these cases, optimists a few years ago argued that we needed to give the products more time to mature. They often drew explicit parallels (as I did with 3D printers) to the early days of the PC.
But there is a big difference between these products and the famous examples of the PC and the internet.
People underestimated early PCs because they were drastically inferior to their successors. A modern PC isn't two, 10, or 100 times better than an Apple II circa 1977 — it has 100,000 times more computing power.
As a consequence it can do a lot of things — like editing large graphic and video files, playing sophisticated video games, and rendering complex webpages — that would have been far beyond the capabilities of the first PCs. The first PCs were slow, expensive, and bulky, but people just had to wait a few years for Moore's law to produce computer chips that were faster, smaller, and more affordable.
But not all problems can be solved with more computing power. If an errant cat toy jams your Roomba, no algorithm is going to get it unstuck. More sophisticated software won't necessarily make people interested in having computers on their faces. A faster computer chip isn't going to bring down the cost of the fairly expensive plastic used by most entry-level 3D printers — nor will it make consumers interested in having a lot of plastic junk lying around the house.
So when trying to predict if a new digital product will get better over time, it's helpful to ask whether the big problems are related to a lack of computing power or something else. For example, I'm bullish about self-driving cars because the challenges there mostly are software-related. It seems likely that collecting enough data and throwing enough computing power at it will eventually lead to cars that can drive themselves more safely than could human beings.
But a lot of other futuristic gadgets are being held back by physical complexity, consumer inconvenience, or a simple lack of value for consumers. These are not problems that more computing power can fix.
A few years ago, Nest was widely viewed as one of Silicon Valley's brightest stars. Founded by Tony Fadell, a key figure in Apple's iPod team, Nest aimed to produce a line of user-friendly, connected home appliances. Given Fadell's Apple background and Nest's focus on hardware, many people wondered if Nest would become the new Apple.
But last Friday, Fadell announced he was stepping down as Nest's CEO after months of criticism for an erratic management style and slow growth at the company.
The company's first product, the Nest Learning Thermostat, got rave reviews when it was introduced in 2011. Nest added a smoke detector to its product line in 2013. Google was so impressed by the company that it paid $3.2 billion for it in 2014.
But since then, Nest has struggled. It acquired Dropcam in 2014 and rebranded Dropcam's flagship security camera as the Nest Cam in 2015. Beyond that, Nest hasn't introduced a single new hardware product, and it looks increasingly unlikely that it can justify that lofty acquisition price.
It's not surprising that Fadell is stepping down after years of disappointing performance. But Nest's problems go beyond the failings of any single CEO. The larger problem seems to be that consumers just don't seem that interested in buying a bunch of expensive "smart home" gadgets.
The beginning of the end of Fadell's tenure as Nest CEO came in March, when the Information's Reed Albergotti published a scathing portrait of Fadell's leadership. Albergotti portrayed a company in chaos, with low morale and a stalled product road map. Albergotti placed much of the blame for Nest's poor performance on Fadell and his abrasive and erratic management style.
According to Albergotti, more than half of the 100 Dropcam employees Nest hired when it acquired the company two years before had left by March 2016. In this kind of situation, most CEOs would be diplomatic. Not Fadell. "A lot of the employees were not as good as we hoped," Fadell told Albergotti in an interview.
Fadell's comments infuriated Greg Duffy, a Dropcam co-founder who left Nest in 2015. In a scathing Medium post, he argued that half his team had left Nest because "they felt their ability to build great products being totally crushed."
"All of us have worked at big companies before, where it is harder to move fast," Duffy wrote. "But this is something different, as evidenced by the continued lack of output from the currently 1200-person team and its virtually unlimited budget."
Fadell's critics say that he is both too prone to micromanaging subordinates and too prone to changing his mind. As a result, Nest employees seem to be stuck in an endless cycle of product revisions, causing new releases to be delayed.
Of course, much of this could have been sour grapes from a disgruntled minority of engineers. But the fact that Nest's hundreds of employees hadn't produced a new product in more than two years certainly seemed like an ominous sign.
In a Tuesday email, Nest spokeswoman Ivy Choi disputed claims that Nest has suffered from slow growth and poor employee morale. "Since Nest began shipping products 4.5 years ago, Nest revenue has grown in excess of 50% year over year," she noted.
Choi added that the most recent version of the Nest thermostat — released in 2015 — sold a million units in half the time as the previous version, and she pointed to high marks the company has earned at the employment website Glassdoor.
One reason for Nest's slow development of new hardware products is that Nest has been spending time making sure its products work together seamlessly with each other and with devices created by third parties.
There are now lots of internet-connected devices on the market, but a big problem with many of them is that they require too much effort to set up and manage. It's hard enough to convince someone that it's worth paying a premium for an internet-connected lightbulb or washing machine. It becomes an even harder sell if customers are required to separately configure devices from different companies.
Part of the value proposition for connected devices is their ability to work together — for example, to turn off all the lights in your house with a single tap on your smartphone. But this becomes more — rather than less — of a hassle if you have to open several different apps to turn off the lights.
Nest hopes to play a central role in solving this dilemma. Over the past couple of years, the company has convinced the manufacturers of a wide variety of products — from lightbulbs to washing machines — to participate in a program called "Works with Nest."
For example, you can configure internet-connected smart lightbulbs all over your house to flash when the Nest smoke detector detects a fire. Or if you have an internet-connected lock on your front door, you can program your thermostat to turn down the heat when you leave the house.
Building the software infrastructure to make all these devices work together takes more effort than merely building a suite of standalone products. But if Nest succeeds in establishing Works with Nest as an industry standard, it could give Nest a long-term competitive advantage.
When I wrote a feature article on connected devices two years ago, I argued that these tiny devices represented the latest step in a long-term evolution of the computer industry. Every decade or two, there's a new generation of computing products that is dramatically smaller and cheaper than its predecessors. The washing-machine-size minicomputer of the 1960s was displaced by the desktop PC in the 1980s, which was replaced by the pocket-size smartphone in the 2000s.
Each new generation found a much larger market than the one that came before. The PC market dwarfed the earlier mainframe and minicomputer markets. Today the smartphone industry has eclipsed PCs.
Since the smartphone revolution a decade ago, computer chips have continued to get smaller and cheaper. You can now buy a thumbnail-size computer-on-a-chip, including wifi networking capabilities, for a fraction of the cost of a smartphone. It's a reasonable guess that these tiny chips could set off another round of disruptive innovation — and that this could set the stage for the next great computing platform.
But so far, there's not much sign of this actually happening. Wifi-connected lightbulbs, thermostats, crock pots, and smoke detectors have been on the market for several years now, and they don't seem to be generating anything like the level of enthusiasm or market demand that smartphones and PCs did in previous generations.
Nest's smart thermostat has some valuable features that pre-internet thermostats were missing. But they don't offer the kind of revolutionary capabilities that caused people to line up to buy an iPhone. We just don't interact with our thermostats that much, so there's only so much a better thermostat can do to improve our lives.
In other cases, like connected lightbulbs, the value proposition is even more opaque. It's possible to think of exotic circumstances in which someone would want their lightbulbs connected to the internet. But for most people, most of the time, old-fashioned lightbulbs work just fine.
The fact that consumers have so far greeted connected household devices with a yawn suggests that the Apple business model — the high-quality, high-margin model Nest is implicitly following with its own products — might not be the one that wins this market.
Apple and its competitors have been able to sell hundreds of millions of iPhones because it's obvious why it's useful to have an internet-connected computer in your pocket. People look at their smartphones dozens of times every day, so even small improvements to the user experience are worth paying a premium for. And progress has been so rapid that Apple has been able to sell a new iPhone to customers every two to three years.
Connected home devices don't seem to be like that at all. A smarter thermostat is nice, but normal people don't spend a lot of time interacting with their thermostats. So it's not obvious that they're going to be willing to pay a big premium for the one with the best interface. And people want to upgrade their thermostats every two or three decades, not every couple of years, so the number of devices you can sell every year is going to be much smaller.
All of that means that selling smart thermostats is likely to be much less lucrative than selling smartphones.
And smart thermostats are the best-case scenario. For other connected devices — like smart lightbulbs, crock pots, and washing machines — the benefits of an internet connection seem slight and the advantage of a highly polished user interface is slighter still. When shopping for a washing machine, people mostly care about how good it is at cleaning clothes. Few are going to be upset if it has a clunky user interface and can't be controlled with an app.
Of course, it would be silly to conclude that connected devices are never going to become mainstream. Eventually, wireless chips will become cheap enough that non-technology manufacturers can incorporate them into their products without significantly increasing the sale price — much in the way that small digital clocks are now routinely incorporated into toasters, microwaves, and other appliances. And standardized software platforms will allow them to build user-friendly connected devices with minimal engineering knowhow.
But that might not leave much room for a company like Nest, whose business model depends on people paying a premium to get the best available connected devices. People may only start buying these devices en masse when they become so cheap and user-friendly that customers don't have to think about the connected features.
Home 3D printing was supposed to be the next big thing.
"You’ve heard of 3D printers, but you probably don’t own one yet," wrote Wired editor Chris Anderson in 2012. Anderson was profiling MakerBot, a company looking to bring a 3D printer into everyone's home. He compared MakerBot's new $2,200 gadget to the PCs of the 1970s.
A 3D printer lets you transform a 3D model of a digital object — say, a model of the Colosseum, a whistle, or an elaborate marble machine — into a physical object. A few years ago, enthusiasts imagined a future where ubiquitous 3D printing rendered a lot of conventional manufacturing obsolete, as people printed everything from dishes to automobile parts at home instead of buying them in a store or online.
But four years later, the home 3D printing revolution hasn't panned out, as even MakerBot spokesman Johan Broer conceded when I talked to him last month.
"We were very focused on the consumer market around 2014," he said. "Back then the expectations for the consumer market were very high."
But it turned out that the average household doesn't have a lot of need for 3D-printed goods. And when they do have use for them, it's simpler to order from an online 3D printing service than to buy a 3D printer.
"A lot of people within the industry thought the consumer market would grow faster," Broer told me in a phone interview. But the expected demand for cheap home 3D printers never materialized. So in 2015, "we changed our strategy to focus on the education and professional space."
It wasn't an easy transition. The company went through two rounds of layoffs in 2015 and had to shutter its retail stores aimed at drumming up interest among consumers. Then in April 2016, the company announced it was shuttering its domestic manufacturing operations. Instead, its products will be manufactured in China.
MakerBot's struggles in the home 3D printing business have not surprised industry analyst Terry Wohlers. "We've never felt there was a market for consumer printers," he told me in a May interview.
"This notion of consumers buying their own machine and printing for themselves just is not working out, because it's not easy," he said. "You need to have some design talent, and most people aren't designers. You need to learn design software, and most people don't want to mess with it."
And 3D printers cheap enough for the consumer market tend to be less sophisticated than the industrial-strength models, Wohlers added.
"3D printers that are affordable are limited in size, material color, surface finish, a lot of things," he said. "You're really limited as to what you're going to print with them, even if you have some design experience."
The result: Home 3D printers are too expensive for amateur tinkering but not sophisticated enough for professional use. Ultimately, they're not that compelling to anyone.
To be clear, this doesn't mean that the 3D printer market is a failure across the board. Quite the contrary. Wohlers has collected data showing robust growth in 3D printing overall even as home 3D printing companies have struggled:
Most of these sales have been to commercial and academic customers rather than home users. Engineering and design firms use the printers to help with rapid prototyping. Industrial-grade 3D printers can work with more types of materials, they can print with multiple materials at the same time, and they can print larger objects.
And while few homes are choosing to buy 3D printers, there's robust demand for 3D printing as a consumer service. Websites like Shapeways let you order 3D-printed items online. These services let you avoid all the hassle of running the 3D printer and just send you the printed result.
Unless you're in a big hurry — and people rarely have an urgent need for a plastic model of the Colosseum — ordering 3D printed items is likely to make more sense for most people. Shapeways offers a wide variety of materials — plastic, metal, ceramics, and more — more options than a consumer-grade 3D printer is ever likely to offer.
A 3D printer is extremely useful for certain tasks, like rapid product prototyping or personalization. But don't expect 3D printing to replace conventional manufacturing any time soon.
On the one hand, 3D printers are nowhere close to being able to reproduce complex gadgets. Most 3D printers can only deposit one or two materials at a time, so it's not easy to manufacture a product like a smartphone that has metal, glass, plastic, and other materials inside of it. That's to say nothing of the complex computer chips whose microscopic features are far too tiny for any 3D printer to reproduce.
On the other hand, 3D printing isn't a very efficient way to produce simple objects. Injection-molding techniques, for example, allow people to produce thousands of identical copies of plastic objects in a matter of minutes with minimal human involvement. For simple products being manufactured in bulk, conventional manufacturing techniques will just be more economical than printing them one at a time on a 3D printer.
This shouldn't be too surprising. After all, most of us have printers that — in theory — could be used to print out entire books. But in practice, it makes more sense to buy printed books produced using conventional techniques. Printing out a book on your home printer is tedious and produces an inferior result. And thanks to economies of scale, printing a book the old-fashioned way is often cheaper than home printing.
An individual retirement account (IRA) helps people shield their retirement savings from the tax man. It's especially useful for people who don't have access to an employer-sponsored 401(k) retirement plan, as well as for people who are rich enough to contribute the legal 401(k) maximum — currently $18,000 per year — and still have savings left over.
However, if you're a high earner trying to squirrel away extra cash in an IRA, you're likely to run into a big roadblock: IRAs have maximum income limits. The details are a little bit complicated (I'll explain more below), but in a nutshell if you make more than $132,000 — or more than $194,000 if you're part of a couple — you can't contribute directly to one type of IRA (called a Roth IRA) and lose a crucial tax deduction for the other type.
Presumably, Congress felt that a family making $200,000 per year was going to do fine without extra help from the tax code. But in 2005, Congress created a loophole that made these income limits effectively toothless. It's called a backdoor Roth IRA, and it provides high-income Americans with a sneaky but almost certainly legal way to save an extra $5,500 per year without paying taxes on their earnings.
A traditional IRA is like a 401(k) plan you can sign up for without help from an employer. You don't have to pay income taxes on money you contribute to your IRA, and money in the account grows tax-free. You only have to pay taxes when you start withdrawing money from the IRA during your retirement years.
The Roth IRA flips the traditional IRA on its head: You pay taxes on money you contribute to the IRA, but earnings and withdrawals are tax-free.
This is an either-or choice. In any given year, you can contribute $5,500 to a traditional IRA or a Roth IRA, but not both. Which one you should choose depends primarily on whether you expect to be in a higher tax bracket in retirement than you're in now. If you think your tax rate is going to go up, you should pick a Roth IRA; otherwise, contribute to a traditional account.
Things get more complicated as your income goes up. If you make more than $71,000 ($118,000 for married couples filing joint tax returns), you lose the ability to deduct IRA contributions from income taxes. With the biggest tax advantage of traditional IRAs eliminated, most households above this cutoff are better off investing in a Roth IRA instead.
But then when a worker's income reaches $132,000 ($194,000 for couples), she loses the ability to make Roth IRA contributions. Until 2010, people too rich to contribute to a Roth IRA just had to live with getting a smaller tax benefit from a traditional IRA. They got no tax deduction for contributing to the IRA, but there was still some benefit to being able to defer taxes on capital gains and dividends.
In 2005, Congress passed legislation that — once it took effect for the 2010 tax year — made the nominal limits on Roth contributions practically irrelevant.
You can convert a traditional IRA into a Roth IRA, but prior to 2010 you could only do this if your income was below $100,000. But then Congress eliminated that income limit, allowing anyone to convert traditional IRA money into Roth IRA money.
If a traditional IRA contains pre-tax money (e.g., the account owner took a tax deduction when he made the contribution), then the account owner has to pay income taxes when he converts the money to a Roth IRA. But wealthy taxpayers don't get an IRA tax deduction in the first place; they have to contribute to traditional IRAs with after-tax money. So when they convert a traditional IRA to a Roth IRA, they only owe taxes on the earnings — which will be small if the money was only in a traditional IRA for a short period of time.
So the backdoor Roth technique provides a two-step way for rich people to get money into a Roth IRA without running afoul of income limit. First, contribute money to a traditional IRA. Then convert the account to a Roth IRA.
I asked Jeff Levine, an IRA expert at the advising firm Ed Slott and Company, if there were any pitfalls people should watch for when making a backdoor Roth contribution.
The biggest pitfall, he told me, occurs if you already have a traditional IRA with pre-tax money in it.
"Let's say you have a $45,000 IRA account at one institution," Levine said, and that this money came from tax-deductible contributions at a time when your income was lower. Then you get a big raise that makes you ineligible for the IRA tax deduction. So "you decide to open a brand new IRA at a second institution and make a $5,000, non-deductible contribution."
You might think you can convert the $5,000 traditional IRA into a Roth IRA tax-free, since the contribution wasn't tax-deductible in the first place. But that's not how it works. The IRS treats your IRA money as if it all comes from one big bucket — as if you have a single $50,000 IRA — and treats conversions as if they drew proportionally from all of your IRAs.
In this case, since 90 percent of your total IRA funds are pre-tax and 10 percent of the money is after-tax, any Roth conversion will be 90 percent taxable and 10 percent tax-free. If you converted $5,000, you'd have to pay taxes on $4,500 of it, while only the remaining $500 would be treated as a tax-free conversion.
If you're in this situation, you have a couple of options. The best option is if you have a 401(k) plan at work that accepts incoming IRA rollovers. Then you can transfer all of your pre-tax IRA money into your work retirement account before doing a backdoor Roth conversion. Money in a 401(k) plan isn't counted when the IRS applies these IRA aggregation rules. (Click here for a detailed explanation of this strategy.)
If you don't have access to a 401(k) plan that will accept your pre-tax IRA money, then you may have to just bite the bullet and convert all of your traditional IRA money into a Roth IRA. Of course, if you have a lot of pre-tax money in IRAs, this could mean a huge tax bill. But if you expect your income to be above the Roth contribution limits for the foreseeable future, it might be worth it.
The conversion has tax benefits in its own right, since it'll save you from paying even more in taxes once you reach retirement age. And once you've done this conversion once, you'll be able to take advantage of backdoor Roth contributions every year.
A few investment experts, notably blogger Michael Kitces, have claimed that the backdoor Roth technique could get you into hot water with the IRS. The issue is a rule called the step transaction doctrine, which says that taxpayers can get into trouble if they take a sequence of otherwise legal transactions that produce an illegal result.
But Levine disagrees. "I don't have any concerns about that whatsoever," he told me, and he says he uses the technique himself.
Over the past few years, the backdoor Roth technique has become increasingly mainstream. Vanguard, one of the nation's largest mutual fund companies, has a page on its website advising clients on how to make backdoor Roth contributions with no caveat about possible legal complications. According to Vanguard's data, about 20,000 Vanguard customers made backdoor Roth contributions for the 2013 tax year, and there's no sign that any of them got into trouble for it.
If you are concerned about the fairly remote possibility that the IRS might someday start cracking down on backdoor Roth contributions, skeptics like Kitces recommend introducing a delay between the time you make the initial IRA contribution and the time you convert the money to a Roth. The longer the delay between the two steps, the harder it is for the IRS to argue that they should be regarded as a single transaction for tax purposes. A one-year delay is the safest option, but a delay of a week or a month could theoretically prove beneficial as well.
But Levine believes a year-long delay is overkill — and it means paying taxes on earnings accumulated while the money was in the traditional IRA. Levine has never heard of anyone getting into trouble for using the technique, and he notes that the IRS itself has signaled that it doesn't believe a delay is required to make the technique kosher.
One final sign that backdoor contributions are legal, Levine argues, is that the Obama administration keeps asking Congress to make them illegal. That suggests the administration believes they're currently legal. Otherwise, Obama wouldn't need Congress's help here — he could just ask the IRS to start cracking down on the practice.
And Obama has a point. Whether or not you think rich people deserve larger retirement tax breaks, the current policy makes no sense. Either rich people should be allowed to contribute to Roth IRAs or they shouldn't be — but it makes no sense to require them to go through an awkward two-step procedure to benefit from the Roth IRA's tax breaks.
The US economy created 38,000 jobs in May, the slowest pace of job growth in five years, according to disappointing statistics released today by the Labor Department. It's an ominous sign for the US economy — and for Hillary Clinton's chances of beating Donald Trump in the November election.

The US economy needs to add about 150,000 jobs a month to keep up with population growth. In the past couple of years, the economy has been doing a bit better than that, adding 200,000 jobs in a typical month.
But the May report suggests that the economy may be starting to slow down in a dramatic way. Not only did job growth fall well short of economists' expectations in May, but the Labor Department also revised its estimates for March and April job growth downward by a total of 59,000, suggesting that there are actually fewer people employed than we thought a month ago.
What accounts for the drop? One factor is the strike among Verizon workers, which cost the economy about 34,000 jobs. Those jobs should reappear in future reports. But that's hardly enough to account for a totally underperformance of more than 200,000 jobs.

There's other bad news in the report too. Over the past six months, the economy had started to reverse a years-long decline in the labor force participation rate — a sign that a healthy economy was starting to draw workers who had left the economy back in. But the latest report shows the economy has given most of those gains back, with the labor force participation rate falling from 63 percent in March to 62.6 percent in May.
And that's the right context in which to view the one piece of seemingly good news in the report: the unemployment rate falling from 5 percent to 4.7 percent. Normally, a fall in the unemployment rate would be good news, but in this case it appears that a lot of people simply stopped looking for work and left the labor force — hardly a sign of progress.
Slow economic growth is never good news, but the stakes are especially high now, six months ahead of a high-stakes presidential election. Political science suggests that the performance of the economy in the months before a presidential election has a big impact on election outcomes. In this case, poor economic performance is bad news for Democrats and Hillary Clinton in general. If this month's report signals the start of a recession, that would boost Donald Trump's chances of becoming the next president.
At the same time, it's important to note that jobs data is inherently noisy — sometimes the economy delivers a month of bad job gains and then resumes its upward trajectory. So while the latest data is a reason for workers to worry, we shouldn't panic yet.
Financially speaking, Uber's new $3.5 billion investment with Saudi Arabia isn't that significant. It is bigger than Uber's previous fundraising rounds. But Uber had previously raised a sequence of $1 billion investments over the past couple of years, so it's hardly a game changer.
But politically speaking, Uber's decision to take money from Saudi Arabia's sovereign wealth fund could become a huge deal. The Saudi government doesn't just get a 5 percent stake in the ride-hailing startup; it also gets a seat on Uber's board. That means the Saudi government will be more than a silent partner — it will have a literal seat as the company discusses big strategic decisions.
And for people worried about issues like gender equality, customer privacy, and human rights, it's hard to imagine a worse choice for Uber's newest board member. The Saudi regime is notorious for its unequal treatment of women — who aren't even allowed to drive in the Saudi kingdom — and for its disrespect for human rights in general. By cozying up with Saudi Arabia, Uber CEO Travis Kalanick is sending a clear signal that he intends to run Uber as an amoral profit-maximizing machine.
And that's a big problem, because Uber's long-term success is going to depend on earning the trust and respect of both regulators and customers. There's a good chance that car sharing will be a winner-take-all market, and Uber is trying to become the dominant player in markets around the world. Monopolies inevitably face public scrutiny and pressure for regulation, and it will be a lot harder to resist that pressure if the public views Uber as a company without a conscience.
Saudi Arabia is nominally a US ally, but the kingdom has a dismal record on human rights. According to Human Rights Watch, "Authorities subjected hundreds of people to unfair trials and arbitrary detention." The Saudi government persecutes human rights activists, subjecting them to decade-long prison sentences for advocating political reforms and talking to foreign reporters.
There's every reason to expect the Saudi government to continue its repressive policies in the coming years. And now when the Saudi government violates human rights, Uber will get bad press for it.
The stickiest issue for Uber will likely be Saudi Arabia's treatment of women. Saudi Arabia is infamous for refusing to allow women to drive and for limiting their ability to go out in public without a male chaperone. Uber is likely to face awkward questions about whether its partnership with the Saudi government amounts to an endorsement of these policies.
Uber has also faced criticism for sexual assaults committed by Uber drivers, and for allegedly downplaying the extent of those assaults. It's not clear how much blame Uber deserves for these assaults — after all, taxi drivers commit assault as well. But Uber needs to convince its women customers that it takes this issue seriously and is doing everything it can to keep passengers safe.
That's going to be harder to do now that Uber has given one of its board seats to a regime that once punished a rape victim for being alone with a male non-relative. Having a Saudi board member isn't going to prevent Uber from improving women's safety, but it does seem like a signal that women's rights are not a top priority for the company.
The Saudi royal family also invested $248 million in Uber's biggest US rival, Lyft, last December. But beyond the smaller cash figure, there were a couple of other significant differences. Lyft didn't give the Saudi royal family a board seat as Uber did. Also, Lyft doesn't have operations in Saudi Arabia. Uber does. So Uber is more directly tied to that nation's misogynistic policies.
Uber's deal with Saudi Arabia is the latest in a long line of decisions that seem to have been made without considering their impact on Uber's public image. Over the past few years, Uber has faced accusations that it has spied on its customers and suggested digging up dirt on journalists. It has generated a lot of ill will with massive surge pricing increases on busy nights.
Uber has long cultivated a reputation as a scrappy startup, willing to do whatever it takes to win. That was an appealing image when Uber was actually an underdog. But with a global footprint and billions of dollars in the bank, Uber is no longer an underdog. And when a big, powerful company adopts a take-no-prisoners approach to business, it doesn't seem charming. It seems menacing.
And that matters because the biggest long-term threat to Uber is that a public backlash against the company could lead to much stricter regulation of its business practices. So far, regulators have taken a hands-off approach in the — correct — belief that minimal regulation will allow innovation to flourish. But if Uber continues to grow — and continues to behave in ways that undermine public confidence in the company — it's only a matter of time before the political winds start to shift.
Uber has raised an astonishing $3.5 billion from Saudi Arabia's sovereign wealth fund. It's one of the biggest venture capital investments in history and brings Uber's overall fundraising haul to $11 billion. But while Uber is bragging about the investment, it could reveal a troubling trend in investment trends overall.
In the long run, economic growth depends on our ability to convert cash into productive assets like factories, trucks, machinery, or computer software. But for the most part, recent "investments" in Uber aren't like that. Uber is planning to use its billions to fund brutal, zero-sum price wars with competitors around the world.
Those investments might allow Uber to expand its share of the global ride-hailing market and make big profits for its investors. But money spent on money-losing price competition isn't investment. Price wars do nothing to increase the world's productive capacity.
So the fact that so much money is being invested in Uber — and in other companies deliberately losing millions in an effort to gain market share — could be an ominous sign. It suggests that it's getting harder and harder to spend money in ways that boost long-term economic growth.
Running an on-demand service like Uber isn't very expensive. It costs some money to build the Uber app and run the servers that power Uber's car-sharing service, of course. But the most valuable assets required to provide Uber's service — cars — are owned by drivers who work as independent contractors.
Uber says it needs money to finance expansion internationally to places like China and the Middle East. But when you run a purely online ride-hailing service that doesn't own any vehicles, there's no reason it should cost billions to expand into new markets.
Uber needs billions of dollars because it's planning to wage brutal price wars with competitors around the world. The company is planning to lose money on every ride and hope it can outlast its competitors.
I have firsthand experience with how wasteful this process can be. In 2014, I spent a week driving for Lyft. At the time, Lyft was running a promotional program where new drivers could earn $1,500 in a week if they drove for at least 50 hours. I signed up and drove 50 hours in my first week. During those 50 hours, my passengers paid a total of $596. Lyft paid me $1,500, which means that my week as a Lyft driver cost Lyft's investors $904 (I donated the full $1,500 to charity). To put it another way, Lyft's average profit margin on my rides was negative 150 percent.
This is obviously an extreme example. That $1,500-per-week offer was only in effect for the first month, and Lyft presumably hoped drivers would stay on and work for lower pay in subsequent months. But something similar is happening in other parts of the world. As the New York Times puts it, "China is a difficult battleground, as Uber is spending millions in a subsidy war with Didi Chuxing, the dominant ride-hailing start-up in the country."
Like many technology markets, ride hailing exhibits strong network effects: The more drivers a network has, the more attractive it is to passengers, and vice versa.
And network effects can be very profitable. The world's richest man, Bill Gates, made his billions from the network effects surrounding the world's most popular operating system, Windows. The world's richest man under 50, Mark Zuckerberg, is currently profiting from the network effects from owning the world's most popular social network, Facebook.
It's a reasonable guess that ride hailing will also become a winner-take-all market, with the winner reaping huge profits.
But because ride-hailing services operate in the physical world, competition happens on a city-by-city basis. Different Uber competitors — Lyft in the United States, Gett in Europe, Didi Chuxing in China, Ola in India — are strong in different parts of the world. So in each of these markets, Uber needs cash to allow it to operate at a loss — potentially for years — so it can gain market share and prevent overseas rivals from gaining the upper hand.
Uber isn't alone. Across Silicon Valley, companies are pouring millions of dollars into money-losing price wars, in hope that they'll eventually be able to turn a profit once their competitors are driven out of business.
For example, in recent years there have been dozens of companies — including Instacart, DoorDash, Blue Apron, GrubHub, and HelloFresh — offering app-based food delivery services. Amazon and Google have also gotten into the market with delivery services of their own.
While the exact business models differ, the basic idea is very similar: People use the internet to order food. And because there are so many companies with similar business models, they wound up taking big losses in an effort to gain market share. Recently companies have started slashing compensation for their delivery drivers in an effort to finally turn a profit.
These price wars were obviously great for consumers while they lasted. But the process of "investing" in money-losing price cuts as a means to gain market share is fundamentally different from investing in productive assets like factories, stores, or computer software.
When companies spend billions in a race to build more advanced factories or better software, that boost's society's total productive capacity. Tesla, for example, recently raised $2 billion to help it expand its production facilities and meet its goal of producing 500,000 electric cars per year. Even if Tesla ultimately fails to turn a profit and goes bankrupt, someone will likely acquire the production facilities Tesla is building and put them to use.
And to be sure, Uber is doing some of this kind of investment itself. Uber recently opened a research facility in Pittsburgh to develop self-driving car technology. That could produce technology of lasting value.
But a large share of Uber's money is "invested" in a price war, which produces nothing of lasting value. Venture capitalists hope their millions will buy them a share of the monopoly profits that will exist once the price war is over. But they're fighting over a fixed pie of profits — a long, expensive price war doesn't lead to larger industry profits in the long run.
The massive investments in perpetually money-losing companies wouldn't be so worrisome if it were happening alongside big investments in companies using the money in more conventional ways. The really ominous thing about Uber's investments is that they dwarf most other venture capital spending. Companies like Tesla — companies that can transform a billion dollars into productive capital assets — seem to be few and far between. And that's a bad sign for the long-term growth of the US economy.
Moments after explaining that we're all probably characters in an advanced civilization's video game, Elon Musk was asked how he hopes to see laws made on Mars if he is, in fact, successful in setting up civilization on the red planet. His answer was characteristically interesting:
Most likely the form of government on Mars would be direct democracy, not representative. So it would be people voting directly on issues. And I think that's probably better because the potential for corruption is substantially diminished in a direct versus a representative democracy.
So I think that's probably what would occur. I would recommend some adjustment for the inertia of laws. That would be wise. It should probably be easier to remove a law than create one. That's probably good. Laws have infinite life unless they're taken away.  So I think my recommendation would be something like 60 percent of people need to vote in a law but at any point greater than 40 percent of people can remove it. And any law should come with a built-in sunset provision. If it's not good enough to be voted back in...
That would be my recommendation. Direct democracy where it's slightly harder to put laws in place than to take them away and where laws don't automatically just live forever.
Musk knows far more about Mars than I do, but my guess is this is a recipe for tremendous statutory instability, which seems like a dangerous thing for a young civilization in a harsh climate. Musk is basically taking the structure of the US Senate, where the filibuster creates a 60-vote supermajority for passing laws, and adding a provision that makes undoing existing laws easier. The result is a system that is biased against action and biased toward the reversal of past actions.
I can sort of understand why you might want that system in present-day America, where we already have a lot of laws and you might see diminishing returns to new legislation and mounting costs to old legislation. But in a new society operating under harsh and uncertain conditions, it seems to me that you'd want it to be easier to act, and you wouldn't want to add political instability to what would already be a massive amount of environmental instability.
On some level, I find it particularly odd that Musk is proposing this kind of system: In order to get to Mars, he famously keeps tight control of his companies (Musk has kept SpaceX private, and said he only brought Tesla public because "he didn't have any choice"), and he is famous for backing bets and plans that most people think are nuts, and doing so till long past the point when normal investors would pull their money.
Musk is, in other words, someone who believes that tough missions require institutions where action is relatively easy and risky decisions are given ample time to pay off. The political system he favors, however, is one in which action is nearly impossible and laws would be removed the second that 40 percent of the population lost their nerve. I recognize governments should be run differently than corporations, but this seems like a very sharp correction in a very un-Muskian direction.
.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }

Correction: This post initially said neither Tesla nor SpaceX were public. In fact, Tesla is a publicly traded company.
A few years ago, the database company Oracle sued Google, arguing that Google's Android operating system infringed the copyright of Oracle's Java technology. On Thursday a jury sided with Google, ruling that copyright's fair use doctrine allowed Google to build Java-compatible software without getting a license from Oracle.
This is not the final word in the dispute. Oracle is almost certain to appeal the case to the Federal Circuit Appeals Court, which is known for its pro-patent and pro-copyright views and has already ruled for Oracle once in the case.
The immediate stakes in the case are not that interesting for anyone but the companies involved — either Google will have to write Oracle a big check or it won't. But the case will set a precedent that could have a big impact on the software industry more broadly. If Oracle's legal theory is upheld by the appellate courts, it could hamper efforts to improve software compatibility.
The lawsuit focuses on technical decisions Google made when it created the Android operating system.
Google wanted people who wrote programs in the popular programming language Java to be able to reuse their code in Android apps. To do that, Google had to ensure that Java code written for other purposes ran exactly the same on Android. But negotiations with the company behind Java, Sun Microsystems (which was later acquired by Oracle), broke down, so Google decided to create its own version of Java from scratch.
Google's version of Java didn't reuse any code from Oracle's version. But to ensure compatibility, Google's version used functions with the same names and functionality.
This practice was widely viewed as legal within the software world at the time Google did it, but Oracle sued, arguing that this was copyright infringement. Oracle argued that the list of Java function names and features constitutes a creative work, and that Google infringed Oracle's copyright when it included functions with the same names and features.
Google argued that the list of function names, known as an application programming interface (API), was not protected by copyright law.
Google's defenders pointed to a landmark 1995 ruling in which an appeals court held that the software company Borland had not infringed copyright when it created a spreadsheet program whose menus were organized in the same way as the menus in the more popular spreadsheet Lotus 1-2-3.
The court held that the order of Lotus 1-2-3 menu items was an uncopyrightable "method of operation." And it concluded that giving Lotus exclusive ownership over its menu structure would harm the public:
Under Lotus's theory, if a user uses several different programs, he or she must learn how to perform the same operation in a different way for each program used. For example, if the user wanted the computer to print material, then the user would have to learn not just one method of operating the computer such that it prints, but many different methods. We find this absurd.
Google believed that its own copying was directly analogous to what Borland had done. There were thousands of programmers with expertise in writing Java programs. By designing its platform to respond to the same set of programming commands as Oracle's Java system, Google allowed Java programmers to become Android programmers with minimal training — just as Borland's decision to copy Lotus's menu structure avoided unnecessary training for seasoned Lotus 1-2-3 users.
In 2012, Judge William Alsup agreed with Google. He ruled that copyright only protects the creative aspects of a work, not functional characteristics. Alsup ruled that because the names of Java functions was essential to achieving interoperability, they were a functional characteristic rather than a creative aspect of Java, and using them wasn't copyright infringement.
But in 2014, the Federal Circuit Court of Appeals disagreed. The court was unimpressed with Google's argument that function names were functional characteristics not protected by copyright. In the Federal Circuit's view, the list of Java functions was just another kind of "code" that couldn't be copied without its creator's permission.
But the case wasn't over. APIs might be protected by copyright law, but Google could still invoke copyright's fair use doctrine to justify its use of them. The case was sent back down to Judge Alsup's courtroom, where he was asked to assume (contrary to his previous ruling) that APIs were copyrightable and hold another trial focused on the fair use question.
Now Google has prevailed again in Alsup's courtroom, with the jury ruling that Google's use of the Java APIs were, in fact, fair use.
But the case still isn't over. It will next go back to the Federal Circuit. And given its track record, there's a decent chance it will overrule the lower court's decision once again. Then the losing party will have the chance to appeal to the Supreme Court, which declined to hear the case during the first go-round.
There's a long tradition of computer programs being designed to be compatible with other programs created by third parties. To do that, the developer of the new software needs to copy certain functional characteristics of the old software. Often, software companies have good reasons to do this even when the author of the old software objects.
One good example is the open source project Samba. It was created to allow users of open source operating systems such as Linux to share files with Windows users. To do that, the Samba programmers reverse-engineered and then duplicated the functionality of the Windows file-sharing system. They didn't copy any Microsoft software, but they did duplicate the sequence of commands needed to transfer files, read the contents of folders, and perform other functions in the Windows file-sharing system.
As an open source project, Samba became hugely popular. The early versions of Mac OS X included a version of Samba, and the software was incorporated into a lot of standalone file servers.
The legality of projects like Samba has been widely accepted for more than two decades. But if Oracle's arguments are accepted by the courts, Microsoft might be able to sue Samba for copyright infringement. If the Federal Circuit's precedent isn't overturned, companies could become more reluctant to reverse-engineer competitors' products in order to make them compatible.
A 2014 brief by the Electronic Frontier Foundation offered another example of a case where copying APIs was valuable:
Jeremiah Flerchinger is an electrical engineer with over ten years of service in the Department of Defense, after previous experience with a machine-tool company. When the National Aeronautics and Space Administration (NASA) sought to repurpose old manufacturing robots for a new project, they asked Flerchinger’s company to manufacture and program updated memory chips to store the robots’ new instructions. Configuring firmware to put on the chips required using obsolete software that wouldn’t run on modern computers. Flerchinger reimplemented the software’s API, creating modern software that could fulfill the same functions and work alongside old machines that had the same API hard-coded into their electronics.
Disclosure: My brother works at Google.
Robo-advisory firms are all the rage in the investment world, as I wrote in a recent article. In that article, I focused on Betterment and Wealthfront, two startups that position themselves as automated online alternatives to conventional human investment advisers.
I argued that for nonwealthy customers, these companies simply didn't provide enough value to be worth the significant fees they charge (though high-income families may benefit from their tax loss harvesting services). A target date mutual fund essentially provides the same service, and the best target date funds cost half as much.
But that critique doesn't apply to another popular robo-adviser service offered by stock brokerage Charles Schwab, because its Intelligent Portfolio service has no fees at all!
Before you get too excited, you need to know that the Schwab plan has a big downside. Instead of charging customers directly, Schwab makes money off Intelligent Portfolio by steering customers into investments that don't always serve customers' interests.
Conventional investment advice holds that investment portfolios should contain a mix of stocks and bonds. But Schwab requires clients to hold a significant portion of their portfolios in cash. The company's own example portfolios allocate between 6.9 and 15 percent of customer money to a cash account.
This is a bad investment strategy. Financial planners do encourage people to keep some money in cash for short-term emergencies. But the cash in a Schwab portfolio isn't really available for use in emergencies because Schwab requires customers to keep a minimum percentage of their portfolio in cash at all times. Taking money out of a Schwab Intelligent Portfolio account requires selling off some of your stock and bond funds — which is exactly what a cash emergency fund is supposed to help avoid.
Even worse, Schwab pays a very low interest rate on customers' cash. Online banks like Synchrony and Ally currently pay around 1 percent interest on deposits. Ultra-safe short-term bond funds can also be expected to earn around 1 percent. In contrast, Schwab's current rate (as of Monday) is a paltry 0.08 percent.
This mandatory cash allocation amounts to a hidden fee for using the Schwab account. If your portfolio is 15 percent cash, then the 0.92 percent gap between Schwab's 0.08 percent interest rate and the 1 percent rate you can get elsewhere amounts to a 0.138 percent hidden fee — not much different from the 0.15 percent fee Betterment charges for accounts over $100,000.
And if you're saving for retirement, it's probably better to put all your money in assets with expected returns of more than 1 percent, so the practical cost of this "cash drag" is likely to be even higher.
Of course, Schwab argues that the conventional wisdom is wrong, and holding some of your portfolio in cash makes sense, providing "ballast" in times of economic turmoil. You can read its argument for a cash allocation here. I didn't find it very convincing, but you might.
But the larger problem here is that there's an inherent tension between Schwab's claim to be acting as an "adviser" and the fact that its profits depend on steering customers to investments that make money for Schwab. You can debate whether holding 6 to 15 percent of your portfolio in cash in your portfolio is a good deal for the customer. But there's no doubt that it's a good deal for Schwab, which can invest the money in higher-yielding assets and pocket the difference.
The same critique applies to Schwab's non-cash recommendations. Like other robo-advisers, Schwab puts clients' money in exchange-traded mutual funds. But there's a big difference: Betterment and Wealthfront invest purely in third-party funds, and both companies say they never take a commission from the funds they recommend.
Schwab, by contrast, sometimes puts client money into funds operated by Schwab itself. Schwab also earns money for some of the non-Schwab funds it sells — though a Schwab spokesperson tells me that these payments are not a sales commission, but rather are for "record-keeping, shareholder services, and other administrative services" Schwab provides to third-party fund providers.
"We work hard to be open and transparent to the client," said Schwab executive Tobin McDaniel when I asked him about this on Monday.
He argues that Schwab has a rigorous process for choosing funds. He said Schwab looks for the lowest-cost option that is "large enough and liquid enough for us to trade," and he points out that low-cost Vanguard funds feature prominently in Schwab portfolios.
Still, when Schwab is choosing between its own funds and others supplied by third parties, it's hard to be confident that the best interests of customers is the company's only criteria.
Betterment and Wealthfront don't have this problem. Their business model is clear and transparent — they make their money by directly charging clients, they recommend only third-party funds, and they don't earn payments from third-party fund providers. That means they make exactly the same amount regardless of which investments they recommend.
It's still the case, as I wrote previously, that a target date fund is the best option for most retirement investors. But if you are interested in robo-advisers, it's worth picking a service with transparent pricing and no conflicts of interest. Schwab's "free" service could wind up costing you more in the long run.
Correction: I misplaced a decimal point and incorrectly quoted Schwab's interest rate as 0.8 percent instead of 0.08 percent.
Anytime a major social media platform makes even minor changes, it generates consternation. So we can expect a lot of griping in the coming hours about Twitter's announcement that it's going to change how it deals with replies and when usernames count against the 140-character limit.
And Twitter didn't do itself any favors by writing a confusing blog post on the subject. I  misunderstood Twitter's plans the first time I read the post. Several of my Vox colleagues did too, and their initial reactions weren't positive.
But now that I've read the post more carefully — and also read the less confusing version of the announcement Twitter wrote for developers — I'm convinced that the vast majority of Twitter users will like the new system for handling replies. In fact, the change is long overdue.
Twitter's handling of replies might be the platform's most confusing feature. Right now it works like this: If you start a tweet with a username, the tweet will only show up in the timelines of people who follow both you and the other person you're mentioning.
This seems natural for those of us who have been using Twitter for years. But I'm constantly seeing newer and less prolific users do this wrong, starting a tweet with a username and not realizing most of their followers won't see it. It's one of the biggest ways in which the platform seems hostile to new users.
And the reason Twitter works this way is essentially a historical accident. In the beginning, Twitter didn't have a concept of replies. Every tweet showed up in the timelines of everyone you followed.
But over time, a social convention developed that starting a tweet with a username signaled that you were replying to someone. This created clutter in people's timelines — following only one person in a conversation was like listening to one side of a telephone call. So in 2009 Twitter tried to help out users by hiding tweets from your timeline if they started with a username you weren't following.
Even after Twitter did this, however, the conversational experience was still far from ideal. The problem was that as you followed more people, multiple conversations got jumbled up in your timeline and it became harder to follow any one discussion.
So Twitter made replies a native feature of tweets. It started keeping track of which tweets were replies to which other tweets, allowing the platform to start showing users entire conversation threads instead of just individual tweets.
So for the past three years, Twitter has used two different systems for identifying conversations. It still hides tweets that start with a username you don't follow. But whether you start a tweet with a username is actually irrelevant for Twitter's threaded conversation feature — Twitter keeps track of that information directly when you click the "reply" button on a tweet.
Twitter realized that this first method for identifying conversations had become anachronistic. If the platform is already directly tracking which tweets are replies to which other tweets, it no longer needs to rely on the earlier hack of looking at whether a tweet starts with a username.
And so this week Twitter will announce that it's going to continue providing users with the same useful feature — only showing conversations when you follow both people involved — without forcing users to worry about whether a tweet starts with someone's username.
This will mean the end of one of Twitter's strangest conventions: putting a period before someone's username to make a tweet visible to everyone. In the future, you'll be able to just start a tweet with someone's username — "@ezraklein is the founder of @voxdotcom" — without having to put a period at the beginning to signal to Twitter that you want everyone to see it. And if you want everyone to see a tweet that's a reply to another tweet, you can do this by clicking the retweet button, a more natural strategy than putting a period at the beginning.
Once you start thinking about replies as a native feature of a tweet, you realize that it's not necessary for reply tweets to include people's usernames at all. If Twitter knows which tweet your tweet was replying to, then it's easy to also figure out whom the tweet is replying to. So rather than forcing you to include the other person's username in the tweet (eating up some of your scarce 140 characters), Twitter is going to start treating that as a separate bit of metadata, like the tweet's date and location, and display it outside of the tweet.
But note that this is only true of people you're replying to, not mentions in general. If you write spammy tweets that mention random people, those extra usernames are still going to count against your character count — preventing people from overusing mentions.
One big question that doesn't seem to be answered by Twitter's announcement is how the company will deal with "Twitter canoes" — situations where two or more people hijack one of your tweets to start a long Twitter argument, flooding your mentions tab with tweets you don't care about.
Right now this kind of thing is constrained by the fact that usernames are counted against character counts, so people have an incentive to remove nonessential people from the thread to give them more characters to argue with. But in the future, you won't have to list the people you're replying to, creating the possibility that dozens of people (Twitter documentation suggests the number could be capped at 50) could get dragged into long-running arguments.
But this shouldn't be a terribly difficult problem to solve. Perhaps Twitter will include a thread-specific mute button that lets you bail out of conversations that have become too tedious. Or maybe Twitter will come up with smarter ways to figure out which conversations you're likely to actually care about.
Either way, cluttering up tweets with the names of the person you're replying to was always kind of a hack, and most users will be happy to see it go away.
Bitcoin has struggled to live up to the hype that surrounded its emergence into the mainstream three years ago. Despite more than a billion dollars of venture capital funding, Bitcoin startups have failed to develop applications that appeal to mainstream customers. And over the past year, the Bitcoin community has become paralyzed by a bitter feud over how — and whether — to expand the network's capacity.
The result: For the first time since its creation, Bitcoin is in danger of losing its status as the world's leading cryptocurrency. The new challenger is a Bitcoin-like technology called Ethereum that has seen a surge of interest from users, developers, and the corporate world. The network's currency, called ether, is now worth more than $1 billion — that compares to Bitcoin's total market value of nearly $7 billion. Last week, a leading Bitcoin startup called Coinbase announced it was adding support for Ethereum to its popular currency trading platform.
The growing excitement about Ethereum reflects the fact that it's a lot more than just a Bitcoin clone. People can use the Ethereum network to make payments, just as they can with Bitcoin. But the network can do a lot more than that.
Ethereum is a new kind of virtual computing platform. Its most exciting feature is its ability to create binding financial agreements that can be enforced entirely by software — no involvement by courts or other human mediators required. That, in turn, has made possible virtual organizations that exist only on the internet. One such organization, called the DAO, has raised more than $150 million in virtual currency to fund further work on Ethereum-based technologies.
Like Bitcoin, Ethereum represents a technological breakthrough, allowing people to do things purely in software that weren't possible before. But the big question about Ethereum is whether it has practical applications. Ethereum has gotten techies excited, but so far no one has created an application for Ethereum — or Bitcoin, for that matter — that has appealed to mainstream consumers.
Bitcoin is a global payment network like Visa or MasterCard, but with an essential difference: There's no company with ownership or control over the network. Instead, computers all over the world cooperate to maintain a shared record of transactions called a blockchain.
The key innovation that made this work was a clever scheme for rewarding computers that help build this shared ledger. Computers that participate are rewarded with freshly created bitcoins worth thousands of dollars every hour. As a result, there's no shortage of volunteers to contribute computing power to helping process Bitcoin transactions.
The Bitcoin network is custom-designed to verify and record payments. In 2014, a 20-year-old programmer named Vitalik Buterin realized that he could create a Bitcoin-like network that could perform a much broader range of computational tasks. If Bitcoin is a distributed version of Visa or MasterCard, Ethereum is a bit like a distributed version of cloud computing platforms run by companies like Amazon and Microsoft.
Not only can you use Ethereum to make ether-denominated electronic payments, you can also spend ether to run programs on the Ethereum network itself.
Ethereum is a very unusual cloud computing network. Every calculation is performed simultaneously by thousands of computers around the world, making it thousands of times less efficient than a conventional online server. And because the results of these calculations are stored on the Ethereum blockchain, all data is public. So Ethereum would be a terrible choice for conventional applications like running a web server.
But Ethereum's distributed structure also gives it a unique advantage: Once a program starts running, no one has the power to modify or stop it. That means you can use Ethereum to make binding, long-term commitments — which is why Ethereum programs are known as "smart contracts."
A good way to illustrate Ethereum's capabilities is with an example. One of the biggest challenges of Bitcoin has been the currency's volatility; Ethereum offers a potential solution for this problem: a smart contract that hedges against currency fluctuations.
Two users might each submit $1,000 worth of ether to a smart contract. After a month, the smart contract would look up the current dollar/ether exchange rate, paying one user $1,000 worth of ether at the new exchange rate (which might be more or less ether than originally submitted) and sending the rest of the ether to the second user.
This works the same as a conventional hedging contract, with one important difference: The contract is enforced by a computer program running on the Ethereum network instead of by the courts. Once submitted, the program can't be modified by either party, so neither party has to trust the other.
Of course, the obvious question is why you'd want to use such a convoluted technique to execute an ordinary financial contract. Modern financial markets make it cheap and easy to hedge against a wide variety of price fluctuations, and it's not obvious people are clamoring for a weird, internet-based alternative to these products.
As with Bitcoin, some of the early uses of Ethereum are likely to involve illegal activity. You can use ordinary financial networks to hedge against changes in the price of wheat or crude oil, but if you want to hedge against changes in the street price of cocaine, a smart contract might be your only option.
Ethereum could become a platform for online betting. Bitcoin already supports simple gaming applications, but more complex Bitcoin-based gaming requires players to trust the company running the game not to cheat. Smart contracts could allow the creation of complex, provably fair online games. Ethereum could also allow people to bet on events (like elections) in countries (like the United States) where such gambling is restricted by law.
Ethereum could also prove particularly useful in countries with dysfunctional legal systems. The ability to make binding legal commitments may not be so useful in countries like the United States where legal institutions work fairly well. But in countries where the courts are corrupt, incompetent, or nonexistent, the ability to make and enforce contracts online could be attractive.
As with Bitcoin, legally dubious applications come to mind quickly because Ethereum's decentralized structure makes it hard for governments to control. But the hope is that the same characteristics of decentralization and flexibility will allow people to build entirely new classes of applications that can't be built on top of conventional financial and legal infrastructure. So far, that hope has mostly not panned out for Bitcoin, but it still could happen — and people are just getting started exploring Ethereum's capabilities.
There are a lot of different ways to use Ethereum contracts, but the application that has attracted the most interest is virtual organizations. At a fundamental level, an organization is just a bundle of agreements between groups of people — shareholders, employees, creditors, and so forth. In most organizations, these are conventional contracts enforced by the court system. Ethereum allows the creation of decentralized autonomous organizations, whose contracts and bylaws are enforced by Ethereum smart contracts instead.
This is not just a theoretical possibility. A virtual organization called the DAO has raised more than $150 million over the past few weeks. Technically speaking, the DAO is just a specific Ethereum address controlled by a computer program running on the Ethereum blockchain. People send ether to this address and get back shares in the organization.
Once the fundraising phase is complete, these shareholders will be able to vote on what to do with the money. The idea is that the DAO will act as a kind of venture capital fund for the Ethereum community. Programmers and companies will submit detailed project proposals to the DAO. DAO shareholders will then vote on which proposals to fund.
It's important to take that $150 million figure with a grain of salt. For one thing, the DAO's funds are in the form of ether, and media hype about the DAO has pushed up ether's value, so once things settle down the DAO might not actually have $150 million at its disposal. Also, the DAO has a mechanism for shareholders to request refunds, so again, the full $150 million might not ultimately get spent.
And the DAO — and DAOs in general — are going to face significant challenges.
One challenge relates to governance. The structure of conventional organizations developed over many decades, shaped by hard-won experience. They have boards of directors, CEOs, auditors, and well-defined management hierarchies to ensure that the organization behaves in a coordinated fashion and is accountable to shareholders.
The DAO is essentially starting with a clean slate, with most decisions made by majority rule. It's as if Apple asked its shareholders to vote on which products to develop. That could lead to erratic and unpredictable decisions, making third parties reluctant to enter into long-term relationships.
At the same time, the fact that the company's basic bylaws are hard-coded into the Ethereum blockchain means that a bug in the DAO's software could have disastrous consequences. If a design flaw causes the organization's operating software to behave in an unexpected and undesirable way, there might be no way to fix the problem other than to liquidate the organization and start over. There's no DAO board of directors with the power to make technical, commonsense changes to the bylaws the way they could in a conventional company.
The DAO may also encounter unwanted attention from securities regulators. In the United States, the Securities and Exchange Commission has detailed regulations that companies must follow when they offer investments to the general public, and most other countries have similar rules.
The DAO's creators don't appear to have followed any of these regulations. And indeed, it's not clear that it's even possible for a purely blockchain-based organization to comply with SEC rules, whose authors probably never considered the possibility that a company could be an autonomous computer program running on a blockchain.
In some ways, DAOs are in a similar position with respect to SEC regulations that Bitcoin was in with respect to regulations governing money-transmitting services. Bitcoin seemed to meet the commonsense definition of a money-transmitting service, and arguably should have complied with consumer protection and money laundering laws.
But Bitcoin's decentralized structure meant that there was no specific person whom regulatory authorities could fine or prosecute for flouting the law. And so regulators contented themselves with regulating Bitcoin exchanges — companies that convert bitcoins to dollars, and vice versa — and allowed Bitcoin itself to operate free of regulation.
The big question is whether the SEC (and regulators elsewhere in the world) will take the same laissez-faire attitude toward the DAO. They might decide that it's too difficult to try to force DAOs to comply with securities law, or they might choose to interpret securities laws in ways that exclude virtual, blockchain-based organizations.
But securities regulators might also take a more aggressive posture. No one directly controls the DAO, but 10 prominent members of the Ethereum community — including Ethereum creator Vitalik Buterin — serve an oversight role as "curators" for the DAO. They could conceivably face unwelcome attention from investment regulators.
It's possible that the DAO and other virtual organizations will find ways to navigate these tricky legal waters. For example, a conventional organization called DAO.LINK was recently created to provide conventional services — like invoicing and tax compliance — to blockchain-based organizations. Conceivably, organizations like this could provide legal services to DAOs and help them navigate the tricky regulatory issues they raise.
One of the hottest trends in the retail investment advising world is robo-advisers. These are online sites — two of the most popular are Wealthfront and Betterment — that allow people to manage their investment portfolios with a website.
The companies' marketing pitches are simple: They perform the same task as a conventional investment adviser, but charge a fraction of the cost. Conventional investment companies have fees as high as 1 percent of a customer's invested assets. Betterment and Wealthfront cost about a third as much.
It's true that if you're paying someone a 1 percent fee to manage your money, you should stop doing that. But it's not true that Betterment and Wealthfront are the best alternative. A class of mutual funds called target date funds cost about half as much as robo-advisers, while providing a very similar service.
Betterment and Wealthfront offer tax-related services that may be useful to customers rich enough to exceed the $23,500 annual limit on tax-free savings. But if you're not that rich, you'll probably be better off saving money with an old-fashioned target date fund.
Saving for retirement isn't complicated. You can click here for the full Vox retirement guide, but fundamentally there are three things you need to do to get the most out of your retirement savings:
A financial product called a target retirement fund makes this really easy. You choose the fund that corresponds to your expected retirement date (for example, I'm in my mid-30s so I would choose Vanguard's Target Retirement 2045 fund), and the fund does the rest, gradually shifting to safer assets as you get closer to your retirement date.
There are low-cost target retirement funds available from Vanguard, Fidelity, and State Street. Signing up only takes an hour or two, and once you've deposited your cash with one of these companies, you shouldn't have to think about it again until you reach retirement age.
The development of these target date mutual funds is great news for consumers, but it leaves professional investment advisers — at least those that serve non-wealthy clients — with little to do. But instead of admitting that they'd been rendered obsolete and shutting down, investment advising companies have built elaborate marketing machines designed to obscure the fact that most people don't need professional advice about where to invest their retirement savings.
The marketing has been pretty effective, so a lot of people pay human advisers a lot of money — an annual fee of 1 percent of invested funds isn't uncommon — for advice that they don't really need.
That has created a big opening for a new generation of companies called robo-advisers.  Their pitch is that they'll provide the same basic service as a human investment adviser for about a third the cost. They're called robo-advisers because they provide "advice" using an automated website instead of having you talk to a human investment adviser.
Robo-advisers mostly perform the same functions as a target retirement fund. As markets fluctuate, they automatically rebalance your portfolio to maintain an optimal mix of stocks, bonds, and other assets. And as you get closer to retirement age, robo-advising companies shift more money into the safest investment categories, reducing the risk that a stock market crash will devastate your finances right before you reach retirement age.
To the extent that robo-advising companies are stealing customers away from overpriced investment advisers, that's a great development for consumers. The exact fees Wealthfront and Betterment charge depend on how much money you invest with them, but most investors will pay annual costs and fees between 0.25 percent and 0.37 percent. That compares favorably with conventional advisory services that can charge as much as 1 percent.
On the other hand, Betterment and Wealthfront's fees don't compare favorably to the best target date funds. Vanguard, Fidelity, and State Street all offer target date funds with expense ratios between 0.13 and 0.16 percent — about half what typical customer will pay for service from Wealthfront or Betterment. (It's important to note that not all target date funds are this cheap — most others cost significantly more, so always check a fund's expense ratio before you invest.)
To their credit, Wealthfront and Betterment are pretty candid about the similarities between their products and target date funds. Wealthfront addresses the issue explicitly on its website, writing that "we believe the next best option to having your portfolio managed by Wealthfront is investing in Vanguard’s target date funds."
Wealthfront points to two big reasons its products are better than a target date fund. One is tax efficiency. Wealthfront offers a service called tax loss harvesting that helps investors claim tax deductions when their investments lose value. Wealthfront claims that the tax savings from using the service can add as much as 1 percent annually to investment returns.
Unfortunately, Wealthfront's methodology doesn't stand up to scrutiny — the average investor isn't going to get anywhere close to 1 percent gains from this feature. Also, there are some legal risks to automating a complex tax-avoidance process.
But more to the point, you have to be pretty rich for this feature to be useful. Tax loss harvesting only becomes useful after you max out tax-advantaged accounts like 401(k)s and IRAs. The legal limits on these accounts are $18,000 for a 401(k) and $5,500 for an IRA (note that there are also some income-related limits for both Roth and traditional IRAs). Unless you're in the small minority of workers saving more than $23,500 per year ($47,000 per year for a couple), tax loss harvesting is totally irrelevant.
Wealthfront also argues that its service is better at fine-tuning a customer's portfolio — the same argument Betterment's CEO made when I talked to him in 2012. Target date funds are relatively crude, using exactly the same portfolio for everyone planning to retire within a particularly five-year window. As a result, Wealthfront argues, "the return on the target date fund might be too high or too low for a particular buyer’s risk tolerance."
Yet it's not obvious how much value this adds in practice. Most savers — even fairly sophisticated ones — don't know how to precisely quantify their "risk tolerance." They know they want to maximize their long-term returns without taking on too much risk, but beyond that it's mostly guesswork.
And if you want more or less risk than your target date fund offers, there's a simple way to do it without paying robo-advisers' higher fees: use a target date fund for a different year than your actual retirement year. Suppose, like me, you expect to retire around 2045. If you want to take a bit more risk, you can invest in Vanguard's Target Date 2050 fund, which will keep your money in high-risk, high-reward stocks for five years longer than the Vanguard's 2045 portfolio. Conversely, if you want to play it safe you can invest in the Target Date 2040 fund, which will start shifting to safer assets five years earlier.
At root, robo-advisers' argument against target date funds is that they're out of date. Betterment emphasizes that the funds were invented more than 20 years ago and haven't changed much since then. But the fact that something is old doesn't mean it's bad. And robo-advisers just don't seem to offer much value to justify their higher costs.
Disclosure: I have most of my retirement savings in Vanguard mutual funds, and because Vanguard is structured as a cooperative, that technically makes me a Vanguard shareholder.
Correction: I originally described Betterment and Wealthfront as the two most popular robo-advisors, but Scwab's Intelligent Portfolio product is more popular than either.
Tesla's biggest problem might not be finding customers for its forthcoming $35,000 Model 3 electric sedan but figuring out how to actually build enough to meet the demand.
CEO Elon Musk has set a goal to produce 500,000 cars a year between 2018 and 2020. That's going to require expanding Tesla's production facilities, which will require a lot of money. As a Bloomberg headline put it last week: "Tesla needs billions to meet Musk's ludicrous assembly timeline."
Now we know where those billions are going to come from. Bloomberg is reporting that Tesla will sell $2 billion in new shares to the public as a way of raising extra capital. This extra cash comes on top of billions the company has raised already. And that makes Tesla a real outlier in Silicon Valley. For all the breathless coverage of Silicon Valley's alleged tech bubble, the really striking thing about most Silicon Valley startups is how little cash it takes to get them off the ground.
Fortunately, there's lots of money available to invest in fast-growing, capital-hungry companies like Tesla. The problem for investors and the US economy is that there don't seem to be nearly enough Teslas to go around.
To see how unusual Tesla is here, it's helpful to compare it to Google.
Today, Google is one of the world's most valuable companies, with a market value of almost $500 billion. Yet building the company was comparatively cheap. It raised just $26 million in outside investments prior to its 2004 initial public offering. Google has raised $1.6 billion in its IPO — money that helped fund further expansion. But the company could have gotten along perfectly well without the money, as it had already generated $106 million in profits in 2003 and its search ad revenue was growing rapidly.
Newer startups have been somewhat more aggressive about fundraising. Uber is the industry's fundraising champion, with $10 billion raised over the past seven years. Airbnb has raised $2.4 billion.
But like Google with its IPO money, these companies are largely raising money because they can get it at favorable rates — not because they particularly need it. Uber, for example, has said it's already profitable in the United States but is spending billions to gain market share in China. Uber has also spent some of its capital to fund a zero-sum price war with rival Lyft.
But none of these companies are investing all that much money in long-lasting, tangible assets. They don't manufacture physical products, so they don't need factories. Uber doesn't own the cars its passengers ride in. Airbnb doesn't own the apartments its customers stay in. Google has invested millions in its data centers, but those expenses are a small fraction of its revenues.
The ability to build a gigantic technology company with a small amount of capital is great news for the company's founders and early investors, since it meant they get to keep a large share of the company's eventual gains. But if you're someone with money to invest, the situation isn't so great.
Over the past decade, American companies have been generating more and more profits, and last year they paid more than $1 trillion out to shareholders. But shareholders have struggled to find anything productive to do with the money. If you add up all venture capital firms' investments in 2015 — including big investments in Uber and Airbnb and many smaller investments — the total was just $59 billion. Money raised in initial public offerings accounted for another $30 billion.
With nothing better to do with the money, a lot of investors have simply used their dividends to buy more stock in existing public companies. These payments go to the companies' previous shareholders — not the company itself — so they don't lead the companies to put the money to productive use.
Instead, share prices have been getting bid up. Right now, stocks in the Standard and Poor's 500 — an index of 500 large American stocks — are selling for almost 24 times their annual earnings. This is an unusually high value that signals that stocks are likely to produce below-average returns over the long run. At the same time, interest rates on government and corporate bonds are at their lowest point in decades.
What investors need is a lot more capital-hungry companies like Tesla: companies that need a ton of cash to grow quickly. Tesla is more capital-hungry than most Silicon Valley companies because it sells physical products that take complex manufacturing facilities to build. Tesla is planning to spend $5 billion on just its "gigafactory" — a vast facility to produce batteries for its cars.
And if Tesla is successful, its appetite for capital will only increase. Even if the company achieves its goal of producing 500,000 cars per year, it will still be a very small player in the overall auto market. It's going to need to build many millions of cars a year to achieve Elon Musk's goal of making electric cars the industry standard. And that will require building many more facilities for creating batteries and building cars.
The problem for investors is that there just don't seem to be very many companies like this. Too many investment dollars chasing too few investment opportunities.
This may also explain the slowing growth of the US economy as a whole. Investments in new factories, equipment, and technologies are a major way that advanced economies grow. But while there's lots of money available for this kind of investment, it seems to be getting harder and harder to find productive ways to put that money to work.
Buying a house is the biggest financial transaction most people ever make. And because you may only buy one house in your lifetime, there's a risk that you'll make a lot of mistakes.
Plus, most of the people you'll be dealing with in the home-buying process will be more experienced than you. There's a danger that they'll take advantage of your ignorance.
So here are six ways to be a savvy homebuyer. These tips will help you find a home that's right for you while avoiding common pitfalls, and could save you thousands of dollars in the process.

(Ed Suominen)
Many people want to buy a house as quickly as possible because they think paying rent is "throwing money away." But buying a home too quickly can be an even bigger financial mistake than not buying at all.
For starters, the idea that renting is throwing away money is a little misguided. Almost everyone needs to take out a mortgage to cover the cost of their first house. And in the first few years of a 30-year mortgage, only a small portion of each payment goes toward paying off principal. The majority of the money goes to interest payments, which is essentially "rent" you're paying the bank to use their money. Paying "rent" to a bank isn't any less wasteful than paying rent to a landlord.
More importantly, selling a house and buying another one is an expensive process, typically costing between 6 and 10 percent of the value of the house. So if you buy a house and then need to sell it a couple of years later, these transaction costs will wipe out any equity you might have accumulated.
So you should only buy if you plan to stay in the same house for five years or longer. If you expect you'll need a different house in the next few years — because you might move to a different city for work, you'll need a bigger house to accommodate a growing family, or you simply aren't sure what kind of house you'll want in a few years — it's better to continue renting until you're ready to settle down.

When it comes to home buying, haste makes waste. (Guy Sie)
Once you've decided to buy, it's important not to rush the house-hunting process. "When I've had clients make real estate deals they've regretted, they've almost always coincided with time pressure," said Zach Teutsch, a personal finance coach in the Washington, DC, area, in a 2014 interview. "You almost always overpay relative to what you would have paid if you were on a more cautious timeline."
That means that when moving to a new city, you should consider renting for a few months while you search for a permanent home. While it might seem like a waste to pay rent when you could be building equity, the amount you overpay due to a hasty purchase — or the cost of having to move again after buying a house that doesn't meet your needs — could dwarf the cost of a few months' rent.
In theory, you can buy a home without a real estate agent, but for most first-time buyers it makes sense to hire a professional to guide you through the process. Many buyers find realtors by asking friends and family for recommendations. You can also find a realtor through an online directory.
No matter how you find potential realtors, it's important to ask for references. The best sign of whether a realtor will serve you well is whether his past clients were satisfied with their service.
When evaluating real estate agents, it's important to keep in mind that their incentives aren't aligned with your own priorities as the buyer. You want the best home at the lowest price — and you may be willing to wait quite a while for the right deal to come along. In contrast, agents make more money when they can close deals as quickly as possible — and they make more money when their clients spend more.
The National Association of Realtors has an ethics code that obligates its members to promote the interests of their members, so in theory this kind of conflict of interest shouldn't matter. But of course, some agents are more ethical and conscientious than others.
"One indication that a person is dealing with a good realtor is if they're ready to make an offer and the realtor encourages them to think about whether that property is in poor condition or overpriced," Teutsch told me. "It's a good indication if your realtor is willing to tap the brakes instead of the gas."
Agents get paid whether or not their clients get a good deal. Indeed, if a buyer overpays, his agent actually gets a slightly larger commission. But a good real estate agent will still advise caution if he feels a buyer is bidding more for a house than it's worth, or overlooking serious flaws.
So when you're choosing a realtor, it's good to ask prior clients how often he warned them away from making offers on properties. If a prospective realtor regularly encouraged customers to keep looking for a better deal, that's a good sign. On the other hand, if buyers say they felt pressure to make an offer on every property they saw, that's a sign that the agent may not have clients' best interests at heart.

You probably don't need this much house. (Kay Gaensler)
Buying a house is an exciting experience, and there's a natural temptation to buy the biggest house you can — barely — afford. But Teutsch told me that most people will be happier in the long run if they buy a house that's cheaper than the maximum amount a bank will lend them.
Few things are more stressful than owning a house you can barely afford. It can put you one layoff or medical emergency away from financial disaster. It can also limit your freedom to take a more rewarding but less lucrative job, start your own business, or cut back your hours to spend more time with loved ones.
So it's important to decide how much you're willing to spend and then refuse to go over that amount. One way to do this, Teutsch says, is to set a limit for yourself during the mortgage preapproval process. Rather than getting preapproved for the maximum amount the bank is willing to lend, he says, you can ask to be preapproved only for the amount you're planning to spend.
Once you have this document in hand, show it to your realtor — and don't mention that you could have gotten preapproved for a larger sum. If you're trying to buy a house for less than $400,000 but your realtor knows the bank is willing to lend you $600,000, he might encourage you to consider homes above your price limit. On the other hand, if your realtor knows you're only preapproved for $400,000, then he'll only show you homes below that limit.
This might also give you a bit more leverage in negotiations between your agent and the seller. Obviously it would be unethical for your realtor to tell the agent on the other side of the table that you can afford to pay more. Still, your realtor may not be a great bluffer. When he tells the seller's agent you can't afford to pay any more, he's going to be more convincing if he actually believes it.
You can always go back to the bank and request preapproval for a higher amount if you find the lower ceiling to constraining. But the need to take that extra step will help prevent you from making an impulsive purchase that you might regret later.

(thom)
After you've made an offer on a house and it's been accepted by the seller, the next step is to get the house inspected for issues such as leaks, termites, or mold problems. Most real estate agents will offer to put a buyer in touch with an inspector. But it's generally a good idea to choose an inspector independently.
An inspection comes near the end of the home-hunting process. If the inspector doesn't find any problems, the deal will go through and the agent will get his commission. On the other hand, if the inspector does find problems, it will mean more work for the agent. At a minimum, it will mean an additional round of negotiations to get the seller to compensate the buyer for the problems. If particularly severe problems are discovered, the entire deal could fall through, which means the house-hunting process will have to start all over again.
So while it's in the buyer's interest to choose an experienced and aggressive inspector, it's better for the real estate agent to have an inspector who isn't so picky. While few real estate agents will deliberately recommend an incompetent inspector, you might get a more thorough inspection if you decline your agent's recommendation and choose an inspector based on your own independent research.
(Timothy B. Lee)
Real estate agents perform a variety of useful services, including advising the client on the state of the market, helping the client view homes for sale, writing offers, negotiating with sellers, and guiding the buyer through the purchase paperwork.
Traditional real estate services are highly personalized. An agent will often drive a buyer around town, showing her neighborhoods that meet her criteria and explaining the finer points of the local market. A traditional realtor will serve as a single point of contact for every step of the home-buying process, from initial comparison shopping to signing on the dotted line.
This approach works well for many buyers. But others prefer a more self-directed approach. And for them, the online realtor Redfin can be a good option.
Redfin performs the same essential steps as a conventional buyer's agent. A salaried Redfin agent will show you houses, write offers, and negotiate with sellers on your behalf. But Redfin uses a team-based approach where different functions are performed by different agents. And the Redfin model depends on customers to take more initiative.
"Redfin was perfect for a type-A person like me because it made it really easy to stay organized and on track," said Adrienne Aldredge in 2014, a few months after she bought a house in the Portland area. "Their agents are very knowledgeable and responsive, but they are not going to hand-hold you through finding houses to view."
One of Redfin's traditional selling points was that the company offered big rebates to customers. Traditionally, the buyer's agent gets a 3 percent cut of the sale price — meaning your agent gets about $12,000 if you buy a $400,000 house. In the past, Redfin has rebated as much as half of that commission back to customers.
But in recent years, Redfin has become more like a conventional realtor. It now offers more extensive and personalized service. That has meant smaller rebates.
For example, back in 2014 Redfin was offering a $3,300 rebate to buyers buying a $400,000 house in Washington, DC. Today the rebate has shrunk to just $1,800. On an $800,000 house, Redfin's rebate has shrunk from $8,600 in 2014 to $4,800 today.
So if you're intimidated by the idea of searching for properties online, or you want a single point of contact to take you through the home-buying process step by step, a traditional realtor might be worth the extra money. A traditional realtor is also a good choice if you're buying on a tight schedule or you have your heart set on buying in a specific, high-demand neighborhood.
But if you're willing to be patient and are comfortable searching for stuff on the internet, Redfin can save you some money.
Twitter announced today that it is going to tweak its platform such that links, photos, and other media no longer count against the 140 character limit. That's a great idea, and really they ought to go further and make it possible for tweets of arbitrary length to appear — even if the main feed only displays a short introduction.
This right here is why:

Extended tweets are a great idea pic.twitter.com/ldnTO0F9tp
Speaking as someone who loves Twitter and who worries about the fact that it is failing in stock market terms, I think this is a sign of why bringing back Jack Dorsey as CEO was ultimately a good idea. Simply put, a founder has the standing and clout to make this kind of fundamental change, whereas Dick Costolo never did.
My main fear is simply that it's too little, too late at this point. The Twitter product has been very slow to evolve over the years. To this day, the company seems to be having trouble rolling out new features like polls to the full suite of Twitter apps — TweetDeck doesn't have polls yet, for example — and really making Twitter a home to natively supported longer-form essays is something they should have done a year ago or more.
Update: Twitter CEO Jack Dorsey made a similar argument in his own tweet:
pic.twitter.com/bc5RwqPcAX

One way in which Amazon differs from a conventional supermarket or a place like Walmart or Target is that it offers relatively little in terms of "store-brand" products. There's the Amazon Basics line of electronics accessories and there are Amazon Elements baby wipes, but not much else. But Greg Bensinger reports in the Wall Street Journal that's about to change, with Amazon prepared to roll out the Elements diapers that have long been rumored, plus a much larger array of products that will "include nuts, spices, tea, coffee, baby food and vitamins, as well as household items such as diapers and laundry detergents."
Why? Bensinger cites Bill Bishop, who runs a consulting company called Brick Meets Click, suggesting that "private-label goods boast higher profit margins than name brands because companies save costs on marketing and brand development."
Seeking higher margins would be a somewhat bizarre strategy for Amazon, which has historically had no profits whatsoever but recently stumbled into a high-margin web services business. My guess is that if Amazon goes big into store-label products they'll be priced aggressively to gain market share at razor-thin margins. The goal isn't really going to be making money, it'll be filling more trucks.
Right now, Amazon is much more than a retailer of physical goods. But the retail of physical goods is still at the core of its corporate identity. And at the moment, the company is involved in a massive multifaceted push to get better at the delivery element of that.
For years, the company has offered free two-day shipping to Amazon Prime members. But these days, a wider and wider array of products is available for Prime one-day shipping or even prime same-day shipping.
This is an important strategic initiative for Amazon. If it can make one-day shipping the new normal, it'll make life that much more difficult for hypothetical future competitors. And to the extent that it can make same-day shipping a reality, it will be able to intensify the competition against brick-and-mortar retail and possibly dominate the buzzy but unproven on-demand delivery sector.
The key thing here is that routinized one-day delivery is going to require a ton of infrastructure. You need warehouses near all the major population centers, the warehouses need to be staffed with people and/or robots, and you need to be putting tons of trucks in the field actually doing the door drops.
Amazon has lots of initiatives in the field ranging from drones to physical stores to try to support this ambition, but the ideas all have something in common — they are capital-intensive and involve high fixed costs.
That means that to make it work, you need to spread the cost across as many deliveries as possible. Making generic versions of household staples and selling them cheaply seems like an excellent way to do that. The fact that competitors are counting on these to work as high-margin items only means that the opportunity to steal a price advantage is real.
In most cases, obviously, the idea of earning nothing on each item sold and then making it up in volume is a joke. But for Amazon it's no joke. Each zero-margin item it sells helps create the infrastructure to meet more and more customer needs faster and faster.
For most of the past five years, Apple has been the world's most valuable company, with oil giant ExxonMobil a close second and other companies far behind. But recently, Apple's earnings growth has slowed, while Alphabet — the company most of us know as Google — has seen its profits continue to grow. The result: Alphabet briefly passed Apple on Thursday to become the world's most valuable company — though it ended the day worth $488.7 billion, slightly less than Apple's $489.9 billion.
The remarkable thing about this is that Apple's 2015 operating income (that's profits, basically) of $71 billion was more than three times Google's operating income of $20 billion. So you might expect Apple's market value would be a lot higher than Google's. The fact that Wall Street is valuing them about the same is a signal that the market is a lot more optimistic about Google's growth prospects. Investors expect Google's profits to continue going up, whereas Wall Street thinks Apple's massive iPhone profits have nowhere to go but down.
Google does a lot of different things. It has a browser called Chrome, a smartphone OS called Android, and a wide variety of online services such as Google Maps, Gmail, and Google Docs. Google's parent company, Alphabet, is working on self-driving cars, residential broadband networks in several cities, and a lot more.
But Google's revenues overwhelmingly come from one thing: advertising.
In 2015, ads accounted for about 90 percent of Google's overall revenues, and of this ad revenue three-quarters came from ads on Google's own websites — including a large share for Google's market-leading search engine. The remaining quarter came from Google's ad networks, which sell ads that appear on other people's websites.
Some high-profile Google products aren't directly ad-supported, but most of them indirectly support Google's ads business — and especially its flagship search engine. For example, creating Android and giving it away to smartphone companies helps ensure that Google's search engine and other online services remain popular with mobile users. Similarly, owning the leading web browser, Chrome, helps Google steer users toward its search engine.
This is worth a lot of money: Google paid Apple $1 billion in 2014 to ensure that Google was the default search engine on iPhones. And until 2014, Google paid hundreds of millions of dollars every year to ensure Google is the default search engine for the Firefox web browser (more recently, Yahoo has outbid Google to be the default).
The ad business keeps getting more lucrative for Google:

<!--
 (function() { var l = function() { new pym.Parent( 'vox-google-advertising-revenue__graphic', '//apps.voxmedia.com/at/vox-google-advertising-revenue/'); }; if(typeof(pym) === 'undefined') { var h = document.getElementsByTagName('head')[0], s = document.createElement('script'); s.type = 'text/javascript'; s.src = 'https://cdnjs.cloudflare.com/ajax/libs/pym/0.4.5/pym.js'; s.onload = l; h.appendChild(s); } else { l(); } })();
// -->

While Google makes most of its money from ads, Apple makes most of its money from selling iPhones. The iPhone is a lot more lucrative than any of Apple's other products. Apple sold 231 million iPhones in 2015, generating $154 billion in revenue. In its most recent quarter, Apple generated almost twice as much revenue from the iPhone as all of its other products — iPads, Macs, Apple Watches, and so forth — put together.
Like Google, Apple has seen its revenue soar over the past few years as the iPhone has gotten more popular. But unlike Google, it looks like Apple is seeing a slowdown in its previously rapid growth. In April, Apple reported its first down quarter in over a decade, with revenues falling 13 percent from a year earlier.
The issue seems to be that the iPhone is close to saturating its market. Apple dominates the market for high-end smartphones so completely that most of the people around the world who can afford to buy an iPhone already have one. Apple will be able to continue selling tens of millions of iPhones every quarter as their existing users upgrade, of course. But there doesn't seem to be much room for further growth in iPhone sales.
To continue growing, then, Apple needs to come up with another massive hit. And it hasn't been able to do that. The iPad is pretty popular, but it's nowhere near as popular as the iPhone, and its sales are actually declining. The Apple Watch seems to be a pretty successful product by conventional measures, but, again, it's nothing like the iPhone.
Of course, Apple could always surprise us. The company has a talented staff and tens of billions of dollars to invest in new products.
But Apple's stock price suggests that Wall Street, at least, isn't optimistic about Apple's chances of coming up with anything new that will match the iPhone's success. A key statistic to watch here is the price-to-earnings ratio — that is, the company's stock price divided by its annual profits.
If the P/E ratio is high, that means Wall Street is paying a premium in hopes that profits will rise over time. A low P/E ratio signals that Wall Street expects profits to stagnate or even decline in the future.
Fast-growing technology companies have share prices that are 20, 50, or even 100 times their earnings. Alphabet's P/E ratio, for example, is about 30.
In contrast, Apple's P/E ratio is about 10. That means Wall Street is valuing Apple like a sleepy utility company — still highly profitable, but with little prospect for future growth.
There are lots of technology companies that have one big hit and then stop growing once that initial product matures. Yahoo, Twitter, and eBay are all examples of companies that seem to have hit a wall.
What makes Apple and Google remarkable is that they were able to follow up an initial hit — early PCs in Apple's case, a search engine in Google's — with many other successful products. Apple had the iPod, iPad, and iPhone. Google has had Gmail, Google Maps, Android, Chrome, and so forth.
It's impossible to say exactly why some companies continue to thrive while others turn out to be one-hit wonders, but one common theme seems to be that the most successful companies still have their charismatic founders at the helm. Apple struggled during Steve Jobs's absence between 1985 and 1997, then enjoyed an amazing resurgence between 1997 and Jobs's death in 2011. Google has been under the control of its founders, Larry Page and Sergey Brin, since the beginning.
Founders have an unmatched level of respect among rank-and-file employees. Because they've been at the company since the beginning, founders have relationships with people all across the country.
This is particularly important when a company needs to make a big change. Employees at once-great but now-struggling companies have a tendency to romanticize a company's early days and resist necessary changes. In these moments, no one has more credibility than a company's founders to put these concerns into perspective and make the case that the company needs to change if it wants to survive.
Founders' greater credibility also allows them more leeway to take big risks. Ordinary CEOs serve at the pleasure of the board, so they constantly have to worry that they'll lose their jobs if they go too far out on a limb. Founders tend to be more secure in their jobs, which makes them more able to take big risks and push through necessary changes.
Apple, of course, lost its visionary founder to cancer in 2011. It's now run by Tim Cook, a man who is by all accounts an able manager but doesn't seem to have Jobs's vision. Prior to Jobs's death, Cook's focus was on optimizing Apple's operations, not creating new products. Alphabet, on the other hand, is still run by co-founders Larry Page and Sergey Brin. They've made big bets on everything from Android to self-driving cars. Some of those bets have failed, but others have been big hits.
That might be one reason Wall Street is bullish on Alphabet but bearish on Apple. The iPhone is a great product, but there's little reason to think Cook will be able to create more products like it. On the other hand, the market seems to be hoping that Page and Brin are just getting started.
Disclosure: My brother is an executive at Google.
Correction: I stated that Google pays Firefox to be the default search engine, but Firefox switched to Yahoo in 2014.
On Tuesday, Walmart sued Visa. The claim? That Visa is trying to force Walmart to use a less secure payment system that happens to generate more revenue for Visa.
The conflict highlights the dismal state of American payment technology. When the United States switched to chip-based credit and debit cards last year, it became one of the last developed countries in the world to do so.
America is even further behind in the switch from signatures — which are almost useless for preventing fraud — to the four-digit personal identification numbers (PINs) that customers in many developed countries use to approve credit card transactions.
In the United States, Visa makes more money when customers use this older, less secure technology, and so Visa has insisted that retailers must continue to allow customers to make signature debit payments. Walmart cites 2009 data from the Federal Reserve showing that signature debit transactions had an average transaction fee of 1.53 percent, compared with 0.56 percent for PIN debit transactions.
Walmart is right on this one: The poor security of "signature debit" helps no one other than credit card fraudsters. Requiring customers to enter a PIN — which most debit card customers use to get money from an ATM anyway — is a sensible step. Indeed, it's so sensible that most of the world adopted it many years ago, and it's past time for America to catch up.
When you take money out of an ATM, you probably enter a four-digit PIN. On the other hand, if you use the same debit card to buy something in a store, you may be asked to use a signature to verify the transaction instead.
Unfortunately, signatures are practically worthless as a security measure. If you don't believe me, try scribbling randomly next time you're asked to sign a credit or debit card receipt. I've been doing this for years and I've never had a store clerk decline the transaction because my signature didn't look authentic.
The rest of the world is way ahead of us on this. Over the past decade, the United Kingdom, Canada, and Australia — just to name a few — have switched to PIN-based authentication, in which customers identify themselves with a four- or six-digit code.
In its lawsuit, Walmart points out that Visa has been an advocate for making PIN-based authentication mandatory — in other countries. But here in the United States, Visa has resisted the change, insisting that retailers like Walmart let consumers decide whether to authenticate their purchases with a signature or a PIN.
Why? Walmart's lawsuit suggests one possible reason: When customers make a signature debit transaction with a Visa, it has to be processed by a signature debit network that's owned by Visa. On the other hand, PIN-based purchases are handled using the same financial networks the nation's ATMs use. And this is a more competitive market. Several different networks exist, and some of them are independent of the big credit card networks.
So Visa has been pressuring retailers to use a less secure payment method that happens to earn Visa a lot more in transaction fees. Walmart has been resisting this pressure. And on Monday, that conflict burst out into the open with a Walmart lawsuit.
Walmart has long been obsessed with keeping costs and prices low, so it's not surprising that the retailer has been a leading advocate for switching to PIN-based debit transactions. In April 2015 — a few months before the switch to chip cards — a Walmart executive argued that it was a "joke" that the nation was switching to chips without also introducing a PIN-based system.
Walmart contrasts America with the United Kingdom, where credit card networks worked with the government on a coordinated "I ♥ PIN" campaign that launched in 2002 to promote the switch to PIN-based transactions. By 2006, "99.8 percent of chip transactions were PIN-verified," according to the Atlanta Fed.
In its lawsuit, Walmart points to the 2010 Dodd-Frank Act, which included an amendment from Sen. Dick Durbin (D-IL) guaranteeing retailers the right to route payments over the network of its choice. Walmart argues that this legislation gives it the right to require consumers to make payments with a PIN rather than a signature.
Unfortunately, the lawsuit is heavily redacted, so we don't know the details of Walmart's dispute with Visa or its precise legal argument. But it seems clear that Walmart believes Visa's efforts to promote signature debit run afoul of the Durbin amendment's guarantee of retailer autonomy.
Visa hasn't responded to my email seeking comment — and has declined to comment on the lawsuit to other media organizations — so we don't what Visa's counterarguments will be. But it looks an awful lot like Visa is pushing a less secure payment method because doing so allows it to charge merchants higher fees. In doing so, Visa is not just costing merchants — and ultimately consumers — more money, it's also making it easier for criminals to commit credit card fraud.
If Walmart wins its lawsuit, it could provide momentum for making PINs the default for debit card transactions. That's a big deal because people make more payments with debit cards than credit cards.
Still, to fully bring the US financial system into the 21st century will require using PINs for credit cards as well. This is going to be trickier because people aren't used to remembering a PIN for their credit card (in contrast to debit cards, where people know they need a PIN to use an ATM). Banks worry that if they start pushing customers to use PINs for their credit card, customer will get annoyed and switch to another credit card.
But this is where the kind of coordinated persuasion campaign employed in Britain can be hugely valuable. If everyone starts requiring credit card PINs at the same time — especially in concert with a public campaign explaining why the switch is worthwhile — banks won't have to worry that moving first will cost them customers.
In the long run, having a PIN to make credit card and debit payments will seem as natural as using a PIN to withdraw from your ATM. The only question is how to get there.
If you've used Facebook on a desktop or laptop computer, you might have noticed the "trending topics" box in the right rail. While the title gives the impression that the list is automated, a Gizmodo piece last week reported that a group of 20-something journalists help algorithms choose which stories appear in this box and write headlines and brief summaries for them:

hey. look what’s trending on Facebook. pic.twitter.com/8rWNavwq4p


And on Monday, Gizmodo dropped a bombshell: A former member of this team alleged that the trending topics team was systematically skewing the results to favor liberal viewpoints. According to Gizmodo's source (who declined to be named, so it's hard to verify his claims), Facebook's trending topics team tended to be dismissive of stories that came from right-leaning news outlets such as Breitbart, Washington Examiner, and Newsmax — waiting instead until more mainstream news organizations like the New York Times or the BBC covered them.
"I’d come on shift and I’d discover that CPAC or Mitt Romney or Glenn Beck or popular conservative topics wouldn’t be trending because either the curator didn’t recognize the news topic or it was like they had a bias against Ted Cruz," the former Facebook contractor, who identified himself as a conservative, told Gizmodo.
This has triggered predictable — and justified — consternation among online conservatives. "It is beyond disturbing to learn that this power is being used to silence viewpoints and stories that don't fit someone else's agenda," argued a press release from the Republican Party.
Facebook insists that the allegations are untrue. "There are rigorous guidelines in place for the review team to ensure consistency and neutrality," wrote Facebook's Tom Stocky late on Monday.
Behind this story is a deeper question that unnerves everyone — including established, mainstream outlets — in a media ecosystem that's increasingly dependent on Facebook's whims: The company's power is vast, and that power is not always deployed in ways that are transparent and accountable.
Mark Zuckerberg has always said he wants to build Facebook into "a utility," and he's arguably succeeded. But utilities, because they're so important, are highly regulated. Facebook is approaching utility-level importance, but it answers to no one — not even, given Zuckerberg's control of the company, its shareholders.
The irony here is that the government used to more actively regulate large media companies to prevent them from abusing their power. Conservatives were skeptical of these policies, and they have largely gotten their wish online. As a result, we now have an unregulated free market for internet content. And it has put a lot of power in the hands of left-leaning CEOs like Zuckerberg.
During the second half of the 20th century, broadcast media and daily newspapers played a dominant role in the information diets of ordinary Americans. A typical American city might have one or two daily newspapers and three or four local television stations. There might also be a couple dozen radio stations, only a few of which carried significant news programming.
Strictly speaking, these weren't the only sources of news. But they had enough influence that  the Federal Communications Commission established a complex set of regulations designed to ensure that influence wasn't abused.
For example, a 1975 regulation made it illegal for a company to own a major newspaper and a television station in the same market. Also, in the late 20th century, no company could own television channels reaching more than 35 percent of the country — a cap that was bumped up to 45 percent in 2003.
In 1949 the FCC instituted the Fairness Doctrine, a controversial rule requiring broadcasters to give "equal time" to opposing points of view on controversial issues. The repeal of this rule in 1987 paved the way for talk radio hosts like Rush Limbaugh, who are known for offering a strongly one-sided point of view to listeners.
These rules existed because regulators believed it was dangerous for any single company to exercise too much influence over the national political conversation. By carefully limiting media consolidation, the FCC hoped to provide room for a diversity of voices to be heard.
But in the past couple of decades, this approach has fallen out of favor. Conservative policy experts have argued that the proliferation of media organizations has made it unnecessary for the government to micromanage the structure of the media sector.
"Americans are in no danger of seeing their news and information monopolized, least of all by newspapers," wrote the Heritage Foundation's James Gattuso in 2008, arguing to liberalize the broadcast-newspaper cross-ownership rule. "Rather than increased concentration, recent decades have brought an historic expansion of information sources and their diversity."
And conservatives have largely won this argument. Today, cable television networks and the internet are much more lightly regulated than the broadcast media giants of yesteryear.
In the 20th century, power came from owning the means of distributing content — like a printing press or a broadcasting tower. If your town only had two daily newspapers and four television channels, the companies that controlled those six outlets had outsize influence over public debates.
Not anymore. Thanks to platforms like Medium and YouTube, anyone can distribute text, audio, and video content worldwide for free. But being able to publish something globally isn't the same as getting a lot of people to read it. Instead, power has shifted to companies like Google, Twitter, and especially Facebook that help their massive audiences filter online content.
In some ways, these new media platforms are more powerful than old-fashioned media companies have ever been. Facebook, for example, has a billion daily active users — a far larger loyal audience than any television network or newspaper has ever had.
And these companies' role as the internet's gatekeepers gives them a lot of power over other media companies. Facebook and Google each account for double-digit percentages of the traffic of many online news organizations. Most news sites spend a lot of time thinking about how to maximize the traffic they receive from Google and Facebook. This leads to articles like the Huffington Post's infamous 2011 article, "What Time Does the Superbowl Start?" (and its many imitators), whose entire purpose was to get a good ranking on Google's search results for people wanting to answer that question.
Similarly, the curiosity gap headlines innovated by Facebook-first publishers like Upworthy and now emulated all across the internet ("You'll never believe what happened when...") are an effort to generate high click-through rates and thus win favor from Facebook's content algorithms.
But in other ways, Facebook and Google are not as powerful as the media giants of the past. While many people use Facebook, there are many, many other sites that help you find interesting content. All of them are available with just a couple of clicks. And while Facebook and Google can drive traffic to content they think is worthwhile, they have very little power to stop the spread of content they don't like; people can always share using other platforms like email or Reddit.
Gizmodo's report that a small group of Facebook contractors may have been manipulating users into reading more left-leaning content highlights just how powerful Facebook has become. There's no evidence that Mark Zuckerberg or anyone else in Facebook's leadership made a conscious decision to skew the trending topics box in a left-wing direction. Yet so many people spend so much time on Facebook that even a small shift in the platform's approach could have a big impact on what people read online.
And in principle, Facebook could do a lot more. People spend most of their time interacting not with the trending topics box at the right of the screen but with the news feed in the center. If Zuckerberg wanted to really shape people's media diets, he would have his engineers tweak how the newsfeed works to steer people toward favored content.
A particularly chilling example, suggested by the Atlantic's Robinson Meyer, would be for Facebook to turn itself into a turnout operation for Zuckerberg's favorite political candidates. Facebook's own experiments have shown that telling people their friends have voted increases the odds that they'll vote too. If Facebook showed those buttons only to Democrats — and it would be easy for Facebook to figure out who was likely to be a Democrat — it could swing a close election.
But the reaction to Gizmodo's latest story also illustrates that Facebook's power could prove relatively fragile. The news that Facebook may have been manipulating the trending topics box has rocketed around the conservative internet, generating a volume of bad press that has to hurt even for a company of Facebook's size.
And the current furor is tiny compared with the backlash that would occur if it were revealed that Facebook management was deliberately trying to manipulate the political system. That could easily trigger a broad conservative boycott of Facebook and could even create an opening for one of its rivals to gain market share at Facebook's expense.
Precisely because the danger of backlash is so high, it's unlikely that Zuckerberg will ever be so ham-handed as to explicitly censor or promote content on an ideological basis — or to try to directly manipulate an election. But even subtle tweaks to Facebook's algorithm can have profound effects. For example, Facebook's current model has contributed to the internet's "filter bubble" effect, in which people disproportionately read content they agree with, giving a boost to candidates with extreme views.
It wouldn't make sense to try to directly regulate the behavior of major internet platforms. Not only would good regulations be fiendishly difficult to write, but there's a good chance they'd get struck down on First Amendment grounds.
Nevertheless, it's worth taking seriously the dangers that can come from excessive concentrations of power and thinking about ways to limit that power. One obvious way to do this is by scrutinizing mergers more carefully. Back in 2012, for example, the Federal Trade Commission quickly approved Facebook's proposal to acquire Instagram, which quickly became one of the largest social media sites on the internet.
It's probably not fair to blame the FTC for missing the significance of this merger, since Instagram has grown fast over the past four years. But federal regulators should think harder the next time Facebook (or a large rival such as Google) wants to acquire a fast-growing online platform like Instagram.
There's also room for users to engage in self-help here. If you find it creepy that a secret Facebook algorithm has so much influence over your reading habits, there are a lot of other ways to find news stories that are less prone to manipulation. You can subscribe to your favorite sites using an RSS reader. You can spend more time on Twitter, which shows posts in a more chronological order, leaving less room for manipulation. Or you can browse the web the old-school way: bookmarking the homepages of your favorite news sites and visiting them directly.
Amazon's new streaming video service, Amazon Video Direct, is being widely reported as an effort to compete with YouTube, but it's probably better to think of it as an extension of Jeff Bezos's ongoing war with Netflix.
Netflix, of course, lets you stream unlimited video for a monthly fee. For a long time, Amazon made a streaming video library available to Amazon Prime subscribers — a broader, annual service whose main selling point was free two-day shipping. More recently, though, Amazon has created a video-only tier of Prime membership that competes directly with Netflix. And while Netflix has blazed the trail of creating streaming-native premium content like House of Cards and Unbreakable Kimmy Schmidt, Amazon is in the game, too, with shows like The Man in the High Castle and Transparent.
That's probably the right strategic context in which to understand Video Direct, which does have a YouTube-like element but also does important work to supplement their Prime offerings.
Video Direct is fundamentally a creator-facing platform, a set of infrastructure whereby people who make videos can upload them to Amazon's video service. Having done so, they have four options for how to make the videos available to customers:
The first of these options genuinely competes with YouTube, but it's not very compelling. YouTube has an enormous head start, and Amazon isn't trying to compete with it on price.
Options two and three are kind of interesting, but seem like they'd only be applicable to really big media brands.
It's option four that seems potentially disruptive, because it's giving people and organizations the ability to do something they genuinely can't do now — get into the premium video content game. Right now you can earn a living making ad-free television shows, but to do it you need to talk executives at Netflix or HBO or Showtime into paying you. What Amazon is doing is saying anyone who wants to can make a show with an absolute guarantee that if the show proves popular they will get paid.
A good hint that the pay-television market is Amazon's real target is offered by the company's Video Direct marketing material, which emphasizes the idea of watching Amazon video on your television or perhaps a tablet rather than watching casually on a web browser.
As with any new idea, the risk of course is that it will be a giant flop.
Existing premium television shows are created with plenty of upfront investment and marketing hype, and it's not clear that any of that would work on Amazon's pure royalty model.
But the company has likely gained some confidence from the Kindle Direct Publishing Select program, which produces e-books on a fundamentally similar model. KDP Select paid out a total of $14.9 million in March, up from $14 million in February.
And authors are in some ways more wedded to traditional distribution models than are video producers, many of whom have already embraced YouTube in the recent past and may be open to taking their ideas in a direction that isn't ad-supported.
"Sharing economy" services like Airbnb are thought of as revolutionary and democratizing. They let ordinary people make extra money using something they already have, like an apartment, and they offer consumers a wider variety of affordable choices than more traditional institutions, like hotels.
But despite its upstart nature, Airbnb still suffers from an old, institutional problem: racism.
The hashtag #AirbnbWhileBlack recently went viral, as black Airbnb users discussed their experiences with discrimination while using the service.
Research backs up their experiences. A working paper by three Harvard researchers found that Airbnb hosts were 16 percent more likely to reject black guests than white guests.
That's not nearly as frequent as, say, the rate at which people of color are denied home loans, a subject fraught with deeply embedded institutional discrimination that has denied equal housing access to people of color for generations. But it shows that even when you try to democratize things and subvert the old institutions like Airbnb does, there's still a core of implicit racial bias that just won't go away.
In a field experiment, the Harvard researchers created Airbnb user profiles that were the same except for the names — some sounded distinctly African American, and some sounded white. The researchers then used these profiles to ask about the availability of 6,400 Airbnb listings in five different cities. Profiles with white-sounding names got a positive response 50 percent of the time, but African-American names only got a 42 percent positive response rate.
As our online lives start to look more like our offline ones, baggage like racial bias comes with it
The results were incredibly consistent. The host's race, gender, and age didn't matter. The type and size of the property, or the type of neighborhood it was in, didn't matter. It didn't matter whether the host was a casual Airbnb user or a seasoned professional: All hosts largely discriminated against black guests at about the same rate. The only mitigating factor was whether a host had already had at least one black guest in the past.
People discriminated in this way even though it ended up costing them: Hosts only found a replacement guest 35 percent of the time, and they passed up an estimated $65 to $100 in revenue by rejecting a black guest.
It's unlikely that hosts did this consciously. Implicit bias is a powerful phenomenon that constantly informs our daily decisions and reinforces discrimination against marginalized groups — yet most people don't realize they have these biases. And an institution can never be truly egalitarian unless it finds ways to work against implicit bias.
Airbnb could work against racial discrimination in several ways, the paper's authors say. It could conceal guest names, or have people use pseudonyms like eBay does. It could also expand its "Instant Book" option, which works more like booking for traditional hotels or bed and breakfasts, where guests aren't screened before they're accepted.
In a statement following the release of the Harvard working paper, Airbnb seemed open to considering options to fix this problem. "We recognize that bias and discrimination are significant challenges, and we welcome the opportunity to work with anyone that can help us reduce potential discrimination in the Airbnb community," the company said.
The internet has the potential to increase equality. After all, the paper's authors argue, there's almost no way for platforms like Amazon or Expedia to discriminate based on race or any other personal category. And they point out that some inequities, like black people being charged more than white people when they buy cars, go away when the transaction happens online.
But as our online lives start to look more like our offline ones, baggage like racial bias comes with it. And as it turns out, even Amazon can still find ways to discriminate.
Facebook chief operating officer Sheryl Sandberg is both famous and infamous for her book Lean In: Women, Work, and the Will to Lead. She has been praised for frankly discussing the deep gender inequities in the corporate world and for giving women sound advice on how to navigate them. She has also been harshly criticized for presenting a narrow, corporate version of feminism that ignores, or even actively harms, disadvantaged women.
But on the Friday before Mother's Day, Sandberg wrote a long, touching Facebook post that gives both her fans and her detractors something to think about.
Sandberg, who lost her husband just over a year ago, writes about how she never truly understood the difficulties of being a single mother until she became one herself.
She writes about how lucky she is compared with many others: She is financially stable after losing her husband, whereas one in five US widows live in poverty by age 65. Sandberg was in a heterosexual marriage, so she was unquestionably entitled to the things that cohabiting or same-sex couples might not be if their partner dies.
And she writes about how the experience has made her rethink the perspective with which she wrote Lean In:
In Lean In, I emphasized how critical a loving and supportive partner can be for women both professionally and personally—and how important Dave was to my career and to our children’s development. I still believe this. Some people felt that I did not spend enough time writing about the difficulties women face when they have an unsupportive partner or no partner at all. They were right.
I will never experience and understand all of the challenges most single moms face, but I understand a lot more than I did a year ago. Our widespread cultural assumption that every child lives with a two-parent heterosexual married couple is out of date. Since the early 1970s, the number of single mothers in the United States has nearly doubled. Today, almost 30 percent of families with children are headed by a single parent, and 84 percent of those are led by a single mother. And yet our attitudes and our policies do not reflect this shift.
Single moms have been leaning in for a long time—out of necessity and a desire to provide the best possible opportunities for their children.
Some critics argue that the backlash against Sandberg and Lean In was overblown and unfair — that she was always writing from her own experiences, that she wasn't trying to speak for all women, and that her self-described "sort of" feminist manifesto shouldn't be read as any sort of full-blown treatise.
But it's also telling and important to see how such a dramatic shift in Sandberg's personal experiences might have colored her advice to other women.
Last month, Uber announced that it was settling two big driver lawsuits that could have forced the company to recognize its drivers as employees rather than independent contractors. In a post titled "Growing and growing up," Uber CEO Travis Kalanick argued that it was "time to change" Uber's relationship with drivers, offering drivers more transparency and due process.
Uber should make these the first steps toward a broader rethinking of the company's public image and the way it does business. Uber began its life as a scrappy startup trying to break up the taxi cartels that dominated most of America's big cities. When Uber was a clear underdog, many people appreciated the company's rebellious streak. Without it, the company probably wouldn't have survived.
But Uber is no longer an underdog. It's a huge company, and it's likely to become more dominant in the coming years. And when a company with real power behaves like Uber circa 2013, it doesn't seem scrappy. It seems menacing.
And that's a problem because Uber has ambitions to become much more than just another taxi service. Uber wants to become a kind of transportation utility — the default way that people and goods move around a city. That would make Uber one of the most powerful institutions in our cities.
But if Uber continues to be seen as a bull in the china shop, its growing authority will inevitably become a source of concern from voters and public officials. If Uber wants people to be comfortable with it wielding that kind of influence, it's going to have to become a company that people trust and admire. Yet Uber still seems focused on maximizing short-term profits rather than the public interest — and ironically, that approach is likely to make Uber less successful in the long run.
New companies need to think about growth and achieving profitability. But often, successful companies reach a point where they generate profits reliably enough that they can relax and think about larger strategic issues.
One of the most important things big companies have to think about is their public image. And different companies have chosen to handle this in different ways. Some companies — think about technology companies like Amazon or Apple, for example — are beloved by customers for their well-designed products and excellent customer service. Others — think especially about telecommunications providers like Comcast (whose NBC Universal subsidiary is an investor in Vox Media) or Verizon — are disliked by many customers who have little choice but to use them anyway.
Right now, Uber's reputation is in an awkward limbo between these two extremes. On the one hand, many people love Uber's service, and admire the company for its role in opening up the taxi market around the country.
On the other hand, Uber has had more than its share of bad publicity over the years — allegedly spying on customers, threatening to dig up dirt on journalists, and downplaying sexual assault concerns. Uber's surge pricing — which can lead to customers paying up to nine times the normal fare — is also a source of continued frustration among some customers.
The public's ambivalence means that Uber has more opportunity than most companies to reshape its public image. If Uber makes a concerted effort to improve its public image, it could cultivate a reputation that's more like its Silicon Valley peers. On the other hand, if it ignores the issue, it could be one or two scandals away from a more toxic public image.
And Uber's public reputation also matters more than it would for most companies because Uber is a lightly regulated company in an industry — transportation — that has traditionally been heavily regulated. If Uber succeeds in its goal of becoming a more and more important part of America's transportation system, it is going to face greater scrutiny from regulators. A good reputation for the public will be invaluable as Uber tries to shape the laws to its own advantage.
Uber's relationship with its drivers has emerged as a key point of controversy. Many drivers, of course, love the flexibility and independence provided by Uber's just-in-time model. But others — especially those allied with the labor movement — have grown dissatisfied. They fault Uber for its arbitrary and opaque process for removing drivers from the platform. And more broadly, they fault Uber for failing to provide its drivers — even those who effectively work for Uber full time — with the job stability and benefits they would enjoy if they were legally considered to be Uber employees.
Uber has flatly rejected calls to make its drivers employees, arguing persuasively that switching to a formal employer-employee relationship would be bad for both Uber and its drivers. A fundamental aspect of an employer-employee relationship is that the employer sets the employee's schedule — yet for many drivers, the ability to decide when and where to work is a big selling point for the Uber platform.
At the same time, Uber could be doing more for its drivers. Uber tacitly acknowledged this when it announced its settlement with California and Massachusetts drivers. Some of the company's key concessions — including an official policy on driver deactivation and an appeals process for unjustified terminations — are taking effect nationally, not just in California and Massachusetts. That's because they weren't really "concessions" — they were ways to improve driver satisfaction, which benefits Uber in the long run.
But there's more Uber could be doing here. As part of the settlement, Uber agreed to fund drivers' associations in Massachusetts and California that could serve as a forum for drivers to raise grievances with Uber management. But there's no reason to do this only in those two states — forming such organizations in all 50 states would be a good way to make sure drivers nationwide have an effective channel for raising concerns, promoting driver satisfaction and benefitting Uber in the long run.
And while it doesn't make sense to make Uber drivers formal employees, there's more Uber could do to reduce the volatility of driver earnings. When I drove for Lyft, the company had a promotion where they guaranteed that drivers would earn a minimum amount if they drove at least 50 hours, including 10 hours during peak times. Uber could adopt a similar policy, ensuring that its most loyal drivers don't have to worry that a light week will make it hard for them to pay the rent. (Uber does offer its riders some earning guarantees but they're much more limited.)
Improving the driver experience isn't just important because it will help Uber recruit and retain drivers — so long as the labor market remains weak, recruitment isn't a big challenge. The bigger issue is that hundreds of thousands of satisfied, loyal drivers could be a powerful political weapon. Happy drivers are going to be more willing to mobilize on Uber's behalf in cases where the basic ride-hailing business model is threatened by regulators.
This is something that major telecommunications companies like Verizon understand.
Verizon and the labor unions representing its workers have their share of conflict over pay, benefits, and working conditions. But at other times — like the recent fight over network neutrality regulations — the Communications Workers of America have been a key ally of major broadband providers. After all, a labor union of telecommunications workers can only prosper if telecommunications companies are economically healthy.
Uber and Uber drivers, similarly, have a shared interest in seeing the ride-hailing business succeed. And Uber drivers are much more sympathetic advocates for the Uber business model than Uber management. So ensuring that Uber drivers aren't just satisfied but enthusiastic about Uber will pay political as well as economic dividends.
Another major Uber flashpoint is over surge pricing. Many riders have a visceral negative reaction to the idea that they could be forced to pay three, five, or even nine times the standard rate during periods of peak demand.
Once again, Uber has refused to abandon variable prices. And once again, Uber has a valid point: If Uber charged the same rate at all times, the service would suffer from massive driver shortages during periods of peak demand.
Yet there's a straightforward way that Uber could preserve the benefits of variable pricing while blunting the backlash: Use discounts rather than surcharges. Instead of setting a low standard rate and then charging multiples of that amount during periods of high demand, it should set a high standard rate and then offer generous discounts at times when demand is low.
The actual prices charged under this system might be identical (except during periods of very high demand). But customers get less angry about a company cutting prices unexpectedly than raising them. This is why restaurants and bars offer half-price happy hour specials instead of doubling prices on Friday and Saturday nights.
If Uber isn't ready to go that far, an even simpler step would be to shift its pricing so that 100 percent of surge pricing revenue goes to drivers. That would make Uber's argument that surge pricing is needed to get drivers on the road more plausible.
Uber's goal should be to become a universal public utility. And one thing public utilities do is provide service to everyone.
One way Uber can make this point while generating positive publicity for itself and developing better relationships with public officials would be to expand its service to disabled riders. Last year, Uber launched a pilot program for disabled riders in Austin, Texas. But the company continues to face criticism from disability advocates for doing too little to ensure that its service is accessible to disabled customers.
But there's room for Uber to go a lot further. The Americans with Disabilities Act requires transit agencies to provide disabled customers with paratransit services to help them get to and from bus and subway stops. These services are very expensive for transit agencies to provide, and they rarely provide the kind of on-demand service that Uber riders now take for granted.
Uber has flirted with becoming a paratransit provider, but the company should do more than flirt: It should start lobbying transit officials across the country for contracts to provide paratransit services. With the infrastructure Uber already has, it should be able to offer services that are a lot more convenient than conventional paratransit, while saving local governments money.
This probably wouldn't earn Uber big profits, but it would have benefits that could be far more important. It would get Uber a lot of positive press for improving the lives of people with disabilities. It would allow them to deepen their ties with local officials whose help they may need in the future. And it could convert disabled people — and the organizations that advocate on their behalf — into key Uber allies.
The current economic recovery has been under way since 2009, making it one of the longer periods of continuous economic growth in American history. But it has also been one of the most sluggish recoveries on record. The economy just isn't creating as many jobs — or pushing wages up as quickly — as it did in previous decades.
On Friday, the Labor Department released new data showing the performance of the labor market in April. And it continued the pattern of the past couple of years. The economy added 160,000 jobs, a bit below the 200,000-per-month average of the past couple of years:
Wages have grown 2.5 percent over the last year — a respectable figure given that the inflation rate was only about 1 percent during the same period.
The unemployment rate was unchanged at 5 percent.
But the most disappointing statistic in this month's numbers is the labor force participation rate. This figure shows the fraction of the workforce that is either working or looking for work. Normally, this figure falls during recessions and then goes back up during economic recoveries. But it fell from 2008 until 2015 — a sign that the economy wasn't growing robustly enough to lure back workers who had left the workforce. Over the past six months, the labor force participation rate had started to rise, a hopeful reversal of that trend.
But in April, the LFPR declined by 0.2 percent. That obviously could prove to be a blip if the LFPR sees big gains in future months. Still, it seems like another sign that the economy has yet to deliver the kind of rapid growth the country has been hoping for.
Most of us take it for granted that we have a right to quit our current job and find a new one. But a growing number of workers are seeing this freedom curtailed by noncompete contracts that bar employees from taking jobs with a competitor of their previous employer.
A new report from the White House portrays this trend as a threat to both worker rights and economic growth. Limits on worker mobility can slow the spread of knowledge and, therefore, the rate of technological progress. These agreements also reduce worker bargaining power, which can lead to lower wages and make it harder for workers to find jobs they enjoy.
The report is the latest sign of growing interest in this issue from policymakers. Last year Hawaii banned noncompete agreements for workers in the technology sector, while Oregon banned them for doctors and Oregon and Utah placed new time limits on the deals.
But in recent years, no state has gone as far as outlawing these agreements outright. And that's true even though one of the few states where noncompete agreements are totally unenforceable, California, is also one of the nation's most innovative.
Noncompete agreements have traditionally been used against highly skilled workers to prevent them from taking company secrets to competitors. In theory, the enforcement of these deals gives workers a greater incentive to provide worker training and invest in research and development programs.
However, it's not obvious that stopping the spread of knowledge in this way is actually good for the economy as a whole. Critics point to a famous 1999 paper that attributes Silicon Valley's success to the state's refusal to enforce noncompete agreements. With California courts refusing to enforce these contracts, Silicon Valley developed a job-hopping culture that promoted the rapid spread of new ideas.
It's impossible to know for sure, but critics of noncompetes argue that this freewheeling culture was a key factor that allowed the San Francisco area to outpace other regions — like Boston's Route 128 corridor — as the nation's premier technology center.
There's a lively debate about whether it's a good idea to enforce noncompete agreements against highly skilled employees. But it's harder to defend the use of noncompete agreement against lower-skilled workers. The White House cites one study finding that 14 percent of workers making less than $40,000 per year and 15 percent of workers without college degrees are subject to noncompete agreements.
These workers are unlikely to receive much training, and they rarely possess valuable trade secrets. In one infamous case, for example, the Jimmy John's sandwich chain has been asking its employees to sign agreements promising not to take their sandwich-making skills to competing sandwich shops. It's hard to imagine that this restriction increases the incentive Jimmy John's has to invest in better sandwich-making techniques.
What it can do, however, is limit workers' bargaining power and thereby push down wages. The White House points to empirical research finding a connection between stronger enforcement of noncompete agreements and lower wages.
One way to address this problem would be to ban noncompete agreement for low-wage workers. A proposal from Sen. Al Franken (D-MN) and Sen. Chris Murphy (D-CT) would do just that, prohibiting the contracts among workers making less than $15 per hour.
But there are also other, less dramatic actions policymakers could take. For example, employers often ask employees to sign noncompete agreements after they've already accepted the job, limiting their bargaining power. The White House argues that employers could be required to notify employees of the noncompete agreement at the time they make a job offer, giving employees an opportunity to look for a different job if they want to.
The White House also suggests that noncompetes should be unenforceable in cases where a company lays off workers. The White House also points to states where the courts automatically strike down overly broad noncompete language, giving employers a stronger incentive to write narrower, less onerous noncompete rules.
The White House is a naturally conservative institution, so it's not surprising that it's sticking to these relatively modest reform ideas. But it's worth asking whether states shouldn't go even further.
Noncompetes have been unenforceable in California for more than a century. Not only does this not seem to have hurt the state's economy, it may have helped the state attract the talent that made Silicon Valley what it is today.
More fundamentally, noncompete agreements impose a major restriction on the freedom of employees to decide where they want to work. We're a nation that values freedom, and where you work is one of the most important decisions any working adult makes.
In recent years, no state has gone the full California route and banned the enforcement of noncompete agreement outright. But some of them should try it. Absent compelling evidence that noncompetes are economically beneficial — and if anything,  the evidence suggests the opposite — states might want to err on the side of protecting worker autonomy.
Update (May 5, 2016): In a new post on his website, Wright has now tacitly admitted that he can't prove his identity as Bitcoin's creator. But rather than admitting the obvious — that he isn't actually Satoshi Nakamoto — Wright claims that "As the events of this week unfolded and I prepared to publish the proof of access to the earliest keys, I broke. I do not have the courage. I cannot."


For years, people have been trying to unmask Bitcoin's enigmatic creator, known only by the pseudonym Satoshi Nakamoto. Previous efforts have not panned out. But this morning three media organizations — the BBC, the Economist, and GQ — reported that an Australian man named Craig Wright was claiming to be Bitcoin's creator.
And this story is different from previous efforts to unmask Nakamoto in a crucial way: Wright claims he can offer cryptographic proof of his identity. As the BBC puts it, "Wright has provided technical proof to back up his claim using coins known to be owned by Bitcoin's creator."
Most convincing of all, a prominent Bitcoin developer named Gavin Andresen says he believes that Wright is Nakamoto. Andresen is the man Nakamoto chose to lead the Bitcoin project when he abruptly left the Bitcoin community in 2011, and he's still one of the most prominent figures in the Bitcoin world.
Yet something doesn't add up about Wright's claims. The real Nakamoto would be able to settle all doubt about his identity by publishing a mathematical proof called a digital signature. Instead, Wright seems to have orchestrated an elaborate smoke-and-mirrors campaign, offering private demonstrations to a handful of people — most of whom weren't in a position to fully verify the evidence he provided.
Security researcher Dan Kaminsky is particularly harsh. "This is a scam," he wrote in a Monday blog post. "Not maybe. Not possibly. Wright’s done classic misdirection by generating different scams for different audiences."
The idea that Wright is Nakamoto isn't new. Wired and Gizmodo first reported the possible Wright-Nakamoto connection back in December. These publications were contacted by a man posing as a hacker with a grudge against Wright. He provided what appeared to be incontrovertible proof that Wright was, in fact, Satoshi Nakamoto.
But this evidence proved hard to authenticate. The documents included email discussions with computer forensics expert Dave Kleiman from 2008 — before Bitcoin was released to the public. Unfortunately, Kleiman died in 2013, so he can't confirm their authenticity. Other, more recent, documents merely proved that Wright had been telling various associates that he was Nakamoto — but of course that doesn't rule out the possibility that Wright could have been lying.
But the biggest red flag in these December stories was identified by Wired reporter Andy Greenberg. Early Wright blog posts appeared to link him to Bitcoin at a time when the cryptocurrency was still in its infancy. The problem is that these posts were modified in 2013 or later to add key references to Bitcoin.
Similarly, forensic examination of a cryptographic key purportedly belonging to Nakamoto — and linked to Wright — showed that it was likely created by a version of cryptographic software that didn't yet exist when the key was created in 2008.
In short, most of the evidence purportedly showing that Wright was connected to Bitcoin during its creation in 2008 and 2009 appears to have been manufactured years after the fact. We don't know who was perpetrating this apparent hoax, but some in the Bitcoin community started to suspect that the anonymous "hacker" was actually Wright himself.
Wright still insists that the December revelations occurred against his will. He says that those reports led to a swirl of rumors that have negatively affected his friends, family, and employees. And so, he claims, he decided to set the record straight by finally acknowledging that he is Bitcoin's creator.
This ought to be easy to do. Bitcoin is based on a cryptographic technology called digital signatures. Bitcoin users "sign" transactions before submitting them to the Bitcoin network, ensuring that only the owner of a particular Bitcoin account can spend money from it.
Some of the earliest Bitcoin transactions were signed with a private key belonging to Satoshi Nakamoto. So the easiest and most convincing way to show that you're Nakamoto is to sign something with this private key. Assuming Nakamoto has practiced good security, no one else should be able to do this. And once a signature is published, anyone in the world can use standard software tools to mathematically verify that it was signed with the same private key as Nakamoto's earliest Bitcoin transactions.
On Monday, Wright published a long, rambling blog post purporting to do just that. But security experts say it does nothing to establish Wright's identity. What Wright appears to have done is to find an old digital signature generated by Nakamoto years ago, reformatted it, and then presented it as a new signature generated by Wright.
So the post doesn't just fall short of proving that Wright is Nakamoto. It suggests that Wright is willing to go to elaborate lengths to trick people into believing that he is Bitcoin's creator.
Wright's strongest bit of evidence is the endorsement of Gavin Andresen. When Nakamoto stopped contributing to the Bitcoin project in 2011, he turned effective control over the project to Gavin Andresen, who was then a software developer in his 40s. Today, Andresen is the chief scientist for the Bitcoin Foundation and a member of Bitcoin's core development team.
So when Andresen wrote this morning that he believed Wright was the creator of Bitcoin, people paid attention.
"During our meeting, I saw the brilliant, opinionated, focused, generous – and privacy-seeking – person that matches the Satoshi I worked with six years ago," Andresen wrote. "And he cleared up a lot of mysteries, including why he disappeared when he did and what he's been busy with since 2011."
However, Andresen had never met Nakamoto face to face before, so this didn't mean a whole lot. The key question was whether Wright had Nakamoto's encryption keys. And Andresen claimed Wright did just that.
Andresen described the procedure he used to verify Wright's identity in a Reddit post. Wright cryptographically signed a message chosen by Andresen, transferred it to a new laptop, and then used software to verify that the signature was valid.
If taken at face value, this appears to show that Wright has Nakamoto's private keys. But this verification process leaves lots of room for a hoaxster to trick a gullible observer. The key question is whether Wright tampered with the software used to verify the digital signature — if he did, then obviously this verification is meaningless.
And crucially, Wright didn't allow Andresen to verify the signature on his own laptop, keep a copy of the signature, or (best of all) publish it so it could be verified by anyone in the world. So if there was something fishy about the software Wright used for his demonstration, Andresen didn't have any opportunity to confirm that.
The demonstration Wright provided to the Economist was similarly limited. The newspaper wrote that "information that allows us to go through the verification process independently was provided too late for us to do so fully." The Economist concluded that "as far as we can tell," Wright had control of Nakamoto's private key. But under the circumstances, that doesn't mean a whole lot.
So were Andresen, the Economist, and other observers tricked by the digital equivalent of a magic trick? No one other than Wright knows for sure. But given the elaborate lengths someone has gone to manufacture other evidence linking Wright to Nakamoto, it's worth being very skeptical.
But one thing we do know for sure is this: If Wright were really Bitcoin's creator, he could put all these doubts to rest very quickly. All it would take is for him to publish the digital signature he claimed to have generated for Gavin Andresen. In a matter of minutes, independent experts would be able to check the signature and verify that it was created using the same key as the earliest Bitcoin transactions.
But Wright hasn't done this. And it's hard to think of any plausible explanation other than the obvious one: that he hasn't done it because he can't do it.
Indeed, the way Wright has stage-managed the latest revelations about himself seem inconsistent with what we know about Nakamoto. Wright chose to give his scoop to the BBC, the Economist, and GQ. These are all excellent publications, but none of them are known for their in-depth coverage of computer security. The real Satoshi Nakamoto should have anticipated that no one would give much weight to a GQ scoop about his identity.
Bitcoin was Nakamoto's attempt to create a financial system that didn't require trusting the fallible human beings that run the banking system. Yet when Wright decided to reveal his identity as Nakamoto, he chose to do it via face-to-face meetings with a handful of journalists and Bitcoin insiders instead of providing mathematically rigorous proof that anyone could verify. It's hard to believe that's what Nakamoto would have done.
A class action lawsuit against Starbucks is seeking $5 million in damages over the key difference between hot drinks and cold drinks: ice.
When you order a 24-ounce "venti" coffee at Starbucks, you're getting 24 ounces of coffee. But when you order a 24-ounce iced coffee, you're getting 14 ounces of coffee, plus a whole bunch of ice.
And if customers knew that, a lawsuit filed in federal court alleges, they wouldn't have been willing to pay nearly as much. The lawsuit is seeking $5 million in damages on behalf of everyone who's bought a Starbucks cold drink since 2006.
This is a silly lawsuit; it implies, among other things, that customers don't realize that ice takes up space in drinks. But the fact that Starbucks, which got its start with hot coffee and blended cold drinks, is facing a lawsuit over how much ice it dumps into customers' cups actually is an interesting insight into how its business is changing.
Selling coffee in the morning is a lot more competitive than it used to be, and Starbucks is getting crunched at both ends of the market. Dunkin' Donuts has expanded nationally. Krispy Kreme is trying to make its coffee seem more upscale. The Canadian chain Tim Hortons has merged with Burger King and is increasing its presence in the United States. And at the higher end of the market, super-upscale coffee — pourovers, single-origin beans, and so on — is growing so much that Starbucks opened a fancy tasting room in New York in 2014.
So Starbucks' future growth doesn't rely entirely on hot coffee at all. Instead, according to the company's five-year plan, the overall goal is to keep people coming into stores in the afternoon and evening, long after they've finished their morning coffee.
That means selling beer and wine, food, and yes, iced drinks:
And iced drinks are a moneymaker in part because people are willing to pay more for them. Americans really want their cold beverages, ice and all.
On Monday, Puerto Rico is expected to skip payment on $389 million in debt owed by the island's Government Development Bank to bondholders. Gov. Garcia Padilla of Puerto Rico has been warning for over a year that this government will be unable to fully pay what it owes, and has asked Congress to provide for a structured path to bankruptcy. He didn't get it, largely due to opposition from congressional Republicans, so he is now being forced to resort to unilateral repudiation of debt.
"Faced with the inability to meet the demands of our creditors and the needs of our people, I had to make a choice," he said "I decided that essential services for the 3.5 million American citizens in Puerto Rico came first."
The Obama administration and many members of Congress had been pushing to create a bankruptcy process for Puerto Rico that would allow bondholders to recoup some of what they are owed while allowing the island's government to continue functioning. Since Congress has, so far, not acted we are now on the precipice of a much more uncertain and chaotic situation in which Puerto Rico will attempt to selectively cancel debts and bondholders will seek to use the federal courts to block the Puerto Rican government from operating until it pays up.
Puerto Rico is an island in the Caribbean Sea that is also a largely self-governing territory of the United States.
For years, a quirk of US law created a tax subsidy for Puerto Rican debt that encouraged middle class Americans to binge on loaning money to Puerto Rico without really realizing that's what they were doing. The Puerto Rican government took advantage of this situation by borrowing a lot of money, but didn't manage to spend the money that they borrowed to accomplish much that was useful in the long term.
Then starting in 2006, Puerto Rico was hit with a series of economic misfortunes. At that point, Puerto Rico's strong ties to the United States became a liability because Puerto Rico could not adjust to bad economic times with currency depreciation, but Puerto Rican people could adjust to bad times by moving to the mainland United States.
That has left Puerto Rico in a 10-year downward spiral of tax hikes, spending cuts, emigration, and higher interest rates. About a year ago, Padilla announced that Puerto Rico is in a "death spiral" that he needs to halt, and he began saying that bondholders will not be fully paid. He asked for the creation of a legal bankruptcy process but warned that even absent one the money would not all be paid.
The bad news is that Puerto Rico is really facing two separate death spirals.
One is the basic death spiral of self-fulfilling default risk. The more money you owe, the more likely it is that you won't be able to pay back all the money that you owe. That means that when your debts come due and you need new loans to pay off the old ones, investors start demanding that you compensate them for their risks in the form of higher interest rates. Those higher interests rates increase the financial burden on your country, and that in turn makes default more likely.
But the death spiral Garcia was referring to is a second one.
People generally don't like paying taxes but do enjoy receiving high quality government services. Consequently, a given territory's ability to turn tax revenue into useful services is an important driver of whether people will want to live and do business there. To the extent that your tax revenue is going to pay off old debts, it is not going to provide current services. Thus the more of your budget that you dedicate to debt repayment, the worse the value proposition that you are delivering to your territory's residents and businesses.
The harder Puerto Rico squeezes, in other words, the more its economy suffers. But the more the Puerto Rican economy suffers, the harder it is for Puerto Rico to pay back its debts. In other words: death spiral.
Puerto Rico's total debt outstanding is $72 billion, which is small relative to the overall United States economy but big pretty much any other way you slice it.
Two US states have more debt than that — California and New York — but Puerto Rico is much smaller, with approximately the population of San Diego County. New York and California are also richer than the average US state whereas Puerto Rico is poorer. Almost all US states have growing populations, but Puerto Rico's is shrinking. In October of 2013, the Economist reported that "in America’s 50 states the average ratio of state debt to personal income is 3.4%" whereas the ratings agency Moody's says the comparable figure for Puerto Rico is 89 percent.
Hawaii, the most indebted US state by this measure, has a 10 percent ratio.
In other words, Puerto Rico's debts really are way out of line with what any state is financing, and there's no real precedent for paying down debts of this magnitude. There's no real precedent for refusing to pay them either, but default is by no means a crazy option.
A large share of the money was initially lent by people not so different from you or me — middle class Americans, especially those living in higher tax states. As for what they were thinking, they probably weren't thinking much of anything in particular.
They were just putting money away for retirement in municipal bond funds to diversify their portfolios.
Those funds, in turn, were invested in a diverse array of US public sector bonds. Since Puerto Rican bonds feature some unusual tax advantages, there was an unusually robust level of demand for Puerto Rican debt. Successive Puerto Rican governments responded to demand for their debt in the economically rational way — they borrowed an unusually large amount of money. But none of this lending was driven by particular scrutiny of the details of Puerto Rico's economic situation, and when Puerto Rico's economic fortunes began to change about 10 years ago the dynamics became untenable.
The flip side of this mindless lending is that Puerto Rico failed to take real advantage of the financial windfall it provided. In theory, loads of cheap debt could have been used to finance incredibly useful public works projects and other social services that laid the foundations for enduring prosperity. But it didn't happen. Instead, Puerto Rico seems to have mostly taken advantage of the opportunity to run a somewhat more generous welfare state than the island could really afford over the long term. Thus when the easy money went away, the country was left with a huge pile of debts rather than a huge pile of enduringly useful infrastructure.
Not exactly, though "America" from the 1957 musical West Side Story does deal with many of the relevant issues — debt, Puerto Rico's relative impoverishment vis-a-vis the United States, the possibility of mass emigration, and the island's oft-misunderstood political relationship with the mainland United States. The recent Glee version tones down the minstrelry relative to the original:

But problematic though it may be, Rita Moreno's classic 1961 film performance is still worth your time and provides clearer context. A crucial eight-bar musical phrase from the song is replicated in Metallica's "Don't Tread on Me," if heavy metal is more your thing.
All municipal bonds are exempt from federal income taxes. In addition, if you buy municipal bonds issued by the place where you live, those bonds are exempt from state and local income taxes as well. Such bonds are known as triple tax exempt, and they're a big deal for municipal finance and high tax places like New York and California.
But Puerto Rico's bonds are triple tax exempt regardless of where you live.
This is not a huge deal for most Americans, but for a high-income person living in a high-tax state it can be a very big deal. And it helped fuel a lot of lending to Puerto Rico that wasn't necessarily thought through in a very serious way.
Starting in 2006, the island has been hit by a series of negative shocks that have undermined its economy and its creditworthiness.
That was the year that Puerto Rico lost its longstanding federal tax advantages as a location for US companies to do business in. From 1986 to 1996, these took the form of special tax credits (pre-1986 the tax advantage worked differently but had a similar impact) that were rationalized as a way to help Puerto Rico be competitive with developing countries as a manufacturing location, given that Puerto Rico-based firms need to comply with basic US labor rights and safety standards. But starting in 1996 these advantages were placed on a 10-year phase-out schedule and, despite the hopes of Puerto Rican politicians (and tax break hungry business), they were never extended or replaced. That prompted an exodus of businesses from the island from which it has never really recovered.
After that:
A related ongoing development is that in response to Puerto Rico's economic woes, Puerto Rican people have increasingly chosen to leave the island.
Pew
For any given individual, migrating to the mainland makes a lot of sense given the economic conditions on the island. But each person who departs leaves the people who remain with a higher share of old debts to repay. That makes the economic situation even worse and the debt even harder to pay.
No. People throw the word bankrupt around a lot, but it actually has a specific technical meaning laid out in the US Bankruptcy Code and other laws. There are a lot of different kinds of bankruptcy, and they all lay out one way or another for a person or organization that cannot pay back its debts to restructure its payments and move forward. The key is that bankruptcy is an organized, rule-governed process designed to bring some clarity to the situation and ideally to resolve it sooner rather than later.
But US bankruptcy law makes no provision for Puerto Rico (or a US state) to declare bankruptcy. Indeed, what's really special about Puerto Rico is that Puerto Rico's municipalities and municipal corporations — unlike towns, utilities, and school districts in the US mainland — can't declare bankruptcy either.
Puerto Rico's supporters in the White House, in Congress, and the Treasury Department have been trying to create a bankruptcy process for Puerto Rico, but bondholder organizations have characterized this as a "bailout" and blocked action in Congress. So now we get a situation that is going to be a lot more chaotic and generate a lot more revenue for lawyers.
First, the Puerto Rican government will try to unilaterally decide which bond payments it needs to skip in order to keep the lights running. Then there will be lawsuits — lots of lawsuits — in which various creditors try to force Puerto Rico to pay them ahead of paying Puerto Rico's civil servants, pensioners, social assistance recipients, and other creditors.
It is relatively unlikely that a federal court based in New York will actually try to dispatch cops to Puerto Rico to physically haul money off. But it's easy to imagine a scenario in which banks based in the mainland are told it's illegal to participate in processing payments of salaries for Puerto Rico's police officers because Puerto Rico's bondholders have not yet been paid.
Puerto Rico should be an independent country.

To many residents of the mainland United States, separation between the USA and Puerto Rico seems like a natural solution to the island's financial woes as well as the most logical resolution of an anomalous constitutional situation. After all, the empire-building and thirst for military bases that led the United States to take Puerto Rico away from Spain in 1898 are long since obsolete, and Puerto Rico is linguistically, culturally, and economically distinct from the United States.
What's more, sovereignty could help Puerto Rico in a number of ways. For starters, an independent Puerto Rico would have its own currency and could set monetary policy that is appropriate to Puerto Rican conditions.
Right now the Federal Reserve does things with little regard for their impact on Puerto Rico, and the value of Puerto Rico's currency (the US dollar) is driven by factors that have nothing to do with Puerto Rico's situation. An independent Puerto Rico could also establish a tax and regulatory framework that is suitable to its status as a middle-income country, rather than subjecting businesses to policies designed for the much richer United States.
Wikipedia graphic / Puerto Rico government data
The big problem with this idea is that Puerto Ricans don't want to be independent. In a 2012 referendum, a large minority of the population said Puerto Rico should continue with the status quo. Among the 54 percent who desired change in Puerto Rico's constitutional status, 60 percent said Puerto Rico should seek to become a US state. Only 5.5 percent of the 54 percent favored independence.
There are some practical reasons for this, but the main reason is that Puerto Ricans have been Americans for a long time and just like other Americans feel a strong connection to their country. Most Puerto Ricans have friends and family members living on the US mainland, and many people go back and forth. Consequently, the idea of independence is just a total non-starter. Statehood has more appeal to Puerto Ricans, but would not address any of the factors leading to Puerto Rico's debt problems, and securing a reputation for the island as a deadbeat is unlikely to inspire the mainland United States to become excited about statehood.
Things are looking up for Mark Zuckerberg. While a lot of other major tech companies — including Apple, Google, and Microsoft — have posted disappointing financial results for the first quarter of 2016, Facebook announced yet another quarter of spectacular growth on Wednesday. The company's quarterly revenues have grown by more than 50 percent over the last year — an impressive achievement for a company of Facebook's size.
To some extent, Zuckerberg is simply riding a wave of popularity that started when he created the social network 12 years ago. But Zuckerberg has also made a series of savvy decisions that have allowed the company to maintain momentum as others have stumbled. And Facebook stock price is about 90 times its annual earnings (the stock market's overall average is about 24), suggesting that Wall Street expects the company to continue outperforming other companies of its size.
Zuckerberg believes Facebook's success has been enabled by Facebook's unusual corporate structure, which gives him permanent and near-total control over the company he founded — despite owning fewer than 18 percent of Facebook shares. On Wednesday, Zuckerberg announced a new plan that would allow him to sell most of his shares — or donate them to charity — further reducing his stake in the company without losing his majority of the firm's voting rights.
You might think Facebook shareholders would get nervous about the prospect of Zuckerberg having total control of Facebook — with no accountability to other shareholders  — even as his financial stake in the company dwindles. So far there's been little sign of opposition, but there's a good chance Facebook will face a lawsuit before the new plan can go into effect.
Initially, Facebook was a website, and a less savvy CEO might have missed the significance of smartphones and tablets — as some other web companies did. Flickr, for example, was a popular web-based photo sharing service in the late 2000s, but its owner, Yahoo, never figured out how to turn it into a popular smartphone platform.
But Zuckerberg quickly grasped the importance of smartphones and ensured that Facebook would have a full-featured mobile app.
Zuckerberg further shored up Facebook's position in the mobile marketplace by buying other companies. When Facebook bought Instagram for $1 billion in 2012, the move was mocked by observers like the Daily Show's John Stewart. Now, of course, Instagram has more than 400 million users, and there's little doubt that it would be worth a lot more than $1 billion if it were an independent company.
Facebook also paid $21 billion in 2014 for WhatsApp, a mobile messaging company that had more than 400 million users at the time.
Facebook's focus on mobile platforms has paid off. In its most recent quarter, mobile ads accounted for a whopping 82 percent of the company's overall revenue.
Most companies are run on a principle of one share, one vote. That means that as the company grows and raises money from outside investors, the initial founders wind up with a minority of the shares, making it possible for them to be fired if other shareholders become convinced that someone else would run the company better.
But some companies — notably Facebook and Google — are structured to avoid this situation. These companies have two different classes of shares, with the shares held by insiders like Zuckerberg (and Google co-founders Larry Page and Sergey Brin) having a lot more voting power than the shares controlled by the general public. The result: Zuckerberg owns just 18 percent of Facebook shares but controls 56 percent of the company's voting rights, giving him the right to hand-pick the board of directors that, in turn, selects the CEO.
That's a pretty sweet deal for Zuckerberg, because it means he can't be fired no matter how dissatisfied the other shareholders — who collectively own more than 80 percent of the company's value — get with his leadership. But even this isn't enough for Zuckerberg.
Zuck has vowed to give 99 percent of his wealth to charity during his lifetime, and he wants to get started on this now instead of waiting until after he leaves Facebook. But if he just sold a bunch of shares, his voting power would drop below 50 percent and he could lose control of the company.
So now Facebook is proposing to create a third class of shares, and give every existing shareholder — both insiders like Zuckerberg who have extra voting rights and members of the general public who don't — two extra shares for each share they already have. After this stock split, shareholders would have three times as many shares and each share would be worth one-third as much.
And crucially, these new shares would have no voting rights at all. That means Zuckerberg could sell them, liquidating two-thirds of his economic stake in Facebook, without diluting his voting power. In other words, he could go from owning 18 percent of Facebook to owning 6 percent, while maintaining total control of the company.
You might expect this to trigger a backlash from shareholders upset at the idea of somebody with a shrinking share of their company keeping total control over it. And indeed, when Google proposed a similar move back in 2012, a large majority of non-management shareholders opposed it. But founders Page and Brin already controlled a majority of the voting rights, and they used their power to approve the plan to give themselves even more power. Shareholders filed a lawsuit, but the lawsuit was settled, clearing the way for the plan to go forward.
The Facebook proposal for a new class of stock is even more aggressive than the Google proposal was. The Google proposal issued one new share for each share outstanding. Facebook's proposal issues two shares for every existing share — allowing Zuckerberg to sell two thirds of his shares and still maintain a majority of voting power.
There hasn't been much sign of a backlash yet, but it's a safe bet that the proposal will lead to shareholder objections and a lawsuit. Facebook's lawyers argue that the proposal benefits public shareholders because it gives Zuckerberg a stronger incentive to remain as CEO for the long run. Because the Google lawsuit was settled, we never got a ruling on the merits, so it's not clear if the courts will buy this kind of argument.
The clear signal, however, is that Zuckerberg intends to maintain control of Facebook even as he diverts most of his Facebook wealth to charitable causes. Zuck could have followed the lead of Bill Gates, devoting his 20s, 30s, and 40s to running Facebook and then devoting the later years of his career to giving his wealth away.
But Zuckerberg is evidently in a hurry to get started on the philanthropic phase of his career while he's still the boss at Facebook. Last year, he created an organization called the Chan Zuckerberg Initiative that will serve a similar function to the Bill and Melinda Gates Foundation, allowing Zuckerberg and his wife Priscilla Chan to begin donating his billions to worthy causes. If he's able to donate two-thirds of his Facebook stake to the Chan Zuckerberg Initiative, the organization will have a $30 billion war chest that it can begin dispensing to worthy charitable organizations immediately.
Correction: This article originally mis-stated how Google's stock split worked.
A bunch of companies in Silicon Valley, Detroit, and around the world are racing to build cars that drive themselves. And they have a big, important disagreement about what these vehicles will look like.
The disagreement centers around the steering wheel — and specifically whether cars will have one or not. On one side of the debate are conventional car companies who largely view self-driving capabilities as a new feature to add to their existing fleet of self-driving cars. On the other side of the debate is Google, which has already built a self-driving car prototype that's designed to operate in a fully autonomous mode.
This is partly a debate about safety; each side claims its own approach will save more lives. But it's also a clash of business models. If human beings can take over in situations the computer can't handle, it will be easier for car companies to gradually introduce self-driving capabilities to their existing vehicle fleets. On the other hand, companies like Google and Uber are starting from scratch anyway, and they can more easily adopt on-demand business models that are more compatible with fully autonomous operation.
The debate matters because regulators are currently drafting safety standards for autonomous vehicles. Reuters reports that California's standards may require cars to have a steering wheel, while federal regulations may allow fully autonomous operation.
If you're worried about the safety of self-driving vehicles, you might expect it to be safer to allow human drivers to take over in an emergency. It's easy to imagine nightmare scenarios where a self-driving vehicle suddenly malfunctions — or is hacked — and a helpless passenger is hurtled to his death.
Including an option for human-driven operation also allows self-driving technology to be conservative about when it takes control. In situations where the software isn't sure about the right thing to do — say, a construction site or a blizzard — it can hand over control to a human driver.
But some experts say that cars capable of switching between automatic and self-driving can actually be more dangerous than purely automated driving. That's because the most dangerous moment is the instant a vehicle switches between an autonomous and human-driven state.
If a driver wasn't paying attention just before the switchover, he's more likely to make a misjudgment about how fast the car is moving, which vehicles are around, and so forth. Human drivers might also get confused and assume the car is driving itself when it isn't. And over time, as cars become more and more automated, human drivers might get out of practice, making them less safe drivers if they're forced to take over in unusual situations.
This isn't just a theoretical problem — it's something that has cropped up with the autopilot feature on airplanes. As a 2014 New Yorker article pointed out, a number of crashes have occurred because pilots simply weren't paying close enough attention as the plan largely flew itself. That caused them to make crucial mistakes when they were forced to take over control.
One study found that with higher levels of automation, "pilots' ability to make complex cognitive decisions suffered a palpable hit. They were less able to visualize their plane's position, to decide what navigational step should come next, and to diagnose abnormal situations."
With the plane doing most of the driving, pilots had more trouble concentrating on the task in front of them. Their minds tended to wander That made them less well-prepared when an emergency required them to exercise good judgment and quick thinking.
Car companies are just starting to introduce partially self-driving cars onto the market, so we don't yet know if the same kind of problem will crop up on our roads. But it provides a powerful argument for advocates of allowing fully autonomous driving.
It's not a coincidence that Google has been leading the fight to allow fully self-driving vehicles, while many car companies prefer the gradual route. These technological approaches dovetail with the business strategies each company is likely to pursue.
Car companies are mostly in the business of selling finished automobiles directly to customers. And if you're buying a car, you naturally expect it to work in all circumstances. Yet building a car that can work in all circumstances — in rural as well as urban areas, in rain and snow, on mountain passes and in construction zones — is a difficult technical and logistical challenge that might take many years to fully solve.
There are two big issues here. One is that certain environments are just inherently challenging for an autonomous vehicle to navigate. A big snowfall, for example, will render most of a self-driving car's landmark-detection capabilities useless. Driving through a construction zone is another case where fully automated driving could be challenging, since lanes aren't as clearly marked as on a normal road.
The other issue is that self-driving software may need detailed maps to navigate safely. And mapping the entire country will take a lot more effort than mapping major populated areas.
A mixed-mode vehicle provides a nice solution to this dilemma. The car can drive in situations where it's safe to do so — when it's not snowing, not in a construction zone, and in an area where maps are available. The rest of the time, the human driver can take control.
Fully autonomous vehicles are going to have to take a different approach. If the vehicle isn't confident it can drive safely in a particular situation — in snow, in remote areas, on dirt roads, and so forth — it's going to have to refuse to drive altogether. And that means cars like Google's prototype that have no steering wheel are going to have some major limitations, especially in the early years.
Of course, that's going to be a hard sell if Google is trying to sell its vehicles directly to customers. Who wants to own a car that can only drive to certain parts of the country or certain weather conditions? But this wouldn't be such a big problem for an Uber-style on-demand service, since people who take taxis typically rely on a mix of transportation modes to get around anyway.
Over the long run, this is likely to emerge as the biggest cleavage in the self-driving car market. Uber is working on its own self-driving car technology, and we can assume they'll focus on building an on-demand service like the one they already have. GM and Lyft have a partnership to work on self-driving technology, suggesting that GM may be  interested in shifting toward a more on-demand model.
We can expect these on-demand vehicles to come without steering wheels. And without the need for a driver's seat, they may wind up looking a lot different than a conventional car. They might be smaller, have rear-facing seats, and offer less storage space. With their ability to go back and recharge after a few trips, they may have smaller batteries and a shorter range — and as a result, be more affordable.
On the other hand, we can expect some car companies to focus on an incremental strategy, gradually adding self-driving capabilities to conventional vehicles.
My guess is that in the long run, the fully automated, on-demand model will come to dominate, especially in urban areas. But for a while, we're likely to have a mix of partially and fully automated vehicles, with fierce competition between them — both in the consumer market and in the halls of government.
Disclosure: My brother works at Google.
Apple posted its worst quarterly financial results in more than a decade on Tuesday. Sales of iPhones, iPads, and Macs all fell by double digits, leading to a 13 percent drop in total revenue. The markets have reacted harshly, with the company's stock losing more than 7 percent of its value in after-hours trading.
Apple is still an enormously profitable company — it pulled in more than $10 billion in net income last quarter. But the latest figures represent the end of an era in which CEO Tim Cook — and before him Steve Jobs — could seemingly do no wrong.
However, the disappointing results don't necessarily mean that Cook has made any major management blunders. The issue is simply that the iPhone has been one of the most successful consumer products in world history. It's an almost impossible act to follow.
To get a sense for the magnitude of the iPhone's success, it's helpful to look back at Apple's financial picture nine years ago. In April 2007, a few months before the iPhone first appeared in stores, the company reported that it had sold 1.5 million Macs and 10.5 million iPods between January and March 2007, generating $2.3 billion and $1.7 billion in revenue, respectively. That was considered a big success and an impressive turnaround for a company that was close to bankruptcy when Jobs took over in 1997.
Today, of course, those figures look puny. Between January and March 2016, Apple sold 51 million iPhones, generating $33 billion in revenues — more than 10 times as much revenue as the iPod generated in its heyday. In addition, Apple sold 10 million iPads and 4 million Macs, for another $10 billion in revenue.
The markets are reacting negatively because the 51 million iPhones Apple sold in the first three months of 2016 is smaller than the 61 million iPhones Apple sold during the same quarter a year earlier. But 51 million is still a massive number. For that matter, most companies would consider selling 10 million iPads to be a big hit. Apple's financial results are only disappointing compared to the very high expectations set by previous quarters' results.
A good way to tell if Wall Street is optimistic or pessimistic about a company is to look at the ratio of its stock price to its earnings. When this number is high, it's a sign that investors are optimistic about the company's growth. Google, for example, has a price/earnings ratio of 30, while Facebook's is 84. As impressive as these companies' profits have been in the past, Wall Street expects them to make even more money in the future.
Apple is in the opposite situation. Its price/earnings ratio is around 10 — on par with stodgy companies like Ford and Verizon. That's a sign that investors aren't optimistic that Apple will be able to come up with another product that's anywhere close to the success of the iPhone.
So far, that skepticism has proven justified. Apple has sold millions of iPads, but iPad sales seem to have peaked in 2013. Apple hasn't provided sales figures for the Apple Watch, but estimates suggest that it's only a modest hit so far.
Presumably, Apple is working on other products that it hopes will be big hits. Maybe the rumored Apple Car will generate iPhone-level revenues and profits. But if Apple fails to find a successor to the iPhone, that won't be a sign that they've done anything wrong. iPhone-level hits are just very, very rare.

Apple's second quarter earnings report showed a company that is still enormous and still enormously profitable, but whose sales and profits are declining across the board — revenue fell 10 percent year-on-year in the Americas, 5 percent in Europe, 26 percent in China, and 25 percent in the rest of Asia, way too much to be offset by a 24 percent increase in Japan.
What's particularly striking about this, as Benjamin Mayo of 9to5Mac noted, is that Apple simply hasn't had a down quarter in so long that proclaiming each quarter to be a "record" has become a dull routine.

A word is missing from Apple's press release title today. pic.twitter.com/fv2Niabgd0
Proclamations that this means we've reached "peak Apple" are probably premature. It's not rare for companies as a whole to have down stretches and up stretches, and the fact that Apple is falling now doesn't mean they can't rise to new heights in the future.
But this quarter's earnings do show that the endless ascent the company's been on since the introduction of the iPhone has come to an end. People still love their iPhones, but Apple's already sold enough of them that it's going to take something more than business as usual to push them to new heights.
This week, the Federal Reserve is meeting to decide whether to raise interest rates. Arguments about Fed decisions tend to get pretty deep pretty fast. What tends to get lost in the shuffle are the most fundamental and important issues: that a somewhat obscure government agency exercises enormous control over the economy by changing the price of money at regularly scheduled meetings.
Since the state of the economy ends up influencing everything from your ability to get a new job to the outcomes of presidential elections, that makes these meetings one of the most important events on the calendar. Yet they're rarely discussed outside specialist circles except by the occasional crank, leaving ordinary people in the dark.
An interest rate is the price lenders charge to borrowers to use their money. An interest rate of 5 percent means that someone borrowing $1,000 will have to make interest payments of $50 per year — in addition to eventually paying back the amount that was originally borrowed.
There are different interest rates for different types of lending — home mortgages, business loans, credit cards, and so forth.
But when economists talk about the Fed "raising interest rates," they're referring to a specific rate called the federal funds rate. That's the rate big banks charge one another for short-term loans.
The way the Fed manipulates the federal funds rate has broad economic effects
People often talk about the Fed "setting" this interest rate, but that's not quite accurate. What happens is that the Fed announces a target for the federal funds rate and then uses its ability to create or destroy money to reach its target.
Of course, these actions don't only affect the federal funds rate. When the Fed pushes the rate up or down, it tends to push other interest rates in the same direction as the federal funds rate. So ultimately, the Fed's interest rate decision will have an impact on the rates you pay the next time you borrow money — whether it's with a mortgage, an auto loan, or a credit card purchase.
By itself, the federal funds rate isn't especially important to anyone but bankers. However, when the Fed manipulates the federal funds rate, it can have broad economic effects.
Money is an essential fuel for economic activity
This is often described mechanically, as a question of the interest rates spurring or strangling economic activity. For example, if mortgage rates rise, it becomes harder for people to buy new houses, which can hurt employment in the construction industry. If interest rates for business loans go up, it becomes harder for companies to finance the construction of a new factory. And so forth.
That's all true, but it can also introduce confusion because causation can move in the other direction. When economic activity is robust there's a lot of demand for loans, which can pull interest rates up. And focusing too much on specific lending markets can obscure a more fundamental point about why the Fed's decisions matter: Money is an essential fuel for economic activity. Recessions happen when people spend less than they did before. Booms happen when people spend more. So all else being equal, putting more money into people's pockets is going to produce more demand for companies' products, more economic activity, and more jobs.
From 2008 until 2015, the Fed kept interest rates below 0.25 percent, in an effort to help the economy recover from the Great Recession. But in December 2015, the Fed announced that it would raise its target for the federal funds rate to between 0.25 and 0.5 percent. That was the first interest rate hike since 2006.
At the time, many people expected this would be the first of several interest rate hikes that would occur over the course of 2016. But with the economy continuing its slow but not spectacular growth over the last few months, the Fed chose not to raise rates further at its January or March meetings. And most Fed observers aren't expecting an interest rate hike at this week's meeting either.
If low interest rates are so good for the economy, you might be wondering why they should ever be increased. The reason is that pumping more money into the economy only works up to a certain point.
During a recession, there are a lot of idle resources. People are unemployed, factories are producing below their maximum capacity, trucks and ships sit empty a lot of the time, and so forth. In that situation — the kind of situation we had in 2001 and 2009 — getting people to spend more will mobilize idle resources and boost the real output of the economy.
The traumatic inflation of the 1970s looms large in the minds of senior Fed policymakers
But during an economic boom, things look different. With few idle resources sitting around, there's no way for more consumer spending to translate to more output. If the Fed cuts rates during a boom, the result is likely to just be that prices go up — inflation — without generating much economic growth.
That's what happened in the late 1970s. The Fed kept interest rates too low for too long because it feared that higher interest rates would be economically harmful. That produced double-digit inflation that created chaos for many Americans.
The traumatic inflation of the 1970s looms large in the minds of senior Fed policymakers, most of whom are old enough to remember it firsthand. They're determined not to repeat the mistakes of their predecessors and let inflation get out of control.
The theoretical case for raising rates to ward off inflation is strong. But the case for raising rates right now runs into a huge problem: Inflation is really low right now. It's been low since 2008, and market forecasts suggest that it will continue to be low over the next decade.
Like many countries around the world, the Fed has set an inflation goal of 2 percent. Yet over the last year, prices — as measured by the Fed's preferred inflation measure, known as the personal consumption expenditures index — have increased by just 1 percent. That's partly because oil prices have been falling; if you exclude volatile food and energy prices, the inflation rate is 1.7 percent. Moreover, markets are projecting that the average inflation rate will be below 2 percent over the next decade.
If inflation shows signs of picking up, the Fed can always raise interest rates later
And while the economy has been doing pretty well, there's reason to think it could be doing better. True, the unemployment rate is down to 5 percent, not too far from what economists regard as the full-employment level. But the labor force participation rate — the fraction of all adults participating in the labor force — is close to a 30-year low. After falling for several years, it has only ticked up slightly in the last few months, suggesting that there's still room for an economic boom to draw more people into the labor market.
The economy has been growing at a respectable but not spectacular rate, and wages have barely been growing faster than inflation.
We don't know if keeping interest rates low will boost economic growth. But given that the inflation rate is actually a bit below the Fed's target, it seems there's not much risk in giving it a try. If inflation shows signs of picking up, the Fed can always raise interest rates later.
People have made a number of arguments in favor of raising interest rates, but on some level they all boil down to the view that seven years of ultra-low rates is unnatural.
Prior to 2008, it had been many decades since the federal funds rate was zero, and a lot of people find the current interest rate environment deeply unnerving. As Vox's Matt Yglesias has written, there's a widespread view that zero percent interest rates are a kind of life-support measure for the economy. Now that the patient is recovering, people think, we should remove the breathing tube so he can get back to breathing normally.
What happens if we keep the patient on zero-percent-interest life support too long? As we've seen, people normally worry that low interest rates will generate high inflation. And in the first few years after the Fed slashed rates in 2008, a lot of people warned that inflation was just around the corner. But after seven years of low interest rates and low inflation, those fears have started looking a bit silly.
So today, advocates of higher rates mostly focus on bubbles. A good example is Sen. Rand Paul (R-KY), son of longtime Federal Reserve critic, gold standard advocate, and former Rep. Ron Paul (R-TX). The younger Paul co-authored an op-ed for the Wall Street Journal in September 2015 blaming low interest rate policies over the past 20 years for the stock market bubble of the late 1990s and the real estate bubble that popped in 2007.
In Paul's view, prolonged periods of low interest rates encourage people to make risky, unsustainable investments. Recessions, in his view, are a painful but necessary process that purges the economy of bad investments. When the Fed keeps rates "artificially" low, it merely prolongs the day of reckoning and allows these bubbles to get bigger than they otherwise would have gotten. Hence, because the Fed tried to cushion the 2000 stock market crash with low interest rates, we got an even bigger crash in 2008. Paul predicted we'll have a third crash — perhaps even bigger than the previous two — as a result of recent Fed policies.
But this argument doesn't explain how to tell whether rates are "too low." The federal funds rate was around 5 percent in the late 1990s — that was low relative to the previous couple of decades, but it was actually higher than rates for most of the 1950s and 1960s. There's widespread agreement among monetary hawks that monetary policy should be more "normal" — i.e., not zero — but little clarity about how high rates need to be to avoid bubbles or other financial calamities.
Sure thing. Listen to the classic Dire Straits song "Money for Nothing."

The song is written from the perspective of ordinary workers who envy rock stars on MTV who get "money for nothing and the chicks for free." Meanwhile, regular guys have to "install microwave ovens," do "custom kitchen deliveries," and move refrigerators and color TVs.
Obviously, monetary policy is never going to remedy this kind of inequality. Someone has to install microwave ovens and do custom kitchen deliveries, so we're never going to live in a world where everyone gets to enjoy the perks of being a rock star full time.
But there's still a lot monetary policy can do to help those guys wrangling refrigerators and color TVs. For most of the past eight years, it was hard for regular guys (and girls) to earn a living even if they were willing to do unglamorous work like installing microwave ovens. Pumping money into the economy couldn't turn those guys into rock stars, but it did generate economic activity and make it easier for them to find jobs.
And while the labor market is a lot better than it was a few years ago, there's still room for improvement. Wages for low-end workers have been stagnant for more than a decade. If we had a few years of tight labor markets — like we had in the late 1990s — ordinary workers would have more bargaining power. Many would get raises. That's why the guys who do custom kitchen deliveries might want to root for the Fed to keep interest rates low.
It's certainly true that seven years of near-zero percent interest rates was historically unusual. But whether recent Fed policies have been too tight, too easy, or just about right is open to debate.
There's a lot monetary policy can do to help those guys wrangling refrigerators and color TVs
It's helpful to think about a time when the Fed was in a very different situation. The late 1970s was a period of high interest rates. By the start of 1979, the federal funds rate had risen above 10 percent.
Yet inflation soared, reaching a high of 14.8 percent in March 1980, and it stayed above 10 percent until well into 1981. That's a sign that even the historically high rates of early 1979 weren't enough to keep inflation under control. With interest rates above 10 percent, monetary policy might have seemed tight, but it was actually too loose. As it turned out, the Fed had to let rates go as high as 19 percent in 1981 in order to get inflation under control.
Interest rates were high because the market was factoring high expected inflation into interest rates. If you lend money at 10 percent but the inflation rate is 12 percent, you're actually losing money! So the "natural" interest rate — the rate that struck the best balance between inflation and recession — was abnormally high.
Today we're in the opposite situation. Inflation expectations are low. The US population and economy are growing slowly, which limits demand for credit. And that means the natural rate of interest may be a lot lower than it was three or four decades ago.
The US isn't alone here. Interest rates are low across the developed world. Japan and the eurozone have actually adopted negative interest rates in recent months. The United Kingdom, Canada, and Australia all have interest rates at their lowest levels in decades.
And the experience of the eurozone suggests this isn't really the fault of central banks. As economist Scott Sumner has pointed out, the European Central Bank tried raising rates in 2011, believing the worst of the recession was over. The result was a double-dip recession that quickly forced the ECB to bring rates back down.
The US economy is now stronger than the eurozone was in 2011, so this week's rate hike probably won't trigger a recession. But the low rates of the past few years aren't really the doing of central banks. Central banks are just reacting to market signals — cutting rates when unemployment rises, raising them when inflation becomes a problem — and the result has been historically low interest rates.
The Fed and other central banks have been setting interest rate targets for so long that a lot of people think of monetary policy and interest rate changes as synonymous. But there's actually no law requiring the Fed to do monetary policy this way. Fundamentally, the Fed conducts monetary policy by creating money and buying stuff with it. There's no reason the amount of money they create needs to be determined by an interest rate target.
One example of this was between 2008 and 2014, when the Fed engaged in a technique called quantitative easing. The federal funds rate had already reached zero, so the Fed couldn't drive it any lower. But the Fed still wanted to do more to support an economy that was in a major recession. So the Fed just announced that it was going to create a certain amount of money every month. It worked fine, and many economists believe it helped speed the economic recovery over the last seven years.
Still, quantitative easing has two big disadvantages. One is that it's pretty ad hoc. It's hard for the Fed to know how much money to create or how long the process should go on.
The even larger problem, though, is political. Because the Fed's "normal" monetary policy approach is to target interest rates, quantitative easing generally gets labeled "unconventional" or "extraordinary" — even though the actual mechanism of creating money and buying government securities is very similar in both cases. This tends to create a political backlash and make the Fed reluctant to use QE as forcefully as might be appropriate.
The Fed twice halted its bond-buying programs — once in 2010 and again in 2012 — before bad economic news forced them to restart them. This tentative approach may have hampered the economic recovery.
A different approach would be to stop targeting interest rates and instead directly target a variable the public cares about, such as the growth of total spending in the economy. In an approach known as nominal GDP targeting, the Fed would commit to creating enough money so that total spending in the economy grows at 5 percent per year.
The Fed's interest rate decisions might seem pretty remote, but they can actually have a big impact on every American. When the Fed keeps interest rates low, it means there will be more money flowing through the economy, which is likely to mean more economic activity and more jobs.
This week's meeting, in which the Fed is widely expected to leave the rate unchanged, is likely to be a bit of a snoozer. But the Fed's broader policy approach — whether to steadily raise rates, keep them near 0.25 percent, or even push them back down toward 0 — will have a big effect on the economy.
If you'd like to see the economy grow more quickly, unemployment fall, and wages rise, then you should be rooting for the Fed to keep rates low — and maybe even reverse December's rate hike. In contrast, if you're most worried about inflation or another big economic bubble, you might want to cheer the Fed's decision to raise rates in December and hope the Fed raises rates further in the coming months.
Late yesterday, Uber announcement settlement of two lawsuits that posed a significant threat to its on-demand business model. Drivers will get more clarity about when they can be terminated, as well as a new appeals process if drivers feel they've been terminated unfairly. They'll also get some cash. In return, Uber has neutralized two big threats to its existing model for managing drivers.
This probably isn't the end of the story, though. The settlements only affect drivers in two states, and they don't actually settle the legal question at the heart of the lawsuit. So litigation in one of the other 48 states could eventually force Uber to change how it handles its relationship with drivers.
Drivers in California and Massachusetts had sued Uber arguing that they should have been legally classified as employees rather than independent contractors. If they'd won their lawsuit, they could have been entitled to a variety of legal rights, from overtime pay to reimbursement for expenses.
If the drivers had won the lawsuit, it could have forced Uber to dramatically change its business model. Under the current model, drivers are free to work when and where they want, and they get paid based on the number of rides they actually complete. That freedom could have been curtailed in a world where drivers are legally employees and are guaranteed a minimum wage and other legal benefits.
Instead, Uber has reached a settlement that features clear benefits for drivers but lets the company retain its core business model.
Labor law in California (and other states) draws a fundamental distinction between employees and independent contractors. Employees have a direct, long-lasting relationship with their employers, and as a consequence they are eligible for a number of benefits and legal protections.
Independent contractors, on the other hand, are service providers with an arms-length relationship to their customers. For example, if you hire a plumber or electrician to do work on your house, that person might be classified as an independent contractor. It wouldn't make sense for the law to treat everyone who pays for outside help on their house as an employer.
Whether a worker is an employee or an independent contractor is an issue that's decided by regulators and the courts. Employers can't opt out of the legal requirements of labor law merely by getting employees to agree to call themselves independent contractors.
Labor laws were written in an era when nothing quite like an on-demand car service existed. So Uber drivers don't fit comfortably on either side of the divide between employees and independent contractors.
There are a number of ways that Uber drivers seem a lot like conventional employees. Uber drivers must undergo a lengthy application and background checking process before they can begin work. Many drivers treat Uber as a full-time job, working 40 or more hours per week for months at a time.
Uber sets a wide variety of rules governing how drivers should provide services to Uber passengers. Drivers' fees are fixed by Uber, and Uber sometimes provides drivers with bonuses, minimum earning guarantees, and other forms of compensation that don't come directly from customers.
But Uber pointed to important ways that drivers seemed more like independent contractors. Normally, employers set their employees' schedules, requiring them to work a minimum number of hours and often dictating when and where to report for work. By contrast, Uber drivers set their own schedules. They can work as much or as little as they want. They also have total freedom to decide where to work.
Employees generally use equipment supplied by their employers, while independent contractors usually supply their own tools. Uber has cited the fact that drivers supplied their own vehicles as evidence that they are independent contractors. Uber argued that it merely provided a kind of online marketplace — like an eBay for transportation services — connecting independent ride providers with customers who wanted to purchase their services.
Some of Uber's critics — especially those connected to the labor movement — believe Uber is making things unnecessarily complicated. In their view, Uber is a rich company that could afford to provide workers with the benefits of conventional employment if they wanted to.
But it's not clear shifting to an employee-employer model would be good either for Uber or for its drivers. The key problem here is about scheduling.
In a normal employment relationship, the employer exercises control over the employee's schedule. If the employee works long or inconvenient hours, it's often because the employer requested it. Labor law's scheduling rules are built on this assumption. Workers must receive time-and-a-half compensation if they work more than 40 hours in a week. Some states, including California and Massachusetts, also mandate that workers be provided with breaks and meal times.
But these rules don't make much sense on a platform like Uber where work hours are entirely under the control of the driver. Drivers can take breaks whenever they want, and they're never forced to work more than 40 hours per week. If Uber were subject to a time-and-a-half rule, it might simply prohibit drivers from working more than 40 hours per week, which would be bad for drivers.
A similar point applies to the minimum wage. If Uber drivers were classified as employees in California, they'd be entitled to make at least $10 per hour — and $15 per hour by 2022.
More money is obviously nice for workers, but the tricky thing here is that it would force Uber to exert more control over where and when workers put in their hours. Right now, drivers have a natural incentive to work at times and places where demand is high — like near bars on Friday and Saturday nights. If drivers were guaranteed $10 per hour, they'd no longer have to worry about that so much. They'd be more inclined to work at times and in places that are convenient for the drivers, even if those are not times when many customers needed rides.
So a minimum wage would effectively force Uber to start telling drivers when and where they can work. That would mean drivers could lose one of the best things about being an Uber driver: the freedom to set their own hours and decide which parts of town to drive in.
By itself, settling with Uber drivers in California and Massachusetts doesn't accomplish that much for Uber. The company operates in dozens of other states, and drivers there are free to sue. The key to the settlement is the compromises Uber is making to try to address drivers' concerns and make them less interested in seeking the status of employees.
The key demand focuses on transparency and due process for worker terminations. Uber has finally published a formal policy on driver deactivation, spelling out the circumstances in which a driver would be terminated from Uber. These include drug and alcohol use, persistently low ratings, and a high rate of ride cancellations.
And while Uber drivers will not be represented by a union or subject to collective bargaining, Uber has agreed to create a driver representative organization — in effect a trade association rather than a union — and give drivers a process for appealing termination decisions.
It's not obvious that either of these changes strengthens Uber's argument that drivers are not employees in a narrow legal sense. Nevertheless, Uber's willingness to work with its drivers may make a favorable impression on judges who oversee similar lawsuits in other states. If Uber can convince judges (and legislators) that it is addressing key driver grievances under the existing legal framework, they might be less likely to force Uber to treat drivers as employees instead.
Are fully functional self-driving cars right around the corner? After years of optimism, we're starting to see people wonder if they might be further away than people thought. And skeptics point to mapping as a key obstacle.
Writing at the Marginal Revolution blog, for example, economist Tyler Cowen argues that "mapping the territory, reliably, will remain the key problem. Until that is solved, driverless cars will be a form of mass transit — except without the mass — along predesignated routes."
Creating detailed and comprehensive maps is difficult in the sense that it takes a lot of work, but it's not a hard technical problem. Google has already done it for roads around their corporate headquarters in Mountain View and some of its competitors likely already have the same capabilities. Expanding these maps nationally doesn't require a conceptual breakthrough, it just takes money — and Google has a lot of money.
We can expect Google and its rivals to start their mapping projects in major cities where the bulk of Americans live. That means that most of us are likely to get access to door-to-door self-driving car service long before the last mile of rural American roads has been mapped.
Computers have been able to beat human beings at chess since 1997, when the IBM's Deep Blue beat human champ Garry Kasparov. But in his 2013 book Average Is Over, economist Tyler Cowen pointed out that (at least at the time he was writing) mixed teams of humans and computers — known as freestyle chess teams — were even better at chess than computer software alone. Humans provided valuable strategic insights to complement the massive computing power of the machines.
Google's self-driving car technology works on the same principle. A computer inside the car is responsible for making second-by-second driving decisions. But the car is in constant contact with Google headquarters, where a large team of human analysts — backed up by the vast computing power of Google's data centers — maintains an extremely accurate, detailed, and up-to-date map of the streets where Google's cars are driving.
Google described its effort to build this map in its most recent monthly update on the self-driving car program.
"Before we drive in a new city or new part of town, we build a detailed picture of what’s around us using the sensors on our self-driving car," Google writes. "Our mapping team then turns this into useful information for our cars by categorizing interesting features on the road, such as driveways, fire hydrants, and intersections."
Google's self-driving cars are able to navigate city streets pretty well even without this kind of detailed map. But when people's lives are at stake, "pretty well" isn't good enough.
The human-annotated map provides an extra margin of safety, allowing a car to know its location within about 4 inches. And identifying permanent, immovable road features ahead of time, the map allows a car's onboard software to quickly focus in on objects that aren't labeled in the map. These new objects tend to be people, animals, or vehicles that are likely to move, requiring the car to be extra cautious.
There's a lot that software can do to speed up the process of identifying objects like street signs and fire hydrants, but Google still employs human analysts to do much of this work. When a single mistake could lead to an accident, it's better to be safe than sorry.
Right now, Google only has this kind of detailed maps for a small fraction of the country's roads — primarily in the area around Silicon Valley and Austin, Texas. Taking these maps national will be expensive. Only Google knows exactly how expensive, but we can make some educated guesses by looking at how much online mapping companies are spending to maintain their maps today.
A rough 2012 estimate found that maintaining the data for a global mapping service costs $1 billion to $2 billion per year, a figure that's in line with industry rumors.
We should expect the maps used for self-driving cars to be even more expensive because they're going to have to be a lot more detailed. The maps that power Google's self-driving cars are going to have to mark a lot of features — fire hydrants, driveways, street signs, and bushes — that aren't relevant for merely providing turn-by-turn directions.
Ironically, then, the effort to automate driving may actually create a lot of jobs, especially in the early years as self-driving technology is being rolled out. As Google and its competitors expand their self-driving vehicle programs nationwide, they're going to have to hire thousands of human analysts to produce the detailed maps that enable cars to drive safely.
And this won't just be a one-off development, either. Landscapes are changing constantly, with changing speed limits, new construction, and trees growing and being cut down. So while maintaining maps may require less manpower than creating them initially, self-driving car technology is likely to employ a lot of people for the foreseeable future.
If a company had to build a nationwide map before it could bring its self-driving technology to market, that could be a major obstacle. But companies don't have to do that.
The alternative is to introduce the vehicles as an on-demand service rather than a product customers can buy. For example, Google might just map the city of San Francisco and then offer a self-driving car service that competes with taxis, Uber, and Lyft within the city limits. As the service grows in popularity, Google could expand its service territory, first to other parts of the San Francisco Bay Area, then to other major metropolitan areas.
It might take many years before Google manages to offer services in outlying rural areas. But as Uber has demonstrated, there's a ton of demand for on-demand rides restricted to major metropolitan areas. And without the need to pay a human driver, we can expect Google's self-driving cars to be dramatically cheaper than Uber's, which will mean even greater demand for the services.
Google seems to believe that detailed maps are an essential resource for allowing cars to drive themselves safely. Of course, it's possible that some of the other companies working on self-driving car technology — Uber, Tesla, Apple, and several major car companies are all rumored to be working to develop self-driving technology of their own — will find ways to build fully self-driving cars that aren't reliant on maps. But it's also possible that maps will be an essential resource for self-driving systems for the foreseeable future.
If that happens, it will provide a big strategic advantage for companies that have experience managing map data. That includes Google, of course. It also includes Apple, which has a mapping app for the iPhone. It may include BMW, Daimler and Audi, which jointly paid $3 billion for Nokia's mapping division last year. And it may also include Uber, which has been buying up mapping assets.
The expense of managing maps may also be a major reason why GM teamed up with Lyft earlier this year. Right now, GM's business model is to sell cars to people who expect to be able to drive them anywhere they want nationwide. But if self-driving cars need detailed maps of everywhere they go, then this business model would force GM to build a detailed nationwide map before selling its first self-driving car, an extremely daunting prospect.
Instead, the Lyft partnership gives GM the opportunity to build self-driving cars that, like Lyft's service, only operate in major metropolitan areas. Collecting the mapping data required to operate in these limited areas is a much more manageable problem.
Still, the importance of maps to the self-driving market is another reason that car companies may struggle to remain market leaders as the industry shifts to fully autonomous technologies. Google, Apple, and Uber have a lot of experience collecting, analyzing, and distributing vast quantities of fast-changing geographic data. Ford, GM, and Toyota don't.
This also may explain why car companies have been focusing on developing partially self-driving technologies like adaptive cruise control, emergency braking, and self-parking. These relatively simple self-driving capabilities don't rely on maps, and they're compatible with car companies' existing business models. Car companies hope that these will provide customers with enough of the benefits of self-driving technology to provide a competitive alternative to fully self-driving products from Google and others. But in the long run, this approach seems unlikely to work that well, as the benefits of fully self-driving cars will be massive.
Disclosure: My brother is an executive at Google.
"The most important article you write on your blog is the second article someone reads ... If that next article is also really good, then you've established something meaningful with that reader. You have the inkling of a relationship." — Ben Thompson
Since launching Stratechery in 2013 from his home in Taiwan, Ben Thompson has established himself as one of the smartest and most thoughtful analysts at the intersection of media, business, and technology.
Just as impressive: Thompson has also figured out a way to turn Stratechery into a viable business; his one-man operation operation were earning more than $200,000 in annual revenue by January 2015. Thompson is coy about the number now, but it's safe to assume it's considerably higher.
So getting to geek out with Thompson on the state of the media business is a lot of fun. In the latest edition of my podcast, (which you can listen to by subscribing to my podcast or streaming it on SoundCloud), we talked about a number of topics close to my heart, including:
And much, much more.
This is a particularly great episode if you're in new media, or thinking of getting into it, or just want to understand the business dynamics behind the content you consume. There's a lot of advice here that I wish I had known when I started as a journalist.
A big thanks again to Thompson for taking so much time for this conversation. And for more podcast conversations — including episodes with Rachel Maddow, Bill Gates, political scientist Theda Skocpol, and conservative activist Michael Needham — subscribe to The Ezra Klein Show. For write-ups of past episodes, head here.
The news that Harriet Tubman will be replacing Andrew Jackson on the front of the $20 bill is significant for all sorts of reasons. Slave owner Jackson is being pushed to the back of the bill by a former slave; Tubman, who led more than 300 slaves to freedom on the Underground Railroad, is displacing a president who drove 16,000 Cherokees (and thousands more from other native tribes) out of their homelands on the Trail of Tears.
But even if Tubman weren't displacing Jackson, the $20 would be the perfect bill to honor her, because the sum of $20 played a significant role in her life on two separate occasions.
For one thing, $20 was the amount she earned as a monthly pension after the Civil War, for which she helped the Union as a scout and spy. It was still less than the $25 a month paid to full soldiers, but it was the result of a long legal fight to earn a soldier's pension at all. (Vox's Phil Edwards wrote about this last year, when the social media campaign to put Tubman or another woman on the $20 was at its height.)
But even before that — as Yoni Appelbaum of the Atlantic pointed out on Twitter — the sum of $20 played a huge role in Tubman's efforts to rescue her own father from slavery.
In Tubman's first biography, the 1869 book Scenes in the Life of Harriet Tubman, author Sarah Hopkins Bradford told the story of Tubman's efforts to save her parents as an example of just how rare it was for Tubman to ask for anything from others. "But though so timid for herself," Bradford wrote, "she is bold enough when the wants of her race are concerned" — and unafraid to embarrass powerful people, if necessary.
In this case, Bradford writes, Tubman believed she'd gotten "directed" by God to ask for funds to rescue her parents from "a certain gentleman in New York," whom Appelbaum identifies as prominent abolitionist Oliver Johnson:
When she left the house of her friends to go there, she said, "I'm gwine to Mr.--'s office, an' I ain't gwine to lebe there, an' I ain't gwine to eat or drink till I git enough money to take me down after the ole people."
She went into this gentleman's office.
"What do you want, Harriet?" was the first greeting.
"I want some money, sir."
"You do? How much do you want?"
"I want twenty dollars, sir."
"Twenty dollars? Who told you to come here for twenty dollars?"
"De Lord tole me, sir."
"Well, I guess the Lord's mistaken this time."
"I guess he isn't, sir. Anyhow I'm gwine to sit here till I git it."
So she sat down and went to sleep. All the morning and all the afternoon she sat there still, sleeping and rousing up--sometimes finding the office full of gentlemen--sometimes finding herself alone. Many fugitives were passing through Now York at that time, and those who came in supposed that she was one of them, tired out and resting. Sometimes she would be roused up with the words, "Come, Harriet, you had better go. There's no money for you here." "No, sir. I'm not gwine till I git my twenty dollars."
Ultimately, Tubman got her twenty dollars — and then some. Bradford writes that Tubman eventually fell asleep in the office, and woke up to find $60 in her pocket. But they hadn't come from Johnson; they'd come from the other "fugitive" ex-slaves passing through the office, who managed to raise a tremendous amount of money to help Tubman bring one more to their ranks.
Tubman used the money to rescue her father — who was on trial for helping slaves escape — and bring him all the way up to Canada, where he couldn't be recaptured into slavery.
Admittedly, $20 doesn't go as far as it used to. But once Tubman's face is being minted onto new $20 bills, she'll be part of every exchange in the amount of cash even a prominent abolitionist wouldn't give her to save her own father. And to people who know that story, it might even serve as a reminder of how much more valuable $20 is to those who have less.
June 6, 2005, seemed to be a triumphant moment for Intel. The chipmaker was already dominating the market for processors that powered Windows-based PCs. Then Steve Jobs took the stage at Apple's World Wide Developers Conference to announce that he was switching the main Windows alternative, Macintosh computers, to Intel chips as well. The announcement cemented Intel's status as the leading company of the PC era.
There was just one problem: The PC era was about to end. Apple was already working on the iPhone, which would usher in the modern smartphone era. Intel turned down an opportunity to provide the processor for the iPhone, believing that Apple was unlikely to sell enough of them to justify the development costs.
Oops.
On Tuesday, Intel announced that it was laying off 12,000 employees, 11 percent of its workforce, the latest sign of the company's struggle to adapt to the post-PC world. Intel still isn't a significant player in the mobile market — iPhones, iPads, and Android-based phones and tablets mostly use chips based on a competing standard called ARM.
The company is still making solid profits — it just announced a $2 billion profit for the first quarter of 2016. But the company's growth has stalled, and Wall Street is getting worried about its future.
Obviously, Intel made a mistake by missing out on the iPhone business. Intel's error in judgment is a classic example of what business guru Clay Christensen calls "disruptive innovation." The term disruption has become so overused in the technology world that it's sometimes treated as a joke. But Christensen gave it a more precise meaning that fits Intel's situation perfectly: a cheap, simple, and less profitable technology that gradually erodes the market for a more established technology.
Intel is just the latest in long line of companies that have failed to effectively deal with  this kind of disruptive threat.
Intel invented a chip standard called x86 that was chosen for the IBM PC in 1981 and became the standard for Windows-based PCs generally. As the PC market soared in the 1980s and 1990s, Intel grew with it.
The key to success in the PC business was performance. Chips with more computing power could run more complex applications, complete tasks more quickly, and run more applications at the same time. During the 1990s, Intel and its rivals raced to increase their chips' megahertz ratings — a measure of how many steps the chips could perform in a second.
One thing these early chipmakers didn't care about was power consumption. Higher-performance chips often consumed more energy, but this didn't matter because most PCs were desktop models plugged into the wall. Even laptops had large batteries and could be plugged in most of the time.
But this became a problem in the late 2000s, when the market began to shift to smartphones and tablets. These devices had smaller batteries (to keep the weight down), and users wanted to use them all day on a single charge. Existing x86 chips were a poor fit for these new applications.
Instead, these companies turned to a standard called ARM. Created by a once-obscure British company, it was designed from the ground up for low-power mobile uses. In the mid-2000s, ARM chips weren't nearly as powerful as high-end chips from Intel, but they consumed a lot less power, which was important for smartphones from Apple and BlackBerry.
Even better, the ARM architecture is designed for customization. ARM licenses its design to other companies such as Qualcomm and Samsung, which make the actual chips. That provides flexibility that allows smartphone makers to combine a number of different functions on a single chip. And packing a bunch of functions — like data storage and image processing — onto one chip helps to keep power consumption down.
Today, ARM chips totally dominate the mobile device business. iPhones and iPads run on a chip called the A9 (and predecessors such as the A8 and A7) that are based on the ARM platform, designed by Apple, and manufactured by chipmakers like Samsung and TSMC. Most Android-based phones run on ARM-based chips from Samsung, Qualcomm, and other ARM chipmakers.
Intel had not just one but two opportunities to become a major player in the mobile chip market. One was the opportunity to bid on Apple's iPhone business. The other was its ownership of XScale, an ARM-based chipmaker Intel owned until it sold it for $600 million in 2006.
Intel sold XScale because it wanted to double down on the x86 architecture that had made it so successful. Intel was working on a low-power version of x86 chips called Atom, and it believed that selling ARM chips would signal a lack of commitment to the Atom platform.
But Atom chips didn't gain much traction. Intel has made a lot of progress improving the power efficiency of its Atom chips. But ARM-based chipmakers are experts at building low-power chips, having focused on that task for more than a decade. So they had the early advantage. And at this point, ARM has a huge share of the market. That gives them all of the advantages — more engineers, better software — that come with being a dominant platform.
On one level, you can say that Intel just got unlucky and backed the wrong horse. The chipmaker could have tried harder to win Apple's iPhone contract, and it could have bet on its XScale ARM subsidiary instead of trying to create Atom processors. But it chose not to.
But on a deeper level it's not surprising that Intel took the path it did, again because of Christensen's theory of disruptive innovation.
Intel's basic problem was that the mobile chip market didn't seem profitable enough to be worth the trouble. Intel had built a sophisticated business around the PC chip. Its employees were experts at building, selling, distributing, and supporting PC chips. This was a lucrative business — often Intel could charge several hundred dollars for its high-end chips — and the company was organized around the assumption that each chip sale would generate significant revenue and profits.
Mobile chips were different. In some cases, an entire mobile device could cost less than the price of a high-end Intel processor. With many companies selling ARM chips, prices were low and profit margins were slim. It would have been a struggle for Intel to slim down enough to turn a profit in this market.
And in any event, Intel was making plenty of money selling high-end PC chips. There didn't seem to be much reason to fight for a market where the opportunity just didn't seem that big.
What this analysis missed, of course, was that the mobile market would eventually become vastly larger than the PC market. ARM-based chipmakers might make a much smaller profit per chip, but the market was destined to grow to many billions of chips per year. Even a small profit per chip multiplied by billions of chips could add up to a big opportunity.
Meanwhile, Intel had to worry that jumping wholeheartedly into low-power mobile chips would undermine demand for its more lucrative desktop chips. What if companies started buying Intel's cheap mobile chips and putting them in laptops? That could hurt Intel's bottom line more than the added mobile revenue would help it.
Obviously, Intel's leadership now recognizes that they made a mistake. They're now so far behind that it's going to be a struggle to gain a foothold in the new market. And as cheap mobile chips get more and more powerful, we can expect more and more companies to put them into low-end laptop and desktop computers, eroding demand for Intel's more expensive and power-hungry chips.
Ironically, Intel is now suffering the same fate that it inflicted on an earlier generation of computing innovators three decades ago. In the 1980s, there was a thriving community of "minicomputer" makers led by a company called the Digital Equipment Corporation.
These washing machine–size minicomputers were only "mini" compared to the room-size mainframe computers that preceded them, and they cost tens of thousands of dollars.
Early PCs based on Intel chips were referred to as microcomputers, and companies like DEC dismissed them as toys. They did this for exactly the same reasons Intel dismissed the mobile market — selling a $2,000 PC was a lot less profitable than selling a $50,000 minicomputer, and DEC didn't expect PCs to be a big enough market to be worth the effort.
Of course, that turned out to be totally wrong. The PC market turned out to be vastly larger than the minicomputer market, just as the mobile market is now much larger than the PC market. But by the time this became clear, it was too late. DEC and most of its peers were forced out of business by the end of the 1990s.
When we think about productivity in an economy, we are normally referring to the output of human workers. But some segments of the economy also feature a nonhuman labor force.
Dairy cows, for example, are a key part of the milk industry. And according to this striking chart from the US Department of Agriculture, their productivity has surged over the past generation:
Cow wages, presumably, have not increased commensurately during this period.
These days, if you want to get full-featured cable or satellite television, you typically need to pay not just for the television service itself but for a rented set-top box from your television provider. This is an easy and lucrative stream of revenue for cable and telecom companies, with consumers paying an average of $230 a year in rental fees for relatively unsophisticated boxes.

The Obama administration wants to change that, and will at 9 am release a formal request that the Federal Communications Commission require pay television providers to open up that market to competition. They're not asking for any particular technical solution, but they want an enforceable guarantee that there will be some way for third parties to make and sell cable boxes.
They anticipate that this will save consumers money. But more importantly, if it happens it will turn a stagnant element of the electronics landscape into an innovative one. Future iterations of the Xbox, Playstation, Roku, Apple TV, or other integrated media streamers could serve as complete substitutes for cable boxes. Or maybe ultra-cheap low-feature boxes will emerge to serve customers who really just want a traditional linear cable experience.
At the same time, President Obama will unveil an executive order that tries to ensure more regulatory actions in this spirit — giving every executive agency a 60-day deadline to do a top-to-bottom review of the areas it supervises and report back on what it can do to break down barriers to competition in the American economy.
Set-top boxes are one micro-scale example of a problem that has increasingly become a macro-scale concern for Obama's economic team: evidence that the American economy has become less competitive and more ridden with monopoly market power.
The big-picture concern is that corporate profits have risen to an unusually high level as a share of total national income, and then stayed high. Profits are, of course, an integral part of a capitalist economy. But in a healthy capitalist economy the idea is that high profits inspire businesses to invest more in order to capture a share of the profits for themselves. That investment creates jobs and innovation, and leads to competition that whittles away the profits.
In recent years, we've seen the profits but not the investment.
Some analysts blame activist investors on Wall Street for the investment drought but a dearth of competition is another plausible suspect. There's evidence that fewer new companies are being founded and many industries are becoming more concentrated — both signs of declining concentration.
When the conversation turns to competition policy, it's natural to think of anti-trust enforcement as practiced by the Department of Justice and the Federal Trade Commission.
The White House is interested in that, but it also thinks that reviews of merger and acquisition activity only get you so far in terms of genuinely driving competition.
FCC activism around set-top boxes is a case in point. Breaking Verizon or Comcast into multiple smaller companies wouldn't do anything to change the fact that most consumers only have access to one or two pay-television providers, all of whom require set-top box leasing. Introducing competition into that market requires direct regulatory intervention — just as the administration earlier acted to mandate that wireless carriers allow consumers to "unlock" their smartphones.
Air travel is another example where the Transportation Department may have to look not just at the scale of airlines as a whole, but the allocation of slots at particular airports.
But there are also cases where too much regulation may be standing in the way of competition. Obama's economic team has previously taken aim at occupational licensing rules that set up de facto cartels in the provision of certain kinds of services. There's also increasing concern that even when several companies operate in the same industry they may have so many common shareholders that they are subtly pressured to avoid competing.
The internet also appears to have spawned several opportunities for companies to obtain market power via network effects — everyone uses Facebook in part because everyone else uses Facebook, making them very hard to dislodge — with implications that policymakers have not yet fully understood.
Set-top boxes are a small thing, and completely reinvigorating competition in the American economy is probably too big a thing to be achieved with an executive order.
But like the administration's springtime crackdown on the banking industry, the competition initiative is a sign of a White House that is fiercely resisting the lame-duck label and trying to put its stamp on national policy. The order says, in effect, that the president wants his appointees to hurry up and see what they can still get done before they have to leave office.
Over at the Upshot, Margot Sanger-Katz and Reed Abelson have an interesting discussion of the difficulties some insurers are having adjusting to Obamacare's insurance exchanges.
On Friday, UnitedHealth announced its intentions to pull out of Georgia and Arkansas, and the week before the Blue Cross and Blue Shield Association released a research paper arguing that new enrollees had been sicker — and thus less profitable — than expected. We're entering the season when insurers submit next year's pricing proposals to regulators, and pretty much everyone expects substantial rate hikes.
And that's where things get interesting — or, depending on your point of view, troubling. An early question with the Obamacare exchanges was whether enrollees would respond to rate hikes by shopping around each year to get a better price. The fear was that having chosen a plan once, they would stick with it, even if premiums rose sharply, and so insurers wouldn't be forced to cut costs. For competition to revolutionize the insurance market, enrollees have to actually force insurers to compete for their business.
Happily, that's exactly what enrollees did. Only a third of Obamacare enrollees in 2016 had been on the same plan in 2015. About 25 percent of exchange users switched their plans in 2016. This is precisely what health wonks hoped would happen. In response, insurers have fought to keep costs down, narrowing networks and hiking deductibles.
The result is that Obamacare is proving much cheaper than the Congressional Budget Office originally expected. But many of the participating insurers aren't much enjoying the experience — some priced their plans too low and are losing money, and others simply don't see how to make enough money to justify the effort of participating in these state markets.
"I’m not sure I know what the business model is for an insurer, if the expectation is that you’re going to keep your customers for only a year," writes Abelson. "It makes achieving long-term goals like keeping people healthier and focusing on preventive measures much harder because there may be no payoff for the insurer."
Sanger-Katz agrees, and draws out the underlying tension a bit more. "The fact that people are actually switching seems like a sign that this market is functioning as it was designed. But … all that churn sure makes it hard for an insurer to make money by investing in its customers' long-term health."
I'm a bit skeptical that an inability to invest in customers' long-term health is really the problem insurers are facing. For all their rhetoric to the contrary, I haven't seen much evidence that insurers — particularly individual-market insurers — are any good at improving the health of enrollees. In most cases, I don't think they even try particularly hard to affect enrollee health beyond paying for medical claims.
But there's no doubt that in an idealized insurance system, insurers would try to invest in the long-term health of their customers. The problem is that it doesn't make much sense for them to make those investments if they're just going to lose their enrollees to a competitor a few years later. This is one of the underdiscussed downsides of the push for competitive insurance markets: The easier you make it for enrollees to switch insurers, the harder you make it for insurers to invest in the future health of their enrollees.
And perhaps that's fine. Maybe the right model for insurers is something like Southwest Airlines: low prices, narrow networks, exceptional customer service, and not much else. I lean toward that view myself, in part because I don't think insurers have the tools or the trust to effectively change behaviors among their enrollees. Probably the most effective ways for insurers to invest in enrollee health would be to lobby state and local governments to pass smoking bans or tax alcohol.
But if you think the people who pay for hospital stays should also be responsible for keeping the number of hospital stays to a bare minimum, competitive insurance markets probably won't get you there. The model that makes sense for that purpose is something more like single-payer, where the insurer knows that if it makes costly investments in enrollee health now, it will be the one to reap the benefits later.
For more discussion of Obamacare — including some big wins the law has recently scored in the Medicaid and employer markets — listen to the April 8 episode of Vox's policy podcast The Weeds, which you can subscribe to on iTunes, stream on SoundCloud, or download wherever fine podcasts are, well, downloaded.
Tesla is a very exciting car company, and lots of people like to use the word "disruptive" when talking about it, so it's natural that headlines proclaiming Tesla to be disrupting something or other are rampant on the internet.
But describing Tesla in this way stretches the concept of "disruption" beyond the point of usefulness and misunderstands the nature of the company's success. The real message of Tesla is that not everything needs to be disruptive to be innovative and successful. Sometimes making something great really is good enough.
Here are a few reactions to the release of Tesla's Model 3:
And, of course, it's true that Tesla's cars are well-reviewed and popular, and if the company can really deliver the Model 3 on the scale that it's promising, that will be a significant challenge over the medium term for other car companies.
But "disruption" theory, introduced by Harvard Business School's Clay Christensen, actually refers to something more specific than the generic idea of new competition. His coinage is a useful one and worth preserving — while also recognizing that it simply doesn't apply to every successful new business.
The core idea of disruptive innovation is that successful companies tend to become obsessed with getting better and better at serving their existing high-margin customers.
Those customers provide the profits, so they get studied closely and provided with more and more services and features to ensure their continued loyalty. Eventually a new competing product comes into the marketplace that is generally cheaper, simpler, and, in an abstract sense, inferior. This is the disruptor. What makes it disruptive is that even though the incumbent company could, technically, copy the disruptive new product, it can't bring itself to actually do so, because that would involve undercutting its existing profit margins.
Instead, the tendency is for the incumbent to reassure itself, accurately, that the new product simply does not meet the needs of the most valuable customers and to continue focusing on them.
The problem for incumbents is that over time the new disruptive product tends to get better and better, eating away at a bigger and bigger share of the incumbent's market share. At the same time, because the disruptive new product was designed from the beginning to be cheaper, simpler, and lower-margin it manages to enlarge the market far beyond its previous size.
In this sense, smartphone cameras have disrupted the traditional camera industry. Even a very good smartphone camera takes photos that are lower quality than what you could get with a dedicated camera. But these days, the marginal price of owning a phone that's also a camera as opposed to just a phone is essentially $0. That has devastated the market for dedicated cameras, even as it's created a world in which people buy more cameras (called "phones") and take more photos than ever before.
Tesla's products do not have any of these hallmarks.
Even the new Model 3 — the "affordable" Tesla — is very expensive. It's affordable in the sense that unlike the Model S or the Model X, a middle-class family could decide to buy one if they wanted to stretch their budget. But it's unquestionably an indulgence, and not an affordable alternative to anything.
More importantly, in terms of sequencing Tesla followed the exact opposite strategy.
It first introduced the Roadster, a high-end electric sports car that was both expensive and impractical for most generic automotive applications. It was a car aimed at an extremely narrow target audience of rich weirdos. But it served the purpose of demonstrating that Tesla could, in fact, make an all-electric car that was safe and fun to drive.
Next Tesla made the Model S, a very expensive luxury sedan aimed at the wider audience of rich people who enjoy fancy cars. This and its successor, the larger and more expensive quasi-SUV Model X, served to establish the idea that Tesla was a luxury brand and a maker of cool, high-quality cars.
Now the company is moving further down market with the more accessible Model 3. It's a car that benefits from the technical innovations that went into the earlier cars, but that also benefits from the brand halo effect. As Stratechery's Ben Thompson writes, "It's a Tesla," which everyone now knows is a good thing to be.
A disruptive approach to the electric vehicle market would have involved a vehicle that looked more like a Smart car: cheap, small, and with a limited range that limited its appeal to conventional car buyers but that was extremely reliable due to the extreme simplicity of an all-electric drivetrain.
Such a car could, along the lines of classical disruption, appeal primarily to buyers who aren't well-served by the existing auto manufacturers. Smart cars might have sold primarily as fleet vehicles for new on-demand, app-based ride-hailing services that rarely need to take passengers on long trips. Disruption would have emerged as the batteries and range improved and, more importantly, as automation eventually made on-demand fleets into a larger and larger share of the overall transportation marketplace.
A Tesla, by contrast, essentially competes head to head with a BMW or an Audi as a product.
It's true that the underlying technology is different, but the basic value proposition is the same — it's intended to be an awesome car by the standards of the people who buy expensive cars today, and to appeal to them for similar reasons.
Thompson's view is that Tesla's success — and the larger-scale success of Apple's high-end gadgets — is that disruption theory doesn't apply to consumer markets. That's an overstatement. Companies like Ikea, Walmart, Aldi, and Warby Parker have all applied disruption-style ideas to consumer markets.
But Thompson is right to say that at least some segments of some consumer markets persistently resist disruption.
That's because for many classes of consumer goods, products never become truly "good enough." Nobody needs a Lexus to get to work and run errands. For that matter, nobody needs a brand new Toyota when a used one would be cheaper. And nobody needs features like power steering, anti-lock breaks, Bluetooth connectivity for smartphones, or dozens of other features that were once high-end options and have now become standard.
It just happens to be the case that most people spend a lot of time in their car, and consequently, to the extent that they can afford to do so they always prefer a nicer car to a less-nice one.
Value still counts in this kind of consumer market, but it's not the all-consuming priority in the way it is for some other markets. And it means that entering on the high end to build your capabilities and your brand makes at least as much sense as starting at the bottom.
Over the next decade, Detroit's big car companies are likely to face existential threats as Silicon Valley companies like Google, Tesla, Apple, and Uber invade the auto market. Ford CEO Mark Fields plans to meet this threat with a preemptive strike — but even his bold idea probably isn't bold enough to meet the scale of the challenge, a problem that underscores how genuinely hard it is for business leaders to deal with the threat of massive technological change.
"Our approach is to first disrupt ourselves," Fields said in a recent interview with The Verge (which, like Vox.com, is owned by Vox Media). His plan is to create a new subsidiary called Ford Smart Mobility LLC. Based in Tesla's hometown of Palo Alto, the new company will house Ford's work on ride-sharing and self-driving cars.
By creating an independent subsidiary to deal with a disruptive threat, Fields is taking a page from The Innovator's Dilemma, the 1997 Clayton Christensen book that introduced the concept of disruptive innovation. Locating the new organization in Silicon Valley will give it some insulation from the bureaucratic culture of its corporate parent and an opportunity to absorb the culture of the technology sector.
Unfortunately, Fields does not seem prepared to give the new subsidiary the degree of independence it will likely need to succeed. When The Verge's Chris Ziegler asked if Ford Smart Mobility could work with other automakers, Fields said no, arguing that "we want it to be dedicated to Ford."
"It’s not moving from an old business to a new business, just a bigger business," Fields said. "They’re interconnected."
Unfortunately for Ford, the reason incumbents struggle to adapt to disruptive innovations isn't that they're bureaucratic or dumb — it's that establish companies rarely have the stomach to introduce new products that undercut the market of their existing ones. The more interconnected a subsidiary is to its corporate parent, the more vulnerable it is to this problem and the less likely it is to be truly disruptive.
Over the next decade, we're likely to see three major automotive innovations that could threaten established auto companies like Ford:
These three trends reinforce one another. Automation will make on-demand vehicles more affordable, dramatically expanding their use and causing more people to opt out of car ownership. If more riding shifts to on demand, there will be a bigger market for electric vehicles designed specifically for on-demand use in urban areas. These may be significantly lighter, smaller, and therefore cheaper than conventional cars. Self-driving cars will be able to automatically drive themselves to a charging station when they run low on power, so they won't need the heavy, expensive batteries that make electric cars uneconomical for most consumers today.
Put all these trends together, and the result is likely to be cars that look dramatically different from the cars Ford and its competitors are selling today.
And the problem, as Christensen explained, is that it's extremely hard for a mature organization like Ford to adapt to this kind of change. The problem isn't just that individual employees would have to learn new skills. It's that successful companies have cultures focused on serving their existing customers. And by definition, Ford's customers mostly want to buy Ford's existing gasoline-powered, customer-owned, non-self-driving cars. As an institution, Ford isn't well positioned to produce the kind of cars that are likely to succeed in a self-driving, ride-sharing future.
So Fields has exactly the right instinct: By creating a subsidiary in Silicon Valley, he starts building a team that is steeped in the culture of the technology sector and unencumbered by a need to serve Ford's existing customer base.
But the project will only succeed if Fields is personally committed to giving the company the autonomy to be truly disruptive. "In our studies of this challenge, we have never seen a company succeed in addressing a change that disrupts its mainstream values absent the personal, attentive oversight of the CEO," Christensen wrote in The Innovator's Dilemma.
In particular, there's a good chance that the next generation of automotive technologies will eventually cannibalize the market for conventional, customer-owned, gasoline-powered cars. If Ford is serious about having its new subsidiary succeed, it needs to be prepared for it to participate in that process — creating products that undermine the profitability of its parent company.
But saying things like, "It’s not moving from an old business to a new business, just a bigger business," suggests that Fields doesn't fully grasp what it could mean for Ford to "disrupt ourselves." When a disruptive technology enters the market, the result is often that most of the established companies go bankrupt.
If Ford's plan succeeds and Ford Smart Mobility really does start to disrupt its parent company, Fields is going to face a lot of pressure from within Ford's Detroit headquarters to rein it in. For the plan to work, Fields needs to not only resist this pressure, he needs to give the subsidiary's leaders confidence that he'll continue doing so no matter how strong it gets.
Instead, Fields is already sending the opposite signal, telling the media that the new subsidiary is going to be "dedicated to Ford" and will be limited to working with Ford's own vehicles. That suggests he's not actually prepared to stand up for Ford Smart Mobility's leadership if it winds up truly disrupting its parent company.
CEOs of large American companies saw their compensation fall in 2015, according to a data analysis by Theo Francis and Joann Lublin of the Wall Street Journal.
Not every S&P 500 company has yet released information on its 2015 pay, but about 300 members of that index of big publicly traded companies have. (The rest will do so over the next couple of months.) Of the companies in their sample, they found that median pay fell from $11.2 million to "only" $10.8 million — a decline of nearly 4 percent — largely because growth in cash bonuses slowed and pension contributions got less generous.
Nobody is going to cry for these CEOs, of course, but it is one measure in which inequality fell last year. Compensation growth for normal workers was not particularly robust by historical standards, but it did go up rather than down.
Alongside the aggregate data, the Journal put together a fun interactive comparing CEO pay to companies' stock performance. It shows that there's not a ton of rhyme or reason to who gets paid what at these lofty heights of the corporate world.
Google Inc. CEO Sundar Pichai had the highest pay of anyone by far, earning $100.5 million by taking over one of America's largest and most dynamic companies and delivering 45 percent shareholder return. But at No. 2 we have Philippe Dauman of Viacom, who earned $54.2 million while watching his company's stock slide 42 percent.
Steve Ells and Montgomery Moran of Chipotle earned $13.8 million and $13.6 million, respectively, while their company was plagued with food safety problems and lost 30 percent of its stock market value.
The bottom of the pay rankings is dominated by founder-owners, like Whole Foods' John Mackey and Google's Larry Page, who both earned just $1 in 2015.
Of course for founders, share price accumulation is its own reward. That 45 percent increase in the value of Google stock made Page a lot richer than he was at the start of the year, even though it technically wasn't salary. Mark Zuckerberg paid himself $610,455, and Warren Buffett earned $470,244 — again, with both men making their real money through share price accumulation.
That's a reminder that as lavishly paid as CEOs are, for the truly rich, getting paid isn't the name of the game. The top 20 richest people in America all founded a company (most of them), inherited a fortune (Jim, Rob, and Alice Walton; Jacqueline, John, and Forrest Mars), or did a little of both (Charles and David Koch). No pay package, no matter how splendid, can compete with the possible rewards of owning things — either things you created yourself or things you inherited from your parents.
On Sunday the world became familiar with the Panama Papers: a massive 2.6-terabyte leak of confidential documents revealing a deep web of international corruption and tax evasion from the world's political elite.
Panamanian law firm Mossack Fonseca — which specializes in helping foreigners set up international shell companies to protect their financial assets –  leaked the papers to the International Consortium of Investigative Journalists to expose the offshore holdings and hidden financial dealings of some of the world's most familiar names.
Vox's Matt Yglesias explains:
The documents provide details on some shocking acts of corruption in Russia, hint at scandalous goings-on in a range of developing nations, and may prompt a political crisis in Iceland.
But they also offer the most granular look ever at a banal reality that's long been hiding in plain sight. Even as the world's wealthiest and most powerful nations have engaged in increasingly complex and intensive efforts at international cooperation to smooth the wheels of global commerce, they have willfully chosen to allow the wealthiest members of Western society to shield their financial assets from taxation (and in many cases divorce or bankruptcy settlement) by taking advantage of shell companies and tax havens.
More than 100 media organizations spent a year poring over 11.5 million leaked files with 40 years' worth of data connecting more than 214,000 offshore companies to people in 200-plus countries. Here are some key resources to help you catch up on the unfolding scandals.

1) The Russian government has already said it was "obvious" President Vladimir Putin was the main target of the Panama Papers leak. Whether or not the objective of the leak was to smear the Russian president, the findings showed a $2 billion trail of secret offshore deals and loans all pointing toward Putin.
The Guardian explains the "fabulous fortunes of the Russian president's inner circle":
Though the president’s name does not appear in any of the records, the data reveals a pattern – his friends have earned millions from deals that seemingly could not have been secured without his patronage.
The documents suggest Putin’s family has benefited from this money – his friends’ fortunes appear his to spend.
[...]
Cash was also handed over directly to the Putin circle, this time in the form of very cheap loans, made with no security and with interest rates as low as 1%. It is not clear whether any loans have been repaid.
2) Icelandic Prime Minister Sigmundur Gunnlaugsson walked out of an interview after a Swedish reporter asked him about investments exposed in the Panama Papers.
.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
Gunnlaugsson and his wife, Anna Sigurlaug Pálsdóttir, bought the offshore company Wintris in 2007 to invest millions in Icelandic banks — a line of questioning the prime minister found "totally inappropriate."
The BBC explains Gunnlaugsson's questionable investments:
The leaked documents show that Mr Gunnlaugsson was granted a general power of attorney over Wintris - which gave him the power to manage the company "without any limitation". Ms Palsdottir had a similar power of attorney.
Court records show that Wintris had significant investments in the bonds of three major Icelandic banks that collapsed during the financial crisis which began in 2008. Wintris is listed as a creditor with millions of dollars in claims in the banks' bankruptcies.
Mr Gunnlaugsson became prime minister in 2013 and has been involved in negotiations about the banks which could affect the value of the bonds held by Wintris.
3) While China’s top leader, Xi Jinping says he is ready to take on the "armies of corruption," the Panama Papers revealed that, like many of the world leaders exposed in this data dump, members of Xi and other top Chinese officials' families are tied to multiple offshore companies.
The papers named Xi's brother-in-law Deng Jiagui and the daughter of former China Premier Li Peng, Li Xiaolin — both names that were previously exposed in a 2014 ICIJ report that the Chinese government denied.
The Washington Post explains why this can create an uncomfortable tension in a country that has tried to wage an aggressive — albeit selective — anti-corruption campaign:
Although there are legal uses for shell companies, the charges are sure to rile Beijing.
China’s ruling Communist Party does not like to discuss the wealth of its leaders, or their families, especially as it wages an aggressive, if selective, anti-corruption campaign.
[...]
In 2014, a report jointly published by the ICIJ and the Center for Public Integrity found 22,000 alleged tax haven clients from Hong Kong and China. That investigation found offshore accounts linked to more than a dozen of China’s richest people, including members of the National People’s Congress and executives from state-owned firms caught up in corruption probes.
[...]
Asked about the story at a Foreign Ministry press conference that year, a Chinese government spokesperson called the investigation "hardly convincing."  The report was subsequently blocked. The Chinese press did not play up the story.
4) It's not only political elites. Top names in the entertainment and athletics industries also made the long list of Mossack Fonseca's clientele. Notably, the list included names closely tied with the already corruption-mired FIFA.
The records showed that four of the 16 FIFA officials indicted in the United States used offshore companies. And then there is famed Argentine player Lionel Messi, who, already under indictment for using offshore companies to skirt taxes, was found to have owned yet another offshore company in Panama: Mega Star Enterprises.
Messi has already said he is ready to sue the Spanish newspaper that outed his record of tax evasion for defamation.
You can read ICIJ's full report on the financial underbelly of international soccer and other athletics here.
5) The Panama Papers made one thing very clear: Tax havens, meaning countries or independent areas where taxes are levied at a low rate, are ubiquitous. So much so that avoiding them is nearly impossible, Nicholas Shaxson explained in his exposé on the world's tax havens for the Guardian.
"See if you can dodge all my bear traps, and declare yourself untainted by tax havens. If you succeed, you win my Hermit of the Year prize," Shaxson writes of the pervasiveness of these loopholes:
Do you celebrate Christmas? If you do (or even if you do not), did you buy any gifts on Amazon last December? If so, then your goods were quite likely to have been routed through a byzantine world hosted – only on paper, you understand – by the Grand Duchy of Luxembourg, where Amazon has located its European headquarters, slashing its tax bills around the world. In 2011, Amazon revealed that the US Internal Revenue Service was chasing it for $1.5bn in back taxes. More recently, Amazon has said it will stop routing its UK sales through Luxembourg.
[...]
Let’s cut this challenge short. Did you at any point consume the services of any of these: AIG, Aviva, Barclays, Black & Decker, British American Tobacco, Burberry, Citigroup, Deutsche Bank, Facebook, FedEx, GlaxoSmithKline, Ikea, HSBC, JP Morgan, Microsoft, Pepsi, Skype, Starbucks, Vodafone or Walt Disney? This is just my quirky personal selection from a list of more than 350 multinationals whose convoluted tax schemes were revealed last November by a whistleblower, working for one accountancy firm, PricewaterhouseCoopers (PwC), in one European tax haven, Luxembourg.
6) Tax havens aren't only in Panama and the Cayman Islands. For some, states like Delaware and Wyoming in the United States are tax havens as well.
Bloomberg published "The World’s Favorite New Tax Haven Is the United States." Why? Because even American law firms dedicated to protecting the financial assets of the world's elite say the US is a perfectly effective tax haven:
You can help your clients move their fortunes to the United States, free of taxes and hidden from their governments.
Some are calling it the new Switzerland.
After years of lambasting other countries for helping rich Americans hide their money offshore, the U.S. is emerging as a leading tax and secrecy haven for rich foreigners. By resisting new global disclosure standards, the U.S. is creating a hot new market, becoming the go-to place to stash foreign wealth. Everyone from London lawyers to Swiss trust companies is getting in on the act, helping the world’s rich move accounts from places like the Bahamas and the British Virgin Islands to Nevada, Wyoming, and South Dakota.
"How ironic—no, how perverse—that the USA, which has been so sanctimonious in its condemnation of Swiss banks, has become the banking secrecy jurisdiction du jour," wrote Peter A. Cotorceanu, a lawyer at Anaford AG, a Zurich law firm, in a recent legal journal. "That ‘giant sucking sound’ you hear? It is the sound of money rushing to the USA."
7) But while everyone has heard of the existence of tax havens, the actual practices that go into wealth management are more secretive.
The Atlantic explains how a select few legally "enable their clients to sidestep many laws policies" in its piece "Inside the Secretive World of Tax-Avoidance Experts":
Wealth management is a profession on the defensive. Although many people have never heard of it, it is well known to both state revenue authorities and international agencies seeking to impose the rule of law on high-net-worth individuals. Those individuals—including the 103,000 people classified as "ultra-high-net-worth" based on having $30 million or more in investable assets—pay wealth-management professionals hefty fees to help them avoid taxes, debts, legal judgments, and other obligations the rest of the world considers part of everyday life. The general public doesn’t hear much about these professionals, since there are only a few of them worldwide (just under 20,000 belong to the main professional society) and they strive to keep a low profile, both for themselves and their clients.
8) Somewhat surprisingly, it is really easy to set up a shell company. In fact, NPR's podcast Planet Money set up its own shell companies just to test this out: Unbelizeable, Inc., in Belize, and Delawho? in Delaware.
Listen to Planet Money's discoveries on the ins and outs of owning a shell company, and why they are easy to set up and a hassle to deal with.
(The Planet Money team even drafted a resolution that would allow them to go to Belize to meet the "director" and "shareholder" of the company.)
At 2.6 terabytes, the massive leak of confidential documents now known as the Panama Papers offers a deep, complicated look at an international web of corporate finance, corruption, and tax avoidance.
But the heart of the story — a bunch of individuals and organizations storing their money in secret offshore locations like Panama — isn't that complicated. Over at Reddit, user DanGliesack gave one of the best explanations I've read yet:
When you get a quarter you put it in the piggy bank. The piggy bank is on a shelf in your closet. Your mom knows this and she checks on it every once in a while, so she knows when you put more money in or spend it.
Now one day, you might decide "I don't want mom to look at my money." So you go over to Johnny's house with an extra piggy bank that you're going to keep in his room. You write your name on it and put it in his closet. Johnny's mom is always very busy, so she never has time to check on his piggy bank. So you can keep yours there and it will stay a secret.
Now all the kids in the neighborhood think this is a good idea, and everyone goes to Johnny's house with extra piggy banks. Now Johnny's closet is full of piggy banks from everyone in the neighborhood.
One day, Johnny's mom comes home and sees all the piggy banks. She gets very mad and calls everyone's parents to let them know.
Now not everyone did this for a bad reason. Eric's older brother always steals from his piggy bank, so he just wanted a better hiding spot. Timmy wanted to save up to buy his mom a birthday present without her knowing. Sammy just did it because he thought it was fun. But many kids did do it for a bad reason. Jacob was stealing people's lunch money and didn't want his parents to figure it out. Michael was stealing money from his mom's purse. Fat Bobby's parents put him on a diet, and didn't want them to figure out when he was buying candy.
Now in real life, many very important people were just caught hiding their piggy banks at Johnny's house in Panama. Today their moms all found out. Pretty soon, we'll know more about which of these important people were doing it for bad reasons and which were doing it for good reasons. But almost everyone is in trouble regardless, because it's against the rules to keep secrets no matter what.
As Vox's Matt Yglesias explained, chances are most of the people involved hid their money in a way that's technically legal in order to avoid taxes. And it's something that's been going on for decades due to bad policy, allowing big businesses to dodge taxes in a shady but legally permissible manner.
Still, this isn't the kind of behavior that Americans want giant corporations engaging in. After all, this is money that should be going to US coffers but just isn't. So even those who did everything by the book are in trouble — and the big controversy may convince lawmakers to finally do something about the corporate tax system.
Richard Branson, Virgin's billionaire founder, is sad that Virgin America airlines is being sold to Alaska Airlines for $2.6 billion: "I would be lying if I didn’t admit sadness that our wonderful airline is merging with another," he wrote on his blog. "There was sadly nothing I could do to stop it."
And Branson isn't the only one.
whaaaaaaaplease dont crapify virgin b/c i love it https://t.co/c1MKTrioNO
Virgin America is a tiny minnow in the US airline market, but it's a beloved one. Customers are besotted by the mood lighting, leather seats, oddly catchy safety instruction videos, and snack selection. It wins international awards.
JetBlue, one of the few US airlines approaching Virgin America in popularity, bid for the airline but lost. Still, it could be worse: Most rankings agree that Alaska Airlines is one of the better carriers. And some evaluations say it actually does a better job, even if its planes don't have quite as much mood lighting.
Virgin Airlines ranked first in the latest Airline Quality Rating study, which evaluates American airlines based on objective factors like on-time arrival and lost bags; Alaska came in fifth, behind Virgin, JetBlue, Southwest, and Delta.
Planes are more likely to be on time with Alaska than with Virgin, according to the study: 87 percent of Alaska Airlines' flights landed on time, compared with 82 percent of Virgin's. And Alaska Airlines has the lowest rate of passengers who file official complaints about problems with their trip. Only about four out of every 1 million passengers complains, less than half of Virgin's rate.
Compared with Virgin, Alaska was about three times as likely to mishandle baggage and nearly five times as likely to bump passengers off flights because they're overbooked.
But Alaska placed first, just ahead of Virgin, on the Wall Street Journal's most recent airline rankings, a spot it's held every year since 2013. As well as involuntary bumping and on-time arrivals, the WSJ's evaluation took into account delays of at least 45 minutes and two-hour delays on the tarmac — both things Alaska was very good at avoiding — along with canceled flights.
And studies of customer satisfaction have found that people are generally pretty happy with Alaska Airlines. Alaska Airlines was the top-scoring traditional carrier in rankings from JD Power and Associates and came in third, behind JetBlue and Southwest, in the American Customer Satisfaction Index's airline survey. Virgin America was too small to be included in either survey.
If there's one consistent trend in customer satisfaction surveys, it's that airlines that started or expanded after industry was deregulated in 1978, such as JetBlue, Southwest, and Virgin, are consistently more popular than the older carriers, such as Alaska, Delta, or American.
The problem is that it's very hard to succeed in the airline business. Since 1978, more than 250 new airlines have been started in the US, and nearly all of them failed. It took Virgin America seven years to turn a profit. And in the meantime the airline industry has consolidated, with big players getting bigger.
Since 2007, when Virgin America started flying, there have been four major airline mergers. Four carriers — American, United, Southwest, and Delta — now control 80 percent of the US market. The merger with Virgin will make Alaska the fifth-largest airline, smaller than the big four but bigger than JetBlue.
Two big stories about Russian corruption have broken in the past week. A leak of papers from a Panamanian law firm appears to show perhaps $2 billion, presumably owned by President Vladimir Putin, stashed away in offshore companies under the name of a close friend and the godfather of his oldest daughter, the cellist Sergei Roldugin. Meanwhile, the brave Organized Crime and Corruption Reporting Project hit on a story about the women in Putin's life being given posh apartments.
Together, these stories tell us something important about how corruption works in Russia. Whereas in many countries corruption is the means by which elites turn their power into money, in Russia it is the other way around — corruption is a way to get and keep the political power that is so much more important than mere wealth.
Even before these stories broke, the Kremlin spin machine was briefing against what it was calling an "information attack" in a global media war. But corruption in Russia is, sadly, hardly news, which goes to show why these stories speak to something much more significant than, say, tax avoidance.
It is too easy simply to see Russia as a kleptocracy or, more misleading yet, a "mafia state." Yes, corruption is endemic to the system, not a byproduct but a central feature of Putin's methodology of power. But this doesn't fully explain why there is such corruption in Russia today.
Assuming those $2 billion are Putin's, how did he get them? Did people hand him suitcases of cash? Highly unlikely. Rather, he takes or is given stakes in assets, rights to shares in profits. This is not so much for the money itself — anything he could want, he can get the state or the oligarchs to buy, from a palace on the Black Sea to a $35 million yacht — but rather for the political power they give him.
The real currency in Russia is not money but power — and the latter can buy the former, but not necessarily the other way around. You can be rich today, but the state can impoverish you tomorrow. Conversely, if you have power, you can always get money, as we are likely seeing with the Panama Papers, or else simply don't even need it.
While the Panama Papers involve far more cash, and are likely to draw far more attention, it's the other story that is the more meaningful, for demonstrating that Russia's real currency isn't money — it's power and connections.
It alleges that Grigory Baevsky, a little-known Russian businessman who formerly ran a state property agency and now works for Arkady Rotenberg — one of Putin's oldest friends and, coincidentally, one of the richest billionaires in Russia — has been involved in transferring apartments in Moscow for a string of women connected with Putin. They include his youngest daughter, Katerina Tikhonova; the sister and, it is thought, grandmother of his current alleged girlfriend, rhythmic gymnast Alina Kabaeva; and Alisa Kharcheva, a woman whose main claim to fame is her near-naked appearance in an infamous "We Love You" pinup calendar dedicated to Putin.
In today's Russia, official corruption is so normal that it scarcely even merits much mention. Ministers and officials routinely turn out to have massive mansions, such as Defense Minister Sergei Shoigu's $18 million pagoda-themed estate.
In part, the Kremlin's prickly response may be precisely because Putin is notoriously secretive about his private life. But given that the preemptive spin from Kremlin spokesperson Dmitry Peskov (who himself apparently lives in a $7 million house on an income of less than $140,000) made it even more of a high-profile story, it is likely more than that.
The answer may be that the case illustrates the way corruption really works in Russia. For sure, the driver pulled over by a traffic cop, the contractor looking to speed up approval of building plans, or the store owner being shaken down by a fire inspector all have to pay cash on the nail. But this kind of corruption is pretty small-scale and, something most Russians never have to face.
The real corruption that is sucking the blood out of today's Russia is the industrial-scale profiteering taking place at the top of the system. According to INDEM, one of the last independent liberal think tanks left in Russia, corruption costs the country between a quarter and a third of its total GDP.
Put it another way: Corruption by the Russian elite could be costing the country up to six times as much as all the sanctions imposed by the West since Russia invaded Crimea.
The way this corruption works is through access and favors rather than actual transfers of money. When Putin wants to reward his friends, he allocates them contracts and monopolies they can milk for all their worth.
But in return, they know that likewise they are rich only so long as they have political power and Putin's goodwill. Part of the price is to understand that at times they will be called on for favors, and they better deliver.
There is, after all, no evidence that Putin actually paid for any of the properties Baevsky doled out. Rather, it is more likely that Putin made his wishes known, maybe through his friend Rotenberg — who incidentally received more state contracts than anyone else last year — and everything was arranged to keep the boss happy. These valuable assets and sums of money are often something that is much more valuable in Russia than mere cash: They are tools used to express, deliver, or exert political power.
Money can be moved, hidden, willed to your kids. Power, though, is something active, ephemeral, needing constantly to be refreshed and reasserted. It is something you either have or you don't. Back in Soviet times, one reason so many leaders died in office was because they knew the day they retired everything they had — the cars, the mansions, the summer dachas — could be taken away from them. The tragedy of modern Russia is that the same is true.
Putin and other senior Russian figures are like sharks: They have to keep swimming or they drown. To an extent this worked in the 2000s and early 2010s, when the economy was growing and there was always more sea in which to swim. Now, though, as the waters drain, the competitions and collisions are getting more common.
But these conflicts are not always or only about cash and contracts. Sometimes they are about other manifestations of power. Chechen strongman Ramzan Kadyrov already has a private zoo with a tiger and a $1.25 million Lamborghini supercar, for example, so for him the Kremlin offers more medals and honors than he could fit on his chest.
Or for Investigations Committee Chief Alexander Bastrykin, one of the Kremlin's main political enforcers, the really valuable thing is not money (he seems to be of relatively moderate means) but rather success in his regular political struggles with his rivals, especially Prosecutor General Yuri Chaika.
Either way, the property story actually illustrates how corruption really works in Russia, and the Panama Papers tell us how power really works in Russia. It's not about following the money, but following the power.
Mark Galeotti is a professor of global affairs at New York University and a visiting fellow with the European Council on Foreign Relations. He blogs at In Moscow's Shadows and is on Twitter as @MarkGaleotti.
On Thursday, Elon Musk unveiled a prototype of Tesla's new Model 3, the much-hyped vehicle that, he hopes, will finally help electric cars go mainstream when it hits the market in December 2017.
The news has induced the sort of feeding frenzy usually associated with shark chum and Apple products. More than 180,000 people have already plunked down a (refundable) $1,000 deposit to reserve their Model 3 a year in advance. In California, Colorado, even Australia, hundreds of fans were lining up to preorder a vehicle they hadn't even laid eyes on yet:

Tesla fans stand in line inside Park Meadows Mall March 31, 2016 to preorder the new Tesla due to be unveiled Thursday night. (Photo By John Leyba/The Denver Post via Getty Images)
Partly that's because the car seems pretty neat. Musk says the all-electric Model 3 will start at just $35,000, considerably cheaper than Tesla's flashy $75,000+ high-end models. The base version is expected to get a range of 215 miles on a single charge, though the company is hoping to bump up that number eventually. We'll see if Tesla can actually follow through on these grand promises: it's banking on the massive battery-making GigaFactory outside Reno, Nevada, to churn out cheap batteries and keep the car's price down.
But there may be another curious policy reason for this week's Tesla stampede. Right now, the federal government offers a tax credit worth up to $7,500 for anyone who buys an electric car. Except it comes with a catch: The credit starts phasing out for any manufacturer that sells a cumulative total of 200,000 electric vehicles and plug-in hybrids in the US.
Tesla is on track to do just that. It has already sold roughly 65,000 of its high-end Model S vehicles over the past three years and is hoping to sell another 90,000 or so cars this year as its Model X hits the showroom. Some analysts think the company could reach 200,000 cumulative sales sometime in 2017. Once that happens, the federal tax credit for Tesla vehicles would fall by 50 percent for the next two quarters, and then by 75 percent for the two quarters after that1. And then it's gone — unless Congress decided to expand the program.
Note: Separately, California also offers a $4,000 electric vehicle tax credit for anyone whose income is less than 300 percent of the poverty line. It phases out for higher incomes and isn't available for anyone making more than $250,000.
Which means only a fraction of buyers may end up actually qualifying for the tax credit after the Model 3 comes out in Christmas 2017. It's unclear how many of the people standing in line yesterday were aware of this, but they certainly have incentive to try to reserve while it's hot.

The "Fight for 15" movement got its biggest win yet on Thursday as the California legislature passed a bill to phase in a statewide $15-per-hour minimum wage over the next six years. Gov. Jerry Brown is expected to sign the legislation.
There's a lively debate among economists about the economic impact of minimum wage hikes. Higher minimum wages provide raises to some workers, but some economists argue that they also prompt substantial job losses. Other economists dispute this, saying there's little or no effect on employment and that businesses compensate for higher costs through reduced turnover, improved productivity at work, lower compensation for better-paid workers, and price increases.
So who is right? When I set out to interview economists about the effects of California's minimum wage hike, I was expecting some strong disagreements. Instead, I found a broad consensus: California's hike is so large — and would result in a minimum wage so high — that no one really knows what will happen. None of the three economists I interviewed was willing to make a prediction about how the new law would affect employment in California.
"It would be foolhardy to believe you could project what's going to happen with any degree of confidence," said Jeff Clemens, an economist at the University of California San Diego whose research has found that higher minimum wages have caused job losses in the past. That sentiment was echoed by Arindrajit Dube, whose research has suggested that minimum wage hikes do not cause significant job losses.
Of course, that in itself is a reason to be concerned, since California lawmakers are taking a risk with the livelihood of millions of low-wage California workers. But advocates of the California proposal argue that it's a risk worth taking.
Companies employ workers if the value they get from the workers' labor exceeds the costs of employing them. The higher the minimum wage is, the harder it will be for employers to afford to pay workers. So if the minimum wage gets too high, job losses are inevitable.
But economic theory doesn't tell us how high a minimum wage has to get before significant job losses occur. And over the past quarter-century, this has become one of the most hotly debated questions in economics. A famous 1993 study examined the effects of a minimum wage hike in New Jersey by comparing employment in nearby counties in New Jersey and neighboring Pennsylvania. Surprisingly, the authors of that study, David Card and Alan B. Krueger, found that employment at New Jersey restaurants affected by the wage hike actually increased faster than employment at nearby restaurants in Pennsylvania, where the minimum wage did not increase.
Of course, that was only one study. Over the next two decades, many other economists have performed similar studies, with varying results. For example, a recent study by Clemens found that the most recent hike in the federal minimum wage — from $5.15 in 2006 to $7.25 in 2009 —  "reduced employment among individuals ages 16 to 30 with less than a high school education by 5.6 percentage points." On the other side, a comprehensive study of state-level minimum wage hikes between 1990 and 2006 by Dube and two co-authors found "no detectable employment losses from the kind of minimum wage increases we have seen in the United States."
Unfortunately, little if any of that past research is directly applicable to California's proposal, which would take the state's minimum wage to unprecedented highs. California's current $10-per-hour minimum wage is already among the highest in the country — only Washington, DC, has a higher minimum wage at $10.50 per hour. California is planning to boost its minimum wage by another 50 percent over six years. Even after adjusting for inflation, the new rate of $15 per hour could be the highest minimum wage ever adopted by a US state.
Raising the minimum wage to $15 an hour by 2022 "will likely mean that 30 to 40 percent of the California workforce will get a raise," Dube said in a phone interview. "This will be a big experiment. It's far outside of our evidence base."
I asked Dube — generally seen as a supporter of a higher minimum wage — if it was a mistake for a state as large as California to try such a big increase. Would it be better to let $15-an-hour experiments in San Francisco and Los Angeles play out?
"If you're risk-averse, this would not be the scale at which to try things," Dube told me. "On the other hand, if you think that wages are really low and they've been low for a really long time and we can afford to take some risks, doing things at this scale will get us more evidence."
"A $15-an-hour national minimum wage would put us in uncharted waters, and risk undesirable and unintended consequences," wrote Alan Krueger, who has served as an economic adviser in the Obama administration, last October. Krueger supports raising the national minimum to $12 per hour, and he acknowledged that some cities and states might be able to absorb a $15-per-hour minimum wage. But he argued that a $15 minimum is "beyond international experience, and could well be counterproductive."
Clemens said one reason it's hard to predict the effects of California's wage hike is that it will affect different types of workers than previous hikes. In his previous work, Clemens studied the federal wage hike from $5.15 to $7.25 between 2006 and 2009. This increase affected a relatively small number of workers at the very bottom of the wage scale.
In contrast, the California wage hike will affect workers making between $10 and $15 per hour. Clemens said that "the types of workers, the amount of experience they have, the length of time they've been in their particular employment relationship" are all different in this higher income bracket. So a $15-per-hour minimum wage might have significantly different economic impacts than a $10 minimum wage.
It's also significant that the new minimum would take effect statewide, not just in wealthy cities like San Francisco and Los Angeles. Dube points out that — for better or worse — the law will have the biggest effect in less affluent areas like Fresno, where average wages are lower. More workers in those areas will get raises, but there's also a greater danger that businesses will be forced to lay off workers.
Experts told me there's no direct analogue to California's wage hike, but one of the closest parallels is Puerto Rico in the early 1980s. The standard of living in Puerto Rico is significantly lower than on the US mainland, and so until the 1970s Congress allowed the island to set its own, lower minimum wage on an industry-by-industry basis. But in 1974, Congress changed that, phasing in the higher US minimum over a decade. By 1983, Puerto Rico had the same $3.35 minimum wage that applied throughout the continental US. And because Puerto Rico is poorer than the mainland, the higher minimum affected a lot of workers.
"Puerto Rico experienced massive job losses as a result of the application of the U.S. minimum to the island," wrote Alida J. Castillo-Freeman and Richard B. Freeman in a widely cited 1992 study of the minimum wage hike. "Imposing the U.S.-level minimum reduced total island employment by 8 to 10 percent compared to the level that would have prevailed had the minimum been the same proportion of average wages as in the United States."
The researchers found that the higher minimum wage forced many low-skilled workers to flee to the United States in search of work — something they could easily do because there are no restrictions on migration between the island and the mainland. Before the higher minimum wage took effect, people leaving Puerto Rico tended to be more educated than those who stayed. Afterward, migrants became predominantly less well-educated, suggesting that the higher minimum may have forced the least skilled workers to leave the island in search of work.
Yet some economists question whether the Puerto Rico experience is really the cautionary tale that conservatives portray. A follow-up study by Alan Krueger examined the same data with different statistical tools and found no sign that the higher minimum wage had reduced employment in Puerto Rico.
Moreover, Puerto Rico was in a very different economic condition in 1980 than California is today. Puerto Rico was one of the poorest parts of the United States and saw its minimum wage increased from a very low level to a level typical of the United States as a whole. In contrast, California is one of the wealthier parts of the US and already has one of the nation's highest minimum wages. So the economic consequences of its new, even higher minimum wage could be very different.
Surprisingly, Richard Freeman, the co-author of the original Puerto Rico study, doesn't see the island's experience as a cautionary tale for California. "A big jump in the minimum wage did cost some jobs" in Puerto Rico, he told me,  "but it was not a giant disaster at all."
Freeman argues that California's higher minimum wage might be worth it even if it costs some people their jobs.
"If the minimum wage goes up 50 percent and you lose 5 percent of work, there are huge benefits flowing to lots of people," he says.
Freeman likes a provision of the California proposal that would allow the governor to halt the increases in the event of an economic downturn. "You want to raise the minimum as much as possible to benefit people and obviously stop when we're causing some serious harm," Freeman says.
The tricky thing, however, is that it may not be easy to tell if California's higher minimum wage is harming employment.
"A lot of people were saying that if some of these increases are implemented, we'll finally 'get an answer' to the question" of how big minimum wage increases affect employment, Clemens told me. But he's skeptical about this. Because the new minimum is going to be phased in over six years — and possibly longer — it could be hard to draw any firm conclusions about the law's effects, even after the data is in.
After all, people are still arguing about the effects of minimum wage hikes that occurred in the 1980s, 1990s, and 2000s. We can expect that California's experience will be studied intensively over the next decade. But there's no guarantee a consensus will be reached on its economic effects.
Charlie Stross wrote a blog post arguing that Apple's interest in strong encryption is linked to its interest in secure payments, which in turn is linked to its long-term plan to leverage the company's enormous stockpile of cash into becoming a bank and disrupting the entire consumer banking industry.
It's a fun idea to think about. And when you start thinking about it, you quickly realize that not only do Apple's consumer relationships suggest a possible banking opportunity but so do Amazon's and Google's and Facebook's. Yet there's a pretty good reason none of these companies have taken this step yet: It's generally illegal for non-bank companies like Apple to go into the banking business.
Theoretically, Apple could try to convince regulators to let it get involved in some bank-like activities. But if it wanted to do that, it would be bending over backward to show it was willing to cooperate with regulators on other issues. Instead, Apple has done just the opposite, staging a high-profile fight with the FBI over iPhone privacy. That's not the behavior of a company that's preparing to get into a new, highly regulated industry.
We talk a fair amount in US politics about an old law called Glass-Steagall, its repeal in 1999, and proposals to reinstate some version of it. What that law said was that a bank (in the sense of the kind of place where you might have a bank account or go to get a mortgage) couldn't be part of the same company as other kinds of financial services companies, like investment banks or insurance companies.
But it's important to understand that the commercial bank/investment bank combination wasn't something that was uniquely banned during the New Deal; it's something that was uniquely legalized during the financial deregulation of the 1980s and '90s.
Which is to say that even though a bank can be part of the same company as an investment bank or an insurance company or a stock brokerage firm, a bank can't sell pizza or cars or also do HVAC repair. To be a company that owns banks you need to be a bank holding company, and then you can't be owning businesses that aren't financial services companies.
The line here can get a little bit fuzzy at times since non-bank companies that sell durable goods sometimes run financing subsidiaries that make loans, which seems pretty bank-like. But regulators do try to police the lines. One example is that General Motors' financing arm, GMAC, used to be a big moneymaker for the company. But it was only after GMAC was spun out as a separate company called Ally Bank in the wake of the 2008 financial crisis that it started taking deposits. GMAC was owned by a car company, so it couldn't do banking. Ally Bank isn't, so it could.
Just because an Apple Bank would normally be illegal (and not just because that's already the name of a bank) doesn't mean it couldn't happen. Walmart fought and failed to get a banking license, but then eight years later succeeded in securing permission to do some banking-like stuff. Apple is a big company, and it could hire lawyers and lobbyists to try to get favorable regulatory rulings and new legislation passed.
But here's where Stross's theory of the linkage between banking aspirations and taking on the FBI really goes south.
If you need big regulatory favors from the US government, the last thing you want to do is go and make a huge public stink about your disinclination to cooperate with the government on matters the FBI regards as urgent national security questions.
Most likely, Apple's willingness to take a strong stand on behalf of its users' privacy against government requests for help indicates the opposite: Apple is not planning on making any kind of big new regulatory asks in the foreseeable future. Apple Pay works by partnering with stakeholders in the existing bank and credit card industries. That's partly to secure widespread acceptance, but it also helps to avoid regulatory issues. The company's willingness to take a hard line against the Feds suggests that Apple has no plans to change this partnership-based strategy.
Walk into almost any grocery store in the continental United States, and you'll be able to buy bananas for 79 cents a pound or less.
To most of us, this seems utterly normal. The banana is so cheap and widely available that, decades ago, it surpassed the apple to become the most widely consumed fruit in the country.
But the fact that bananas have become so commonplace is fairly astounding when you think about it. The banana is a delicate tropical fruit: It must be picked and packed by hand, then shipped and refrigerated for thousands of miles before reaching your door. Until the early 20th century, the banana was an exotic, expensive delicacy, unknown to most Americans. Nowadays, there's growing concern that Panama disease, a parasitic fungus, could wipe out the most popular banana variety altogether.
And yet, says Dan Koeppel, the author of Banana: The Fruit That Changed the World, "bananas remain incredibly cheap. When you adjust for inflation, they're nearly as cheap and in some cases even cheaper than they were decades ago, when we first started importing them widely."
How is this possible? Part of the answer is a fascinating biological trait unique to the banana plant — and part simply has to do with how we mass-produce commodities in today's globalized food system.
Virtually all of the cheap foods we find in the supermarket today — think potatoes, eggs, or ground beef — are the products of finely tuned, highly industrialized agricultural systems geared toward mass production. And, as Koeppel details in his excellent book, bananas were among the first foods to be turned into a commodity.
In the 1880s and '90s, American businessmen Andrew Preston and Minor Cooper Keith began importing Gros Michel bananas (a different variety from the Cavendish bananas widely eaten today) under the auspices of the Boston Fruit Company, which would eventually become the United Fruit Company and then Chiquita. Betting that American consumers could be taught to develop a taste for the exotic fruit, they cleared tracts of land in Jamaica for plantations and began running their own steamships to bring the bananas to US markets along the East coast.
"No one had ever shipped fruits over the ocean before," Koeppel says. "It was unheard of." To get them to port without rotting, the company built a network of ice-cooled warehouses, boxcars, and ships, vertically integrating the whole operation in a way that paralleled the oil and steel monopolies developing during the same era.
Preston had predicted that bananas could become "more popular than apples" — and he was right. Decades of aggressive ad campaigns — including tactics such as distributing manuals to schoolchildren that extolled the nutritional benefits of bananas — gradually enshrined the fruit as a wholesome, fundamental part of the American diet.
As demand in the US grew, banana executives colluded with authoritarian regimes in the so-called "banana republics" of Costa Rica, Honduras, and Guatemala in order to expand production. The companies would offer concessions — or in some cases, bribes — in exchange for cheap or free land and other favorable policies that would enable them to grow more bananas at low cost.
"They were able to say, 'We'll build you a railroad in return for land and some tax breaks,'" says John Soluri, a Carnegie Mellon historian and banana expert. "They also got the controversial right to import labor: black workers from the West Indies." These workers were severely underpaid, a huge factor in driving down the cost of the product, given that banana harvesting is a labor-intensive process.
Today's banana laborers are somewhat less heavily exploited, but the cost of labor in the developing world is still low. That, along with the still intact political influence of the banana multinationals and the infrastructure networks they began building a century ago, are all core reasons why bananas are so inexpensive compared with homegrown crops.
Around the world, there are hundreds of banana varieties grown by small-scale farmers. But in the United States, you're only likely to find one type for sale — the yellow Cavendish banana. That's by design. It's also a big reason why bananas remain so cheap.
"When the Gros Michel variety was wiped out [by the Panama disease fungus] in the 1950s and '60s, the banana companies were looking for another variety that was easy to handle, pack, and ship," says Rony Swennen, a Belgian biologist who studies the diversity of banana varieties.
Enter the Cavendish. "The Cavendish has bunches that grow three meters above the soil, with a shape that makes them easy to pack, and a beautiful color to boot," he says. The fact that it's significantly less flavorful than other varieties wasn't considered a major problem.
All those Cavendish bananas also look and taste identical. And that's largely due to the fact that banana plants reproduce asexually.
When a farmer wants a new banana plant, he or she removes a part of an existing plant (either a side shoot, called a "sucker," or an underground root-like structure called a "corm") and puts it in the ground. In time, it will develop into its own genetically identical plant. Without sexual reproduction — a grain of pollen fertilizing an egg, as occurs with most other fruit species — there's no random variation among plants that growers need to worry about. Every banana you've ever eaten is a clone.
This is how virtually all commercially grown bananas are produced worldwide, and it means that every banana plant behaves in the same perfectly predictable manner. Their fruits grow at the same rate, in the same abundance, and ripen at precisely the same time.
"It's almost wrong to think of the banana as a fruit, as a product of a what we traditionally think of as farm," explains Koeppel. "The Cavendish banana is a factory product in every bit the same way that a potato chip or a BMW is."
Today we have a high-tech system honed to growing this standardized product on a massive scale. Engineers can remotely monitor refrigerated shipping containers at sea at the precise temperature (ideally between 54.5 and 55 degrees Fahrenheit) needed to preserve them as long as possible. Upon arrival in the US, bananas can be kept in airtight warehouses and exposed to just the right amount of ethylene gas to cause artificial ripening before they're stocked in supermarkets, in a uniform pale yellow-green color.
"When you have just one thing to manufacture, it's a lot cheaper to do it," says Koeppel. "That's economies of scale at work."
The standardization of the Cavendish has allowed agribusinesses to grow bananas for low cost and for shoppers to become accustomed to cheap bananas. We even expect it. If prices ever do go up, retailers just take the hit and keep selling them for 79 cents a pound. "Supermarkets view bananas as a loss leader. They use their price to get you in the store," says Soluri.
But there are downsides to our current system of cheap bananas, too. "These companies can keep bananas so cheap only because they don’t count the cost to the environment," says Swennen. This, he notes, includes the damage caused by excess fertilizer dumped on banana plantations that seeps into surrounding groundwater, and the fossil fuel emissions produced to carry the fruit thousands of miles from plantations to our houses.
The cheap price of bananas also doesn't take into account how current growing techniques have allowed for the spread of a new form of Panama disease (known as Tropical Race 4), which could wipe out the Cavendish in coming years — just as a previous strain of the pathogen eradicated the Gros Michel back in the 1950s.
When growers replaced the Gros Michel with the Cavendish, they believed the latter was impervious to Panama disease. But because all their bananas are clones, they're easy targets for a slightly altered version of the disease that has since evolved and can exploit just one vulnerability to attack every single banana. Meanwhile, today's densely cultivated plantations and interconnected shipping networks, it's believed, have allowed the fungus to spread more efficiently. Since 1990, Tropical Race 4 has jumped from Malaysia to Pakistan to Africa, and many fear it could hit Latin America at any time.
All this has led some observers to claim that the Cavendish is on the verge of going extinct — which isn't exactly true. Tropical Race 4 is very unlikely to wipe out the Cavendish entirely. However, it — along with other threats such as the fungal disease black sigatoka — could certainly devastate banana crops and drive up retail prices of the fruit in coming years. The era of the cheap banana, more than a century long, might not last forever.
After six years of strong returns, the market suddenly lost about 10 percent of its value in mid-August 2015. In September, after the market had regained some of August’s losses, Yale economist Robert Shiller, a Nobel laureate, told CNBC that the stock market might be in a bubble. "I think the market might do what it did in 2000," he said.
Six months later, the market is roughly where it was the day Shiller made his comments. With plunging oil prices, China’s serious economic troubles, and the ever-present possibility of a security crisis, one can hardly rule out the possibility of a big further drop.
If you remember the past two stock market meltdowns, in 2000 and 2008, you might be tempted to dump your stock investments so you can avoid big losses when the current bubble — if it is in fact a bubble — ultimately pops.
I asked several experts what they thought about this question. They told me that trying to predict market bubbles is generally a fool’s errand. However, there are some steps ordinary investors can take to protect themselves that work whether there’s a bubble or not.
The lessons are a bit different regarding a housing market bubble. Because these are more immediately dangerous to regular people, you should be more restrained in your decisions about housing, especially if local real estate prices race ahead of local rents, or if your life situation would makes it hard to ride things out if housing prices really dropped.
The first thing to remember is that we don’t know if we’re in a bubble right now. Shiller suggested we’re in a bubble, and he’s pretty smart. But there are plenty of smart people who think current prices are sustainable.
"Bubbles are only ‘predictable’ after the fact," writes Princeton’s Burton Malkiel, author of a famous investment book called A Random Walk Down Wall Street, in an email.
We all know about the 2000 dot-com bubble and the 2008 real estate bubble. We like to think that if we’d been there we would have seen them coming. But bubbles are more deceptive than that. Genuinely smart investors lose serious money when bubbles burst. After all, that’s how they become bubbles in the first place: If they were truly obvious, so many people wouldn’t have gotten fooled.
During a bubble, there’s always a plausible argument that higher prices are justified by more favorable market fundamentals. After the dot-com bubble burst, for example, it seemed obviously stupid to bet the farm on Pets.com. But during the 1990s boom, it sure looked like Amazon, Microsoft, and other innovative companies were changing the nature of American business.
The same point applies today. Maybe the possibility of Indian and Chinese growth has spawned unrealistic expectations about the companies that sell goods and services there. Maybe those hopes will actually be realized. Either way, it will seem more obvious afterward than it does right now.
And it’s important to remember that the stock market has experienced solid periods of strong growth that were not bubbles at all. For example, after the stock market boom of the mid-1980s, a lot of people thought the market was due for a fall. Pessimists predicted that the 22 percent, single-day stock market crash on October 19, 1987, would be the start of a big recession and a bear market. Instead, the market shrugged it off, delivering spectacular stock market returns for another 12 years. People who panic-sold in October 1987 expecting further declines missed out on big gains instead.
Even if some financial experts might be able to recognize stock market bubbles, you and your financial adviser probably aren’t in this group. If you’re saving for retirement, what you care most about is stock market returns over the long run. Neither excessive alarmism nor undue optimism is helpful.
The key to investment success is to save 10 to 20 percent of your gross income over the course of your career — regardless of what the stock market is doing. You’ll ride some serious upswings, and you’ll experience some stomach-churning declines. But in the long run, you’re likely to do fine, or at least as well as you would do following any other available strategy that doesn’t involve a time machine.
Don’t panic-sell when the market plummets. Don’t get overconfident when the market soars, either.
Avoiding the cacophony of financial media can help you stay on course. As my University of Chicago colleague Richard Thaler told me: "Never watch a financial news network.  ESPN is much better for your financial health."
If you live below your means during your working years, you’ll build up a nice cushion against a major stock market downturn. If you get lucky, you might enter your golden years with greater wealth than you expected. As worst-case scenarios go, that one isn't so bad.
As you age, you can also reduce your risk by somewhat reducing your exposure to stocks — which offer higher returns but are more volatile. A "target date" fund set to your date of likely retirement can do this automatically for you. There are more details on how to do this in my new book (with Helaine Olen), The Index Card.
Today’s high stock prices don’t necessarily mean that stocks are about to crash. But they do mean you should expect stock market returns to be a bit lower in the future than they have been in the past.
"If someone gives you a rule of thumb that you can expect 7 percent real returns in the stock market over a long-term, this is wrong," writes Dean Baker, an economist at the Center for Economic and Policy Research. That’s the average rate of return over the past century, but these generous returns occurred at a time when stock prices were often less than 15 times corporate earnings.
Today, by contrast, stocks are worth more than 20 times earnings. And that means we should expect the rate of return on stocks to be correspondingly lower. Baker says current stock values imply that the inflation-adjusted rate of return is likely to be below 5 percent over the long run.
That doesn’t mean you should run away from stocks. Other options, like bonds and money market funds, are also offering lower returns than they used to. But it is a reason to save a little more and spend a little less in the expectation that the stock market won’t be going like gangbusters as it did in the late 1980s, the late 1990s, or between 2009 and 2015.
Shutterstock
I don't believe ordinary investors should spend much time wondering if the stock market is overvalued. I’m more cautious when it comes to housing, because a fall in your local housing market can do you greater and more immediate harm.
The money you put down on your house is different from the money you put down on an index fund for your retirement. Your house is the most leveraged, illiquid, and undiversified investment you will ever make. So a 10 to 20 percent swing in your local housing market can have an outsize impact on your wealth.
Housing runups and bubbles are distinctive risks for a second reason, too. Most people don’t need to draw on much of their stock portfolio until they’re at or near retirement. Even then, people draw down their wealth gradually in their 70s, 80s, and hopefully beyond.
In contrast, buying a house is an all-or-nothing proposition. And depending on what happens in your life, you might need to sell unexpectedly. You or your partner might lose a job. You might have a child or get a divorce. You might have to move to a new city to find work. If the local housing market drops just as you need to sell your house, that can be financially devastating.
How vulnerable you are depends a lot on your own life situation. If you’re confident you’ll be able to stay in the same house for a decade or more, you can ride out a bursting housing bubble in the same way you can ride out a bursting stock bubble. You’re less vulnerable if you are a tenured professor than if you work in a more volatile profession. Employment is not the only variable in play, either. If your marriage is troubled, that’s harder to talk about, but it also increases the risk that a separation will force you to sell earlier than you expected.
The best defense against these dangers is to be patient — especially if there has recently been a runup in the local housing market. The more you feel yourself stretching to buy a house, the more it makes sense to rent longer or to buy a more modest home. A vanilla fixed-rate mortgage with a 20 percent down payment is your best bet. These monthly payments, plus your property taxes, plus a few percent annually for repairs and maintenance, shouldn’t exceed 30 percent of your income.
Pay attention to the ratio of home prices to annual rents for similar properties. Dean Baker suggests that when this ratio exceeds 15 to 20, "caution is in order." Readily available data isn't perfect. Typical data compares median home prices to average annual rent. In areas that include a preponderance of hugely expensive homes, this can be misleading. But if this ratio creeps well above an area’s historic average, think twice before buying.
One final risk arises when your house rapidly appreciates. That risk is you. It’s tempting to spend too much by drawing on your home equity. During the Great Recession, millions of Americans learned that home equity can fall as quickly as it rises. I’m leery of home equity loans for a luxurious kitchen upgrade, a vacation, or a costlier car than you would otherwise drive. In this, as in so many things, living below your means is the best way to achieve financial security.
Correction: This article originally misidentified Dean Baker's affiliation.
When someone wins the lottery, it can be bad news for his neighbor's finances. A new study from the Federal Reserve Bank of Philadelphia examines the relationship between lottery winners in a particular Canadian province and bankruptcies in the same province — it found that neighbors of lottery winners are unusually likely to go bankrupt, and the larger the lottery prize, the more likely bankruptcy becomes.
Specifically, every $1,000 in lottery winnings translates to a 2.4 percent higher probability of a nearby neighbor declaring bankruptcy.
The researchers have an explanation for why this happens, too. When people declare bankruptcy in Canada, they have to disclose all their major assets — things like houses, cars, boats, and motorcycles — to the courts. The researchers found that the larger the lottery prize, the more money bankrupt neighbors spent on big-ticket vanity purchases — and the more likely they were to run out of money.
The clever study is one of the first to provide statistically rigorous evidence for a claim that seems plausible but is hard to prove: that rising inequality causes people to spend beyond their means in an effort to "keep up with the Joneses." This is the idea that when someone's wealth suddenly increases, her neighbors — and probably her friends and relatives — feel pressure to spend more to avoid being upstaged. Ultimately, this kind of competition can leave everyone worse off.
And while big lottery winnings are rare, the study could have much broader implications. Critics of income inequality have long argued that large income disparities make people unhappy. The Philly Fed study provides further evidence for this point of view. While it focuses on lottery winners, the same basic problem is likely to arise anytime some people enjoy rapid income growth at the same time that others' incomes are not rising.
The federal government is blocking the sale of hoverboards in the US until the self-balancing scooters can be proven to meet newly created safety standards.
Anyone who "imports, manufactures, delivers, or sells" hoverboards that don't meet the new standards could be sued or charged with a crime, according to a letter the US Consumer Product Safety Commission sent to the hoverboard industry Thursday.
The letter will probably lead to recalls of hoverboards that have already been sold as well. The safety standards, from UL, a company that certifies the safety of consumer products, have only existed for about two weeks, so there are no hoverboards currently on the market that meet them.

The commission's letter is the culmination of the rise and fall of one of 2015's hottest toys — one that literally began catching on fire. Major airlines banned hoverboards from their aircraft. Amazon in the UK told customers who bought hoverboards before Christmas that, actually, they should probably throw them away.
Hoverboards have a very weird, very 2015 backstory. They're the plastic-and-battery equivalent of a viral Instagram joke. The story of the hoverboard is the story of how a product itself can go viral, and the corners that can get cut along the way.
If you've somehow managed to avoid the hoverboard phenomenon so far, the most important thing to know is this: hoverboards do not actually hover.
The idea of a personal transportation device that allows you to float a few inches off the ground is still a mostly futuristic idea. Instead, what everyone is calling "hoverboards" are technically self-balancing electric scooters — basically, it's a much cooler name for a hands-free Segway.
Hoverboards run on battery power. You direct them by leaning forward or back, or by putting your weight on one foot to turn. They're usually stabilized by gyroscopes that keep you upright.
The scooters aren't particularly practical: their top speed is around 6 miles per hour, not too much faster than a brisk walk.
It's surprisingly hard to say where the nickname came from. One of the earliest self-balancing scooters, which started as a Kickstarter project in 2013, was called the Hovertrax; the name might have come from there.
But it's also possible that 2015 was simply fated to be the Year of the Hoverboard. Hoverboards were making news this year before self-balancing scooters were, inspired by all of us reaching the year Marty McFly traveled to in Back to the Future Part II and finding it depressingly devoid of actual hoverboards.
The first recorded use of "hoverboard" to refer to the hands-free scooters seems to come from the Luxury Technology Show, where visitors reference Back to the Future but don't directly call them hoverboards (although the video title does):
Calling it a hoverboard seems to have originated on YouTube. Casey Neistat, a filmmaker who posts vlogs on YouTube and has more than 2 million subscribers, was talked into buying a self-balancing scooter on Amazon in early June and quickly started referring to it as a hoverboard.
In July, TmarTn, a YouTube personality, bought one and described it in a video viewed more than 6 million times as a "Hoverboard, Segway-type thing":
Hoverboard manufacturers were incredibly successful at getting their product into the hands of celebrities, and the boards rolled their way into the national consciousness via Instagram, YouTube, and Vine.
Kendall Jenner glided around on one gracefully (until she fell off) on Instagram back in March. Justin Bieber wheeled around on a hoverboard a few weeks later. Jamie Foxx rode a hoverboard onto the Tonight Show with Jimmy Fallon in May. Rapper Wiz Khalifa was tackled and handcuffed at Los Angeles International Airport in August because he wouldn't stop riding a hoverboard. Even Mike Tyson got in the mix.
But then, a few weeks before Christmas, they started going viral for another reason: they were catching on fire. The hoverboards' batteries were exploding, leading to fires while hoverboards were plugged in and charging, being ridden, or simply sitting on the floor at a shopping mall.
The fires caused a quick hoverboard backlash. In the week leading up to Christmas, Amazon and Overstock stopped selling most hoverboards; the US Postal Service refused to ship them by air mail; and at least six airlines, including all the major carriers, said they weren't allowed on board.
The Consumer Product Safety Commission eventually investigated 52 fires in 24 states. In one case, a 13-year-old boy was riding his hoverboard inside when it started to smoke; after he took it outside, it burst into flames and the boy had to use a fire extinguisher to put it out.
The fires are the result of the lithium ion batteries that power the hoverboards. When a lithium ion battery is punctured, it explodes, a battery expert told Wired. Batteries can also explode if they short-circuit or overheat.
Lithium ion batteries aren't inherently dangerous. They're in everything — powering smartphones and laptops, cars and airplanes. But the way hoverboards rose from nowhere to everywhere, with few dominant or well-known manufacturers making a type of gadget that didn't exist a few years ago, has created particular safety headaches.
The CSPC wasn't able to get any boards to catch on fire during the testing process. But they said they suspected the fires could have been prevented if board manufacturers had to meet safety standards.
With hoverboards, it's the general idea, not a specific manufacturer, that's become popular.
The original two-wheeled, hands-free, self-balancing scooter was (probably — it's disputed) the Hovertrax, which got its start as a Kickstarter in 2013. Inventor Shane Chen, whose company Inventist has come up with other wacky, futuristic ideas for moving people around — easy-to-ride unicycles, something billed as "a cross between a skateboard and in-line skates" — promised the device would be the "future of powerful and portable transportation."
Chen more than met his $40,000 Kickstarter goal, and the first Hovertrax were shipped to the US in December 2014. But before they arrived, other hoverboards were being promoted in the US, including the Chic Smart S1. Chen, who patented his design, is now in a patent fight with IO Hawk, the leading American distributor of hoverboards, who buys from manufacturers like Chic.
The shifting manufacturing market for hoverboards has a direct bearing on their safety problems. That's partly because, as Wired reported this summer, underneath different labels and packaging, hoverboards are pretty similar.
Hoverboards almost universally come from Chinese manufacturers who can quickly replicate each other's designs. And not just one or two factories, but thousands of them, all of which got into the hoverboard business more or less overnight. Buzzfeed's Joseph Bernstein wrote in a fantastic exploration of the hoverboard supply chain:
The hoverboard industry that has unfurled in the concrete of Bao An and other similar districts is on-demand IRL content production, a super-flexible churn that hands us the playthings of social-media-driven seasonal diversion. It is the funhouse mirror reflection of the viral internet, the metal-and-cement consequence of our equally flexible commercial hype machine… Call it memeufacturing. It starts when a (typically) Western company, eager to cash in on a product made popular by the social internet, contracts a Chinese factory to make it. From here, the idea spreads throughout the elaborate social networks of Chinese electronics manufacturing until the item in question is being produced by hundreds and hundreds of competitors, who subcontract and sell components to each other, even as they all make the same thing.
Social media virality makes everything move faster — a local news story goes national or international within a few hours; a hoverboard goes from a Justin Bieber Instagram to a neighborhood dad in a matter of weeks or months. And the same process happens with products that happens with gossip or news: sometimes, corners are cut.
If you want to understand how weird the hoverboard industry is, look at the pricing, which is all over the place: an IO Hawk goes for nearly $1,800; a PhunkeeDuck is $1,500; a Swagway is $500, and you can get a no-name hoverboard on Chinese retailer Alibaba for $300.
It would be one thing if the IO Hawk or PhunkeeDuck came with additional bells and whistles. But the truth is that all hoverboards are fundamentally similar. This isn't necessarily the difference between a brand name and a knockoff; none of these brands existed before the hoverboard craze started.
The viral internet hasn't just outpaced the ability for individual brands to get a foothold in the market — it moved too fast for regulators, too. UL didn't develop safety standards for hoverboards until recently, meaning there was no independently certified way to know the products were safe.
The safety warnings before Christmas didn't stop a lot of kids from unwrapping hoverboards. But the hoverboard crackdown from airlines and Amazon led to a manufacturing dropoff in China, Quartz reported in December.
Now hoverboard manufacturers are going to have to be able to prove their products are safe, fast, or risk losing access to the American market. Hoverboards that don't meet standards will be seized when they get to the US, the CPSC said in its letter.
A Wall Street Journal story sent alarmed ripples through the internet on Wednesday, with tweets flying about how bosses could be sent "alerts" that their employees have stopped using birth control and might be planning an expensive pregnancy.
Turns out, not so much. A more careful read of Rachel Emma Silverman's piece doesn't actually support these claims. And one of the third-party data companies Silverman wrote about told Vox that employers definitely can't get information about individual employees through their service.
If anybody is getting "alerts," it's the employees, and if the bosses are being sent data about employee health, it's not about identifiable individuals.
That said, the piece still raises some serious privacy concerns about the way employers are using "big data" to try to curb health care costs.
Silverman reported that some companies, including Walmart, are hiring third-party firms like Castlight Healthcare Inc. to help them reduce their health care costs.
The outside firms do this by collecting data about employees, using it to calculate people's risks of certain health conditions, and sending messages that "nudge" individual employees into making healthier choices that could lower their (and their employer's) health care costs in the long run.
Here's the passage from Silverman's Wall Street Journal article that set off the alarm bells about birth control "alerts":
To determine which employees might soon get pregnant, Castlight recently launched a new product that scans insurance claims to find women who have stopped filling birth-control prescriptions, as well as women who have made fertility-related searches on Castlight’s health app.
That data is matched with the woman’s age, and if applicable, the ages of her children to compute the likelihood of an impending pregnancy, says Jonathan Rende, Castlight’s chief research and development officer. She would then start receiving emails or in-app messages with tips for choosing an obstetrician or other prenatal care. If the algorithm guessed wrong, she could opt out of receiving similar messages.
Out of context, this definitely sounds like some kind of dystopian fertility monitoring system with troubling implications for a woman's job security. Pregnancy discrimination still happens all the time, after all, despite laws against it.
But in context, Silverman notes that employers only see aggregated data, that federal health privacy laws generally prohibit employers from viewing their workers' health data anyway, and that workers can opt out of the health messaging service. And representatives from Castlight told Vox there's no way that data could be traced back to individual employees.
"Employers that use our system never, under any circumstance, see individual employee data from Castlight Action," said Jim Rivas, senior director of corporate communications for Castlight. The data is anonymized and aggregated, he said, and the employer only sees the number of people who are at risk for certain conditions.
Castlight also limits the size of the group that can be displayed — since in a small enough group, it could be easy to guess which employee is at risk for which condition (such as pregnancy). Castlight senior product manager Alka Tandon said bosses won't receive any data for groups smaller than 40 people. That's almost four times as high as the privacy threshold that the Center for Medicare and Medicaid Services recommends, which is 11 people.
So unless a company had more than 40 soon-to-be-pregnant employees and was prepared to cut paid family leave or institute mass layoffs in response (which is still not an inconceivable scenario), the privacy of pregnant women workers who use Castlight is probably pretty safe.
Tandon added that the company uses a "robust" development process that includes testing whether users find the health messages creepy or invasive, or helpful and empowering. Employees also have to actively create an account to use the service in the first place.
Frankly, there's something appealing about these "big data" health services. They can help people who don't have a primary care provider find a good in-network doctor, or they can refer employees with back problems to physical therapy before they turn to expensive, possibly unnecessary spinal surgery. Prenatal care is also essential for good maternal and infant health, so alerting pregnant women to their care options actually seems like a great idea.
These services could legitimately help people stay healthier, which would also end up saving companies money. Everybody wins, right?
But there's also something disturbing about how much these third-party firms can know about you, based on not just your health records but also things like your spending habits.
"I bet I could better predict your risk of a heart attack by where you shop and where you eat than by your genome," one consultant told Silverman.
Castlight doesn't use that kind of outside data, Tandon said. But other companies, as Silverman reported, buy information from data brokers that helps connect people's consumer spending habits with their health care usage.
Indiana University law professor Nicolas Terry told Fortune that this space is completely unregulated, and that insurance claims and search queries aren't necessarily protected in the same way as other private health information under federal law.
The whole thing brings to mind how Target creeped out its customers a few years ago by using data mining to send ads for baby products to women who didn't even know they were pregnant yet, or who hadn't yet told their families and had to have an awkward conversation after the mail arrived. But of course the stakes are higher when it's your employer.
Privacy experts also told Silverman that these data-mining efforts carry other huge risks. Sensitive health data could be vulnerable to hacks or breaches, for instance, which could lead to privacy concerns even worse than your boss knowing you're trying to get pregnant.
Read the Wall Street Journal article for more on the subject.
Neel Kashkari was an assistant secretary in George W. Bush's Treasury Department who wound up running TARP (a.k.a. the bank bailout) and then launching a comically inept Republican campaign for governor of California. More recently, he became president of the Federal Reserve Bank of Minneapolis; he just gave his first major speech in that capacity, and it's a real shot across the bow.
He says the current approach to regulating the biggest banks in America is conceptually flawed, and that while policymakers should finish dotting the i's and crossing the t's on Dodd-Frank, they also need to start considering "transformational measures" that would create "fundamental change."
This is a particularly surprising view from a guy who ran for office as a Republican just two years ago and who's described himself repeatedly as a "free market" and "pro-growth" Republican. National Republicans have, of course, been critical of the Obama administration's approach to bank regulation but generally by arguing that the White House has put too much red tape in the way of industry. Kashkari, by contrast, says that what President Obama has done has been helpful but doesn't go nearly far enough. It's the kind of speech you could imagine Bernie Sanders or Elizabeth Warren giving, except in some respects he ends up embracing ideas that are more radical than theirs.
The whole speech is worth a read, but Kashkari's key idea is really contained in this paragraph. His point is that it's one thing to look at a financial institution and ask yourself what would happen if it went bankrupt due to a random act of massive incompetence, and another thing entirely to ask what would happen if banks were going bankrupt as part of a larger meltdown of the American economy:
I learned in the crisis that determining which firms are systemically important—which are [Too Big To Fail]—depends on economic and financial conditions. In a strong, stable economy, the failure of a given bank might not be systemic. The economy and financial firms and markets might be able to withstand a shock from such a failure without much harm to other institutions or to families and businesses. But in a weak economy with skittish markets, policymakers will be very worried about such a bank failure. After all, that failure might trigger contagion to other banks and cause a widespread downturn. Thus, although the size of a financial institution, its connections to other institutions and its importance to the plumbing of the financial system are all relevant in determining whether it is TBTF, there is no simple formula that defines what is systemic. I wish there were. It requires judgment from policymakers to assess conditions at the time.
Kashkari says he's reasonably confident that the regulatory tools put in place after the crisis can do a good job of dealing with systemically significance financial institutions in a non-crisis situation. The problem, he says, is that in a crisis, that calculation would become a lot less certain and policymakers would become a lot more risk-averse. The exact problem that faced the Bush administration in 2007 and 2008 would recur:
Given the massive externalities on Main Street of large bank failures in terms of lost jobs, lost income and lost wealth, no rational policymaker would risk restructuring large firms and forcing losses on creditors and counterparties using the new tools in a risky environment, let alone in a crisis environment like we experienced in 2008. They will be forced to bail out failing institutions—as we were. We were even forced to support large bank mergers, which helped stabilize the immediate crisis, but that we knew would make TBTF worse in the long term. The risks to the U.S. economy and the American people were simply too great not to do whatever we could to prevent a financial collapse.
So what does he think should be done about it?
Kashkari says we should "give serious consideration to a range of options" and that bank breakups ought to be on the menu. His other two ideas are less politically sexy but, if anything, more radical.
One is that large banks should be regulated like public utilities, "by forcing them to hold so much capital that they virtually can’t fail (with regulation akin to that of a nuclear power plant)." The other is "taxing leverage throughout the financial system to reduce systemic risks wherever they lie."
Both these proposals are, essentially, extreme curbs on banks' ability to finance their operations with borrowed money, which would make them a lot less profitable. Breaking up banks is something that would be really bad for the executives of the existing big banks and probably a little bad for their shareholders, but would still leave the banking industry as a whole in a prosperous condition. Curbing risk across the board would be more severe.
Saudi Arabia and Russia, the world's two largest oil producers, joined Qatar and Venezuela at a big OPEC meeting to announce a plan to cap oil production and bring a halt to the slide in crude oil prices that's rocked global markets over the past couple of years.
But don't expect it to work.
The key problem is that even though Saudi Arabia and Russia are huge producers of crude oil, they haven't been responsible for much of the recent increase in oil supply. So capping production at January levels, which is what they are talking about, won't necessarily do much to prevent new supply from coming online. The real sources of new crude are Iran and Iraq, and they're not party to the deal.
Iran, as you have probably heard, recently signed a major diplomatic agreement with the United States and other world powers, whose terms involve verifiable nuclear disarmament in exchange for sanctions relief. The main points of this from an Iranian point of view are to make it easier to export oil and to make it easier to import the foreign capital, equipment, and expertise needed to increase domestic oil production.
Under the circumstances, agreeing to a production cap would amount to giving up the lion's share of the gains from the deal with no real upside.
Somewhat similarly, Iraq has been steadily increasing its oil production as part of the postwar rebuilding process but continues to see output hampered by security problems. The Iraqi government may or may not manage to make further progress in securing oil facilities from the Islamic State, but if it does succeed, the Iraqis are going to do everything they can to bring that oil to market and give it the revenue it needs to fight the insurgency.
Last but by no means least, the United States — the world's No. 3 producer of oil — isn't party to any such deal and doesn't even attend these kinds of meetings, because we are net importers of crude oil.
American production is down a bit from its highs in the summer of 2015. But that's not a policy decision to cut production, it's not security problems, and it's not a lack of exploitable oil resources. What's stymied US production is that the price of oil fell steeply enough that the pace of new drilling collapsed.
That's been the one ray of good news for Persian Gulf oil producers in 2016. But it also in effect puts a cap on oil prices. Even if the Saudis and the Russians could get Iran and Iraq on board for production cuts to raise prices, if prices rise too much, US production will come roaring back.
Fear of this scenario is precisely why Saudi Arabia has thus far been reluctant to agree to meaningful production cuts. Today's agreement reflects less a change of heart on that score than a desire to create the appearance of progress by reaching a diplomatic deal that isn't so meaningful.
Around the world, markets are in chaos. Japan's stock market plunged 5 percent on Friday, while markets in France, Germany, and the UK all saw big losses on Thursday. The US stock market is doing better than most, but it is also down since the start of the year. Oil hit a new low on Thursday of $26 per barrel.
These declines reflect growing concerns that the world economy is headed for another recession. Before 2007 we’d say, "If things get bad, the Fed will cut interest rates." But with the Fed’s benchmark rate below 0.5 percent already, a substantial cut would mean rates that are below zero. That's an unorthodox strategy, and it might not even be legal, according to testimony by Fed Chair Janet Yellen before congressional committees this week.
The Fed needs a new strategy: Stop targeting interest rates and instead target the growth of the overall economy. Moving away from interest rate targeting would give markets confidence that the Fed has the tools to deal with the next economic downturn, which would reduce the danger of another 2008-style meltdown.
Unfortunately, there's little sign that the Fed is laying the groundwork for a shift in strategy. Instead, Yellen seemed to be in denial about the magnitude of the challenge she is facing.
"Let’s remember that the labor market is continuing to perform well," she said to the Senate Banking Committee on Thursday. "We want to be careful not to jump to a conclusion about what is in store for the economy." Maybe not — but the Fed needs to be prepared for the worst.
"The Fed needs to change their fundamental approach," argues Scott Sumner, a monetary policy expert at the Mercatus Center. Right now the Fed's policy discussions are all about where to set short-term interest rates. But not only does that approach stop working when interest rates fall to zero — as they did in 2008 — but interest rates aren't even what people actually care about.
Instead, Sumner argues, the Fed should start directly targeting a variable people do care about: either the inflation rate or (even better) the total amount of spending in the economy. He argues that the Fed should focus on setting long-run goals for these variables and then doing whatever it takes to meet those goals.
The Fed currently has an official target of 2 percent inflation. But the central bank's actions make it clear that it's not serious about this target. Last December, for example, the Fed's own forecast showed that inflation would be around 1.6 percent in 2016 — and the forecast inflation rate had actually been falling. Yet the Fed raised interest rates anyway. That was a pretty clear signal to the markets that the Fed cared more about returning to "normal" interest rates than it did about achieving its inflation target.
The problem isn't just that the economy will grow a little bit slower in early 2016 than it could have otherwise. By ignoring its own targets, the Fed sent a message that it wasn't really committed to robust growth over the long run, which undermines businesses' confidence in the recovery and discourages investment.
The solution, Sumner argues, is for the Fed to use a strategy called level targeting to make its own targets more credible. Under a level targeting regime, the Fed would compensate for missing its target in one year by overshooting the following year. For example, in 2015, the Fed's preferred measure of inflation came in at 1.4 percent — 0.6 percent below the Fed's 2 percent target. Under level targeting, the Fed would aim to achieve 2.6 percent inflation in 2016, delivering 2 percent inflation on average in 2015 to 2016. That would not only support faster economic growth in 2016, it would also give the markets more confidence in the Fed's forecasts for 2017, 2018, and beyond.
Abandoning interest rate targeting might seem radical, but the Fed has actually done it once before, in the late 1970s. Back then, it seemed that no matter how high the Fed raised interest rates, it couldn't get inflation under control. So in 1979, the Fed stopped targeting interest rates altogether.
Instead, it simply set a target for the total amount of money in the economy. Fed Chair Paul Volcker knew that if the amount of money in circulation stopped rising, the inflation rate would eventually have to stop rising too. It took a couple of years (and helped induce a major recession in 1980), but it worked.
A big reason the strategy worked is that markets believed Volcker was serious about the target. Targeting the money supply directly signaled he was willing to let interest rates go as high as they had to in order to get inflation under control. Once markets believed he was serious, they started doing a lot of his work for him — and businesses began to curtail price increases in the expectation that the overall inflation rate was going to decline.
Today we're facing the reverse situation. A big reason the economy has been recovering so slowly is that businesses are worried about sluggish growth — or, worse, another 2008-style meltdown — in the coming years. So they've been reluctant to invest, making slow growth a self-fulfilling prophecy.
If the Fed can convince businesses that it's serious about delivering consistent growth, businesses will start investing more in the expectation that demand for their products will grow — and that investment will itself produce growth.
Level targeting is a strategy for giving the market more confidence in the Fed's long-term targets. And while inflation-based level targeting would work better than what we're doing now, Sumner argues that the best strategy would be to target the total amount of spending in the economy. This approach, known as nominal GDP targeting, has been endorsed by prominent economists such as Christina Romer.
There are two big problems with the Fed's current strategy of focusing on interest rates. The obvious one is that once rates hit zero, the conventional approach to monetary policy becomes ineffective. After rates hit zero in 2008, the Fed was forced to use an ad hoc strategy known as "quantitative easing" to pump more money into the economy. Lacking experience with this new strategy, the Fed twice made the mistake of ending easing too early, slowing the economic recovery between 2010 and 2012.
The larger problem with zero interest rates, however, is politics. People are used to thinking of low interest rates as a sign of easy money and vice versa, so the zero interest rate struck many as a sign of recklessly easy monetary policy. In the years after the financial crisis, critics warned that Fed policies would create runaway inflation.
In retrospect, it's clear that these concerns were unfounded. The average inflation rate since 2008 has been well below the Fed's 2 percent target, while the economy has suffered from persistently slow job and wage growth. But fear of a political backlash for doing "too much" discouraged Yellen's predecessor, Ben Bernanke, from acting decisively to promote economic growth.
More recently, the Fed has come under a lot of pressure to "normalize" — that is, raise — rates above zero percent. After resisting these pressures for most of 2015, the central bank finally pulled the trigger in December, boosting its target rate from zero percent to 0.25 percent. It did this despite the fact that — as Vox's Matt Yglesias pointed out at the time — most economic indicators suggested that a rate hike would do more harm than good.
This is the flip side of the situation the Fed faced in the 1970s. Because interest rates were high, many people thought monetary policy was too tight, and the Fed faced a lot of pressure to cut rates. But cutting rates triggered another wave of inflation, forcing the Fed to raise rates once again. Interest rate targets had become a distraction, and abandoning them helped Volcker focus on the variable he really cared about: inflation.
Negative interest rates are one way the Fed could try to salvage the current regime of focusing on interest rates. But it's not a very good one.
Just as you might have a savings account with your bank, so a nation's banks all have accounts with their nation's central bank. The money deposited in these accounts is called reserves, and in recent years a lot of banks around the world have chosen to build up huge reserve war chests. That's frustrating to central bankers who have been trying to encourage banks to stimulate economic activity by lending out the money.
So recently Japan's central bank and some central banks in Europe have been experimenting with negative interest rates on reserves. Last month, the Japanese central bank announced that banks would be assessed a 0.1 percent penalty on their reserves — an interest rate of -0.1 percent. A bank with a billion yen deposited with the Bank of Japan will now have to pay 1 million yen per year for the privilege.
Obviously, 0.1 percent is not a very big number, so the direct effects of Japan's new policy won't be very large. But going negative breaks through an important psychological barrier — once a central bank has instituted a slightly negative interest rate, it's more likely to cut rates further in the future.
In her testimony before Congress this week, Yellen was pressed on whether the Fed would follow its European and Japanese counterparts and impose a penalty on reserves. Yellen demurred, saying that the Fed's experts were still studying whether negative interest rates would be legal and technically feasible.
But even if the Fed ultimately decides to adopt negative rates, there are real limits on how far they can go. If negative interest rates get too steep, banks have an obvious alternative: They can get physical cash and store it in a big warehouse. By definition, cash is worth as many dollars a year from now as it is today.
There are a couple of ways central banks could try to make negative interest rates more feasible. University of Michigan economist Miles Kimball, for example, advocates a shift to a new form of electronic money that would allow central banks to impose economy-wide negative interest rates. Others argue that the Fed should raise its inflation target so that real, inflation-adjusted interest rates can go lower. But not only do both of these approaches have technical challenges, they're also likely to be intensely unpopular with voters.
So even if the Fed adopted negative rates, it wouldn't improve the effectiveness of the current interest rate targeting regime very much. Just as the Fed got stuck at zero percent interest rates in 2008, it could get stuck at -1 percent interest rates in 2017 or 2018. So the Fed is going to need a new framework that's less dependent on interest rates regardless. It might as well get started.
Twitter is at a crisis point in the wake of an earnings report that showed its number of users has stopped growing. At the same time, depending on how you count Twitter employees' stock options, the company is either still continuing to lose money or only modestly profitable. The thin ray of good news is that revenue grew — if that continues, the company's profits could improve steadily over time. But investors who bought Twitter shares back in 2013 were expecting explosive, Facebook-style growth, not modest profitability.
This brings Twitter to a key decision point. Does CEO Jack Dorsey push for fundamental changes in how the service works in hopes of dramatically increasing Twitter's user base, even though big changes might alienate current users and completely destroy the company? Or does he decide to try to make do with what he has, keeping current customers pleased and focusing on optimizing revenue and trimming costs in pursuit of a decent near-term profit?
That's why a small change Twitter rolled out this week is attracting so much interest in financial markets and so much anxiety among hardcore Twitter lovers.
If you walk by the desks of software developers here at Vox — or at any other company — there's a good chance you'll see them using a Unix command-line interface that dates back to the 1970s. Geeks continue to use the Unix command line because once you know how to use it, it's a powerful way to perform complex computing tasks.
But most users don't need the power, complexity, or hassles of using a Unix command line, which is why it never caught on with ordinary users. Until recently, most people used Windows PCs or Macs with a more user-friendly graphical user interface. Lately people have been shifting toward even simpler computing technologies: smartphones and tablets. These mobile platforms provide fewer capabilities than a conventional PC — it's hard to write a blog post or manage a complex spreadsheet on an iPhone — but they're easy to learn and demand little of users.
The older, more complex computing platforms didn't die. Certain kinds of professionals still use them. But most people use the simplest, most user-friendly platform available: smartphones.
A similar principle applies to social media technologies: The most popular services tend to be the ones that demand the least from their users. And Twitter demands a lot more of users than Facebook does.
To make Twitter manageable, you have to carefully curate the list of people you follow to avoid being overwhelmed. The rules for Twitter conversations — for example, the fact that starting a tweet with someone's username will hide it from anyone not already following that user — are not intuitive to someone dropping by the site for the first time, or even the 10th. And many users find it intimidating to write tweets that anyone in the world could read.
Twitter helps users sift through a lot of information efficiently, which makes it invaluable for a media professional like me. But most people aren't interested in sifting through large volumes of information. They just want to look at pictures of their friends' weddings, vacations, and babies with a minimum of hassle. And Facebook makes this really easy.
The result: Twitter's growth has stalled at the same time other social networks such as Facebook and Instagram are continuing to grow. And that has created a lot of friction between Twitter's management and Wall Street.
Many investors bought Twitter stock hoping that it would grow to become a Facebook-size behemoth. But it now looks like Twitter will wind up being much smaller than Facebook, which will mean it generates a lot less ad revenue — and ultimately profit — than Facebook.
Last week, BuzzFeed reported that Twitter was about to change the order in which it showed tweets to the user. This might seem innocuous, but it could have profound consequences for Twitter's user experience.
When you log in to Twitter, the service has traditionally shown tweets in strict chronological order, with the most recent tweets at the top. This means you only see tweets that occur at times when you happen to be paying attention. If a friend announces an engagement or pregnancy at a time when you're not online, for example, you might never scroll back far enough to see it.
Facebook takes a different approach: It shows the posts that a special Facebook algorithm thinks you will find most interesting. Facebook hasn't explained how its algorithm works, but we know it uses a wide variety of factors to gauge how interesting a post is. Posts about weddings, pregnancies, and other major life announcements tend to show up at the top of the Facebook news feed, as do posts with a lot of likes and those from your closest friends.
Facebook's approach is better for casual users. To get started on Facebook, you just have to identify your real-life friends and "like" posts that interest you. Facebook takes it from there, sifting through thousands of things your friends post to identify content you're likely to find interesting. You don't have to worry that friending too many people — or the wrong people — will turn your timeline into an unusable mess.
Twitter has been taking small steps toward a more Facebook-like service. Last year, the company introduced a feature called "While you were away." If you log in to Twitter after being away from the service for several hours, Twitter will show you older tweets that its software thinks you'll be interested in. However, it only shows a handful of older tweets — if you scroll down further, you get back to Twitter's standard reverse-chronological tweet order.
Then on Wednesday, Twitter announced a new feature called "Show me the best Tweets first" that seems to be very similar to "While you were away." The post doesn't explain how the two features differ, but it seems like the main difference may simply be that the new feature shows more "best tweets" than the old one did.
This might seem pretty innocuous — and it is. But when rumors of this feature first started circulating, it was pretty controversial among Twitter users.
The reason is that Twitter's current approach of showing tweets in chronological order means that most of the people who see a given tweet will see it within a few minutes of posting. This makes Twitter a real-time platform in a way that no other social media platform really is.
Twitter is the perfect place to banter about the Super Bowl, the Oscars, or a presidential debate. You can tweet about what just happened and assume that many of your followers will immediately understand the context. This real-time character makes the service especially useful to journalists, who want to always be up to date on the latest developments in their beats.
This kind of thing simply doesn't work on Facebook, because you never know if your Facebook post is going to be read an hour, a day, or a week from now. That means you can't assume that most readers of a particular Facebook post just saw the same thing you did. The real-time nature of Twitter also enables a type of sprawling real-time conversation among many users that's not really possible on any other platform.
Facebook's approach also gives more power to Facebook, since Facebook ultimately decides what users see. On Twitter, by contrast, what you see is entirely determined by whom you follow and when you're logged in. Naturally, power users who have put a lot of effort into curating whom they follow don't like the idea of ceding more power to Twitter over which tweets they see.
The big fear of Twitter users, then, was that Twitter would go full Facebook and totally replace its real-time feed with a Facebook-style news feed that shows the best tweets rather than the newest tweets. Or, at a minimum, that a Facebook-style feed would become the default, eroding Twitter's distinctive real-time culture.
But so far, that seems to have been a false alarm. Twitter is trying to help out infrequent users by showing a few high-quality tweets while staying firmly committed to reverse-chronological order as the main way people read tweets.
Still, if the "best tweets" experiment goes well, Twitter could gradually shift to Facebook's model over time. All they'd have to do is expand the number of "best tweets" they show, to the point where most people don't bother scrolling back to see the live tweets.
Underlying recent debates over Twitter's future has been a basic assumption that Twitter needs to resume the growth of its user base to be considered a success. That assumption is driven by Wall Street investors, who bought shares in the company's initial public offering in the hopes that the company will continue to grow.
Instead, Twitter's user growth has ground to a halt. And while Twitter will undoubtedly experiment with new ways to expand its audience, the company might ultimately have to admit that it's never going to be anywhere close to Facebook's size.
Twitter has more than 300 million users, and many of those are passionate daily users. That should be a large enough user base for a highly profitable business. Probably not profitable enough to justify the $30 billion valuation the company had a year ago, but perhaps profitable enough to justify today's company value of around $10 billion.
And Twitter may be able to find ways to play to the unique strengths of its user base. It doesn't have as many users as Facebook, but it can count among its users a large number of prominent investors, entertainers, academics, journalists, and other influential people.
These are people that some advertisers may be more keen to reach than the much broader audience reachable using Facebook ads. So if Twitter can figure out how to position itself as a premium advertising platform for precisely targeting its highly influential user base, it could earn a lot of money.
At the same time, it's not obvious that making Twitter more like Facebook would allow Twitter to grow to Facebook's size. People who like the Facebook experience can already get it on Facebook. Aping Facebook could easily alienate core Twitter users without attracting many new ones.
A new study says hiring more women in leadership roles literally pays off — big time.
The study, released Monday by the Peterson Institute for International Economics, finds that companies with more women executives tend to be more profitable.
This study isn't the first to find that better gender balance in leadership can be good for business. But it is the most extensive, sampling 21,980 firms with headquarters in 91 countries. Most other studies looking at this issue have been more limited or have just focused on one country.
Researchers found that companies with at least 30 percent of women in their most senior "C-suite" management positions are about 15 percent more profitable than firms with no top women executives.
This huge difference is probably because gender diversity is also correlated with skill diversity, which improves corporate performance on average. It's not that individual women will always outperform individual men in leadership. It's that women often bring different skills to the table — and firms that discriminate against women won't benefit from those skills.
Despite the profit-boosting power of women leaders, the report found a relative lack of women in upper management worldwide. Sixty percent of the companies studied had no women board members. Just over 50 percent had no women in C-suite executive positions, and 57 percent of the remaining half had just one woman executive. Fewer than 5 percent had a woman CEO. (Women CEOs also lead fewer than 5 percent of US companies on the S&P 500 index.)
But it's important to note: Having a woman CEO didn't actually have a measurable effect on a firm's performance one way or another. Having other women senior executives did, though — so much that the authors wonder if the same effect would apply to women in leadership below the C-suite level. Having women board members helps too, but not as much as having women executives.
"This pattern underscores the importance of creating a pipeline of female managers and not simply getting lone women to the top," the report reads.
Some countries, like Norway, have gender quotas for corporate boards. But, the report notes, these quotas don't seem to do much to help companies' performances.
One reason for this is the so-called "golden skirt effect," where a small number of women sit on the boards of several different companies and perform worse because they're overcommitted.
There could still be a role for quotas — especially ones that companies impose on themselves — in helping fight gender discrimination, the authors note. But it depends on a lot of different factors, including how far down the corporate ladder the glass ceiling reaches. If women struggle even just to reach upper management, it's more helpful to pursue policies to help mid-career women before tackling corporate board representation.
What definitely does help? Being located in countries that are really good at gender equity overall.
There's a positive correlation between companies with more women executives (and thus better profit potential) and countries where girls do better at math and where the public has less discriminatory attitudes toward women executives.
The researchers said that when women get a better education and have a better chance to get work experience free of discrimination, it fills the pipeline with lots of women who are qualified to serve in executive-level positions.
Paid paternity leave also makes a huge difference — but paternity leave specifically, not maternity leave.
This makes sense because, as other research has found, paid leave for dads boosts gender equality overall. It helps shift social norms away from the expectation that women are always going to be the primary caregivers for children, which helps remove a big stumbling block that keeps women from participating as fully as men in work and society.
This is another reason why boardroom quotas may not work — they can easily operate at a surface level. They can help companies look more diverse without solving the broader, deeper problems that hold down women's workplace advancement in the first place.
Every Chipotle restaurant will close for several hours today, February 8, so that the company can hold an unprecedented all-hands meeting to discuss issues related to the chain's food safety crisis. Multiple outbreaks of food contamination of varying levels of severity have pummeled the chain in recent months.

Here's the announcement notice on my local Chiptole, explaining vaguely that "we're closed for lunch today to attend a meeting with all other Chipotle employees" without mentioning the bacteria that have caused the meeting.

Sales have been plummeting as the chain, which used to market itself to health-conscious consumers with bogus arguments about the virtues of avoiding GMO foods, is learning that local and organic produce is actually harder to manage safely than conventional food.
A recent Bloomberg cover story makes clear that to an extent, Chipotle has been a victim of its own success. The chain has been wildly popular for years, and has been opening stores — and hiring staff — at a rapid pace. This large-scale hiring combined with an obsessive focus on serving customers at a more rapid clip seems to have made it challenging to maintain best practices on a staffing level. At the same time, relentless expansion of the Chipotle supply chain has made it harder and harder to track the sources of contamination.
In the short term, the question for Chipotle is whether things like the the shutdown can succeed in stemming the problems. But in the longer term, the big question is whether Chipotle can ever regain its brand as a "better" sort of fast food.
When I went to Chipotle last week for the first time since the crisis started, for example, they had no lettuce. It wasn't the biggest deal in the world, and I think it reflects a welcome newfound commitment to food safety. But it also reflects the fact that the restaurant seemingly can't fully deliver on its vision of quick service dining featuring fresh ingredients in a way that is both consistent and safe.
The Super Bowl is the purest distillation of America there is. It's brutal, lucrative, tawdry, spectacular, amazing, and full of delicious snacks. It's the one moment when we as a nation truly come together to reflect on our greatest achievements as a society — television, innovative chicken and cheese products, and over-the-top marketing. This year, there's also going to be a football game on.
But as the social pressure to watch a football game — or at least be in the presence of others who are watching — mounts, you may find yourself with some nagging questions. This is particularly true if you're not a football fan but are bowing to social pressure and pretending to be one for the day. We have the answers.
The most important question about any Super Bowl is what time is the Super Bowl? The answer is that this year’s big game will take place at 6:30 pm Eastern time on February 5, at NRG Stadium in Houston, Texas.
It’s on Fox, so if you’re interested in how to watch the Super Bowl, the answer is to tune in to Fox. If you want to know how to stream the Super Bowl, the answer is that you can stream the Super Bowl on the FoxSportsGo.com website or with the Fox Sports Go app.
If you are a Verizon customer who is wondering how to stream the Super Bowl to your phone, the answer is that you can stream the Super Bowl on your phone using the NFL app.
If you’re wondering why this section is written awkwardly, it’s because we are trying to load it with commonly searched keywords in hopes of improving its showing in Google and other search engines. This is called “search engine optimization,” and it’s an important part of contemporary digital content strategy.
Super Bowl 51 will be played at NRG stadium in Houston, Texas. This stadium is the home of the Houston Texans, and is part of a larger complex of sports facilities known as NRG Park that also includes the Astrodome (where the Houston Astros used to play), and which houses the Houston Livestock Show and Rodeo.
Most American sports leagues play their championship games at the home venues of the teams competing, but that’s not the case for the Super Bowl, which is such a big event that it’s invariably played at a neutral site selected long in advance.
Each year, the Super Bowl pits the champion of the National Football Conference against the winner of the American Football Conference. Super Bowl 51 features the Atlanta Falcons and the New England Patriots.
The NFL playoffs' single-elimination tournament structure sometimes allows real underdog teams into the Super Bowl, but that’s not really the case this year. The Patriots had the best record in the AFC and the best overall record in the NFL. The Falcons didn’t quite have the best record in the NFC, but they were close, and they had the second seed in the NFC playoff bracket.
Super Bowl ads are a big deal because they're extraordinarily expensive — $5 million for a 30-second spot this year — and Super Bowl ads are extraordinarily expensive because of the intersection of two trends.
One is the tremendous popularity of professional football. Lots of people watch the game.
The other is the declining popularity of everything that isn't live sports. The highest-rated non–Super Bowl broadcast of all time was the 1983 M.A.S.H. finale, which 60 percent of households watched. After that is a 1980 Dallas episode and the 1977 Roots finale. In the modern world, with audiences fragmented by cable television, distracted by the internet, and time shifting with DVR and on-demand services, it simply isn't possible for anything other than live events to reach very large segments of the population. This makes the Super Bowl a unique marketing opportunity that commands a uniquely high price.
Because Super Bowl ads are so expensive, companies that buy them tend to take the opportunity to roll out signature ads and new campaigns, which heightens the attention paid to the advertisements.
Are the ads worth the money? A team of researchers from the University of Wisconsin's Eau Claire campus has found some evidence that they may be. Films that are advertised during the Super Bowl see a 40 percent boost in ticket sales, and publicly traded companies that advertise during the game see their stock overperform the S&P 500 in the short term.
No. The winner gets the Vince Lombardi Trophy, named for the legendary coach of the Green Bay Packers in the 1960s. The origin of the Super Bowl name is somewhat tangled, but in brief:
Yale University's football team has long played in a bowl-shaped arena known as the Yale Bowl (not to be confused with Yale Bowls, which sells actual bowls). In 1923, a similarly shaped arena was constructed in Pasadena, California, and dubbed the "Rose Bowl." Pasadena had been the site of an important postseason college football match for about 20 years before the construction of the Rose Bowl, and once the new arena was complete, the match became known by the same name as the arena. From there, the tradition of referring to postseason college football games as "bowls" spread.
Meanwhile, in 1920 a number of professional football teams banded together to form the National Football League. In 1960, a rival professional football league — the American Football League — was established. In 1966, the two leagues agreed to merge. The merger was not complete until 1970, but starting in 1967 the winner of the NFL championship tournament played the winner of the AFL championship tournament in a championship game.
Once the merger was finalized, pro football was reorganized so that the old NFL became the National Football Conference and the old AFL became the American Football Conference, and the whole thing combined was the National Football League. The Super Bowl is played between the AFC champion and the NFC champion, and determines the overall league champion.
Like so many other things these days, this is basically a matter of partisan politics.
Donald Trump is a big fan of New England Patriots quarterback Tom Brady, whom he says “is a friend of mine, we play golf together.”
Conversely, while feuding with Rep. John Lewis (D-GA), Trump referred to his district — which includes the vast majority of the city of Atlanta — as “crime infested,” “in horrible shape,” and “burning.”
Some people like Trump, and they will enjoy rooting for Trump’s team, the Patriots. Other people do not like Trump, and they will enjoy rooting for Lewis’s team, the Falcons.
Of course, to make things somewhat problematic, there are serious Falcons fans throughout the state of Georgia, which Trump won, and Patriots fans throughout New England states, all of which Trump lost. So actual residents of these areas may feel significantly cross-pressured. But if you don’t live in Georgia or New England, your choice is clear.
Football has a lot of rules. But here are the basics:
The complete rulebook is here if you happen to be very bored. Note that even very serious football fans often don't fully understand all the different aspects of the rules, whose details change a bit from year to year within the basic framework.
I don't know. That's why they play the games!
I can't. Please watch the great video below, which Joseph Stromberg did on this subject, and read his article.
Yahoo is one of the best-known brands on the internet, but its core internet business is in a grim situation. How grim? There's a debate over whether the company itself — what most of us think of when we think of Yahoo — is actually worth less than zero dollars.
Back in 2005, Yahoo invested $1 billion in one of China's hottest technology startups, Alibaba, getting a roughly 40 percent stake. The bet has paid off handsomely. In 2012, Yahoo sold part of its stake back to Alibaba for $7.6 billion. Since then, Alibaba has continued to grow rapidly, and Yahoo's remaining stake is now worth around $25 billion.
That number is remarkable because Yahoo as a whole isn't worth much more than that. Indeed, if you subtract the value of all of Yahoo's major assets — including a multibillion-dollar stake in Yahoo Japan (an independent subsidiary in which Yahoo is a minority shareholder) and a few billion dollars in cash — from its market value, you get a big negative number. "If you just solve for the missing number, you are forced to conclude that Yahoo's actual core business of being Yahoo (and Tumblr and whatever) is worth negative $13 billion," as Bloomberg's Matt Levine put it in December.
I don't think that's true, for reasons I'll get into shortly. But it's created a secondary crisis for the company — and a deep distraction for its CEO, Marissa Mayer.
In 2012, Yahoo's board hired Mayer, then one of Google's best-known executives, to turn the company around. Nearly four years later, it's becoming clear that her turnaround effort is failing. Mayer has invested lavishly in both engineering and media talent, but there's no sign that these investments are paying off in the form of higher revenue.
And over the past year, Mayer's management of Yahoo the business has been overshadowed by an argument over whether Yahoo can distribute its Alibaba holdings to shareholders without paying billions of dollars in taxes on them.
Things came to a head on Tuesday, when Yahoo released its quarterly financial results along with a new turnaround plan. Yahoo announced that it was laying off 1,700 workers and focusing on its most successful products — including its search engine and popular email service. But in the same press release, the chairman of Yahoo's board announced that the board is going to "engage on qualified strategic proposals" — that is, consider offers to sell the company.
It's a humiliating announcement for Mayer, because it clearly signals that the board is losing patience with her turnaround efforts. And the looming possibility of a sale is going to make it all the more difficult for her to motivate Yahoo's remaining staff to work hard on her latest turnaround plan.
The most successful companies in Silicon Valley — including Google, Facebook, and Apple — have an intensely technology-focused culture. These companies are obsessive about hiring the most talented engineers (and in Apple's case, designers) so they can build the best technology products. And this culture tends to be self-perpetuating — very skilled, highly motivated people like to work with other very skilled, highly motivated people. Once you have a critical mass of such people it becomes easy to recruit more of them.
Yahoo has never had the same kind of obsessive focus on recruiting technical talent. Paul Graham, a well-known Silicon Valley investor who sold his company to Yahoo in 1998, has written that even in the late 1990s, Yahoo was ambivalent about its status as a technology company.
"One of the weirdest things about Yahoo when I went to work there was the way they insisted on calling themselves a 'media company,'" Graham wrote. Yahoo employed a lot of programmers and produced a lot of software, of course — and still does. But it never made software as core to its identity as some of its major competitors.
That's probably because at the time Yahoo was founded, in 1994, no one had ever heard of an ad-supported software company. Back then, software companies sold their products in shrink-wrapped boxes at Best Buy. Yahoo had the same business model as CNN and the New York Times — build up a large audience and then make money by selling ads — so it was natural for Yahoo to think of itself as being in the same industry. But one consequence of this was that Yahoo didn't focus as much as it could have on recruiting the best programmers.
Marissa Mayer's roots are as an engineer at Google, and she has made an effort to beef up Yahoo's technical talent. She instituted a more rigorous hiring process, and the company has worked hard to hire more computer scientists, especially from top universities.
But there's little sign that these moves have changed the culture or improved morale among Yahoo's programmers. "I just try to ship products that I’m not ashamed of," a Yahoo executive told the New York Times in December. This is not an attitude that tends to produce excellent products.
At the same time, Mayer has doubled down on the "media company" side of Yahoo's personality. In 2013, she hired television news anchor Katie Couric for Yahoo's news site. Couric's contract was renewed last year in a deal reportedly worth $10 million. Mayer also recruited gadget reviewer David Pogue from the New York Times to anchor Yahoo's relaunched technology news section.
But despite these investments, Yahoo doesn't have nearly the prestige of a New York Times or a CBS. The company is seen as something of an also-ran both in Silicon Valley and in the media world. Yahoo creates technology products that people use and media properties that have an audience, but its attempt to be a technology company and a media company simultaneously has resulted in an organization that's less than the sum of its parts.
In the past few years, Yahoo's media and tech businesses have been overshadowed by a third line of business: venture capital. At the same time Yahoo's core business has been in decline, its Alibaba investment has been soaring in value. Indeed, when you subtract the value of Yahoo's major assets from the total market value of the company itself, you get a large negative number.
The uncharitable way to interpret this is that the core Yahoo business is actually destroying value. It's possible that Marissa Mayer could increase her stock price by simply announcing that she was shutting down all of Yahoo's websites and laying off all of its employees.
But there's another major factor in Yahoo's depressed share price: taxes. On paper, Yahoo's Alibaba share is worth around $25 billion. However, if Yahoo ever tried to sell its stake and pay out the proceeds to shareholders, it would owe billions of dollars in taxes to the IRS.
After adjusting for these tax liabilities, it's possible to get a positive number for the value of Yahoo's core business. But it's still a small number. When Levine crunched the numbers in December, he concluded that Yahoo's core businesses were worth just $1.7 billion, less than 10 percent of Yahoo's overall market value.
So Yahoo's search engine, email service, news site, and other properties might not literally be worth less than nothing. But right now the stock market doesn't seem very optimistic about their chances.
It's not hard to see why Wall Street would value Yahoo's core business as close to worthless. Yahoo has several different ways to measure its profits, but all of them have been getting worse over the past two years:
At a time when other internet companies have enjoyed healthy growth, Yahoo's numbers are moving in the opposite direction. Analysts who have looked at Yahoo's cash flow think Yahoo's internet business would be worth around $4 billion as a separate company, a small fraction of its value a few years ago. In short, there's no sign that Mayer's turnaround efforts have been working. She has invested heavily in both engineering and media talent. But those investments haven't yet improved Yahoo's bottom line.
The big fear of Yahoo's Wall Street critics isn't just that Yahoo management will fail to turn a profit; it's that they'll burn up billions of dollars in a futile effort to turn Yahoo around. Yahoo has enough cash in the bank to continue its current losses for several more years, and after that it could sell its Alibaba and Yahoo Japan stakes to buy itself many more years of money-losing operation.
But while Yahoo's management and employees obviously like to have a big cash cushion, shareholders aren't interested in endlessly subsidizing a money-losing business. And so over the past year, Wall Street has been steadily ratcheting up the pressure on Mayer to separate Yahoo's core internet business from its stakes in Alibaba and Yahoo Japan.
To mollify Wall Street, Mayer announced a plan last summer to spin off Yahoo's Alibaba shares into a new holding company. Under tax law, a company can spin off part of its business tax-free if it's doing so for a legitimate business purpose, but it can't do so merely as a tax dodge. In the past, the IRS hasn't enforced this rule very strictly, but when Yahoo asked the IRS to bless its spinoff proposal, the IRS demurred. That meant Yahoo could face a multibillion-dollar tax bill. So in December, Yahoo announced that it was canceling the spinoff.
In the goofy world of tax law, another option is a "reverse spinoff." In this plan, Yahoo would essentially spin off the entire company into a new legal entity, leaving the existing corporation as a holding company for Alibaba shares (and possibly Yahoo Japan shares) and a few other assets.
You might think spinning off everything except Alibaba shares would have the same tax implications as spinning off Alibaba shares, but that's not necessarily the case. This could allow shareholders to get their Alibaba shares without the baggage of Yahoo the business, and without paying any taxes on Alibaba's increased value.
In a January letter, the hedge fund Starboard Value was scathing about Mayer's performance. "The management team that was hired to turn around the Core Business has failed to produce acceptable results," the firm wrote.
So Starboard urged Yahoo's board to choose a third option: selling Yahoo's core business to another company.
A spinoff would leave Mayer in control of Yahoo the business, which Starboard views as a disadvantage. The hedge fund believes a new owner could do more with the company than the current management, and would therefore pay more for the company than the current market value.
And Starboard isn't just making an idle suggestion. Starboard is an activist investment firm. Its strategy is to buy a stake in a company and then use it as leverage to force management to make changes. In 2014, for example, Starboard successfully ousted the management of the Olive Garden after writing an epic 300-page slide deck criticizing the company's management. The slide deck faulted the restaurant chain for stale breadsticks and mushy pasta, along with weightier criticisms of its real estate portfolio and business strategy.
Starboard is threatening to take that same approach at Yahoo. "If the Board is unwilling to accept the need for significant change," the company wrote on January 6, "then an election contest may very well be needed so that shareholders can replace a majority of the Board with directors who will represent their best interests."
It's easy to get bogged down in tax and accounting details, but the big-picture lesson here is that Yahoo is failing under Mayer.
If Apple had made a prescient billion-dollar investment in Alibaba 10 years ago, Wall Street wouldn't be pressuring CEO Tim Cook to spin off the shares. And that's not just because Apple's success gives Cook leverage over Wall Street in general. It's also because so long as Apple continued to be profitable, there'd be no reason to worry about Cook raiding profits from one part of the business to cover losses in another part.
But there's a real danger that Mayer — or one of her successors — will dip into those Alibaba profits to help pay for Yahoo's continued losses. And because those Alibaba shares are worth a lot more than the rest of Yahoo put together, Wall Street's top priority is to make sure that a continued meltdown of Yahoo the company won't drain value from Yahoo's investments. The easiest way to do that is to put them into separate companies.
Reasonable people can debate whether the failure of Yahoo's core business is Mayer's fault or whether Yahoo was beyond saving when she arrived. Given the long line of failed CEOs that preceded her, I'm more inclined to go with option two. But either way, it's becoming clear that Mayer's turnaround plan isn't working.
In November, Amazon opened its first bookstore, and reports from the CEO of one of America's largest shopping mall operators Tuesday afternoon suggest that the company is prepared to open several hundred new ones across the country. This prompted many to ask why the company that destroyed the physical bookstore industry would possibly want to operate a physical bookstore.
Part of the answer is that, as the announcement of the original store location said, "At Amazon Books, you can also test drive Amazon’s devices," meaning Kindles, Echos, Fire TVs, and Fire Tablets "are available for you to explore, and Amazon device experts will be on hand to answer questions and to show the products in action." Apple has physical retail stores for its digital devices, as do (albeit less successfully) Microsoft, Sony, and Samsung. Since Amazon makes Amazon-branded devices, why shouldn't it have a store too?
But the bookstore framing is no coincidence, and the reality is that something bigger and more profound is happening than a simple desire to let people window shop for Fire TV sticks. Amazon is interested in bookstores for two big reasons:
For years now, Amazon has been the most terrifying competitor on the planet. And a possible move into physical retail should be taken as a reminder that no business of any kind should view itself as protected from Jeff Bezos's plan for global domination.
Large American technology companies like Apple, Google, Facebook, and Microsoft are ridiculously profitable and, indeed, infamous for the lengths to which they go to avoid paying corporate income tax on their gargantuan profits. Since Amazon is also a large American technology company, it is easy to assume that it, too, must be ridiculously profitable. But this is not the case. The retail industry has traditionally been a very low-margin matter, and Amazon has outcompeted traditional retailers in part by offering even lower margins.
But that's recently changed. In the most recent two quarters, Amazon has earned meaningful profits — profits driven by the success of Amazon Web Services, an enormously popular technology infrastructure company that enjoys tech-like economics rather than retail-like ones. That means 25 percent profit margins on a business that does $2 billion in revenue a quarter and is growing at a 70 percent annual rate.
As the technology industry analyst Ben Thompson put it, Amazon became profitable because "AWS is simply spinning off more cash than Amazon knows what to do with."
The key to understanding Amazon as a business is that earning a profit is antithetical to its corporate culture and mission. Rather than increasing the value of Amazon stock by pushing out cash to shareholders, Bezos's strategy is to increase the value of Amazon by literally making the company bigger. Each year, Amazon owns more warehouses, more customer data, more intellectual property, wider distribution channels, etc., and therefore becomes a more valuable enterprise than it was before.
On occasion, Amazon will turn a profit either to prove to Wall Street that it can, or else because (as with AWS recently) a particular venture simply proves more lucrative than expected.
But that AWS revenue was never going to sit around in the corporate treasury or be paid out as dividends. A surge in revenue needs to be met by a surge in new expenses. Recently prestige video content (Bezos says he wants to win an Oscar) and an effort to create a two-hour delivery service called Amazon Now have been soaking up the extra money. Brick-and-mortar retail is both another potential money sink and also a possible launching pad for Amazon Now services, which are obviously going to require some kind of logistical infrastructure.
So why brick-and-mortar retail? Most likely because Amazon's long-term strategy is simple: It wants everyone, everywhere to buy everything from Amazon.
And it's clear that whatever the struggles of some major big-box retail chains lately, people do in fact continue to buy things in stores. For some people, some of the time, a physical store is where they want to shop. These days you probably could buy everything you need online, but almost nobody actually does. Which means nobody buys everything from Amazon. Which is unacceptable.
So why bookstores? For the same reason Amazon.com was originally an online bookstore. You've got to start somewhere, and the book industry is a relatively soft target. Since Amazon's already basically crushed the national bookstore chains, nobody can really stop the company from getting a foot in the door of this niche.
Ultimately, it might be a total dead end. But even if the effort to establish stores fails, it will be a potentially valuable learning experience. Amazon prides itself on a value it calls "customer obsession," but lacking a physical presence means the company ends up with a somewhat limited view of what its customers look like and how they behave. A retail presence can help change that.
The bigger, less irony-laden thing that Amazon is working on right now is same-day delivery for Amazon Prime members. Currently, same-day delivery is sporadically available — for some products, in some cities, some of the time. It feels kind of like magic when it works, but it's not nearly predictable enough right now to be a real driver of business rather than an impressive occasional delight for customers.
Amazon's long-term aspirations in this field appear to involve fleets of driverless trucks and even flying delivery drones.
But in the human-powered present (and perhaps even in the drone-full future) same-day delivery requires stockpiles of merchandise that are more numerous and located more directly adjacent to population and transportation hubs than the company's existing warehouses. The geography of same-day delivery depots, in other words, looks a lot like the geography of classic big-box stores. You wouldn't have just one Borders serve an entire region. Instead, a given metro area would feature one or more downtown locations plus a bunch of mall spots in the surrounding suburbs. The goal was to ensure that nobody who bought books regularly was ever all that far from a Borders.
Essentially replicating this structure but combining it with Amazon's logistics infrastructure, immense supply-chain bargaining power, vast stockpile of consumer knowledge, and the Prime subscription revenue model is at least a plausible vision of the future. And if the depots can serve as showrooms for Amazon hardware and help get traditional brick-and-mortar shoppers into the Amazon lifestyle, then why not?
Amazon shares plummeted late last week as the company's quarterly earnings report revealed profits that were well below Wall Street's expectations. Notably, it's not as if people stopped buying things through Amazon. Sales grew 22 percent. It's just that analysts had gotten it into their heads that profit margins were going to rise as well. Loyal Vox readers, of course, were warned that this wouldn't happen, but some people don't listen.
I would say that if you want to understand Amazon, you should just look at this chart:
This, to me, does not look like a company that is trying and failing to increase its profits. Nor does it look like a brand new startup that is eschewing short-term profits in order to establish its business. Amazon is a pretty old company, and its profits have never meaningfully differed from zero even as the company gets bigger and bigger.
That's because — drumroll, please — Amazon's leadership, from CEO Jeff Bezos on down, are deliberately redeploying every dollar of revenue Amazon earns into making the company bigger and bigger.
One can debate the wisdom or sustainability of this strategy (I think it's awesome, personally) but it's clearly the strategy. Nothing in life lasts forever, and this will presumably change at some point due to some drastic change of circumstances. But the expectations around Amazon this quarter were much more banal. People decided that because Amazon Web Services is profitable and growing fast, Amazon would start exhibiting fast-growing profits.
The mistake here is in understating Amazon's ambitions. Profits stayed meager because Amazon amped up its spending by 20.5 percent. Company leaders explained that they are spending more on original video content (Bezos wants to win an Oscar) and are trying to create something called Prime Now that will deliver you stuff in two hours rather than two days, which is totally ridiculous. There's no limit to how much you can spend chasing these kind of goals, and absent some kind of fundamental change in the company's structure or legal status, that's where all the money will go.
For years, pundits have speculated that online instruction could begin to overtake traditional higher education, but too often have offered few details about how this would happen. Already, however, you can see a path through which tech companies could gain a foothold in the higher ed market.
Here's one scenario — told through the vantage point of Amazon's Jeff Bezos in 2030. I don't know if it's going to happen, but as you'll see in the footnotes, Amazon is already making moves that could suggest it would be a potent competitor to existing colleges and universities.
To Our Shareowners:
After the spectacular and occasionally criminal failure of several for-profit college companies in the years following the Great Recession1, some critics argued that the pursuit of profit was fundamentally at odds with the mission of high-quality education. Amazon University, now in its 15th year and serving more than a million customers, has proven those critics wrong and delivers higher-quality, lower-cost educational offerings than the once-vaunted nonprofits that continue to charge high tuition in pursuit of a short-term revenue boost.2
A dreamy business offering has at least four characteristics. Customers love it, it can grow to very large size, it has strong returns on capital, and it’s durable in time — with the potential to endure for decades.3 Amazon University meets this standard, and then some.
Like some of our other ventures in the past, Amazon University was started out to fill an internal need.4 We needed our employees to have certain skills. So we found a way to give them just that.
We started out with Career Choice, a benefit offered to employees through which we paid 95 percent of tuition for classes in in-demand fields to help our employees progress in their careers. Within a few years, we started offering classrooms on site.5 From the beginning, it was about filling needs not just at Amazon but in the larger community: The first graduate of the program earned a nursing degree.6
Separately, in 2013, we launched a certification program for various Amazon Web Services engineers, letting companies using AWS easily train or hire workers who could manage AWS infrastructure.7
But we also found that many of our employees, even those with advanced degrees from prestigious universities, were incompetent at core aspects of their jobs here at Amazon. Due to our obsession with rewarding competency, hard work, and results,8 we searched for a solution. Luckily, it was right there in front of us.
In 2013, our subsidiary Zappos became a trailblazer in the use of "badges" to  let employees demonstrate mastery of a skill and earn raises.9 Leaning heavily on that expertise, Amazon, in 2018, decided to revamp Career Choice and build an internal competency-based education system.10 Employees would earn a badge for a discrete skill, and earning a number of related badges awarded mastery of a "track."11 Over time, we created tracks for supply-chain logistics, factory equipment operation, and, eventually, management, accounting, and more.
We then began offering these courses at cost online, and created an official policy within Amazon that allowed for hiring talent at all levels who had Amazon badges in place of, or in addition to, more traditional education credentials.12 We called it Amazon University.
Over time, we found badges to be nearly a prerequisite for hiring in many divisions of the company, and internal surveys demonstrated less and less interest in where or if the potential hire went to college, and more interest in completion of (and scores in) certain tracks.
Again, we offered these products at low prices because we were attempting to improve the pool of potential talent, and we succeeded. Over time, we also learned that other Silicon Valley businesses were using our badge system as a proxy for hiring and promotion, and that over the course of a few years more traditional firms, like General Electric, were doing so as well.13
While we considered lucrative licensing deals for these products, we were, as always, focused on creating lifelong loyal Amazon customers, and therefore continued to offer our educational services at cost and introduced many as free to Prime members.14 Over time, some schools and new nonprofits created wraparound services that included encouragement and a physical sense of community, which helped certain types of students who may not have otherwise completed badges to do exactly that, and we began expanding our physical classroom presence within or near our warehouses.15 We also decided early on to not participate in the federal aid system. A study of the system found the process cumbersome, confusing, and dissatisfying to customers; it also had tended to create bad incentives among institutions participating.16
We continue to be obsessed with the quality of our educational offerings, which is reflected in our students' enthusiasm and persistence. Of those students who completed one badge, 90 percent go on to complete a second, and 75 percent go on to complete a "track.".17 Of those who complete one track, 40 percent go on to complete a second.18 In surveys of customers, 95 percent believe that their badges helped them secure a job or salary increase and were worth the cost,19 while market research suggests that as many as half of major US employers now consider Amazon badges to be one of their top five criteria when determining whom to hire.
Over time we have heard from many customers — or, as we like to call them, alumni — who have told us how important their Amazon education has been to them, and asking how to give back. We tell them the single best thing they can do is to purchase shares in Amazon. Not only does this give the company more capital for us to invest in educational services and continue to offer them at cost, but it also broadens our pool of investors who share our core value of doing everything we can for the customer, thus helping prevent takeover attempts from those seeking short-term profits.20 We also suggest they volunteer their time to nonprofits that assist students and keep them on track in completing their Amazon badges.
Recently, due to customer demand, we have begun to explore offering some courses in the liberal arts and are already offering these courses at a subsidy to select Prime members. We are also investing a significant amount of money into artificial intelligence that could help reduce the need for human evaluators.
The future of Amazon University is strong as we continue to become the first truly global university. As our alumni network and reputation with customers and employers grow, we predict increasing demand for our services. We are excited about the future of a more educated, less indebted citizenry.
Jeffrey P. Bezos
Founder and Chief Executive Officer
Amazon.com, Inc.
April 2030
Alexander Holt is a policy analyst with the Education Policy Program at New America, where he conducts research on the economics of higher education.

Be sure to subscribe to Vox on YouTube for more explainer videos
A new study finds that sexism is rampant in the tech industry, with almost two-thirds of women reporting sexual harassment and nearly 90 percent reporting demeaning comments from male colleagues.
The study, called "Elephant in the Valley," surveyed 200 women who work at tech companies, including large companies like Google and Apple as well as startups. The study focused on women who had 10 years of experience in the industry, and most worked in Silicon Valley.
The project was inspired by former Reddit CEO Ellen Pao's failed gender discrimination suit against her former venture capital firm. Two co-authors of the study, former Yahoo executive Michele Madansky and Pao's former co-worker Trae Vassallo, said that after Pao's suit they started hearing an outpouring of stories from their peers about harassment and other problems.
"What we realized is that while many women shared similar workplace stories, most men were simply shocked and unaware of the issues facing women in the workplace," the report said. So the authors decided to go out and get some data. They asked women about their experiences in five main areas: feedback and promotion, inclusion, unconscious biases, motherhood, and harassment and safety.
The results were startling. Sixty percent of the women surveyed had experienced sexual harassment. (That's about twice as often as women report harassment overall.) The vast majority of the women surveyed said they'd had demeaning comments made to them by male colleagues, or that they'd witnessed sexist behavior at company offsites or industry conferences. Significant numbers of women also felt pressured based on their actual or perceived family choices, or even felt physically unsafe at work.

<!--
new pym.Parent('vox-women-tech-discrimination__graphic', '//apps.voxmedia.com/at/vox-women-tech-discrimination/', {xdomain: '.*\.voxmedia\.com'});
// -->

Some of the sexist incidents were subtle but still infuriating. "At Company X we had a joke that there were only two reviews for women — you are either too reticent or you are too bossy — no middle ground," said one respondent. Another woman, a venture capitalist, described a pitch meeting with her two male colleagues and a male founder: "Despite my background/skill set being clearly the most relevant, the founder didn't make eye contact, and didn't really listen to the questions I asked before answering." Another had male colleagues tell her that once women get pregnant they become "irrelevant."
Other examples were more egregious. One woman was groped by her boss at a company event, reported it, and had to leave the company after being retaliated against. Another had a client ask her to sit on his lap in exchange for buying her products.
Women in science, math, engineering, or technology have a hard time at just about every level of hiring for academic positions, due to the unconscious bias of those who do the hiring. There's overt sexism, like that awful peer reviewer who said women researchers should get help from men, and subtler forms, like how male researchers tend to choose fewer female trainees to work in their labs. And the STEM bias starts as early as grade school.
This usually isn't because people intentionally discriminate against women and girls. Harvard has a fascinating research project that tries to measure unconscious biases. You may believe with all your heart that women and men are equally capable at science or math, Harvard's researchers say, but your automatic associations may show otherwise. This explains how even a feminist science teacher who knows she is being observed can still give a disproportionate amount of talking time to the white male students in her classroom.
Pao argued in November that the low numbers of women in technology fields isn't just a "pipeline problem" caused by lack of interest among women. The problem is that women are treated badly at every level of entry to these fields. One study found that an uncivil workplace culture systematically pushes women out of engineering fields. While 20 percent of engineering graduates are women, just 11 percent of all engineers are women.
Pao also said that sexism in Silicon Valley is getting better — not really in the sense that men are doing less of it, but more that women and other marginalized groups are banding together to speak out and fight back against it. The culture of silence around these issues is starting to crack, and people are bringing consciousness to unconscious bias.
Conventional maps, by their nature, emphasize sheer spaciousness even though many large areas can be relatively devoid of human beings or other forms of activity. This cool diagram from HowMuch.net gives us a different way to visualize the entire US economy, depicting the whole thing as a big circle and then slicing it up by state, with each state's area representing its share of total economic output:
The basic news that California and Texas are really big shouldn't come as a huge shock. But you also see that Florida punches a bit below its weight in terms of population, in part because a large share of the state's residents are retired.
There are also disparities related to wealth. New Jersey has fewer people than North Carolina, Michigan, or Georgia, but it contributes more to the national economy since the productivity per worker in New Jersey is much higher.
At 10:59 pm Eastern time, the winning numbers in the Powerball lottery will be announced, with the current jackpot up to a record high of $1.5 billion after the past several drawings have failed to produce a winner.
Lottery jackpots have grown in recent years, with all 10 of the biggest nominal hauls coming since 2012 and inflation adjustments making little difference to the overall rankings.
Perhaps the most important thing to know about lotteries in the United States is that the winner of a $1.5 billion jackpot doesn't get $1.5 billion. Instead, what he or she gets is the choice between an annuity that pays out a total of $1.5 billion over the course of 30 years or a cash prize of $930 million right away. And that's before taxes. Depending on where the winner lives, he or she will actually end up getting around half of that.
Most winners choose the lump sum of cash, but most people who do so are making a poor financial decision. (In general, playing the lottery is a poor financial decision, so this is perhaps unsurprising.) As Josh Barro argues, the annuity benefits from what amounts to favorable tax treatment of investment income and will likely come out ahead of any reasonable conservative strategy you could take with your lump sum. A risky investment strategy might work well, but risky investment strategies are, well, risky, and lottery players do not have a great track record of managing risky investments.
Total spending on lotteries amounted to a shocking $70 billion in 2014, which amounts to about $300 per adult. As Derek Thompson wrote last year for the Atlantic, there is tremendous state-by-state variation in lottery spending, with Rhode Island and South Dakota spending huge sums on a per capita basis while North Dakota and Oklahoma spend relatively little.
The lottery is a famously regressive source of state government revenue, with the poor spending  a much larger share of their income on lottery tickets. But a 2003 study by economist Emily Oster suggests that massive lottery jackpots may be more egalitarian in their distributive impact. Oster found that the regressive nature of the lottery as a revenue source is driven by the downscale demographics of very frequent lottery players. When jackpots get bigger, more people are induced to play, and the regressivity diminishes — though it doesn't vanish.
Oster found that, hypothetically, a jackpot of about $806 million would likely be large enough to make the lottery progressive. At the time of her study, no jackpot of that level had ever been recorded, so the analysis was purely speculative. But now that we are actually well above the threshold she posited, future researchers could and should do follow-up analysis to see if rich jackpots really do lead to a more equal outcome.
On January 11, staffers of the New Republic learned via a note to the staff and a post on Medium that their beleaguered owner Chris Hughes had decided to sell the publication.
That announcement came a bit more than a year after a massive shake-up at the magazine, in which Hughes tried to replace top editors Franklin Foer and Leon Wieseltier with more digitally minded recruits, only to provoke mass resignations from then-current senior staffers and an immense backlash from the magazine's extensive network of alumni.
The general theme of these alumni takes was that Hughes had not only made a decision they disagreed with, but had also literally destroyed the publication. Jonathan Chait called his appreciation of the magazine's legacy a "eulogy," which was actually restrained compared with novelist Cynthia Ozick, who commemorated the change with an original poem alleging that "Thought and Word lay dead and cold" in the wake of Hughes's management.
This in turn provoked a counter-backlash from left-wing critics of TNR and its former owner Martin Peretz. "A publication that buoyed anti-black, anti-Latino, anti-Arab, Islamophobic racism was tolerable; a publication that fired two beloved white men was not," as Vox's Max Fisher put it, not so much in defense of Hughes as in incredulity over the scale of the anti-Hughes rhetoric.
SNORT RT @shani_o: So, is TNR going to become more diverse now or…?
The resulting fray was deeply confusing to many readers who are not professional journalists — and even to many journalists under the age of 30 — who couldn't quite understand why management turmoil at a single low-circulation publication could possibly be worthy of so many pixels and takes.
At a basic level, the sheer scale and clout of the TNR alumni network explains the level of attention it got. The magazine long served as an important incubator of talent, a place whose staffers would leave to become writers and editors at higher-circulation publications but who retained enormous affection for each other and the publication.
But at a larger level, "TNRmageddon" has attracted the intense interest of 30-something to 60-something politically minded journalists because the magazine and its woes stood — usually self-consciously so — at the intersection of a number of ideological and demographic trends that are profoundly shaping the broader political culture.
TNR stood for a certain model of publishing, but also for a certain model of liberalism whose viability is diminishing under strain from partisan polarization and increasing demands that an ideological movement whose political power comes mostly from women and minorities be represented in public by figures other than white men.
The world of Washington, DC-based magazines and websites is incredibly incestuous, so I have no way of writing about this without stepping all over too many conflicts of interest to count.
But some noteworthy ones include the fact that I applied for a job with the Peter Beinart–era TNR and didn't get it after a disastrous job interview. I was recruited for jobs in both the first and second Foer eras. I used to work closely with Richard Just (who was editor between Foer stints) before he worked at TNR. I dated a TNR staffer for a while, was roommates with Spencer Ackerman at the time Foer fired him from TNR, and am very close friends with a current staffer at TNR. I also lived in the same dorm with Hughes for a year in college.
All of which is to say that while my coverage of this can hardly be objective, it's also pretty well-informed.
The small-minded literal answer is that both Foer and Wieseltier are very well-liked and well-connected in the journalistic and literary worlds, and when they got fired they deployed their connections to torch Hughes.
The broader, more important, reason is that the New Republic has long been at the center of profound arguments over the direction of American liberalism. It was founded in 1914 by Herbert Croly, Walter Lippmann, and Walter Weyl with financing from Willard Straight and Dorothy Payne Whitney, and it's really only a slight exaggeration to say that the magazine's ideological program laid the foundation for what we now know as the ideology of modern liberalism.
Croly sought a synthesis of the political traditions of Thomas Jefferson and Alexander Hamilton, promoting the view that Hamiltonian means (an active state and a strong central government) were necessary to promote Jeffersonian ends (small-d democracy and an egalitarian economy) under the conditions of modern industrial life. Many of the specific policy views of the Croly-era New Republic would be unrecognizable to the liberalism of a century later, but those broad themes are very evident today and even the subject of a hit musical.
In the 1940s, '50s, and '60s TNR was involved in the struggle to define liberalism's relationship to the Cold War. In the mid-1970s, the magazine was bought by Martin Peretz, who used it to once again try to chart an ideological agenda for liberalism. Under Peretz, the magazine stood for a hawkish, fervently pro-Israel form of liberalism that, while fairly conventional in its domestic politics, was always on the lookout for excessive left-wing deviationism. It sought controversy and made enemies — though it also left a broad network of friends scattered throughout prestige media in the United States.
But by the mid-aughts, Peretz and the magazine were in a state of financial distress; the magazine suffered a series of rounds of budget cuts and was taken over by an unstable set of investors backing Peretz. In 2012, Chris Hughes bought the magazine, provided a much-needed infusion of cash, brought back beloved former editor Frank Foer, and invested in some marquee journalists and new office space.
He was, briefly, hailed as a savior of the magazine. And then it all went wrong.
TNRmageddon has come to be at the nexus of a wide range of ideological conflicts, but on the most literal level it was about something else entirely. Simply put, from day one Hughes had always wanted to own a version of the New Republic that would be a relevant digital brand in the contemporary world, and Foer was always an editor with a print-first mentality and a deep loyalty to a core group of writers who felt they did their best work at a considerably more leisurely pace than you find at most websites.
This was not primarily a disagreement about ideology, or even one about commercial versus noncommercial values. A number of noncommercial publications — including Mother Jones on the left and National Review on the right — have successfully built large and influential digital operations on the backs of old-time print brands. Commercial publications like the Atlantic and New York magazine have done the same.
Conversely, there are both commercial (the New Yorker) and noncommercial (N+1) publications that have deliberately chosen to march into 2016 still offering a distinctly analog mentality.
The original sin of Foer and Hughes's relationship was that they simply didn't see eye to eye on this.
Hughes is a millennial, a digital native who made his fortune in Facebook stock. To him it is natural that to be a force in the media world means to be a digital force. Foer is older, but more importantly he is a proud neo-traditionalist who thinks Amazon is destroying literary culture, and during both of his editorships he treated digital work as less important than the job of editing the print magazine.
There's nothing wrong with either of these approaches, but the Hughes-Foer TNR renaissance was fundamentally an exercise in mutual deception and self-deception. Hughes brought back Foer in order to gain the prestige and cachet that would come with the acclamation of the TNR alumni network, even though he had no real affection for the kind of journalism Foer values; Foer told Hughes what he wanted to hear in order to get the funding for the magazine of his dreams, even though he had no real affection for the kind of digital publication that Hughes wanted.
When the deception became untenable, Hughes massively mishandled the aftermath. Rather than arranging an amicable parting of ways, he brought in a new CEO, Guy Vidra, with experience at Yahoo who, as Ryan Lizza recounts, "spoke in a Silicon Valley-inflected jargon that many of T.N.R.’s journalists found grating and bewildering."
Foer himself discovered that Hughes wanted to replace him with a new editor through leaks rather than a face-to-face conversation. (This method of firing, while appallingly rude, is actually itself a bit of a TNR tradition, as former editor Charles Lane apparently learned he'd been replaced by Beinart from a Washington Post column rather than from Peretz.)
In large part as a result of his own mishandling of the situation, Hughes wound up losing more than a couple of top editors: An enormous fraction of the magazine's staff — including relatively new Hughes-era hires like Julia Ioffe — resigned en masse.
Rather than break up politely citing irreconcilable differences over digital-first versus print-first editorial strategies, the TNR quitters and their allies in the old guard developed a curious alternate history of the New Republic over the past generation.
In this view, TNR was not a perennially money-losing ego project through which Peretz subsidized a prestigious journalism operation in order to lend a sheen of respectability and influence to his own crank view. It was, instead, a "public trust" that had been owned by wise stewards who, in the words of former TNR senior editor John Judis, had "devoted themselves to philanthropy and to what they understood as the public interest." In contrast, Judis wrote, Hughes's "commitment to social responsibility turned out to be skin-deep."
In this retconning of New Republic history, both Peretz's rancid political views and his mercurial hiring and firing of not-always-obviously-qualified editors vanished down the memory hole. After all, if one acknowledges that Peretz had treated the magazine as his personal property (which, after all, it was), then one would have to acknowledge that Hughes was just as entitled to be fickle as his predecessor.
In fact, the owner who fit Judis's description was not Peretz but Hughes, who, unlike Peretz, is deeply involved in philanthropic pursuits (for example, giving cash grants to desperately poor people in rural East Africa) and does not fancy himself a writer or crave his own byline.
What neither side of the dispute really wants to forthrightly acknowledge is the likely connection between Hughes's shifting view of his investment in the New Republic and his husband's failure to win a congressional seat in the 2014 midterms. Hughes was willing to spend generously on Eldridge's political career, outspending Eldridge's opponent 3 to 1 in a district Barack Obama carried in 2012 and contributing ancillary spending on real estate and local charitable endeavors to grease the wheels for Eldridge's run.
In a world where Hughes and Eldridge were a young political power couple, making an open-ended financial commitment to a Washington-based prestige journalism operation made a lot of sense. It was something Hughes could do with his time and money that wouldn't enmesh him in ethical conflicts of interest, and it would allow Eldridge to punch greatly above his weight as a backbench member of the minority party.
With Eldridge's political career in shambles and the couple reevaluating their priorities, the calculus obviously looks different. For Peretz, one major virtue of owning TNR was that it afforded him an opportunity to grind his favored ideological axes. In 2016, any rich guy who wants to gain attention with ideological ax grinding can do it for free on Twitter or Medium. And Hughes doesn't even appear to have any particularly interesting axes to grind. So what's the point?
The result of this, it seems, was that Hughes's willingness to absorb the losses necessary to run the New Republic dimmed. In his announcement that he was putting the magazine up for sale, he said he's invested $20 million in the institution just in the short time he's owned it, and the implication is he's no longer willing to lose that kind of money indefinitely. To many of his critics, that is the core of his betrayal — by purchasing the magazine in the first place, he was committing himself to bearing its attendant losses, perhaps not indefinitely, but certainly for longer than a few years.
Once the anti-Hughes takes started flying from the TNR old guard, the magazine's owner was clearly in need of allies and swiftly found himself making an alliance of convenience with a group of longtime TNR critics who didn't appreciate the magazine's approach to foreign policy and, especially, race.
Arguments about Peretz-era TNR's content have raged for years and will continue to do so. But one fact that seems essentially indisputable is that Peretz-era TNR was well-known as an incubator of journalistic talent, and the talent that was incubated there was essentially all white and very largely male — unapologetically so, as one of the magazine's main ideological heterodoxies was opposition to affirmative action.
Consequently, as Ta-Nehisi Coates put it during the December 2014 meltdown, "the family rows at TNR's virtual funeral look like the 'Whites Only' section of a Jim Crow-era movie-house."
This critique of TNR and race dovetailed nicely with liberals' longstanding critique of the New Republic's hawkish foreign policy, which was ostensibly animated by high-minded idealism but often seemed grounded in Peretz's crude anti-Arab racism.
If Hughes had been genuinely bothered by these longstanding concerns, he presumably wouldn't have bought the magazine to begin with. But once on the offensive, he and new editor Gabriel Snyder decided to go to war with the allies they had and join the pile-on. Version 2.0 of the Hughes-era TNR featured a much more diverse set of hires, and kicked off with a Jeet Heer feature criticizing TNR's historical record on race.
Foer's allies then mounted a backlash to the backlash, which mostly went to show how real the ideological differences between the magazine's new hires and its ex-staffers had become:
Am excited to see if the new @tnr can write about anything other than identity politics. Like, you know, the rest of the world.
Perhaps the central irony of TNRmageddon is that the scorched earth tactics Foer's allies in the old guard have deployed have fundamentally imperiled the viability of the magazine and its legacy.
After all, the one thing TNR has going for it is the idea of cultural prestige — a bank of influence and importance that long predates the Peretz era and that managed to survive several ownership transitions and many firings of many editors. The mass campaign of op-eds, interviews, and takes proclaiming the death of the New Republic served to massively undermine that prestige.
This succeeded in hurting Hughes's personal standing in society as well as his financial interests, but fundamentally Hughes is still going to be a rich guy living a decent life.
What it also did was massively undermine Hughes's interest in indefinitely subsidizing a prestige journalism play that was no longer bringing him prestige. Worse, it probably undermined anyone else's interest in taking on the burdens of owning TNR.
This is in part a question of money, but in a larger sense it's about risks to reputation. Virtually any other monetary investment in journalism would be less likely to invite loud public second-guessing. The Peretz-era alumni network went after Hughes so viciously that they've made it much less likely anything vibrant will ever thrive under the TNR brand.
The good news for people who enjoy small-circulation, ideas-focused magazines is that, TNR's struggles aside, this genre of publication is thriving perhaps as never before. It's true that niche publishing is an awkward fit for the commercial logic of the internet, where scale rules all. But it's also true that this sort of publication has never been a truly commercial endeavor, whether formally organized as a nonprofit or not. Digital technology, by drastically reducing the cost of publishing and distributing articles, has served to drastically increase the bang for your buck involved in financing such publications.
Consequently, there's been a blossoming of new ideas-focused small publications like New Inquiry, N+1, and Jacobin, joined by a revived version of the Baffler often espousing left-wing ideas well out of the mainstream of the commercial press. The right has its own high-quality journals — of which National Affairs is probably the best — while Democracy holds the torch for a more conventional form of liberalism.
When I entered journalism there were three places I badly wanted to work: The American Prospect, the Washington Monthly, and the New Republic. The decision turned out to be easy: TNR never called me back. The Washington Monthly didn't have entry-level positions. But I got lucky: The Prospect took me on as a writing fellow, and it turned out to be a dream job.
Ten years later, the Prospect, the Monthly, and TNR are all in crisis. The Prospect laid off much of its staff and is retrenching to its roots as a policy journal. The Washington Monthly has downsized to a bimonthly. The New Republic was bought by Facebook-founder Chris Hughes only to become a cautionary tale when Hughes ineptly fired its editor Frank Foer — a move that caused massive staff resignations and a devastating backlash from the TNR-veterans laced throughout the rest of the media.
Now Hughes is putting the magazine back up for sale.
"I will be the first to admit that when I took on this challenge nearly four years ago, I underestimated the difficulty of transitioning an old and traditional institution into a digital media company in today's quickly evolving climate," Hughes wrote in an article that, oddly, was published on Medium rather than the New Republic's web site.
TNR's problems have been largely being laid at Hughes's doorstep. And, to some degree, that's fair. Transitioning an old and traditional institution into a digital media company is hard, but it's not as hard as Hughes made it look.
Marty Peretz, who owned the New Republic from 1974 to 2002, fired a slew of editors without causing mass resignations among the staff (though, as my colleague Max Fisher has written, the contrast between the indulgence of Peretz's racism and the backlash to Hughes does not reflect well on TNR, or journalism more broadly.). And plenty of other venerable institutions — the Atlantic, the Washington Post, the New York Times, National Geographic, etc — have made the leap to digital without TNR-ian tumult.
What happened at the New Republic was a colossal management failure compounded by cultural tensions between the journalism and technology worlds, and it was the job of Hughes, and those he hired, to make sure that didn't happen.
But, to reprise an argument I made amidst TNR's 2014 crisis, there's a deeper story here too. The New Republic was sold to Hughes in the first place because the institution was bleeding so much money that Peretz, its previous owner, couldn't sustain it. And Hughes's alarm over the magazine's future — the alarm that led him to bring in Guy Vidra, an ex-Yahoo executive, to lead the institution, and that convinced him he needed to fire Foer — was based on watching the magazine's influence wane, its traffic lag behind competitors, and its business prospects dim.
Behind the crisis caused by Hughes's management, in other words, was the crisis that led to Hughes's management.

The American Prospect.
TNR.com might flourish under its next owner. But it won't be what the New Republic was. And that's because the thing the New Republic was has already died. The eulogy that needs to be written isn't for the New Republic. It's for the New Republic and The American Prospect and the Washington Monthly and their peers. It's for the role once played by Washington's small fleet of ambitious policy magazines.
This sprawling conversation over Washington policymaking used to be centered in a handful of elite-focused policy magazines, of which the New Republic was perhaps the best known and most ambitious. There was policy reporting in the newspapers, of course, but it didn't go as deep, and it was crippled by the faux-evenhandedness that is death to any serious conversation over solutions. There were the journals, like The Public Interest, but they came out more rarely, and their tone was often staid and impenetrable.
The policy magazines had a number of unique characteristics, but two were central. The first was what they covered — which was, for the most part, politics through the lens of policy (though the New Republic, in particular, had an amazing culture section).
Policy — wrongly, in my view — was broadly considered a boring topic in journalism, and so if you wanted to keep large masses of readers engaged, you couldn't do too much of it, too often. But these magazines could, because they weren't trying to keep large masses of readers engaged. They were niche products comfortable with small, enthusiastic audiences. They were by people who loved writing about policy and for people who loved reading about policy, and that allowed them to serve their audience in a way mass-market publications couldn't touch.
The second was the angle that animated their coverage. The American Prospect was labor-liberalism. The Washington Monthly was technocratic neoliberalism. The New Republic oscillated from editor to editor, but tended towards a hawkish, contrarian neoliberalism (hence the "Even the liberal New Republic" meme). The Nation, which is based in New York, and Mother Jones, which is based in San Francisco, were a bit less policy-oriented, but a lot more liberal.
On the right, you had (and still have) the National Review, which tended towards conservative fusionism. The Public Interest was the birthplace of neoconservatism, and that mantle was later taken on by the Weekly Standard. (The most interesting policy publication on the right at the moment is Yuval Levin's reformist-inclined journal National Affairs.)
These magazines, in other words, stood for something. That set them apart from newspapers and general-interest magazines, which were carefully neutral in ways that made their coverage both less vibrant and less clear. It's very, very hard to cover policy topics well if you can't, at the end, say which solutions you think most promising.
People often talk about the problems of the transition to digital in broad terms, but for policy magazines, the transition to digital created specific problems: namely, the qualities that once set organizations like the New Republic apart were adopted by their bigger, richer competitors.

Wha'cha reading? (JEWEL SAMAD/AFP/Getty Images)
To the wonk in the 1970s, '80s, and '90s, these magazines were the thrumming center of the policy conversation in Washington. That's why it was believable that the New Republic was the "in-flight reading on Air Force One." President Bill Clinton was a policy wonk. What else was he going to read?
But they're no longer the center of the policy conversation in Washington. That conversation has spilled online, beyond their pages, outside their borders. The in-flight reading on Air Force One is probably saved to President Obama's iPad.
And he has much to choose from. The internet is now thick with outlets that pride themselves on covering Washington's vast policymaking apparatus in much the way the policy magazines once did, and often with staff culled from those same policy magazines.
Vox is one of those outlets, as is the New Republic, but so are the Washington Post's Wonkblog, the New York Times' Upshot, Politico, Bloomberg View, and FiveThirtyEight, to name just a few. Many of these publications are attached to bigger institutions that would never have published so much policy, or allowed such conclusions-driven coverage, in their print products.
And that doesn't even include the individual bloggers who are must-reads if you're following policy: Kevin Drum, and Tyler Cowen, and Brad DeLong, and Paul Krugman, and Ross Douthat, and Ramesh Ponnuru, and Jonathan Chait, and Scott Sumner, and Megan McArdle, and Jonathan Bernstein, and, again, the list goes on. At another time, most of these names would be published inside policy magazines. Now they're either employed by bigger institutions or forging ahead on their own blogs, supported by foundations, advertising, and speaking fees.
The other problem is on the business side. Hughes believed his charge was to make TNR a viable web publication, in a world where viability — and, arguably, influence — requires web traffic. That meant publishing more, publishing faster, and publishing the kinds of quick hits and aggregations that help build audience on the cheap.
In a strange sentence on his Medium post, Hughes writes, "Even though our search for a workable business model has come up short, we have shown that digital journalism isn't at odds with quality and depth." But if digital journalism of quality and depth is at odds with a workable business model, then digital journalism of quality and depth won't survive.
This was, according to many accounts, the central tension between Hughes and Foer: Hughes wanted to solve TNR's business problems in part by increasing the size of its audience, which meant it had to to become more like its audience-hungry competitors. But many at TNR felt Hughes had promised to preserve the TNR of yore, even if it lost money indefinitely — as it had in the past. To them, forcing TNR into a fight for audience was forcing it into a fight that would destroy what made it unique even as it failed to create a model on which TNR could thrive.
The New Republic, for better and for worse, has stood for less in recent years. So too do its competitors, who sound a bit more like everyone else and a bit less like themselves.
This isn't a criticism of Foer, who led TNR to publish great journalism. And nor is it a criticism of Snyder, who pushed the magazine in new and interesting directions. But the TNR that stood for so much routinely saw subscriber numbers of 100,000 or less. It would never fulfill its publisher's hopes of acquiring tens of millions of monthly digital readers.
Behind this fight is a deeper tension in digital journalism: The pressure for convergence is strong
Behind this fight is a deeper tension in digital journalism: The pressure for convergence is strong. We feel it at Vox, and sometimes give into it. It's easy to see which stories are resonating with readers. It's obvious that John Oliver videos do big numbers. And that's fine. Right now, almost all successful digital publications are partially built on internet best practices and partially built on that publication's particular obsessions, ideas, and attitude. Digital publications need to be smart about their mix of what everyone else does and what no one else does.
But what made the New Republic and its peer policy magazines so great was how restlessly, relentlessly idiosyncratic they were — that's how they drove new ideologies and new ideas to the fore. They were worse at covering policy than their digital successors because they were slower and more distant from the news cycle, but they were probably better at thinking.
Part of this was because they simply cared less what the audience thought — they saw their role as telling their audience what to think, and they expected a readership in the low six or high five figures, not the mid-eight figures. That gave them a freedom to truly be themselves that more mass-market publications don't have.
As someone who really loathed a number of TNR's previous eras (see the Bell Curve, or No Exit, or A Fighting Faith, or much of what Hughes's predecessor Marty Peretz wrote, for examples), I'm probably a bit less nostalgic for its past. But something is being lost in the transition from policy magazines to policy web sites, and it's still an open question how much of it can be regained.
For TNR, however, the problem is more specific. There's so much expectation and history attached to the publication that its room for strategic movement is limited — as Chris Hughes found out. But an owner who wants to buy and support the TNR of yesteryear is buying into a market that makes it much harder for the TNR of yesteryear to thrive.
Related: How the American Prospect changed policy journalism.
Correction: This article initially said Marty Peretz was Hughes's predecessor as owner of the New Republic. Peretz owned the magazine from 1974 to 2002, when he sold majority control to Michael Steinhardt and Roger Hertog. Peretz bought back the magazine later in the decade, and stepped down as editor-in-chief in 2011.
Inequality is perhaps my favorite interview topic in Silicon Valley: It's fascinating to watch ordinarily confident titans of industry squirm in their seats before offering a conspicuously measured response. But occasionally a leader in the community breaks an unspoken rule by being brutally honest in public.
The tech world encountered one of these deliciously informative moments earlier this week, when a well-known startup investor and mentor, Paul Graham, admitted that he was personally (and unabashedly) responsible for rising inequality.
"I've become an expert on how to increase economic inequality, and I've spent the past decade working hard to do it," Graham wrote. "Eliminating great variations in wealth would mean eliminating startups."
In response, friends carefully avoided denouncing Graham or his core beliefs, but disagreed with his approach.
"Yes, income inequality exists and yes it’s a natural consequence of capitalism and other forms of government are decidedly worse than capitalism because they inefficiently create and allocate resources," wrote fellow investor Mark Suster. "But the celebratory nature of today’s conversation felt tone deaf."
Of course, it’s hard to know whether a handful of blog posts really represent the views of Graham's fellow technology moguls. The plural of anecdote is not data. But for the past five years, I've been systematically collecting data on what Silicon Valley believes through interviews with CEOs, including Graham's colleagues in the startup world.
After dozens of interviews with new and big-name tech startup founders, I designed a structured battery of political and philosophical questions and randomly selected people from an exhaustive database of funded companies (more details on the methods here).
What emerged from my interviews and survey results was a set of views that are somewhat more nuanced than Graham’s, but also in agreement with his fundamental view of the world. Founders believe that equality of opportunity is crucial to a fair and healthy economy, while equality of outcome is economically paralyzing.
They believe that a relatively small slice of geniuses advance humanity more than the combined efforts of everyone else, and that economic growth is better at improving the overall quality of life than burdensome redistribution schemes.
And many believe that the best long-term solution to inequality may be a guaranteed basic minimum income, which minimizes regulation on innovation but ensures that the masses are well-off.
It’s little surprise that a group of people who grew wealthy building successful businesses have a positive view of the economic system that made that success possible. And they see more growth as the solution to broader social problems.
"If we have 4 percent a year of GDP growth, all these problems would get solved," PayPal billionaire Peter Thiel told me when I quizzed him about inequality.
A plurality of founders agree: Among 33 founders I surveyed, 48 percent said that mediocre growth was more problematic than financial inequality, while 42 percent believed the opposite. Among the general population (as represented by 595 people polled on SurveyMonkey), 59 percent of people believe inequality is more important.
Silicon Valley is an optimistic crowd. In my survey, 80 percent of the 129 startup founders I surveyed told me "almost all change is good over the long run," compared with just 48 percent of the general public.
And it’s not as though Valley folks are tone deaf to the problems of inequality. I asked tech founders and members of the public to tell me which of four goals is the best way to improve the world: reducing inequality, addressing threats to national security, reducing government intervention, or getting citizens more active.
Interestingly, founders were more likely than the general public to choose inequality as their top issue (37 percent to 32 percent). They were less likely to say that cutting government (47 percent to 38 percent) or beefing up national security (17 percent to 9 percent) was of paramount importance.
But the biggest difference was on citizen involvement. Twenty-four percent of startup founders saw that as the most important issue, compared with just 11 percent of the general public. Founders have a unique optimism that having an active and informed citizenry can make the world a better place.
So far, this might give you the impression that startup founders have garden-variety liberal politics. And it’s true that many Silicon Valley moguls lean to the left. But there’s one big way that Silicon Valley’s elite see the world differently than a lot of others on the political left.
For tech CEOs, the most comfortable response to the growing economic gap is to support "equality of opportunity, not equality of outcome." At some point in my research process, I got tired of hearing this response. Who isn’t in favor of equal opportunity? But over time I came to realize that Silicon Valley elites view issues of opportunity and accomplishment differently than most people.
Toward the end of my research, I asked tech CEOs to tell me how equal or unequal an economy would be in a perfectly meritocratic society where everyone's income was precisely proportional to their productivity. I asked this question of 14 tech founders (including one billionaire), and all predicted that a meritocracy would lead to a very unequal economy. Most said that the top 10 percent of talent would naturally earn more than 50 percent of the nation's wealth.
"An uninspired population is a stagnant population. Inequality breeds creativity, and fosters motivation to change one's situation," wrote Byron Morgan, founder of the music startup Vinylmint. "Mass change starts with one person inspiring another."
This is perhaps a more artful way to articulate the point Graham was trying to make when he wrote, "Most people who get rich tend to be fairly driven. Whatever their other flaws, laziness is usually not one of them."
The idea that workers have radically different levels of productivity is so commonplace in the Bay Area that Red Bull even referenced it in an ad, cheekily suggesting that drinking its product would transform a 10Xer — someone who was 10 times as productive as the average programmer — into a hyperproductive 100Xer.
This also helps explain Silicon Valley's continued obsession with Massive Open Online Courses, which have been repeatedly shown to be much worse for low-income students. In a moment of rare honesty, MIT's Andrew McAfee told a crowd in San Francisco that MOOCs' actual promise is to be "diamond finders" — large nets that give opportunity to the rare geniuses born into poor circumstances.
"Very few are contributing enormous amounts to the greater good, be it by starting important companies or leading important causes," one of my survey respondents wrote.
During a Twitter discussion of Graham's essay, one billionaire pointed me toward an essay that Graham's colleague, Sam Altman, had penned on the same subject. It's a more compassionate version of the same arguments, and Altman floats an idea that is increasingly popular among his elite friends: a basic minimum income, paid for by heavily taxing the rich.
If not everyone can contribute to the economy, the best-case scenario is to just subsidize the entire world with a respectable quality of life (what is lovingly known as "automated luxury communism"). In the kind of healthy, fast-growing economy Thiel and others envision, the economic pie would be growing quickly and there would be plenty of wealth to go around.
But fundamentally, Paul Graham is not much of an outlier among Silicon Valley’s elites when it comes to the relationship between ability and financial rewards. Most successful technology moguls know better than to be as blunt as Graham, but deep down a lot of them believe that a small minority — like them — create a hugely disproportionate share of the world’s wealth.
December's job growth numbers are in, and they make it official: 2015 was the second-strongest year for job growth since the 1990s, and only slightly behind the big gains of 2014. Unemployment fell in 2015 from an already low 5.6 percent at the end of 2014 to 5 percent in December 2015. Wages grew 2.5 percent during 2015, which isn't a huge number but looks more impressive when you remember that inflation was close to zero for the year.
So what accounts for the second straight year of strong economic results? The US economy is a complex system, so it would be a mistake to point to any single factor as driving economic growth and job creation. No one fully understands how and why economies grow. And to some extent, you could look at 2015's solid but not spectacular performance as the kind of thing that happens when there's nothing holding the economy back.
But we can also identify several specific factors that positively influenced economic growth in 2015.
Chung Sung-Jun / Getty Images News
Short-term interest rates fell to zero percent in 2008, and the Federal Reserve kept them there until December 2015. Lower interest rates tend to promote economic growth and job creation. Some people believe the Fed should have done even more — earlier in the recession the Fed ran a series of "quantitative easing" programs to pump even more money into the economy, which it phased out in 2014 — but there's little doubt that the Fed's decision to keep interest rates near zero percent for most of the year promoted faster job growth than an earlier interest rate hike would have done.
In December, the Federal Reserve raised its target interest rate by 0.25 percent, and signaled that it may increase rates further in 2016. That may create a drag on the economy this year.
Oil prices were high — around $100 per barrel from 2011 until mid-2014. But then prices started to fall, and they haven't stopped since. They were around $50 per barrel when 2015 began, and they've now fallen below $35 per barrel.
Energy is an important input to lots of different products and services, so cheaper oil (and other fossil fuels like natural gas) meant that everyone not associated with the oil industry had a bit of extra cash in their pockets in 2015.
It's hard to say exactly where that extra cash went — at least some of it went to boost people's savings and pay down debt — but consumers also spent some of it on other stuff. That provided a nice economic tailwind throughout 2015.
Since the Great Recession, state and local governments have been tightening their belts. Early in the recession, this added to the magnitude of job losses; later, it partially offset job gains in the private sector.
But as this data from the Brookings Institution shows, things started to change in mid-2014. After years of shedding employees, state and local governments started hiring again.
Overall, Brookings estimates that federal, state, and local government spending and tax policies exerted a modestly positive effect on the growth of gross domestic product — that's after four years of spending cuts and tax hikes that Brookings argues imposed a net fiscal drag on economic growth.
At 10:59 pm Eastern on Saturday, January 9, there will be a Powerball lottery drawing for an estimated $800 million jackpot.
That's the biggest jackpot in history, even when adjusted for inflation.
Lottery jackpots have grown in recent years, with all 10 of the biggest nominal hauls coming since 2012 and inflation adjustments making little difference to the overall rankings.
But by design these jackpots are small in comparison to the amount of cash Americans spend on lottery tickets. Total spending on lotteries amounted to a shocking $70 billion in 2014, which amounts to about $300 per adult.
As Derek Thompson wrote last year for the Atlantic, there is a tremendous quantity of state-by-state variation in lottery spending, with Rhode Island and South Dakota spending tremendous sums on a per capita basis while North Dakota and Oklahoma spend relatively little:
About 40 percent of lottery revenue is sent to states (often nominally earmarked for schools, but in practice money is fungible), and then winnings are subject to a hefty 45 percent windfall income tax, so the lottery is a major moneymaker for the government. In terms of revenue sources it's one of the most regressive out there, with the poor buying a vastly disproportionate share of lottery tickets.
But a 2003 study by economist Emily Oster suggests that massive lottery jackpots may be more egalitarian in their distributive impact. Oster found that the regressive nature of the lottery as a revenue source is driven by the downscale demographics of very frequent lottery players. When jackpots get bigger, more people are induced to play, and the regressivity diminishes — though it doesn't vanish.
Oster found that, hypothetically, a jackpot of about $806 million would likely be large enough to make the lottery progressive. Even this weekend's record jackpot won't quite reach that level, but it's extremely close. And if there is no winner on Saturday, it's possible we'll have America's first distributionally progressive lottery.
Apple is currently the world's largest company by market capitalization. But the Kingdom of Saudi Arabia is seriously considering creating a much larger company — by staging an initial public offering (IPO) for shares in its state-owned oil company, Saudi Aramco.
Rumors of a potential IPO flew on Thursday following a statement by the kingdom's deputy crown prince, Mohammed bin Salman, and confirmed by a terse statement from Aramco headquarters Friday morning.
It's impossible to know how much Aramco would be worth on the private market if sold, especially without details regarding the terms under which it might be privatized. But based on the scope of Aramco's operations and the values of other oil companies, it seems likely that it would achieve a market capitalization of more than $1 trillion — far ahead of any American company.
The company currently known as Saudi Aramco has its roots in an exclusive concession to explore for oil in Saudi Arabia that the kingdom granted to Standard Oil of California (now Chevron) way back in 1933.
This concession was operated by a subsidiary called the California-Arabian Standard Oil Company, and it became a joint venture with Texaco in 1936. In 1944, both Standard Oil of New Jersey (which later became Exxon) and Socony-Vacuum (which later became Mobil and then merged with Exxon) joined the consortium — which was named Arabian-American Oil Company, or Aramco.
In essence, all of America's major oil companies were working together to exploit a monopoly on Saudi oil.
In 1950, Saudi King Abdullah threatened to nationalize the oil company — a way to get Aramco to agree to a deal in which 50 percent of profits would be handed over to the Saudi government. In response, the US Congress instituted a tax credit known as the "golden gimmick" that ensured all of Abdullah's money would come out of Uncle Sam's tax haul rather than out of the pockets of Aramco shareholders.
This arrangement didn't last long. By the 1970s, with anti-Western sentiment and Arab nationalism running high in the Middle East, the Saudis decided to nationalize anyway. Except instead of doing a full expropriation, the Saudis did a gentler takeover, using oil revenue to buy Aramco from its parent companies.
As recounted by Charles McPherson in his comparative study of oil nationalization schemes, "The Saudi approach to nationalization was very different than that of other countries" — not only in the financial terms under which it was done but in how the company was operated after nationalization.
By 1980, McPherson writes, Saudi Arabia was the sole owner of Aramco, but "under strict instructions from the King, the new Aramco [was] left very much to itself on operational matters" and "many of the Aramco companies continued as advisors to Saudi Aramco ensuring continuity of management." So the company was nationalized, but US-based oil companies got all its money and were still running the company operationally.
That deal came more than a generation ago. In the subsequent 35 years, the Saudi government has been able to use its control of Aramco to move the corporate headquarters from New York to Saudi Arabia and to fill more and more jobs with Saudi natives. As of 2015, it really operates like a state-run company, seeking revenue but more broadly pursuing the Saudi government's policy objectives.
Having gone through the literally decades-long slog of nationalizing the world's most valuable oil company in a non-disruptive way that preserved the country's positive relationship with the United States and major private oil companies, why would Saudi Arabia consider turning around and re-privatizing?
The starting point for understanding is that Prince Salman told the Economist he wants to list a minority stake in Aramco — perhaps 5 percent — while leaving control firmly in government hands.
It is not entirely clear what this would accomplish. In follow-up reporting, the Economist suggested that the issue may involve concerns about Aramco's current management practices:
Questions surround the company, though. Mr DeLucia says 87% of its output is oil; it needs to develop more gas to satisfy the country’s needs for cleaner, cheaper power. Some argue that its reserves, which have barely budged since the late 1980s, are overstated. Internal documents about them are "phenomenally closely guarded secrets" says a local observer.
The company does not report its revenues. Its fleet of eight jets, including four Boeing 737s, and a string of football stadiums suggest that it is not run on purely commercial lines. It is the government’s project manager of choice even for non-oil developments, and runs a hospital system for 360,000 people. A listing would require it to become more transparent.
The Economist analogized the situation to concerns about state-owned oil companies in Mexico and Brazil that have been accused of corruption and mismanagement, in response to which the Mexican government has begun to open things up to foreign investors — an idea that conservative politicians in Brazil have also proposed.
But Mexico and Brazil are democracies with free media and judicial systems where the transparency induced by a listing would operate as a lever for change. Saudi Arabia is a closed autocracy with a controlled media, so it's not clear through what mechanism transparency per se could shift management. Aramco presumably operates football stadiums and a hospital system because that's what the Saudi government wants it to do — selling a minority stake that leaves all control in the hands of the Saudi government wouldn't really change anything.
One way to understand the Aramco IPO talk is probably through the same lens that has to be used to understand all of the Saudi government's other recent decision-making: fear.
The Saudis are beset by problems ranging from the low price of oil to Iran's regional activism to the rise of ISIS. A successful flotation of a minority stake in ARAMCO would generate a good news story about Saudi Arabia. Aramco would be the world's largest company by market capitalization, and the Saudi stock exchange would suddenly be a big deal.
Partial privatization would serve as a token of change and reform in a country that is often seen in the West as excessively hostile to change and reform. It would also give a new generation of Saudi leadership a chance to put their personal stamp on things — not the best reason for taking dramatic action but at least a reason.
Perhaps most fundamentally, as Jennifer Williams has explained, one thing the Saudis fear is "abandonment by the United States in favor of Iran, or even just US disengagement from the region in general, thus depriving Saudi Arabia of its great power protector."
Giving Western investors a concrete financial stake in the Saudi national oil company would serve as a way to deepen and broaden the coalition of stakeholders in the kingdom's stability. The US-Saudi alliance is about much more than oil, but oil was its ground floor, and bringing foreign investors back into the Saudi oil business could be a way to strengthen its foundation.
The US economy created new jobs at an unexpectedly strong pace in December. There was a net increase of 292,000 jobs in the month, according to figures released today by the Bureau of Labor Statistics. Economists were expecting the economy to add about 200,000, in line with the average pace during 2015.
Even better, BLS says that last month's jobs report had undercounted the number of jobs created in October and November by a total of 50,000. That means the economy was creating jobs at an unexpectedly strong pace throughout the last quarter of 2015. The economy created 2.7 million jobs in all of 2015, a strong result but a bit weaker than the 3.1 million jobs created in 2014.

These results confirm that the economy has finally entered the kind of robust economic growth that seemed so elusive for the first few years after the 2008 financial crisis. The unemployment rate stayed at 5 percent, where it has been for the past couple of months:
Yet there are also signs in the report that the economy still has some room to grow. One is the labor force participation rate:
The fraction of the population that is employed has been declining since the last recession began in 2007. That's largely due to long-term demographic forces — people are living longer, staying in school longer, and so forth — but we should still expect a strong economic boom to pull some people back into the labor force. That hasn't happened yet.
The other sign that the economy is strong but could still be stronger is earnings. The average worker's hourly earnings rose by 2.5 percent — just slightly faster than the (currently quite low) rate of inflation. In a really strong economic boom — the kind we enjoyed in the 1990s, for example — we should be seeing workers' wages growing much faster than the inflation rate, producing real increases in people's standard of living.
It's a safe bet that President Obama will tout the economy's strong 2015 performance in his State of the Union speech next week. And if the strong results continue over the next year, it will give a boost to Hillary Clinton, who can be expected to run on Obama's economic record.
Silicon Valley philosopher king Paul Graham's essay on inequality has set tech Twitter on fire for the past few days. It's about as intensely loved and hated as any piece on the topic I can remember. Which makes sense, because it manages to jam some very good points together with some very bad ones.
Graham, a famed adviser to technology startups, appears to have experienced the inequality conversation as an attack on his life's work. His essay is thus more a defense of startups and their worth than it is an analysis of the trends driving inequality. More than anything, Graham seems terrified of policies that, in trying to combat inequality, would end up targeting the founders of tech companies.
This is a slightly bizarre interpretation of the debate. The inequality argument has emerged at a moment when Silicon Valley startups are lionized but protesters take to the streets to demand the dissolution of Goldman Sachs. If inequality were driven by technology startups, there would be much less concern over it.
But Graham's focus on startups as a cause and consequence of inequality is also empirically wrong. "Startups are almost entirely a product of this period [of inequality]," he writes, which is simply not true. For all the Silicon Valley hype, the startup rate has actually been declining as inequality has been rising.
In a separate (and, I think, more interesting and nuanced) essay, Graham argues that in the mid-20th century, there were startups, but they were very different in composition. "Educated people," he writes, worked for large corporations, because "there was practically zero concept of starting what we now call a startup: a business that starts small and grows big."
So perhaps that falling startup rate obscures a rise in the kind of startups that interest Graham. Even if that's true — Graham doesn't present data to prove it, but it certainly seems correct as a description of Silicon Valley trends  — it doesn't change the fact that there is no observable relationship nationally in recent decades between the rate of startup formation and inequality. I wonder whether Graham's perch in Silicon Valley isn't warping his analysis of what's going on in the rest of the economy.
When you dig into the occupations of the top 0.1 percent, you find that "the incomes of executives, managers, supervisors, and financial professionals can account for 60 percent of the increase in the share of national income going to the top percentile of the income distribution between 1979 and 2005."
Some of that is certainly Silicon Valley executives. But it's the financial professionals that this conversation is really about.
You can see that in the timing. The debate over inequality doesn't coincide with either the rise of Silicon Valley (late '90s, and then again in the early 2000s) or even the rise of income inequality (1980s). It coincides with the aftermath of the Great Recession — a time when finance professionals crashed the global economy, immiserated millions of people, and managed to remain incredibly, infuriatingly rich. There's a reason it was called Occupy Wall Street rather than Occupy Google.
By the time of the 2007 crash, financial sector profits accounted for about 35 percent of all corporate profits. These weren't startups. Indeed, the fear in the financial sector was over firms so ridiculously large that the government wouldn't permit them to fail.
"I know the rich aren't all getting richer simply from some sinister new system for transferring wealth to them from everyone else," Graham writes. In Silicon Valley, that's largely true. But if you lost your construction job because of the financial crisis even as a trader who bet on subprime bonds kept both his career and his bonuses, you've got good reason to think the rich are getting richer from a rigged system.
Graham tries to acknowledge this in a footnote. "Others will say I'm clueless or being misleading by focusing on people who get rich by creating wealth that startups aren't the problem, but corrupt practices in finance, healthcare, and so on. Once again, that is exactly my point. The problem is not economic inequality, but those specific abuses."
But this gets almost tautological. The inequality debate is driven by the belief that those kinds of abuses are disparate and widespread. People need a way to talk about the idea that economic gains are being shared unfairly and inequality is the word they've chosen to do it. Dismissing the source of their anger only leads you to miss the point of their critique.
An important point Graham makes is that while people are angry about income inequality, they usually prioritize fixing other problems. When it comes down to it, they really care about poverty, or social mobility, or median wages, or political power.
Consider two worlds. In one, the Gini coefficient — the standard measure of inequality — remains the same, but median wages are double their current level. In another, the Gini coefficient falls, but median wages are 10 percent lower and poverty is 3 percentage points higher.
Would anyone choose the second world? Bueller?
But having made that point, Graham spends much of his essay grappling with strawmen. Statements like "Ending economic inequality would mean ending startups" confuse the conversation. No one is talking about ending startups. No one is even talking about ending inequality. And you can certainly ameliorate inequality without destroying the ability to found new companies. Sweden, for instance, has a higher startup rate than America, and less income inequality — as do a number of other countries.
Graham's belief that you can't ease inequality without declaring war on technology runs deep. "I think rising economic inequality is the inevitable fate of countries that don't choose something worse," he writes.
He argues that the long fall in inequality in the 20th century was an anomaly driven by wars and government-backed oligopolies. He goes on to write:
The acceleration of productivity we see in Silicon Valley has been happening for thousands of years. If you look at the history of stone tools, technology was already accelerating in the Mesolithic. The acceleration would have been too slow to perceive in one lifetime. Such is the nature of the leftmost part of an exponential curve. But it was the same curve.
You do not want to design your society in a way that’s incompatible with this curve. The evolution of technology is one of the most powerful forces in history.
This is far too fatalistic. Modern societies have long figured out how to manage this curve. Technology makes individuals grow more productive, in part because they stand atop the knowledge and industrial base of their societies, and societies redistribute part of that wealth, in part because that's necessary to sustain the political stability and economic freedom required to protect those individuals.
There are difficulties and trade-offs inside this system, but they're manageable, and all in all, it actually works pretty well. The taxes the Silicon Valley elite pay in the 21st century are much, much higher than anything their predecessors paid in the 18th century, but somehow people still like inventing new things and getting rich.
There's no particular level of inequality that is inevitable, and it's both pessimistic and ahistorical — two qualities I don't tend to associate with Graham, who tends towards optimism and a strong grasp of history — to believe the drive towards technological progress is so flimsy that modest changes to the tax code or social programs will derail it.
What is all the land in Manhattan worth? Economists Jason Barr, Fred Smith, and Sayali Kulkarni think they know the answer based on a data set of vacant parcel sales in Manhattan. By recording all the vacant sales and doing a little math, you can interpolate the value of all the land and conclude that in 2014 the island's land was worth about $1.4 trillion — almost 10 percent of the annual income of the entire United States.
You can also see how the price of Manhattan has changed over time:
These numbers are adjusted for inflation and are on a logarithmic scale, so that 21st-century increase is truly enormous. They say that land prices have risen 15.8 percent per year since 1993 and that if you go all the way back to Dutch settlement in 1626, there's been a 6.4 percent annual rate of return over the past 388 years.
It's a fascinating paper, but one thing that's striking to me is the mere fact that it has to be written at all. The government collects, creates, and records tons of economic data. Using government websites, you can look up how many new homebuilding permits were issued last year or what the average retail price of a dozen eggs was or what the wholesale price of a pig was. But there's no authoritative tracking of land prices, even though the price of land is of fundamental economic importance.
Most of the country's top economists are gathered this week in San Francisco for the annual American Economic Association conference. And from what I'm hearing, the buzz is all about Robert Gordon, a Northwestern professor who's been arguing for some time that American economic growth is over. His book The Rise and Fall of American Growth: The US Standard of Living Since the Civil War will be out on January 12 and is largely dedicated to putting that claim into historical context.
The way Gordon sees it, rather than view the US's slowdown in economic growth since the mid-1970s as an anomaly, we should understand the rapid growth from 1870 to 1970 as exceptional. Around most of the world, for most of history, growth has been slow.
And he expects growth in average people's living standards in the US to continue to be slow for four big reasons: Educational attainment is slowing down, there's a big overhang of debt, the population is aging, and the rich are gobbling up more and more of the diminished economic pie.
Normally these kinds of big-think books end with a whimper, as the author totally fails to identify solutions to the problem he is writing about. But Gordon's conclusion offers some admirably definitive policy advice.
Specifically, he thinks we should:
To me, what's striking about this is that for such a big and ambitious book that is attracting so much debate, the policy prescriptions aren't especially far out.
Items 1, 2, 3, 5, 7, and 8 are all things the Obama White House has come out for, and 9 and 10 are things they've definitely signaled openness to. Items 3, 7, 8, 9, and 10 are all things I've heard leading Republicans endorse, along with some openness to 5. Total drug legalization isn't something that's very mainstream politically, but the overall political consensus is clearly shifting in that broad direction. Only 6 really seems like a nonstarter politically, even though it's absolutely a reasonable idea.
The overall tone of the end of the book is very much on the pessimistic side, but these seem like  policy solutions that one could be realistically optimistic about.
The self-driving car wars are heating up. A few weeks after we learned that Ford and Google are creating a joint venture to develop self-driving cars, General Motors and Lyft have announced a self-driving joint venture of their own.
Lyft is raising $500 million in investment capital from GM along with another $500 million from other investors. Those other investors include two Chinese companies that are huge in Asia, albeit obscure in the United States — Alibaba in e-commerce and Didi Kuaidi in ride-hailing. Together, it amounts to a global alliance to battle Uber, a direct Lyft competitor that is working on its own self-driving technology.
The odds of this new alliance triumphing seem slim, as it doesn't have either the resources or the artificial intelligence expertise of Google, Tesla, Uber, and some of the other companies working in this space. But the deal does tell us a lot about how the global ride-hailing market is evolving — and why competition in the ride-hailing market is really just the prelude to a much bigger fight over dominance of the self-driving car market over the next couple of decades.
Here are four big lessons from GM's deal with Lyft.
We're used to thinking about cars as personal property that people own. Taxis and rental cars exist, of course, but they're niche products used in unusual situations or in a handful of central cities. Most households do most of their trips in cars the household owns.
I've argued before that self-driving cars will flip this around. Without the need to pay a driver, self-driving taxis will be so cheap that ordinary middle-class consumers, even in the suburbs and smaller towns, will find them cost-effective for most trips. Indeed, because rental allows several people to effectively share a single vehicle, getting around via self-driving taxi will likely become more affordable — not to mention more convenient and versatile — than owning a dedicated vehicle.
If on-demand rentals are the future of self-driving car technology, then teaming up with an on-demand rental service is a smart strategic move for a carmaker like GM. By the time self-driving cars start showing up in the marketplace — likely sometime in the 2020s — many customers will have grown accustomed to hailing cars using a smartphone-based service like Uber or Lyft. That will give them a lot of influence over consumers' use of self-driving technology, and so it makes sense that GM is acting now to make sure it has an ally in the ride-hailing market.
New car technologies come along all the time, and it would be easy for car companies to assume that self-driving capabilities will be just another feature to add to their existing vehicles. But there's reason to believe self-driving technology is a much bigger deal, and that car companies that don't adapt quickly won't survive the transition.
Putting software in control of cars is likely to be a lot more than a cosmetic change. Just as the internet is fundamentally transforming industries from music to retail — and threatening incumbents in those industries — so self-driving cars are likely to prompt a fundamental rethink of how cars work. If cars are primarily rented, rather than owned, then they can be optimized for shorter trips and more heavily specialized for different use cases. Short-range, high-efficiency electric vehicles will become more practical. Companies may make cars in a wider variety of sizes and shapes, from hyper-efficient one-seaters to luxury minivans to support family vacations.
And car companies will also face new reliability and security challenges they've never faced before. When cars are controlled by software, they become vulnerable to hacking, and car companies' current manufacturing techniques — which involve delegating most of the work to hundreds of subcontractors — make car software almost impossible to audit. If car companies don't change their software development techniques, companies like Uber, Tesla, Google, and Apple are going to run circles around them.
Forming a joint venture with a prominent software company like Lyft gives GM a chance to start with a fresh slate. It could even be a sign that GM management recognizes that truly radical innovations will only take place if they're insulated from GM's bureaucratic culture.
GM is Lyft's biggest new investor, but the latest round of funding also includes money from Alibaba (China's answer to Amazon.com) and Didi Kuaidi (Uber's biggest rival in China).
These investments represent a growing recognition that self-driving cars are likely to be a global market. Developing self-driving technology won't be cheap, and so companies that can spread those development costs across multiple big markets will have a distinct advantage. The United States and China are the world's two biggest markets, so it makes sense that companies in these markets would team up. Once GM and Lyft have developed self-driving car technology, they'll be able to turn to Alibaba and Didi Kuaidi to help sell it to Chinese consumers.
Lyft is seeking new allies because it's a huge underdog, as reflected by the terms of this new deal. Lyft sold shares to its new investors on terms that valued the company as a whole at $5.5 billion. A few years ago, that would have been considered a high valuation for a ride-hailing company, but it's tiny compared with the more than $60 billion valuation of Lyft's biggest rival, Uber. Lyft raised $1 billion in its latest fundraising round and $680 million total in 2015. By contrast, Uber has raised nearly $5 billion in the last year.
Uber has been spending all of that cash not only consolidating its lead in US markets but also extending its reach internationally. Its global reach means that if and when Uber's own self-driving car project — which it has been working on for a year — comes to fruition, Uber will be able to deploy the cars in dozens of countries around the world.
The Lyft-GM alliance will also have to play catch-up with other companies that have big head starts in self-driving technology:
Starting with the March issue, due to hit newsstands this weekend, Playboy magazine will no longer feature explicit nudity. It sounds like an April Fools' joke, but it's not: The New York Times was given a copy of the latest issue, and what used to be an NC-17 publication is now more like PG-13.
This isn't about prudishness so much as it is about money: Playboy CEO Scott Flanders believes the Playboy brand can transcend its salacious origins and become a lucrative vehicle for selling mainstream products. There's already a wide variety of Playboy-branded clothing and jewelry out there, and the Playboy brand is particularly popular in China, where pornography is officially illegal.
Playboy tested this strategy with the Playboy.com website, which has been free of explicit nudity since 2014. The company says it's been a big success, attracting a much bigger and younger audience. Now it's hoping to expand on that success with what used to be the country's most popular pornographic magazine.
Beginning with next month's issue, there won't be any explicit nude images of women in the US edition of Playboy magazine. The Times reports that there are still some naked women in the issue but that the images are "shot in ways intended for strategic concealment" — much like men's magazines such as Maxim and Stuff.
In 1953, Playboy made its mark by being one of the first mainstream magazines to feature pictures of nude women. In the pre-internet era, porn was a lot harder to obtain, so there was a big market for pornographic magazines. The magazine grew to more than 5 million subscribers by the 1970s and attracted a bunch of competitors.
But the internet has totally transformed the pornography industry. Today, any kind of porn you can imagine is just a Google search away and in most cases is available for free. So over the past couple of decades, the value proposition of paying $19.95 a year to have a few dozen nude images delivered in dead-tree format each month has become less and less compelling. By last year, the magazine only had around 800,000 subscribers.
The internet has totally transformed the pornography industry
On the other hand, Playboy has always aspired to be more than just a pornographic magazine. Over the decades, those 5 million subscribers allowed Playboy to do interviews with a wide variety of famous people, including Martin Luther King Jr., Jimmy Carter, and Steve Jobs.
There's a long-running joke about people "reading Playboy for the articles," but Playboy's non-pornographic content really has been pretty good over the years. Now it won't be such a joke anymore. Playboy will tone down the nudity while beefing up its coverage of other topics, including a new sex column and expanded coverage of the liquor business.
Playboy used to look like a conventional media company with a stable of magazines, websites, television stations, and so forth. But that business model hasn't done well in the internet age, and it reached its nadir in the wake of the 2008 financial crisis.
So the company began downplaying its media properties and focusing instead on promoting and licensing its iconic brand. And Flanders started to wonder whether distributing pictures of naked women was becoming a business liability. "You could argue that nudity is a distraction for us and actually shrinks our audience rather than expands it," he argued in 2014.
Dropping the naked women dramatically expanded the potential audience for Playboy.com
Lots of people are attracted to the risqué vibe of the Playboy brand, but there are situations in which outright pornography isn't allowed. Apple's App Store, for example, doesn't allow apps to have sexually explicit imagery, for example, nor do Facebook and Instagram.
So last year, Playboy overhauled its primary website, Playboy.com, and took out all the explicit nudity (there are still plenty of racy near-nude shots of the type you'll find in other men's magazines). Playboy executives told the New York Times that this was a huge success: Traffic quadrupled, and the average age of readers fell from 47 to 30.
In other words, young people who grew up in the porn-saturated world of the internet aren't that interested in Playboy.com as a place to get porn. And the existence of naked women on the site made it awkward to read Playboy articles at work — where many people spend time goofing off online — or share Playboy content on social media sites. Dropping the naked women dramatically expanded the potential audience for Playboy.com without significantly reducing its appeal.
More traffic and a younger audience are big successes in their own right, but even more importantly, the shift helps make the Playboy brand more mainstream. There's already a large demand for Playboy-branded merchandise, and Flanders is betting that that demand will grow even more if Playboy becomes less associated with explicit pornography in the minds of the public.
The decision to drop nudity from the magazine is best seen in this light. The goal isn't so much to make the magazine itself more successful — though presumably its owners would like to do that — but to make the magazine a more effective sales tool for the Playboy brand more generally.
Playboy's magazine hasn't been a big moneymaker in years. Flanders told the New York Times last year that the US edition of the magazine lost around $3 million last year. But Playboy's efforts to cash in on its brand — and particularly its famous bunny logo — are paying big dividends.
The goal is to make the magazine a more effective sales tool for the Playboy brand more generally
Playboy's brand is not only widely known in the West, it's also surprisingly popular in China. In 2014, Playboy-branded products generated $1.5 billion in revenues in China, about a third of the worldwide total. Playboy merchandise is available in 3,500 retail outlets in China — which is particularly remarkable because pornography is officially illegal there.
Flanders hopes that making the magazine less porny and more mainstream will help make the Playboy-branded products more mainstream as well — and dramatically expand the market for them.
There will no longer be naked ladies in Playboy magazine or at Playboy.com, but that doesn't mean we'll stop seeing explicit imagery distributed under the Playboy brand.
In 2011, Playboy signed a deal with the internet porn company Manwin, since renamed MindGeek, to manage many of the company's online properties and television channels. Playboy later regained control over the Playboy.com site, but the rest of Playboy's pornographic empire, including the Playboy Plus subscription service and Playboy TV, continues to be operated by MindGeek.
This might mean that Playboy can have the best of both worlds: It could enjoy the commercial benefits of a more mainstream image while continuing to profit indirectly from its pornography business.
On the other hand, if dropping pornography from the website pays big dividends for Playboy's licensing business, it's possible the company will seek to shut down its other pornographic properties as well. That might be tricky, since Playboy's licensing agreement with MindGeek runs for 15 years (meaning Playboy might not get control back until 2026). But if Playboy becomes determined to separate itself fully from the pornography business, it might be able to cut a deal with MindGeek to end the deal early, or to choose a new brand name for its pornographic content.
Via Fred Wilson, here's a look at the 25 most popular mobile apps as of mid-2015, a list that you'll see is utterly dominated by Google, Facebook, and Apple:
As Wilson notes, "There isn’t a single 'startup' on that list and the youngest company on that list is Snapchat which is now over four years old." In other words, we've experienced a closing of the app frontier and entered a phase of consolidation.
But another theme I would note is that this chart calls into question two bits of conventional wisdom that currently exist in the technology press. One is that the strength of Apple's iPhone franchise fundamentally rests on the strength of the app ecosystem — developers develop for iOS because iOS has the best customers, and high-end buyers flock to iOS because it has the richest app ecosystem. The second bit of CW is that this means Apple needs to cultivate a similar virtuous circle between users and developers to succeed with products like iPads, Apple TV, and Apple Watch. In other words, for its new products to succeed, Apple needs to do more to improve its strained relationship with independent developers.
From this chart, though, it seems maybe Apple's relationship with independent developers is strained because Apple can see that those developers aren't as important as they think they are. The main thing people want to do with their mobile devices is access services from Facebook and Google, meaning that the important thing for Apple to manage is the delicate relationship between giant companies that compete with each other while also being codependent.
The Big Short has earned acclaim from critics and financial journalists alike for the skillful way in which it dramatizes and humanizes financial products and concepts that are complicated, and at times deliberately obscure. But like many other financial crisis narratives, it fundamentally misses the part of the story that turned the 2008 financial crisis into the sort of epochal event that people would make movies about in 2015 — the Great Recession that sent the unemployment rate soaring to 10 percent and kept it above 8 percent for two more years.
The thing about this recession is that it's a lot simpler than the financial machinations that blew up and then deflated the housing bubble. So simple that it would make a terrible movie. But despite its simplicity, it's a story that is oddly missing from American political dialogue.
Excavations of the plumbing behind the financial crisis often seem to be implicitly written from an alternate universe in which the events of the fall of 2008 played out entirely differently. In this universe, the Federal Reserve wasn't able to organize a rescue for Bear Stearns, or allow Goldman Sachs to hastily convert to bank holding company status, Congress didn't approve the Troubled Asset Relief Program, AIG wasn't nationalized so the claims it had underwritten didn't get paid off, and the Bush-Obama lame duck period was characterized by a series of epic bankruptcies of large, diverse financial institutions.
If all that had happened, then we would be sitting around saying that economic devastation visited upon American working people in the subsequent years was caused by a massive banking crisis. We would note that there's a certain irony in the fact that Fed Chairman Ben Bernanke was a leading academic proponent of the view that an uncontrolled series of bank failures caused the Great Depression, and yet an uncontrolled series of bank failures rolled out under his watch, followed by a second Great Depression.
Except none of that happened.
The unsound bets on mortgage-backed securities that are detailed and explicated in the Big Short nearly brought the American banking sector to its knees, but with the exception of poor Lehman Brothers none of the major banks actually did fail. Everyone got access to the Fed's discount window. AIG's bills were paid even though it had no money. New capital was injected by the federal government on a generous basis, and an implicit guarantee halted runs. There was a scary near-miss on the "all the banks fail" thing, but it didn't happen. Before the crisis, Citigroup, Wells Fargo, JP Morgan Chase, Bank of America, and Goldman Sachs were the biggest and most important financial companies in America, and that's still the case today.
So why did we have such a gigantic recession? This is an ultimately more important question, albeit one that wouldn't make a great movie. And the answer is pretty simple — the collapse in house prices led to a sharp slowdown in residential construction spending (because building new houses wasn't lucrative any more) and it also led to a sharp slowdown in consumer spending (because people became poorer) and those slowdowns led to a slowdown in business investment spending (because nobody was buying anything).
You can call this a big drop in demand, a big drop in total economy-wide spending, a big drop in nominal gross domestic product, or any number of other things. But that's what happened.
But there's supposed to be a fix for this kind of thing. The government — through a mix of money-printing by the Federal Reserve and tax cuts and deficit spending by congress — is supposed to plug the gap. The tax cuts finance consumer spending, the deficit spending directly employs many people, and the Fed action boosts net exports all while business investment continues apace to keep up with these streams of demand.
In that scenario, we wouldn't remember 2009-2010 as painless times. Construction workers still would have lost their jobs and had to go find new work, perhaps work that didn't take advantage of their specialized skills and didn't pay as well. Affluent people would find that foreign travel and imported German cars had become less affordable, while the poor and middle class would have worried more about import prices for things like clothing and children's toys. But after a modest bump of bad times we'd have moved on to the enduring issues of public controversy — the appropriate size of the welfare state, the balance between environmental regulation and short-term economic growth, and whatever Donald Trump said yesterday.
But that didn't happen. We had a significant fiscal stimulus bill from the Obama administration, but it was scaled to a smaller recession than the one that was actually happening. The Federal Reserve cut interest rates to zero, but then got timid about additional money-printing. Unemployment never got as bad as it did during the Great Depression, but it got pretty bad and only recovered slowly.
That's why we remember the financial crisis as such a big deal. But financial crises don't cause years-long spells of mass unemployment unless the political system lets them. The country's governing elite mobilized during the fall and winter of 2008 to prevent a banking crisis from destroying the economy, but then during the spring and summer of 2009 didn't take the kind of decisive action that could have led to a quick employment recovery. That's the real tragedy of the era, and you won't see it in any movie theater.
Star Wars: The Force Awakens has crushed the competition at the box office, making the $4 billion price Disney paid for ownership of the Star Wars franchise look like an enormous bargain — especially when you consider that even more money will come pouring in from ancillary licensing deals. Disney also owns Marvel Studios, which has become an increasingly reliable engine of hit content, spawning not only an endless parade of movies but also critically acclaimed television shows like Jessica Jones that could be an even better home for Marvel properties than the cinema. Combine those two franchises with Disney's effective long-term stewardship of Pixar, and the company is clearly positioned as the dominant content franchise of the future.
Yet Disney stock is down a bit over 1 percent on the year, with almost $2 billion in market value having been eliminated. And it really all comes down to one thing: ESPN.
Live sports has been an important part of the television ecosystem for decades, and ESPN has been one of the most successful and important networks throughout the cable television era. But in the 21st century, live sports has truly emerged as the killer app for linear cable television.
The first factor was the rise of the DVR, which has made it cheaper and easier than ever before for people to record their favorite shows and watch them at their leisure. This has been great for television artistically, since it means creators can now more readily assume that every single episode of their show will be consumed in sequence. But in business terms, it's devalued the audience, since marketers know that DVR-watchers are skipping ads. But people don't normally like to record sports and watch them later — they watch live, meaning a sports viewer has increasingly become more valuable than a non-sports viewer.
Streaming television services then came along and made live sports even more valuable. Suddenly the vast majority of basic cable networks became a fundamentally dispensable commodity. A Netflix or Hulu Plus subscription would give anyone a vast array of programming to watch without the need for a cable subscription. But for sports fans, cable remained vital.
Which meant that sports became even more vital to cable companies. Cable companies are blessed to face little to no competition in most circumstances, but they do need to worry about non-consumption of pay television. Ensuring that ESPN is in your bundle of channels was the key to avoiding non-consumption, so ESPN could demand increasingly high fees from cable companies that wanted to carry it.
But this is the year it all started to fall apart. As of the fiscal year that ended on October 3, ESPN had 92 million subscribers — down from 95 million in 2014 and 99 million in 2013. Every pay television operator under the sun still subscribes to ESPN, but fewer and fewer people are subscribing to cable.
The overall cable bundle in which you pay a high price for a ton of content works out to be a pretty good deal for most consumers. And it's been a great deal for live sports, since it means they collect high fees not just off the large number of sports fans in America but also off the large minority of Americans who don't watch sports. But it's not a good deal for everyone, and more and more of the people for whom it's not a good deal are opting out and relying on streaming services or à-la-carte purchases for their entertainment needs.
And the problem with the bundle is that the more people who opt out of it, the worse a deal it becomes. More and more content owners will want to try to reach the cord-cutting audience, which means that subscribing to the bundle will be necessary for a smaller group of people, which means that even more people will opt out. The move to a purely over-the-top world in which everything is consumed via the internet will happen in fits and starts, but once the process is underway it's unstoppable.
And that's very scary for ESPN. In theory, the channel could thrive in a post-bundle world by becoming its own sports-specific bundle. As the incumbent worldwide leader in sports, the network is better positioned than any other particular provider to assemble the ultimate omni-sport package that every sports fan in America would buy. But the chance of someone else — Fox Sports, NBC Sports, or even a brand new upstart — doing this isn't zero. And the chance that sports itself will completely unbundle with each team selling its own games to its own fans isn't zero either.
So while ESPN is hardly doomed, it's gone from a situation in which its dominance was guaranteed to a situation in which its dominance is not guaranteed. And as goes ESPN, so go Disney's other cable channels — they are all plunging into a very uncertain world.
Uber’s devastating effect on the New York cab industry is plain to see in this chart, produced by Goldman Sachs, showing the change in the price of New York City taxi medallions since 2004.
New York’s yellow taxicabs are the only vehicles in the city allowed to pick up passengers who hail them from the street. The medallion system, in place since 1937, sets an upper limit on the number of those cabs. As demand grew, medallions became more and more valuable.
This chart shows how medallion prices rose from about $250,000 in January 2004 to a peak of just over $1 million for an individual medallion — and about $1.3 million for a corporate one — in March 2013.
But starting in 2010, Uber’s drivers — who aren’t allowed to accept street hails — started filling this government-created vacuum. As Uber added more and more drivers, medallion priced stagnated, then started to fall precipitously. The more people hail cars through Uber, the less money cab drivers make, and the worse taxi medallions look as an investment.
One of the city’s largest taxi companies, White & Blue Group, saw its monthly medallion-leasing income drop as much as 50 percent in the past year, according to a lawsuit it filed against the city last month.
Discussions of "gentrification" are commonplace in contemporary urban America, with complaints usually focusing on two main themes. One is change in the built environment — people often liked neighborhoods the way they were when they moved there, and resent the construction of new structures that differ in scale and style from what was there previously. The other is economic impact — people often worry that an influx of affluent newcomers will raise housing costs in a way that disadvantages less privileged people.
The good news is that this latter problem can be fixed. The bad news is that the best way to do it is to increase the pace at which a city's built environment changes.
The experience of expensive, politically liberal coastal cities tends to dominate media discussions of urbanism, so it's important to note that these cities are the exception rather than the rule. Most American central cities are relatively affordable, and many of them — especially in the Midwest — are still suffering from the population loss and disinvestment associated with white flight. A Detroit, Cleveland, or St. Louis could greatly benefit from an influx of affluent newcomers whose presence would create new job opportunities and bolster local tax bases.
At the same time, in any city it does seem to be true that an influx of newcomers will tend to raise prices. Research by Veronica Guerrieri, Daniel Hartley, and Erik Hurst shows empirically how this works. Price increases tend to concentrate in specific neighborhoods rather than spreading across a city as a whole. They model this as a question of spillovers. More and less affluent people place systematically different values on different kinds of retail opportunities. So affluent young people might be drawn to proximity to a Whole Foods and an array of independent coffee shops and yoga studios, while working-class families might prefer a cheaper supermarket and proximity to some home-based day care providers. When affluent people start moving to a neighborhood, the retail mix shifts in favor of things affluent people like, which draws more affluent people to that specific neighborhood but not necessarily to other places in the city.
But whether this is good or bad for older residents of the city depends on other factors. Janna Matlack and Jacob L. Vigdor examined market data from 1970 to 2000 and found that the net economic impact of gentrification varies according to local housing conditions.
"In tight housing markets," they write, "the poor do worse when the rich get richer," whereas in slack markets, "some evidence suggests that increases in others' income, holding own income constant, may be beneficial."
When houses are plentiful, in other words, gentrification can be a win-win — increases in other people's incomes create new opportunities for the poor. But when houses are scarce, increases in other people's incomes merely exacerbate scarcity and leave the poor worse off than ever.
So what creates a "slack" housing market where gentrification can be a win-win? Data from the real estate website Trulia shows it can basically happen one of two ways.
Markets like Detroit, Cleveland, or Rochester are cheap essentially because they are economically depressed. There are plenty of empty houses, so if affluent newcomers show up and fix some of them up it doesn't generate any real scarcity.
Tight markets like New York and the Bay Area can't replicate that approach to affordability. But they could learn a lesson from the other kind of slack housing market — Sunbelt markets like Raleigh and Atlanta where new houses are being built at a very rapid clip.
Those fast-growing metros are mostly adding houses by spreading their geographical footprint deeper into the suburbs. That's not necessarily an appealing option for cities whose sprawl is limited by oceans or already-gargantuan commuting times. But fortunately, technology exists that allows house builders to pack large quantities of dwellings into limited land. Rather than detached houses each perched in their own yard, rowhouses or townhouses can be built. Where land is even scarcer, American builders have the capacity to erect apartment buildings — some of them two dozen stories high or more — whose floors are connected by elevators. The big problem is that in the most expensive metropolitan areas it is illegal to deploy these technologies on large swaths of land. Zoning codes and historic preservation rules generally prevent even the priciest neighborhoods from becoming denser.
Relaxing these zoning rules would transform gentrification of neighborhoods in generally affluent cities into a win-win that benefits the poor. But it would mean accelerating the pace at which gentrification reshapes the built environment of those neighborhoods. Those worried about gentrification, in other words, likely need to choose what it is they are primarily worried about — the aesthetic or economic dimensions of the issue — and recognize that addressing one will likely exacerbate the other.
Seattle and the San Francisco Bay Area have a lot in common — coastal locations, high-tech economies, and relatively high wages. But as California's Legislative Analysis Office wrote in a recent report, it's much easier to get permission to build new houses in the Seattle area. Consequently, the Seattle area's housing stock has grown twice as quickly as the Bay Area's. The CLAO writes that in recent years, Seattle's total number of housing units "grew at an average annual rate of 1.4 percent per year while San Francisco and San Jose’s housing stock grew by only 0.7 percent per year." The main reason for this is that Washington state centralizing more planning functions at the state level, which gives hyperlocalized Not in My Backyard sentiments less when determining what people are going to be allowed to build.
So what happened? While prices in the Bay Area have been skyrocketing, some Seattle landlords have actually seen the rents they can charge start to fall. Mark Stiles of the Puget Sound Business Journal writes that landlords are finding the trend "alarming" — though if you're a tenant in Seattle you probably feel differently:
The big warning sign for landlords is what the report says is "price resistance" in the most expensive submarkets: the downtowns of Bellevue and Seattle, including Belltown and South Lake Union, and Sammamish/Issaquah. After increasing during the first three quarters, rents dropped this quarter in all but South Lake Union, with the average decline hitting $59 a month. Further, when all of these submarkets are considered, the average vacancy rate increase was nearly a full percentage point.
Meanwhile, across all markets, more landlords are offering tenants sweeter incentives, such as free rent. The average value of incentives is $15 a month this quarter, which is nearly double what it was last quarter, when 16 percent of landlords were offering incentives. Now 20 percent are.
Skeptics often note that new construction tends to target the high end of the market, rather than creating affordable dwellings for the working class. And that's true here. Stiles reports that the weakness in the market is at the high end, "where 5.4 percent of the units are vacant," while cheaper rentals feature a lower vacancy rate.
But these markets are all logically linked. As proprietors of luxury buildings begin to lower rents in response to high vacancy rates, some people currently in mid-market housing will take advantage of the opportunity to upgrade. That puts downward price pressure on the middle of the market and draws more people further up the chain. None of this exactly makes Seattle a cheap place to live — land is still more expensive there than it is in Atlanta, and multi-family apartment buildings are often more expensive to build than sprawling single-family homes. But a high level of construction ensures that the homeland of Microsoft and Amazon remains substantially cheaper than the Bay Area homeland of Apple and Google.
Chipotle likes to emphasize the quality of its food, a goal summed up in the company's slogan, "Food with integrity." So it might seem paradoxical that Chipotle, of all companies, has gotten hit by a string of food safety problems.
Yet it turns out that it's not so paradoxical. This paragraph from a regulatory disclosure Chipotle filed in February — before the current food safety crisis began this summer — explains why (emphasis added):
We have made a significant commitment to serving local or organic produce when seasonally available, and a small portion of our restaurants also serves produce purchased from farmers markets seasonally as well. These produce initiatives may make it more difficult to keep quality consistent, and present additional risk of food-borne illnesses given the greater number of suppliers involved in such a system and the difficulty of imposing our quality assurance programs on all such suppliers. Quality variations and food-borne illness concerns could adversely impact public perceptions of Food With Integrity or our brand generally.
There's plenty to dislike about factory farms, but one big advantage of large-scale conventional agriculture is that it allows sophisticated quality control measures. By aggressively embracing local and organic food, Chipotle put itself — and its customers — at greater risk of doing business with suppliers with substandard safety and quality control procedures.
And as we've noted before, the benefits of organic food are scientifically dubious to start with.
Thanks to Andrew Lang for pointing out Chipotle's filing.
The news about Chipotle's food safety record keeps getting worse. In recent months, people in California, Washington state, Minnesota, Boston, and elsewhere have gotten sick after eating at Chipotle. On Monday, the Centers for Disease Control and Prevention reported another round of infections — five Chipotle customers in Kansas, North Dakota, and Oklahoma.
The run of bad news is ironic because Chipotle has actually spent a lot of time this year thinking about where its ingredients come from. Back in April, Chipotle became the first major restaurant chain to announce that all of its food was free of genetically modified organisms. Many customers saw that as a sign of progress — though others complained that some of its "GMO-free" meat came from animals fed GMO grains.
Yet study after study has found that GMO foods are perfectly safe. While genetically modified food sounds scary to a lot of people, it's been widely available in the United States for about two decades with no apparent ill effects.
So rather than pandering to groundless fears about GMO safety, Chipotle would have served its customers better by focusing on the very real dangers of food tainted with E. coli, norovirus, or salmonella. Theoretically, it should be able to do both, of course, but like any organization Chipotle has limited resources. A dollar it spends guarding against the overblown threat of GMOs is a dollar it can't devote to preventing actual health problems.
Chipotle's stock took a beating on Friday after the Centers for Disease Control and Prevention reported on another outbreak of E. coli infections linked to the fast-casual restaurant chain. According to the CDC, five people in Kansas, North Dakota, and Oklahoma contracted E. coli infections between November 18 and November 26. All five had previously eaten at Chipotle.
It's the latest in a long string of revelations about food safety problems at Chipotle restaurants. This newly reported outbreak actually occurred a few days before an outbreak that sickened 120 students in Boston. In late October, the company closed 43 restaurants in Oregon and Washington in an effort to get the problem under control, and has also faced outbreaks in Minnesota and California. Thankfully, no deaths have been reported.
The string of food safety problems has battered Chipotle financially. In a regulatory filing last Friday, Chipotle predicted that sales at a typical restaurant would fall by about 10 percent as a result of the outbreak, and that the company would have to spend $6 million to $8 million to address the crisis — not counting possible legal costs if customers sue. The company's stock has fallen by about 30 percent since August.
With $600 million in cash on hand, Chipotle shouldn't have any trouble weathering this storm financially. But the larger question is whether the string of outbreaks — which may or may not be over — will damage the restaurant's reputation for "food with integrity." Chipotle's commitment to using fresh ingredients from local farms makes it more vulnerable to foodborne illnesses. Chipotle is going to have to work hard to improve its food handling procedures to stamp out foodborne illnesses and win back customers' trust.
The string of outbreaks stretches across the country. In early December, the Centers for Disease Control and Prevention created this map showing the location of E. coli outbreaks in recent weeks — this map does not include the most recent revelations of illnesses in Kansas, North Dakota, and Oklahoma.
The CDC says that 52 people had been infected with E. coli, and that 47 of them reported eating at Chipotle in the preceding week — strong circumstantial evidence linking Chipotle to the outbreak.
And that's not all. In August, dozens of California Chipotle customers and employees were sickened with a different infection called norovirus — one of the most common foodborne pathogens. The same month, at least 45 people in Minnesota became ill with a third pathogen, salmonella, a problem that was eventually traced to tainted tomatoes at area Chipotles.
In early December, we learned that more than 120 people had become ill with norovirus after eating at a Chipotle near Boston College.
As each outbreak has come up, Chipotle has moved aggressively to contain the problem. As I already noted, Chipotle closed restaurants across Washington state and some parts of Oregon in late October and early November to try to contain the E. coli outbreak there.
In early December, Chipotle announced further steps to beef up its food safety protections. It brought in independent food safety expert Mansour Samadpour to improve its food safety practices and implemented their recommendations. "I am happy to report that our proposed program was adopted in its entirety, without any modification," Samadpour said on December 4. "While it is never possible to completely eliminate all risk, this program eliminates or mitigates risk to a level near zero, and will establish Chipotle as the industry leader in this area."
The improvements included "high-resolution testing of all fresh produce," testing ingredients as they reach the end of their shelf life, continual monitoring of the supply chain to identify suppliers with quality problems, and better employee training.
Then Chipotle faced another big outbreak in Boston. (The just-announced illnesses in Kansas, North Dakota, and Oklahoma all began in November, before this announcement.)
One reason the company may be struggling is its commitment to fresh ingredients. Big restaurant chains that serve canned or frozen foods can centralize their food distribution systems, allowing them to more easily monitor their supply chains and implement stringent procedures to eliminate foodborne illnesses. But Chipotle uses fresh ingredients sourced from a wide variety of local suppliers that may not have the capacity for stringent quality controls. That makes the company more vulnerable to outbreaks, and puts more of the onus for food safety on the managers of the company's 1,700 restaurants.
For years, Chipotle has touted the superior quality of its ingredients, emphasizing that its meat is organic, humanely raised, and GMO-free. These are all strong selling points with Chipotle's upscale customers, but of course they won't mean much if the chain develops a reputation for getting its customers sick.
Concern about long-term damage to Chipotle's brand may explain why the company's stock has lost a quarter of its value since the current string of outbreaks began in August.
Chipotle's situation could be worse. In 1993, for example, hundreds of people were sickened — and four children died — as a result of a massive E. coli outbreak caused by undercooked hamburgers at the fast-food restaurant Jack in the Box. Jack in the Box survived the crisis, instituting innovative new food safety procedures and running ads showing it dynamiting the company's corporate headquarters.
Chipotle is going to have the pursue the same two-pronged strategy: First make sure it's identified and fixed the problems that allowed the outbreaks in the first place, and then make sure the public knows that Chipotle has turned over a new leaf when it comes to food safety.
You know setting aside money for retirement is smart, but you don’t know anything about how it works. You need advice. Where do you turn?
Maybe you have a financially savvy friend or family member. But a lot of people turn to professional advisers.
Until now there have been two different kinds of investment advisers: those who are required to work in your best interests, and those who — amazingly — are not.
But a new rule from the Department of Labor, which becomes final today, aims to change that. Under the new rule, savers will gain the right to sue or initiate arbitration against advisers who don’t meet high ethical standards or who fail to disclose conflicts.
The effect, say both experts and many industry advocates, will be to drive many advisers to change their business models, while pushing others entirely out of the business.
And that’s probably a good thing.
The Department of Labor developed the new standards over the past few years, as part of a larger regulatory effort to reduce risk for middle-class savers after the 2008 financial crisis.
The two kinds of retirement savings pros are called "registered investment advisers" and "broker-dealers." Registered advisers — who often work with wealthier, more sophisticated clients — are required to uphold fiduciary standards, a special legal arrangement most often associated with lawyers and doctors. They are required to work in the best interests of their clients and can be punished for doing otherwise. Registered advisers are paid directly by their clients, and generally charge based on the size of the fund they manage.
"Broker-dealers," on the other hand, are often more like salespeople (a dealer is a broker who works independently). They’re often paid commissions, increasing their incentive to suggest products with higher fees. Many also receive "revenue-sharing" payments from the banks and mutual funds that generate the investments the brokers sell, again potentially influencing them to steer savers toward products that charge more.
The suggestions broker-dealers make to clients are held to a "suitability standard." That is, brokers are required to only propose products that are appropriate for the saver given his or her age, retirement goals, etc.
"I've never met anyone who's going to go to a dealership to ask advice about whether to buy a new car"
What brokers dole out is "advice" in the conventional sense, but the current rules don’t require it to be particularly good, or even fully well-meaning.
"A sales pitch where they're going to get a huge commission isn't ‘advice,’" says Betsey Stevenson, a professor of economics and public policy at the University of Michigan and a former member of the White House Council of Economic Advisers (CEA) who co-authored its report on the rule.
"Car salesmen get commissions, and everyone’s fine with that," she adds. "But I've never met anyone who's going to go to a dealership to ask advice about whether to buy a new car or take the bus."
A decades-long shift from traditional pensions to 401(k) savings plans means more people than ever are responsible for deciding where to invest their own retirement savings.
The CEA’s report, published in February 2015, found that bad advice from conflicted broker-dealers reduced savers’ returns by about 1 percent a year — as much as $17 billion a year nationwide.
The greatest losses occur when workers roll their 401(k) balance into a higher-fee individual retirement account when leaving a job, rather than keeping it with their former employers.
"A typical worker who receives conflicted advice when rolling over a 401(k) balance to an IRA at age 45 will lose an estimated 17 percent from her account by age 65," the CEA’s report said. The report adds that if there is $100,000 in that account, it could grow to $216,000 in 20 years without the bad advice, as opposed to $179,000 with the conflict of interest.
Opponents of the new rules argue that most brokers already act the way regulators want them to.
"If you don’t help someone, they’re not going to continue with you as a client," said Lisa Bleier, managing director at the Securities Industry and Financial Markets Association, an industry group, in a December interview.
Bleier said the Labor Department’s proposed rules were haphazard, applying only to retirement advisers and not to investment advisers generally. They’d require savers to sign extra contracts, including one the moment they walk in the door, she said, and would expose brokers to frivolous lawsuits.
Stevenson argues that most savers would be fine with so-called "robo advice" from online services
I pressed Bleier on the ethical issues involved in advisers receiving commissions or revenue-sharing arrangements. "I don’t see those compensation structures as a problem, no," Bleier responded. "The SEC has approved them."
Both Stevenson, the professor and former Obama administration economist, and Bleier agree on one likely effect of the rules: A lot of brokers will stop serving middle-class clients.
Broker-dealers will have three options:
In late 2012, the UK passed a set of regulations that were similar to the Obama proposal — but stronger. The regulations not only imposed a fiduciary standard on retirement brokers, but also banned nearly all commissions, while also strengthening disclosure and professional training requirements.
The result: The number of advisers in the UK fell from about 27,080 in 2009 to 23,640 in 2014, a reduction of about 13 percent, according to the Association of Professional Financial Advisers.
<!--
new pym.Parent('vox-number-of-advising-staff-working-in-financial-adviser-firms-__graphic', '//apps.voxmedia.com/at/vox-number-of-advising-staff-working-in-financial-adviser-firms-/', {xdomain: '.*\.voxmedia\.com'});
// -->
The revenue generated by advisers was impacted as well. Advisers’ total income never fell, but it stopped growing for three years, from 2011 to 2013. But growth resumed in 2014 with a surge of fee-based income replacing a very large drop in commissions. (Outside factors like the eurozone debt crisis likely influenced advisers’ revenue, as well.)
In the years since the new rules went into effect, both government and industry have expressed concern about a growing "advice gap" for middle- and lower-income retirement savers. While regulators have said it's difficult to measure the gap — measuring something customers aren’t doing — a research firm called Fundscape found, based on survey data, that many advisers were turning away investors who had less than about $148,000 in total assets. The average British investor has a portfolio of about $30,000.
Bleier and Stevenson both believe something similar will happen here in the United States — less wealthy clients simply can’t afford the upfront fees charged by non-conflicted investment advisers. But while Bleier sees that as a big problem, Stevenson disagrees.
Stevenson argues that most savers would be fine with so-called robo advice from online services that use data and algorithms to offer advice based on savers' specifications. Another option: They can read the Vox guide to retirement savings.
"It’s not about good guys and bad guys," said Stevenson. "These guys are just trying to feed their families. And if they can offer a product where they get a $500 commission versus one where they get an $800 commission — they don’t want to give bad advice, but what if it’s hard to tell?"
With the fiduciary rule, the government aims to resolve that tension and legally obligate advisers to put aside their self-interest. The result for savers, though, won’t necessarily be better advice. It could be less advice, at least of the kind you get from human beings.
But that’s okay. Most people don’t need much advice, because the principles of saving for retirement aren’t complicated: Save 20 percent of your income, invest in low-cost index funds, and don’t touch the money until you reach your retirement age.
Cigarette packs in Australia look different from anywhere else in the world. There are no brand logos, no bright colors. Every pack is the same shade of dull brown, plastered with graphic images showing the health impacts of smoking. There’s the gangrenous foot, mouth cancer, and everyone’s favorite, the creepy sickly eye.
Tobacco companies don’t do this by choice. In 2011, Australia became the first country in the world to pass plain packaging laws that severely restrict what can appear on cigarette packs.
Naturally, Big Tobacco hates these laws and has done everything it can think of to get rid of them. On Friday, Australia defeated a challenge by cigarette giant Philip Morris after a four-year international trade dispute in Singapore.
Big Tobacco is clearly running scared — and it should be. The latest ruling is likely to open the door for plain packaging laws around the world. Already, 11 other countries — including England, New Zealand, France, Brazil, India, and South Africa — have plans to implement their own plain packaging rules.
Other countries, including the United States, should follow Australia’s lead. Plain packaging laws are a simple, cost-effective way to cut smoking rates. That’s why tobacco companies hate them.
Philip Morris exploited an obscure clause in Australia’s free trade agreement with Hong Kong known as investor-state dispute settlement. President Obama’s controversial Trans-Pacific Partnership trade agreement includes an ISDS provision, and it has caused a lot of controversy. Critics worried that big companies — like Philip Morris — could use the ISDS process to overturn countries’ democratically enacted laws.
Australia may have won this round, but its government now faces an estimated $50 million legal bill. And Philip Morris is not the only company to push back against plain packaging laws. In 2012, British American Tobacco (BAT) challenged the laws in Australia’s high court, alleging they infringed the company’s intellectual property. The court ruled in favor of the government.
Eyebrows were raised at the World Trade Organization when Ukraine — a country that doesn’t actually export tobacco to Australia — challenged plain packaging as anti-trade. Later, it was discovered BAT had footed the country’s legal bill.
It’s not clear whether similar challenges from Cuba, the Dominican Republic, and Honduras are also funded by Big Tobacco.
While in public the tobacco giants insist plain packaging laws won’t work, behind closed doors they have spent millions of dollars trying to discredit the laws.
In 2010, a newly formed group called the Alliance of Australian Retailers launched a media blitz criticizing plain packaging. It was a case of "astroturfing" — an attempt to give the impression of a widespread grassroots backlash against the laws that just didn’t exist
Australian investigative journalists unearthed that Philip Morris, BAT, and Imperial funneled more than $5 million into forming the Alliance of Australian Retailers. The group’s argument against plain packaging was distilled into a simple message: "It won’t work, so why do it."

Of course, the obvious response is: If it won’t work, why are you spending millions of dollars to fight it? The alliance wasn’t effective. In fact, one study found for some people the ads increased their support for plain packaging.
The goal of the plain packaging laws is to make cigarettes less attractive and reduce the glamour of smoking. In lab studies, researchers have found they achieve this. More than 20 studies, undertaken over two decades in five countries, strongly suggest that plain packaging increases the impact of health warnings and reduces the appeal of cigarettes.
The laws aren’t a silver bullet to make people stop smoking. Instead, they work alongside higher taxes, public health campaigns, and education to reduce smoking rates over time.
As Australia is the only country to have implemented the laws, real-world evidence on plain packaging is limited. However, there is a growing body of research gathered since the laws were enacted three years ago. While we don’t yet have long-term evidence pointing to changes in smoking rates, short-term shifts seem positive.
A large study in the Australian state of Victoria found that in just 12 months plain packaging both reduced the appeal of smoking and increased desire to quit for adult smokers. Another study found calls to Quitline, an Australian government service to help people quit smoking, rose by 78 percent after the plain packaging laws came into effect. Smoking in outdoor areas, where the graphic packaging is visible to more people, also declined.
British American Tobacco hit back with its own study by Deloitte, which argued health warnings haven’t been effective in reducing consumption of cigarettes. But this study wasn’t independent — it was commissioned and funded by BAT. The study itself admits the researchers were highly selective about what brands they included.
If cigarettes were like any other product, high taxes would be enough to make people stop buying them. But cigarettes are highly addictive, so rising prices don’t have too much impact on a smoker’s decision to buy cigarettes.
It’s estimated that in rich countries, a 10 percent increase in the price of cigarettes will only bring about a 4 percent drop in demand. In poor to middle-income countries, the figures vary quite a bit because of a host of cultural and economic factors. For some though this sizable price bump would reduce demand by just 2 percent.
Of course, smokers could just take their cigarettes out of the packs. They could smoke rollies and put their tobacco in a tin. However, the evidence suggests very few people do. So the plain packs are always there, a graphic reminder of the harms of smoking. It’s a behavioral nudge that smokers carry around with them.
Globally, tobacco kills around 6 million people every year. Smoking costs the US more than $300 billion a year through both direct medical bills and lost productivity. Taxes, education, and public health campaigns are all important in reducing harm. However, Australia has shown that plain packaging needs to be in the mix too. It’s a simple and effective tool in the fight against a deadly habit.
On Wednesday night I went to the London Star Wars premiere. Or, if you want to get technical, I was a very lost tourist caught up in the mass of people trying to catch a glimpse of Carrie Fisher’s dog, Gary.
London’s Odeon Theatre was transformed into a sprawling red carpet, running the length of a city block. A line of luxury cars, each more impressive than the last, stretched as far as you could see towards Piccadilly Circus. Paparazzi swarmed around the entrance of the W Hotel, craning to snap the stars milling around the official preparty.
The scale of the whole thing was overwhelming, but it shouldn’t come as a surprise given the numbers around this release.
The Star Wars franchise has generated more than $32 billion in revenue over the past 38 years. The Force Awakens is expected to be its biggest earner yet, but the film’s fortune won’t be made in the cinema. It will be in the mall, where shoppers are expected to pick up $5 billion worth of merchandise over the next 12 months.
Branded bags of oranges, a $3999 Millennium Falcon kid’s bed and toy lightsabers — these are the real force behind Star Wars.
Back in 2012, Disney bought Lucasfilm — the studio that created Star Wars — for $4.02 billion. The pair’s first foray together, Strange Magic, was released in January this year. It was a total flop, one of the worst ever opening weekends for a widely released film.
But Disney wasn’t worried. Buying Lucasfilm had one purpose: to acquire the rights to Star Wars. It’s the world’s most lucrative franchise, and it has been sitting on the shelf for 10 years since Star Wars Episode III: Return of the Sith.
Star Wars: The Force Awakens is widely expected to be a huge hit for Disney, easily making back its $200 million budget. It’s on track to become the third highest-grossing film of all time, behind James Cameron’s blockbusters Avatar ($2.8 billion) and Titanic ($2.2 billion).
When the Star Wars franchise launched in 1977, it transformed the movie industry. Creator George Lucas suspected that people actually going to see a film in the cinema would represent only a tiny fraction of its potential revenue; the real money would be in merchandise. It was an insight that made Lucas a billionaire.
Back in the early 1970s, Lucas was a young director with just two films on his résumé and a curious idea for an epic space opera. When 20th Century Fox decided to pick up Star Wars, Lucas came back with a deal: He was willing to accept a $350,000 pay cut as director in order to keep the film’s merchandise rights, along with the rights to any sequels.
Not anticipating how popular the film would be, Fox accepted. Lucas is now worth more than $5 billion, having made one of the most profitable bets in history. In the 38 years since Episode IV: A New Hope, Star Wars has grossed $28 billion in revenue. And less than a sixth of this has come from ticket sales.
In buying Lucasfilm, Disney has secured both Star Wars’ film and merchandise rights. Already, the studio has laid out extensive plans to profit from the deal between now and 2020.
There will be two more "saga" films in 2017 and 2019, plus the "anthology" series — three standalone films within the Star Wars universe. The first, Rogue One: A Star Wars Story, is set between Episode III: Revenge of the Sith (2005) and Episode IV: A New Hope (1977) and will be released in December 2016.
Then, of course, there are the collector’s items. Disney has always been good at merchandising, pioneering the practice with Mickey Mouse toys in the 1930s. For the release of 101 Dalmatians in 1996, the studio made deals with more than 130 companies, including McDonald’s and Dr. Pepper.
Disney is already the world’s biggest licensor, selling more than $45.2 billion in 2014. Merchandise is particularly important for the studio in this age of illegal downloading. It’s much easier to torrent a film than a to-scale toy replica of the Death Star.
Disney has made deals with scores of companies to create Star Wars branded products, from CoverGirl to Lego. Hasbro released more than 100 new toys in the lead-up to the release of The Force Awakens. Walmart announced it would carry more than 500 products in store and thousands more online.
How much does the studio stand to make? It’s estimated that for every branded item sold, Disney takes a cut between 10 and 15 percent. Star Wars merch is expected to bring in $5 billion in sales over the coming 12 months, rising to as much as $20 billion in the next five years.
While the typical image of a Star Wars collector is a fanboy, women are a big focus of The Force Awakens’ merchandise campaign. This is likely the effect of two forces — the popularity of the new film’s heroine Rey and a growing awareness of the female market.
CoverGirl has released a Star Wars–branded makeup collection, Hot Topic has a clothing line. In one Walmart ad, a young girl plays with Star Wars toys as her mom asks why the princess doesn’t let boys rescue her. "Because she’s a modern, empowered women," the girl replies, "unfettered by the antiquated gender roles of a bygone era." It has been viewed nearly 21 million times.

China’s maturing market also offers big opportunities for Disney. This month, the studio’s chair, Andy Bird, pitched Star Wars: The Force Awakens–branded merchandise to more than 800 Chinese cinema owners, opening the door to millions of potential collectors.
At the moment, box office takings still account for around 80 to 90 percent of film revenue in China. Over the next few years, though, it’s expected the country will come to account for a huge chunk of the $241.5 billion in global annual licensed merchandise retail sales.
While there’s been a lot of focus on Star Wars products, building excitement around the film and actually getting people into cinemas is still vitally important. If you’ve stepped outside your house in the past few weeks, you would’ve seen Star Wars everywhere — on billboards as you walk to work, TV ads, radio spots, endless articles analyzing the teaser trailers as you scroll through Facebook. Disney didn’t get all that hype for free.
The average global marketing budget for a "tentpole" film (a release whose profits fund smaller productions) is around $100 million. Building hype overseas is increasingly important for blockbusters. Since 1999’s Phantom Menace, the franchise has been making more money overseas than at the US box office.

Long before Star Wars was on every billboard, though, Disney started building hype for the merchandise. In September, the film’s YouTube channel hosted a worldwide "unboxing," where fans in 12 countries unveiled the new toys via live stream. This came just ahead of Force Friday, the first release of official Force Awakens merch.
Much like the film’s plot, the tie-in merchandise was shrouded in secrecy, protected by nondisclosure agreements between Disney and its retail partners. What we do know is that companies making Star War–branded merchandise have already smashed the previous advertising record of $26.5 million spent to promote Minions.
Subway and Dodge have both released ads. Duracell’s "Fight for Christmas Morning" has been viewed 15 million times on YouTube. The ubiquitous Star Wars ad even earned itself a spoof on Saturday Night Live.

Star Wars merchandise occupies a unique position in the film world. Don’t get me wrong — there are certainly some ghastly offerings. Suggestive C-3PO tape dispenser, anyone?

But on the whole, the Star Wars merchandise isn’t foisted on fans; they covet it. It’s this love that Disney will be riding all the way to the bank. It’s just a bonus the film is pretty great as well.
Martin Shkreli became one of the most hated men on the internet back in September when he raised the price of an essential drug, Daraprim, more than 5,000 percent. Shkreli seemed to revel in his notoriety, firing taunts right back at people who attacked him on social media.
This morning, federal officials arrested the entrepreneur at his apartment in Manhattan. And it appears the charges have nothing to do with the Daraprim price hike that made him infamous online.
Instead, Shkreli is charged with defrauding his investors. According to the federal indictment, Shkreli moved funds between companies he co-founded under false pretenses, effectively stealing millions from the investors of one company in order to settle legal disputes involving another company.
It’s a sudden and shocking downfall. And he’s not likely to get much sympathy from his legions of online haters.
Rich Howells" data-chorus-optimize-field="main_image">

Rich Howells">
Martin Shkreli. (Rich Howells)
Toxoplasmosis is a parasitic disease. It’s harmless in healthy people, but in those with weakened immune systems it can cause headaches, fever, fatigue, and seizures. Expectant mothers are particularly at risk, as toxoplasmosis can cause miscarriage.
Back in 1953, Nobel Prize–winning American scientist Gertrude Elion developed a drug called Daraprim, which can treat both toxoplasmosis and malaria. It’s included in the World Health Organization’s List of Essential Medicines — the basics required for any health system.
Until recently, Daraprim wasn’t very expensive. In 2014, a single pill cost $13.50 in the United States. Then a Shkreli company, Turing Pharmaceuticals, came onto the scene. Shkreli bought up the rights to Daraprim and immediately jacked up the price to $750 per pill.
The extortionate move caused national uproar. Unlike in the UK and Europe, US law allows drug companies to set their prices with little regulation. But that didn’t mean people had to like it.
Bernie Sanders publicly refused a $2,700 campaign donation from Shkreli. Donald Trump called him a "spoiled brat." People began to label him the "pharma bro." He’s become so toxic even PhRMA, the biotech industry group, has distanced itself from him.

.@TuringPharma does not represent the values of @PhRMA member companies.


Shkreli seemed to revel in this hatred. After Hillary Clinton criticized Daraprim’s pricing, he simply tweeted back, "lol."
He’s bragged about being the world’s most eligible bachelor and taunted DC lawmakers.

50-100 date solicitations a day for me, the world's most eligible bachelor. Sorry, but you have to be a shareholder to meet me.



In DC. If any politicians want to start, come at me. pic.twitter.com/YrxWoSPQ1H



(Isaiah Trickey/FilmMagic)
Earlier this year, the seminal rap group Wu-Tang Clan finished their seventh album, Once Upon a Time in Shaolin. A single copy of the recording was made, 31 new songs presented in a hand-carved wooden box with a price tag of $2 million.
"We’re about to put out a piece of art like nobody else has done in the history of music," RZA told Forbes. "We’re making a single-sale collector’s item. This is like someone having the scepter of an Egyptian king."
The album was sold through Paddle8, an online bidding agency, which organized private listening parties for prospective buyers in New York. When the buyer’s name was kept anonymous, fans speculated it might have been Quentin Tarantino.
No one was excited when it was revealed the buyer was Shkreli. It’s reported his favorite Wu-Tang song is "C.R.E.A.M," which stands for "Cash Rules Everything Around Me."

Private album just for me. Who's next?


Shkreli is supposedly yet to listen to the album, saving it "for a rainy day." RZA has given away most of his proceeds from the sale in protest of Shkreli’s business practices.

Within 10 years, more than half of all rap/hip-hop music will be made exclusively for me. Don't worry--I will share some of it.


While the public hatred was mostly focused on Shkreli’s arrogance and greed, it’s his creative accounting that may prove his real downfall.
Early Thursday he was led from his apartment in handcuffs, charged with misusing $11 million in company funds from Retrophin, a biopharmaceutical company he founded in 2011.
In September 2014, Retrophin dumped Shkreli as chief executive
Before he became a notorious pharmaceutical executive, Shkreli was something of a Wall Street wunderkind. Starting as an intern at Mad Money host Jim Cramer’s hedge fund at age 17, he worked his way up to found three companies in his mid-20s — Elea Capital Management, MSMB Capital Management, and MSMB Healthcare. Then at just 28 years old, he shifted to pharmaceuticals, launching Retrophin.
In September 2014, Retrophin dumped Shkreli as chief executive and launched an investigation into his actions at the helm. Shkreli then founded Turing, the firm whose extortionate pricing for Daraprim made him famous. Meanwhile, Retrophin’s board found that Shkreli had used company funds to settle the debts of MSMB, which he’d left in financial ruin.
According to the federal indictment unsealed on Thursday, Shkreli and his outside counsel Evan Greebel orchestrated "three interrelated fraudulent schemes" between September 2009 and September 2014. The pair allegedly defrauded investors in MSMB Capital and MSMB Healthcare, before using funds from Retrophin to pay off both companies' debts.
In a filing with the Securities and Exchange Commission, Retrophin reported that Shkreli had used Retrophin funds to pay off former MSMB investors, as well as to settle personal lawsuits. According to the company, Shkreli fraudulently reclassified a $900,000 investment MSMB made in Retrophin as a loan. Shkreli then allegedly had Retrophin pay this "loan" back to MSMB — enabling the hedge fund to use the money to settle one of its many legal disputes. And while Shkreli founded both companies, that doesn’t give him the right to shift money between them without consent from the companies’ boards.
Between September 2013 and March 2014, 612,500 company shares were issued to former MSMB investors. While Shkreli described them as "consulting agreements," Retrophin now says that was a ruse. Rather, their purpose "appears to have been to settle and release claims against the MSMB Entities or Mr. Shkreli personally, and not to provide meaningful and sustained consulting services to the Company."
In August this year, Retrophin sued Shkreli for $65 million. The former chief quickly shot back, demanding $70 million from the firm and citing damage to his image as a businessman.
Even before Retrophin announced the findings of its internal investigation of Shkreli, there had been a criminal investigation underway by the US Attorney for the Eastern District of New York.
In January, Retrophin received a subpoena requesting "information regarding, among other things, the Company’s relationship with the MSMB Entities and Mr. Shkreli." In its SEC filings Retrophin made it clear that Shkreli, not the company itself, was the focus of the criminal investigation.
Share prices of KaloBios Pharmaceuticals, a company Shkreli recently acquired a majority stake in, fell 50 percent today before Nasdaq halted its trading indefinitely. Shkreli and Greebel, who was also arrested, are yet to comment or tweet about the charges.
On the internet, people have been having fun with Shkreli's arrest:

Martin Shkreli's bail was going to be set at $500,000 but they raised it to $27,500,000 just for him.

The basic dilemma facing the Federal Reserve is this: Low interest rates promote economic growth but create the risk that the economy will "overheat" with too many dollars chasing too few actual goods and services — causing inflation. Today, the Fed announced it was raising rates for the first time since 2006, signaling that it is starting to worry more about inflation and less about jobs and growth.
The weird thing about this is that the Fed's own forecasts show inflation getting lower, not higher.
Today, as it does every quarter, the Fed released a document showing how the people on the Fed's Open Market Committee — which makes the Fed's interest rate decisions — predict various economic variables will change in the next few years. And the latest projections show something surprising: The Fed's decision-makers have revised their projection for 2016 inflation down. In September, the Fed thought inflation would be 1.7 percent in 2016. Now the central bank thinks it will be just 1.6 percent.
Obviously that's a small difference, but it's also a telling one. The Fed is raising rates because it's worried that keeping rates low for too long will cause inflation to "overshoot," rising significantly above 2 percent and forcing the Fed to raise rates rapidly to keep inflation under control.
Yet the Fed's own actions tell a different story. In September, Fed officials projected that inflation would be only 1.7 percent in 2016, so it kept rates low. Now the Fed thinks that inflation will be even lower — 1.6 percent — in 2016, yet it's raising rates anyway.
This doesn't make sense.
Suppose you're riding a bus from Washington, DC, to New York, and you want to know if the bus will stop in Philadelphia. If the passenger next to you says the bus is going to stop in Philadelphia, that's a projection. But if the driver of the bus tells you the bus is going to stop in Philadelphia, it doesn't make sense to call that a projection — the driver is just announcing a decision he's made.
The same point applies to the Fed. The Fed has a ton of influence over future inflation rates, so its "projections" aren't really projections at all; they're statements about the Fed's own priorities. If the Fed raises rates at the same time it "projects" that it's going to miss its inflation target by even more than previously expected, that's a signal that the Fed isn't actually serious about hitting the target.
The Fed has been consistently overestimating inflation rates for several years. At the final meeting of 2012, Fed decision-makers projected that "core" inflation — excluding volatile food and energy prices — would be at least 1.8 percent in 2015. That was revised downward to 1.6 percent in 2013 and 1.5 percent in 2014. Now it looks like core inflation will be 1.3 percent in 2015.
Scott Sumner, an economist at the Mercatus Center, argues that if the Fed is serious about reaching a 2 percent inflation target, it needs to act like it's serious.
"If they could craft a clear statement of what they're trying to do, I think that would help," he says. He argues that if the Fed announced that it won't raise rates again until policymakers are confident that they'll actually hit the 2 percent goal, that would actually boost the economy and help the Fed hit its own target.
It might seem like this doesn't matter very much — 1.3 percent isn't that different from 2 percent, and no one likes to see prices go up anyway.
But low rates don't just produce more inflation; they also produce more job and wage growth. Low rates encourage businesses and consumers to borrow and invest, generating more demand for products and services. Companies respond to that demand by hiring more people. As the labor market tightens, employers are forced to offer the employees raises.
That kind of economic boom would be good for almost everyone. The main downside is that it could produce too much inflation. But right now, inflation is at its lowest level in decades, suggesting that the Fed has an opportunity to boost the economy without creating dangerous inflation. And the Fed is blowing it.
At 2pm, the Federal Reserve made it official: it is raising a key interest rate for the first time since 2006. The move was widely expected, but it also remains somewhat controversial, because critics say it could hamper a still-anemic recovery.
Arguments about Fed decisions tend to get pretty deep pretty fast. What tends to get lost in the shuffle are the most fundamental and important issues: that a somewhat obscure government agency exercises enormous control over the economy by changing the price of money at regularly scheduled meetings.
Since the state of the economy ends up influencing everything from your ability to get a new job to the outcomes of presidential elections, that makes these meetings one of the most important events on the calendar. Yet they're rarely discussed outside specialist circles except by the occasional crank, leaving ordinary people in the dark.
An interest rate is the price lenders charge to borrowers to use their money. An interest rate of 5 percent means that someone borrowing $1,000 will have to make interest payments of $50 per year — in addition to eventually paying back the amount that was originally borrowed.
There are different interest rates for different types of lending — home mortgages, business loans, credit cards, and so forth.
But when economists talk about the Fed "raising interest rates," they're referring to a specific rate called the federal funds rate. That's the rate big banks charge one another for short-term loans.
The way the Fed manipulates the federal funds rate has broad economic effects
People often talk about the Fed "setting" this interest rate, but that's not quite accurate. What happens is that the Fed announces a target for the federal funds rate and then uses its ability to create or destroy money to reach its target.
Of course, these actions don't only affect the federal funds rate. When the Fed pushes the rate up or down, it tends to push other interest rates in the same direction as the federal funds rate. So ultimately, the Fed's interest rate decision will have an impact on the rates you pay the next time you borrow money — whether it's with a mortgage, an auto loan, or a credit card purchase.
By itself, the federal funds rate isn't especially important to anyone but bankers. However, when the Fed manipulates the federal funds rate, it can have broad economic effects.
Money is an essential fuel for economic activity
This is often described mechanically, as a question of the interest rates spurring or strangling economic activity. For example, if mortgage rates rise, it becomes harder for people to buy new houses, which can hurt employment in the construction industry. If interest rates for business loans go up, it becomes harder for companies to finance the construction of a new factory. And so forth.
That's all true, but it can also introduce confusion because causation can move in the other direction. When economic activity is robust there's a lot of demand for loans, which can pull interest rates up. And focusing too much on specific lending markets can obscure a more fundamental point about why the Fed's decisions matter: Money is an essential fuel for economic activity. Recessions happen when people spend less than they did before. Booms happen when people spend more. So all else being equal, putting more money into people's pockets is going to produce more demand for companies' products, more economic activity, and more jobs.
At every meeting since 2008, the Fed has decided to keep interest rates near 0 percent.
But this time, the Fed announced that it would raise its target for the federal funds rate to 0.25 percent. That's the first interest rate hike since 2006.
At the same time, it issued a statement signaling that it won't do further rate hikes too quickly, giving the economy more room to grow.
If low interest rates are so good for the economy, you might be wondering why they should ever be increased. The reason is that pumping more money into the economy only works up to a certain point.
During a recession, there are a lot of idle resources. People are unemployed, factories are producing below their maximum capacity, trucks and ships sit empty a lot of the time, and so forth. In that situation — the kind of situation we had in 2001 and 2009 — getting people to spend more will mobilize idle resources and boost the real output of the economy.
The traumatic inflation of the 1970s looms large in the minds of senior Fed policymakers
But during an economic boom, things look different. With few idle resources sitting around, there's no way for more consumer spending to translate to more output. If the Fed cuts rates during a boom, the result is likely to just be that prices go up — inflation — without generating much economic growth.
That's what happened in the late 1970s. The Fed kept interest rates too low for too long because it feared that higher interest rates would be economically harmful. That produced double-digit inflation that created chaos for many Americans.
The traumatic inflation of the 1970s looms large in the minds of senior Fed policymakers, most of whom are old enough to remember it firsthand. They're determined not to repeat the mistakes of their predecessors and let inflation get out of control.
The theoretical case for raising rates to ward off inflation is strong. But the case for raising rates right now runs into a huge problem: Inflation is really low right now. It's been low since 2008, and market forecasts suggest that it will continue to be low over the next decade.
Like many countries around the world, the Fed has set an inflation goal of 2 percent. Yet over the last year, prices — as measured by the consumer price index — have increased by just 0.5 percent. That's mostly because oil prices have been falling; if you exclude volatile food and energy prices, the inflation rate is 2 percent — exactly in line with the Fed's target. Another inflation measure that's a favorite of the Fed's, called the core personal consumption expenditure index, currently stands at 1.3 percent — below the 2 percent target. Moreover, markets are projecting that the average inflation rate will be below 2 percent over the next decade.
If inflation shows signs of picking up, the Fed can always raise interest rates later
And while the economy has been doing pretty well, there's reason to think it could be doing better. True, the unemployment rate is down to 5 percent, not too far from what economists regard as the full-employment level. But the labor force participation rate — the fraction of all adults participating in the labor force — is close to a 30-year low, suggesting that an economic boom might draw more people into the labor market. The economy has been growing at a respectable but not spectacular rate, and wages have barely been growing faster than inflation.
We don't know if keeping interest rates low will boost economic growth. But given that the inflation rate is actually below the Fed's target, it seems there's not much risk in giving it a try. If inflation shows signs of picking up, the Fed can always raise interest rates later.
People have made a number of arguments in favor of raising interest rates, but on some level they all boil down to the view that seven years of ultra-low rates is unnatural.
Prior to 2008, it had been many decades since the federal funds rate was zero, and a lot of people find the current interest rate environment deeply unnerving. As Vox's Matt Yglesias has written, there's a widespread view that zero percent interest rates are a kind of life-support measure for the economy. Now that the patient is recovering, people think, we should remove the breathing tube so he can get back to breathing normally.
What happens if we keep the patient on zero-percent-interest life support? As we've seen, people normally worry that low interest rates will generate high inflation. And in the first few years after the Fed slashed rates in 2008, a lot of people warned that inflation was just around the corner. But after seven years of low interest rates and low inflation, those fears have started looking a bit silly.
So today, advocates of higher rates mostly focus on bubbles. A good example is Sen. Rand Paul (R-KY), son of longtime Federal Reserve critic, gold standard advocate, and former Rep. Ron Paul (R-TX). The younger Paul co-authored an op-ed for the Wall Street Journal in September blaming low interest rate policies over the past 20 years for the stock market bubble of the late 1990s and the real estate bubble that popped in 2007.
In Paul's view, prolonged periods of low interest rates encourage people to make risky, unsustainable investments. Recessions, in his view, are a painful but necessary process that purges the economy of bad investments. When the Fed keeps rates "artificially" low, it merely prolongs the day of reckoning and allows these bubbles to get bigger than they otherwise would have gotten. Hence, because the Fed tried to cushion the 2000 stock market crash with low interest rates, we got an even bigger crash in 2008. Paul predicts we'll have a third crash — perhaps even bigger than the previous two — as a result of current Fed policies.
But this argument doesn't explain how to tell whether rates are "too low." The federal funds rate was around 5 percent in the late 1990s — that was low relative to the previous couple of decades, but it was actually higher than rates for most of the 1950s and 1960s. There's widespread agreement among monetary hawks that monetary policy should be more "normal" — i.e., not zero — but little clarity about how high rates need to be to avoid bubbles or other financial calamities.
Sure thing. Listen to the classic Dire Straits song "Money for Nothing."

The song is written from the perspective of ordinary workers who envy rock stars on MTV who get "money for nothing and the chicks for free." Meanwhile, regular guys have to "install microwave ovens," do "custom kitchen deliveries," and move refrigerators and color TVs.
Obviously, monetary policy is never going to remedy this kind of inequality. Someone has to install microwave ovens and do custom kitchen deliveries, so we're never going to live in a world where everyone gets to enjoy the perks of being a rock star full-time.
But there's still a lot monetary policy can do to help those guys wrangling refrigerators and color TVs. For most of the past seven years, it was hard for regular guys (and girls) to earn a living even if they were willing to do unglamorous work like installing microwave ovens. Pumping money into the economy couldn't turn those guys into rock stars, but it did generate economic activity and make it easier for them to find jobs.
And while the labor market is a lot better than it was a few years ago, there's still room for improvement. Wages for low-end workers have been stagnant for more than a decade. If we had a few years of tight labor markets — like we had in the late 1990s — ordinary workers would have more bargaining power. Many would get raises. That's why the guys who do custom kitchen deliveries might want to root for the Fed to keep interest rates low.
It's certainly true that seven years of zero percent interest rates was historically unusual. But whether recent Fed policies have been too tight, too easy, or just about right is open to debate.
There's a lot monetary policy can do to help those guys wrangling refrigerators and color TVs
It's helpful to think about a time when the Fed was in a very different situation. The late 1970s was a period of high interest rates. By the start of 1979, the federal funds rate had risen above 10 percent.
Yet inflation soared, reaching a high of 14.8 percent in March 1980, and it stayed above 10 percent until well into 1981. That's a sign that even the historically high rates of early 1979 weren't enough to keep inflation under control. With interest rates above 10 percent, monetary policy might have seemed tight, but it was actually too loose. As it turned out, the Fed had to let rates go as high as 19 percent in 1981 in order to get inflation under control.
Interest rates were high because the market was factoring high expected inflation into interest rates. If you lend money at 10 percent but the inflation rate is 12 percent, you're actually losing money! So the "natural" interest rate — the rate that struck the best balance between inflation and recession — was abnormally high.
Today we're in the opposite situation. Inflation expectations are low. The US population and economy are growing slowly, which limits demand for credit. And that means the natural rate of interest may be a lot lower than it was three or four decades ago.
The US isn't alone here. Interest rates are low across the developed world. Japan has had short-term interest rates near zero for two decades. The eurozone, the United Kingdom, Canada, and Australia all have interest rates at their lowest levels in decades.
And the experience of the eurozone suggests this isn't really the fault of central banks. As economist Scott Sumner has pointed out, the European Central Bank tried raising rates in 2011, believing the worst of the recession was over. The result was a double-dip recession that quickly forced the ECB to bring rates back down.
The US economy is now stronger than the Eurozone was in 2011, so this week's rate hike probably won't trigger a recession. But the low rates of the past few years aren't really the doing of central banks. Central banks are just reacting to market signals — cutting rates when unemployment rises, raising them when inflation becomes a problem — and the result has been historically low interest rates.
The Fed and other central banks have been setting interest rate targets for so long that a lot of people think of monetary policy and interest rate changes as synonymous. But there's actually no law requiring the Fed to do monetary policy this way. Fundamentally, the Fed conducts monetary policy by creating money and buying stuff with it. There's no reason the amount of money they create needs to be determined by an interest rate target.
One example of this was between 2008 and 2014, when the Fed engaged in a technique called quantitative easing. The federal funds rate had already reached zero, so the Fed couldn't drive it any lower. But the Fed still wanted to do more to support an economy that was in a major recession. So the Fed just announced that it was going to create a certain amount of money every month. It worked fine, and many economists believe it helped speed the economic recovery over the last seven years.
Still, quantitative easing has two big disadvantages. One is that it's pretty ad hoc. It's hard for the Fed to know how much money to print or how long the money-printing process should go on.
The even larger problem, though, is political. Because the Fed's "normal" monetary policy approach is to target interest rates, quantitative easing generally gets labeled "unconventional" or "extraordinary" — even though the actual mechanism of printing money and buying government securities is very similar in both cases. This tends to create a political backlash and make the Fed reluctant to use QE as forcefully as might be appropriate.
The Fed twice halted its bond-buying programs — once in 2010 and again in 2012 — before bad economic news forced them to restart them. This tentative approach may have hampered the economic recovery.
A different approach would be to stop targeting interest rates and instead directly target a variable the public cares about, such as the growth of total spending in the economy. In an approach known as nominal GDP targeting, the Fed would commit to printing enough money so that total spending in the economy grows at 5 percent per year.
The Fed's interest rate decisions might seem pretty remote, but they can actually have a big impact on every American. When the Fed keeps interest rates low, it means there will be more money flowing through the economy, which is likely to mean more economic activity and more jobs.
By itself, this week's 0.25 percent rate hike isn't going to have a big impact on the US economy. But it's significant because it could signal the start of a sequence of interest rate hikes that could have significant effects on the economy. The Fed has tried to mollify those fears with a statement signaling that it won't raise rates too quickly in the future.
Of course, if the Fed keeps rates low for too long, the economy could overheat, producing inflation. But right now there just isn't much evidence that the economy is overheating. The inflation rate is well below the Fed's 2 percent target, and the economy has been adding jobs more slowly than in previous economic expansions.
So if you'd like to see the economy grow more quickly, unemployment fall, and wages rise, then you might see this week's decision to raise rates as premature. In contrast, if you're most worried about inflation, you should be happy that the Fed has started to raise rates — just to be on the safe side.
After seven years of keeping a key interest rate near zero percent, the Federal Reserve has voted for a rate increase. The decision signals the central bank's growing confidence in the economy.
The Fed is raising its target for the federal funds rate — the rate banks charge when they lend money to one another — from 0 percent to 0.25 percent. By itself, that modest increase isn't going to have a big impact on the American economy. But the move is significant because it's widely seen as the first step in a longer sequence of rate increases over the next couple of years.

In a statement accompanying the rate increase, the Fed tried to signal that it would not move too quickly on further rate increases. "The stance of monetary policy remains accommodative after this increase, thereby supporting further improvement in labor market conditions and a return to 2 percent inflation." In plain English, the Fed is worried that moving too quickly could strangle the still-fragile economic recovery.
An obscure federal agency raising an obscure interest rate might not seem like big news. But Fed decisions have broad impacts on the US economy.
What really matters here isn't the fact that a particular interest rate is going up — it's the way the Fed is going to accomplish the interest rate hike. The Fed has the power to create or destroy money at will, which allows it to make cash more scarce (pushing up the amount of money people will pay to borrow it) or more plentiful (pushing interest rates down). For the past seven years, the Fed has been flooding the market with cash in the hope that this will boost the economy. Now the Fed is starting to reverse course.
So when the Fed raises its target rate, what it's really doing is signaling that it's going to make cash scarcer across the entire economy. That is likely to push up the interest rates on other loans — mortgages, car loans, credit cards, and so forth. It's also expected to have broad economic effects, slowing the rate at which the economy grows and creates jobs.
Indeed, in a sense this is the point of the rate hike: The Fed is worried that keeping rates too low for too long will trigger inflation, or possibly another bubble like the real estate bubble of 2007 and the technology bubble of 1999. By starting to tap on the brakes now, the Fed hopes to head off an overheated economy before it happens.
The weird thing about this is that inflation is actually below the Fed's 2 percent target. It's been below target for most of the past seven years, and markets expect the rate to  continue to be below target, on average, over the next decade. So arguably, the Fed is exacerbating a real problem — sluggish job and wage growth — to deal with a problem that has yet to materialize.
While most of the media coverage so far has focused on whether the Fed would raise rates at this week's meeting, the more important question is what the Fed does in the future — and what it says about its plans in the statement it releases with the announcement.
When businesses, venture capitalists, and others make investment decisions, a big factor they consider is the likely rate of future economic growth. If they expect a big boom over the next couple of years, they're more likely to make investments now to capitalize on the healthy economy. And one big factor shaping the economy's future growth is the Fed's interest rate decisions.
And that means that the Fed's statements about future interest rate hikes can have a significant impact on the economy now. The Fed's relatively dovish statement — signaling that its policy "remains accommodative" — is a signal to investors that they shouldn't be too spooked by a quarter-point rate increase. The Fed might have raised rates this week, but it's not going to do further increases too quickly.
When I bought a house back in June, my lender encouraged me to lock in our interest rate quickly because mortgage rates couldn't stay so low — they were around 3.85 percent at the time — for much longer. Six months later, the average mortgage rate in the Washington DC area is still 3.85 percent.
People have been predicting imminent rate hikes for years: Rates were said to be about to go up in 2013, in 2011, and in 2009. In 2010, a New York Times columnist predicted that "interest rates have nowhere to go but up." Mortgage rates are now lower than they were at the time any of those articles was written.
It's not too surprising that people were fooled. If you compare today's rates with those that prevailed in the 1970s, 1980s, or 1990s, they look freakishly low. But the New York Times's Neil Irwin points out that if you take a longer-term perspective, today's low rates don't look so anomalous:
The interest rate on 10-year government bonds is currently 2.2 percent, about the same as it was in the 1930s, 1940s, and 1950s. If you take this longer perspective, the high interest rates of the past few decades look like the anomaly, and today's 2.2 percent rate looks like a return to normal.
Irwin argues that this is mostly about inflation. When inflation is high, people demand higher interest rates to compensate for the declining value of the principal. So when inflation skyrocketed in the '70s, interest rates skyrocketed too — and fears that inflation would return kept rates high throughout the '80s and '90s, even as inflation fell. Only recently have interest rates returned to a more historically normal level.
Irwin is clearly right that low inflation is an important reason for interest rates to be low. But another big factor is the changing demographics of the American economy.
Many factors contribute to interest rates, but on the most fundamental level they reflect a market judgment about how much opportunity there is to generate future wealth from present-day investments.
When the economy is growing quickly, there are lots of ways to turn dollars into productive assets — factories, roads, houses — which can then generate wealth in the future. So the competition for investment dollars is strong, and interest rates rise.
On the other hand, if tomorrow's economy won't be much bigger than today's, then we'll be able to serve most of our needs with the factories, roads, and houses we already have. So few people will want to borrow and invest, pushing interest rates down.
Japan's population has actually been falling the past few years
We can't say exactly how fast the economy is going to grow over the next few decades, but one thing we can be pretty sure about is that the US population is going to be growing more slowly. In the 1960s, investors could look forward to a future with many more people — and many more potential consumers — than there are now. Today, that's not as true. The US population is still growing, but thanks largely to falling birthrates, it's growing at about half the rate it did in the mid-20th century.
And the population growth rate has fallen even more in other rich countries. Japan has 127 million people, barely more than the 124 million it had in 1990, and its population has actually been falling the past few years. Italy has 61 million people, just 7 percent more than the 57 million who lived there in 1981.
If your country's population has stopped growing, there's little point in building a bunch of new houses, stores, or office buildings. There's less need for new roads, schools, or other infrastructure. Of course, you'll still need to spend some money repairing or replacing facilities that become obsolete or worn out, but the total amount of investment spending is going to be a lot lower.
It's not a coincidence that Japan — the rich country with the biggest population slowdown — has also been stuck in a low-inflation, low-interest rate, low-growth rut for longer than other rich countries. It's hard for a country with a shrinking workforce to have a growing economy, and without economic growth there's little demand for borrowing.
On the other hand, Australia is one of the world's fastest-growing rich countries. Over the past 20 years, Australia's population has grown 33 percent — about the same as the US growth rate between 1950 and 1970. And it's probably not a coincidence that Australia (blue line) has higher interest rates than the United States (green), while Japan's (red) interest rates are lower.
The United States still has a growing population and a growing economy. But both are growing more slowly than in decades past, and as a result we're experiencing a milder version of Japan's doldrums. One symptom of that is lower interest rates. And since America's low population growth isn't likely to change any time soon, its low interest rates may not either.
A lot of people have pointed to the Federal Reserve as the cause of today's low interest rates, but the Fed has less power than people think. It's true that the Fed controls short-term interest rates, and is expected to raise those rates this week for the first time in nine years.
But long-term interest rates are controlled by the market, not directly by the Fed. And in practice, the Fed's ability to raise even short-term rates are constrained by market conditions. If the Fed raises rates too high or too quickly, it will trigger a recession, forcing the Fed to cut rates once again. So in practice, broad economic trends, not the whims of Fed Chair Janet Yellen, are the main reason interest rates are so low.
Everybody hates Comcast. The cable giant consistently ranks last or near last among all companies on consumer satisfaction surveys. Hurling insults at Comcast — its prices, its speeds, its customer service — has risen nearly to the level of a national pastime.
But what if there's nothing the company can do to change its customers' minds? What if most of what people hate about Comcast has its roots in the structure of America's cable market?
That's what the company's CEO, Brian Roberts, suggested last weekend when asked about the company's poor record in an interview with Business Insider founder Henry Blodget.
Roberts argues that people might hate Comcast, but they hate other cable companies too:
We're with all the other cable companies, within spitting distance of each other. As a group, that is what the results show.
Like... "Well, your people didn’t show up on time" and therefore let’s fix that and will that actually change the score and suddenly we’re the best company in America? Google’s free. Facebook is free. We charge, and we collect for every piece of content rights. Every movie star. Every athlete. Every possible piece of content we pay.
We’re up to well in excess of $13 [billion] to $14 billion a year at this one company to procure that content on behalf of the consumer, and it’s grown on average as an industry about 8% to 12% a year compounded for a decade. If you drop a channel, you’re incredibly unpopular, and if you pass along a rate increase, you’re incredibly unpopular.
The problem isn’t Comcast’s service, Roberts is saying; it’s that people have to pay for it. Comcast operates by striking deals with content creators and publishers — ABC, CBS, FOX, ESPN, HBO, and the rest — for the right to broadcast their shows, movies, football, baseball, and basketball games. And as Roberts said, it doesn’t come cheap.
And there’s not much the company, or other cable companies, can do about that, in Roberts's view, because content companies have too much leverage.
One problem with Robert’s argument is that Comcast makes money too — a lot of it.
In 2014, it brought in nearly $69 billion in revenue, with $14.9 billion of that being operating income, a.k.a. profits.
So, yes, Comcast has to charge its customers, but it could charge them less if it wanted to. It could also invest more heavily in more and better-trained customer service workers. It could boost those data caps that customers are always complaining about.
To be fair, elsewhere in the interview Roberts described some of the company’s plans to improve its customer service, including an Uber-like tracking system for Comcast’s technicians.
But he seems to believe that it may just never be enough. No matter how hard Comcast tries to make its customers happy, they still wind up disgruntled. Customers just can’t stand paying for things, and the only way Comcast could really earn their love is by giving away its product, as Google and Facebook do.
The problem with this argument is that most companies do charge for their products, and few if any are as hated as Comcast. Indeed, the cable TV industry’s upstart rivals — Netflix, Hulu, Amazon Prime — charge their customers as well.
And customers don't hate Netflix the way they hate Comcast. In 2014, Comcast scored a 54 out of 100 on the American Customer Satisfaction survey — down from 64 in 2001. On the same survey, Netflix came in at 81. In eight years of measurement, it's never dropped below a 74.
One reason for this: Online streaming services charge a lot less. Netflix’s basic service costs $7.99 a month, as does Hulu’s. Comcast’s "digital starter" TV package, which doesn’t include premium channels like HBO, costs $49.99.
On the other hand, some companies charge a premium while still earning high marks, because their products are great. For instance, iPhones are way more expensive than a lot of Android-based smartphones, yet they’ve earned fanatical customer loyalty.
So maybe Comcast can’t please its customers. Maybe it’s doomed to its abysmal Yelp reviews and a rock-bottom reputation. But it could probably try a bit harder too. Who knows?
Disclosure: Comcast is an investor in Vox Media, the parent company of Vox.com.
Serial, one of the most popular podcasts of all time, returned last week with an episode chronicling the capture of Bowe Bergdahl, a US service member imprisoned for five years by the Taliban.
The show’s first season — a deeply researched reinvestigation of the 1999 murder of Hae Min Lee, a Baltimore teenager, and the subsequent trial and conviction of her ex-boyfriend, Adnan Syed — put podcasting on the map. Host Sarah Koenig, long of This American Life, narrated lengthy, penetrating interviews with Syed himself, speaking from prison, as well as painstaking recapitulations of every detail of evidence and testimony.
It made for gripping storytelling, and listeners ate it up. As of March of this year, the show’s first season’s 12 episodes had been downloaded a total of 75 million times. And that was part of a broader trend: Podcasting is becoming a mainstream phenomenon.
About 17 percent of Americans 12 or older, about 46 million people, listened to a podcast in the past month, up from 12 percent in 2013.

The medium is taking off now because of the happy convergence of three big trends. The technology has finally improved enough that listening to podcasts is easy and convenient for ordinary listeners. Talented professionals — many of them veterans of NPR or other radio outlets — have begun to focus on the medium. And a new generation of podcast-focused businesses are figuring out how to convert these professionally produced, popular podcasts into serious money.

raneko" data-chorus-optimize-field="main_image">

raneko">
The old and busted way to listen to podcasts. (raneko)
Podcasting has been around for about a decade — the term is a reference to the iPod, which older readers will remember as an iPhone that only played music. But while the idea of listening to music on an iPod quickly became popular, it’s taken a lot longer for the concept of listening to podcasts to catch on.
A big reason for this is that downloading podcasts to an iPod or other MP3 player was cumbersome. Typically, you had to subscribe to podcasts on your computer, download episodes, transfer the episodes over to your iPod player using a USB cable, and then listen to the episodes on your iPod.
And you’d have to repeat this ritual every time new episodes came out, which might happen every day, week, or longer. Most users simply didn’t have the patience for this. Some people listened on their computer; many didn’t listen at all.

The wide adoption of smartphones with mobile internet capabilities, beginning around 2010, removed these hurdles. Only 10 percent of Americans owned smartphones in 2009. This year, that number has jumped to 71 percent. Fans gained the power to tune in to podcasts much like they tuned in to radio shows, instantly, wherever they happened to be.

New software platforms like Stitcher, Overcast, and Castro, along with Apple's own undeletable Podcasts app, have emerged in recent years, making the process of aggregating, downloading, and streaming podcasts even easier.

Podcast listeners have migrated to mobile in droves. In just one year, from 2013 to 2014, the percentage of listeners who said they primarily listen on smartphones, tablets, and portable audio players rather than on a computer jumped from 34 percent to 51 percent.


Charles Wiriawan" data-chorus-optimize-field="main_image">

Charles Wiriawan">
(Charles Wiriawan)
The thing that’s really driven the widespread adoption, though, has been new technologies that make it easier to listen to podcasts in your car.
The market for in-car audio programming is huge. A disproportionate share of radio listening — about 44 percent — takes place in cars, compared with about 29 percent at home and 15 percent at work. More than 50 million Americans each week tune in to news-talk radio stations, which offer programming that sounds a lot like what you can find on many podcasts. Programs like The Rush Limbaugh Show and NPR's Morning Edition draw millions of listeners every day, and have done so for decades.
So you might have expected podcasts to disrupt the talk radio market the way digital music disrupted the record labels. But until recently, that wasn’t happening because — again — the technology was too cumbersome.
Driving with headphones is a bad idea no matter how smart your phone is. Would-be listeners could use tape-deck adaptors or digital transmitters, but both cost extra, were a hassle to use, and could mangle the audio quality.
But in recent years, carmakers have made it a lot easier to pipe audio from customers’ smartphones to their stereo systems. Carmakers added auxiliary audio inputs, then USB ports (which had the added benefit of charging your device). More recently, they’ve begun adding support for Bluetooth, which allows you to play podcasts from your smartphone without even taking it out of your pocket or purse.

Tech giants such as Google and Apple are investing heavily in the development of "connected car" platforms, which allow smartphones to totally take over the interface of cars’ in-dash entertainment consoles. One consequence of this is that listening to podcasts in your car is becoming as easy as listening to AM or FM radio.

Many early podcasts were amateur efforts; others were simply on-demand versions of radio programs produced by major outlets such as National Public Radio or BBC Radio. But recently, there’s been a new wave of professional, dedicated podcasters making shows with the same high production values you hear on the radio.
Serial’s Koenig is only the most famous example. Another This American Life vet, Alex Blumberg, co-founded NPR’s Planet Money podcast, and, more recently, created StartUp, a podcast chronicling the rise of a podcast network, Gimlet Media, that he co-founded with another former NPR producer, Matt Lieber.
Carmakers have made it a lot easier to pipe audio from customers’ smartphones to their stereo systems
The host of Gimlet's Mystery Show, Starlee Kine, is also a This American Life alum. The hosts of its Reply All, Alex Goldman and P.J. Vogt, were once producers at On the Media, and were hosts of its short-lived spinoff podcast TLDR. Roman Mars, host of 99% Invisible and founder of the PRX's Radiotopia podcast network, is also a public radio vet.

Other successful podcasters — such as Marc Maron, host of WTF, and Joe Rogan of The Joe Rogan Experience ­­— got their start in standup comedy, TV, commercial radio, or all the above.

Major media outlets have also jumped in with more conviction than before. Slate recently brought its various podcasts together under an umbrella organization called Panoply, whose owners recently bought a stake in Gimlet as well. Public Radio International and American Public Media have both recently launched networks as well, called SoundWorks and Infinite Guest, respectively.


Alex Blumberg and Matt Lieber, co-founders of Gimlet Media. (Yana Paskova/the Washington Post via Getty Images)
One reason the medium has been able to recruit so much talent recently is that podcasts are making money. Many podcast hosts, like some radio hosts, read or even write their own ads. Listeners connect with a host's voice and personality in a deeper and more intimate way than, say, the readers of newspapers or magazines or their web equivalents.
And talented podcasters are often talented admakers. A Mailchimp spot that aired during Serial — which featured a tourist mispronouncing the company's name ("Mail ... kimp?") — became a bona fide web phenomenon in its own right.
Podcast ads are lucrative. As of late last year, podcasters reported CPMs — cost per a thousand ad impressions, the standard industry metric — of between $20 and $45. Network TV programs, for comparison, earned about $5 to $20 per thousand impressions, radio ads made between $1 and $18 per thousand, and regular web ads between $1 and $20.
Those kinds of numbers have attracted a bit of a gold rush to the podcasting business.
Slate’s Panoply network produces podcasts, but it also offers distribution, sales, and audience development services.
Many shows on its 58-podcast roster are long-running Slate staples —such as the Political Gabfest and Culture Gabfest. Others are made in partnership with outside news organizations, such as the New York Times, New York Magazine, and Popular Science, which are choosing to take advantage of the platform Panoply offers rather than develop their own podcasting operations from scratch. (Vox’s own The Weeds podcast debuted on the network in September).
Gimlet Media — the startup featured in season one of StartUp — is, at least for now, a smaller operation, with just four shows in production. But its founder’s reputation and his idea of the company as an "HBO for podcasts" have attracted keen interest from investors.
Chris Sacca, an early backer of Twitter and Uber, was one of the first investors in the company. Just this month, Gimlet raised $6 million in Series A financing round that valued it at $30 million. Of that, $5 million came from Panoply’s parent company, Graham Holdings, former owner of the Washington Post.

The latest investment — discussed at length in a recent StartUp episode — will allow the company to add eight new shows and triple its headcount from 25 to 75 employees.
Podcasts may not replace radio talk any time soon. Live radio, live TV, and their streaming equivalents seem likely to dominate key niches like breaking news and sports into the future.
But smartphones and car integration mean podcasts will finally exist right beside radio, giving many more listeners a chance to choose between them. Some of radio’s top producers — along with some savvy investors — are betting they’ll choose podcasts more often.
Correction: This story originally misidentified This American Life as a National Public Radio program. It's actually produced by WBEZ and distributed by the Public Radio Exchange.
Imagine President Barack Obama announced a plan for a new tax that he said would raise the price of borrowing money in America. Every new mortgage would become more expensive. So would every auto loan and small-business loan. Towns and school districts would find the cost of new bonds elevated, as would large corporations. All across the land, credit availability would diminish.
Republicans would, of course, denounce him. Why would the president impose a new job-killing tax at a time when the American people have been suffering from an agonizingly slow labor market recovery and years of flat wages?
And then imagine the Democratic reaction when Obama explained that it wasn't his aim to spend the money on some new social program, or even use it to reduce the deficit. His only goal with the new tax was precisely to reduce the pace of job growth. To make sure that unemployment didn't get too low. That workers' bargaining power didn't become excessive.
After all, it's been a long, difficult recession, but the economy is a lot stronger now than it used to be. Let growth continue and wages might rise, forcing corporate profit margins to shrink until companies had no choice but to start raising prices.
"When it comes to inflation," the president might say, "it's better safe than sorry. So here's your new job-killing tax!"
It's unimaginable, of course. Congress, the press, and the public would all throw a fit. Yet at this point it is considered all but certain that the Federal Reserve is going to do exactly this by raising interest rates at its meeting next week. There's broad agreement among economists that this kind of tight-money policy leads to slower economic growth and fewer jobs being created. Yet it's happening with barely a word of public concern.
It's natural to wonder why any government official would ever decide that it makes sense to increase borrowing costs across the board and slow job growth. But the reason is inflation. At times, a central bank can boost growth with easy money, but there are fundamental limits to this strategy. A country that is out of workers, or out of natural resources, or out of machines and equipment or new ideas isn't going to be able to produce more stuff just because the central bank has made it cheap to borrow and spend. All that's going to happen is prices are going to rise.
The Federal Reserve is structured as an independent agency precisely on the theory that for the long-term good of the economy we sometimes want the central bank to slow the pace of job creation in order to avoid inflation, even though standing in front of a podium and saying, "I want to slow the pace of job creation" sounds terrible.
But the weird thing about this week's push for higher interest rates is that there's no inflation problem to solve.
Thanks to the global collapse in oil prices, there has been literally no inflation at all throughout 2015. If you ignore food and energy prices — which people often do, since they shift rapidly for non-monetary reasons — then inflation looks quite a bit higher. But it has still been below the Fed's 2 percent target for all of 2015. And it was below the Fed's 2 percent target for all of 2014. And it was below the Fed's 2 percent target for all of 2013. And it was below the Fed's 2 percent target for about half of 2012.
It's true that if you ignore food and energy prices, inflation is currently close to the Fed's 2 percent target. That's why the Fed is now eager to raise rates. It doesn't want to be caught "behind the curve" and facing a period in which inflation lingers above target as it enacts several rounds of rate increase. But given that we've survived three and a half years of consistently below-target inflation, it doesn't seem like being a little behind the curve would be the worst thing in the world.
The reason the Fed is now comfortable with the idea of a rate hike is that the labor market has improved considerably from where it was a few years ago. The unemployment rate is down to 5 percent and seems to be falling. That means the economy clearly can tolerate somewhat higher interest rates, and it's making the Fed eager to implement them.
But even though the labor market is in much better shape than it was a year or two ago, it's honestly still not in such great shape. A broad gauge of the labor market — the share of 25- to 54-year-olds who have a job — shows that something between 2 and 5 percent of the prime age population has vanished from the workforce:
It's not clear how many of these people could be tempted back into work — or would be considered hirable by employers — if we let the economy keep growing. But the number almost certainly isn't zero. The risk that keeping interest rates low a little too long could lead to a little bit of inflation needs to be balanced against the risk that slowing the pace of job creation could keep hundreds of thousands of people permanently trapped out of the labor force.
At a congressional hearing earlier this year, Fed Chair Janet Yellen was asked about the black-white unemployment gap and said basically that there's nothing she can do about it. If you delve into the data, it's easy enough to see what she means — the African-American unemployment rate and the white unemployment rate move in tandem, at a 2-to-1 ratio that seems to be fixed by factors that are out of control of monetary policy.
But as Jared Bernstein points out, this semi-fixed ratio actually means that monetary policy matters a great deal for the racial gap. If white unemployment goes from 10 percent to 5 percent, the Fed has achieved a 5 percentage point reduction. At the same time, we would expect black unemployment to fall from 20 percent to 10 percent — a much larger 10 percentage point reduction.
With the United States currently enjoying a lowish 5 percent unemployment rate, it's easy for relatively privileged people to neglect the benefits of further small reductions. But for an African-American population that will enjoy a double-scale version of any drop in the unemployment rate, the stakes remain quite high.
The same is true of other kinds of vulnerable populations. The college-educated cohort that dominates discussion of economic policy already has a very low unemployment rate. But working-class Americans could see considerable benefit from a stronger labor market.
Most economists think I am wrong and the Fed should raise rates. But the thinking behind this, as measured in things like the IGM Survey of prominent economists, is awfully fuzzy.
Anil Kashyap of the University of Chicago says he strongly agrees with a rate hike because, "As Mike Mussa once famously said, 'If not now, when?'"
Darrell Duffie of Standard says "the macro vital signs look healthy enough now."
The need for a rate hike has become so much part of the conventional wisdom that many supporters haven't articulated a rationale at all. But among those who are speaking — both in academia and on Wall Street — the predominant sentiment is one of impatience. Like Kashyap, many are simply sick and tired of waiting for a liftoff from the zero rates that have been in effect ever since 2008. And like Duffie, many feel that since the patient is now well enough to survive a rate hike, we may as well end life support.
But though this kind of impatience is understandable (I'm bored of having this argument too), it is ultimately not the point. The relevant question is whether, under the circumstances, the risks of a little bit of inflation are really worse than the risks of sluggish job growth. This is a subject that deserves to be debated squarely. Instead, it's been largely ignored by political authorities and even evaded by economists. But when rates go up, and six months from now politicians and voters alike are complaining that job growth has been too weak, they'll have the Federal Reserve — and their own inattention to it — to blame.