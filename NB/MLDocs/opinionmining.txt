Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 720–728,
October 25-29, 2014, Doha, Qatar. c 2014 Association for Computational Linguistics
Opinion Mining with
Deep Recurrent Neural Networks
Ozan ˙Irsoy and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY, 14853, USA
oirsoy, cardie@cs.cornell.edu
Abstract
Recurrent neural networks (RNNs) are connectionist
models of sequential data that are
naturally applicable to the analysis of natural
language. Recently, “depth in space” — as
an orthogonal notion to “depth in time” — in
RNNs has been investigated by stacking multiple
layers of RNNs and shown empirically
to bring a temporal hierarchy to the architecture.
In this work we apply these deep RNNs
to the task of opinion expression extraction
formulated as a token-level sequence-labeling
task. Experimental results show that deep,
narrow RNNs outperform traditional shallow,
wide RNNs with the same number of parameters.
Furthermore, our approach outperforms
previous CRF-based baselines, including the
state-of-the-art semi-Markov CRF model, and
does so without access to the powerful opinion
lexicons and syntactic features relied upon by
the semi-CRF, as well as without the standard
layer-by-layer pre-training typically required
of RNN architectures.
1 Introduction
Fine-grained opinion analysis aims to detect the subjective
expressions in a text (e.g. “hate”) and to characterize
their intensity (e.g. strong) and sentiment (e.g.
negative) as well as to identify the opinion holder (the
entity expressing the opinion) and the target, or topic,
of the opinion (i.e. what the opinion is about) (Wiebe et
al., 2005). Fine-grained opinion analysis is important
for a variety of NLP tasks including opinion-oriented
question answering and opinion summarization. As a
result, it has been studied extensively in recent years.
In this work, we focus on the detection of opinion expressions
— both direct subjective expressions (DSEs)
and expressive subjective expressions (ESEs) as de-
fined in Wiebe et al. (2005). DSEs consist of explicit
mentions of private states or speech events expressing
private states; and ESEs consist of expressions that indicate
sentiment, emotion, etc., without explicitly conveying
them. An example sentence shown in Table 1 in
which the DSE “has refused to make any statements”
explicitly expresses an opinion holder’s attitude and the
The committee , as usual , has
O O O B ESE I ESE O B DSE
refused to make any statements .
I DSE I DSE I DSE I DSE I DSE O
Table 1: An example sentence with labels
ESE “as usual” indirectly expresses the attitude of the
writer.
Opinion extraction has often been tackled as a sequence
labeling problem in previous work (e.g. Choi
et al. (2005)). This approach views a sentence as
a sequence of tokens labeled using the conventional
BIO tagging scheme: B indicates the beginning of an
opinion-related expression, I is used for tokens inside
the opinion-related expression, and O indicates tokens
outside any opinion-related class. The example sentence
in Table 1 shows the appropriate tags in the BIO
scheme. For instance, the ESE “as usual” results in the
tags B ESE for “as” and I ESE for “usual”.
Variants of conditional random field (CRF) approaches
have been successfully applied to opinion expression
extraction using this token-based view (Choi
et al., 2005; Breck et al., 2007): the state-of-the-art
approach is the semiCRF, which relaxes the Markovian
assumption inherent to CRFs and operates at the
phrase level rather than the token level, allowing the incorporation
of phrase-level features (Yang and Cardie,
2012). The success of the CRF- and semiCRF-based
approaches, however, hinges critically on access to an
appropriate feature set, typically based on constituent
and dependency parse trees, manually crafted opinion
lexicons, named entity taggers and other preprocessing
components (see Yang and Cardie (2012) for an up-todate
list).
Distributed representation learners provide a different
approach to learning in which latent features are
modeled as distributed dense vectors of hidden layers.
A recurrent neural network (RNN) is one such
learner that can operate on sequential data of variable
length, which means it can also be applied as a sequence
labeler. Moreover, bidirectional RNNs incorporate
information from preceding as well as following
tokens (Schuster and Paliwal, 1997) while recent
advances in word embedding induction (Collobert and
Weston, 2008; Mnih and Hinton, 2007; Mikolov et
720
al., 2013; Turian et al., 2010) have enabled more effective
training of RNNs by allowing a lower dimensional
dense input representation and hence, more compact
networks (Mikolov et al., 2010; Mesnil et al.,
2013). Finally, deep recurrent networks, a type of
RNN with multiple stacked hidden layers, are shown
to naturally employ a temporal hierarchy with multiple
layers operating at different time scales (Hermans
and Schrauwen, 2013): lower levels capture short term
interactions among words; higher layers reflect interpretations
aggregated over longer spans of text. When
applied to natural language sentences, such hierarchies
might better model the multi-scale language effects that
are emblematic of natural languages, as suggested by
previous results (Hermans and Schrauwen, 2013).
Motivated by the recent success of deep architectures
in general and deep recurrent networks in particular, we
explore an application of deep bidirectional RNNs —
henceforth deep RNNs — to the task of opinion expression
extraction. For both DSE and ESE detection,
we show that such models outperform conventional,
shallow (uni- and bidirectional) RNNs as well as previous
CRF-based state-of-the-art baselines, including the
semiCRF model.
In the rest of the paper we discuss related work
(Section 2) and describe the architecture and training
methods for recurrent neural networks (RNNs), bidirectional
RNNs, and deep (bidirectional) RNNs (Section
3). We present experiments using a standard corpus
for fine-grained opinion extraction in Section 4.
2 Related Work
Opinion extraction. Early work on fine-grained
opinion extraction focused on recognizing subjective
phrases (Wilson et al., 2005; Munson et al., 2005).
Breck et al. (2007), for example, formulated the problem
as a token-level sequence-labeling problem and apply
a CRF-based approach, which significantly outperformed
previous baselines. Choi et al. (2005) extended
the sequential prediction approach to jointly identify
opinion holders; Choi and Cardie (2010) jointly detected
polarity and intensity along with the opinion expression.
Reranking approaches have also been explored
to improve the performance of a single sequence
labeler (Johansson and Moschitti, 2010; Johansson and
Moschitti, 2011). More recent work relaxes the Markovian
assumption of CRFs to capture phrase-level interactions,
significantly improving upon the token-level
labeling approach (Yang and Cardie, 2012). In particular,
Yang and Cardie (2013) propose a joint inference
model to jointly detect opinion expressions, opinion
holders and targets, as well as the relations among
them, outperforming previous pipelined approaches.
Deep learning. Recurrent neural networks (Elman,
1990) constitute one important class of naturally deep
architecture that has been applied to many sequential
prediction tasks. In the context of NLP, recurrent neural
networks view a sentence as a sequence of tokens
and have been successfully applied to tasks such as language
modeling (Mikolov et al., 2011) and spoken language
understanding (Mesnil et al., 2013). Since classical
recurrent neural networks only incorporate information
from the past (i.e. preceding tokens), bidirectional
variants have been proposed to incorporate information
from both the past and the future (i.e. subsequent
tokens) (Schuster and Paliwal, 1997). Bidirectionality
is especially useful for NLP tasks, since information
provided by the following tokens is generally
helpful (and sometimes essential) when making a decision
on the current token.
Stacked recurrent neural networks have been proposed
as a way of constructing deep RNNs (Schmidhuber,
1992; El Hihi and Bengio, 1995). Careful empirical
investigation of this architecture showed that multiple
layers in the stack can operate at different time
scales (Hermans and Schrauwen, 2013). Pascanu et al.
(2013) explore other ways of constructing deep RNNs
that are orthogonal to the concept of stacking layers on
top of each other. In this work, we focus on the stacking
notion of depth.
3 Methodology
This section describes the architecture and training
methods for the deep bidirectional recurrent networks
that we propose for the task of opinion expression mining.
Recurrent neural networks are presented in 3.1,
bidirectionality is introduced in 3.2, and deep bidirectional
RNNs, in 3.3.
3.1 Recurrent Neural Networks
A recurrent neural network (Elman, 1990) is a class of
neural network that has recurrent connections, which
allow a form of memory. This makes them applicable
for sequential prediction tasks with arbitrary spatiotemporal
dimensions. Thus, their structure fits many
NLP tasks, when the interpretation of a single sentence
is viewed as analyzing a sequence of tokens. In this
work, we focus our attention on only Elman-type networks
(Elman, 1990).
In an Elman-type network, the hidden layer ht at
time step t is computed from a nonlinear transformation
of the current input layer xt and the previous hidden
layer ht−1. Then, the final output yt is computed
using the hidden layer ht. One can interpret ht as an intermediate
representation summarizing the past, which
is used to make a final decision on the current input.
More formally, given a sequence of vectors
{xt}t=1..T , an Elman-type RNN operates by computing
the following memory and output sequences:
ht = f(W xt + V ht−1 + b) (1)
yt = g(Uht + c) (2)
where f is a nonlinear function, such as the sigmoid
function and g is the output nonlinearity, such as the
721
Figure 1: Recurrent neural networks. Each black, orange and red node denotes an input, hidden or output layer,
respectively. Solid and dotted lines denote the connections of forward and backward layers, respectively. Top:
Shallow unidirectional (left) and bidirectional (right) RNN. Bottom: 3-layer deep unidirectional (left) and bidirectional
(right) RNN.
softmax function. W and V are weight matrices between
the input and hidden layer, and among the hidden
units themselves (connecting the previous intermediate
representation to the current one), respectively, while
U is the output weight matrix. b and c are bias vectors
connected to hidden and output units, respectively.
As a base case for the recursion in Equation 1, h0 is
assumed to be 0.
Training an RNN can be done by optimizing a discriminative
objective (e.g. the cross entropy for classifi-
cation tasks) with a gradient-based method. Backpropagation
through time can be used to efficiently compute
the gradients (Werbos, 1990). This method is essentially
equivalent to unfolding the network in time
and using backpropagation as in feedforward neural
networks, while sharing the connection weights across
different time steps. The Elman-style RNN is shown in
Figure 1, top left.
3.2 Bidirectionality
Observe that with the above definition of RNNs, we
have information only about the past, when making a
decision on xt. This is limiting for most NLP tasks.
As a simple example, consider the two sentences: “I
did not accept his suggestion” and “I did not go to
the rodeo”. The first has a DSE phrase (“did not accept”)
and the second does not. However, any such
RNN will assign the same labels for the words “did”
and “not” in both sentences, since the preceding sequences
(past) are the same: the Elman-style unidirectional
RNNs lack the representational power to model
this task. A simple way to work around this problem
is to include a fixed-size future context around a single
input vector (token). However, this approach requires
tuning the context size, and ignores future information
from outside of the context window. Another way to
incorporate information about the future is to add bidirectionality
to the architecture, referred as the bidirectional
RNN (Schuster and Paliwal, 1997):
−→h t = f(
−→W xt +
−→V
−→h t−1 +
−→b ) (3)
←−
h t = f(
←−W xt +
←−
V
←−
h t+1 +
←−
b ) (4)
yt = g(U→
−→h t + U←
←−
h t + c) (5)
where
−→W,
−→V and
−→b are the forward weight matrices
and bias vector as before;
←−W,
←−
V and
←−
b are their
backward counterparts; U→, U← are the output matrices;
and c is the output bias.1 Again, we assume
−→h 0 =
←−
h T +1 = 0. In this setting
−→h t and
←−
h t can
be interpreted as a summary of the past, and the future,
respectively, around the time step t. When we make
a decision on an input vector, we employ the two intermediate
representations
−→h t and
←−
h t of the past and
1As a convention, we adopt the following notation
throughout the paper: Superscript arrows for vectors disambiguate
between forward and backward representations. Superscript
arrows for matrices denote the resulting vector representations
(connection outputs), and subscript arrows for
matrices denote incoming vector representations (connection
inputs). We omit subscripts when there is no ambiguity.
722
the future. (See Figure 1, top right.) Therefore in the
bidirectional case, we have perfect information about
the sequence (ignoring the practical difficulties about
capturing long term dependencies, caused by vanishing
gradients), whereas the classical Elman-type network
uses only partial information as described above.
Note that the forward and backward parts of the network
are independent of each other until the output
layer when they are combined. This means that during
training, after backpropagating the error terms from the
output layer to the forward and backward hidden layers,
the two parts can be thought of as separate, and
each trained with the classical backpropagation through
time (Werbos, 1990).
3.3 Depth in Space
Recurrent neural networks are often characterized as
having depth in time: when unfolded, they are equivalent
to feedforward neural networks with as many
hidden layers as the number tokens in the input sequence
(with shared connections across multiple layers
of time). However, this notion of depth likely does not
involve hierarchical processing of the data: across different
time steps, we repeatedly apply the same transformation
to compute the memory contribution of the
input (W), to compute the response value from the current
memory (U) and to compute the next memory vector
from the previous one (V ). Therefore, assuming the
input vectors {xt} together lie in the same representation
space, as do the output vectors {yt}, hidden representations
{ht} lie in the same space as well. As a
result, they do not necessarily become more and more
abstract, hierarchical representations of one another as
we traverse in time. However in the more conventional,
stacked deep learners (e.g. deep feedforward nets), an
important benefit of depth is the hierarchy among hidden
representations: every hidden layer conceptually
lies in a different representation space, and constitutes
a more abstract and higher-level representation of the
input (Bengio, 2009).
In order to address these concerns, we investigate
deep RNNs, which are constructed by stacking
Elman-type RNNs on top of each other (Hermans and
Schrauwen, 2013). Intuitively, every layer of the deep
RNN treats the memory sequence of the previous layer
as the input sequence, and computes its own memory
representation.
More formally, we have:
−→h
(i)
t = f(
−→W(i)
→
−→h
(i−1)
t +
−→W(i)
←
←−
h
(i−1)
t
+
−→V
(i)−→h
(i)
t−1 +
−→b
(i)
) (6)
←−
h
(i)
t = f(
←−W(i)
→
−→h
(i−1)
t +
←−W(i)
←
←−
h
(i−1)
t
+
←−
V
(i)←−
h
(i)
t+1 +
←−
b
(i)
) (7)
when i > 1 and
−→h
(1)
t = f(
−→W(1)xt +
−→V
(1)−→h
(1)
t−1 +
−→b
(1)) (8)
←−
h
(1)
t = f(
←−W(1)xt +
←−
V
(1)←−
h
(1)
t+1 +
←−
b
(1)) (9)
Importantly, note that both forward and backward representations
are employed when computing the forward
and backward memory of the next layer.
Two alternatives for the output layer computations
are to employ all memory layers or only the last. In
this work we adopt the second approach:
yt = g(U→
−→h
(L)
t + U←
←−
h
(L)
t + c) (10)
where L is the number of layers. Intuitively, connecting
the output layer to only the last hidden layer forces the
architecture to capture enough high-level information
at the final layer for producing the appropriate outputlayer
decision.
Training a deep RNN can be conceptualized as interleaved
applications of the conventional backpropagation
across multiple layers, and backpropagation
through time within a single layer.
The unidirectional and bidirectional deep RNNs are
depicted in the bottom half of Figure 1.
Hypotheses. In general, we expected that the deep
RNNs would show the most improvement over shallow
RNNS for ESEs — phrases that implicitly convey
subjectivity. Existing research has shown that these
are harder to identify than direct expressions of subjectivity
(DSEs): they are variable in length and involve
terms that, in many (or most) contexts, are neutral
with respect to sentiment and subjectivity. As a result,
models that do a better job interpreting the context
should be better at disambiguating subjective vs. nonsubjective
uses of phrases involving common words
(e.g. “as usual”, “in fact”). Whether or not deep RNNs
would be powerful enough to outperform the state-ofthe-art
semiCRF was unclear, especially if the semiCRF
is given access to the distributed word representations
(embeddings) employed by the deep RNNs. In
addition, the semiCRF has access to parse tree information
and opinion lexicons, neither of which is available
to the deep RNNs.
4 Experiments
Activation Units. We employ the standard softmax
activation for the output layer: g(x) = e
xi /
P
j
e
xj
.
For the hidden layers we use the rectifier linear activation:
f(x) = max{0, x}. Experimentally, recti-
fier activation gives better performance, faster convergence,
and sparse representations. Previous work also
reported good results when training deep neural networks
using rectifiers, without a pretraining step (Glorot
et al., 2011).
Data. We use the MPQA 1.2 corpus (Wiebe et al.,
2005) (535 news articles, 11,111 sentences) that is
manually annotated with both DSEs and ESEs at the
phrase level. As in previous work, we separate 135
documents as a development set and employ 10-fold
CV over the remaining 400 documents. The development
set is used during cross validation to do model
selection.
723
Layers |h| Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
Shallow 36 62.24 65.90 65.63* 73.89* 63.83 69.62
Deep 2 29 63.85* 67.23* 65.70* 74.23* 64.70* 70.52*
Deep 3 25 63.53* 67.67* 65.95* 73.87* 64.57* 70.55*
Deep 4 22 64.19* 68.05* 66.01* 73.76* 64.96* 70.69*
Deep 5 21 60.65 61.67 56.83 69.01 58.60 65.06
Shallow 200 62.78 66.28 65.66* 74.00* 64.09 69.85
Deep 2 125 62.92* 66.71* 66.45* 74.70* 64.47 70.36
Deep 3 100 65.56* 69.12* 66.73* 74.69* 66.01* 71.72*
Deep 4 86 61.76 65.64 63.52 72.88* 62.56 69.01
Deep 5 77 61.64 64.90 62.37 72.10 61.93 68.25
Table 2: Experimental evaluation of RNNs for DSE extraction
Layers |h| Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
Shallow 36 51.34 59.54 57.60 72.89* 54.22 65.44
Deep 2 29 51.13 59.94 61.20* 75.37* 55.63* 66.64*
Deep 3 25 53.14* 61.46* 58.01 72.50 55.40* 66.36*
Deep 4 22 51.48 60.59* 59.25* 73.22 54.94 66.15*
Deep 5 21 49.67 58.42 48.98 65.36 49.25 61.61
Shallow 200 52.20* 60.42* 58.11 72.64 54.75 65.75
Deep 2 125 51.75* 60.75* 60.69* 74.39* 55.77* 66.79*
Deep 3 100 52.04* 60.50* 61.71* 76.02* 56.26* 67.18*
Deep 4 86 50.62* 58.41* 53.55 69.99 51.98 63.60
Deep 5 77 49.90* 57.82 52.37 69.13 51.01 62.89
Table 3: Experimental evaluation of RNNs for ESE extraction
Evaluation Metrics. We use precision, recall and Fmeasure
for performance evaluation. Since the boundaries
of expressions are hard to define even for human
annotators (Wiebe et al., 2005), we use two soft notions
of the measures: Binary Overlap counts every overlapping
match between a predicted and true expression
as correct (Breck et al., 2007; Yang and Cardie,
2012), and Proportional Overlap imparts a partial correctness,
proportional to the overlapping amount, to
each match (Johansson and Moschitti, 2010; Yang and
Cardie, 2012). All statistical comparisons are done using
a two-sided paired t-test with a confidence level of
α = .05.
Baselines (CRF and SEMICRF). As baselines, we
use the CRF-based method of Breck et al. (2007)
and the SEMICRF-based method of Yang and Cardie
(2012), which is the state-of-the-art in opinion expression
extraction. Features that the baselines use are
words, part-of-speech tags and membership in a manually
constructed opinion lexicon (within a [-1, +1] context
window). Since SEMICRF relaxes the Markovian
assumption and operates at the segment-level instead
of the token-level, it also has access to parse trees of
sentences to generate candidate segments (Yang and
Cardie, 2012).
Word Vectors (+VEC). We also include versions of
the baselines that have access to pre-trained word vectors.
In particular, CRF+VEC employs word vectors
as continuous features per every token. Since SEMICRF
has phrase-level rather than word-level features,
we simply take the mean of every word vector for a
phrase-level vector representation for SEMICRF+VEC
as suggested in Mikolov et al. (2013).
In all of our experiments, we keep the word vectors
fixed (i.e. do not finetune) to reduce the degree
of freedom of our models. We use the publicly available
300-dimensional word vectors of Mikolov et al.
(2013), trained on part of the Google News dataset
(∼100B words). Preliminary experiments with other
word vector representations such as Collobert-Weston
(2008) embeddings or HLBL (Mnih and Hinton, 2007)
provided poorer results (∼ −3% difference in proportional
and binary F1).
Regularizer. We do not employ any regularization
for smaller networks (∼24,000 parameters) because we
have not observed strong overfitting (i.e. the difference
between training and test performance is small).
Larger networks are regularized with the recently proposed
dropout technique (Hinton et al., 2012): we randomly
set entries of hidden representations to 0 with
a probability called the dropout rate, which is tuned
over the development set. Dropout prevents learned
724
Model Precision Recall F1
Prop. Bin. Prop. Bin. Prop Bin.
DSE CRF 74.96* 82.28* 46.98 52.99 57.74 64.45
semiCRF 61.67 69.41 67.22* 73.08* 64.27 71.15*
CRF +vec 74.97* 82.43* 49.47 55.67 59.59 66.44
semiCRF +vec 66.00 71.98 60.96 68.13 63.30 69.91
Deep RNN 3 100 65.56 69.12 66.73* 74.69* 66.01* 71.72*
ESE CRF 56.08 68.36 42.26 51.84 48.10 58.85
semiCRF 45.64 69.06 58.05 64.15 50.95 66.37*
CRF +vec 57.15* 69.84* 44.67 54.38 50.01 61.01
semiCRF +vec 53.76 70.82* 52.72 61.59 53.10 65.73
Deep RNN 3 100 52.04 60.50 61.71* 76.02* 56.26* 67.18*
Table 4: Comparison of Deep RNNs to state-of-the-art (semi)CRF baselines for DSE and ESE detection
features from co-adapting, and it has been reported
to yield good results when training deep neural networks
(Krizhevsky et al., 2012; Dahl et al., 2013).
Network Training. We use the standard multiclass
cross-entropy as the objective function when training
the neural networks. We use stochastic gradient descent
with momentum with a fixed learning rate (.005)
and a fixed momentum rate (.7). We update weights
after minibatches of 80 sentences. We run 200 epochs
for training. Weights are initialized from small random
uniform noise. We experiment with networks of various
sizes, however we have the same number of hidden
units across multiple forward and backward hidden layers
of a single RNN. We do not employ a pre-training
step; deep architectures are trained with the supervised
error signal, even though the output layer is connected
to only the final hidden layer. With these configurations,
every architecture successfully converges without
any oscillatory behavior. Additionally, we employ
early stopping for the neural networks: out of all iterations,
the model with the best development set performance
(Proportional F1) is selected as the final model
to be evaluated.
4.1 Results and Discussion
Bidirectional vs. Unidirectional. Although our focus
is on bidirectional RNNs, we first confirm that the
SHALLOW bidirectional RNN outperforms a (shallow)
unidirectional RNN for both DSE and ESE recognition.
To make the comparison fair, each network has
the same number of total parameters: we use 65 hidden
units for the unidirectional, and 36 for the bidirectional
network, respectively. Results are as expected:
the bidirectional RNN obtains higher F1 scores than the
unidirectional RNN — 63.83 vs. 60.35 (proportional
overlap) and 69.62 vs. 68.31 (binary overlap) for DSEs;
54.22 vs. 51.51 (proportional) and 65.44 vs. 63.65 (binary)
for ESEs. All differences are statistically significant
at the 0.05 level. Thus, we will not include comparisons
to the unidirectional RNNs in the remaining
experiments.
Adding Depth. Next, we quantitatively investigate
the effects of adding depth to RNNs. Tables 2
and 3 show the evaluation of RNNs of various depths
and sizes. In both tables, the first group networks
have approximately 24,000 parameters and the second
group networks have approximately 200,000 parameters.
Since all RNNs within a group have approximately
the same number of parameters, they grow narrower
as they get deeper. Within each group, bold
shows the best result with an asterisk denoting statistically
indistinguishable performance with respect to
the best. As noted above, all statistical comparisons
use a two-sided paired t-test with a confidence level of
α = .05.
In both DSE and ESE detection and for larger networks
(bottom set of results), 3-layer RNNs provide the
best results. For smaller networks (top set of results),
2, 3 and 4-layer RNNs show equally good performance
for certain sizes and metrics and, in general, adding additional
layers degrades performance. This could be related
to how we train the architectures as well as to the
decrease in width of the networks. In general, we observe
a trend of increasing performance as we increase
the number of layers, until a certain depth.
deepRNNs vs. (semi)CRF. Table 4 shows comparison
of the best deep RNNs to the previous best results
in the literature. In terms of F-measure, DEEP RNN
performs best for both DSE and ESE detection, achieving
a new state-of-the-art performance for the more
strict proportional overlap measure, which is harder to
improve upon than the binary evaluation metric. SEMICRF,
with its very high recall, performs comparably to
the DEEP RNN on the binary metric. Note that RNNs
do not have access to any features other than word vectors.
In general, CRFs exhibit high precision but low recall
(CRFs have the best precision on both DSE and
ESE detection) while SEMICRFs exhibit a high recall,
low precision performance. Compared to SEMICRF,
the DEEP RNNs produce an even higher recall
but sometimes lower precision for ESE detection. This
suggests that the methods are complementary, and can
725
(1)
The situation obviously remains fluid from hour to hour but it [seems to be] [going in the right direction]
DEEPRNN The situation [obviously] remains fluid from hour to hour but it [seems to be going in the right] direction
SHALLOW The situation [obviously] remains fluid from hour to hour but it [seems to be going in] the right direction
SEMICRF The situation [obviously remains fluid from hour to hour but it seems to be going in the right direction]
(2)
have always said this is a multi-faceted campaign [but equally] we have also said any future military action
[would have to be based on evidence] , ...
DEEPRNN have always said this is a multi-faceted campaign but [equally we] have also said any future military action
[would have to be based on evidence] , ...
SHALLOW have always said this is a multi-faceted [campaign but equally we] have also said any future military action
would have to be based on evidence , ...
SEMICRF have always said this is a multi-faceted campaign but equally we have also said any future military action
would have to be based on evidence , ...
(3)
Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and “ [not enough] ” food had been taken into the country .
DEEPRNN Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and “ [not enough] ” food had been taken into the country .
SHALLOW Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was [not yet] secure
for aid agencies to operate in and “ [not enough] ” food had been taken into the country .
SEMICRF Ruud Lubbers , the United Nations Commissioner for Refugees , said Afghanistan was not yet secure
for aid agencies to operate in and “ not enough ” food had been taken into the country .
Figure 2: Examples of output. In each set, the gold-standard annotations are shown in the first line.
potentially be even more powerful when combined in
an ensemble method.
Word vectors. Word vectors help CRFs on both precision
and recall on both tasks. However, SEMICRFs
become more conservative with word vectors, producing
higher precision and lower recall on both tasks.
This sometimes hurts overall F-measure.
Among the (SEMI)CRF-based methods, SEMICRF
obtains the highest F1 score for DSEs and for ESEs
using the softer metric; SEMICRF+VEC performs best
for ESEs according to the stricter proportional overlap
measure.
Network size. Finally, we observe that even small
networks (such as 4-layer deep RNN for DSE and
2-layer deep RNN for ESE) outperform conventional
CRFs. This suggests that with the help of good word
vectors, we can train compact but powerful sequential
neural models.
When examining the output, we see some systematic
differences between the previously top-performing
SEMICRF and the RNN-based models. (See Figure 2.)
First, SEMICRF often identifies excessively long subjective
phrases as in Example 1. Here, none of the models
exactly matches the gold standard, but the RNNs
are much closer. And all three models appear to have
identified an ESE that was mistakenly omitted by the
human annotator — “obviously”. At the same time,
the SEMICRF sometimes entirely misses subjective expressions
that the RNNs identify — this seems to occur
when there are no clear indications of sentiment in the
subjective expression. The latter can be seen in Examples
2 and 3, in which the SEMICRF does not identify
“but equally”, “would have to be based on evidence”,
“not yet”, and “not enough”.
We also observe evidence of the power of the DEEPRNN
over the SHALLOWRNN in Examples 4 and 5.
(See Figure 3.) In contrast to Figure 2, Figure 3 distinguishes
subjective expressions that are (correctly)
assigned an initial Begin label from those that consist
only of Inside labels2 — the latter are shown in
ALL CAPS and indicate some degree of confusion in
the model that produced them. In Example 4, SHALLOWRNN
exhibits some evidence for each ESE — it
labels one or more tokens as Inside an ESE (“any” and
“time”). But it does not explicitly tag the beginning
of the ESE. DEEPRNN does better, identifying the first
ESE in its entirety (“in any case”) and identifying more
words as being Inside the second ESE (“it is high time).
A similar situation occurs in Example 5.
5 Conclusion
In this paper we have explored an application of deep
recurrent neural networks to the task of sentence-level
opinion expression extraction. We empirically evaluated
deep RNNs against conventional, shallow RNNs
that have only a single hidden layer. We also compared
our models with previous (semi)CRF-based approaches.
Experiments showed that deep RNNs outperformed
shallow RNNs on both DSE and ESE extrac-
2
Sequences of I’s are decoded as the associated DSE or
ESE even though they lack the initial B.
726
(4)
[In any case] , [it is high time] that a social debate be organized ...
DEEPRNN [In any case] , it is HIGH TIME that a social debate be organized ...
SHALLOW In ANY case , it is high TIME that a social debate be organized ...
(5)
Mr. Stoiber [has come a long way] from his refusal to [sacrifice himself] for the CDU in an election that
[once looked impossible to win] , through his statement that he would [under no circumstances]
run against the wishes...
DEEPRNN Mr. Stoiber [has come a long way from] his [refusal to sacrifice himself] for the CDU in an election that
[once looked impossible to win] , through his statement that he would [under no circumstances
run against] the wishes...
SHALLOW Mr. Stoiber has come A LONG WAY FROM his refusal to sacrifice himself for the CDU in an election that
[once looked impossible] to win , through his statement that he would under NO CIRCUMSTANCES
run against the wishes...
Figure 3: DEEPRNN Output vs. SHALLOWRNN Output. In each set of examples, the gold-standard annotations
are shown in the first line. Tokens assigned a label of Inside with no preceding Begin tag are shown in ALL CAPS.
tion. Furthermore, deep RNNs outperformed previous
(semi)CRF baselines, achieving new state-of-the-art results
for fine-grained on opinion expression extraction.
We have trained our deep networks without any pretraining
and with only the last hidden layer connected
to the output layer. One potential future direction is
to explore the effects of pre-training on the architecture.
Pre-training might help to exploit the additional
representational power available in deeper networks.
Another direction is to investigate the impact of finetuning
the word vectors during supervised training.
Additionally, alternative notions of depth that are orthogonal
to stacking, as in Pascanu et al. (2013) can be
investigated for this task.
Acknowledgments
This work was supported in part by NSF grant IIS-
1314778 and DARPA DEFT Grant FA8750-13-2-0015.
The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or the
U.S. Government.
References
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trendsR in Machine Learning,
2(1):1–127.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying
expressions of opinion in context. In IJCAI,
pages 2683–2688.
Yejin Choi and Claire Cardie. 2010. Hierarchical sequential
learning for extracting opinions and their attributes.
In Proceedings of the ACL 2010 Conference
Short Papers, pages 269–274. Association for Computational
Linguistics.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of HLT/EMNLP, pages 355–
362. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings
of the 25th international conference on Machine
learning, pages 160–167. ACM.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton.
2013. Improving deep neural networks for lvcsr
using rectified linear units and dropout. In Acoustics,
Speech and Signal Processing (ICASSP), 2013
IEEE International Conference on, pages 8609–
8613. IEEE.
Salah El Hihi and Yoshua Bengio. 1995. Hierarchical
recurrent neural networks for long-term dependencies.
In Advances in Neural Information Processing
Systems, pages 493–499.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceedings
of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Volume,
volume 15, pages 315–323.
Michiel Hermans and Benjamin Schrauwen. 2013.
Training and analysing deep recurrent neural networks.
In Advances in Neural Information Processing
Systems, pages 190–198.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing coadaptation
of feature detectors. arXiv preprint
arXiv:1207.0580.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expression
detection. In Proceedings of the Fourteenth
Conference on Computational Natural Lan-
727
guage Learning, pages 67–76. Association for Computational
Linguistics.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting opinion expressions and their polarities:
exploration of pipelines and joint models. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages
101–106. Association for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
2012. Imagenet classification with deep convolutional
neural networks. In NIPS, volume 1, page 4.
Gregoire Mesnil, Xiaodong He, Li Deng, and Yoshua ´
Bengio. 2013. Investigation of recurrent-neuralnetwork
architectures and learning methods for spoken
language understanding. Interspeech.
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan ´
Cernocky, and Sanjeev Khudanpur. 2010. Recur- `
rent neural network based language model. In INTERSPEECH,
pages 1045–1048.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528–5531. IEEE.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality.
In Advances in Neural Information Processing
Systems, pages 3111–3119.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
M Arthur Munson, Claire Cardie, and Rich Caruana.
2005. Optimizing to arbitrary nlp metrics using ensemble
selection. In Proceedings of HLT/EMNLP,
pages 539–546. Association for Computational Linguistics.
Razvan Pascanu, C¸ aglar G ˘ ulc¸ehre, Kyunghyun Cho, ¨
and Yoshua Bengio. 2013. How to construct
deep recurrent neural networks. arXiv preprint
arXiv:1312.6026.
Jurgen Schmidhuber. 1992. Learning complex, ex- ¨
tended sequences using the principle of history compression.
Neural Computation, 4(2):234–242.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional
recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for
Computational Linguistics.
Paul J Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550–1560.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emotions
in language. Language resources and evaluation,
39(2-3):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP, pages 347–354. Association for Computational
Linguistics.
Bishan Yang and Claire Cardie. 2012. Extracting
opinion expressions with semi-markov conditional
random fields. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335–1345. Association for
Computational Linguistics.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings
of ACL.
728

 4086 | International Journal of Current Engineering and Technology, Vol.4, No.6 (Dec 2014)
Research Article
International Journal of Current Engineering and Technology
E-ISSN 2277 – 4106, P-ISSN 2347 - 5161
©2014 INPRESSCO®
, All Rights Reserved
Available at http://inpressco.com/category/ijcet
Procedure of Opinion Mining and Sentiment Analysis: A Study
Rushabh ShahȦ*
and Bhoomit PatelȦ
Ȧ
Information Technology,Dwarkadas J.Sanghvi College of Engineering,Vile Parle(W), Mumbai-400056, India
Accepted 02 Dec 2014, Available online 10 Dec 2014, Vol.4, No.6 (Dec 2014)
Abstract
This paper covers all the essential details one must know about sentiment mining. It provides information on recent
trends, applications of sentiment mining, different fields where it is used and also lot of useful information on the current
research work being carried out in this area of data mining. Also, the basic workflow of the sentiment analysis process
has been explained extraordinarily. Further, this paper also exemplifies the challenges and the future research being
planned in the field of sentiment and opinion mining.
Keywords: Data Mining; Data Pre-processing; Analysis; Opinion; Tweets.
1. Introduction
1As we know internet has provided us with variety of
means to flourish all groups of industries. All the social
medias like twitter, MySpace, Linkedin, Facebook,
YouTube and many others have gained so much reputation
that they cannot be ignored. Internet offers effective means
to communicate and share one‟s opinion. It simply allows
people to frame links with their colleagues, friends and
families. It allows people to share all kinds of information
and use different types of services available such as
sharing blogs, reviews, etc.
Opinions regarding almost all the global entities are
available on the internet. We find many blogs dedicated to
particular topics like sports, news, marketing, finance,
education, history, science and many more. Opinions are
expressed by the people in the form of natural language.
Social networking sites can easily provide one with all
the information required to take a particular decision, for
example, buying any item A from shopping sites. Social
media is an excellent channel to put forward one‟s opinion
in front of the world. The data available in order to mine
the opinion from it is magnificent. There have been many
research projects based on the analysis of sentiments
expressed on social media. Sentiment analysis poses
newer and various challenges to gather information from
the text in natural language. The region of Sentiment
analysis aims to understand all the opinions expressed in
natural language and categorize them. Sentiment analysis
is carried out on review sites and social medias like twitter
where tweets gives us more accurate and varied opinions
of the people from all over the world which can be about
latest cellular phone like Iphone6. The reviews regarding a
product would definitely affect the buyer‟s decision.
The key challenge faced by the researchers in
analyzing these reviews from the internet is that they are

*Corresponding author: Rushabh Shah
in the form of natural language. Processing of natural
language is implicitly problematic , gathering information
from the unstructured reviews is even more problematic.
In this paper we have discussed most of the key challenges
faced in opinion mining. We have included the flow of
sentiment analysis for understanding of the concept. The
paper includes ongoing research and future scope of
opining In this paper we have considered twitter as social
media.
2. Recent Trends
Sentiment analysis is not a novel research theme. The use
of automation in sentiment analysis has increased
significantly in the past couple of years. Natural Language
Processing, Machine Learning and Opinion mining are
few streams of computer science on which the research
theme is dependent. Nowadays, lots and lots of data is
available and can be adopted for use. For making any
important decisions, devising business strategies it is a
necessity to analyse enormous amount of data available
from various sources. Social media is one of the upcoming
sources providing indefinite data which is used for
sentiment analysis. But the data obtained from social
media is disorganized. Obsolete content analysis methods
focused on predicting topics. Since past few years
opinions, emotions and sentiments are some qualities
which are reflected by the content from the social media.
The volume of data obtained from social media is gigantic
making it more complex to analyze. As a result, decrease
in interest of semantic-based application and inclination
towards statistics and visualization is observed.
3. Applications
Sentiment mining covers a vast range of applications in
several fields. These applications assist in making sense of 
Rushabh Shah Procedure of Opinion and Sentiment Analysis: A Study
4087 | International Journal of Current Engineering and Technology, Vol.4, No.6 (Dec 2014)
hundreds of applications. Sentiment mining works
diversely from the traditional survey methods and depends
on listening in spite of asking which depicts more accurate
reality. The main domain of applications implemented and
managed by sentiment mining are as follows:
 Applications to review related websites
It has the same capabilities as a review-related search
engine and acts as an alternative to sites such as
Opinions that collects information in the form of
reviews and feedback. With such applications it
becomes possible to summarize user reviews, fix
blunders in user ratings and provide evidence that
proves that user ratings was biased or those ratings
need some correction.
 Applications as sub-component technology
Sentiment analysis is having a potential role in
supporting technologies used for other systems.
Detecting „flames‟ in email or other means of
communication, augmenting to recommendation
systems which avoids recommending items getting lot
of negative response, detecting web pages that
includes sensitive information and averts displaying
ads on those pages are some of the examples of
application being executed in this field.
 Applications in business and government intelligence
Sentiment mining is extremely important when
business intelligence is the factor one is focusing on.
In business and government intelligence, sentiment
mining mainly handles reputation management, public
relations and monitoring sources responsible for
increment in negative or hostile communications.
Extracting information helps organizations to develop
better business strategies, find answers to their decline
and failure, review their products based on people‟s
comments or tweets which would all help any
organization walk on the path of success.
Other applications include areas like politics, question
answering, summarizing important points, improving
extraction by discarding petty information, citation
analysis, strong holding human-computer interaction etc.
4. Workflow
4.1. Extraction
In the extraction phase of Sentiment mining, social media
acts as a source of data. In order to explain this process
easily further details are in resemblance with twitter. In
twitter, number of users gives their reviews by posting
messages which are called as tweets. These tweets depict
the sentiments of the users. The process of sentiment
mining is basically analyzing this data and converting it
into knowledge. Following are few fundamental
characteristics observed in the data while performing
extraction:
 The length of the twitter message is limited to 140
characters.
 Moreover, we observe the presence of spelling errors
and informal or cyber slang in these messages.
 The amount of data available is copious and as most
of the twitter messages are available in public domain
it can be used for the purpose of sentiment mining.
Data extracted from the social media like twitter is
updated very frequently. Therefore, it helps to give the
feeling of real time representation of the sentiments. In
order to obtain the data on run-time an internet bot can be
used known as web crawler. A web crawler browses
through the World Wide Web in organized manner to
index the web pages. It is one of the many fundamental
components which constitute web search engines. The
indexing of the web pages grants the user to issue queries
and get the required pages as per the query issued.
Fig.1 Working of sentiment analysis
4.2. Pre-Processing
As the name suggests, in this phase of sentiment analysis
processing of the data is carried out. In the pre-processing
stage, the extracted data is cleaned as it contains large
amount of noise before sending the text for analyzing. The
extracted text contains lot of grammatical errors as the text
is of limited length. Pre-processing of the data is necessary
and it is a crucial part as one needs to make sure that the
unnecessary part of the text is removed and the relevant
part of the text which stores the sentiment of the user is
not removed. Following are few techniques explained in
brief, keeping in mind twitter as the social media used,
which are generally used in order to draw information for
sentiment mining.
 Supplanting emoticon
In twitter as one is restricted to post his or her views
by using only 140 characters, emotions is proved to be
an easy way to depict one‟s sentiments. Moreover, 
Rushabh Shah Procedure of Opinion and Sentiment Analysis: A Study
4088 | International Journal of Current Engineering and Technology, Vol.4, No.6 (Dec 2014)
Emotions assist in deciding the polarity of the text.
For example, a SMILE keyword can be used to
supersede few similar emotions such as :D , :] , =) ,
etc.
 Uppercase and Lowercase Identification
It is often observed that in order to express strong
sentiments ( like anger e.g. GET OUT) one uses all
the characters in uppercase. It is generally used to
show the intensity of the emotions. This is considered
as an indicator to decide the polarity of the text and is
known as e-shouting. We observe inconsistent casing
(e.g. TwITteR) in texts on social media. It is
necessary to make sure that there is consistency in
casing of the texts.
 Extraction of URL
In order to share extra content due to the limitation of
the tweet, many tweets consist of URL which is an
extension to the posts. The information obtained from
the URL basically supports the sentiments which are
expressed in tweets. But the cost of crawling is very
expensive. Therefore a compact equivalent class
<URL> can be used consisting of all URLs.
 Pointer Detection: We have observed that many
people uses „@‟ ahead of the users name in order to
point to other users and „#‟ is used by users to tag
twitter posts related to some category. Similar to
URL, <USER> and <HASHTAG> can be used to
supplant this portion of the tweets.
 Punctuations: In order to avoid the usage of grammar
rules and to give stress on the emotions expressed in
the tweets excessive usage of the punctuation is done
by the users. (E.g. Hurrah!- to show excitement). In
this technique of pre-processing stage, removal of
punctuation marks which are not relevant is done.
 Compression of words: Informal language is generally
used by the users of twitter and many users stretch the
words in order to highlight stress on those words
which clearly describes emotions. For example, „Air
is sooooooo necessary‟ simply depicts the idea of
necessity of that thing. The term „sooooooo‟ simply
bears the idea of importance of the „Air‟. Therefore,
we can reduce it to shorter sequence like „soo‟. It is
not stored as usual term „so‟ so that it can be used to
differentiate.
4.3. Analysis
Sentiment analysis is carried out on the data which is
obtained after the pre-processing stage. From the
sentiments contained in the data, the number of repetitions
observed in the tweets and the location of the tweets is
also analyzed. It is a vital stage in sentiment mining. The
tweets are mined for different keywords describing
emotions and are rated in this stage. If during the tenure of
analysis a existing keyword is seen in a sentence, it is
scored and is added to the score of the entire text to decide
the polarity of the text.
During the stage of sentiment analysis it is better
analyze on the basis of a scale and not merely on binary
judgment because co-relation compared the predicted
value to the target value which makes it better than
precision. But most of the algorithms for sentiment
analysis use basic terms to review and express opinions
about a service or product. So in this stage, proliferating
contexts, cultural factors, linguistic differences are taken
into consideration to get a precise result and the
complexity of deciding whether a sting of text is a pro or a
con sentiment gets simpler and efficient.
4.3. Knowledge Discovery
To find the opinion of the people with respect to any
particular occurrence, it is essential to store the data which
is related to the event. Once the polarity of the sentiments
is known it can be used to generate statistical graphs and
charts. The knowledge gathered from these electronic texts
from the web when shown in graphs would aid the
individuals in making decision as it would show the
polarity of the sentiments of the individuals and to what
extent it can be followed by referring to the graphs.
After all these stages are completed, the process of
sentiment mining is successfully executed.
5. Current Research
Nowadays, profiles like data mining, data analysis,
business analysis have become really prominent. For any
organization to stay alive in corporate world, it has
become necessary to make judicious decisions based on a
tremendous amount of data from ample of sources. But the
quantity and quality of data available from web sources
such as blogs, social media and discussion forums are
copious with sentiments, opinions, therefore, current
research is directed towards the area of opinion mining.
 It is essential to compose a system which can
identify the sentiment or opinion from the text available
and easily classify the sentiments or opinion. In order to
identify sentiments, current research is hunting down
methods for reduction of human efforts to perform
operations while analyzing the content. Classification of
sentiments with the help of sentiments which is already
known by using the pre-existing dictionary of words
obtained from the electronic text from the web. Visual
mapping of bipolar or conflicting opinions is also
something n which the current research in concentrating.
 Also, the focus is on ameliorating the precision of
algorithms for opinion detection, identifying the policies
for analyzing opinionated materials and finalizing the
experts for sentiment analysis who are highly experienced
and capable of working smoothly with large amounts of
data.
But, the main subject on whom the current research is
working is improving the precision of algorithms for
opinion detection.
Rushabh Shah Procedure of Opinion and Sentiment Analysis: A Study
4089 | International Journal of Current Engineering and Technology, Vol.4, No.6 (Dec 2014)
6. Key Challenges
Opinion mining has become really important as nowadays
for devising even smaller strategies, corpuses of data has
to be analyzed. So solutions for opinion and sentiment
mining are evolving at a great pace and reducing the
burden of human shoulders. There are many techniques
available for sentiment analysis. But still it‟s very difficult
to say which technique works best because each technique
has its own issues and certain challenges.
Mainly there are two techniques used for opinion mining.
1) Lexicon based and 2) Learning based
Lexicon based techniques involves high precision but on
the other hand gives low recall. Also, another issue in this
technique is that lexicons aren‟t available in all the
languages. Learning based techniques makes use of
labeled examples to classify text. But it requires learning
training as well as training dataset which becomes as issue
too. Another technique-syntactic technique does yield god
results but the n it isn‟t language independent. Few other
challenges that erupt during the process of classify text in
opinion mining have dependency on factors like:
 Value for n-when n-gram framework is used,
choosing a higher value of n will degrade the
performance.
 Occurrence of word-the number of occurrence of a
word should be at the most 2-3 times for sentiment
analysis.
 Features to be used-Deciding what features must be
used is also a challenge as the list of features returned
by tokenization contains few irrelevant features.
Hence it becomes necessary to select relevant features
which will determine the precision of a classifier.
 People are habitual to using slang and casual language
on social websites which it makes really difficult to
predict people‟s opinion. So to eradicate such issues
methods must be developed and existing ones should
be modified to adapt to the kind of language used on
social websites.
Now the term “spam messages” or “fake reviews” have
become really common and used on a large scale on social
websites. This creates a hurdle in the process of sentiment
mining. So one of the biggest challenges is to identify such
spam messages and fake reviews which can mainly be
done through the comparison of qualitative with summary
reviews. Also, identifying duplicates, detecting outliers
and knowing the reputation of the reviewer is to be kept in
mind while implementing opinion mining. There are
limitations in collaborative filtering which is responsible
for identifying most famous concepts and suggest some
out of the box thinking. Another challenge is the risk of
filter bubble, where combination of automated content
analysis with behavioral analysis proves to be very
effective but eventually deviates selection of useful
opinions making user unaware of content that is different
from what he expects in some manner. Integrating opinion
with implicit data and behaviour to validate data and
provide analysis beyond the opinion expressed is another
common challenge. Asymmetry in available opinion
mining software and the need for continuous improvement
in the usability and user-friendliness of opinion mining
software and other tools is another key challenge in the
field of sentiment and opinion mining. Skewness in the
dataset that is responsible for impacting recall is another
challenge in sentiment mining.
Conclusion
The explosion of usage of social networking sites in order
to give one‟s reviews has helped a lot to produce and use
varied technologies to mine people‟s opinions and
sentiments. As it involves natural language processing,
Sentiment mining arises as a challenging field with many
hindrances. It has varied diversity of applications that
could prove to be advantageous to many fields such as
marketing, business analytics, knowledge bases and so on.
To understand texts as human is the key challenge of this
field when it comes to machine‟s ability. It is very
important to extract knowledge from the opinions which
are expressed on the social networking sites for many
companies and institutions, whether it is in terms of public
mood, or feedback of particular product. In this paper we
have covered all the challenges faced in the current
research work. We have analysed the flow of the process
of sentiment analysis along with detailed techniques and
explanation of the stages in the process.
 Many other challenges are there like sarcasm
detection for which natural language processing can be
used. Similarly there are many other research work in
progress and soon will be unveiled in coming years to
overcome the current problems and enable smooth
functioning eliminating the challenges.
Future Research
Currently there are many significant matters which seek
attention. After all these requirements are accomplished, it
will become possible to divert attention on the flaws,
shortcomings in the current methods, algorithms and
devising solutions and strategies to reduce them and make
the process of sentiment analysis simpler and more
efficient. The future research work can be generalized into
either short term or long term research.
Short-term
 Bipolar evaluation of opinions
 Visual representation
 Multilingual collection of writings for reference
 Real-time and dynamic sentiment mining
 Opinion mining across different platforms
 Audiovisual opinion mining
 Algorithms for learning machines
 Algorithms for recommending opinion and comment
Long-term
 Non-bipolar evaluation of opinions
 Detecting irony automatically
Rushabh Shah Procedure of Opinion and Sentiment Analysis: A Study
4090 | International Journal of Current Engineering and Technology, Vol.4, No.6 (Dec 2014)
 Autonomous machine learning and artificial
intelligence
 Developing usable tools for citizens to let them carry
out opinion mining
To overcome the problem of skewness in the dataset,
techniques such as undersampling and oversampling are
being developed. AdaptiveBoost is being used in
conjunction with classifiers to eradicate petty entries in the
training set which would apparently help in improving
recall rates. Classification of international words and
foreign expressions is another area where the future
research would focus on.
Acknowledgement
We would like to thank Prof. Arjun Jaiswal for giving us
an opportunity to work and provide us a helping hand. We
would also like to thank our honorable principal Dr. Hari
Vasudevan of D. J. Sanghvi College of Engineering and
Dr. Abhijit Joshi, Head of Department of Information
Technology, for giving us the facilities and providing us
with a propitious environment for working in college. We
would also like to thank S.V.K.M for encouraging us in
such co-curricular activities.
References
Akshi Kumar and Teeja Mary Sebastian (July 2012) Sentiment
Analysis on Twitter. IJCSI International Journal of Computer
Science Issues, Vol. 9, Issue 4, No 3.
Bo Pang and Lillian Lee (2008) Opinion mining and sentiment
analysis. Foundations of Information Retrieval, Vol 2, Nos.
12, 1-135.
David Osimo and Francesco Mureddu (2012) Research
Challenge on Opinion Mining and sentiment analysis.
http://www.w3.org/2012/06/pmod/opinionmining.pdf),
Mark Kantrowitz (2000) Method and apparatus for analyzing
affect and emotion in text. U.S. Patent 6622140, 2003. Patent
filed in November
Balakrishnan Gokulakrishnan , Pavalanathan Priyanthan ,
Thiruchittampalam Ragavan ,Nadarajah Prasath, AShehan
Perera (2012) Opinion Mining and Sentiment Analysis on a
Twitter Data Stream. The International Conference on
Advances in ICT for Emerging Regions - ICTer, 182-188
Emma Haddia, Xiaohui Liua, Yong Shib (2013)The Role of Text
Pre-processing in Sentiment Analysis. Procedia Computer
Science 17, 26 – 32.
Walaa Medhat , Ahmed Hassan , Hoda Korashy (2014)
Sentiment analysis algorithms and applications:A survey. Ain
Shams Engineering Journal.
Mita K. Dalal andMukesh A. Zaveri (2013) Semisupervised
Learning Based Opinion Summarization and Classification for
Online Product Reviews. Hindawi Publishing Corporation,
Applied Computational Intelligence and Soft Computing,
Article ID 910706.
Lalita Sharma, Shweta Shukla. Classification of Web Blog
Mining for Movie Review. International Journal of
Engineering and Advanced Technology (IJEAT) ISSN: 2249 –
8958.
Meishan Hu, Aixin Sun and Ee-Peng Lim (2007) Commentsoriented
blog summarization by sentence-extraction. In
Proceedings of the ACM SIGIR Conference on Information
and Knowledge Management(CIKM), 901-904,. ISBN 78-1-
59593-803-9, Post paper.
Charlotta Engstrom (2004) Topic dependence in sentiment
classification. Master‟s thesis, University of Cambridge.
Soo-Min Kim and Eduard Hovy (2004) Determining the
sentiment of opinions. In Proceedings of the International
Conference on Computational Linguistics (COLING).

FLAME: A Probabilistic Model Combining Aspect Based
Opinion Mining and Collaborative Filtering
Yao Wu
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
wuyaow@sfu.ca
Martin Ester
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
ester@cs.sfu.ca
ABSTRACT
Aspect-based opinion mining from online reviews has attracted
a lot of attention recently. Given a set of reviews,
the main task of aspect-based opinion mining is to extract
major aspects of the items and to infer the latent aspect
ratings from each review. However, users may have different
preferences which might lead to different opinions on the
same aspect of an item. Even if fine-grained aspect rating
analysis is provided for each review, it is still difficult for a
user to judge whether a specific aspect of an item meets his
own expectation. In this paper, we study the problem of
estimating personalized sentiment polarities on different aspects
of the items. We propose a unified probabilistic model
called Factorized Latent Aspect ModEl (FLAME), which
combines the advantages of collaborative filtering and aspect
based opinion mining. FLAME learns users’ personalized
preferences on different aspects from their past reviews, and
predicts users’ aspect ratings on new items by collective intelligence.
Experiments on two online review datasets show
that FLAME outperforms state-of-the-art methods on the
tasks of aspect identification and aspect rating prediction.
Categories and Subject Descriptors
H.3.3 [Information search and retrieval]: Text Mining
General Terms
Algorithms, Experimentation
Keywords
Collaborative Filtering; Opinion Mining; Text Mining
1. INTRODUCTION
Nowadays, products and services offered on most online
E-commerce websites are accompanied by abundant usergenerated
reviews, which can help users make better decision.
For instance, if a user wants to know more about the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
WSDM’15, February 2–6, 2015, Shanghai, China.
Copyright 2015 ACM 978-1-4503-3317-7/15/02 ...$15.00.
http://dx.doi.org/10.1145/2684822.2685291.
Figure 1: A Sample Review On Amazon
battery life of a laptop, comments on the battery life of this
specific laptop by other users are more reliable than those
given in the official description of the product. However, the
volume of reviews grows so rapidly that it gets extremely
difficult for users to find useful information in short time.
Thus mining useful information out of these huge amount
of reviews has become an important way to improve user
satisfaction of these online E-commerce websites.
Aspect-based opinion mining [10] has attracted a lot of
attention recently. Given a collection of reviews on a set
of items, aspect-based opinion mining methods extract major
aspects out of every item based on how often they have
been commented by users, and learn users’ sentiment polarities
toward each aspect based on the opinionated words they
used in the reviews. Figure 1 shows a sample review from
Amazon1
. The user assigns a 5-star overall rating, and expresses
his opinions on several aspects of the product. From
a set of reviews like this, aspect-based opinion mining methods
can automatically extract the aspects of the product,
such as performance, display, value and size, as well as infer
latent sentiment scores for each aspect, e.g., 5 stars on its
display.
Much work has been proposed to help users digest and
exploit large number of reviews by aspect-based opinion
mining techniques, including information extraction from reviews
[11], uncovering the latent aspects of the review sentences
[13], inferring the latent aspect ratings and aspect
weights of each review document [22, 23], aspect-based review
summarization for products [11, 9], etc. These methods
either focus on review-level analysis (extracting useful information
within each review) to help users easily find what
they need from a piece of review, or make product-level summarization
(aggregating the opinions of all the users) to provide
an overview of users’ feedback on a product. However,
an important factor is typically ignored – preference diver-
1
http://www.amazon.com
sity, i.e., users have different preferences that their opinions
on the same item may differ from each other. For example,
the food of the same restaurant might be delicious for some
users but terrible for others. When choosing restaurants, a
user might want to know whether a restaurant meets his own
expectation on the aspect of food. But when facing a large
number of reviews expressing various opinions, it becomes
extremely difficult for the user to make the decision: 1) it’s
impossible for the user to read all the reviews even if the finegrained
review-level analysis is provided. 2) the user has no
idea of which reviews are more reliable or which reviewers
share similar tastes with him. 3) product-level summarization
is also unreliable since it is generated from reviews by
users with different tastes. To help users better utilize the
existing reviews, we argue that a new method is required,
which can learn a user’s personalized preferences on different
aspects from his past reviews of other items, and predict
his preferences on the aspects of a given item by mining the
opinions by other users with similar preferences.
A popular method of learning user preferences is Collaborative
Filtering, which predicts a user’s interests by collaboratively
collecting preferences from many other users. Typical
collaborative filtering methods take the numeric overall
ratings as inputs [7, 19], assuming that users with the same
ratings share the same tastes. However, two users who have
assigned the same 5-stars to a restaurant might have significantly
different reasoning; one might like its food while
the other likes its service. Text reviews provide rich information
to make it possible to understand preferences of
users at a finer granularity. Some recent work has shown
the benefits of utilizing text reviews within the collaborative
filtering methods [12, 24]. Unlike these work, we aim at
collectively exploring users’ preferences on different aspects
of the items. A challenge here is that aspect-based sentiment
scores are not explicitly specified by users, but implicitly expressed
in the reviews. We propose a new model combining
aspect-based opinion mining and collaborative filtering to
collectively learn users’ preferences on different aspects.
In this work, we introduce the problem of Personalized Latent
Aspect Rating Analysis. Given a collection of reviews
of a set of items by a set of users, the goal is to solve the following
two tasks: a) learn the latent aspects and their word
distribution over a pre-defined vocabulary, and the latent
aspect ratings for each review; b) for any user u in the data
set, predict the latent aspect ratings on the items that he
has not yet reviewed. Existing aspect-based opinion mining
methods such as [22, 23, 14] are able to solve task a, but are
unsuitable for solving task b since they require the text of
user u’s review for item i as input. Task b is also different
from the well-studied rating prediction problem in recommender
systems, the goal of which is to predict the overall
rating while we want to predict the aspect ratings.
To address the problem of Personalized Latent Aspect
Rating Analysis, we propose a unified probabilistic model
called Factorized Latent Aspect ModEl (FLAME), which
combines the advantages of both collaborative filtering and
aspect-based opinion mining so that the two methods can
mutually enhance each other. The general idea of FLAME
is that we can learn users’ preferences based on their past reviews,
so that we can collaboratively predict a user’s preference
of an aspect of an item from the opinions of other users
with similar tastes. FLAME improves existing aspect-based
opinion mining methods by being able to infer aspect ratings
of users on new items2
, and enhances collaborative filtering
methods by leveraging reviews to analyze users’ preferences
on different aspects of items.
We empirically evaluate the proposed FLAME on a hotel
review data set from TripAdvisor3
and a restaurant review
data set from Yelp4
. Experimental results show that
FLAME can effectively extract meaningful aspects and predict
aspect ratings of a user on new items to him.
The remainder of the paper is organized as follows. Section
2 is devoted to related work. Section 3 introduces the
problem definition and useful notations. Section 4 presents
the proposed model and describes the inference and parameters
estimation techniques. In Section 5 & 6, we report
the experimental results on two review data sets and discuss
some other applications of the proposed model. Finally,
Section 7 concludes the paper with a summary and discusses
potential future work.
2. RELATED WORK
Our work is related to two research topics: Collaborative
Filtering and Aspect-based Opinion Mining.
2.1 Collaborative Filtering
Collaborative filtering (CF) is a popular method widely
used in recommender systems. The assumption behind collaborative
filtering is that a given user is more likely to like
items that are liked by other users with similar tastes. Various
state-of-the-art CF methods are based on latent factor
models [7]. Latent factor models assume that a user’s rating
on a particular item depends on the inner dot product of the
latent user factors and the latent item factors.
Some work combining collaborative filtering with Topic
Models has been proposed to leverage text information in
recommender systems. Topic models are introduced by [1]
for learning the hidden dimensions of text. The basic assumption
of topic models is that documents are represented
by mixtures of some latent topics where topics are associated
with a multinomial distribution over words of a vocabulary.
The earliest work integrating collaborative filtering
with topic model is CTM [21], which is proposed for article
recommendation. CTM simultaneously trains a topic model
on the collection of articles and a latent rating factor model
on the ratings of users on articles, while assuming that the
latent factors of items depend on the latent topic distributions
of their text. A recent work [12] proposes a model
called HFT, which aims at improving collaborative filtering
using reviews. HFT considers latent rating factors of an item
as the properties that the item possesses, and assumes that
if a product exhibits a certain property (higher latent rating
factor value), this will correspond to a particular topic
being discussed frequently (higher probability in topic distribution)
[12]. HTF first aggregates all the reviews of an item
into a single document, and uses a similar method as CTM
to train a topic model and a latent factor model together.
Different from CTM and HTF which learn topic distributions
for each item, our approach learns for each review its
aspect distribution as well as its rating distribution on each
aspect.
2Note that new items for a user are the items that he has
not rated yet.
3
http://www.tripadviosr.com
4
http://www.yelp.com
2.2 Aspect-based Opinion Mining
The main task of aspect-based opinion mining is extracting
the aspects and learning the aspect ratings from a collection
of reviews of a given item. Most of the early works
of opinion mining are frequency-based approaches [5, 11].
These approaches usually mine the aspects and sentiments
by counting the frequencies of words and their co-occurrences
with some pre-defined seed words. Recently, several methods
based on the variants of topic models [1] have been
proposed [20, 25, 6, 14, 15] to learn the aspects and sentiments
automatically from the data. These work extends
topic models by adding another kind of latent variables to
model the latent sentiments of words, i.e., words in reviews
are not only dependent on the topics they belong to, but are
also related to the sentiments of the reviewers. The most
related work is the Latent Aspect Rating Analysis Model
(LARAM) [22, 23], which aims at inferring the latent aspect
ratings of given reviews. LARAM assumes the overall
rating of a review is generated by a weighted sum of the
latent aspect ratings, and are generated from the words and
the latent topic allocations of the words by a linear regression
function. LARAM learns the latent aspect ratings for
each review and aspect weights for each reviewer. It should
be noted that the aspect weights in LARAM are different
from the personalized aspect ratings in our problem. The
weights in LARAM represent the importance of the aspects
for a reviewer, but personalized tastes represent the ratings/sentiments
of users on different aspects. Two reviewers
may share similar aspect weights but have totally different
ratings on a given aspect.
The main limitation of above aspect-based opinion mining
methods is that they do not consider user preferences (across
multiple reviews and items) in the learning procedures so
that they are unable to predict users’ opinions on other items
which they have not written reviews on.
The very recently published ETF [24] also considers aspect
based opinion mining and collaborative filtering simultaneously.
However, ETF employs the aspect-based opinion
mining as a preprocessing step, while ours is a unified model
with opinion mining as a part of the model. This enables
our approach to be used to analyze the aspect distributions
of the reviews and latent aspect ratings expressed in the reviews
as in [22, 23]. Besides, ETF can not predict users’
preferences on the aspects of items.
3. PROBLEM DEFINITION
We assume as input a collection of reviews of some products
from a specific category (e.g. restaurant) by a group of
reviewers, and each review comes with an overall rating (e.g.
1-5 stars) to express the overall satisfaction of the reviewer.
Review: A review is a piece of text describing opinions
of a reviewer towards a specific item. Formally, we use D =
{d1, d2, ..., dD} to denote a set of review text documents. For
each d ∈ D, ud ∈ U denotes the user who writes review d
and id ∈ I denotes the reviewed item. We use Du to denote
the set of reviews that user u writes and use Di to denote
the set of reviews for item i.
Overall Rating: The overall rating rd of a review document
d is a numerical rating indicating the overall opinion
of d, i.e., rd ∈ R, where R = {1, 2, ..., R}.
Aspect: An aspect is an attribute of the item that has
been commented on in a review, e.g., “food”, “location” and
Table 1: Mathematical Notations
Symbol Size Description
U U Users U = {u|u = 1, ..., U}
I I Items I = {i|i = 1, ..., I}
D D Documents D = {d|d = 1, ..., D}
A A Aspects A = {a|a = 1, ..., A}
R R Numerical ratings R = {r|r =
1, ..., R}
φu R
K latent vector of user u
φi R
K latent vector of item i
φi,a R
K latent vector of aspect a of i
η R
A background aspect distribution
ηu R
A aspect distribution of user u
ηi R
A aspect distribution of item i
βa R
V word distribution of aspect a
γa,r R
V word distribution of aspect a and
rating r
θd R
A aspect distribution of document d
ϕd,a R
R rating distribution of aspect a of
document d
at R
1
aspect of sentence t
st R
1
aspect rating of sentence t
“service” for a restaurant. In this paper, we only consider
the case that all the items are from a same category, i.e.,
they share the same set of aspects. We use a to denote an
aspect, where a ∈ A and A = {1, 2, ..., A}.
Aspect Rating: The aspect rating rd,a of a review document
d is the reviewer ud’s rating towards to the aspect a
of the item id.i It indicates the opinion of the reviewer regarding
to the properties of the corresponding aspect of the
item. Note that our method does not need aspect ratings as
input, but instead it infers them from the data.
Personalized Latent Aspect Rating Analysis: Given
a collection of reviews of a set of items by a set of users, the
goal is to solve two tasks: a) learn the latent aspects, which
represents each aspect as a distribution on a pre-defined
vocabulary, and the latent aspect ratings for each review,
which indicate the opinions of the reviewer towards the aspects
of the item; b) predict the latent aspect ratings for
user u on new item i that he has not reviewed.
Some important notations used in this paper are listed in
Table 1. We use bold math symbols xi to denote vectors,
where the subscript i is used for indexing different vectors.
The j-th element of the vector xi is denoted by xi[j].
4. PROPOSED MODEL
In this section, we propose the unified probabilistic model
Factorized Latent Aspect ModEl (FLAME) to address the
problem of Personalized Latent Aspect Rating Analysis.
When writing the review, the reviewer first selects a subset
of aspects he wants to comment on. We assume that each
review document d is associated with an aspect distribution
θd ∈ R
A, which represents the importances of the aspects
in the review. The aspect distribution θd depends on three
factors: the global aspect distribution η0, the aspect distribution
of the reviewer ηu and the aspect distribution of the
item ηi. η0 represents how much each aspect is likely to be
mentioned among all the reviews. ηu represents reviewer u’s
preferences on the aspects to comment, e.g., if a user cares
more on the value of a hotel, he prefers to mention this aspect
in his reviews. ηi indicates which aspects of item i are
more likely to be mentioned. Some aspects are more likely
to be mentioned in the reviews of an item. For example,
if the food of a restaurant is great, it will receive a lot of
praises of the food in the reviews. On the other hand, if
some aspects of an item are terrible, reviewers would like to
criticize on these aspects in their reviews.
Based on this assumption, we define θd using a additive
generative methods as follows:
θd[a] = exp (η0[a] + ηu[a] + ηi[a])
PA
a′=1 exp (η0[a
′
] + ηu[a
′
] + ηi[a
′
])
(1)
where {η} = {η0, ηu, ηi|u ∈ U, i ∈ I} are A-dimensional
vectors generated from zero-mean Gaussian distributions.
η0 ∼ N (0, σηI)
ηu ∼ N (0, σηI)
ηi ∼ N (0, σηI)
(2)
For each aspect, the reviewer has a latent sentiment polarity
expressing his opinion on that aspect of the item.
We extend Probabilistic Matrix Factorization (PMF) [19] to
model the user-specific aspect ratings. PMF assumes that
the user u has a vector of latent factors φu ∈ R
K, which represents
his personalized preferences that influence his opinions.
Analogously, each item has a latent vector φi ∈ R
K.
The overall rating of u for i is generated by the dot product
of the user latent factor and the item latent factor. In our
model, to predict user u’s opinion on a specific aspect a of
item i, we assume there is a latent factor φi,a ∈ R
K for each
aspect a of an item i, and the aspect rating rd,a of review
document d is generated from the dot product of the user latent
vector φu and the item aspect latent vector φi,a
5
. The
item aspect latent vector φi,a describes the latent properties
of the corresponding aspect of the item.
rd,a ∼ N (φ
⊤
u φi,a, σ
2
a) (3)
To control the model complexity, zero-mean Gaussian priors
are placed on the latent factors:
φu ∼ N (0, σ
2
uI)
φi,a ∼ N (0, σ
2
i,aI)
(4)
We can not directly use the continuous value rd,a to model
the word generative process since we need discrete ratings
to define the aspect-sentiment vocabulary (see Equation 11).
We introduce another latent variable ϕd,a ∈ R
R to represent
document d’s rating distribution on aspect a, where ϕd,a[r]
is the probability of p(rd,a = r) and PR
r=1 ϕd,a[r] = 1. We
define ϕd,a as follows:
ϕd,a[r] = N (r|φ
⊤
u φi,a, σ2
r,a)
PR
r′=1 N (r
′
|φ⊤u φi,a, σ2
r,a)
=
exp 
−
(r−φ⊤
u φi,a)
2
2σ2
r,a 
PR
r′=1 exp 
−
(r′−φ⊤u φi,a)
2
2σ2
r,a 
(5)
5Note that for simplicity we always assume that there is an
extra constant column in user/item latent factors to model
the bias effect.
wn
at
st
θd
ϕd,a
rd
φi,a φu
βa
γa,r
η0 ηu ηi
A
W
R
A
A U
I
T
D
U I
Figure 2: FLAME in graphical model notation.
We assume the overall rating of document d is generated
from the weighted sum of the aspect ratings, where the aspect
weights consist to the aspect distribution of the document.
rd ∼ N (
X
a
θd[a]E[rd,a], σ
2
r ) (6)
where E[rd,a] = φ
⊤
u φi,a.
For the process of generating words, we follow the assumption
in [20, 13, 25] that the words in one sentence of a review
refer to the same aspect. Topics learned under this assumption
are local topics that preserve sentence-level word concurrences
[20], while models like LDA [1] produce global topics
that preserve document-level word concurrences. Global
topic models are not suitable for aspect identification. For
example, because the words room and location appear together
in most reviews, global topic models are mostly likely
to cluster them in the same topic, but local topic models
assume they refer to different topics.
For each sentence t in the review d, we draw an aspect at
from the aspect distribution of the review:
at ∼ Multi(θd) (7)
and a sentiment rating st ∈ {1, 2, ..., R} on the aspect at
from the aspect rating distribution:
st ∼ Multi(ϕd,at
) (8)
Then, in each sentence t, the reviewer selects a set of words
wn ∈ t to express his opinions on the aspect at. We define
an aspect-sentiment multinomial word distribution αa,s on
the vocabulary, where αa,s[j] represents the probability of
generating the j-th word from the vocabulary for aspect a
and aspect rating s. wn can be an aspect word, e.g., battery,
or a sentiment word, e.g., good. So we assume that αa,s
depends on two factors: βa and γa,s, where βa represents
the correlation between the words and the aspect a, and γa,s
represents the correlation between the words and the pair of
Algorithm 1 Generative process of FLAME.
Draw η ∼ N (0, σ2
ηI)
for all u ∈ U do
Draw φu ∼ N (0, σ2
uI)
Draw ηu ∼ N (0, σ2
ηI)
end for
for all i ∈ I do
Draw ηi ∼ N (0, σ2
ηI)
for all a ∈ A do
Draw φi,a ∼ N (0, σ2
i,aI)
end for
end for
for all a ∈ A do
Draw βa ∼ N (0, σ2
βI)
for all r ∈ R do
Draw γa,r ∼ N (0, σ2
γI)
Set αa,r using Equation (9)
end for
end for
for all d ∈ D do
Set θd using Equation (1)
Set ϕd using Equation (5)
Draw rd using Equation (6)
// Generate each sentence t in document d
for all t ∈ d do
Draw at ∼ Multi(θd)
Draw st ∼ Multi(ϕd, at)
// Generate each word n in sentence t
for all n ∈ t do
Draw wn ∼ Multi(αat,st
)
end for
end for
end for
aspect a and aspect rating s.
αa,s[j] = exp(βa[j] + γa,s[j])
PV
l=1 exp(βa[l] + γa,s[l])
(9)
where βa and γa,r are V -dimensional vectors generated
from zero-mean Gaussian distributions.
βa ∼ N (0, σβI)
γa,r ∼ N (0, σγI)
(10)
The word wn is generated as follows:
p(wn|at, st, α) ∼ Multi(αat,st
) (11)
Figure 2 shows the graphical representation of FLAME,
omitting the priors {σ}. We summarize the generative process
in Algorithm 1.
4.1 Variational Inference
In general, we use an EM-style method to learn the parameters
in our model. We adopt a mixture of maximum a
posteriori (MAP) point estimates and Bayesian inference as
in [3]. To be specific, we use a combination of MAP estimation
over Ξ = {{η}, {φ}, β, γ} and Bayesian variational
inference over the other latent variables ∆ = {a, s} to derive
a lower bound of the log likelihood of the data, and
maximize the bound with respect to Ξ and variational parameters.
It should be noted that θ and ϕ are not latent
variables in our model. They are fixed given η and φ, as
shown in Equation (1) and (5).
The variational distributions of the latent variables in ∆
are defined as follows:
q(a, s|π,λ) = Y
d
Y
t∈d
q(at|πt)q(st|λt)
(12)
where πt ∈ R
A and λt ∈ R
R are free multinomial parameters.
We get the lower bound of the log likelihood of the data
as follows:
L =
X
d

hlog p(rd|φu, φi,a, θd)i +
X
t∈d

hlog p(at|θd)i
+ hlog p(st|ϕd, at)i +
X
n∈t
hlog p(wn|at, st, β, γ)i

+
X
u

hlog p(φu|σu)i + hlog p(ηu|ση)i

+
X
i

hlog p(φi|σi)i +
X
a
hlog p(φi,a|σi,a)i
+ hlog p(ηi|ση)i

+ hlog p(η|ση)i
+
X
a
hlog p(βa|σβ)i +
X
a
X
r
hlog p(γa,r|σγ)i
−
X
d
X
t∈d

hlog q(at|πt)i + hlog q(st|λt)i

(13)
where hp(Ω)i denotes the expectation of the probability of
p given the distribution q(Ω).
4.2 Learning the Parameters
In general, the learning procedure can be viewed as coordinate
ascent in L, i.e., alternatively optimizing one set of
parameters while fixing the others.
Updating π: We get the solution of πt by setting ∂L[πt]
∂πt
=
0 with the constraint P
a πt[a] = 1.
πt[a] ∝ θd[a]
Y
r

ϕd,a[r]
λt[r] Y
j
αa,r[j]
ct,jλt[r]
!
(14)
where ct,j is the frequency of the j-th word in sentence t.
Since ct,j is sparse, the complexity of updating πt is O(ct ·R·
A), where ct is the number of words in sentence t. The total
complexity for updating π in one EM iteration is O(c·R·A),
where c is the number of words of all the documents.
Updating λ: The update procedure of λ is similar to that
for π.
λt[r] ∝
Y
a
ϕd,a[r]
πt[a] Y
j
αa,r[j]
ct,jπt[a]
(15)
The complexity of updating λ in one EM iteration is also
O(c · A · R).
Updating φu : We can get L[φu] by only retaining those
terms in L that are a function of φu:
L[φu] =
X
d∈Du

−
(rd −
P
a
θd[a]φ
⊤
u φi,a)
2
2σ2
r
+
X
t
X
a
X
r
πt[a]λt[r] log ϕd,a[r]
!
−
φ
⊤
u φu
2σ2
u
(16)
The derivative of L[φu] with respect to φu depends on φu,
so we have to use a gradient ascent based method to update
φu.
Updating φi,a:
L[φi,a] =
X
d∈Di

−
(rd −
P
a
θd[a]φ
⊤
u φi,a)
2
2σ2
r
+
X
t∈d
X
a
X
r
πt[a]λt[r] log ϕd,a[r]
!
−
φ
⊤
i,aφi,a
2σ
2
i,a
(17)
We also use a gradient ascent based method to update
φi,a.
Updating η:
L[η0] =
X
d∈D

−
(rd −
P
a
θd[a]E[rd,a])2
2σ2
r
+
X
t∈d
X
a
πt[a] log θd[a]

−
η
⊤
0 η0
2σ2
η
=π
⊤
Dη0 −
η
⊤
0 η0
2σ2
η
−
X
d∈D
(rd −
P
a
θd[a]E[rd,a])2
2σ2
r
−
X
d∈D
Nd log X
a′
exp
η0[a
′
] + ηu[a
′
] + ηi[a
′
]

!
(18)
where πD =
P
d∈D
P
t∈d πt and Nd =
P
t∈d
P
P a πt[a] =
t∈d
1 is the number of sentences in d.
We apply gradient ascent method to optimize η0. The
derivative with respect to η0[a] is :
g(η0[a]) =πD[a] −
X
d∈D
Ndθd[a] −
η0[a]
σ2
η
+
X
d∈D
(rd −
P
a
θd[a]E[rd,a])(θd[a])(1 − θd[a])
σ2
r
(19)
The update formula of ηu and ηi is similar. The only
difference is to replace D in Equation (19) with Du and Di
respectively, where Du is the set of reviews of user u and Di
is the set of reviews of item i.
Updating β and γ:
L[βa] = c
⊤
a βa −
β
⊤
a βa
2σ
2
β
−
X
d
X
t∈d
πt[a]ct
X
r
λt[r] log X
l
exp (βa[l] + γa,r[l])!
(20)
where ca[j] = P
d
P
t∈d πt[a]ct,j , and ct =
P
j
ct,j denotes
the number of words in sentence t.
We use Newton’s method to optimize βa. The derivative
with respect to βa is :
g(βa) =ca −
X
r
Ca,rαa,r −
βa
σ
2
β
(21)
where Ca,r =
P
d
P
t∈d πt[a]ctλt[r] represents the expected
word counts for each (a, r) combination. The Hessian matrix
is:
H(βa) = X
r
Ca,rαa,rα
⊤
a,r − diag(
X
r
Ca,rαa,r +
1
σβ
1)
(22)
The update formula for βa is:
β
(t+1)
a = β
(t)
a − H−1
(β
(t)
a )g(β
(t)
a ) (23)
We use a linear algorithm for the Hessian matrices with
special structure [18, 1, 3], which lets the complexity of computing
H−1
(βa)g(βa) be O(V ) instead of O(V
3
).
We can also get the derivative and Hessian of γa,r as follows:
g(γa,r) =ca,r − Ca,rαa,r −
γa,r
σ2
γ
(24)
where ca,r[j] = P
d
P
t∈d πt[a]λt[r]ct,j .
H(γa,r) = Ca,rαa,rα
⊤
a,r − diag(Ca,rαa,r +
1
σγ
1) (25)
The complexity of updating γa,r is also linear in the size
of the vocabulary.
Computational Complexity: To conclude, the complexity
of one update iteration is O(c · A · R +T · A · K + D · K +
(I + U)· A + A · R · V ), where c is the total number of words
in the corpus, T is the number of sentences in the corpus,
and D is the number of documents in the corpus. Usually
K, A and R are small constants, so the complexity is linear
to the size of the review dataset.
Implementation Notes: An important issue is how to initialize
the model. We use the following initialization steps.
Taking the TripAdvisor data set as an example, we initialize
βa using the names of the aspect, i.e., we set βroom,room = 1
for the aspect room, and then learn the aspect distribution
of each sentence only based on the initialized β. Similar
techniques are also used in [13]. The aspect ratings of each
sentence are initialized using the overall rating of the review.
The parameters {σ} can also be learned using the coordinate
ascent-like procedure. We set them manually in our
implementation, e.g., we set σ
2
r = 1, σ
2
r,a = 0.5, ση = 10,
etc. Some optimization techniques, e.g., L-BFGS [17] and
backtracking line search [2], are applied to accelerate the
gradient ascent updates.
5. EXPERIMENTAL EVALUATION
In this section, we first describe the data sets we used in
our experiments and then discuss the experimental results
on different tasks.
5.1 Data Sets and Preprocessing
We use two review data sets for our experimental evaluation:
the TripAdvisor hotel review data6
and Yelp review
data7
. In the TripAdvisor data, besides the overall rating,
users are also asked to provide the aspect ratings on 6
pre-defined aspects: Location, Sleep Quality, Room, Service,
Value and Cleanliness, on a scale from 1 star to 5 stars. We
use these ground-truth aspect ratings to evaluate our model
on the task of aspect rating prediction. For Yelp data set, we
6
http://www.cs.cmu.edu/~jiweil/html/hotel-review.html
7
http://www.yelp.com/dataset_challenge
Table 2: Dataset Statistics
TripAdvisor Yelp
# Users 9,419 6,944
# Items 1,904 3,315
# Reviews 66,637 115,290
Density 0.37% 0.50%
# Sentences Per Review 12.60 ± 8.64 11.67 ± 7.80
# Words Per Sentence 7.50 ± 3.76 6.47 ± 4.64
extract a subset which only contains the reviews on restaurants.
We use the following preprocessing procedure on both of
the data sets. We first remove non-English reviews and reviews
with less than 3 sentences or 20 words, and then iteratively
remove users with less than 5 reviews and items
with less than 5 reviews. For the text in reviews, we remove
stop words and words that occur in less than 5 reviews, and
stem the remaining words using the PorterStemmer8
. After
the preprocessing, we have a hotel review data set including
66,637 hotel reviews of 1,904 hotels and a restaurant review
data set including 115,290 reviews of 3,315 restaurants. The
detailed statistics are listed in Table 2.
We randomly split both of the data sets into training and
test sets. Specifically, for each user, we randomly select 20%
of his reviews as test examples (For users with less than 10
reviews, we randomly select 2 reviews as test examples) and
put the rest reviews into the training sets. We train the
models on the training data sets and test their performance
on the test data sets. We use the model initialization method
and parameters selection strategies as discussed in Section
4.
5.2 Quantitative Evaluation
5.2.1 Perplexity on Held-out Reviews
As in standard topic models, we use perplexity of the heldout
test data sets to compare the generalization performance
of FLAME with some other state-of-the-art models.
Evaluation Measure. Perplexity is a standard measure
for topic models to measure how the model can generate future
documents [1]. For review mining, a good aspect-based
topic model should be able to predict what the reviewer will
write in a new review, which leads to a lower perplexity. A
strong correlation of the perplexity and accuracy of aspectbased
topic models is shown in [15]. We use a lower bound
on perplexity as in [4].
Perplexity(Dtest) = exp 
−
P
d
hlog p(wd|Ξ)i − hp(∆d)i
P
d Nd

We compare FLAME with the basic LDA model [1] and
the D-LDA model presented in [15]. D-LDA is a state-of-theart
aspect-based opinion mining model which can be seen as
a generalization of several other models [20, 6]. For D-LDA,
we also use the assumption that the words in one sentence
refer to the same aspect as in FLAME and other models
[15, 20, 6]. In the aspect-based topic models, we actually
use A × R latent topics, so we compare with LDA using
both A topics (LDA-A) and A × R topics (LDA-AR).
8
http://tartarus.org/martin/PorterStemmer/
Table 3: Perplexity on the held-out data sets
TripAdvisor Yelp
LDA-A 1012.80 767.24
LDA-AR 918.07 728.00
D-LDA 771.05 621.24
FLAME 733.12 590.46
For all the models, we use the same parameter settings
and stopping criteria. We set R = 5 for all the aspect-based
topic models. We train the models using the reviews in the
training sets and evaluate the perplexity on the test sets.
We test various numbers of latent aspects A = {6, 12, 24}.
Since the relative results are similar, we choose A = 6 for
discussion. Table 3 shows the perplexity on test data sets
of FLAME and the comparison partners. We can see that
D-LDA and FLAME, which are specifically designed for
aspect-based opinion mining, significantly outperform basic
LDA methods. FLAME achieves the best results among all
the models on both of the data sets. We believe this is because
FLAME can predict personalized aspect distribution
as well as aspect rating distribution, which other models do
not consider.
5.2.2 Aspect Rating Prediction on Held-out Reviews
Since we need the ground-truth aspect ratings to quantitatively
compare FLAME with other methods, we evaluate
the aspect rating prediction only on the TripAdvisor data
set. In order to align the latent aspects to the pre-defined
aspects in the TripAdvisor data set, we set A to be 6 and
use the initialization techniques discussed in Section 4.
We use the evaluation measures in [22, 23] to evaluate
different methods:
• Root Mean Square Error (RMSE) of the predicted aspect
ratings compared with the ground-truth aspect
ratings.
• Pearson Correlation inside reviews (ρA) to measure
how well the predicted aspect ratings preserve the relative
order of aspects within a review.
ρA =
1
D
XD
d=1
ρ(sd, s
∗
d)
where s
∗
d is the ground-truth aspect ratings for document
d, and sd is predicted aspect ratings.
• Pearson Correlation between personalized ranking of
items ρI . For each user and each aspect, we rank the
items by their predicted aspect ratings, and measure
how the ranked lists preserve the ground truth.
ρI =
1
U · A
XU
u=1
XA
a=1
ρ(sIu,a, s
∗
Iu,a)
where Iu is the set of items in user u’s test data, sIu,a
is the predicted aspect ratings on the set of items and
sIu,a is the ground-truth ratings.
• Zero-One Ranking loss (L0/1) [8], which measures the
percentage of mis-ordered pairs of items for each user.
Table 4: Aspect rating prediction on test set of TripAdvisor
data
PMF LRR+PMF FLAME
RMSE 0.970 1.000 0.980
ρA N/A 0.110 0.195
ρI 0.304 0.177 0.333
L0/1 0.210 0.238 0.196
It is computed as follows:
X
u
X
i,j∈Iu
1
Zu
XA
a=1
1[(su,i,a−su,j,a)·(s
∗
u,i,a−s
∗
u,j,a) < 0]
where Zu is the number of pairs in user u’s test set,
su,i,a is the predicted rating of user u of item i on aspect
a, and s
∗
u,i,a is the ground-truth aspect rating. We
do not choose nDCG since each user has few samples
in the test data (2.2 test samples per user), the values
of nDCG tend to be very close to 1 for all comparison
partners.
An intuitive solution of aspect rating prediction is just using
the overall rating of the review as prediction. We use
PMF [19] to predict the overall ratings of the reviews in the
test set and use the predicted overall ratings as predictions
for aspect ratings. To our best knowledge, [22, 23] are the
only work that predict aspect ratings at review-level. However,
they can only predict aspect ratings based on users’
reviews. In order to predict the aspect ratings in test set,
we first apply the LRR model [22] to extract aspect ratings
for each review in the training set, and then use PMF [19]
to train and to predict the aspect ratings in test set (we call
it LRR+PMF). We use the code of LRR provided by the
authors. Same training and testing strategies are applied to
all the models for fair comparison. We also test the performance
of with different values of the dimensions of latent
factors. The relative results are similar, so we only choose
K = 10 for discussion. Table 4 presents the results for aspect
rating prediction on the test set. We also highlight the
best performance in each measure in bold.
A general observation is that FLAME outperforms other
baseline models on all the measures except RMSE. It has
been discussed in [22] that RMSE is less important than
other measures since it does not reflect how the relative order
of the aspect ratings is preserved. ρA measures how a
method preserves the relative order of aspect ratings within
a review. PMF uses the same predicted ratings for all aspects,
so it is not applicable for ρA. ρI and L0/1 are the
most important measures for our task where we want to
rank items based on the predicted aspect ratings. PMF
outputs exactly the same item ranking lists for all aspects,
thus it is not suitable for real-world applications. We can
see that FLAME gets the best results on the two measures.
The gain is especially significant compared to LRR+PMF,
where there are about 90% improvement on ρI and 40%
improvement on L0/1.
Note that LRR+PMF does not achieve desirable performance.
The reason is that it is a two-step approach that the
errors induced in the first step have significant influence on
the performance of the second step.
5.3 Qualitative Evaluation
In this subsection, we evaluate FLAME on the task of
aspect identification. We perform qualitative analysis of the
top words obtained by FLAME to see whether FLAME can
produce meaningful aspects.
Figure 3 shows the word-cloud visualization of top words
(after stemming) with the highest generating probability in
the aspect-rating specific word distributions. We only show
3 aspects due to space limits. The three word-cloud figures
in the left column present the topic distribution β for the aspects
location, service and room, respectively. In general, the
top words generated by FLAME represent meaningful and
interpretable topics. We observe that the top words match
our intuition, e.g., words like “location”, “walk”, “street” have
higher weights in the word distribution of aspect location.
The middle and right columns show the top words of the
2-star (γa,2) and 5-star (γa,5) word distributions of for the
three aspects,. The aspect-rating specific word distribution
can automatically learn the sentiment oriented words, e.g.,
words like “bad”, “old”, “creepy” and “homeless” have high
weights in the 2-star word distribution of the aspect location,
while the words like “view”, “great”, “perfect”, “best”
have high weights in the 5-star word distribution of location.
One contribution of FLAME is that the aspect-rating topics
have sentiment polarities, i.e., 5-star topics are more positive
than 4-star, and so on. This is different from previous
work [20, 14, 16] where the latent ratings in these models
are rating labels which do not correspond to sentiment polarities.
6. FURTHER APPLICATIONS
The detailed analysis on personalized latent aspect ratings
enables a wide range of applications. Here we discuss three
sample applications.
Figure 4: Aspect Weights. Global represents the values
of η0. user-1 and user-2 are the aspect weights
ηu of two randomly sampled users, and item-1 and
item-2 are the values of ηi for two randomly sampled
items.
Aspect Distribution: Since FLAME can infer the aspect
weights for users and item, we can easily use the values
of η0, ηu and ηi for the rating behavior analysis. Figure
4 shows some sample results on TripAdviosr data. From
the histogram Global in the figure, we can see that Value
and Room are the most discussed aspects, and most people
rarely mention the aspect Sleep. Note that the values of ηu
(a) Location (b) Location 2-star (c) Location 5-star
(d) Service (e) Service 2-star (f) Service 5-star
(g) Room (h) Room 2-star (i) Room 5-star
Figure 3: Word-cloud visualization of top words with highest generating probability in β and γ. The left
Word size reflects the weight of each word. The three rows are the top words in the topic distributions for
the aspects location, service and room, respectively. The left column shows the top words in the aspect-word
distributions β of the three aspects. The middle and right columns show the top words in the aspect-ratingword
distributions γ. The middle column shows negative ones (2-star) and the right column shows positive
ones (5-star).
indicates the biases of users deviating from the global aspect
weights. We can see that user-1 likes to comment on
the Location and Sleep, while user-2 cares more about the
Service, Clean and Location. The two users have opposite
weights for the aspects Service and Sleep. Thus, when they
are searching for hotels, the aspects they care about are different.
It indicates that letting users choose to rank items
based on aspects which they cares about is very useful. The
aspect weights of items can be used to help merchants to
improve their services. If a specific aspect is discussed a lot
and most of the reviews are negative, the merchant should
think about how to improve this aspect.
Personalized Review Recommendation: As discussed
in Section 1, facing a large number of reviews expressing different
opinions, a user might have no idea of which reviews
are reliable. FLAME can alleviate this problem by sorting
the reviews by the similarities between reviewers with
current user. A simple way of computing the similarities
between users is to compute the distance between their latent
factors. Since personalized review recommendation is
hard to evaluate, we would like to leave it as a future work
on some data sets with ground-truth of user feedback on
reviews.
Recommendation Explanation: Traditional collaborative
filtering methods only provide predicted scores for
then items, but can not produce reasonable explanations
with the recommendations. A recent work [24] has shown
the possibility of using the aspect weights to generate some
explanations. FLAME can produce more persuasive recommendation
explanations by the predicted aspect ratings and
some selected reviews written by similar users.
7. CONCLUSION
In this paper, we introduce the problem of Personalized
Latent Aspect Rating Analysis to model users’ preferences
on different aspects. We propose a unified probabilistic
model FLAME which combines aspect-based opinion mining
and collaborative filtering. FLAME extracts aspect ratings
from the review text, and predicts aspect ratings of an item
that a user has not yet reviewed based on aspect ratings
within other reviews of the item by other users with similar
preferences. Our experimental evaluation on a hotel review
data set and a restaurant review data set shows that FLAME
can effectively solve the research problem. The qualitative
evaluation shows that FLAME can automatically extract
meaningful aspects and sentiment-oriented aspects. We also
investigate the ability of FLAME on the task of generating
future review text. Most importantly, our experiments on
TripAdvisor data sets show that FLAME significantly outperforms
state-of-the-art methods in terms of accuracy of
aspect rating prediction.
Some websites like TripAdvisor provide the option of rating
some pre-defined aspects. Although these aspect ratings
are typically incomplete, they may be helpful to partially
guide the learning of latent aspect ratings. It is worth to
explore a semi-supervised extension of FLAME. In this paper,
we only consider one same type of items, e.g., hotels
or restaurants. We plan to consider datasets with multiple
types of items which have different aspects, where users may
also have different preferences on different types of items.
8. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. J. Mach. Learn. Res., 3:993–1022,
Mar. 2003.
[2] S. P. Boyd and L. Vandenberghe. Convex
optimization. Cambridge university press, 2004.
[3] J. Eisenstein, A. Ahmed, and E. P. Xing. Sparse
additive generative models of text. In Proceedings of
the 28th International Conference on Machine
Learning (ICML-11), pages 1041–1048, 2011.
[4] M. Hoffman, F. R. Bach, and D. M. Blei. Online
learning for latent dirichlet allocation. In advances in
neural information processing systems, pages 856–864,
2010.
[5] M. Hu and B. Liu. Mining and summarizing customer
reviews. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’04, pages 168–177, New York, NY,
USA, 2004. ACM.
[6] Y. Jo and A. H. Oh. Aspect and sentiment unification
model for online review analysis. In Proceedings of the
fourth ACM international conference on Web search
and data mining, pages 815–824. ACM, 2011.
[7] Y. Koren, R. Bell, and C. Volinsky. Matrix
factorization techniques for recommender systems.
Computer, 42(8):30–37, 2009.
[8] J. Lee, S. Bengio, S. Kim, G. Lebanon, and Y. Singer.
Local collaborative ranking. In Proceedings of the 18th
international conference on World wide web, 2014.
[9] F. Li, C. Han, M. Huang, X. Zhu, Y.-J. Xia, S. Zhang,
and H. Yu. Structure-aware review mining and
summarization. In Proceedings of the 23rd
International Conference on Computational
Linguistics, pages 653–661. Association for
Computational Linguistics, 2010.
[10] B. Liu. Opinion mining and sentiment analysis. In
Web Data Mining, pages 459–526. Springer, 2011.
[11] B. Liu, M. Hu, and J. Cheng. Opinion observer:
analyzing and comparing opinions on the web. In
Proceedings of the 14th international conference on
World Wide Web, WWW ’05, pages 342–351, New
York, NY, USA, 2005. ACM.
[12] J. McAuley and J. Leskovec. Hidden factors and
hidden topics: understanding rating dimensions with
review text. In Recsys, 2013.
[13] J. McAuley, J. Leskovec, and D. Jurafsky. Learning
attitudes and attributes from multi-aspect reviews. In
Data Mining (ICDM), 2012 IEEE 12th International
Conference on, pages 1020–1025. IEEE, 2012.
[14] S. Moghaddam and M. Ester. Ilda: interdependent lda
model for learning latent aspects and their ratings
from online product reviews. In Proceedings of the 34th
international ACM SIGIR conference on Research and
development in Information Retrieval, SIGIR ’11,
pages 665–674, New York, NY, USA, 2011. ACM.
[15] S. Moghaddam and M. Ester. On the design of lda
models for aspect-based opinion mining. In
Proceedings of the 21st ACM international conference
on Information and knowledge management, CIKM
’12, pages 803–812, New York, NY, USA, 2012. ACM.
[16] S. Moghaddam and M. Ester. The flda model for
aspect-based opinion mining: addressing the cold start
problem. In Proceedings of the 22nd international
conference on World Wide Web, WWW ’13, pages
909–918, 2013.
[17] J. Nocedal. Updating quasi-newton matrices with
limited storage. Mathematics of computation,
35(151):773–782, 1980.
[18] G. Ronning. Maximum likelihood estimation of
dirichlet distributions. Journal of statistical
computation and simulation, 32(4):215–221, 1989.
[19] R. Salakhutdinov and A. Mnih. Probabilistic matrix
factorization. In Advances in neural information
processing systems, pages 1257–1264, 2007.
[20] I. Titov and R. McDonald. Modeling online reviews
with multi-grain topic models. In Proceedings of the
17th international conference on World Wide Web,
pages 111–120. ACM, 2008.
[21] C. Wang and D. M. Blei. Collaborative topic modeling
for recommending scientific articles. In Proceedings of
the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’11, pages
448–456, New York, NY, USA, 2011. ACM.
[22] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating
analysis on review text data: a rating regression
approach. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 783–792. ACM, 2010.
[23] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating
analysis without aspect keyword supervision. In
Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 618–626. ACM, 2011.
[24] Y. Zhang, G. Lai, M. Zhang, Y. Zhang, Y. Liu, and
S. Ma. Explicit factor models for explainable
recommendation based on phrase-level sentiment
analysis. In Proceedings of the 37th international ACM
SIGIR conference on Research and development in
Information Retrieval, SIGIR ’14, Gold Coast,
Australia, 2014. ACM.
[25] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly
modeling aspects and opinions with a maxent-lda
hybrid. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 56–65, Stroudsburg, PA, USA,
2010. Association for Computational Linguistics.

Public Opinion Mining on Social Media: A Case Study
of Twitter Opinion on Nuclear Power1
DongSung Kim2 and Jong Woo Kim2,3
2 222 Wangsimni-ro, Seongdong-gu Seoul 133-791, Korea
{paulus82, kjw}@hanyang.ac.kr 3 Corresponding Author
Abstract. Social media, including micro-blogs such as Twitter, have become
main channels to communicate and share public social opinion among people.
The opinion on nuclear power is an important social issue because nuclear plant
construction needs national consensus. This paper proposes an opinion mining
approach to monitor public sentiments on nuclear related issues using tweets on
Twitter. The proposed process consists on (1) crawling related tweets, (2) text
preparation, (3) sentiment dictionary construction, and (4) sentimental scoring.
Based on experiment using nuclear related tweets in Korean between 2009 and
2013, we verify the usefulness on the proposed approach and confirm the
changes on national opinion on nuclear generation depending on critical events
such as Fukushima Daiichi nuclear disaster.
Keywords: public opinion mining, sentiment analysis, nuclear power
1 Introduction
The popularity of social media such as Facebook and Twitter turned them into main
channels to communicate and share opinions on political, economic, social, and
cultural issues. Even though social media contribute to changing consumers to
prosumers (producers plus consumers), there are also some drawbacks on public
opinion conversation and convergence such as fraudulent and biased messages, witch
hunting, and extrusion of personal information. Reflecting the increase of interest on
opinion on social media, there has been trials and experiments to monitor and analyze
public opinion on specific issues on social media [4, 8].
Nuclear power is a significant national issue because it is a double-edged sword.
Economic efficiency of power generation is the most important benefit of nuclear
power. However, the potential risk of radiation leakage is the biggest difficulty of
nuclear energy. So, nuclear power plant construction requires national consensus and
agreement of residents in construction area. Traditional survey approach has been
 1 This work was supported by the Nuclear Power Core Technology Development Program of
the Korea Institute of Energy Technology Evaluation and Planning (KETEP) granted
financial resource from the Ministry of Trade, Industry & Energy, Republic of Korea (No.
201300000003242).
Advanced Science and Technology Letters
Vol.51 (CES-CUBE 2014), pp.224-228
http://dx.doi.org/10.14257/astl.2014.51.51
ISSN: 2287-1233 ASTL
Copyright © 2014 SERSC
used to monitor and investigate public opinion on nuclear power; however, it takes
excessive cost and time. Opinion mining approach can provide an alternative way to
monitor public opinion on nuclear power. This paper aims to suggest and verify
opinion mining approach on nuclear power. The structure of the paper is as follows.
In section 2, the researches of opinion mining approaches to monitor customer
opinion and public opinion are described briefly. In section 3, the proposed opinion
mining approach will be presented. In section 4, the experimental results using
Twitter data between 2009 and 2013 are provided to demonstrate the usefulness of the
proposed approach. Section 5 includes conclusion remarks.
2 Related Works
Recently, there have been many researches to monitor public opinion and social
trends [1, 6]. They include election prediction using Twitter data [3, 11], monitoring
of customer sentiment on a certain brand [7], movie performance prediction using
Twitter [2, 9], disease and disaster tracking using Internet information [10], and
unemployment benefit prediction using Internet search information [5]. Especially,
public opinion monitoring is useful in sensing public opinion trends and reduction of
potential social risks and conflicts [8]. In this study, we mainly focus on public
opinion on nuclear power.
3 Proposed Approach and Experimental Design
The proposed procedure for public opinion mining consists of four phases: (1)
crawling social media data, (2) cleansing and preprocessing texts, (3) construction of
a sentiment dictionary, and (4) tweets sentiment prediction. The detail of such
experimental procedure is shown in Fig. 1.
Fig. 1. Experimental Procedure
In the crawling phase, tweets including terms “Nuclear” or “Nuclear power” in
Korean are crawled by a crawling tool, LocoySpider2. The tweets from 1st 2009 to
 2 www.locoy.com
Advanced Science and Technology Letters
Vol.51 (CES-CUBE 2014)
Copyright © 2014 SERSC 225
December 31th 2013 are within the scope of crawling. After excluding irrelevant
tweets on nuclear power issue, potential sentimental terms are extracted though stop
words elimination, stemming, morphological analysis, and POS (Part of Speech)
tagging using Korean morphological analyzer, KoNLP (Korea Natural Language
Processing)3. Finally, nouns are extracted as potential sentimental terms.
The tweets from 2009 to 2011 are used to construct sentimental dictionary. The
extracted nouns are reviewed by human evaluators and classified to positive terms
and negative terms. The number of positive terms is 1,012 and that of negative terms
is 3,291, which reflects that negative tweets are dominant in nuclear power tweets.
The tweets between 2012 and 2013 are used to evaluate the performance of
sentimental classification. Sentimental classification is based on sentimental scores of
tweets. The sentimental score of a tweet is calculated based on the number of positive
terms and the number of negative terms in the tweet. The range of sentimental score is
between -1 and 1. Tweets are classified as positive tweets when sentimental scores are
greater than 0, and as negative tweets when those are less than 0. The tweets with 0
sentimental scores which mean that there are no positive terms and negative terms, or
the number of positive terms and negative terms are the equal.
))(_())(_(
))(_())(_( )(_
N Positive terms NegativeNt terms t
N Positive terms NegativeNt terms t Sentimental Score t
+
− = (1)
4 Experimental Results
Table 1 exhibits the sentimental prediction results. To evaluate the accuracy, before
applying proposed approach, human evaluators are classified tweets between 2012
and 2013 into three categories, Positive, Negative, and Neutral. The result shows that
the proposed approach provides more than 50% prediction accuracy on positive and
negative tweets.
Table 1. Sentimental prediction accuracy
Sentiment
2012 2013
Accuracy Rate No of
Tweets
Accuracy Rate No. of
Tweets
Positive 51.58% 948 50.55% 991
Negative 61.19% 2067 64.08% 2289
Neutral 38.96% 2066 21.67% 1375
To trace the changes on public opinion on nuclear power, we propose a measure,
monthly Nuclear Opinion Index (NOI). As shown in formula (2), a monthly NOI is
 3 Heewon Jeon (2013). KoNLP: Korean NLP Package. R Package Version 0.76.9.
http://CRAN.R-project.org/package=KoNLP
Advanced Science and Technology Letters
Vol.51 (CES-CUBE 2014)
226 Copyright © 2014 SERSC
defined based on the number of positive tweets, negative tweets, and total tweets of
the month. Fig. 2 shows monthly NOIs between 2009 and 2013. In Fig. 2, we can see
the dramatic changes of nuclear power opinion in March 2010 due to Fukushima
nuclear disaster.
100100
))(_(
))(_())(_( )( +× − = TotalN tweets m
N Positive tweets NegativeNm tweets m Nuclear Opinion mIndex (2)
Fig. 2. Changes on monthly nuclear opinion index
5 Conclusions
In this paper, we propose public opinion mining approach to monitor nuclear power
opinion on Twitter. The procedure consists of four phases: (1) crawling related tweets,
(2) extracting potential sentimental terms, (3) building sentiment dictionary, and (4)
tweets sentiment scoring and prediction. The experiments using tweets between 2009
and 2013 showed that the proposed approach provided acceptable performance on
sentimental prediction. Also, NOI (Nuclear Opinion Index) is proposed to visualize
the sentimental changes on nuclear power opinion.
References
1. Akcora, C.G., Bayir, M.A., Demirbas, M., Ferhatosmanoglu, H.: Identifying Breakpoints in
Public Opinion. In: 1st Workshop on Social Media Analysis, pp. 62--66. Washington, DC
(2010)
2. Baek, H.M., Ahn, J.H., Oh, S.W.: Impact of Tweets on Movie Sales: Focusing on the Time
when Tweets are Written. J. ETRI. (2014)
3. Boutet, A., Kim, H., Yoneki, E.: What's in Your Tweets? I Know Who You Supported in the
UK 2010 General Election. In: The International AAAI Conference on Weblogs and Social
Media (2012)
Advanced Science and Technology Letters
Vol.51 (CES-CUBE 2014)
Copyright © 2014 SERSC 227
4. Choi, H., Varian, H.: Predicting the Present with Google Trends. Technical Report, Google
(2009)
5. D’Amuri, F., Marcucci, J.: “Google it!” Forecasting the US Unemployment Rate with a
Google Job Search Index. In: Conference on Urban and Regional Economics (2009)
6. Diakopoulos, N., Shamma, D.A.: Characterizing Debate Performance via Aggregated
Twitter Sentiment. In: Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems, pp. 1195--1198. ACM (2010)
7. Liu, Y., Huang, X., An, A., Yu, X.: ARSA: a Sentiment-aware Model for Predicting Sales
Performance Using Blogs. In: Proceedings of the 30th Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval, pp. 607--614.
ACM(2007)
8. Lee, C.H., Hur, J., Oh, H.J., Kim, H.J., Ryu, P.M., Kim, H.K.: Technology Trends of Issue
Detection and Predictive Analysis on Social Big Data. J. Electronics and
Telecommunications Trends. 28, 62--71 (2013)
9. Rui, H., Liu, Y., Whinston, A.: Whose and What Chatter Matters? The Effect of Tweets on
Movie Sales. Decision Support Systems. 55, 863--870 (2013)
10. Sakaki, T., Okazaki, M., Matsuo, Y.: Earthquake Shakes Twitter Users: Real-time Event
Detection by Social Sensors. In: 19th International Conference on World Wide Web, pp.
851--860. ACM (2010)
11. Tumasjan, A., Sprenger, T.O., Sandner, P.G., Welpe, I. M.: Election Forecasts with Twitter
How 140 Characters Reflect the Political Landscape. J. Social Science Computer Review. 29,
402--418 (2011)
Advanced Science and Technology Letters
Vol.51 (CES-CUBE 2014)
228 Copyright © 2014 SERSC

International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
DOI:10.5121/iju.2014.5201 1
A proposed Novel Approach for Sentiment
Analysis and Opinion Mining
Ravendra Ratan Singh Jandail
Computing Science and Engineering, Galgotias University, India
Abstract
as the people are being dependent on internet the requirement of user view analysis is increasing
exponentially. Customer posts their experience and opinion about the product policy and services. But,
because of the massive volume of reviews, customers can’t read all reviews. In order to solve this problem,
a lot of research is being carried out in Opinion Mining. In order to solve this problem, a lot of research is
being carried out in Opinion Mining. Through the Opinion Mining, we can know about contents of whole
product reviews, Blogs are websites that allow one or more individuals to write about things they want to
share with other The valuable data contained in posts from a large number of users across geographic,
demographic and cultural boundaries provide a rich data source not only for commercial exploitation but
also for psychological & sociopolitical research. This paper tries to demonstrate the plausibility of the idea
through our clustering and classifying opinion mining experiment on analysis of blog posts on recent
product policy and services reviews. We are proposing a Nobel approach for analyzing the Review for the
customer opinion.

I. INTRODUCTION
Human opinion and perception always be the part of decision making either in the historical days
by the Ruler, for employing any scheme and in some critical situation for their kingdom or in now
days by any organization or country to know about its product, policy and services. Opinion
Mining is a process, used for automatic extraction of knowledge from the opinion of others bout
some particular topic or problem. Human perception and user opinion has greater potential for
knowledge discovery and decision support. The goal of OM is to make computer able to
recognize and express emotions. A thought, view, or attitude based on emotion instead of reason
is called sentiment. Thus OM is also referred as sentiment analysis. During decision making
process most of us get help from others. It is a natural phenomenon that good decision can be
taken on the basis of opinion of others. Automatic detection of emotions in texts is becoming
increasingly important from an applicative point of view. Survey, blogs and review site are used
to collect customer opinion about products to get the knowledge about the reputation of the
company in the market [1]. Now day’s social media is the best tool to know about the
People opinion, advice, comment, complement and their perception about any product policy and
services. We are utilizing social media for the manufacturer and consumer, service provider and
user, policies and its effects. 
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
2
Social media has demonstrated itself to be a proven source of information towards the marketing
of products. This unique source of data provides a rapid means of customer feedback that is used
to support a number of business areas. Social media can provide immediate feedback in minutes
and as such it represents a new challenge in communication between a customer and business.
One can use social media to gauge customer response, in the form of what they like (or don't
like), along with associated details. This can support many facets of a business including product
development, customer service or marketing. Thus, it is important to utilize social media to avoid
any shifts in brand image and corporate reputations when certain issues are not handled early and
effectively. By leveraging such channels appropriately, the customer relationship can be
strengthened, increasing value in all aspects of a company’s business [2]. As The Lord Krishna
said in our BHAGVAT GEETA that every relation of this universe is based on some needs and
while the need is fulfilled the relation will be sweet but if the chain of need braked then the
relation will also be destroy itself. So we are trying providing “not, what the manufacturer wants
to develop?” But “What the consumer wants?” not “what the services is available?” but “what the
quality of services user wants” not “what policies are applied?” but “how the policies should be
for people?” by utilizing the social media blogs and reviews
II. DEFINATION
Many definition of sentiment analysis and opinion mining has been provided by different authors’
publisher and writer but the most appropriate is that “Sentiment analysis is the way of
Analyzing the social, psychological, philosophical, behavioral perception and opinion of a
Person, people or group in a specific situation and condition about any
• Product
• Services
• Event
• Thought
• Person or organization
• Policy
• Weather and
Particular situation”
III.DIFFERENT LEVEL OF ANALYSIS
A. Document level
In this approaches whole document is considers as a single entity and the analysis approaches
in applied on the whole document. The result generated in document level sometimes not
appropriate
B. Sentence level
In the sentence level approaches every sentence is considered as an entity and analysis
approaches is applied on individual sentence then their result is summarized to provide the
overall result of the document. It is known as Clause level analysis
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
3
C. Entity and Aspect level
 It is a feature based approaches in this approaches we
 Considered positive and negative sentiments for the
 Quality of features. This approaches is based on the
 Opinion mining and summarization
.
IV- ISSUES IN SENTIMENT MINING
There are six kind of issues in the sentiment analysis but sometimes the main problem is occur
while opinion suitable for user group became harmful for the manufacturer or vice versus which
are elaborate by Bing Liu in his book “Sentiment Analysis and Opinion Mining”.
a. A positive or negative sentiment world may have their opposite meaning in a particular
domain so it is hard to predict by its keyword meaning.
Ex- the picture quality of this camera is high and the resuming time of this camera is also high. In
this sentence the first HIGH is showing the positive sentiment for the camera but the second high
is showing the negative sentiment for the camera.
b. Interrogative Sentence An interrogative sentence may not have neither positive nor negative
sentiment but the key word used in the opinion may be positive or negative.
Ex1. Can anybody tell me is HCL a good laptop for an engineering student for multicore
programming?
2. What are the good and bad functionality of hero-Honda motorcycle?
Such kind of sentences don’t have any positive or negative sentiment for analysis but the
keyword used in such sentences have its precious and valuable meaning for analyzing any
sentences.
c. Sarcastic Sentences
Few sentences in the form of jocks may violate the meaning of the whole sentences such kind of
sentence need a power full attention toward the keywords and sentences. These funny sentences
not only violet the sentence of a particular sentence but also destroy the value of the whole
document. Ex- 1. What a Great Car? It stopped Working in 10 days. Who is going to purchase
such beautiful car?
4. Sentiment without sentiment words sometimes sentiments does not use any sentiment words
like good , better , best , worst ,bad etc. but the sentences may have its positive or negative
feedback about the product , services and policies. Ex- this car consume lot diesel from Delhi to
Chandigarh then the other one. This sentence showing the negative sentiment but it is not using
the any sentiment word.
5. Natural language Issues Change Place to Place
Motorola word can be used as motto, Lamia word can be used as Lummi. Such kind of sentences
changes the meaning of the issue and sentences.
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
4
6. Conditional sentences conditional sentences are also an issue in Sentiment mining
conditional sentences is also creating the same problem like interrogative sentences.
Ex- If the picture quality of this camera is good I will buy it.
7. Author and Reader understanding point (person to person varying)
Dollar price is increasing with respect to Indian rupee.
This document have both the positive and negative meaning and its value is varying from person
to person. This sentence has the positive sentiment for the Exporter while this same sentence has
the negative sentiment for the importers.
8. Spam Reviews Spam sentiments are those sentiments which are posted by the opposite or
competitor organization for increasing their product value or their organization value among the
users. Some politician may use the same spam review to just for their publicity.
V. APPLICATION OF SENTIMENT ANALYSIS
Early applications of sentiment analysis focused on classifying movie reviews or product reviews
as positive or negative or identifying positive and negative sentences, but many recent
applications involve opinion mining in ways that require a more detailed analysis of the sentiment
expressed in texts. One such application is to use opinion mining to determine areas of a product
that need to be improved by summarizing product reviews to see what parts of the product are
generally considered good or bad by users. Another application requiring a more detailed analysis
of sentiment is to understand where political writers fall on the political spectrum, something that
can only be done by looking at support or opposition to specific policies A couple of others
applications, like allowing politicians who want a better understanding of how their constituents
view different issues, or predicting stock prices based on opinions that people have about the
companies and resources involved the marketplace, can similarly take advantage of structured
representations of opinion. These applications can be tackled with a structured approach to
opinion extraction [4].One specific application of sentiment in NLP that can be used for this
purpose is sentiment analysis. It can be used to identify and extract subjective information from
the information source collected. With all these processes and methods, it is possible to build a
system which can extract application dependent information, process it and produce data which
can be used for studying and deductions based on the information retrieved [5].Much of the
current opinion mining research has focused on business and e-commerce applications, such as
product reviews and movie ratings. Little research has tried to understand opinions in the social
and geopolitical context [6].
Applications to Review-Related Websites
Applications as a Sub-Component Technology
Applications in Business and Government Intelligence
Applications across Different Domains [7]
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
5
VI. MACHINE LEARNING TECHNIQUE USED CLASSIFICATION OF
TEXT IN SENTIMENT MINING
Since the beginning of this century, several researchers have studied the implementation of
machine learning technique to capture sentiments from online reviews using this technique, most
researchers treated opinion mining as a text classification problem. Conventionally, this method
involves the creation of a model using statistical technique such as Support Vector Machine
(SVM) and Naïve Bayes (NB). The sentiment of a new sentence or document is predicted using a
particular statistical model. The results of opinion mining were compared using three statistical
models. They concluded that even though these techniques produce good result in a normal text
classification problem, it is not the same case with opinion mining [8]
Engineers and computer scientists use machine-learning techniques for automatic affect
classification from video, voice, text, and physiology. Psychologists combine the long tradition of
emotion research with their own discourse, models, and methods. Opinion mining and sentiment
analysis are inextricably bound to the affective sciences that attempt to understand human
emotions. Affect-sensitive systems and psychological emotion research must develop together
[9].
The most widely used machine learning techniques are following
1. Naive bayes classification
2. Maximum entropy rule
3. Decision induction tree
4. Neutral network
5. Probability latent semantic
6. Latent Dirichlet Allocation
VII. PROBLEM DEFINITION AND OUR PROPOSAL
The primary question for sentiment analysis is how to map a tweet to a correct emotion, which
user tried to express. The first problem is unstructured, ungrammatical text. Since tweet messages
are restricted to 140 characters length, users may have a propensity to use abbreviations, slangs,
or emoticons to shorten the text. This issue can lead to unusual messages.
The second problem is the fact that tweet messages are not always correct. During fast typing, or
using mobile phones as input device, user may have mistyped text and make the analysis step
harder.
The third problem is ambiguity. Due to the small amount of information, it is difficult to identify
the corresponding objects of interest. For example: “Apple” can either be a laptop brand or a fruit.
The fourth problem concerns which concrete emotion to focus on analyzing since human emotion
is very diverse.
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
6
The difficulty lies in the fact that there could be mixed opinions in a document, and with the
creative nature of natural languages, people may express the same opinion in vastly different
ways, sometimes without using any opinion words:
We are also try to overcome the issue related to sentiment mining as we discussed earlier i the
section 2.
VIII. OUR METHODOLOGY
Generally approached are used for Sentence level, Document level, or feature level but our
approaches will work for all the three above mentioned levels. First we Split the document in the
sentences then every sentence will classify according to its problem domain, if sentence is not
belongs to any domain then it will be consider as the normal sentence.
No of available keyword in any sentence will be the weight age of that particular sentence.
In case of product, the product will be classified in its attributes and on the basis of keyword
available for a particular attributes we give a weight age -5 to +5. In this way every attribute will
be weighted -5 to +5. After all, the average of the all attributes will be the average of the weight
age of the overall product. In this way not even we can summaries about the product but also the
individual attributes available in the product.
For Example – A Mobile phone have attributes like , weight , picture quality, Resolution ,
touchpad , keypad , voice quality, music quality, Bluetooth , etc. By our approach we can predict
about a particular attribute and the overall product according to the user perception.
A. FLOW CHART
Flowchart is showing the working of our designed algorithm for People product review Analysis.
Flowchart is showing top down approach for methodologies.

 
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
7
No
Yes
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
8
We are Using Support Vector Machine Learning Technique for text classification in its classes
and predefined Feature. If we take the Example of Cell Phone than Cell phone is the class and
Camera, Sound , Keypad , Battery are its predefine feature of the class cell phone.
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
9
IX. SENTIMENT WORD DICTONARYWe
designed a Dictionary of 576 sentiment words after analyzing the more than 5000 reviews of
the mobile phone collected from www.amazon.com , www.areena.com and twitter. We have
assigned weight for every sentiment word from -5 to +5 as its importance by user sentiment for
example “the Sound of this speaker is heavy and this camera is also heavy to carry”. In this
sentence the Word heavy is used for both Camera and the Speaker , everyone do understand that
the Word “Heavy” is positive for the Speaker but this word is negative for the Camera Weight. So
we assigned rating for every word as its value for ever sentiment.
X. SENTIMENT CALCULATION
We are able to analyses the total text in its positive and negative polarity for the particular feature.
S(W+ve) = Set of Positive Sentiment words
S (W-ve) = Set of Negative Sentiment words
For Nth Feature –
S (W+) = (W1 + W2 + W3 +………Wn) Set of
 Positive Sentiment Words
S (W-) = (W1 + W2 + W3 +………Wn) Set of
 Negative Sentiment Words
First of all we calculate the negative and positive polarity every sentiment by calculating its
probability. Then we shall calculate the mutual information of its positive or negative sentiment.
After calculating the overall polarity of the individual feature we shall calculate the Mutual
information for every feature for its final product review Analysis.

XII. RESULT
Initially we obtained these results for the mobile phone only, we include 16 most usable feature
and their related keywords for the mobile phone. We will also apply this for movie sentiments
and other most usable products. We are designing dictionary and keywords for the specific
products like camera and washing machine. 
International Journal of UbiComp (IJU), Vol.5, No.1/2, April 2014
10
IX FUTURE WORK
Initially we are applying our approach on product usability analysis .It will classify users review
in its components and all components usability is decide by keywords base learning. In future we
will proceed it for policy and services. We believe that we will provide better result for the
Product usability analysis. After completing this phase we shall apply it for the situations also.
References-
[1] Khairullah Khan, BaharumB.Baharudin, Aurangzeb Khan, Fazal-e-Malik , Mining Opinion from text
Documents: A Survey, 3rd IEEE International Conference on Digital Ecosystems and Technologie,
2009
[2] David Alfred Ostrowski, Sentiment Mining within Social Media forTopic Identification ,IEEE Fourth
International Conference on Semantic Computing 2010
[3] ANA SUFIAN ,RANJITH ANANTHARAMAN ,Social Media Data Mining and Inference system
based on Sentiment Analysis 2011
[4] KENNETH BLOOM,Sentiment Analysis Based On Appraisal Theory And Functional Local
Grammar 2011.
[5] ANA SUFIAN ,RANJITH ANANTHARAMAN , Social Media Data Mining and Inference system
based on Sentiment Analysis 2011
[6] Hsinchun Chen and David Zimbra,AI and Opinion Mining ,IeeeInTeLLIGenTSySTeMS 2010
[7] Bo Pang and Lillian Lee Opinion Mining and Sentiment Analysis, Foundations and TrendsR_
inInformation Retrieval 2008
[8] NorlelaSamsudin, MazidahPuteh, Abdul RazakHamdan, MohdZakree Ahmad Nazri , Is Artificial
Immune System Suitable for OpinionMining? 4th Conference on Data Mining and Optimization
(DMO)
, Langkawi, Malaysia 02-04 September 2012
[9] rik Cambria, Bj rnSchuller, unqing ia, Catherine Havasi, Published by the IEEE Computer
Society 2013
[10] ThanhThung ,evaluation of natural language Processing technique for Sentiment Analysis , 2012

Int. J. Advance. Soft Comput. Appl., Vol. 6, No. 1, March 2014
ISSN 2074-8523; Copyright © SCRG Publication, 2014

Text Opinion Mining to Analyze News for
Stock Market Prediction
Yoosin Kim1
, Seung Ryul Jeong1
, Imran Ghani2
1Business IT Graduate School, Kookmin University, Seoul, South Korea
e-mail: (trust,srjeong)@kookmin.ac.kr
2Universiti Teknologi Malaysia (UTM), Skudai, Johor Bahru, Johor, Malaysia
e-mail: imran@utm.my
*Corresponding author: Seung Ryul Jeong
Abstract
 This is a known fact that news and stock prices are closely related
and news usually has a great influence on stock market investment.
There have been many researches aimed at identifying that
relationship or predicting stock market movements using news
analysis. Recently, massive news tests, called unstructured big-data,
have been used to predict stock price. In this paper, we introduce a
method of mining text opinions to analyze Korean language news in
order to predict rises and falls on the KOSPI (Korea Composite
Stock Price Index). Our method consists of carrying out the NLP
(Natural Language Processing) of news, describing its features,
categorizing and extracting the sentiments and opinions expressed
by the writers. The method then identifies the correlation between
news and stock market fluctuations. In our experiment, we show that
our method can be used to understand unstructured big-data, and we
also reveal that news’ sentiment can be used in predicting stock price
fluctuations, whether up or down. The algorithm extracted
experiments can be used to make predictions about stock market
movements.
 Keywords: Big Data, Text Mining, Opinion Mining, Stock Prediction, KOSPI,
NLP.
1 Introduction
Recently, unstructured text data, also known as big data, in web, mobile and SNS
communications has rapidly emerged as a great source of useful information,
ranging from news articles to personal opinions. Accordingly, text mining
technology is becoming increasingly important as a means of rapidly analyzing 
Yoosin Kim et al. 2
massive texts in order to derive significant information intelligently. It has many
potential applications that could be used very effectively in such sales, marketing,
manufacturing and various other areas [1].
In particular, the prediction of stock values using the news or personal opinions on
the web constitutes an attempt to extract meaningful characteristics from texts
using text mining; classify the news as good or bad for stock prices, simulate
investment and predict price trends. However, it is still difficult to analyze and
extract meaning because such text data comes in many diverse types, and
comprises multi-meanings, complex words and various features.
The news, generally, presents both positive and negative aspects of the stock
markets in a somewhat neutral tone, making it difficult to identify the underlying
truth behind such news. Furthermore, there is a danger that the news may be
analyzed and interpreted differently depending on the writer [5, 10, 12].
Thus, in order to overcome such limitations, we propose an text opinion mining
system for supporting decision-making to invest, , whereby massive news articles,
unstructured big data, are gathered, parsed, tagged, analyzed, and converted to
opinions suitable for making stock market predictions. Also, we built a stock
market oriented sentimental word dictionary; a lexical resource for sentiment
analysis and opinion mining, as opposed to a general purpose a sentimental word
dictionary. At the end, we experimented with news originating from two different
media, and tested its accuracy in forecasting stock price fluctuations.
2 Related Works
2.1 Opinion Mining
Opinion mining, as a sub-discipline with data mining and computational
linguistics, is referred to as the computational techniques used to extract, classify,
understand, and assess the opinions expressed in various online news sources,
social media comments, and other user-generated content [3]. ‘Sentiment’
analysis is often used in opinion mining to identify sentiments, affect, subjectivity,
and other emotional states in the online text.
Works on opinion mining, such as that by [6], showed the effectiveness of
automatic movie review mining and summarization of movies. The [13] proposed
a technique by which to list ranked product reviews according to the intentions of
the searcher. The [4] showed how market moods monitored from Twitter may be
used to predict the flow of stock prices. Many works have focused on predicting
stock prices using news opinion mining [7, 8, 9, 10, 11].
In order to more accurately extract opinions and sentiments from a text, it is very
important to build up a lexicon of opinion mining. If a lexicon such as a
sentimental word dictionary is developed properly, the results of opinion mining 
3 Text Opinion Mining to Analyze News for Stock Market Prediction
will be good. Furthermore, sentimental word dictionaries are more effective when
domain specific characteristics are taken into consideration [12].
2.2 Stock Prediction Using the News
Many works over the years have continued to prove that the news is closely
related to stock prices. In particular, with the recent explosive increase in the
amount of unstructured text data from the internet, mobile channels, and SNS
(Social Network service), there have been attempts to predict stock movements
using such text data. [7], using the stock prediction system NewsCASTS (i.e. the
News Categorization And Trading System, which consists of three engines,
namely, news pre-treatment, categorization and trading), analyzed media news on
specific companies, and experimented with a comparison between news and stock
price flows. The [10] gathered, analyzed and extracted individual investors’
opinions disclosed on the web, analyzed their sentiments, calculated their author’s
reliability, and predicted the stock values of three companies via machine learning.
The [8] proposed the machine learning system, AZFinText(Arizona Financial
Text System), to infer stock price prediction variables from the news, and showed
higher returns than the market average through trading simulation.
In Korea, [11] proposed an automatic news classifier and showed that the use of
pattern matching-based news classification delivers a rate of accuracy of 69%
when predicting whether stock prices will rise and a rate of 64% when predicting
whether they will fall. The [9] extracted the features of news texts to compare
stock price variations, and experimented with a classifier to predict whether
specific companies’ stock prices would go up or down, using the Naïve Bayesian
model. The [5] suggested an intelligent investment model by text opinion mining
which analyzes the sentiments of ‘news big data’, and showed that predictions
using a logistic regression analysis achieved a rate of accuracy of 70.0% for
increases in stock prices and 78.8% for decreases.
3 Approach
3.1 System Overview
The Figure 1 shows the overall outline of our system. The first step consists of
news gathering. In this step, we got a scrapping of the online economic news
boards on Naver.com and stored the data in a database. The next step consisted of
natural language processing in order to extract sentiment from unstructured news
texts. We removed stop words such as punctuation, numbers, English, html tags,
and so on. The remaining useful words were then used to build a dictionary of
sentiments. We conducted a sentiment analysis and opinion mining using the
sentiment word dictionary, and then conducted supervised learning experiments
aimed at predicting rises and falls in stock prices. 
Yoosin Kim et al. 4
Fig.1: System overview
3.2 Data Collection
For the experiment, we gathered 78,216 economic news articles of media
company M and H, over a period of one year (2011) from Naver.com (the No.1
portal in Korea), using scrapping technology. The two media display some
different characteristics in terms of news. The online media M, a new entity
established in 1999, focuses more on stock investment issues, and the rapid
delivery of information on market situations and investment strategies. On the
other hand, H media’s vision is represented as ‘the global comprehensive
economic media’. H media, launched in 1964, provides a wide range of economic
information, realty and industrial issues encompassing newspapers, TV, and the
Internet.
The experiment divided the data into seven months of news articles from January
1 to July 31, 2011 as a learning data set, and into five months of news articles
from August 1 to December 31, 2011 as a verification data set. Of the data, the
number of media M’s news articles amounted to 44,305, while that of media H’s
news’ articles amounted to 33,911. The Table 1 shows the data sets by media.
Table 1: Experiment Data Set
Data Set M H M+H
Training Data(1/1~7/31) 25,955 16,023 41,978
Test Data(8/1~12/31) 18,350 17,888 36,238
Total 44,305 33,911 78,216 
5 Text Opinion Mining to Analyze News for Stock Market Prediction
3.3 Sentiment Analysis
Analyzing the sentiment of the news involves recognizing and defining the
‘emotional state’ expressed in the text. The sentiment word dictionary plays a
crucial role in opinion mining to build linguistic resources, which classify
sentiment polarity, quantify the breadth of sentiment, and discriminate between
sentiments. In particular, [12] built a stock domain specific dictionary, which
showed greater accuracy compared with general sentiment dictionaries in terms of
its ability to predict price stock movements. Similarly, we extracted sentiment
word from the news using NLP, calculated the sentiment score, and mining the
opinion. The Figure 2 shows a flow diagram of the development of a sentiment
word dictionary aimed at predicting rises and falls in stock prices.
Fig.2: Flow of development of dictionary
In more detail, to develop a sentiment word dictionary, two phases are needed.
The first phase consists in deleting stop words such as punctuation, numbers,
English and one-character words, extracting high performance words using term
frequency, and cleaning duplicate words and proper noun as product name.
The next phase consists in forming an opinion of the news by calculating the
probability of recurrence of certain ‘sentimental’ words. The sentiment of a word
is defined as the ratio of the number of stock price up or down following to the
total number of news containing the word, whose calculation formula is expressed
as follows: 
Yoosin Kim et al. 6
(1)
In the formula, the score for a sentimental word ranges from a maximum of 1 to a
minimum of 0. In other words, 1 is a fully positive sentiment whereas 0 is an
entirely negative one. The sentiment score of a news article is calculated based on
the average sentiment score for all sentimental words contained in the news.
Likewise, the sentiment score for a day is calculated by the average sentiment
score of the total news of that day.
3.4 Stock Predictions
The sentiment score should be changed an opinion to predict a rise or fall in the
stock price. An opinion for prediction is decided to be positive, or not on a certain
point. A next trading day’s stock price is predicted to rise, or, conversely,
predicted to fall. Likewise, since the setting of threshold criteria for predicting
stock price fluctuation is crucial. The Figure 3 shows a flow diagram of the
learning experiment for predicting stock prices. The ratio of accurate predictions
to total predictions is defined as ‘prediction accuracy’.

Fig.3: Experiment model overview 
7 Text Opinion Mining to Analyze News for Stock Market Prediction
In addition, we tried to compare the news offered by two different media, and
measured the prediction accuracy of each media at the threshold.
4 Experiments and Results
4.1 Evaluation
The accuracy of opinion mining in new sentiment analysis can be evaluated using
statistical measures such as recall, precision, F1-score [7, 20, 11]. Accuracy,
which is defined as the percentage of sentiments correctly predicted, is one
method of evaluating. The quality of the results is also measured by comparing
two standard performance measures, namely, recall and precision. Recall is
defined as the proportion of positive sentiments which are correctly identified.
(2)
Precision is defined as the ratio between the numbers of correct sentiments
predicted to the total number of matches predicted:
(3)

But this would often decrease the precision of the result. In general, there is an
inverse relationship between recall and precision. An ideal learning model would
have both high recall and high precision. Sometimes, recall and precision are
combined together into a single number called F1, which represents a harmonic
mean of recall and precision:
(4)
4.2 Analyzing Sentiment
In previous works, the use of a domain specific dictionary rather than a general
dictionary when analyzing the sentiment of news using a sentimental word
dictionary showed greater accuracy of prediction [12]. That being the case, we
developed a stock market specific dictionary and evaluated the sentiment of the
words appearing in the news. In order to develop a stock domain specific
dictionary, we performed NLP of 78,216 news articles. At the first parsing, we
had over ten million words, but, after completing several pre-processing and
selection procedures, we finally chose seven hundred sentimental words. The 
Yoosin Kim et al. 8
selected words were calculated by a sentiment scoring formula. The Table 2 is a
part of the sentimental word dictionary.
Table 2: Sample of sentimental words
Neg-Word Score Pos-Word Score
더블딥(double dip) 0.167 경기부양(pumppriming)

0.700
동결(freeze) 0.246 신뢰(confidence) 0.692
방어(defense) 0.296 확충(expand) 0.677
급락장(slump) 0.333 호전(improve) 0.647
비교(compare) 0.341 정상(peak) 0.625
근거(basis) 0.365 합의(agree) 0.612
요구(require) 0.365 대장주(leading
stock)
0.608
기준금리(base rate) 0.365 강한(strong) 0.565
우려감(fear) 0.366 사자(buy) 0.563
낙폭(range of drop) 0.382 급등세(sudden rise) 0.563
If the sentiment score of a word is closer to 1.0, it means that the word is more
positive. Conversely, if a word is nearer to 0.0, it means the word is negative. As a
result, some rare words showed extremely negative or positive polarity, while
most words fell in the middle of the range, i.e. 0.3 ~ 0.7. Also, regarding the
sentiment of the words, there was a relatively equal distribution of both positive
and negative words. NewsOpn, i.e. the sentiment scores of news, was also
calculated using a sentiment scoring formula. DayOpn was same way. The Table
3 shows some of the results of the calculation of news and day opinions.
Table 3: Sample of sentiment scores
DATE NewsID NewsOpn DATE DayOpn
2011-01-04 640 0.3000 2011-01-13 0.1735
2011-01-04 652 0.1765 2011-01-14 0.1521
2011-01-04 661 0.1600 2011-01-15 0.1801
2011-01-04 663 0.1053 2011-01-16 0.1366
2011-01-04 667 0.3103 2011-01-17 0.1636
2011-01-04 668 0.2424 2011-01-18 0.1808
2011-01-04 669 0.1429 2011-01-19 0.1714
~ ~ ~ ~ ~
2011-07-30 43011 0.1500 2011-07-22 0.2022 
9 Text Opinion Mining to Analyze News for Stock Market Prediction
2011-07-30 43012 0.2353 2011-07-23 0.1250
2011-07-30 43013 0.0000 2011-07-24 0.1772
2011-07-31 43015 0.2319 2011-07-25 0.1905
2011-07-31 43016 0.1951 2011-08-26 0.2043
2011-07-31 43016 0.1860 2011-07-27 0.2334
DayOpn, Sentiment score of a day, was converted into stock price fluctuation
prediction variables. We tried to determine the threshold, the critical value with
the highest prediction accuracy by test.
4.3 Predicting Increases/Decreases in Stock Price
The aim of the experiments is to determine whether there is a correlation between
the news sentiment and the rise of fall of stock prices, and whether the results of
prediction differ according to each media. For the experiment, we separated three
groups, media H, M and H + M, from the data set. The Figure 4 shows the level of
prediction accuracy by the critical value of each type in the training data set.
In Figure 4, media M news’ opinions showed the best prediction accuracy of
65.2% at the critical value of 0.22. H news’ opinions, meanwhile, showed 60.3%
prediction accuracy at the critical value of 0.19. The difference in accuracy
between the two media is 5%, which means that M’s news, compared to H’s
news, could predict with greater accuracy any rises and falls in stock prices.
Actually, in terms of the content of the news articles, media M presents stock
market situations, prospects and so forth in a comparatively clear tone, while
media H presents more macroeconomic infrastructure and post-incident
evaluations.
Another interpretation of the difference is that the newer media (M) is an online
exclusive channel that focuses aggressively on stock market news coverage,
whereas the older player (M) focuses on the overall economic sector rather than
on presenting a specific area and predictions. On the other hand, M+H news’
opinions showed 60.1% prediction accuracy at the critical value of 0.11. It is
understood that the mixed analysis of news articles of the two somewhat
contrasting media made the classification of opinions unclear, thereby lowering
accuracy.
For the verification, thresholds drawn from the training data sets were applied to
the test data sets, and the prediction accuracy of each group was tested. The
Figure 4 shows the results of the experiment for the three groups in the test data
set. Media M news’ opinions, which showed the highest prediction accuracy in
the training data set, also showed the highest accuracy of 54.8% in the test data 
Yoosin Kim et al. 10
set. Media H news’ opinions showed a rate of accuracy of 52.3%, a lower figure
than that obtained by the training data set, while M+H news showed a very low
level of accuracy at just 48.1%.
Fig.4: Threshold and accuracy in the test data
While the above results are focused on determining the threshold and prediction
accuracy through the training and test data sets, the next thing that needs to be
examined is how the prediction was performed to assess the quality of prediction.
In order to assess the prediction quality, as mentioned in the evaluation, we
conducted F1, the harmonic mean of recall and precision. Generally, if the
prediction accuracy is higher, the result is better, and, when the level of accuracy
is identical, if the F1 score is higher, it shows a higher degree of predictive power.
In Table 4 each experimental group’s prediction accuracy and F1 featured in the
training data sets were evaluated as follows: media M news’ accuracy was the
highest, at 0.652, compared to the other experimental groups, and its F1 was also
the best at 0.626. This must mean that media M’s news has the best prediction
probability.
Table 4: Prediction quality in the training data
Training Data Max Critical Point Accuracy α F1Score
M 0.22 0.652 0.626
H 0.19 0.603 0.526
M+H 0.11 0.594 0.736
Then, we tested the prediction accuracy and quality of the test data-which
inherited threshold of the training data set. The result is shown in Table 5 where
media M’s news still displayed the highest prediction power. Although its 
11 Text Opinion Mining to Analyze News for Stock Market Prediction
accuracy was 0.10 points lower compared to the training data, it showed
prediction accuracy of 0.550 and an F1 of 0.630. This result means that rises and
falls in stock prices can be sufficiently predicted using news opinion mining.
Table 5: Prediction quality in the testing data
Training Data Max Critical Point Accuracy α F1Score
M 0.22 0.550 0.630
H 0.19 0.532 0.532
M+H 0.11 0.484 0.652
Ultimately, it showed that, although there are differences in the level of prediction
accuracy and quality according to each media’s opinion falls and rises in stock
prices could be predicted using news opinions; and that sentiment analysis and the
threshold, as learned through opinion mining, could be effectively used to predict
the actual stock market.
In summary, news articles have their own characteristics, depending on the media.
We showed that news articles are classified, opinions are extracted from there, an
appropriate critical value, threshold, is performed, and then stock up/down is
predicted. However, we should be aware that media’s characteristics can render
the result of opinion mining somewhat unclear and lower its predictive power.
5 Conclusion
This study, assuming that news and stock prices have a close correlation, sought
to find patterns in the new that could be useful in predicting positive and negative
fluctuations in stock prices. Many previous studies revealed that the news
influences stock prices, and a number of studies on stock price prediction have
been made using actual news articles. However, we have conducted a novel
attempt to compile a stock domain specific sentimental word dictionary from the
news as unstructured big data, to analyze ‘sentiment’ using that dictionary, and to
mine opinions in order to predict stock price fluctuations (i.e. up/down
movements).
As a result, we built a stock domain specific dictionary via the NLP of 78,216
news articles take from two different style media and showed the sentimental
words, news sentiments and opinions calculated using the dictionary. Then, by
conducting a stock prediction experiment, we found that the opinions extracted
from the news could be useful in accurately predicting stock market movements
and F1 features. Furthermore, we recognized that the media have their own
characteristics, so the accuracy of predicting stock market price movements could
differ depending on the media. 
Yoosin Kim et al. 12
However, there are still many areas in this study which could be investigated in
more detail and extended. The news sample of just one year used in this study
may not be large enough, and the use of two media, M and H, may also be
insufficient. It is particularly difficult to carry out NLP of Korean language when
building a sentimental word dictionary. The Korean language varies greatly in its
usages, and has various contextual meaning, and homonyms. Therefore, future
studies will have to gather longer periods of big data, analyze more diverse media
outlets, and reflect Korean linguistic characteristics more carefully. Furthermore,
if attributes such as market prospects, overseas news, and corporate performance
are classified, it will be possible to analyze its effects on stock prices and the
predictive power according to news types more closely.
References
[1] B. Pang and L. Lee, “Opinion Mining and Sentiment Analysis,”
Foundations and Trends in Information Retrieval, Vol.2, Nos.1–2, 2008,
pp.1–135.
[2] Fu, K. Lee, D. Sze, F. Chung and C. Ng, “Discovering the Correlation
between Stock Time Series and Financial News,” Proceedings of the 2008
IEEE/ WIC/ACM International Conference on Web Intelligence and
Intelligent Agent Technology, 2008, pp.880-883.
[3] H. Chen and D. Zimbra, “AI and Opinion mining,” IEEE Intelligent Systems,
May/June 2010, pp.74-80.
[4] J. Bollen, H. Mao and X. Zeng, “Twitter mood predicts the stock market,”
Journal of Computational Science, Vol.2, 2011, pp.1–8
[5] Kim, Y., N. Kim, S.R. Jeong, “Stock-index Invest Model Using News Big
Data Opinion Mining,” Journal of Intelligence and Information Systems,
Vol.18, No.2, 2012.6, pp.143-156.
[6] L. Zhuang, F. Jing and XY. Zhu, “Movie Review Mining and
Summarization,” Proceedings of the 15th ACM International Conference on
Information and Knowledge Management, November 2006, pp.43-50.
[7] M. A. Mittermayer and G. F. Knolmayer, “NewsCATS: A News
Categorization And Trading System,” Proceedings of the International
Conference in Data Mining, 2006.
[8] R. P. Schumaker and H. Chen, “Textual Analysis of Stock Market
Prediction Using Breaking Financial News: The AZFinText System,” ACM
Transactions on Information Systems, Vol.27, No.2, Article 12, February
2009.
[9] S. Ahn and S. B. Cho, “Stock Prediction Using News Text Mining and Time
Series Analysis,” Proceedings of the KIISE 2010 conference, Vol.37, No.1,
2010.6, pp.364-369.
[10] V. Sehgal and C. Song, “SOPS: Stock Prediction using Web Sentiment,”
Seventh IEEE International Conference on Data Mining – Workshops, 2009,
pp.21-26. 
13 Text Opinion Mining to Analyze News for Stock Market Prediction
[11] W. Paik, M. H. Kyoung, K. S. Min, H. R. Oh, C. Lim and M. S. Shin,
“Multi-stage News Classification System for Predicting Stock Price
Changes,” Journal of the Korea Society for Information Management,
Vol.24 No.2, 2007, pp.123-141.
[12] Yu, Y., Y. Kim, N. Kim, S.R. Jeong, “Predicting the Direction of the Stock
Index by Using a Domain-Specific Sentiment Dictionary,” Journal of
Intelligence and Information Systems, Vol.19, No.1, 2013.3, pp.92-110.
[13] Yune, H.,H. Kim, J.Y. Chang, “An Efficient Search Method of Product
Review using Opinion Mining Techniques,” Journal of KIISE: Computing
Practices and Letters, Vol.16, No.2, 2010.2, pp.222-226.

Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

343
OPINION MINING: APPROACHES, RESOURCES AND
CHALLENGES
1
 MAQBOOL AL-MAIMANI ,
 2 NAOMIE SALIM, 3
 AHMED M. AL-NAAMANY
1
IT Department, Oman Air, Muscat, Sultanate of Oman
2
Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, Johor Bahru,
Malaysia
3 Modern College of Business and Science, Muscat, Sultanate of Oman
E-mail: 1maqbool.almaimani@omanair.com ,
2
naomie@utm.my ,
3
naamany@mcbs.edu.om
ABSTRACT
Web2.0 has contributed tremendously towards the rapid growth of web contents which ensures that
customers utilize existing blogs, discussion forums, e-commerce and many other sites to express their
opinions and read reviews of other people on different products and services which they plan to procure.
Such an online wealth of information over the web has helped customers, firms, manufacturers, service
providers, social and government units to take proper decision to procure or enhance various products and
services. This practice has triggered the need to enhance existing methods and techniques to extract and
summarize opinions from different online reviews. This paper surveys recent and leading methods and
techniques that are used for opinion mining and then outlines available resources which have been
developed in this regard. The paper also presents few challenges and open issues that need to be addressed
and researched in more depth in order to improve the way opinions are extracted and summarized to users
and other interested groups.
Keywords: Mining; Sentiment Analysis; NLP; Objective; Subjective; Polarity; Items; Features.
1. BACKGROUND
In the era of Web 2.0, more and more people like
to express their views and read online user
reviews before they really buy a product /
service. The web offers many sites (like blogs,
discussion forums, e-commerce) to enable
people to access available on line reviews to take
proper decisions. In 2007 and 2008, statistics
showed that 81% of Internet users did online
research on a product at least once and between
73% and 87% report that reviews had a
significant influence on their purchase [1][2]. It
is estimated that 75,000 new blogs emerge daily
with 1.2 million new posts each day covering
many consumer opinions on products and
services. Finding and extracting such opinions is
very essential for various reasons [1][2]:
• To understand customers’ feelings and
opinions on a particular product/services
in order improve the quality and delivery
of such goods/services
• To scientifically record different opinions
and positions of people on a specific
event, accident, incident, occasion etc.
This covers areas like economical
changes, history, scientific explora-
-tions and many other day-to-day issues
in order to take proper measures and
required improvements.
• To improve social services provided to
public by governments and social
organizations by understanding people’s
demands and suggestions.
The paper consists of four main sections. Section
II presents few key concepts used in Opinion
Mining field. Section III discusses few leading
approaches and techniques used in extracting and
summarizing the opinions. Opinion Mining
available resources are outlined in section IV.
Challenges and few open issues of Opinion
Mining are briefly analyzed in section V.
2. KEY CONCEPTS
Opinion mining or sentiment analysis is a
process of a where computer tools and
techniques are to search for results on a given
item / service, generate a list of product attributes 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

344
(quality, features, etc.) and aggregate opinions
about each of them (poor, mixed, good) [2]. This
section highlights few important concepts related
Opinion Mining (OM).
• Objective and subjective information:
Facts are the objective information
representing truth details about different
things (i.e science, geography, history,
political science, space world etc). These
types of information are retrieved using
normal search engines like Google.
Subjective information, on the other hand
is people’s opinions and thoughts which
they express about a particular product,
service, event, situation, incident etc.
This type of information is difficult to
find and extract as one word in one
sentence can mean something while it
means something else in another. For
example, the sentence “This laptop has
long battery life” is different than “This
lecture is boring and it takes long time to
finish.”
• Opinions: an Opinion is a disputed view
or stand of someone about something.
Opinions can be of three types: 1)
Explicit opinions which are direct
opinions that are clearly expressed (e.g.
“this food is delicious”). 2) Implicit
opinions are opinions which can be
implied from text (e.g. “The camera
stopped working in two days”). 3)
Emotional Opinions are related to
people’s emotions like happiness,
sadness, humor, and anger etc. (e.g. I will
never see movies of this director again.”).
• Opinion Polarity: This refers to the
direction of the opinion or subjective
information and can be Positive,
Negative or Neutral. Words like
beautiful, wonderful, good and amazing
are positive; whereas, words like bad,
poor, terrible and loss are negative ones.
• Opinion mining process: Given an object
and a collection of reviews on it, the task
in opinion mining process usually
consists in general view of the following
tasks: 1) Identify and extract object
features that have been commented on in
each review, 2) Making hierarchy of
features, 3) Grouping synonyms of
features, 4) Sentiment Analysis:
Determining the orientation of opinion is
positive, negative, neutral, 5) Provide
summary of opinion in textual or in a
visualization way.
• Blogosphere: Blogosphere is the name
associated to universe of all the blog
sites. A blog is a website that allows
people to write various topics and
express their opinions about different
products and services [2].
3. OM APPROACHES AND TECHNIQUES
The mining process can be as simple as learning
polarity (positive or negative) and sentiment of
the words, or as complicated as performing deep
parsing of data to identify grammar and structure
of the sentences. Approaches and techniques for
opinion mining and sentiment analysis can be
classified into different ways. One classification
is based on manual and automated techniques. In
the manual-based approaches dictionaries and
lexicon based approaches are implemented. On
the dictionary-based methods, resources like
WordNet are used to find opinions from words
Synsets and hierarchies. Seeds are used to search
for synonyms and antonyms in WordNet [1][3].
The manual approach tends to find sentences,
phrases, words and patterns that express
subjectivity and the orientation of the
opinionated text. Words like “honest, important,
mature, large, patient” are used to express
positive opinions; whereas words like “harmful,
hypocritical, inefficient, insecure” are used to
express negative ones. Although the dictionarybased
approach will help to find many of such
words, but it does not help to find context
dependent opinion words like long, big, fat,
small etc. In the lexicon approaches, rule-based
methods are researched and implemented using
corpus and various available datasets. Such
approaches depend on syntactic rules and cooccurrence
patterns to be extracted from large
corpora. These approaches are used to find
domain dependant opinions. However, many
improvements are still required to advance such
rule-based approaches [4]. The other category of
OM techniques are machine-based methods.
Studies showed that standard machine learning
techniques outperform human-based approaches.
In the machine-based approaches, systems are
built and trained to categorize opinionated text
into positive, negative or neutral opinions using
supervised and unsupervised learning techniques
[2].
In a different classification, Yee et. al, presented
OM and sentiment analysis techniques by 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

345
opinion mining components. The techniques are
listed for different components of OM, covering
Items extraction methods, Features extraction
methods, Sentiment Classification techniques,
Strength of sentiments and Summarization of
opinion techniques. We believe the above is
more structured approach for presenting various
methods and techniques used for opinion mining
and hence it is followed in this paper.
Item is a subject matter on which the opinion is
expressed. Kobayashi [5] and Gamon [6]
proposed two frameworks for item extractions
which show significant improvements over other
methods used for item extractions. Kobayashi
and his team researched on a method of opinion
extraction which was based on a structured form.
In their study they concentrated on structuring
the opinions of the customers in an effective way
especially in connection to web documents
where the main focus was on extraction of the
subject/aspect evaluation relations, and
extracting subject/aspect-aspect relations, using a
machine learning-based method, which is
portable across domains. Their study addressed
the main area of opinion extraction where they
combined the contextual clues and the context
independent statistical clues with the help of a
machine-learning technique (‘boosting-based
algorithm’). Experiments were carried out and
evaluation was conducted using 5 fold cross
validation on all data in the aspects of recall and
precision [5]. The developed algorithm had its
own limitations and could not be used especially
when it comes to clustering techniques and
machine-learned sentiment classifier.
Feature extraction is a process that can be
performed after item extraction. This procedure
involves recognizing the features of products that
clients have indicated their opinions on through
evaluations and comments. Consider a digital
camera as an example. The camera has numerous
features, as highlighted in [26]; these features
include image quality, battery life, move,
dimension, and weight. For example, a digital
camera with poor image quality may have an
extremely long battery life or may be very light.
Feature sentiment can refer to an opinion on a
certain item based on its features. After features
are recognized, a feature sentiment can be
indicated for each feature, thus providing
information regarding the strengths and
weaknesses of the features of an item, such as
battery life, dimensions, and colors.
Under this platform, item features are believed to
be provided or determined before an opinion is
identified as beneficial or damaging. Previous
studies suggested using sentiment analysis
methods to categorize web forum opinions in
several languages. Feature sentiment approach
has two important steps: removing an original
group of features and executing selected features.
These measures are employed to perform
sentiment categorization of newsgroup
communications. The test creates an effect
similar to that of standard film review datasets.
The procedure targets file amount categorization
of sentiments.
Item Sentiment term denotes the total sentiments
being expressed about an object. For example, a
camera has positive and negative suggestions
from customers online. This item is extremely
useful when a prevalent opinion must be
recognized immediately. The majority of
literature is focused on discovering merchandise
sentiment. Scientists demonstrate significant
interest on this topic. In particular, Turney [32]
offered a commonly employed paradigm that
provides a foundation to remove the opinion on
an item. The conditions employed for the
evaluation are summarized and may be one of
three organizations, namely, positive, negative,
or neutral. Most studies have focused on
evaluating the subjectivity of negative and
positive terms to determine evaluation
sentiments while overlooking neutral terms.
Nevertheless, other scientists have asserted that
impartial (or objective) conditions should be
considered because such conditions may enhance
the precision of outcomes.
Feature comparison represents better granularity
of an opinion between two different entities,
such as AB cameras vs. XY cameras. AB
cameras generally have more favorable
evaluations than XY Cameras. This factor is
necessary for shoppers who may have enough
time to search for the best cost and merchandise
to purchase. Consumers can save much time in
decision-making processes with regard to
everyday concerns, thus creating a dual economy
involving cash and time. Grams, Wang, and
Araki [30] practiced a book method that
graphically shows comparisons. Several
improvements that occurred in this field include 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

346
graphically exhibiting each feature of an item.
This improvement obviously enhances
purchasing experience by exhibiting the pros and
cons of each feature. Moreover, manufacturer
can readily determine features that do not satisfy
customer expectations and demands [30][31].
Summarization Techniques
Summarization is the final and a very important
stage of the opinion mining process.
Summarization is required because one opinion
does not represent all opinions. There are many
systems used to summarize the extracted and
mined opinion. FBS and OPINS are two
excellent product review summarization system
[11][17]. In addition, there are many techniques
and approaches which were developed to
summarize customer reviews and opinions and
present them in textual and graphical formats to
enable customers and supplier take proper
actions.
Summarization technique was proposed in the
year 2009 by Hu and Wu [18] especially to
summarize the various reviews given by
customers for specific group of products. This
summarization technique was based on a set of
lists which indicated the pros and cons which
could be easily understood just by having a
glance over the lists. The main intention of the
researcher was to classify these pros and cons
into a list. In previous studies which typically
make use of classical summarization techniques
where phrases were used as a primary
component during the whole summarization
process. The main reason to use such technique
was to help the readers to understand, remember
and recognize the opinions. Apart from that the
summarization technique will help readers and
also the manufacturers to gather information of
the main flaws and improvement suggestions
related to their products which could be utilized
in the future.
With regard to the classification process, one
should make use of word weightage technique to
calculate the strength of each word towards both
opinions and word score in order to show the
word strength of expressing the sentiments
accordingly. Chi-square analysis technique can
be deployed in order to calculate the correlation
among the terms used in the review.
Apart from the chi-square analysis one may also
use frequency statistical analysis value for the
word-stem in order to weigh the strength of each
word towards the sentiment in positive
orientation and negative orientation. Using stem
but not the word itself is to make the result more
precise and comprehensive. The Word Weight is
not enough for describing the strength of a
word’s orientation, since the word’s linguistic
type itself plays an important role in expressing
the sentiment [2][4]. Other research found that
adverb and adjective are the core types to
empress human sentiment although verb can
show some polarized intention [20]. Based on
these findings, they used a score algorithm to
combine the linguistic feature of a word to its
weight value.
A significant component of feature-based
strategies is the ability to distinguish features of
various services and products. As opposed to the
related function presented in Hu et al. 26], Dork,
Lawrence, and Pennock [28] completed a
significant improvement in the sentiment
categorization of merchandise reviews in a
document. The objective of their research was to
categorize each evaluation document as
indicating a favorable or unfavorable sentiment
regarding a particular item.
Mishne [27] required a completely distinctive
strategy, which became a pioneer on OM books.
The study of Mishne was more concentrated on
categorizing blog articles according to diverse
emotions. Simply stated, these studies did not
address the task of feature extraction, but
continue using omitted features to help classify
blogs by emotions. These researchers considered
disposition classification to be beneficial to
numerous applications, including enhancing
physician–patient interaction and helping
behavioral scientists. Their objective was
determining the probable state of mind of the
reviewer once a post was composed by using a
developed learning method to recognize several
features that would be applied to the learning
procedure. Ding et al. [25] discussed the issue of
identifying the semantics of opinions on item
features in client reviews as opposed to on the
goods (things) mentioned in the reviews such as
in the studies of Esuli et al. [29]. The objective
of the work of Ding was using language rules.
4. BUILDING DATASETS, CORPUS AND
DICTIONARIES FOR OPINION
MINING
Corpus is the plural form for corpora which is
basically a collection of linguistic data in an
electronic format (in a Computer-readable text). 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

347
There are many efforts made by different
researches to build or refine corpus for Opinion
Mining. The following are few corpus and Data
sets in an Alphabetical order[2]:
• Cornell-movie-review-datasets:
http://www.cs.cornell.edu/people/pabo/m
ovie-review-data/. This dataset contains
[2]: 1) document-level: polarity dataset
v2.0: 1000 positive and 1000 negative
processed reviews 2)sentence-level:
sentence polarity dataset v1.0: 5331
positive and 5331 negative processed
sentences/snippets. 3) Subjectivity
dataset v1.0: 5000 subjective and 5000
objective processed sentences.
• Customer-review-datasets:
http://www.cs.uic.edu/∼liub/FBS/Custo
merReviewData.zip. “This dataset
consists of reviews of five electronics
products downloaded from Amazon and
Cnet. The sentences have been manually
labeled as to whether an opinion is
expressed, and if so, what feature from a
pre-defined list is being evaluated “[20].
• Multiple-aspect-restaurant-reviews:
http://people.csail.mit.edu/bsnyder/naacl0
7. “The corpus, introduced in Snyder and
Barzilay, consists of 4,488 reviews, both
in raw-text and in feature-vector form.
Each review gives an explicit 1-to-5
rating for five different aspects—food,
ambiance, service, value, and overall
experience “[2]
• NTCIR multilingual corpus: “The corpus
for the NTCIR 6 pilot task consists of
news articles in Japanese, Chinese, and
English and formed the basis of the
Opinion Analysis Task at NTCIR6. The
training data contains annotations
regarding opinion holders, the opinions
held by opinion holder, and sentiment
polarity, as well as relevance information
for a set of predetermined topics. The
corpus of the NTCIR Multilingual
Opinion-Analysis Task (MOAT) is
drawn from Japanese, Chinese, and
English blogs” [2].
5. CHALLENGES
Until today the field of opinion mining is not
well developed to provide user with a powerful
opinion and sentiment mining systems. This
section lists few challenges and open issues that
need to be addressed and researched in depth [2]
• The fact that product reviews are written
in different languages has created a
challenge in opinion mining. The main
problem becomes the time spent in
reviewing all available data and resolves
the language barrier. We need a
language independent method that
automatically analyze, extract and
assign values for a given product or
service [16].
• There is a need to develop powerful
techniques in mining emotional related
opinions like happiness, sadness, humor,
anger etc
• OM experiences numerous challenges,
including determining which section of
a text is an opinion, pinpointing the
opinion writer, determining the
beneficial or unfavorable power of an
opinion, and so on.
• Phrase file intricacy, contextual
emotions, heterogeneous files,
benchmark quality, and modal workers
remain difficult issues in this field.
• There is a need to develop opinion
mining search engines which extracts
subjective details from different reviews
of consumers. Furuse came up with a
search engine which extracts opinion
sentences based on open-domain query
from Japanese blog pages [22]. But this
is just very initial efforts. Existing
search engines are for fact searching.
How such a search engine would be
developed? Can we search for opinions
as conveniently as general Web search?
How the opinion search queries should
be formed? Finding the opinion of a
person or organization (opinion holder)
on a particular object or a feature of an
object is yet a challenge [2].
6. SUMMARY
Opinions are essential to any person who is
likely to make a choice. OM is effective for
individuals who need to purchase an item and are
able to choose which item to purchase by
studying opinions instead of lengthy product
reviews and producing outline their faces. OM is
equally essential to businesses by helping them
understand how clients regard their merchandise.
Consequently, businesses may make decisions
regarding their products based on the opinions of
clients. Companies may also modify their 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

348
merchandise in accordance with the opinions of
clients through an efficient and rapid method.
Therefore, businesses may develop better
customer relationships by providing their
requests and satisfying their demands.
Businesses can find, attract, and maintain clients,
thus possibly saving on manufacturing costs by
adopting insights from consumer demands.
To date, excellent advancements have been
achieved in this field, and the challenges
presented in this paper have been addressed by
numerous strategies. Approaches that can handle
the challenges of OM simultaneously are in
demand.
Moreover, the paper highlighted few important
resources and corpuses that are built for data
mining. The paper also presented many opening
areas that need to be addressed and researched to
enhance this field.
REFERENCES:
[1] Esuli A. & Sebastiani F., “Determining the
Semantic Orientation of Terms through
Gloss Classification”, in Proceedings of 14th
ACM International Conference on
Information and Knowledge Management,
CIKM 05, Bremen, DE, pp. 617-624, 2005.
[2] Pang B. and Lee L., “Using very simple
statistics for review search: An exploration,”
in Proceedings of the International
Conference on Computational Linguistics
(COLING), (Poster paper), 2008.
[3] Hu, M. & Liu, B. (2004), ‘Mining and
Summarizing Customer Reviews’, in
Proceedings of the 10th International
Conference on Knowledge Discovery and
Data Mining, KDD-04, Seattle, WA, pp.
168-177, 2004.
[4] Kanayama H. and Nasukawa T.,“Fully
automatic lexicon expansion for domainoriented
sentiment analysis,” EMNLP-06,
2006.
[5] Kobayashi N., Inui, K. & Matsumoto, Y.,
‘Opinion Mining from Web Documents:
Extraction and Structurization’, in
Proceedings of the Transactions of the
Japanese Society for Artificial Intelligence
22, JSAI07, pp. 326-337, 2007.
[6] Gamon, M., Aue, A., Corston-Oliver, S. &
Ringger, E.,”Pulse: Mining Customer
Opinions from Free Text”, in Proceedings of
the Natural Language Processing, Microsoft
Research, IDA, Redmond, WA, pp. 121-
132, 2005.
[7] Turney P., “Thumbs up or thumbs down?
semantic orientation applied to unsupervised
classification of reviews,” Procs. of the 40th
Annual Meeting of the Association for
Computational Linguistics, 2002.
[8] Kobayashi N., Iida, R., Inui, K. &
Matsumoto, Y. “Opinion Mining as
Extraction of Attribute-Value Relations”, in
Proceedings of the Nara Instutute of Science
and Technology, JSAI -2005, Takayama,
Ikoma, Japan, pp. 470-481, 2005.
[9] Mishne G.,“Experiments with Mood
Classification in Blog Posts”, in Proceedings
of the Stylistic Analysis of Text for
Information Access, Style, Amsterdam, The
Netherlands, 2005.
[10]Liu, B., Hu, M. & Cheng, J., “Opinion
Observer: Analyzing and Comparing
Opinions on the Web”, in Proceedings of the
International World Wide Web Conference
Committee, WWW05, Chiba, Japan, 2005.
[11]Sheng Feng, Ming Zhang, Yanxing Zhang,
Zhihong Deng, “Recommended or Not
Recommended? Review Classification
through Opinion Extraction”, 12th
International Asia-Pacific Web Conference,
pg: 350-352, 2010.
[12]Matthew Whitehead, Larry Yaeger,
“Building a General Purpose Cross-Domain
Sentiment Mining Model”, World Congress
on Computer Science and Information
Engineering, pg: 472-476, 2009.
[13]Pang B. and Lee L., “A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts,” in ACL,pp. 271–278, 2004.
[14]Ding X. & Liu, B., “The Utility of
Linguistic Rules in Opinion Mining”, in
Proceedings of the SIGIR 2007, SIGIR,
Amsterdam, The Netherlands, 2007.
[15]Liu B., “Web data mining; Exploring
hyperlinks, contents, and usage data,”
Opinion Mining. Springer,2006.
[16]Alexandra BALAHUR, Andrés
MONTOYO, “A Feature Dependent Method
for Opinion Mining and Classification”,
978-1-4244-2780-2/08, IEEE, 2008 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

349
[17]Zhuang L., Jing F., Zhu X., and Zhang L.,
“Movie review mining and summarization,”
in CIKM, pp. 43–50, 2006.
[18]Xinghua Hu, Bin Wu, “Classification and
Summarization of Pros and Cons for
Customer Reviews”, IEEE/WIC/ACM
International Conference on Web
Intelligence and Intelligent Agent
Technology – Workshops, 2009.
[19]Hu. X. and Wu B., “Automatic keyword
extraction using linguistic features,”
Proceedings of the 6th annual international
IEEE conference on Data Mining, 2006.
[20]Hatzivassiloglou V. and McKeown
K.,“Predicting the semantic orientation of
adjectives,” ACL-1997.
[21]Mohsen Jafari Asbagh, Mohsen Sayyadi,
and Hassan Abolhassani,“Blog
Summarization for Blog Mining”, Software
Engineering, Artificial Intelligence, SCI
209, pp. 157–167, Springer-Verlag Berlin
Heidelberg, Germany, 2009.
[22]O. Furuse, N. Hiroshima, S. Yamada and R.
Kataoka, “Opinion sentence search engine
on open-domain blog,” IJCAI, 2007,
2760T2765.
[23]Kobayashi, N., Inui, K. & Matsumoto, Y.,
‘Opinion Mining from Web Documents:
Extraction and Structurization’, in
Proceedings Of the Transactions of the
Japanese Society for Artificial Intelligence
22, JSAI07, 2007, pp. 326-337.
[24]G. Wang and K. Araki, "An Unsupervised
Opinion Mining Approach for Japanese
Weblog Reputation Information Using an
Improved SO-PMI Algorithm,"
IEICE TRANS. INF. & SYST, vol.
VOL.E91–D, pp. 1032-1041, 2008.
[25]Ding, X. & Liu, B., ‘The Utility of
Linguistic Rules in Opinion Mining’, in
Proceedings of the SIGIR 2007, SIGIR,
2007, Amsterdam, The Netherlands.
[26]Hu, M. & Liu, B., ‘Mining and
Summarizing Customer Reviews’, In
Proceedings of the 10th International
Conference on Knowledge Discovery and
Data Mining, 2004, KDD-04, Seattle, WA,
pp. 168- 177.
[27]Mishne, G., ‘Experiments with Mood
Classification in Blog Posts’, in Proceedings
of the Stylistic Analysis of Text for
Information Access, Style, 2005,
Amsterdam, The Netherlands
[28]Dave, K., Lawrence, S. & Pennock, D.M.,
‘Mining the Peanut Gallery: Opinion
extraction and Semantic Classification of
Product reviews’, in Proceedings of
the 13th International WorldWide Web
Conference, 2003, WWW03.
[29]Esuli, A. & Sebastiani, F., ‘Determining
Term Subjectivity and Term Orientation for
Opinion Mining’, in Proceedings of the
ACL-97, 35th Annual Meeting of the
Association for Computational Linguistics,
CL-06, 2006, Madrid, ES, pp. 174- 181
[30]G. Wang and K. Araki, "An Unsupervised
Opinion Mining Approach for Japanese
Weblog Reputation Information Using an
Improved SO-PMI Algorithm,"
IEICE TRANS. INF. & SYST, vol.
VOL.E91–D, pp. 1032-1041, 2008
[31]Liu, B., Hu, M. & Cheng, J., ‘Opinion
Observer: Analyzing and Comparing
Opinions on the Web’, in Proceedings of the
International World Wide Web Conference
Committee,WWW05, 2005, Chiba, Japan
[32]Turney P, "Thumbs Up or Thumbs Down?
Semantic Orientation Applied to
Unsupervised Classification of Reviews.,"
presented at In Proc. of the Meeting of the
Association for ComputationalLinguistics
(ACL’02), 2002.
[33]Haji Binali, Vidyasagar Potdar, Chen Wu,
“A State Of The Art Opinion Mining And
Its Application Domains”, Digital
Ecosystems and Business Intelligence
Institute, 2008.
[34]Abbasi, A., Chen, H., and Salem, A. (2008).
Sentiment Analysis in Multiple Languages:
Feature Selection for Opinion Classification
in Web Forums. ACM Trans. Inform. Syst.
26(3), Article 12, 34 pages.

Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

343
OPINION MINING: APPROACHES, RESOURCES AND
CHALLENGES
1
 MAQBOOL AL-MAIMANI ,
 2 NAOMIE SALIM, 3
 AHMED M. AL-NAAMANY
1
IT Department, Oman Air, Muscat, Sultanate of Oman
2
Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia, Johor Bahru,
Malaysia
3 Modern College of Business and Science, Muscat, Sultanate of Oman
E-mail: 1maqbool.almaimani@omanair.com ,
2
naomie@utm.my ,
3
naamany@mcbs.edu.om
ABSTRACT
Web2.0 has contributed tremendously towards the rapid growth of web contents which ensures that
customers utilize existing blogs, discussion forums, e-commerce and many other sites to express their
opinions and read reviews of other people on different products and services which they plan to procure.
Such an online wealth of information over the web has helped customers, firms, manufacturers, service
providers, social and government units to take proper decision to procure or enhance various products and
services. This practice has triggered the need to enhance existing methods and techniques to extract and
summarize opinions from different online reviews. This paper surveys recent and leading methods and
techniques that are used for opinion mining and then outlines available resources which have been
developed in this regard. The paper also presents few challenges and open issues that need to be addressed
and researched in more depth in order to improve the way opinions are extracted and summarized to users
and other interested groups.
Keywords: Mining; Sentiment Analysis; NLP; Objective; Subjective; Polarity; Items; Features.
1. BACKGROUND
In the era of Web 2.0, more and more people like
to express their views and read online user
reviews before they really buy a product /
service. The web offers many sites (like blogs,
discussion forums, e-commerce) to enable
people to access available on line reviews to take
proper decisions. In 2007 and 2008, statistics
showed that 81% of Internet users did online
research on a product at least once and between
73% and 87% report that reviews had a
significant influence on their purchase [1][2]. It
is estimated that 75,000 new blogs emerge daily
with 1.2 million new posts each day covering
many consumer opinions on products and
services. Finding and extracting such opinions is
very essential for various reasons [1][2]:
• To understand customers’ feelings and
opinions on a particular product/services
in order improve the quality and delivery
of such goods/services
• To scientifically record different opinions
and positions of people on a specific
event, accident, incident, occasion etc.
This covers areas like economical
changes, history, scientific explora-
-tions and many other day-to-day issues
in order to take proper measures and
required improvements.
• To improve social services provided to
public by governments and social
organizations by understanding people’s
demands and suggestions.
The paper consists of four main sections. Section
II presents few key concepts used in Opinion
Mining field. Section III discusses few leading
approaches and techniques used in extracting and
summarizing the opinions. Opinion Mining
available resources are outlined in section IV.
Challenges and few open issues of Opinion
Mining are briefly analyzed in section V.
2. KEY CONCEPTS
Opinion mining or sentiment analysis is a
process of a where computer tools and
techniques are to search for results on a given
item / service, generate a list of product attributes 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

344
(quality, features, etc.) and aggregate opinions
about each of them (poor, mixed, good) [2]. This
section highlights few important concepts related
Opinion Mining (OM).
• Objective and subjective information:
Facts are the objective information
representing truth details about different
things (i.e science, geography, history,
political science, space world etc). These
types of information are retrieved using
normal search engines like Google.
Subjective information, on the other hand
is people’s opinions and thoughts which
they express about a particular product,
service, event, situation, incident etc.
This type of information is difficult to
find and extract as one word in one
sentence can mean something while it
means something else in another. For
example, the sentence “This laptop has
long battery life” is different than “This
lecture is boring and it takes long time to
finish.”
• Opinions: an Opinion is a disputed view
or stand of someone about something.
Opinions can be of three types: 1)
Explicit opinions which are direct
opinions that are clearly expressed (e.g.
“this food is delicious”). 2) Implicit
opinions are opinions which can be
implied from text (e.g. “The camera
stopped working in two days”). 3)
Emotional Opinions are related to
people’s emotions like happiness,
sadness, humor, and anger etc. (e.g. I will
never see movies of this director again.”).
• Opinion Polarity: This refers to the
direction of the opinion or subjective
information and can be Positive,
Negative or Neutral. Words like
beautiful, wonderful, good and amazing
are positive; whereas, words like bad,
poor, terrible and loss are negative ones.
• Opinion mining process: Given an object
and a collection of reviews on it, the task
in opinion mining process usually
consists in general view of the following
tasks: 1) Identify and extract object
features that have been commented on in
each review, 2) Making hierarchy of
features, 3) Grouping synonyms of
features, 4) Sentiment Analysis:
Determining the orientation of opinion is
positive, negative, neutral, 5) Provide
summary of opinion in textual or in a
visualization way.
• Blogosphere: Blogosphere is the name
associated to universe of all the blog
sites. A blog is a website that allows
people to write various topics and
express their opinions about different
products and services [2].
3. OM APPROACHES AND TECHNIQUES
The mining process can be as simple as learning
polarity (positive or negative) and sentiment of
the words, or as complicated as performing deep
parsing of data to identify grammar and structure
of the sentences. Approaches and techniques for
opinion mining and sentiment analysis can be
classified into different ways. One classification
is based on manual and automated techniques. In
the manual-based approaches dictionaries and
lexicon based approaches are implemented. On
the dictionary-based methods, resources like
WordNet are used to find opinions from words
Synsets and hierarchies. Seeds are used to search
for synonyms and antonyms in WordNet [1][3].
The manual approach tends to find sentences,
phrases, words and patterns that express
subjectivity and the orientation of the
opinionated text. Words like “honest, important,
mature, large, patient” are used to express
positive opinions; whereas words like “harmful,
hypocritical, inefficient, insecure” are used to
express negative ones. Although the dictionarybased
approach will help to find many of such
words, but it does not help to find context
dependent opinion words like long, big, fat,
small etc. In the lexicon approaches, rule-based
methods are researched and implemented using
corpus and various available datasets. Such
approaches depend on syntactic rules and cooccurrence
patterns to be extracted from large
corpora. These approaches are used to find
domain dependant opinions. However, many
improvements are still required to advance such
rule-based approaches [4]. The other category of
OM techniques are machine-based methods.
Studies showed that standard machine learning
techniques outperform human-based approaches.
In the machine-based approaches, systems are
built and trained to categorize opinionated text
into positive, negative or neutral opinions using
supervised and unsupervised learning techniques
[2].
In a different classification, Yee et. al, presented
OM and sentiment analysis techniques by 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

345
opinion mining components. The techniques are
listed for different components of OM, covering
Items extraction methods, Features extraction
methods, Sentiment Classification techniques,
Strength of sentiments and Summarization of
opinion techniques. We believe the above is
more structured approach for presenting various
methods and techniques used for opinion mining
and hence it is followed in this paper.
Item is a subject matter on which the opinion is
expressed. Kobayashi [5] and Gamon [6]
proposed two frameworks for item extractions
which show significant improvements over other
methods used for item extractions. Kobayashi
and his team researched on a method of opinion
extraction which was based on a structured form.
In their study they concentrated on structuring
the opinions of the customers in an effective way
especially in connection to web documents
where the main focus was on extraction of the
subject/aspect evaluation relations, and
extracting subject/aspect-aspect relations, using a
machine learning-based method, which is
portable across domains. Their study addressed
the main area of opinion extraction where they
combined the contextual clues and the context
independent statistical clues with the help of a
machine-learning technique (‘boosting-based
algorithm’). Experiments were carried out and
evaluation was conducted using 5 fold cross
validation on all data in the aspects of recall and
precision [5]. The developed algorithm had its
own limitations and could not be used especially
when it comes to clustering techniques and
machine-learned sentiment classifier.
Feature extraction is a process that can be
performed after item extraction. This procedure
involves recognizing the features of products that
clients have indicated their opinions on through
evaluations and comments. Consider a digital
camera as an example. The camera has numerous
features, as highlighted in [26]; these features
include image quality, battery life, move,
dimension, and weight. For example, a digital
camera with poor image quality may have an
extremely long battery life or may be very light.
Feature sentiment can refer to an opinion on a
certain item based on its features. After features
are recognized, a feature sentiment can be
indicated for each feature, thus providing
information regarding the strengths and
weaknesses of the features of an item, such as
battery life, dimensions, and colors.
Under this platform, item features are believed to
be provided or determined before an opinion is
identified as beneficial or damaging. Previous
studies suggested using sentiment analysis
methods to categorize web forum opinions in
several languages. Feature sentiment approach
has two important steps: removing an original
group of features and executing selected features.
These measures are employed to perform
sentiment categorization of newsgroup
communications. The test creates an effect
similar to that of standard film review datasets.
The procedure targets file amount categorization
of sentiments.
Item Sentiment term denotes the total sentiments
being expressed about an object. For example, a
camera has positive and negative suggestions
from customers online. This item is extremely
useful when a prevalent opinion must be
recognized immediately. The majority of
literature is focused on discovering merchandise
sentiment. Scientists demonstrate significant
interest on this topic. In particular, Turney [32]
offered a commonly employed paradigm that
provides a foundation to remove the opinion on
an item. The conditions employed for the
evaluation are summarized and may be one of
three organizations, namely, positive, negative,
or neutral. Most studies have focused on
evaluating the subjectivity of negative and
positive terms to determine evaluation
sentiments while overlooking neutral terms.
Nevertheless, other scientists have asserted that
impartial (or objective) conditions should be
considered because such conditions may enhance
the precision of outcomes.
Feature comparison represents better granularity
of an opinion between two different entities,
such as AB cameras vs. XY cameras. AB
cameras generally have more favorable
evaluations than XY Cameras. This factor is
necessary for shoppers who may have enough
time to search for the best cost and merchandise
to purchase. Consumers can save much time in
decision-making processes with regard to
everyday concerns, thus creating a dual economy
involving cash and time. Grams, Wang, and
Araki [30] practiced a book method that
graphically shows comparisons. Several
improvements that occurred in this field include 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

346
graphically exhibiting each feature of an item.
This improvement obviously enhances
purchasing experience by exhibiting the pros and
cons of each feature. Moreover, manufacturer
can readily determine features that do not satisfy
customer expectations and demands [30][31].
Summarization Techniques
Summarization is the final and a very important
stage of the opinion mining process.
Summarization is required because one opinion
does not represent all opinions. There are many
systems used to summarize the extracted and
mined opinion. FBS and OPINS are two
excellent product review summarization system
[11][17]. In addition, there are many techniques
and approaches which were developed to
summarize customer reviews and opinions and
present them in textual and graphical formats to
enable customers and supplier take proper
actions.
Summarization technique was proposed in the
year 2009 by Hu and Wu [18] especially to
summarize the various reviews given by
customers for specific group of products. This
summarization technique was based on a set of
lists which indicated the pros and cons which
could be easily understood just by having a
glance over the lists. The main intention of the
researcher was to classify these pros and cons
into a list. In previous studies which typically
make use of classical summarization techniques
where phrases were used as a primary
component during the whole summarization
process. The main reason to use such technique
was to help the readers to understand, remember
and recognize the opinions. Apart from that the
summarization technique will help readers and
also the manufacturers to gather information of
the main flaws and improvement suggestions
related to their products which could be utilized
in the future.
With regard to the classification process, one
should make use of word weightage technique to
calculate the strength of each word towards both
opinions and word score in order to show the
word strength of expressing the sentiments
accordingly. Chi-square analysis technique can
be deployed in order to calculate the correlation
among the terms used in the review.
Apart from the chi-square analysis one may also
use frequency statistical analysis value for the
word-stem in order to weigh the strength of each
word towards the sentiment in positive
orientation and negative orientation. Using stem
but not the word itself is to make the result more
precise and comprehensive. The Word Weight is
not enough for describing the strength of a
word’s orientation, since the word’s linguistic
type itself plays an important role in expressing
the sentiment [2][4]. Other research found that
adverb and adjective are the core types to
empress human sentiment although verb can
show some polarized intention [20]. Based on
these findings, they used a score algorithm to
combine the linguistic feature of a word to its
weight value.
A significant component of feature-based
strategies is the ability to distinguish features of
various services and products. As opposed to the
related function presented in Hu et al. 26], Dork,
Lawrence, and Pennock [28] completed a
significant improvement in the sentiment
categorization of merchandise reviews in a
document. The objective of their research was to
categorize each evaluation document as
indicating a favorable or unfavorable sentiment
regarding a particular item.
Mishne [27] required a completely distinctive
strategy, which became a pioneer on OM books.
The study of Mishne was more concentrated on
categorizing blog articles according to diverse
emotions. Simply stated, these studies did not
address the task of feature extraction, but
continue using omitted features to help classify
blogs by emotions. These researchers considered
disposition classification to be beneficial to
numerous applications, including enhancing
physician–patient interaction and helping
behavioral scientists. Their objective was
determining the probable state of mind of the
reviewer once a post was composed by using a
developed learning method to recognize several
features that would be applied to the learning
procedure. Ding et al. [25] discussed the issue of
identifying the semantics of opinions on item
features in client reviews as opposed to on the
goods (things) mentioned in the reviews such as
in the studies of Esuli et al. [29]. The objective
of the work of Ding was using language rules.
4. BUILDING DATASETS, CORPUS AND
DICTIONARIES FOR OPINION
MINING
Corpus is the plural form for corpora which is
basically a collection of linguistic data in an
electronic format (in a Computer-readable text). 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

347
There are many efforts made by different
researches to build or refine corpus for Opinion
Mining. The following are few corpus and Data
sets in an Alphabetical order[2]:
• Cornell-movie-review-datasets:
http://www.cs.cornell.edu/people/pabo/m
ovie-review-data/. This dataset contains
[2]: 1) document-level: polarity dataset
v2.0: 1000 positive and 1000 negative
processed reviews 2)sentence-level:
sentence polarity dataset v1.0: 5331
positive and 5331 negative processed
sentences/snippets. 3) Subjectivity
dataset v1.0: 5000 subjective and 5000
objective processed sentences.
• Customer-review-datasets:
http://www.cs.uic.edu/∼liub/FBS/Custo
merReviewData.zip. “This dataset
consists of reviews of five electronics
products downloaded from Amazon and
Cnet. The sentences have been manually
labeled as to whether an opinion is
expressed, and if so, what feature from a
pre-defined list is being evaluated “[20].
• Multiple-aspect-restaurant-reviews:
http://people.csail.mit.edu/bsnyder/naacl0
7. “The corpus, introduced in Snyder and
Barzilay, consists of 4,488 reviews, both
in raw-text and in feature-vector form.
Each review gives an explicit 1-to-5
rating for five different aspects—food,
ambiance, service, value, and overall
experience “[2]
• NTCIR multilingual corpus: “The corpus
for the NTCIR 6 pilot task consists of
news articles in Japanese, Chinese, and
English and formed the basis of the
Opinion Analysis Task at NTCIR6. The
training data contains annotations
regarding opinion holders, the opinions
held by opinion holder, and sentiment
polarity, as well as relevance information
for a set of predetermined topics. The
corpus of the NTCIR Multilingual
Opinion-Analysis Task (MOAT) is
drawn from Japanese, Chinese, and
English blogs” [2].
5. CHALLENGES
Until today the field of opinion mining is not
well developed to provide user with a powerful
opinion and sentiment mining systems. This
section lists few challenges and open issues that
need to be addressed and researched in depth [2]
• The fact that product reviews are written
in different languages has created a
challenge in opinion mining. The main
problem becomes the time spent in
reviewing all available data and resolves
the language barrier. We need a
language independent method that
automatically analyze, extract and
assign values for a given product or
service [16].
• There is a need to develop powerful
techniques in mining emotional related
opinions like happiness, sadness, humor,
anger etc
• OM experiences numerous challenges,
including determining which section of
a text is an opinion, pinpointing the
opinion writer, determining the
beneficial or unfavorable power of an
opinion, and so on.
• Phrase file intricacy, contextual
emotions, heterogeneous files,
benchmark quality, and modal workers
remain difficult issues in this field.
• There is a need to develop opinion
mining search engines which extracts
subjective details from different reviews
of consumers. Furuse came up with a
search engine which extracts opinion
sentences based on open-domain query
from Japanese blog pages [22]. But this
is just very initial efforts. Existing
search engines are for fact searching.
How such a search engine would be
developed? Can we search for opinions
as conveniently as general Web search?
How the opinion search queries should
be formed? Finding the opinion of a
person or organization (opinion holder)
on a particular object or a feature of an
object is yet a challenge [2].
6. SUMMARY
Opinions are essential to any person who is
likely to make a choice. OM is effective for
individuals who need to purchase an item and are
able to choose which item to purchase by
studying opinions instead of lengthy product
reviews and producing outline their faces. OM is
equally essential to businesses by helping them
understand how clients regard their merchandise.
Consequently, businesses may make decisions
regarding their products based on the opinions of
clients. Companies may also modify their 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

348
merchandise in accordance with the opinions of
clients through an efficient and rapid method.
Therefore, businesses may develop better
customer relationships by providing their
requests and satisfying their demands.
Businesses can find, attract, and maintain clients,
thus possibly saving on manufacturing costs by
adopting insights from consumer demands.
To date, excellent advancements have been
achieved in this field, and the challenges
presented in this paper have been addressed by
numerous strategies. Approaches that can handle
the challenges of OM simultaneously are in
demand.
Moreover, the paper highlighted few important
resources and corpuses that are built for data
mining. The paper also presented many opening
areas that need to be addressed and researched to
enhance this field.
REFERENCES:
[1] Esuli A. & Sebastiani F., “Determining the
Semantic Orientation of Terms through
Gloss Classification”, in Proceedings of 14th
ACM International Conference on
Information and Knowledge Management,
CIKM 05, Bremen, DE, pp. 617-624, 2005.
[2] Pang B. and Lee L., “Using very simple
statistics for review search: An exploration,”
in Proceedings of the International
Conference on Computational Linguistics
(COLING), (Poster paper), 2008.
[3] Hu, M. & Liu, B. (2004), ‘Mining and
Summarizing Customer Reviews’, in
Proceedings of the 10th International
Conference on Knowledge Discovery and
Data Mining, KDD-04, Seattle, WA, pp.
168-177, 2004.
[4] Kanayama H. and Nasukawa T.,“Fully
automatic lexicon expansion for domainoriented
sentiment analysis,” EMNLP-06,
2006.
[5] Kobayashi N., Inui, K. & Matsumoto, Y.,
‘Opinion Mining from Web Documents:
Extraction and Structurization’, in
Proceedings of the Transactions of the
Japanese Society for Artificial Intelligence
22, JSAI07, pp. 326-337, 2007.
[6] Gamon, M., Aue, A., Corston-Oliver, S. &
Ringger, E.,”Pulse: Mining Customer
Opinions from Free Text”, in Proceedings of
the Natural Language Processing, Microsoft
Research, IDA, Redmond, WA, pp. 121-
132, 2005.
[7] Turney P., “Thumbs up or thumbs down?
semantic orientation applied to unsupervised
classification of reviews,” Procs. of the 40th
Annual Meeting of the Association for
Computational Linguistics, 2002.
[8] Kobayashi N., Iida, R., Inui, K. &
Matsumoto, Y. “Opinion Mining as
Extraction of Attribute-Value Relations”, in
Proceedings of the Nara Instutute of Science
and Technology, JSAI -2005, Takayama,
Ikoma, Japan, pp. 470-481, 2005.
[9] Mishne G.,“Experiments with Mood
Classification in Blog Posts”, in Proceedings
of the Stylistic Analysis of Text for
Information Access, Style, Amsterdam, The
Netherlands, 2005.
[10]Liu, B., Hu, M. & Cheng, J., “Opinion
Observer: Analyzing and Comparing
Opinions on the Web”, in Proceedings of the
International World Wide Web Conference
Committee, WWW05, Chiba, Japan, 2005.
[11]Sheng Feng, Ming Zhang, Yanxing Zhang,
Zhihong Deng, “Recommended or Not
Recommended? Review Classification
through Opinion Extraction”, 12th
International Asia-Pacific Web Conference,
pg: 350-352, 2010.
[12]Matthew Whitehead, Larry Yaeger,
“Building a General Purpose Cross-Domain
Sentiment Mining Model”, World Congress
on Computer Science and Information
Engineering, pg: 472-476, 2009.
[13]Pang B. and Lee L., “A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts,” in ACL,pp. 271–278, 2004.
[14]Ding X. & Liu, B., “The Utility of
Linguistic Rules in Opinion Mining”, in
Proceedings of the SIGIR 2007, SIGIR,
Amsterdam, The Netherlands, 2007.
[15]Liu B., “Web data mining; Exploring
hyperlinks, contents, and usage data,”
Opinion Mining. Springer,2006.
[16]Alexandra BALAHUR, Andrés
MONTOYO, “A Feature Dependent Method
for Opinion Mining and Classification”,
978-1-4244-2780-2/08, IEEE, 2008 
Journal of Theoretical and Applied Information Technology
 20th May 2014. Vol. 63 No.2
© 2005 - 2014 JATIT & LLS. All rights reserved.
ISSN: 1992-8645 www.jatit.org E-ISSN: 1817-3195

349
[17]Zhuang L., Jing F., Zhu X., and Zhang L.,
“Movie review mining and summarization,”
in CIKM, pp. 43–50, 2006.
[18]Xinghua Hu, Bin Wu, “Classification and
Summarization of Pros and Cons for
Customer Reviews”, IEEE/WIC/ACM
International Conference on Web
Intelligence and Intelligent Agent
Technology – Workshops, 2009.
[19]Hu. X. and Wu B., “Automatic keyword
extraction using linguistic features,”
Proceedings of the 6th annual international
IEEE conference on Data Mining, 2006.
[20]Hatzivassiloglou V. and McKeown
K.,“Predicting the semantic orientation of
adjectives,” ACL-1997.
[21]Mohsen Jafari Asbagh, Mohsen Sayyadi,
and Hassan Abolhassani,“Blog
Summarization for Blog Mining”, Software
Engineering, Artificial Intelligence, SCI
209, pp. 157–167, Springer-Verlag Berlin
Heidelberg, Germany, 2009.
[22]O. Furuse, N. Hiroshima, S. Yamada and R.
Kataoka, “Opinion sentence search engine
on open-domain blog,” IJCAI, 2007,
2760T2765.
[23]Kobayashi, N., Inui, K. & Matsumoto, Y.,
‘Opinion Mining from Web Documents:
Extraction and Structurization’, in
Proceedings Of the Transactions of the
Japanese Society for Artificial Intelligence
22, JSAI07, 2007, pp. 326-337.
[24]G. Wang and K. Araki, "An Unsupervised
Opinion Mining Approach for Japanese
Weblog Reputation Information Using an
Improved SO-PMI Algorithm,"
IEICE TRANS. INF. & SYST, vol.
VOL.E91–D, pp. 1032-1041, 2008.
[25]Ding, X. & Liu, B., ‘The Utility of
Linguistic Rules in Opinion Mining’, in
Proceedings of the SIGIR 2007, SIGIR,
2007, Amsterdam, The Netherlands.
[26]Hu, M. & Liu, B., ‘Mining and
Summarizing Customer Reviews’, In
Proceedings of the 10th International
Conference on Knowledge Discovery and
Data Mining, 2004, KDD-04, Seattle, WA,
pp. 168- 177.
[27]Mishne, G., ‘Experiments with Mood
Classification in Blog Posts’, in Proceedings
of the Stylistic Analysis of Text for
Information Access, Style, 2005,
Amsterdam, The Netherlands
[28]Dave, K., Lawrence, S. & Pennock, D.M.,
‘Mining the Peanut Gallery: Opinion
extraction and Semantic Classification of
Product reviews’, in Proceedings of
the 13th International WorldWide Web
Conference, 2003, WWW03.
[29]Esuli, A. & Sebastiani, F., ‘Determining
Term Subjectivity and Term Orientation for
Opinion Mining’, in Proceedings of the
ACL-97, 35th Annual Meeting of the
Association for Computational Linguistics,
CL-06, 2006, Madrid, ES, pp. 174- 181
[30]G. Wang and K. Araki, "An Unsupervised
Opinion Mining Approach for Japanese
Weblog Reputation Information Using an
Improved SO-PMI Algorithm,"
IEICE TRANS. INF. & SYST, vol.
VOL.E91–D, pp. 1032-1041, 2008
[31]Liu, B., Hu, M. & Cheng, J., ‘Opinion
Observer: Analyzing and Comparing
Opinions on the Web’, in Proceedings of the
International World Wide Web Conference
Committee,WWW05, 2005, Chiba, Japan
[32]Turney P, "Thumbs Up or Thumbs Down?
Semantic Orientation Applied to
Unsupervised Classification of Reviews.,"
presented at In Proc. of the Meeting of the
Association for ComputationalLinguistics
(ACL’02), 2002.
[33]Haji Binali, Vidyasagar Potdar, Chen Wu,
“A State Of The Art Opinion Mining And
Its Application Domains”, Digital
Ecosystems and Business Intelligence
Institute, 2008.
[34]Abbasi, A., Chen, H., and Salem, A. (2008).
Sentiment Analysis in Multiple Languages:
Feature Selection for Opinion Classification
in Web Forums. ACM Trans. Inform. Syst.
26(3), Article 12, 34 pages.

TweetAlert: Semantic Analytics in Social Networks for
Citizen Opinion Mining in the City of the Future
Julio Villena-Román1,2
, Adrián Luna-Cobos1,3
,
José Carlos González-Cristóbal3,1
1 DAEDALUS - Data, Decisions and Language, S.A.
2 Universidad Carlos III de Madrid
3 Universidad Politécnica de Madrid
{jvillena,aluna}@daedalus.es, josecarlos.gonzalez@upm.es
Abstract. In this paper a highly configurable, real-time analysis system to automatically
record, analyze and visualize high level aggregated information of
user interventions in Twitter is described. The system is designed to provide
public entities with a powerful tool to rapidly and easily understand what the
citizen behavior trends are, what their opinion about city services, events, etc.
is, and also may used as a primary alert system that may improve the efficiency
of emergency systems. The citizen is here observed as a proactive city sensor
capable of generating huge amounts of very rich, high-level and valuable data
through social media platforms, which, after properly processed, summarized
and annotated, allows city administrators to better understand citizen necessities.
The architecture and component blocks are described and some key details
of the design, implementation and scenarios of application are discussed.
Keywords: Semantic analytics, social networks, citizen, opinion, topics, classification,
ontology, events, alerts, big data, city console.
1 Introduction
With the recent success and proliferation of mobile devices, the democratization of
Internet accessibility and the possibility of meta-information, such as user location,
user profile and demographics, etc., the vastly amount of data that is being generated
has very rapidly grown. This unstructured source of data is already being used in multiple
fields like sociology, advertising, etc. and may also be used to improve public
administration services and functionality, as a new version of e-Gov application. User
interventions in social networks often contains agreement, disagreement or comments
about city services, city administrators, events in the city, etc. However, these data are
not really useful unless some semantic processing or data mining technique is applied
in order to automatically distinguish between relevant and not relevant information
and provide a higher level of abstraction.
This work has been developed in the framework of Ciudad 2020 [1] Spanish national
R&D project, which aims to achieve improvements in areas such as energetic
efficiency, Internet of the Future, Internet of Things, human behaviour, environmental 
sustainability and mobility and transport, in order to design the City of the Future.
The project proposes a new city model designed for the citizen –ad civitates civis–
that aims to include citizen reality into the city decisions.
Usually, the final objective of the government decisions is the citizen welfare.
However, it is not always an easy task for the administration services to quickly identify
the most important facts that their citizens are facing, to correctly scale them regarding
their relative importance according to what citizens think about them, or just
to be quick enough to recognize recent issues that may suddenly appear. In such cases,
citizen opinion mining will be a key factor to identify and later solve such concerns.
Therefore, the citizen is observed here from a dual point of view: on the one
hand as the main user of the services that the city offers, and on the other hand, as a
proactive city sensor capable of generating huge amounts of data through social media
platforms. The citizen sensor is an innovative way to capture high-level heterogeneous
information, very descriptive and with great value, especially when considering
aggregations. If the city administrators get to properly analyze such vast amount of
data coming from Social Media, they will be able to better know trends, generate
hypotheses over urban behaviour models in order to improve municipal management
policies, bringing them closer to the actual reality of the citizens, thus, turning them
into real actors within management mechanisms of smart cities.
In such process of data understanding and mining, technologies to analyze natural
language allow to semantically analyze citizen interventions in social media such as
Twitter. The aim of our system is to provide city promoters with a powerful tool to
rapidly and easily understand what the citizen behavior trends are, what their opinion
about city services, events, etc. is, and finally to provide them a primary alert system
that may improve the efficiency of emergency systems. In the same way, but applied
to a smaller scale as what we propose here, the system could be used to track public
services Twitter profiles’ and collect user opinion about e.g., e-Gov sites or applications,
allowing them to act more quickly to possible lacks of usability, services failures,
etc.
The rest of the paper presents the system description and architecture, and further
explores the details of each block that composes the system. Finally, a discussion and
future work section with insights to improve the system currently in the development
branch are presented.
2 System Architecture
We present a highly configurable, real-time analysis system to automatically record,
analyze and visualize high level aggregated information of user interventions in Twitter
that may be used by public entities to better understand citizen necessities. The
system is composed by four main components, shown in Figure 1.
The central component is the datawarehouse, the core information repository that
is able to store the high volume of data that the system manages and also provides
advanced search functionality to be able to exploit the information. The system is
based on Elasticsearch [2], which is a flexible and powerful open source, distributed, 
real-time search and analytics engine. Its distributed capabilities and the fact that it
scales very good when the system grows were key factors in the selection of this architecture.
Elasticsearch runs on top of Apache Lucene, so it offers quite complex
search capabilities and a scalable and high-performance environment.
Fig. 1. System architecture
The second component is composed by a set of concurrent gatherer processes,
which query the Twitter APIs [3] to collect tweets regarding to certain filters. The
configuration file defines the query parameters to the Twitter streaming API, allowing
to filter tweets by a list of user identifiers, a list of keywords to track (terms, hashtags)
and/or a set of geographical bounding boxes to restrict the search.
The third component is composed of a set of concurrent inquirer processes, whose
task is to annotate the messages using several of our Textalytics Core APIs [4]. The
system is deployed to use the text classification API using two specific models specially
designed for this business case (SocialMedia and CitizenSensor, described later),
the topics extraction API, which extracts topics such as entities, concepts, money,
URI expressions, etc., the sentiment analysis API, which extracts sentiment polarity
and also subjectivity and irony indications, and finally, the user demographics API,
which currently returns the gender, age and type of the author of the tweet.
Specifically, for each tweet, the system tries to identify the thematic area of the
message (energy, transport, economy, politics, social interests…), concepts mentioned
(city services, weather...), events to which the text refers (cultural events, soccer
matches...), special alert situations (road accidents, fires, street violence, security issues…),
and the specific location of the user (a building, means of transport...). This
analysis is complemented by an analysis of the sentiment polarity of the message:
very positive, positive, negative, very negative and neutral.
An example of an annotated tweet is shown in Figure 2, where a Twitter user alerts
from a crash in a public tunnel of the city of Madrid that needed of the presence of the
firemen. The system correctly detects that the issue is located in a public road, classifies
the message in the topic of Security in the Citizen Sensor ontology (under Concepts>Services>Security)
and as Disasters and accidents in the general Social Media
ontology. Furthermore, it finds out that the entity Calderón (soccer stadium nearby)
appears in the sentence and also several concepts: accident, tunnel, closed, firefighter, 
exit, lane, etc. Finally, it detects that it is an objective, non ironic comment with negative
polarity written out by a male aged in the range of 35 to 65 years.
Fig. 2. Example of a tweet annotated by the system
The semantic annotation task is the highest time consuming task and constitutes
most of the times the bottleneck of the system. The inquirer processes annotate the
unprocessed messages in descending order of insertion time, so that the most recent
information is available first to be able to react to early alerts. If the input rate of messages
being indexed in the system is higher than the multi-threaded annotation rate, it
is still not possible to access the high-level annotations on real time, but once this
peak situation is reversed and the system manages to annotate at a higher rate than the
indexing of new documents, it will start annotating the rest unprocessed documents.
Finally, the visualization component is used to exploit the annotated data. Several
widgets have been developed to present the data, either just for query and reporting or
also for data analytics purposes. These visualization modules can be specifically
adapted to better match the city needs.
The datawarehouse and the gatherers are obviously language independent, but the
inquirer components are strongly dependent on language lexicons and models. Although
the text classification engine is itself language independent, classification models
(consisting of training text and rules) are developed for a specific language. The
topic extraction engine relies on Part-of-Speech and parsing modules specifically
designed to build a sentence syntactic tree in a given language. Moreover, the sentiment
analysis engine makes use of that syntactic tree and also depends on a lexicon
containing polarity units and modifiers for a given language. The user demographics
engine is the only module where no information in a given language is used for creating
the model. Our initial business case is deployed to analyze data in Spanish, but
modules exist for other languages: English, French, Italian, Portuguese and Catalan.
3 Semantic Annotation
Much effort has been invested in the semantic annotation task, specifically focusing
on this scenario, tuned to properly deal with the special singularities of this kind of
text snippets (tweets) that usually contain misspellings, emoticons, typographic symbols,
letter/number homophones, shortenings, contractions, etc.
The inquirer provides several levels of analyses to classify the text with respect to
several (customizable) categories of specifically-defined ontologies, identify topics,
perform a demographics analysis to get the user age range, gender and whether he/she
is a person or an organization, and sentiment analysis of polarity and subjectivity.
All modules have been exhaustively tested and successfully evaluated in various
scenarios, both separately and also integrating two or more modules, in actual systems
currently in production, and also in different national and international evaluation
workshops such as SEPLN [5], CLEF [6] [7], NTCIR [8] and SemEval [9].
3.1 Text Classification
Another semantic annotation dimension is obtained with an automatic text classification
[10] according to pre-established categories defined in a model. The algorithm
used [11] [12] combines statistical classification with rule-based filtering, which allows
to obtain a high degree of precision for very different environments. Two ontologies
were specially designed for this system including concepts and situations that
we find relevant to this particular problem; however, the system allows building particular
ontologies and classification models for each scenario.
The Social Media ontology defines the general topic classification of the tweet, and
contains the first-level categories shown in Figure 3a. The Citizen Sensor ontology,
shown in Figure 3b, focuses on features considering the citizen as a sensor.
Fig. 3. a) Social Media ontology; b) Citizen Sensor ontology (1st and 2nd level categories)
3.2 Topics Extraction
Topics extraction process is carried out by combining a number of complex natural
language processing techniques that allow obtaining morphological, syntactic and
semantic analyses of a text and using them to identify different types of significant
elements. In short, the text is first divided into paragraphs, sentences and tokens, and
then each token is lemmatized and tagged with its Part-Of-Speech. A rule-based parser
in a series of sequential steps creates the sentence syntactic tree, detecting and
tagging the existing coordinated and subordinated clauses, word groups and dependencies
among them, and also recognizing named entities and concepts, based on both
language resources and also language dependent heuristics (such as
[Mr.|Sir|Dr.]+NAME=>PERSON). This process also carries out a disambiguation
step for the morphosyntactic and semantic information of each token and also anaphora
detection and resolution for sentence interlinking.
Currently the system is able to identify (allowing word inflections, variants and
synonyms) the following topic categories: named entities (people, organizations,
places, etc.), concepts (significant keywords in the text), time expressions, money
expressions and URIs.
3.3 Sentiment Analysis
The system also includes functionality to perform a detailed multilingual sentiment
analysis of texts from different sources. The text provided is analyzed to determine if
it expresses a positive/negative/neutral sentiment polarity. First [6], the local polarity
of the different sentences in the text is identified and the relationship among them is
evaluated, resulting in a global polarity value for the whole text. Besides polarity at
sentence and global level, natural language processing techniques also detect the polarity
associated to both entities and concepts in the text (aspect-based polarity).
Moreover, although perhaps not very useful in this context, the sentiment analysis
module can also detect if the text processed is subjective or objective and if it contains
irony marks, both at global and sentence level, giving the user additional information
about the reliability of the polarity obtained from the sentiment analysis.
3.4 User Demographics
The user demographics analysis module extracts some important demographics (type,
gender, age) for a given Twitter user. State-of-the-art information extraction and text
classification algorithms are used to guess those facts from his/her login, name and
profile description, based on n-grams model, developed using Weka [13].
4 Visualization
The visualization is a web interface that allows to easily building complex queries in a
structured way, enabling, thus, a versatile filtering of the data and high level visuali-
zation with the aim to provide the final user with a highly aggregated and condensed
information at a first sight. The system is designed to provide both real time analysis
and backtracking of previously stored data.
The system console is created defining several elements called widgets, in such a
way that the template may be changed between different user cases (different cities
and their particular needs) to adapt the system to each community.
Some of the components make use of the Highcharts JavaScript library [14] to create
intuitive and interactive charts, OpenLayers [15] to display maps and geoposition
information, as well as self-customized components. The user interface makes use of
the capabilities of Elasticsearch, allowing the user to create their own queries by filtering
on the semantic tags and aggregating information using its Facets API.
An example of an analysis dashboard using some of the built widgets is shown in
next figures. Figure 4 shows filter capabilities, analysis of total number of tweets and
alerts as well as last minute tracked tweets and alerts, a timeline tracking the number
of tweets and alerts per minute, as well as the number of positive and negative ones.
Fig. 4. Dashboard with filters, statistics and timelines
Figure 5 presents several pie charts with user statistics (number of users by age
range and gender), global sentiment polarity, and a list of the most frequent alerts,
locations and events.
Fig. 5. Dashboard with user demographics, sentiment polarity, alerts and events
The console also displays a map with the locations of the alerts that contained this
information and also includes the semantically annotated tweets that match the filtering
criteria (Figure 6).
Fig. 6. Dashboard with tweets and map
Last, Figure 7 shows some widgets with tag clouds listing the most relevant (by
number of appearance) topics, entities, concepts and hashtags.
Fig. 7. Dashboard with tag clouds of topics, entities, concepts and hashtags
5 Discussion and Future Work
In this work a real time semantic annotation engine for Twitter data with
datawarehouse capabilities and a search engine for backtracking and later data analytics
has been described. The system allows community promoters to more quickly
react to specific events that may happen (catastrophes, accidents, traffic congestion,
etc.), react to people feelings and detect which initiatives are more likely to be improving
quality of life for their citizens, to detect the topics that are worrying the citizens...
Thus, it will increase the degree of engagement of the smart cities that use the
system with their citizens.
Currently the system in beta-testing process, adapting the interfaces, fine-tuning
the different modules and removing noise in the annotations. The system will be deployed
in different scenarios in a short or medium term. There are several business
cases under negotiation. The first scenario is to build a city console for a local administration
to be able to analyze in real-time the behavior and topics of interest of the
citizens, with two components: a private console, internal for the city services, and a
public console, a dashboard with attractive, summarized, non-confidential information
to be projected or displayed at selected public locations of the city (town hall, librar-
ies, museums) or even in a LED video wall in a populous square in downtown, to
engage citizens with these technologies and also promotion. The second scenario is to
focus on emergencies services, providing early detection of security-related issues.
Regarding the technology, the storage capabilities of the system allow not only to
analyze real time data, giving a snapshot of the current city state, but also to apply
data mining algorithms to the stored data in order to better understand particularities
of the population, clustering and profiling of the different groups that form the city
environment, compare the singularities of the different detected clusters, etc. Currently,
steps to further explore this path are being taken: city mobility analysis (how,
when, why people move from one place to another), relevant topics analyzed at
neighbourhood level, city reputation and brand personality, etc.
Finally, the same approach that has being used analyzing Twitter data will be used
with other sources of information. The gatherer will be extended to capture data from
other sources like other social network like Facebook, LinkedIn (in smart-city related
groups), Tuenti, or other social sites such as YouTube, Flickr, Pinterest, etc. In addition,
we want to better adapt our core models for NLP to the special features that Social
Networks language introduce.
Acknowledgements. This work has been supported by several Spanish R&D projects:
Ciudad2020: Hacia un nuevo modelo de ciudad inteligente sostenible
(INNPRONTA IPT-20111006), MA2VICMR: Improving the access, analysis and
visibility of the multilingual and multimedia information in web for the Region of
Madrid (S2009/TIC-1542) and MULTIMEDICA: Multilingual Information Extraction
in Health domain and application to scientific and informative documents
(TIN2010-20644-C03-01).
References
1. Ciudad 2020 - Hacia un nuevo modelo de ciudad inteligente sostenible. Website.
http://innprontaciudad2020.es.
2. Elasticsearch.org. Open Source Distributed Real Time Search & Analytics.
http://www.elasticsearch.org
3. Twitter REST API v1.1. https://dev.twitter.com/docs/api/1.1
4. Textalytics API. http://textalytics.com
5. Díaz Esteban, A., I. Alegría, and J. Villena-Román (eds). Proceedings of the TASS workshop
at SEPLN 2013. Actas del XXIX Congreso de la Sociedad Española de Procesamiento
de Lenguaje Natural. IV Congreso Español de Informática. 17-20 September 2013, Madrid,
Spain.
6. Villena-Román, J., S. Lana-Serrano, C. Moreno-García, J. García-Morera, and J.C. Gonzá-
lez-Cristóbal. 2012. DAEDALUS at RepLab 2012: Polarity Classification and Filtering on
Twitter Data. CLEF 2012 Labs and Workshop Notebook Papers, Rome, Italy, September
2012.
7. Villena-Román, J., and S. Lana-Serrano. MIRACLE at VideoCLEF 2008: Topic Identification
and Keyframe Extraction in Dual Language Videos. 2009. Evaluating Systems for
Multilingual and Multimodal Information Access. 9th Workshop of the Cross-Language 
Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008, Revised Selected
Papers. Carol Peters et al. (Eds.). Lecture Notes in Computer Science, Vol. 5706,
2009.
8. Villena-Román, J., S. Lana-Serrano, and J.C. González-Cristóbal. 2008. MIRACLE at
NTCIR-7 MOAT: First Experiments on Multilingual Opinion Analysis. 7th NTCIR Workshop
Meeting. Evaluation of Information Access Technologies: Information Retrieval,
Question Answering and Cross-Lingual Information Access. Tokio, Japón, December
2008.
9. Villena-Román, J., J. García-Morera, and J.C. González-Cristóbal. 2014. Daedalus at
SemEval-2014 Task 9: Comparing Approaches for Sentiment Analysis in Twitter. Proceedings
of the 8th International Workshop on Semantic Evaluation, SemEval’14, Dublin,
Ireland, 2014 (to be published).
10. Sebastiani, F. 2002. Machine learning in automated text categorization. ACM Computing
Surveys, 34(1), pp 1–47.
11. Villena-Román, J., S. Collada-Pérez, S. Lana-Serrano, and J.C. González-Cristóbal. 2011.
Método híbrido para categorización de texto basado en aprendizaje y reglas. Procesamiento
del Lenguaje Natural, Vol. 46, 2011, pp. 35-42.
12. Villena-Román, J., S. Collada-Pérez, S. Lana-Serrano, and J.C. González-Cristóbal. 2011.
Hybrid Approach Combining Machine Learning and a Rule-Based Expert System for Text
Categorization. Proceedings of the 24th International Florida Artificial Intelligence Research
Society Conference (FLAIRS-11), May 18-20, 2011, Palm Beach, Florida, USA.
AAAI Press 2011.
13. Hall, M., E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H. Witten. 2009. The
WEKA Data Mining Software: An Update. SIGKDD Explorations, Volume 11, Issue 1.
14. Highcharts - Interactive JavaScript charts for your webpage. JavaScript library website.
http://www.highcharts.com
15. Openlayers. http://openlayers.org 
