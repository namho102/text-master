Exploring Big Data using Visual Analytics Daniel A. Keim Data Analysis and Information Visualization Group University of Konstanz, Germany Data Mining for Business Intelligence, Beer Sheva, Israel April 24, 2014 2 Automated Analysis Good News Automated Analysis of big data works! (under certain preconditions) Automated Analysis Bad News The preconditions are rarely met! Preconditions:  Data is clearly structured  Data semantics is well-defined  Data is complete, correct, and not changing over time AND  Problem is well-defined 3 Automated Analysis Network Security Data: IP Flows Task: Detection of Novel Viruses Fraud Detection Data: Credit Card (or Phone Call) Data Task: Detection of Fraud Business Analytics Data: Customer Records Task: Define Customer Target Groups Molecular Biology Data: Patient DNA Records Task: Functional Root Cause Analysis for an Illness The Role of Visualization Visualization is needed in addition to analytics to  identify the structure (based on user knowledge)  bridge the semantic gap (bring in user knowledge)  help with incomplete or incorrect and changing data  understand the problem, generate hypotheses and define the problem  and steer the analysis process in dealing with massive data (local optimization) 4 Visualization Bad News Visualization of big data does not work! 5 Search Planning Diagnosis Prediction Logic Data Storage Computing Power General Knowledge Perception Creativity Visual Analytics 6 "Computers are incredibly fast, accurate, and stupid; humans are incredibly slow, inaccurate, and brilliant; together they are powerful beyond imagination." attributed to Albert Einstein Visual Analytics www.visual-analytics.eu Roadmap from the VisMaster EU Project Video 7 Outline The Role of Visualization and Analytics in Exploring Big Data Why automated analysis does not work When visual exploration can help Visual Analytics Examples Network Security Document Analysis Financial Analysis Molecular Biology Visual Analytics Perspectives Visual Network Analysis data: rzstat3 date: 29 Nov 2005 measure: outgoing connections Hierarchy: Continents Countries Autonomous Systems Networks 8 NetFlow Records Visual Analytics of Network Data Honeypot Intrusion Detection Alerts NetFlow Records Aggr. NetFlows IP Packet Stream Processing Processing Global Internet University Network Gateway 9 Visual Network Analysis NetFlow Summary IDS Summary Summary of Input Data Netflow Visualization Network Visual Analytics Detail Analysis Host Details Netflow Details 10
ISSN 2347 - 3258
Volume 1, Issue 2 (2013) 36-39
International Journal of Advance Research and Innovation
Big Data and Distributed Data Mining: An Example of
Future Networks
Prashant Kumar b,* , Khushboo Pandey a
a
b
Department of Computer Science & Engineering, DSITM, Ghaziabad, Uttar Pradesh, India
Department of Computer Science &Engineering, Shri Venkateshwara University, Gajraula, U. P, India
Article Info
Abstract
Article history:
Received 1 August 2012
Received in revised form
22 August 2013
Accepted 28 August 2013
Available online 20 September 2013
This paper describes the perspective on the analytics of big data
generated by sensors and devices on the edge of networks. The paper
includes a discussion of the importance of data at the edge of networks
where some of ―biggest‖ big data is generated. Also quick overview of
emerging technologies, including distributed frameworks such as the
Apache Hadoop framework and Apache* Map Reduce.
Keywords
Big Data,
Networks,
Business intelligent,
Apache Hadoop,
Apache MapReduce
1. Introduction
The explosion of big data is testing the variety
[1] [5], and velocity of this flood of complex,
capabilities the explosion of big data is testing the
variety [1] [5], and velocity of this flood of complex,
capabilities of even the most advanced analytics
tools. IT is challenged by the sheer volume,
structured, semi structured, and unstructured data
which also offers organizations exciting opportunities
to gain richer, deeper, and more accurate insights into
their business.
1.1. What is Big Data?
Big data is a buzzword, catch-phrase, used to
describe a massive volume of both structured and
unstructured data that is so large that it's difficult to
process using traditional database and software
techniques[6] [7]
Fig: 1. Big Data
* Corresponding Author,
E-mail address: Prashant.iftn@gmail.com
All rights reserved: http://www.ijari.org
IJARI
Fig: 1.1. Big Data
36ISSN 2347 - 3258
Volume 1, Issue 2 (2013) 36-39
International Journal of Advance Research and Innovation
Big data is typically described by the first three
characteristics. The term big data is believed to have
originated with Web search companies who had to
query very large distributed aggregations of loosely-
structured[23] data.
Big data analytics requires capturing and
processing data where it resides. This paper explores
the value of data at the edge of networks, where some
of ―biggest‖ big data is generated. As the use of
sensors and devices as well as intelligent systems [4]
[5] [6] continues to expand, the potential to gain
insight from the flood of data from these sources
becomes a new and compelling opportunity.
Businesses that can harness the power of big data at
the edge and unlock its value to the organization will
outperform their competitors with greater capabilities
to innovate creatively and solve complex problems
whose solutions have been out of reach in the past.
Below-sometimes referred to as the three Vs.
However, organizations [6] [7] [12] need a fourth—
value—to make big data work.
•
Volume. Huge data sets that are orders of
magnitude larger than data managed in traditional
storage and analytical solutions. Think petabytes
instead of terabytes.
• Variety. Heterogeneous, complex, and variable
data[23][31], which are generated in formats as
different as e-mail, social media, video, images,
blogs, and sensor data—as well as ―shadow data‖
such as access journals and Web search histories.
• Velocity. Data is generated as a constant stream
with real- time queries for meaningful information
to be served up on demand rather than batched.
• Value. Meaningful insights that deliver predictive
analytics for future trends and patterns from deep,
complex analysis based on machine learning,
statistical modeling, and graph algorithms. These
analytics go beyond the results of traditional
business intelligence querying and reporting.
1.2. An Example of Big Data?
(The Apache Hadoop Framework and
MapReduce) New technologies are emerging to make
big data analytics possible and cost-effective [31].
The Apache Hadoop* framework is evolving as the
best new approach. The Hadoop framework redefines
the way data is managed and analyzed by leveraging
the power of a distributed grid of computing
resources.
The Hadoop open-source framework [5] [6] [7]
[21] uses a simple programming model to enable
distributed processing of large data sets on clusters of
computers. The complete technology stack includes
common utilities, a distributed file system, analytics
IJARI
and data storage platforms, and an application layer
that manages distributed processing, parallel
computation,
workflow,
and
configuration
management. In addition to offering high availability,
the Hadoop framework is more cost- effective for
handling large, complex, or unstructured data sets
than conventional approaches, and it offers massive
scalability and speed.
Fig: 1.2. The Apache Hadoop Framework
Fig: 1.3. MapReduce
2. Big Data at the Edge
Much of the current discussion about big data
analytics today focuses on managing and analyzing
unstructured data from business and social sources
such as e-mail, videos, tweets, Face book posts,
reviews, and Web behavior. While this type of big
data analytics promises to provide significant value to
organizations, data generated at the edge of the
network from sensors and other devices represents
another huge, untapped resource with the potential to
deliver insights that can transform the operations and
strategic initiatives of public and private sector
organizations.
Data from intelligent systems and sensors is
some of the largest volume, fastest streaming, and/or
most complex big data. The data sources are
distributed across the network and data is collected
37Volume 1, Issue 2 (2013) 36-39
ISSN 2347 - 3258
International Journal of Advance Research and Innovation
by an enormous variety of equipment, such as utility
meters, traffic and security cameras, RFID [22] [26]
[29] [31] readers, factory-line sensors, fitness
machines, and medical devices. Ubiquitous
connectivity and the growth of sensors and intelligent
systems have opened up a whole new storehouse of
valuable information. Edge data can provide
significant value to both the private and public sector
as a source of enormous potential for gaining deeper,
richer insight faster and more cost-effectively than in
the past. In many cases, analysis of edge data can
help organizations respond to events and solve
problems that were previously out of reach.
3. Implications for Technology
For data to be analyzed where it resides, compute
and storage capabilities must be local at the edge and
in the cloud. This local infrastructure must address a
set of unique challenges based on characteristics of
the data and related issues.
•
•
•
•
Sensed data is massive and streams 24-7.
Data is noisy and dirty and requires
preprocessing.
Data has strong locality characteristics, meaning
that the devices are operated and consumed
locally.
Data ownership, interoperability, security, and
privacy are big issues.
How does this translate into a real-life example?
Here‘s a transportation and public safety example.
•
•
•
Road sensors may belong to different
departments.
Some cameras are owned by public security,
while others belong to public transportation.
Data is generated on private vehicles.
The issues: Can the data from these multiple systems
be integrated and analyzed for meaningful insight?
Who owns the data generated on private vehicles? Is
the data secured?
These issues are well worth resolving. Multiple
data stream scan unlock intrinsic correlations [4] [5]
[16] that can have great significance overall. A recent
study in a city in the People‘s Republic of China
(PRC) shows that if you can detect morning wash
time from the water supply subsystem, you can infer
the morning rush hour; similarly, if you can detect
when offices are powered down in the evening, you
can infer the evening rush hour. Understanding these
relationships can help cities better handle traffic at
peak times as well as improve availability of water
and electrical resources when they are most needed.
4. What’s next?
Big data is a game changer and it‘s already here.
While most of the momentum around big data today
is around social media sources, I believe that
realizing the promise of big data [1][2][21] analytics
must include a way to harness the potential of big
data from intelligent systems and sensors.
•
Understand use cases and their implications. We
must understand how existing disparate data
sources can be evolved into a network of
integrated, intelligent, connected systems.
• Define the usage model requirements for the
analytics of edge data. The architecture must
take advantage of big data distributed
frameworks [24] [27] to move computation
closer to where the data resides and support big
data analytics at the edge via intelligent systems
and local clouds.
• Enable the fast and secure delivery of aggregated
data from edge analytics systems [27] [28] to
other cloud and analytics platforms for further
analysis.
• Address issues related to data ownership,
interoperability, security, and privacy.
4.1. Take the Next Steps to Manage and
Analyze Edge Data
Here‘s how you can get ready to take advantage
of this fast moving area for your organization.
•
•
Keep up-to-date with what‘s happening. For
example Intel offers practical guidance to help
you deploy big data environments more quickly
and with lower risk.
Explore business opportunities deriving from the
analytics of edge data. Collaborate with the
business to understand existing edge systems and
the potential use for data. For more information
References
[1] http://www.webopedia.com/TERM/B/black
berry_playbook.html
[2] White, Tom (2012). Hadoop: The Definitive
Guide. O'Reilly Media p 3 ISBN 978-1-4493-
3877-0
IJARI
[3] MIKE2.0, Big Data Definition"
[4] Kusnetzky, Dan. "What is "Big Data?"". DNet
Vance, Ashley (2010). "Start-Up Goes After Big
Data With Hadoop Helper". New York Times
Blog
38Volume 1, Issue 2 (2013) 36-39
ISSN 2347 - 3258
International Journal of Advance Research and Innovation
[5] a b c d e f "Data, data everywhere". The
Economist (25) 2010 Retrieved (9) 2012
[6] "E-Discovery Special Report: The Rising Tide of
Nonlinear Review". Hudson Global Retrieved
2012.by Cat Casey and Alejandra Perez
[7] "What
Technology-Assisted
Electronic
Discovery Teaches Us About The Role Of
Humans In Technology — Re-Humanizing
Technology-Assisted Review" Forbes. Retrieved
2012
[8] Francis, Matthew (2012). "Future telescope array
drives development of Exabyte processing".
Retrieved 2012
[9] "Community cleverness required". Nature 455
(7209): 1. (4) 2008. Doi: 10.1038/455001a.
[10] Sandia sees data management challenges spiral".
HPC Projects 2009
[11] Reichman, O.J.; Jones, M.B.; Schildhauer, M.P.
(2011)"Challenges and Opportunities of Open
Data in Ecology" Science 331 (6018): 703–5.
doi:10.1126/science.1197962.
[12] Hellerstein, Joe (2008). "Parallel Programming
in the Age of Big Data" Gigaom Blog
[13] Segaran, Toby; Hammerbacher, Jeff (2009).
Beautiful Data: The Stories behind Elegant Data
Solutions. O'Reilly Media.p 257.ISBN 978-0-
596-15711-1
[14] a b Hilbert &López 201.
[15] "IBM what is big data? Bringing big data to the
enterprise" www.ibm.com. Retrieved 2013
[16] Oracle and FSN, "Mastering Big Data: CFO
Strategies
to
Transform
Insight
into
Opportunity", 2012
[17] Jacobs, A. (2009). "The Pathologies of Big Data"
ACMQueue
[18] Magoulas, Roger; Lorica, Ben (2009).
"Introduction to Big Data" Release 2.0
(Sebastopol CA: O‘Reilly Media) (11).
[19] a b Snijders, C., Matzat, U., &Reips, U.-D.
(2012).  ̳Big Data‘: Big gaps of knowledge in the
field of Internet. International Journal of Internet
Science,
7,
1-5.
http://www.ijis.net/ijis7_1/ijis7_1_editorial.html
[20] Hogan, M. (2013). "Large Databases"
[21] Douglas, Laney. "3D Data Management:
Controlling Data Volume, Velocity and Variety".
Gartner Retrieved 2001
[22] Beyer, Mark. "Gartner Says Solving 'Big Data'
Challenge Involves More Than Just Managing
Volumes of Data". Gartner Archived from the
original on 10 July 2011.Retrieved 2011
[23] Richard Waters (2013). "Google search proves to
be new word in stock market prediction".
Financial Times Retrieved 2013
[24] David Leinweber (2013). "Big Data Gets Bigger:
Now Google Trends Can Predict The Market".
Forbes. Retrieved 2013
IJARI
[25] Jason Palmer (2013). "Google searches predict
market moves". BBC Retrieved 2013
[26] Kalil, Tom. "Big Data is a Big Deal". White
House Retrieved 2012
[27] Executive Office of the President (2012). "Big
Data across the Federal Government" White
House Retrieved 2012
[28] "How big data analysis helped President Obama
defeat Romney in 2012 Elections". Bosmol
Social Media News 2013 Retrieved 2013
[29] Hoover, J. Nicholas. "Government's 10 Most
Powerful Supercomputers" Information Week
UBM Retrieved 2012
[30] Bamford, James. "The NSA Is Building the
Country‘s Biggest Spy Center (Watch What You
Say)". Wired Magazine Retrieved 2013
[31] Groundbreaking Ceremony Held for $1.2 Billion
Utah Data Center" National Security Agency
Central Security Service. Retrieved 2013
[32] Layton,
Julia.
"Amazon
Technology"
Money.howstuffworks.com Retrieved 2013.
39
Knowledge Bases in the Age of Big Data Analytics
Fabian M. Suchanek Gerhard Weikum
Telecom ParisTech University
46 Rue Barrault
F-75634 Paris Cedex 13, France Max Planck Institute for Informatics
Campus E1.4
D-66123 Saarbruecken, Germany
fabian@suchanek.name weikum@mpi-inf.mpg.de
ABSTRACT
This tutorial gives an overview on state-of-the-art methods
for the automatic construction of large knowledge bases and
harnessing them for data and text analytics. It covers both
big-data methods for building knowledge bases and knowl-
edge bases being assets for big-data applications. The tuto-
rial also points out challenges and research opportunities.
1.
MOTIVATION AND SCOPE
Comphrehensive machine-readable knowledge bases (KB’s)
have been pursued since the seminal projects Cyc [19, 20]
and WordNet [12]. In contrast to these manually created
KB’s, great advances have recently been made on automat-
ing the building and curation of large KB’s [1, 16], using
information extraction (IE) techniques and harnessing high-
quality Web sources like Wikipedia. Prominent endeavors
of this kind include academic research projects such as DB-
pedia [3], KnowItAll [10], NELL [5] and YAGO [29], as well
as industrial ones such as Freebase. These projects provide
automatically constructed KB’s of facts about named enti-
ties, their semantic classes, and their mutual relationships.
They contain millions of entities and billions of facts about
them. Moreover, several KB’s are interlinked at the en-
tity level, forming the backbone of the Web of Linked Data
[14]. Such world knowledge in turn enables cognitive appli-
cations and knowledge-centric services like disambiguating
natural-language text, entity linking, text summarization,
deep question answering, and semantic search and analytics
over entities and relations in Web and enterprise data (e.g.,
[2, 6, 8, 13]). Prominent examples of how KB’s can be har-
nessed include the Google Knowledge Graph [27] and the
IBM Watson question answering system [17].
This tutorial presents state-of-the-art methods, recent ad-
vances, research opportunities, and open challenges along
this avenue of knowledge harvesting and its applications.
Particular emphasis is on the twofold role of KB’s for big-
data analytics: using scalable distributed algorithms for har-
vesting knowledge from Web and text sources, and leverag-
ing entity-centric knowledge for deeper interpretation of and
This work is licensed under the Creative Commons Attribution-
NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this li-
cense, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain per-
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st - 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 13
Copyright 2014 VLDB Endowment 2150-8097/14/08.
better intelligence with big data. The following sections out-
line the structure of the tutorial. An extensive bibliography
on this theme is given in [30].
2.
BUILDING KNOWLEDGE BASES
Digital Knowledge: Today’s KB’s represent their data
mostly in RDF-style SPO (subject-predicate-object) triples.
We introduce this data model and the most salient KB
projects, which include KnowItAll [10, 11], BabelNet [22],
ConceptNet [28], DBpedia [3, 18], DeepDive [24], Freebase
[4], ImageNet [7], NELL [5], Wikidata [31], WikiNet [21],
WikiTaxonomy [26], and YAGO [29, 15]. We briefly dis-
cuss industrial projects like the Google Knowledge Graph
and related work at Google [9, 13, 25], the EntityCube and
Probase projects at Microsoft Research [23, 32], and IBM’s
Watson project [17].
Harvesting Knowledge on Entities and Classes: Ev-
ery entity in a KB (e.g., Steve Jobs) belongs to one or mul-
tiple classes (e.g., computer pioneer, entrepreneur). These
classes are organized into a taxonomy, where more special
classes are subsumed by more general classes (e.g., person).
We discuss two families of methods to harvest such informa-
tion: Wikipedia-based approaches that analyze the category
system, and Web-based approaches that use techniques like
set expansion.
3.
HARVESTING FACTS AT WEB SCALE
Harvesting Relational Facts: Relational facts express
properties of and relationships between entities. There is a
large spectrum of methods to extract such facts from Web
documents. We give an overview on methods from pattern
matching (e.g., regular expressions), computational linguis-
tics (e.g., dependency parsing), statistical learning (e.g., fac-
tor graphs and MLN’s), and logical consistency reasoning
(e.g., weighted MaxSat or ILP solvers). We also discuss to
what extent these approaches scale to handle big data.
Open Information Extraction: Alternatively to meth-
ods that operate on a pre-specified set of relations and en-
tities, open information extraction harvests arbitrary SPO
triples from natural language documents. It aggressively
taps into noun phrases as entity candidates and verbal phrases
as prototypic patterns for relations. We discuss recent meth-
ods that follow this direction. Some methods along these
lines make clever use of big-data techniques like frequent
sequence mining and map-reduce computation.
Temporal and Multilingual Knowledge: Properly
interpreting entities and facts in a KB often requires ad-
ditional meta-information like entity names in different lan-
guages and the temporal scope of facts. We discuss tech-niques for tapping multilingual Web sources, and we cover
techniques for extracting temporal expressions and for in-
ferring the timepoints of events and timespans during which
certain facts hold.
Commonsense Knowledge: Current KB’s focus on facts
about entities. However, there is an orthogonal dimension
of commonsense that machines should acquire, too. This
includes relations between concepts (e.g., mouthpiece partOf
clarinet , clarinet hasShape cylindrical ), properties of concepts
that every child knows but are not obvious to a computer
(e.g., the statement that apples can be red, green, juicy,
sweet, sour, but not fast or funny), and also commonsense
rules (e.g., the assertion that the father of a mother’s child
is usually the husband or partner of the mother as of the
child’s birth). We discuss methods for acquiring such kinds
of commonsense knowledge.
4.
KNOWLEDGE FOR BIG DATA
When analytic tasks tap into text or Web data, it is often
crucial to identify entities (people, places, products, etc.) in
the input for proper grouping and other purposes. An exam-
ple application could aim to track and compare two entities
in social media over an extended timespan (e.g., the Ap-
ple iPhone vs. Samsung Galaxy families). In this context,
knowledge about entities is a key asset.
Named Entity Disambiguation: With text or tables
as input, entities are first seen only in surface form: by
names (e.g., “Jobs”) or phrases (e.g., “the Apple founder”).
Such entity mentions are often ambiguous; mapping them
to canonicalized entities registered in a KB is the task of
named-entity disambiguation (NED). State-of-the-art NED
methods combine context similarity between the surround-
ings of a mention and salient phrases associated with an
entity, with coherence measures for two or more entities
co-occurring together. Although these principles are well
understood, NED remains an active research area towards
improving robustness, scalability, and coverage.
Entity Linkage: Even when entities are explicitly marked
in (semi-) structured data, the problem arises to tell whether
two entities are the same or not. This is a variant of the
record-linkage problem (aka. entity matching, entity reso-
lution, entity de-duplication). For KB’s and Linked Open
Data, the goal is to generate and maintain owl:sameAs in-
formation across knowledge resources at large scale. We give
an overview of approaches to this end, covering statistical
learning approaches and graph algorithms.
5.
REFERENCES
[1] 3rd Workshop on Automated Knowledge Base
Construction, 2013, http://www.akbc.ws/
[2] E. Alfonseca et al.: HEADY: News Headline Abstr-
action through Event Pattern Clustering. ACL 2013
[3] S. Auer et al.: DBpedia: A Nucleus for a Web of
Open Data. ISWC 2007
[4] K.D. Bollacker et al.: Freebase: a Collaboratively
Created Graph Database for Structuring Human
Knowledge. SIGMOD 2008
[5] A. Carlson et al.: Toward an Architecture for
Never-Ending Language Learning. AAAI 2010
[6] T. Cheng et al.: Data Services for E-tailers Leveraging
Web Search Engine Assets. ICDE 2013
[7] J. Deng et al.: ImageNet: A Large-scale Hierarchical
Image Database. CVPR 2009
[8] O. Deshpande et al.: Building, Maintaining, and
Using Knowledge Bases: a Report from the Trenches.
SIGMOD 2013
[9] X. Dong et al.: Knowledge Vault: a Web-scale Appr-
oach to Probabilistic Knowledge Fusion. KDD 2014
[10] O. Etzioni et al.: Unsupervised Named-Entity
Extraction from the Web: An Experimental Study.
Artif. Intell. 165(1), 2005
[11] A. Fader et al.: Identifying Relations for Open
Information Extraction, EMNLP 2011
[12] C. Fellbaum, G. Miller (Eds.): WordNet: An
Electronic Lexical Database, MIT Press, 1998
[13] R. Gupta et al.: Biperpedia: An Ontology for Search
Applications. PVLDB 7(7), 2014
[14] T. Heath, C. Bizer: Linked Data: Evolving the Web
into a Global Data Space, Morgan & Claypool, 2011
[15] J. Hoffart et al.: YAGO2: a Spatially and Temporally
Enhanced Knowledge Base from Wikipedia. Artif.
Intell. 194, 2013
[16] E. Hovy, R. Navigli, S.P. Ponzetto: Collaboratively
Built Semi-Structured Content and Artificial
Intelligence: the Story So Far, Artif. Intell. 194, 2013
[17] IBM Journal of Research and Development 56(3/4),
Special Issue on “This is Watson”, 2012
[18] J. Lehmann et al.: DBpedia - A Large-scale,
Multilingual Knowledge Base Extracted from
Wikipedia. Semantic Web Journal, 2014
[19] D.B. Lenat, R. V. Guha: Building Large
Knowledge-Based Systems: Representation and
Inference in the Cyc Project. Addison-Wesley, 1990
[20] D.B. Lenat: Cyc: A Large-Scale Investment in
Knowledge Infrastructure. CACM 38(11), 1995
[21] V. Nastase, M. Strube: Transforming Wikipedia into a
large scale multilingual concept network. Artif. Intell.
194, 2013
[22] R. Navigli, S.P. Ponzetto: BabelNet: The Automatic
Construction, Evaluation and Application of a
Wide-coverage Multilingual Semantic Network. Artif.
Intell. 193, 2012
[23] Z. Nie, J.-R. Wen, W.-Y. Ma: Statistical Entity Extr-
action From the Web. Proc. of the IEEE 100(9), 2012
[24] F. Niu et al.: DeepDive: Web-scale Knowledge-base
Construction using Statistical Learning and Inference.
VLDS 2012
[25] M. Pasca: Acquisition of Open-domain Classes via
Intersective Semantics. WWW 2014
[26] S.P. Ponzetto, M. Strube: Deriving a Large-Scale
Taxonomy from Wikipedia. AAAI 2007
[27] A. Singhal: Introducing the Knowledge Graph:
Things, not Strings. Google Blog, May 16, 2012
[28] R. Speer, C. Havasi: Representing General Relational
Knowledge in ConceptNet 5, LREC 2012
[29] F.M. Suchanek, G. Kasneci, G. Weikum: YAGO: a
Core of Semantic Knowledge. WWW 2007
[30] F.M. Suchanek, G. Weikum: Knowledge Harvesting in
the Big-Data Era. SIGMOD 2013
[31] D. Vrandecic, M. Kr ̈
otzsch: Wikidata: a Free
Collaborative Knowledge Base. CACM 57, 2014
[32] W. Wu et al.: Probase: a Probabilistic Taxonomy for
Text Understanding. SIGMOD 2012
CS Principles Goes to Middle School:
Learning How to Teach “Big Data”
Philip Sheridan Buffum Allison G. Martinez-Arocho Megan Hardy Frankosky
Computer Science
North Carolina State Univ.
Raleigh, NC, USA Computer Science
Meredith College
Raleigh, NC, USA Psychology
North Carolina State Univ.
Raleigh, NC, USA
Fernando J. Rodriguez Eric N. Wiebe Kristy Elizabeth Boyer
Computer Science
North Carolina State Univ.
Raleigh, NC, USA STEM Education
North Carolina State Univ.
Raleigh, NC, USA Computer Science
North Carolina State Univ.
Raleigh, NC, USA
{psbuffum, agmarti5, rmhardy, fjrodri3, wiebe, keboyer}@ncsu.edu
ABSTRACT
Spurred by evidence that students’ future studies are highly
influenced during middle school, recent efforts have seen a
growing emphasis on introducing computer science to middle
school learners. This paper reports on the in-progress
development of a new middle school curricular module for Big
Data, situated as part of a new CS Principles-based middle school
curriculum. Big Data is of widespread societal importance and
holds increasing implications for the computer science workforce.
It also has appeal as a focus for middle school computer science
because of its rich interplay with other important computer
science principles. This paper examines three key aspects of a Big
Data unit for middle school: its alignment with emerging
curricular standards; the perspectives of middle school classroom
teachers in mathematics, science, and language arts; and student
feedback as explored during a middle school pilot study with a
small subset of the planned curriculum. The results indicate that a
Big Data unit holds great promise as part of a middle school
computer science curriculum.
Categories and Subject Descriptors
K.3.2 [Computers & Education]: Computer and Information
Sciences Education --- Computer Science Education
General Terms
Human Factors.
Keywords
Computer science education, middle school, big data
1. INTRODUCTION
Recent years have seen an increased emphasis on introducing
computer science to K-12 learners. The motivation for these
activities is rooted in an understanding of how students eventually
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGCSE’14, March 5–8, 2014, Atlanta, Georgia, USA.
Copyright © 2014 ACM 978-1-4503-2605-6/14/03...$15.00.
http://dx.doi.org/10.1145/2538862.2538949
choose and succeed with an undergraduate major: previous
positive experience with a subject matter plays a pivotal role
[2, 12]. This point has particular significance within the field of
computer science; for example, a very small number of US
students take the traditional AP CS exam, and there is a troubling
participation gap seen in that small body of test-takers [2]. The
discrepancy corresponds to similarly disappointing statistics for
students who earn bachelor degrees in computer science [29].
In both their stated aspirations and in their development of
subject-specific skills, students begin their career trajectory in
middle school [14]. Researchers have specifically traced the
underproduction and underrepresentation issues in undergraduate
computer science departments back to lack of exposure as early as
middle school [22, 26]. In the U.S., any middle school computer
science intervention must address the challenge that computer
science is essentially absent from standard public education
curricula. Numerous successful middle school interventions have
addressed this challenge through after-school, summer, or
enrichment-based activities; for example, these curricula often use
various visual programming languages [15] or computer games
[25]. Yet important questions remain for a computer science
curriculum designed for in-class settings. How can computer
science learning objectives be aligned with new curricular
standards? How can computer science be successfully delivered
by teachers who are trained in other subjects? And finally, how
can we build learning activities that are highly engaging and
developmentally appropriate for students?
This paper examines these questions in the context of a proposed
curricular unit for Big Data, one of the foci of the proposed AP
Computer Science Principles course [7]. Although a full formal
definition of “Big Data” is still emerging, the phenomenon is
often characterized by collection at high velocity, storage in a
variety of structures, and large volume [8]. In addition to its
widespread societal importance and increasing implications for
the computer science workforce, Big Data holds great promise as
a focus for middle school computer science because of its rich
interplay with other important CS Principles including algorithms,
abstraction, and the Internet. As this paper describes, Big Data
also represents an area with strong correspondence between CS
Principles and emerging US national curricular standards such as
the Common Core in Mathematics [5] and the Next Generation
Science Standards [17, 18]. In addition to its strong links to other
core subjects, the topic of Big Data has great flexibility inincorporating other highly engaging application domains,
including those that are relevant to kids’ everyday lives. For
instance, an excellent example of Big Data can be seen within
social networks and their massive collections of structured and
unstructured data.
The work reported here is conducted in the context of the E NGAGE
project, a multi-year effort that will see development of a full CS
Principles-based middle school curriculum situated within a
game-based learning environment. The CS Principles approach
may be well suited for the middle school level because the
thematic threads of CS Principles Big Ideas can be introduced at a
developmentally appropriate level for middle school students and
thus provide the basis for more advanced work in high school. We
are developing our curriculum in an iterative fashion with pilot
studies and focus groups on each set of prototype learning
activities. This paper presents the work to date on the Big Data
curricular unit, including its alignment with emerging standards,
feedback from an interdisciplinary group of teachers, and results
from a pilot study with middle school students. While the piloted
activity represents only a small subset of the planned curriculum
for E NGAGE , results suggest that Big Data holds appeal for
achieving computer science learning and for showcasing the
relevance of computer science to middle school students.
2. RELATED WORK
Recent years have seen an increase in the number of middle
school computing interventions, many of which innovate to teach
programming with visual or graphical programming languages.
Alice 3D has been used successfully [21], integrating computing
within the context of a wide variety of subjects such as math,
science, and language arts [20], and to help students understand
what their future careers in computing might look like [27].
Scratch programming has also been used extensively [4, 15, 23],
including with students with disabilities [1], urban youth [16], or
particular underrepresented groups [10]. Other languages such as
Logo [15] and more recently Kodu [24] have also been used in
middle school interventions. There is also growing momentum for
using game design and game programming [25] and robotics [13]
as ways to introduce computer science to middle school students.
Efforts have also been made to formalize assessment of
computational thinking at the middle school level [28].
In complement to this middle school work, the AP Computer
Science Principles course is being developed at the high school
level and plans to debut as an official, credit-bearing course in the
2016-17 academic year [7]. CS Principles is organized around
seven Big Ideas, one of which is Big Data. Students must analyze,
interpret, and gain new insight from data sets, both visually and
statistically [7]. Relatedly, Berkeley’s CS Principles pilot course,
"The Beauty and Joy of Computing" has sections on the impact of
Big Data and information technology in modern life [11].
Notably, this aspect of social relevance is a motivating factor for
women to enter and remain in computer science [9]. Our current
work takes inspiration from these CS Principles pilots, in concert
with teacher and student focus group feedback, in developing a
Big Data curricular unit.
3. BIG DATA CURRICULUM DESIGN
An important focus of our ongoing work is to develop a suite of
learning activities that can be adopted within a variety of middle
school contexts including standalone computer science courses or
as enrichment for subject matter areas including mathematics and
science. A vital step toward this goal is aligning the CS
Principles-based curriculum with current standards for middle
Table 1. CS Principles Computational Thinking Practices
& Common Core Practices for Mathematics
CS Principles
Computational Thinking
Practices
P1: Connecting Computing
P2: Developing computational
artifacts
P3: Abstracting
P4: Analyzing problems and
artifacts
P5: Communicating
Common Core Practices
for Mathematics
MP5: Use appropriate tools
strategically
MP4: Model with
Mathematics
MP2: Reason abstractly and
quantitatively
MP1: Make sense of
problems and persevere in
solving them
MP3: Construct viable
arguments and critique the
reasoning of others
MP6: Attend to precision
P6: Collaborating
school grade levels. We first analyzed the Common Core State
Standards for English and Mathematics, and the Next Generation
Science Standards, and then conducted teacher focus groups to
gain feedback on this alignment and to help tailor the proposed
Big Data curriculum to middle school classrooms.
3.1 Alignment with Curricular Standards
3.1.1 Common Core State Standards
Beginning in 2009, the Common Core State Standards Initiative
created a set of curricular standards intended to align the
previously separate curricula of individual states within the U.S.
According to its mission statement, these standards, “are designed
to be robust and relevant to the real world, reflecting the
knowledge and skills that our young people need for success in
college and careers” [5]. Common Core for Mathematics laid out
a series of high-level practices through which students are
expected to find meaning in problems, use abstract and
quantitative reasoning, construct arguments, critique the reasoning
of others, and model with mathematics. Similarly, the CS
Principles curriculum frames seven Big Ideas around the core
computational thinking practices of connecting computing,
developing computational artifacts, abstracting, analyzing
problems and artifacts, communicating, and collaborating [7].
Table 1 highlights some important parallels between the CS
Principles Computational Thinking (CT) Practices and the
practices for the Common Core in Mathematics.
At a high level, both CS Principles and the Common Core include
big ideas on data. While the Common Core standards relating to
data do not map directly to the corresponding CS Principles Big
Idea, we see the opportunity for a curriculum that serves the goals
of both CS Principles and the Common Core in a complementary
fashion. Table 2 shows Learning Objectives for CS Principles and
selected standards from the Common Core that relate to data.
3.1.2 The Next Generation Science Standards
Similar to the Common Core, the precursor Framework report and
Next Generation Science Standards in the U.S. [17, 18] does not
explicitly address computer science, but holds promise for the
development of complementary curricula. The Framework report
states that elements of both computational and mathematicalTable 2. Goals for Data in CS Principles and Common Core
CS Principles Big Idea 3: Data and information facilitate the
creation of knowledge
3.1.1 Use computers to process information to gain insight and
knowledge
3.1.2 Collaborate when processing information to gain insight
and knowledge
3.1.3 Communicate insight and knowledge gained from using
computer programs to process information
3.2.1 Use computing to facilitate exploration and the discovery
of connections in information
3.2.2 Use large data sets to explore and discover information
and knowledge
3.3.1 Analyze the considerations involved in the computational
manipulation of information
Common Core Standards that relate to data
Grade 6. SP.A.1 Recognize a statistical question as one that
anticipates variability in the data related to the question and
accounts for it in the answers
Grade 6. SP.B.4 Display numerical data in plots on a number
line, including dot plots, histograms, and box plots
Grade 6. SP.B.5 Summarize numerical data sets in relation to
their context
Grade 7. SP.A.1 Understand that statistics can be used to gain
information about a population by examining a sample of the
population
Grade 7. SP.A.2 Use data from a random sample to draw
inferences about a population with an unknown characteristic of
interest.
Grade 7. SP.B.4 Use measures of center and measures of
variability for numerical data from random samples to draw
informal comparative inferences about two populations
Grade 8. SP.A.1 Construct and interpret scatter plots for
bivariate measurement data to investigate patterns of association
between two quantities
Grade 8. SP.A.4 Understand that patterns of association can also
be seen in bivariate categorical data by displaying frequencies
and relative frequencies in a two-way table.
thinking are central to K-12 science education, and these are
addressed through the discussion of science and engineering
practices and crosscutting concepts [18]. These science and
engineering practices recognize the importance of identifying
student outcomes that go beyond memorizing narrowly focused
scientific facts; they provide a broader interpretation of science
inquiry that includes computational approaches.
The Next Generation Science Standards stated practices include
both analyzing and interpreting data, and using mathematical and
computational thinking [17]. The Framework was clear that the
eventual Standards documents needed to address how
mathematical and computational tools and techniques are
deployed in the service of analyzing large data sets: “Such data
sets extend the range of students’ experiences and help to
illuminate this important practice of analyzing and interpreting
data” [18]. As this statement illustrates, there is great promise for
creating curricula in which the power of computer science is
illustrated within the context of widely required subject matter.
3.2 Teacher Focus Groups
With this alignment in hand between the proposed Big Data
concepts and standard curricula, the next step was to obtain
detailed feedback from experienced middle school teachers to
inform the development of specific learning activities. We held a
series of in-depth classroom teacher focus groups during the
summer of 2013. Three teachers from an urban middle school in
Raleigh, North Carolina were recruited to participate for
approximately 40 hours each in both group and solo sessions. The
teachers were two females and one male, and all three were from
minority groups that are underrepresented in computer science.
They included one 8 th grade math teacher, one 7 th grade science
teacher, and one 7 th grade language arts teacher. Each focus group
session lasted between three and four hours and the format varied
depending on the topic. Early sessions focused on introducing
computer science topics to the teachers. These sessions involved
both presentations and guided classroom activities conducted by
project team members. Later focus groups were heavily
discussion-based, as teachers brainstormed on how to teach
computer science principles to middle school students, and
drafting classroom activities.
3.2.1 8 th Grade Math
The 8 th grade math teacher expressed some initial hesitation to the
idea of teaching Big Data to middle school students, concerned
that its concepts would be out of reach given the students’
development level. However, she became much more receptive to
the idea after a more thorough discussion and explanation of Big
Data. She emphasized the need to introduce topics in “baby
steps.” She noted that middle school students are still developing
the mathematical background to perform statistical computation.
A question she raised was, “To what degree and depth do we want
them to understand Big Data...and have it support the Common
Core?” Another of her primary concerns was, “How do we do it in
a way that is not overwhelming to them...and that they say ‘Wow,
cool. How can I use this in my life today?’” The other two
teachers echoed this sentiment, emphasizing that the curriculum
should be grounded in human applications to which students can
personally relate. As will be described in Section 4, our iterative
curriculum development process involves measuring the extent to
which students relate to each prototype learning activity.
3.2.2 7 th Grade Science
The science teacher provided insight into how to create middle
school activities that support what students are learning in their
science classes. In particular, she emphasized that we should
reinforce the scientific method, which all students in grades six
through eight have learned and use in their science classes. She
also noted that reading tables and graphs is difficult in general for
students, but is an important skill for them to learn. Making
inferences or forecasting from data is particularly difficult for
middle school students, so she suggested ways to slowly introduce
the material over time, and to avoid overly abstract
representations. This advice guided the scaffolding included
within the first Big Data activity, described in Section 4.
3.2.3 7 th Grade Language Arts
The 7 th grade language arts teacher quickly saw implications for
his classroom. For instance, he uses Wordle 1 to generate “word
clouds” from texts before his class reads a book. As a class, they
use the most prevalent words to make predictions on the dominant
themes that might appear in the book. After the lesson on “What
1
http://www.wordle.netis Big Data”, the teacher saw this activity as a good opportunity to
broaden the discussion in his classroom to include Big Data. Also
certified as a social studies teacher, he drew a parallel between a
challenge that middle school students might have with
programming and one they have in social studies: “sequencing is a
difficulty for kids.” He noted that middle school students struggle
to make the connection from past to present, from present to
future, and from self to global community. Our team aims to
address some of these challenges with E NGAGE ’s game-based
narrative, designed to give students a strong sense of engagement
and personal connection with each problem they solve.
4. PILOT STUDY
Following the conclusion of the focus groups, we conducted a
small two-session pilot study with students from an urban middle
school in Raleigh, North Carolina. The purpose of the study was
to observe students’ success at completing a Big Data activity
prototype and then to get a sense of their retention and continued
interest a week later, as well as have them complete a series of
follow-up Big Data learning activities. The first session consisted
of a pre-survey, followed by working on the learning activity in
collaborative pairs, and then ending with a post-survey. Focus
groups held during the second session one week later centered on
students’ general conceptions surrounding Big Data, as well as
their opinions on the various Big Data learning activities.
4.1 Session 1: Big Data Activity
A total of 15 students in 6 th and 7 th grades participated in the first
pilot study session, which was held after school with a total time
allotment of 50 minutes including pre-survey and post-survey.
Based on experience from prior pilot studies in which students
were predominantly male, the after-school coordinator was asked
to emphasize recruiting female students. This emphasis succeeded
and 13 of the 15 participants were female. The demographic
makeup was six white students, four African-American students,
one Latino student, one Native American student, one Indian
student, one Middle Eastern student, and one multiracial student.
In the pre-survey, students were asked to complete the New
General Self Efficacy (NGSE) scale, which aims to explain
variance in motivation and performance by measuring students’
belief that they can achieve a goal [6]. Scores on the 8-item scale
(a 5 point Likert) revealed a high overall level of self-efficacy for
the group (M=4.408, SD=0.513). This high overall self-efficacy
may reflect a self-selection bias related to the after-school class.
Students also completed Likert scale items scored from 1 (not at
all interested) to 5 (very interested) relating to two application
domains: marine life and social networking. Marine life is the
application domain for E NGAGE ’s game-based narrative and an
application of Big Data in formal science education. Social
networking, on the other hand, is a human application of Big Data
that the teacher focus groups indicated would be more relevant to
students’ everyday lives. On average, students responded to “How
interested are you in marine life?” with a 3.1 out of 5 (SD = 1.1),
and to “How interested are you in social networks?” with a 3.3 out
of 5 (SD = 1.6) While only 2 of the 15 students rated both marine
life and social networks favorably (4 out of 5, or 5 out of 5), the
number of students who rated at least one of these two topics
favorably was 12 out of 15, indicating that a Big Data curriculum
that includes a variety of topics is much more promising than
focusing on just one application domain.
After finishing the online pre-surveys, 37 minutes remained for
the Big Data activity prototype. Students were paired at computers
Figure 1. Example visual subset of the fish database
equipped with screen capture and video recording for future
analysis. In the activity, students progressed through a written
narrative in which they took on the role of computer scientists
sent to rescue an underwater research station in distress. As part of
the adventure, they had to locate a file for a particular fish species
within a massive fish database. The activity guides students step-
by-step through a skip list search algorithm. Figure 1 shows an
example subset of the fish database from the learning activity.
Skip lists, widely used in Big Data applications, provide an
excellent context for teaching about data representation and
algorithms. While the final curriculum will include many other
Big Data learning activities that convey the excitement and
conceptual value of Big Data (building upon the success of
activities piloted elsewhere [7]), this particular Big Data pilot
activity aimed to explore how Big Data applications can provide
the framework for addressing other CS Principles Big Ideas, such
as computing as a creative activity (Big Idea 1) and algorithms
(Big Idea 4). Using a slightly larger subset of the database than
the one illustrated in Figure 1, students were tasked with finding
the shortest path to various target fish. The activity scaffolded
students through each step of the skip list algorithm, using the
visual subset of the database. After mastering the searching task
manually on that concretely depicted subset, students then used a
visual interface (simulating the visual programming language we
will use in the E NGAGE game) to arrange pseudo-code blocks into
a program that could traverse the entire database to find a desired
fish (in this case, Surgeonfish). An ideal algorithmic solution is
shown in Figure 2. As described in the next section, this first Big
Data activity is designed to occupy one entire classroom session
of approximately 50 minutes.
The post-survey included a User Engagement Scale [19] that
measured involvement with the activity, focused attention, and the
student’s desire to repeat a similar activity in the future. Overall,
students agreed with the item, “I felt involved with this activity”
(3.9 out of 5; SD = 0.9), and generally disagreed with, “I felt
discouraged while doing this activity” (1.9 out of 5; SD = 1.0). In
contrast to these overall positive ratings, there was one African-
Step 0: Start at Level 3 Skip List, Bannerfish
Step 1: Peek ahead to next fish in current level
Step 2: If the fish you see ahead comes before <Surgeonfish>
Then Move Forward (to that fish)
And Repeat (return to Step 1)
Else if the fish you see ahead comes after <Surgeonfish>
Then Move Down (to a lower skip list)
And Repeat (return to Step 1)
Else if the fish you see ahead is <Surgeonfish>
Then Move Forward (to that fish)
And STOP
Figure 2. Target algorithm for students to constructAmerican female student who responded with a 2 for “I felt
involved with the activity”, and with a 4 for “I felt discouraged
while doing this activity”. Among all the students in the study,
this student had demonstrated the greatest interest in social
networking, with lower stated interest in marine life, possibly
explaining her lower feeling of involvement and higher
discouragement with the marine life Big Data activity. As will be
described in Section 5.2, this student later made contributions
during the later focus group that shed light on her experience. When considered in light of CS Principles Learning Objectives
3.1.2 and 3.1.3 (Table 2), the above dialogue illustrates the
potential of data to be socially relevant, a key concern for girls in
computer science. Following up on this finding, we also prompted
the students to think about data for social networking. Most
students had never previously given thought to data outside of
science domains, but even students who had been quiet up to this
point in the focus group expressed interest in these examples of
data in their everyday lives.
Overall, students rated, “This activity was fun” with 2.8 out of 5
SD = 1.3), lower than hoped. Because the prototype activity is
being developed for use within E NGAGE ’s game-based learning
environment, fun is an important emphasis. Upon reflection, we
believe that the pace of the learning activity (with several minutes
elapsing between achievements on the task) may have hindered a
sense of fun, which will be addressed in future iterations. It is
important that students frequently feel a sense of achievement,
even if the successes represent incremental steps toward the larger
goal. Also providing helpful feedback, students did not disagree
with “I found this activity to be confusing” (3.2 out of 5; SD =
1.1), indicating room for improvement on the scaffolding. Based
on all of the students’ feedback, we designed session 2 of the pilot
study, which was held one week later. After discussing computer science, programming and data,
students collaboratively worked on a series of short activities
related to the Big Data activity from the session 1. Like that prior
activity, these were prototypes of activities being considered for
E NGAGE ’s game-based learning environment. Consequently, they
were presented as elements of the game’s story, with students told
that their feedback would influence the development of the game.
In designing these activities, we aimed to improve on the design
of the activity from session 1, such as by decreasing the amount of
reading required. We also sought to obtain student feedback on
Big Data applications that convey the full power and conceptual
value of Big Data. Overall, students were noticeably engaged
during this session. It included an activity involving cryptography,
previously used for engaging middle school students in computer
science [3], and a problem in which students solved a mystery by
examining clusters of marine data to track a leaked pollutant.
4.2 Session 2: Focus Groups
Of the original 15 students, 11 returned the next week for the
second session, joined by three new students. As with Session 1,
all students were in 6 th and 7 th grades. The demographics for the
second session were as follows: 12 female and 2 male; 3 white, 6
African-American, 1 Latino, 1 Native American, 1 Indian, 1
Middle Eastern, and 1 multiracial student. The students were
separated into three focus groups of four or five students per
group, each facilitated by a member of the research team. Each
group spent roughly 15 minutes discussing students’ conceptions
of computer science and programming, and then 15 minutes on
data. A series of follow-up activities to the previous session’s Big
Data activity were then done in groups, with the new students
paired up with students who had attended the previous week. The
general purpose of these focus groups was to get student feedback
on what works and what needs improvement in our Big Data
curriculum. Particular attention was paid to eliciting feedback
from students who reported negative opinions on the Big Data
activity in session 1.
As predicted, students had a limited understanding of what Big
Data might be. They could discuss data on a smaller scale,
describing it as information that you can get and store: “anything
you want to keep over a long time.” They were familiar with
examples of data in science, such as weather patterns or plant
biology, and some saw the purpose of data being “to do a science
project.” Although they seemed not to have given much prior
thought to other issues related to data, some students got excited
thinking about the topic. When asked about data storage, one
group of female students interpreted this question in terms of
communication and collaboration. Figure 3 shows their
conversation.
Facilitator: Where is data stored?
Student 1: On the ground, on a paper, you can put it anywhere.
Student 2: You could put it on the back of a spaghetti can.
Facilitator: Yes you could, but would it be helpful there?
Student 3: You could put [the spaghetti can] in the pantry.
Student 2: Like a community pantry.
Figure 3. Students discuss importance of sharing data
The post-survey revealed that students felt more positively about
the activities in this session than they did about the activity from
session 1. Students rated these activities highly for “fun” (4.1 out
of 5; SD = 0.8) and low for “confusing” (2.1 out of 5; SD = 0.8).
Feedback was also more consistently positive, including for the
African-American female student reporting negative feelings after
session 1 (see Section 5.1). During the discussion, she advocated
for including social networking features in the E NGAGE game,
particularly the ability to chat with other players: “Everyone likes
chatting.” Similar to the conversation in Figure 3, this student’s
comments highlight the importance of communication and
collaboration, two of the Learning Objectives for CS Principles
Big Idea 3 (Table 2). While the improved positivity may be
partially explained by repeated exposure to the topic, bringing
greater levels of comfort, it also confirms the notion expressed by
the classroom teachers in our focus groups: a middle school
curriculum can benefit from including activities that are relevant
to students’ everyday lives.
5. CONCLUSION
A curriculum based on Computer Science Principles holds great
promise for middle school, and the CS Principles related to Big
Data may be particularly well suited for this age group. We have
presented a case for teaching Big Data including its strong
alignment with standard U.S. curricula including the Common
Core and the Next Generation Science Standards. We have also
presented results from classroom teacher focus groups that
suggest Big Data can serve to enrich core subject matter including
science, mathematics, and language arts. Lastly, we examined the
findings of a small pilot study with middle school students that
yielded several important lessons for refining the curriculum and
learning tasks. First, although we created a learning activity with
distinct subtasks, breaking these down even further and providing
incremental achievements along the way may increase students’
sense of fun and decrease confusion. Second, including a variety
of application domains can strategically engage a broader set of
learners with the Big Data activities. Finally, focus group
feedback indicates that students may intuitively gravitate towardseeing the importance of communication and collaboration, and
that these facets may be particularly beneficial to emphasize in
Big Data activities for middle school.
To continue creating a successful CS Principles-based Big Data
curriculum for middle school, several directions are important to
future work. Teacher professional development at the middle
school level is key, as computer science and Big Data are new
topics for most teachers who might implement the curriculum.
Additionally, it is important to develop formal CS Principles-
related learning trajectories that map from high school to middle
school and eventually to elementary school. Finally, it is hoped
that as part of a broader effort to bring CS Principles-based
curricula to middle school, this work can substantially increase
access to, and interest in, computer science for K-12 learners.
6. ACKNOWLEDGEMENTS
This work is supported in part by the National Science Foundation
through Grants CNS-113897 and CNS-1042468. Any opinions,
findings, conclusions, or recommendations expressed in this
report are those of the participants, and do not necessarily
represent the official views, opinions, or policy of the National
Science Foundation.
7. REFERENCES
[1] Adams, J.C. 2010. Scratching Middle Schoolers’ Creative
Itch. Proceedings of SIGCSE ’10 (2010), 356–360.
[2] Arpaci-dusseau, A. et al. 2013. Computer Science
Principles : Analysis of a Proposed Advanced Placement
Course. Proceedings of SIGCSE ’13 (2013), 251-256.
[3] Bergner, N. et al. 2012. Cryptography for Middle School
Students in an Extracurricular Learning Place. CSEDU 2012
Proceedings of the 4th International Conference on
Computer Supported Education (2012), 265–270.
[4] Burke, Q. and Kafai, Y.B. 2012. The Writers ’ Workshop for
Youth Programmers Digital Storytelling with Scratch in
Middle School Classrooms. Proceedings of SIGCSE '12
(2012), 433–438.
[5] CCSSI, Common Core State Standards Initiative: 2012.
Retrieved September 05, 2013 from
http://www.corestandards.org/Math
[6] Chen, G. et al. 2001. Validation of a New General Self-
Efficacy Scale. Organizational Research Methods. 4, 1 (Jan.
2001), 62–83.
[7] Computer Science: Principles: 2013. Retrieved September 5,
2013 from http://www.csprinciples.org/
[8] Crawford, K. 2011. Six Provocations for Big Data. A Decade
in Internet Time: Symposium on the Dynamics of the Internet
and Society (2011), 1–17.
[9] Fisher, A. et al. 1997. Undergraduate Women in Computer
Science: Experience, Motivation and Culture. Computer. 29,
(1997), 106–110.
[10] Franklin, D. et al. 2011. Animal Tlatoque: Attracting Middle
School Students to Computing Through Culturally-Relevant
Themes. Proceedings of SIGCSE ’11 (2011), 453–458.
[11] Garcia, D.D. et al. 2012. Rediscovering the Passion, Beauty,
Joy, and Awe: Making Computing Fun Again, part 5.
Proceedings of SIGCSE '12 (2012), 577–578.
[12] Guzdial, M. et al. 2010. A Statewide Survey on Computing
Education Pathways and Influences : Factors in Broadening
Participation in Computing. Proceedings of the ninth annual
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
[25]
[26]
[27]
[28]
[29]
international conference on International computing
research (ICER '12) (2012), 143–150.
Larkins, D.B. et al. 2013. Application of the Cognitive
Apprenticeship Framework to a Middle School Robotics
Camp. Proceedings of SIGCSE '13 (2013), 89–94.
Lent, R.W. et al. 1994. Toward a Unifying Social Cognitive
Theory of Career and Academic Interest, Choice, and
Performance. Journal of Vocational Behavior. 45, (1994),
79–122.
Lewis, C.M. 2010. How Programming Environment Shapes
Perception, Learning and Goals: Logo vs. Scratch.
Proceedings of SIGCSE ’10 (2010), 346–350.
Maloney, J. et al. 2008. Programming by Choice : Urban
Youth Learning Programming with Scratch. Proceedings of
SIGCSE ’08 (2008), 367–371.
Next Generation Science Standards: 2013. Retrieved
September 5, 2013 from http://www.nextgenscience.org/next-
generation-science-standards
NRC, National Research Council. 2011. A Framework for K-
12 Science Education: Practices, Crosscutting Concepts, and
Core Ideas. The National Academies.
O’Brien, H.L.O. and Toms, E.G. 2010. The Development
and Evaluation of a Survey to Measure User Engagement.
Journal of the American Society for Information Science. 61,
(2010), 50–69.
Rodger, S.H. et al. 2009. Engaging Middle School Teachers
and Students with Alice in a Diverse Set of Subjects.
Proceedings of SIGCSE ’09 (2009), 271–275.
Rodger, S.H. et al. 2010. Enhancing K-12 Education with
Alice Programming Adventures. Proceedings of the fifteenth
annual conference on Innovation and technology in
computer science education - ITiCSE ’10. (2010), 234.
Shashaani, L. 1994. Gender Differences in Computer
Experiences and its Influence on Computer Attitudes.
Journal of Educational Computing Research. 11, (1994),
347–367.
Sivilotti, P.A.G. and Laugel, S.A. 2008. Scratching the
Surface of Advanced Topics in Software Engineering: A
Workshop Module for Middle School Students. Proceedings
of SIGCSE ’08 (2008), 291–295.
Stolee, K.T. and Fristoe, T. 2011. Expressing computer
science concepts through Kodu game lab. Proceedings of
SIGCSE '11 (2011), 99–104.
Webb, D.C. et al. 2012. Toward an Emergent Theory of
Broadening Participation in Computer Science Education.
Proceedings of SIGCSE '12 (2012), 173–178.
Webb, H.C. 2011. Injecting Computational Thinking into
Career Explorations for Middle School Girls. 2011 IEEE
Symposium on Visual Languages and HumanCentric
Computing VLHCC. (2011), 237–238.
Webb, H.C. and Rosson, M.B. 2011. Exploring Careers
While Learning Alice 3D: a Summer Camp for Middle
School Girls. Proceedings of SIGCSE ’11 (2011), 377–382.
Werner, L. et al. 2012. The Fairy Performance Assessment:
Measuring Computational Thinking in Middle School.
Proceedings of SIGCSE ’12 (2012), 215-220.
Zweben, B.S. 2013. Computing Degree and Enrollment
Trends From the 2011-2012 CRA Taulbee Survey.
Computing Research Association.The Uses of Big Data in Cities
Luis M. A. Bettencourt
SFI WORKING PAPER: 2013-09-029
SFI	
  Working	
  Papers	
  contain	
  accounts	
  of	
  scienti5ic	
  work	
  of	
  the	
  author(s)	
  and	
  do	
  not	
  necessarily	
  represent
the	
  views	
  of	
  the	
  Santa	
  Fe	
  Institute.	
  	
  We	
  accept	
  papers	
  intended	
  for	
  publication	
  in	
  peer-­‐reviewed	
  journals	
  or
proceedings	
  volumes,	
  but	
  not	
  papers	
  that	
  have	
  already	
  appeared	
  in	
  print.	
  	
  Except	
  for	
  papers	
  by	
  our	
  external
faculty,	
  papers	
  must	
  be	
  based	
  on	
  work	
  done	
  at	
  SFI,	
  inspired	
  by	
  an	
  invited	
  visit	
  to	
  or	
  collaboration	
  at	
  SFI,	
  or
funded	
  by	
  an	
  SFI	
  grant.
©NOTICE:	
  This	
  working	
  paper	
  is	
  included	
  by	
  permission	
  of	
  the	
  contributing	
  author(s)	
  as	
  a	
  means	
  to	
  ensure
timely	
  distribution	
  of	
  the	
  scholarly	
  and	
  technical	
  work	
  on	
  a	
  non-­‐commercial	
  basis.	
  	
  	
  Copyright	
  and	
  all	
  rights
therein	
  are	
  maintained	
  by	
  the	
  author(s).	
  It	
  is	
  understood	
  that	
  all	
  persons	
  copying	
  this	
  information	
  will
adhere	
  to	
  the	
  terms	
  and	
  constraints	
  invoked	
  by	
  each	
  author's	
  copyright.	
  These	
  works	
  	
  may	
  	
  be	
  reposted
only	
  with	
  the	
  explicit	
  permission	
  of	
  the	
  copyright	
  holder.
www.santafe.edu
SANTA FE INSTITUTE	
  
The Uses of Big Data in Cities
Luís M. A. Bettencourt
Santa Fe Institute
1399 Hyde Park Rd
Santa Fe, NM 87501, USA.
Email: bettencourt@santafe.edu
@BettencourtLuis
September 17, 2013
Abstract:
There is much enthusiasm currently about the possibilities created by new and more
extensive sources of data to better understand and manage cities. Here, I explore how big
data can be useful in urban planning by formalizing the planning process as a general
computational problem. I show that, under general conditions, new sources of data
coordinated with urban policy can be applied following fundamental principles of
engineering to achieve new solutions to important age-old urban problems. I also show,
that comprehensive urban planning is computationally intractable (i.e. practically
impossible) in large cities, regardless of the amounts of data available. This dilemma
between the need for planning and coordination and its impossibility in detail is resolved
by the recognition that cities are first and foremost self-organizing social networks
embedded in space and enabled by urban infrastructure and services. As such the primary
role of big data in cities is to facilitate information flows and mechanisms of learning and
coordination by heterogeneous individuals. However, processes of self-organization in
cities, as well as of service improvement and expansion, must rely on general principles
that enforce necessary conditions for cities to operate and evolve. Such ideas are the core
a developing scientific theory of cities, which is itself enabled by the growing availability
of quantitative data on thousands of cities worldwide, across different geographies and
levels of development. These three uses of data and information technologies in cities
constitute then the necessary pillars for more successful urban policy and management
that encourages, and does not stifle, the fundamental role of cities as engines of
development and innovation in human societies.
Keywords: Urban Planning & Policy, Engineering, Feedback Control Theory,
Computational Complexity, Self-Organization, Science of Cities.
	
  	
  
1. New Opportunities for the Use of Big Data in Cites
How does one measure a city? By the buildings that fill its skyline? By the efficiency of its rapid
transit? Or, perhaps, by what Jane Jacobs called the “sidewalk ballet” of a busy street? Certainly
these are the memorable hallmarks of any modern city or metropolitan area. But a city’s true
measure goes beyond human-made structures and lies deeper than daily routine. Rather, cities and
metro areas are defined by the quality of the ideas they generate, the innovations they spur, and
the opportunities they create for the people living within and outside the city limits. - Judith
Rodin, 2013 [1].
The rise of information and communication technologies (ICT) [2,3] and the spread of
urbanization [4] are arguable the two most important global trends at play across the
world today. Both are unprecedented in their scope and magnitude in history, and both
will likely change the way we live irreversibly. If current trends continue, we may
reasonably expect that the vast majority of people everywhere in the world will live in
urban environments within just a few decades and that information technologies will be
part of their daily lives, embedded in their dwellings, communications, transportation and
other urban services.
It is therefore an obvious opportunity to use such technologies to better understand
urbanism as away of life [5,6], and to improve and attempt to resolve the many
challenges that urban development entails in both developed and developing cities [4,7].
Despite its general appeal, the fundamental opportunities and challenges of using big data
in cities have, in my opinion, not been sufficiently formalized. In particular, the necessary
conditions for the general strategic application of big data in cities need to be spelled out
and their limitations must also be, as much as possible, anticipated and clarified. To
address these questions in light of the current interdisciplinary knowledge of cities is the
main objective of this perspective 1 .
Before I start, it is important to emphasize that the use of quantitative data to better
understand urban problems and to guide their solutions is not at all new. In the United
States – and in New York City in particular – we have been building detailed statistical
pictures of urban issues for over a century. Jacob Riis’ influential “How the other half
lives” [11] derived much of its persuasive power from the use of statistics, e.g. by giving
numbers of deaths in tenements in New York City to the person. The NYC-RAND
Institute in the 1970s [12] used detailed urban statistics, modeling and computation
developed for wartime and corporate management to determine resource allocation,
especially for New York City’s Fire Department [13]. Thus, today’s Smart Cities
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
1 	
   In this paper I do not discuss issues of ethics or privacy related to big data, nor of the political or
corporate potential pitfalls of using big data to manage cities. This is not because these issues are
not important but because here I wish to focus on the fundamental possibilities to manage cities
based on data, without such complications. It can probably be said that any of these
considerations will work to limit the scope and effectiveness of the use of data to address urban
issues and will, as a consequence, make the case for its use less compelling. Several authors have
recently written eloquently about several of these issues [8,9,10].
	
  
2	
  	
  
movement [14-28] needs to be placed in perspective: Are the achievements made
possible by modern data and information technologies fundamentally different from what
was possible in the past? Or are they the extensions of old ideas and procedures, albeit
with greater scope and precision?
To answer these questions I formalize the use of data in urban policy and management in
light of the conceptual frameworks of engineering [29]. I show, in the next section, that
this allows us to identify the necessary conditions for the effective use of data and policy
to address a large array of urban issues. In this way I will demonstrate that, in principle at
least, modern information and communication technologies are opening entirely new
windows of opportunity for the application of engineering solutions to cities.
But the problems of cities are always primarily about people. Social and economic issues
in cities define what planners call ‘wicked problems’ [30], a term that has gained
currency in policy analysis well beyond the urban context. These are the kind of issues
that are not expected to yield to engineering solutions, for specific reasons [30] that break
the assumptions of feedback control theory [29]. In section 3 I revisit the issue of
‘wicked problem’ in light of computational complexity theory [31] to make the formal
argument that detailed urban planning is computationally intractable. This means that
solutions requiring the knowledge and forecast of chains of detailed behaviors in cities
have the fundamental property that they become practically impossible in large cities,
regardless of the size of data available. This clarifies the central dilemma of urban
planning and policy: that planning is clearly necessary to address long-term issues that
span the city, e.g. in terms of services and infrastructure, and yet that the effects of such
plans are impossible to evaluate a priori in detail.
In Section 4 I resolve this dilemma. The key is the nature of self-organization of social
and economic life in cities and the development of a general quantitative understanding
of how such processes operate in cities as vast networks, spanning large spatial and
temporal scales. Theory recognizes that most individual details are irrelevant to describe
complex systems as a whole, while identifying crucial general dynamics [32]. For
example, a city exists and functions even as people change their place of residence, jobs
and friendships. The development of this kind of urban theory too follows from
increasing data availability in many cities around the world, both in terms of observations
and, wherever possible, from experiments.
I then discuss how the increasing integration of limited-scope engineering solutions with
the dynamics of social self-organization in cities, articulated by the long and large views
afforded by urban theory provide the appropriate context to understand and manage
cities, while allowing them to play their primary function in human societies of
continuing to evolve socially and economically in open-ended ways.
	
  
3	
  	
  
2. The Nature of Big Data Solutions in Cities
It is a profoundly erroneous truism [...] that we should cultivate the habit of thinking what we are
doing. The precise opposite is the case. Civilization advances by extending the number of
important operations, which we can perform without thinking about them.
- Alfred Whitehead, 1911 [33].
Relatively simple-minded solutions, enabled by precise measurements and prompt
responses, can sometimes operate wonders even in seemingly very complex systems
where traditional policies or technologies have failed in the past.
An example, not immediately related to managing cities, illustrates this point: self-
driving cars. I have recently attended a workshop at the Kavli Institute for Theoretical
Physics in Santa Barbara, CA, as part of a group of interdisciplinary researchers
discussing issues of artificial intelligence, algorithms and their relation to neuroscience.
In a particularly memorable lecture, Prof. Richard Murray from Caltech 2 explained how
his team engineered a self-driving vehicle that completed DARPA’s Grand Challenge in
2005 [34] and was now deploying it on the road in cities 3 . Richard described the array of
diverse sensors for determining surrounding objects and their speed, for geo-locating the
vehicle, recognizing the road and its boundaries and planning its path. I interjected
asking whether the vehicle learned from its experience and, to my surprise, the response
was no: it was all “hardwired” by the team at Caltech, using principles of feedback
control theory. I protested that surely the hardest problem faced by a self-driving car was
not planning its path, recognizing the road or measuring surrounding traffic – but all the
stupid things that people do on the road: “Doesn’t the car need a model of human
behavior? That is surely the hard part!”
The answer, surprisingly, is no, not at all! The reason is that while people surely do
reckless things behind the wheel, they do so, from the car’s perspective, at a truly glacial
pace: While humans behave unpredictably on the time scale of seconds, the car’s
electronics and actuators respond in milliseconds or faster. In practice, this allows the car
more than enough time to take very basic ‘hardwired’ evasive actions, such as breaking
or getting out of the way. In fact, presently self-driving cars have a safety record that
exceeds that of humans [35,36].
The lesson from this story is that relatively simple solutions that require no great
intelligence can, under specific conditions, solve very hard problems. The key is fast and
precise enough measurement and adequate simple reactions. In other words, sometimes
you don’t need to be very smart if you are fast enough.
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
2
This paragraph is based on my personal recollection of Richard Murray’s lecture. Any
inaccuracies or embellishments are mine not the speaker’s.
3
Self-driving cars or autonomous vehicles (most famously developed by Sebastian Thrun’s team
from Stanford U., now at Google) are no longer news in urban environments. Google deploys a
dozen cars on a typical day in the Bay Area [35].
	
  
	
  
4	
  	
  
This is the logic of modern engineering, and more precisely of feedback control theory
[29]. If we know the desired operating point for a system (e.g. avoid collision) and we
have the means to operate on the system, as we observe its state change, via feedback
loops, we can under general conditions turn it into a simple problem. The crucial
conditions are to be able to measure and recognize potential problems just as they start to
arise (car approaching) and act to make the necessary corrections (break or get out of the
way, thereby avoiding collision). The crucial issue is that of temporal scales; every
system has intrinsic timescales at which problems develop, a few seconds in the case of a
car on the road. Cycles of measurement and reaction that avoid such complex problems
by simple means must act well within this window of opportunity.
We now see more clearly why a new generation of bigger data may offer radically novel
solutions to difficult problems. Modern electronics are now so fast in comparison to
most physical, biological and social phenomena that myriads of important policy
problems are falling within this window of opportunity. In such circumstances, models
of system response can be very simple and crude (they can typically be linearized) [29].
Thus, the engineering approach conveniently bypasses the complexities that always arise
in these systems at longer temporal or larger spatial scales, such as the need to develop
models of human behavior. In this way a difficult and important problem can be solved
essentially without theory [37]. This is the (potential) miracle of big data in cities.
Most examples of urban management and policy in cities that use data successfully have
this flavor, regardless of being implemented by people and organizations or by computer
algorithms. Table 1 presents a summary of important urban issues, where I attempted to
roughly characterize their typical temporal and spatial scales and the nature of their
operating points, or outcomes.
For example, think of urban transportation systems, such as a bus network, in these terms
[38]. Buses should carry passengers who wait a few minutes to be transported over a few
kilometers. Measuring the time in between buses at each stop, possibly together with the
number of passengers waiting, gives the planner the basis for a feedback control solution:
Communicate with buses to enforce desired standards of service, quickly place more or
fewer units in service where these parameters start to deviate from the ideal metrics, and
the quality of service as measured by per person waiting times will improve. This type of
strategy can be operated intuitively by human dispatchers but possibly can also be
implemented by an ICT algorithm [38] with access to the necessary measurements and
actions. Feedback control theory provides the framework for the development and
optimization of any of these solutions.
Similar procedures could be devised for water or power supply management and for their
integration with public transportation systems [39,40]. Traffic management, trash
collection, infrastructure maintenance (building, roads, pipes, etc) could also generally be
though of, and integrated together, in analogous ways.
	
  
5	
  	
  
Problem
Transportation
(buses, subway)
Fire
Epidemics
(HIV, Influenza)
Chronic Diseases
Sanitation
Crime
Infrastructure
Timescale
minutes
Spatial Scale Outcome Metric
meters
simple
minutes
years, days meters
city-wide simple
simple
decades
years
minutes
days city-wide
city-wide
meters
meters simple
simple
simple
simple
(roads, pipes, cables)
Traffic
minutes
Meters-km
Trash collection
days
meters
Education
decades
city-wide
Economic Development
decades
city-wide
Employment
years
city-wide
Poverty
decades
neighborhood
Energy and Sustainability
years
city-wide
Public housing
years-decades neighborhood
simple
simple
complex
complex
complex
complex
complex
complex
Table 1. Urban issues, their temporal and spatial scales and the character of their associated
metrics. Outcome metrics are to be interpreted on the time scale in the table. On longer timescales
socioeconomic issues, characterized by long times, become part of every problem.
And there is no reason in principle why certain stubbornly difficult social problems, such
as crime, may not be responsive to patrolling and other law enforcement strategies that
follow similar principles. Crime is local and its outcomes clear enough. Exciting new
experiments in several cities, notably in Los Angeles [41,42], suggest that much.
Recent insights by innovative corporations [43-46] and policy strategists [47] have
correctly anticipated that flows of information underpin such coordinated solutions. Thus,
progress in these problems is fundamentally an ICT problem, enabled by simple actions
or policies that correct the states of systems towards optimal performance.
But other urban issues, especially those that are primarily social or economic acquire, as
seen through this lens, a different character because their operating points are not well
defined or because their dynamics are rather diffuse and play out over large temporal or
spatial scales and are the result of large chains of social events, Table 1. Thus, it has
remained difficult to create engineering solutions to problems of education, public
housing 4 , economic development, poverty or sustainability at the city level.
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
4 	
   Notably, public housing policy in places such as Hong Kong [48] or Singapore [49] has fared
uncharacteristically well when compared to western cities. These successes are likely due to a
combination of an adaptable design, where apartments can be expanded as households become
	
  
6	
  	
  
Public health issues often sit at an intermediate level of complexity. Analogously to
crime, contagious diseases are characterized by simple metrics and by local processes of
social contact between individuals. Thus, containment or eradication of cholera or of
contagious diseases for which vaccination exists, represent some of the greatest successes
of urban policy 5 . But conditions that play out over longer times 6 and may have more
complex and diffuse social causation, such as chronic diseases or even HIV/AIDS have
proven thus far more difficult.
Thus, the simplicity of performance metrics expressed as objective quantitative quantities
and knowledge of their proximate causes in space and time are the crucial conditions for
successful engineering inspired solutions. The important point to always bear in mind,
however, is that these quantities are relative to the properties of the controlling system –
the policy maker or the algorithm – such as their response times, so that with
technological and scientific evolution we may hope to fulfill Alfred Whitehead’s maxim
quoted at the beginning of this section, of achieving progress through the increasing
automation of solutions to once intractable problems 7 .
3. The Planner’s Problem
In order to describe a wicked-problem in sufficient detail, one has to develop an exhaustive
inventory of all conceivable solutions ahead of time. The reason is that every question asking for
additional information depends upon the understanding of the problem-- and its resolution --at
that time. [...] Therefore, in order to anticipate all questions (in order to anticipate all information
required for resolution ahead of time), knowledge of all conceivable solutions is required.
- Rittel and Webber, 1973 [30].
The scenarios developed in the previous section paint a well-defined path for the use of
data in cities, at odds with many decades of urban theory and practice. While many
physical and infrastructural aspects of cities appear at first sight manageable through
engineering practices, many social and economic issues are in fact ‘wicked problems’
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
more prosperous, careful matching between housing characteristics and socioeconomic household
capacities and close monitoring of housing problems and prompt response, in terms of conflict
resolution, maintenance and repair.
5
Dealing with the problem of cholera, by John Snow in London in the 1840s-50s, constitutes one
of the first examples of a statistical approach to public health, though one thoroughly based on
“shoe-leather epidemiology” [50].
6
An exception to this argument is influenza. It is a ‘fast’ disease that has defied public health
response times. Its symptoms are generic and it typically is a large-scale problem, affecting 20-
30% of a population over large spatial regions.
7
The arrow of progress made possible by engineering and automation does not always run in the
same direction. The self-driving car solution that motivated this section works only as long as
most cars are driven by people. Once most cars are self-driven and share the same fast response
times, new instabilities will develop. A preview of such problems can be seen in stock markets
today, where it has been estimated that maybe over 80% of trades are made by algorithms on
ultrafast time-scales [51]. Often such algorithms act on news written by other ultrafast algorithms.
	
  
7	
  	
  
[30]. Over the long run the social and infrastructural aspects of the city are always
entangled.
Wicked problems were originally defined by two urban planners, Rittel and Webber, in
1973 [30]. The essential character of wicked problems is that they cannot be solved in
practice by a (central) planner. I shall reformulate some of their arguments in modern
form in what I like to call the “Planner’s Problem”. The Planner’s Problem has two
distinct facets: i) the knowledge problem and ii) the calculation problem.
The knowledge problem refers to the information (ultimately as data) that a planner
would need to map and understand the current state of the system; the city in our case.
While still implausible, it is not impossible to conceive information and communication
technologies that would give a planner, sitting in a ‘situation room’, access to detailed
information about every aspect of the infrastructure, services and social lives in a city.
Privacy concerns aside, it is conceivable that the lives and physical infrastructure of a
large city could be adequately sensed in several million places at fine temporal rates,
producing large but potentially manageable rates of information flow by current
technology standards 8 . In this way the knowledge problem is not a showstopper.
The calculation problem, however, alluded to in the quote at the beginning of this
section, refers to the computational complexity to perform the actual task of planning, in
terms of the number of steps necessary to evaluate all possible scenarios and choose the
best possible course of action. This problem is analogous to playing a complex game,
like chess, but with millions of pieces, each following different rules of interaction with
others, against millions of opponents, on a vast board. It will therefore not surprise the
reader that the exhaustive approach of evaluating each possible scenario in a city is
impractical as it involves the consideration of impossibly large spaces of possibilities.
I formalize this statement in the form of a theorem, and provide mathematical details and
the sketch of a proof in the Appendix. The result of considering the combinatorial
possibilities of a city is the creation and selection of the best detailed urban plan would
require a computation involving O[P(N)] steps in a city with population size N, where
P(N )  P 0 e N
1+δ
ln N
,
1
where δ  and P 0 is a constant, independent of N. For a city of a million people this is
6
6
P(N ) = P 0 10 6×10 , a truly astronomical number much, much larger than all the atoms in
the Universe. Thus, the Planner’s Problem is practically impossible.
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
8
For the largest city in the world, Tokyo with 35 million inhabitants, we could imagine sensing
each individual and a corresponding number of places, including different infrastructure and
services using, say, 100 million data streams, sampled at one byte each once a second (a faster
rate may be desirable for some applications, slower in others). This results in information flows
of the order of 10 9 bits per second (Gbps) that are already handled by Gigabit Internet and current
generation computing. So while still daunting these numbers can already be handled using current
technologies.
	
  
8	
  	
  
I want to emphasize that the sketch of a proof given in the Appendix relies on the
formulation of the Planner’s Problem as the choice of the best spatial and social
configuration for the city that maximizes asset of urban metrics. A more limited
conceptualization of the problem will likely have lower computational complexity, but
may not guarantee finding the best plan. In such a case, planning must rely on heuristics
and cannot guarantee absolute optimality.
Given these caveats, I have shown that planning the city in detail becomes
computationally impossible in all but the smallest towns. This indicates that the use of
complex models [39,53] in the detailed planning and management of cities has its limits
and cannot be exhaustively mapped and solved in general, regardless of how much data a
planner may acquire. How then should we think of planning under these limitations, and
what becomes the role of bigger and better data in cities?
4. Self-organization, Information and the Role of a Science of Cities
Provided that some groups on earth continue either muddling or revolutionizing themselves into
periods of economic development, we can be absolutely sure of a few things about future cities.
The cities will not be smaller, simpler or more specialized as cities of today. Rather, they will be
more intricate, comprehensive, diversified and larger than today’s and will have even more
complicated jumbles of old and new things as ours do. - Jane Jacobs, 1970 [54]
Part of the answer to the Planner’s Problem comes from economics, where the social
(central) planner faces a similar, if more limited, predicament of organizing economic
markets. The detailed information necessary to run a city in terms of planning the lives of
people and organizations is, of course, best know to those agents themselves. Even more
importantly, the motivation and capacity to make good decisions and act on this
information resides with the very same agents. Thus, cities are self-organizing and
depend crucially not on the detailed instructions from the planner’s ‘situation room’ but
rather on the integration and coordination of myriads of individual decisions, made
possible by suitable information flows 9 .
In this sense, cities have always been the primary creators and users of information and
communication technologies, from the daily newspaper, to postal mail, to the telegraph or
the (cellular) telephone [57]. It should then come as no surprise that new ICTs will
primarily act to enable cities, rather than compete with them. In fact, the problem that
ICTs address in specific niches, is the general problem solved by cities [58]: That is, the
integration and coordination of people and organizations through information sharing in
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
9 	
   The original argument for self-organization in the context of economics was made e.g. by
Friedrich Hayek in the essay “The Use of Knowledge in Society” [55], a title that the present
manuscript echoes. The central argument is that economies are primarily about information and
that the price system of economics is one (good) way of achieving the necessary self-organization
in terms of the allocations of goods and services, labor and capital. This point is also emphasized
by Paul Krugman in his 1992 Ohlin lectures [56].
	
  
9	
  	
  
vast social networks that can confer benefits to individual agents and to society as a
whole.
This then brings us to another perspective on the use of big data in cities, radically
different from the engineering framework of the previous sections. A vital function of
information and communication technologies is in enabling new and better ways for
people to be social. In this respect big data and associated technologies play only a
supportive role to social dynamics, not a prescriptive one. Data and technologies then do
not create or solve urban problems so much as they enable people and social organization
to address them better. This role of ICT has been discussed in the literature on smart
cities [19,21,25,26]. Especially important is the potentially more rapid access to civic and
economic participation of poor or excluded populations, particularly in developing cities
and nations [59].
The second part of the answer stems from the fact that bottom-up self-organization is
clearly not enough for a well functioning city. Many developing cities, think of Mumbai
[60], have self-organization in spades. Similarly, a flash-mob is as much the product of
self-organization as a new art form or a new business. For cities to work, general
properties and constraints that define the city on larger scales must be at play. Such
features, common to all cities, are the focus of urban theory.
Scientific theory is not about the details of a particular practical problem; it is about
general principles that apply across many problem instances. Given how different cities
are from each other and how much they change over time, to what extent can a scientific
theory of cities be possible or useful?
To illustrate this point consider another great triumph of science and engineering: the
Moon shot. It is not completely inconceivable that the Apollo 11 mission could have
been pulled off in the absence of knowledge of the laws of mechanics and gravity. One
could imagine controlling rocket thrusters using feedback by judging whether the
spacecraft was leaving Earth, approaching the Moon at the right speed not to crash, etc.
But of course this is not at all what happened: such solutions would be very difficult in
terms of engineering and computation at the time, especially because of the fast time
scales and large energies necessary for lift-off. Instead, we launch rockets based on their
general properties as prescribed by physical theory, the mass of the Earth and of the
spacecraft, the rotation of the planet, the speed necessary at lift-off, etc. This set of
calculations gives a general strategy for launching any spacecraft from Earth and to place
them in desired orbital positions.
What follows this general solution is a detailed, but much more limited, use of
engineering principles and self-organization to fine-tune solutions to the details of the
problem, such as the conditions for Moon landing given the particular spacecraft’s speed
at that moment, its distance to the surface, the human operator past actions, etc.
Thus, theory can enormously reduce the size of an engineering problem and in many
cases makes solutions possible. In this way, urban theory sets the general parameters for
	
  
10	
  	
  
any city to work as a whole as a bottom-up self-organizing process mediated by social
networks with certain general properties and enabled by infrastructure and services that
follow general performance metrics [58]. The details of these processes must then be
measured and managed, as appropriate, in every situation.
The present state of urban theory is fast evolving and is starting to approach the status of
an interdisciplinary science. New, bigger data that allows for the comparative quantitative
analysis of thousands of cities worldwide has been the key in determining the general
properties of cities and their variation due to both general conditions of development and
local histories [58,61,62]. We now have a quantitative framework that connects the
general structure of cities as vast social networks, to their general properties of
infrastructure networks and land use [58]. In this way we are uncovering what kind of
complex system cities really are [6,58,63] and the general conditions in terms of
infrastructure, energy and resource use, etc, that allow cities to function optimally as
distinct open-ended social systems, capable of socioeconomic growth and innovation in
human societies 10 .
5. Discussion
The last great technological advancement that reshaped cities was the automobile (some might
argue it was the elevator). In both cases, these technologies reshaped the physical aspects of
living in cities – how far a person could travel or how high a building could climb. But the
fundamentals of how cities worked remained the same. What’s different about the information
age that has been ushered in by personal computers, mobile phones and the Internet is its ability
to reshape the social organization of cities and empower everyday citizens with the knowledge
and tools to actively participate in the policy, planning and management of cities.
- Christian Madera, 2010 [66]
Developed cities today are social and technical complex systems characterized by
historically unprecedented levels of diversity and temporal and functional integration.
This growing individual specialization and interdependence makes large cities extremely
diverse and crucially relies on fine temporal and spatial integration and on faster and
more reliable information flows. These processes lie at the core of what makes cities the
economic and cultural engines of all human societies.
A city of 35 million people, like present day Tokyo, is made possible by extremely
efficient transportation, reliable energy and water supply and good social services,
including a very low level of conflict. Many developing urban areas in the world today
will have to replicate and improve upon these metrics if they are to fulfill their promise to
become world cities and enable the pace of human development expected by millions of
their inhabitants.
	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  	
  
10 	
   While recent progress in urban theory has allowed us to better understand cities as multi-level
interacting networks and as processes of self-organization across scales, more work needs to be
done regarding heterogeneous aspects of the city (inequality of incomes, variations across
neighborhoods [64]) and the evolution of cities over time, including the processes of social
development and economic growth [54,65].
	
  
11	
  	
  
I have showed here that the increasing separation of time-scales between information and
communication technologies and the natural pace of human behavior will enable many
fundamentally new engineered solutions that may solve important age-old urban
problems. Thus, new sensing, communications and computational technologies will play
a vital role in enabling new urban scale efficiencies and city socioeconomic growth. In
this sense we should expect that with urban development, services and infrastructure
should become increasingly engineered, invisible, predictable and automatic. How
resilience to infrastructure and service failures may be maintained in these circumstances
is an important open problem. Important as they are such solutions should always be
understood to be short-term and to have a limited scope, as they must continue to adapt
and enable socioeconomic development over the long run.
The world’s most vibrant and attractive cities are not usually the same places where the
buses run impeccably on time. While improvements in infrastructure and urban services
are absolutely necessary for cities to function better, they are not the fundamental sources
of social development or economic growth. These, more fundamental functions of cities,
rely on processes of social self-organization and on the fulfillment of general conditions
that allow cities to operate effectively as multi-level open-ended evolving networks. It is,
curiously, this more fundamental character of cities, at play across scales from the
individual to the city as a whole and from seconds to centuries, that creates and solves the
Planner’s Problem.
In this light, the key to making cities even smarter, through new uses of information and
communication technologies, is not very different from what it has always been. Big data
creates more opportunities, not less, for cities to accentuate their urban character: The
primary uses of big data in cities must then be to continue to enable the creation of new
knowledge by more people, not replace it.
Acknowledgements
I thank the participants of the workshop “How	
  Far	
  Can	
  “Big	
  Data	
  Take	
  Us	
  Towards	
  
Understanding	
  Cities?“ at the Santa Fe Institute September 19-21, 2013, where a version
of this paper was presented. This	
  research	
  was	
  partially	
  supported	
  by	
  the	
  Rockefeller	
  
Foundation,	
  the	
  James	
  S.	
  McDonnell	
  Foundation	
  (grant	
  no.	
  220020195),	
  the	
  John	
  
Templeton	
  Foundation	
  (grant	
  no.	
  15705),	
  the	
  Army	
  Research	
  Office	
  (grant	
  no.	
  
W911NF1210097),	
  the	
  Bill	
  and	
  Melinda	
  Gates	
  Foundation	
  (grant	
  OPP1076282)	
  and	
  
by	
  a	
  gift	
  from	
  the	
  Bryan	
  J.	
  and	
  June	
  B.	
  Zwan	
  Foundation.	
   	
  
References
1. J. Rodin in the foreword to B. Katz, J. Bradley, The Metropolitan Revolution: How
Cities and Metros Are Fixing Our Broken Politics and Fragile Economy. (Brookings
Institution Press, Washington DC, 2013).
	
  
12	
  	
  
2. M. Hilbert, P. López, The World's Technological Capacity to Store, Communicate, and
Compute Information, Science 332: 60–65 (2011).
3. V. Mayer-Schonberger, K. Cukier, Big Data: A Revolution That Will Transform How
We Live, Work, and Think (Eamon Dolan/Houghton Mifflin Harcourt, New York NY,
2013)
4. UN-Habitat, Planning Sustainable Cities — Global Report on Human Settlements
2009. Available http://www.unhabitat.org/content.asp?typeid=19&catid=555&cid=5607
5. L. Wirth, Urbanism as a Way of Life, The American Journal of Sociology, 44: 1-24
(1938).
6. L.M.A. Bettencourt, The Kind of Problem a City is, in D. Offenhuber and C. Ratti.
(eds) Die Stadt Entschlusseln: Wie Echtzeitdaten Den Urbanismus Verandern: Wie
Echtzeitdaten den Urbanismus verändern. (Birkhauser, Heidelberg, 2013). Available (in
english) http://www.santafe.edu/media/workingpapers/13-03-008.pdf
7. UN-HABITAT, The Challenge of Slums: Global Report on Human Settlements (2003).
8. J. Polonetsky, O. Tene, Privacy and Big Data: Making Ends Meet, 66 Stanford Law
Review 2. Available http://www.stanfordlawreview.org/online/privacy-and-big-
data/privacy-and-big-data
9. d. boyd, K. Crawford, Critical Questions for Big Data, Information, Communication
and Society 15: 662-679 (2012).
10. McKensey Global Institute, Big data: The next frontier for innovation, competition,
and productivity (2011). Available
http://www.mckinsey.com/insights/business_technology/big_data_the_next_frontier_for_
innovation
11. J. Riis, How the Other Half Lives: Studies among the Tenements of New York
(Kessinger Publishing, 2004). Originally published in 1890. Available
http://www.authentichistory.com/1898-1913/2-progressivism/2-riis/index.html
12. P. L. Szanton, Analysis and Urban Government: Experience of the New York City-
Rand Institute, Policy Sciences 3: 153-161 (1972).
13. J. Flood, The Fires: How a Computer Formula, Big Ideas, and the Best of Intentions
Burned Down New York City and Determined the Future of Cities (Riverhead Books,
New York, NY, 2011)
	
  
13	
  	
  
14. D.V. Gibson, G. Kozmetsky, R. W. Smilor (eds.), The Technopolis Phenomenon:
Smart Cities, Fast Systems, Global Networks. (Rowman & Littlefield, New York NY,
1992).
15. M. Castells, The Informational City: Information Technology, Economic
Restructuring, and the Urban-Regional Process (Blackwell, Oxford, UK, 1989).
16. M. Batty, The Computable City. In: Fourth International Conference on Computers in
Urban Planning and Urban Management, Melbourne, Australia, July 11-14 (1995).
Available http://www.acturban.org/biennial/doc_planners/computable_city.htm
17. S. Graham (ed.), The Cybercities Reader. (Routlege, London, 2004).
18. W. J. Mitchell, Me++: the cyborg self and the networked city, (The MIT Press,
Cambridge MA, 2004)
19. H. Partridge, Developing a human perspective to the digital divide in the Smart City
(2004). Available http://eprints.qut.edu.au/1299/1/partridge.h.2.paper.pdf
20. R. Giffinger, et al., Smart Cities Ranking of European Medium-Sized Cities.
Centre of Regional Science, Vienna University of Technology, Vienna, Austria
(2007). Available http://www.smart-cities.eu/
21. J. M. Shapiro, Smart cities: quality of life, productivity, and the growth effects of
human capital, The Review of Economics and Statistics 88: 324-335 (2008).
22. S. H. Lee, J. H. Han, Y.T. Leem, T. Yigitcanlar, Towards ubiquitous city: concept,
planning, and experiences in the Republic of Korea. In: T. Yigitcanlar, K. Velibeyoglu, S.
Baum (eds.) Knowledge-Based Urban Development: Planning and Applications in the
Information Era, pp. 148–170. (Information Science Reference, Hershey, 2008).
23. A. Caragliu, C. Del Bo, P. Nijkamp, Smart Cities in Europe. VU University
Amsterdam, Faculty of Economics, Business Administration and Econometrics (Research
Memoranda 0048, Amsterdam, The Netherlands, 2009).
24. C. Harrison, B. Eckman, R. Hamilton, P. Hartswick, J. Kalagnanam, J. Paraszczak, P.
Williams, Foundations for Smarter Cities, IBM J. Res. Develop. 54: 1-16 (2010).
25. N. Taewoo, T. Pardo, Conceptualizing Smart City with Dimensions of Technology,
People, and Institutions, The Proceedings of the 12th Annual International Conference on
Digital Government Research (2011). Available
http://dl.acm.org/citation.cfm?id=2037602
	
  
14	
  	
  
26. M. Batty, K.W. Axhausen, F. Giannotti, A. Pozdnoukhov, A. Bazzani, M.
Wachowicz, G. Ouzounis, and Y. Portugali, Smart cities of the future, The European
Physical Journal Special Topics 214: 481–518. (2012).
27. S. Roche, N. Nabian, K. Kloeckl, C. Ratti, Are ‘Smart Cities’ Smart Enough? In:
Global Geospatial Conference. Global Spatial Data Infrastructure Association (2012).
Available http://www.gsdi.org/gsdiconf/gsdi13/papers/182.pdf
28. R. Papa, C. Gargiulo, A. Galderisi, Towards an Urban Planner’s Perspective on Smart
City, TeMA 1: 5-17 (2013).
29. K. J. Åström, R. M. Murray, Feedback Systems: An Introduction for Scientists and
Engineers (Princeton University Press, Princeton NJ, 2008). Available
http://www.cds.caltech.edu/~murray/amwiki/index.php/Main_Page
30. H.W.J. Rittel, M. M. Webber, Dilemmas in a General Theory of Planning, Policy
Sciences 4: 155-169 (1973).
31. C. More and S. Mertens, The Nature of Computation (Oxford University Press, New
York NY, 2011)
32. P. W. Anderson, More is Different, Science 177: 393-396 (1972).
33. Alfred N. Whitehead An Introduction to Mathematics (Williams & Norgate, London,
1911).
34. Team Caltech at the DARPA Grand Challenges:
http://archive.darpa.mil/grandchallenge/TechPapers/Cal_Tech.pdf
35. Google driverless car: http://en.wikipedia.org/wiki/Google_driverless_car
36. M. M. Moore, B. Lu, Autonomous Vehicles for Personal Transport: A Technology
Assessment (June 2, 2011). Available http://ssrn.com/abstract=1865047 or
http://dx.doi.org/10.2139/ssrn.1865047
37. C. Anderson, The End of Theory: The Data Deluge Makes the Scientific Method
Obsolete, Wired 06.23.08 http://www.wired.com/science/discoveries/magazine/16-
07/pb_theory
38. J.S.J. Ren , W. Wang , S. S. Liao, Optimal Control Theory in Intelligent
Transportation Systems Research - A Review, arXiv:1304.3778 [cs.SY]
	
  
15	
  	
  
39. C. Harrison, Roads to Smarter Cities (2013). Available upon request from the author.
40. MASDAR City. http://www.masdarcity.ae/en/
41. M.B. Short, M.R. D'Orsogna, P.J. Brantingham, and G.E. Tita, Measuring and
modeling repeat and near-repeat burglary effects, Journal Quantitative Criminology 25:
325-339 (2009).
42. J. Rubin, Stopping crime before it starts, Los Angeles Times August 21, 2010.
Available
http://articles.latimes.com/2010/aug/21/local/la-me-predictcrime-20100427-1
43. IBM Global Business Services, A Vision of Smarter Cities: How Cities Can Lead the
Way into a Prosperous and Sustainable Future, (IBM Institute for Business Value,
Somers, NY, 2009)
44. Cisco’s smart city framework (2012):
http://www.cisco.com/web/about/ac79/docs/ps/motm/Smart-City-Framework.pdf
45. Siemens’ Smart City in Vienna: Model Project Aspern (2013).
http://www.siemens.com/innovation/en/news/2013/e_inno_1319_1.htm
46. Smart Cities Council: http://smartcitiescouncil.com
47. European Union’s Report of the Meeting of Advisory group
ICT Infrastructure for energy-efficient buildings and neighborhoods for carbon-neutral
cities (2011)
http://ec.europa.eu/information_society/activities/sustainable_growth/docs/smart-
cities/smart-cities-adv-group_report.pdf
48. D. J. Dwyer (Ed.), Asian Urbanization, a Hong Kong Casebook. (Hong Kong
University Press, Hong Kong, 1971).
49. W. Fernandez, Our Homes: 50 Years Of Housing A Nation (Housing and
Development Board, Singapore, 2011).
50. S. B. Johnson, The Ghost Map: The Story of London's Most Terrifying Epidemic -
and How it Changed Science, Cities and the Modern World (Riverhead Books, New
York NY, 2007)
51. N. Johnson, G. Zhao, E. Hunsader, H. Qi, N. Johnson, J. Meng, B. Tivnan, Abrupt
rise of new machine ecology beyond human response time, Nature Scientific Reports 3,
Article number: 2627 (2013).
52. Historical Internet Traffic Rates: http://en.wikipedia.org/wiki/Internet_traffic
	
  
16	
  	
  
53 Y. Portugali, Complexity, Cognition and the City (Springer, Heidelberg, 2011)
54. J. Jacobs, The economy of cities (Vintage, New York, NY, 1970).
55. F. Hayek, The Use of Knowledge in Society, American Economic Review. 35: 519-
530 (1945).
56. P. Krugman, Development, Geography, and Economic Theory (Ohlin Lectures) (MIT
Press, Cambridge MA, 1997).
57. P. Hall, Cities in Civilization (Pantheon Books, New York NY, 1998)
58. L. M. A. Bettencourt, The Origin of Scaling in Cities, Science 340: 1438-1441
(2013).
59. C. Andris, L.M.A. Bettencourt, Development, Information and Social Connectivity in
Côte d’Ivoire, Santa Fe institute Working Paper Paper #: 13-06-023. Available
http://www.santafe.edu/media/workingpapers/13-06-023.pdf
60. S. Mehta, Maximum City: Bombay Lost and Found (Vintage, New York NY, 2005).
61. L. M. A Bettencourt, J. Lobo, D. Helbing, C. Kühnert, G. B. West, Growth,
Innovation, Scaling, and the Pace of Life in Cities. Proc. Natl. Acad. Sci. U.S.A. 104,
7301-7306 (2007).
62. L.M.A. Bettencourt, J. Lobo, D. Strumsky, G.B. West, Urban Scaling and Its
Deviations: Revealing the Structure of Wealth, Innovation and Crime across Cities. PLoS
ONE 5(11): e13541 (2010).
63 J. Jacobs, The Death and Life of Great American Cities (Random House, New York,
1961).
64. R. J. Sampson, J. D. Morenoff, T. Gannon-Rowley, Assessing “Neighborhood
Effects”: Social Processes and New Directions in Research, Annual Review Sociology
28:443–478 (2002).
65. E. L. Glaeser, H. D. Kallal, J. A. Scheinkman and A. Shleifer, Growth in Cities,
Journal of Political Economy 100: 1126-1152 (1992).
66. C. Madera “The Future of Cities in the Internet Era” , Next City (2010). Available
http://nextcity.org/daily/entry/the-future-of-cities-in-the-internet-era
67. J. M. Harris, J. L. Hirst, M. J. Mossinghoff, Combinatorics and Graph Theory
(Springer, New York NY, 2008).
	
  
17	
  	
  
68. M. Schläpfer, et al. The Scaling of Human Interactions with City Size. Available
http://arxiv.org/abs/1210.5215
69. K. J. Arrow, A Difficulty in the Concept of Social Welfare, Journal of Political
Economy 58: 328–346 (1950).
Appendix
The Planner’s Problem
Definitions:
Consider a city with N people defined as a connected social network [57]. This means
that every person in the city can be connected to any other through a series of social links
between third parties, however long. The social network that characterizes the city is G,
which is made up of K social links. Both the general structure of G and its size K are
functions of N and of other urban factors such as the costs (e.g. transportation, crime,
time) and benefits (wages, information) of social interaction in that city.
The city’s “performance” is monitored in terms of a set of quantities Y 1 ,Y 2 ,...,Y M . These
urban metrics may vary from case to case but I simply require that they are measurable
quantitatively. The important property of each Y i is that it is a function of the city’s social
configuration, that is Y i (G). The metrics Y i can measure familiar things, such as the size
of city’s economy (GDP), the state of its public health, urban crime rates, the cost of
transportation, indices of environmental sustainability, etc, including more subjective
quantities such as happiness, or satisfaction. I show below that what is measured and how
many indicators are considered is not substantially important. The defining property is
that they are functions of the social networks that make up the city, G.
Problem Statement:
Given these definitions, the Planner’s Problem is to choose the plan associated with the
best G, subject on K remaining constant, so that a suitable function of the city’s metrics
Y i is maximized.
Sketch of proof:
The Planner’s Problem is computationally intractable:
Without more specific knowledge, consider a city of N people characterized by all
possibilities for social connections between them, K P . For example, in the simplest case
that these are undirected links we have K P = N(N −1) / 2. Within this space the planner
	
  
18	
  	
  
needs to determine the plan that assigns K connections optimally. The number of plans,
P, therefore is given by the number of combinations of K in K P [67] or
P(K P , K ) =
K p !
ln K !−ln K!−ln(K p −K )!
B(K ,K )
= e p
≡ e p .
K!(K p − K )!
(S1)
For large K P , K I can use Stirling’s approximation to the factorial
ln n!= n(ln n −1)+ O(ln n),
(S2)
in the exponent B(K P , K ) . After some algebra I obtain,
⎡ ⎛ K ⎞ ⎤
⎛ K ⎞
B(K P , K )  K ⎢ ln ⎜⎜⎜ P ⎟⎟⎟ −1 ⎥  K ln ⎜⎜⎜ P ⎟⎟⎟ .
⎢ ⎣ ⎝ K ⎠ ⎥ ⎦
⎝ K ⎠
(S3)
Now, urban scaling theory [58] and cell phone data [68], tell us that K = K 0 N 1+δ , where
1
K 0 is a constant of order a few connections, and δ  . The number of potential
6
connections may vary somewhat depending on additional constraints on the problem, but
the simplest assumption, given above, is that K P ~ N 2 (Metcalfe’s Law).
Thus, with these choices I obtain,
B(N )  B 0 N 1+δ ln N.
(S4)
Then, the number of plans that need to be evaluated to make the best choice is
P(N )  P 0 e N
1+δ
ln N
(S5)
,
which grows faster than exponentially with the number of people in the city. This is an
astronomical number, for example, for a million people N = 10 6 , I obtain
7
6
P(N ) = P 0 e 1.4×10 = P 0 10 6×10 ,
(S6)
much, much larger than all the atoms in the universe (~10 82 ).
The number of computational steps needed to evaluate the best plan is set by the
evaluation of the quantities Y i for each plan. Regardless of details, this can be done in ~
M ×P steps, plus the sorting of these plans by these values. The fastest sorting
algorithms require at least P steps ( P log P is more typical), so I conclude that the
complete algorithm would have a computational complexity proportional to at least
	
  
19	
  	
  
P(N ). Such algorithm would scale with the population size of the city N, faster than
exponentially and would be impractical in all but the smallest towns.
Algorithms that scale exponentially or faster with the size of the input set are considered
technically intractable, that is, they cannot be evaluated in practice as this set gets large.
Thus, the (computational) planner’s problem, under the present set of assumptions, is
intractable.
Notes:
1.
The detailed combinatorial arguments involved in a specific algorithm for the
Planner’s Problem are likely to be more constrained if we include considerations of time,
budget, etc. This may result in effectively a smaller number of potential connections
being viable, though it is uncertain whether such a judgment can be made a priori,
without the consideration of all possibilities as done above. In any case note that the
leading term in the exponent B is set by the number of social connection K in the city, not
K P . As such, these considerations play a sub-leading role in the calculation.
2.
Given a number of urban indicators M ≥ 3 there are well-known difficulties in
producing an ordering of improvements that satisfies most inhabitants in the city. This is
related to Arrow’s Impossibility Theorem [69]. Such problems arise in addition to the
computational complexity of the Planner’s Problem.
3.
Decisions and actions by individuals and organizations based on information they
obtain in their urban environments effectively solve the Planner’s Problem. They do so
by “parallelizing” the problem and its inference and computations through the
simultaneous pursuit of local adaptations that, in principle at least, can maximize each
agent’s preferences under constraints (budgets, knowledge, time, etc). This self-
organizing dynamics is not guaranteed to produce the best outcomes city-wide, though. A
more formal argument for self-organization in cities and what it can and cannot achieve
remains an open problem that includes, but also needs to transcend, some of the
foundational results in microeconomic theory.
	
  
20	
  Changing Engines in Midstream: A Java Stream
Computational Model for Big Data Processing
Xueyuan Su, Garret Swart, Brian Goetz, Brian Oliver, Paul Sandoz
Oracle Corporation
{ First.Last } @oracle.com
ABSTRACT
With the addition of lambda expressions and the Stream
API in Java 8, Java has gained a powerful and expressive
query language that operates over in-memory collections of
Java objects, making the transformation and analysis of da-
ta more convenient, scalable and efficient. In this paper,
we build on Java 8 Stream and add a DistributableStream
abstraction that supports federated query execution over an
extensible set of distributed compute engines. Each query
eventually results in the creation of a materialized result
that is returned either as a local object or as an engine de-
fined distributed Java Collection that can be saved and/or
used as a source for future queries. Distinctively, Distributa-
bleStream supports the changing of compute engines both
between and within a query, allowing different parts of a
computation to be executed on different platforms. At exe-
cution time, the query is organized as a sequence of pipelined
stages, each stage potentially running on a different engine.
Each node that is part of a stage executes its portion of the
computation on the data available locally or produced by
the previous stage of the computation. This approach allows
for computations to be assigned to engines based on pric-
ing, data locality, and resource availability. Coupled with
the inherent laziness of stream operations, this brings great
flexibility to query planning and separates the semantics of
the query from the details of the engine used to execute it.
We currently support three engines, Local, Apache Hadoop
MapReduce and Oracle Coherence, and we illustrate how
new engines and data sources can be added.
1.
INTRODUCTION
In this paper, we introduce DistributableStream, a Java
API that enables programmers to write distributed and fed-
erated queries on top of a set of pluggable compute engines.
Queries are expressed in a concise and easy to understand
way as illustrated in the WordCount example shown in Pro-
gram 1. The framework supports engines that are disk or
memory based, local or distributed, pessimistic or optimistic
This work is licensed under the Creative Commons Attribution-
NonCommercial-NoDerivs 3.0 Unported License. To view a copy of this li-
cense, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain per-
mission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 40th International Conference on
Very Large Data Bases, September 1st - 5th 2014, Hangzhou, China.
Proceedings of the VLDB Endowment, Vol. 7, No. 13
Copyright 2014 VLDB Endowment 2150-8097/14/08.
fault handling. Distinctively it can also federate multi-stage
queries over multiple engines to allow for data access to be
localized, or resource utilization to be optimized. Since Dis-
tributableStream is a pure Java library with an efficient local
implementation, it can also scale down for processing small
amounts of data within a JVM.
Program 1 WordCount
public static Map<String, Integer> wordCount(
DistributableStream<String> stream) {
return stream
.flatMap(s -> Stream.of(s.split("\\s+")))
.collect(DistributableCollectors
.toMap(s -> s, s -> 1, Integer::sum)); }
The contributions of our work include:
• A DistributableStream interface for processing Big Da-
ta across various platforms. The abstraction frees de-
velopers from low-level platform-dependent details and
allows them to focus on the algorithmic part of their
design. At execution time, stream applications are
translated into query plans to be executed on the con-
figured compute engines.
• A pluggable Engine abstraction. This design separates
engine-specific configuration from the stream compu-
tational model and the modular design makes adding
new JVM-enabled engines straightforward, supporting
negotiable push or pull-based data movement across
engine boundaries. Applications developed with the
DistributableStream API can be reused on different
engines without rewriting any algorithmic logic.
• Three engine implementations – two distributed en-
gines, Apache Hadoop and Oracle Coherence, and a
local engine, layered on the Java 8 Stream thread pool
where local computations are executed in parallel on
multiple cores – serve to validate and commercialize
this design. Our implementation approach allows for
both high-level query planner and low-level compiler
optimizations. On the high level, we use explicit data
structures to describe the processing steps and allow
an execution planner to manipulate these data struc-
tures to optimize the movement of data and state be-
tween the engines. On the low level, each portion of
a DistributableStream is assembled as a local Java 8
Stream which arranges its code to make Java Just-in-
Time based compilation optimizations applicable.• Implementations of Java 8 Streams for various external
data sources, including InputFormatStream for access-
ing HDFS and other Hadoop data sources and Named-
CacheStream for accessing Oracle Coherence; and Java
8 Stream Collector implementations, such as Output-
FormatCollector and NamedCacheCollector, for stor-
ing the results of stream computations into Apache
Hadoop and Oracle Coherence.
• Federated query model: Data streams connect fusible
operators within a single engine pipeline and opera-
tors running in parallel on different engines, forming
a clean and easy to understand federated query mod-
el. The ability to exploit multiple engines within a
single query allows us to most efficiently exploit disk
arrays, distributed memory, and local memory for per-
forming bulk ETL, iterative jobs, and interactive ana-
lytics, by applying the appropriate compute engines
for each processing stage. For example, combining
Hadoop with a distributed memory engine (Coherence
or Spark) and an SMP, we can read input data from
HDFS, cache and efficiently process it in memory, and
perform the final reduction on a single machine.
• Applications are the proof of any programming sys-
tem and WordCount is not enough. For that reason
we show this interface at work for distributed reservoir
sampling, PageRank, and k-means clustering. These
examples demonstrate the expressiveness and concise-
ness of the DistributableStream programming model.
We also run these examples on different combinations
of engines and compare their performance.
A variety of features are incomplete as of this writing,
including a global job optimizer, job progress monitoring,
and automatic cleanup of temporary files and distributed
objects dropped by client applications. In addition we have
plans to support other engines and data sources, such as
Spark, Tez, and the Oracle Database, high on our list.
Our discussion starts with an overview of Java 8 Stream
(§2) and DistributableStream(§3). We then present detailed
discussions on the design and implementation of Distributa-
bleStream (§4), example stream applications (§5), and ex-
perimental evaluation (§6). Finally, we explore work that
are promising for future development (§7), survey related
work (§8), and conclude the paper (§9).
2.
STREAM IN JAVA 8
Java SE 8 [12] is the latest version of the Java platform
and includes language-level support for lambda expressions
and the Stream API that provides efficient bulk operations
on Java Collections and other data sources.
2.1
2.1.1
Prerequisite Features for Supporting Stream
Lambda Expressions
A Java lambda expression is an anonymous method con-
sisting of an argument list with zero or more formal param-
eters with optionally inferred types, the arrow token, and a
body consisting of a single expression or a statement block.
(a,b) -> a+b is a simple example that computes the sum
of two arguments, given that the enclosing environment pro-
vides sufficient type information to infer the types of a, b.
Lambda expressions are lifted into object instances by a
Functional Interface. A functional interface is an interface
that contains only one abstract method. Java 8 predefines
some functional interfaces in the package java.util.function,
including Function, BinaryOperator, Predicate, Supplier, Con-
sumer, and others. Concise representation of a lambda ex-
pression depends on target typing. When the Java compiler
compiles a method invocation, it has a data type for each ar-
gument. The data type expected is the target type. Target
typing for a lambda expression is used to infer the argument
and return type of a lambda expression. This allows types to
be elided in the definition of the lambda expression, making
the expression less verbose and more generic.
The Java 8 Stream API is designed from the ground up
for use with lambda expressions, and encourages a stateless
programming style which provides maximum flexibility for
the implementation.
2.1.2
Dynamic Compilation
The Just-in-Time compiler is an important technique used
to optimize the performance of programs at runtime, inlining
methods, removing redundant loads, and eliminating dead
code. The design of the Java 8 Stream API plays naturally
into the strength of such dynamically-compiling runtime,
exploiting its ability to identify and dynamically optimize
critical loops. This design allows the performance of streams
to continue to improve as the Java runtime environment
improves, resulting in highly efficient stream execution.
An important design principle is to ensure the classes for
methods invoked in critical loops can be determined by the
JIT when the loop is compiled, as this allows the inner meth-
ods to be inlined and the loop to be intelligently unrolled.
2.2
The Stream Computational Model
Java Stream is an abstraction that represents a sequence
of elements that support sequential and parallel aggregate
operations. A stream is not a data structure that stores ele-
ments but instead it conveys elements from a source through
a pipeline of computational operations. An operation on a
stream produces a result without modifying its source.
A stream pipeline consists of a source, zero or more in-
termediate operations, and a terminal operation. The ele-
ments of a stream can only be visited once during the life of
a stream. If data is needed as input to several operations, a
new stream must be generated to revisit the elements of the
source, or the data must be buffered for later use.
2.2.1
Streams: Data in Motion
A stream is an abstract representation of possibly un-
bounded elements from an array, a Collection, a data struc-
ture, a generator function, an I/O channel, and so on. In
contrast with lazy data sets such as Pig Latin relations [4]
and Spark RDDs [5], streams conceptually represent data in
motion. As we will see in Section 3, our stream model not
only allows data to easily flow between different components
within a compute engine, but also to conveniently flow from
one engine into another. This makes the federated query
model conceptually clean and easy to understand.
2.2.2
Stream Transforms: Intermediate Operations
Stream transforms, also called intermediate operations in
the API, return a new stream from a stream and are pro-
cessed lazily. A lazy operation is deferred until the finalresult is requested. Lazy operations avoid unnecessary early
materialization and offer potential optimization opportuni-
ties. For example, filtering, mapping, and reducing in a
filter-map-reduce pipeline can be fused into a single pass
over the data with minimal intermediate state. Laziness al-
so avoids unnecessary stream traversal so that a query like
“find the first item that matches the predicate” may return
before examining all elements in the stream.
Commonly used intermediate operations include filter, map,
flatMap, distinct, and sorted. Note that sorted is a blocking
operation but not a terminal operation. The sorted oper-
ation requires incorporating state from previously seen ele-
ments when processing new elements.
2.2.3
Collectors and Terminal Operations
Terminal operations trigger the traversal of data items
and consume the stream. Two commonly used terminal op-
erations are reduce and collect. Reduce is a classical func-
tional fold operation, performing a reduction on the ele-
ments of a stream using an associative accumulation func-
tion. However, users are more likely to use the collect op-
eration, which extends the notion of reduction to support
mutable result containers such as lists, maps, and buffers,
as is common in Java programs.
Supplier
Container
R container = collector.supplier().get();
for (T t : data)
collector
.accumulator()
.accept(container, t);
return collector.finisher().apply(container);
However, as illustrated in Figure 1, the collector can parti-
tion the input, perform the reduction on the partitions in
parallel, and then use the combiner function to combine the
partial results. There is a rich library of Collector building
blocks in Java SE 8 for composing common queries.
2.2.4
Spliterator and Parallelism
Java 8 Streams are built on top of Spliterators, which
combine sequential iteration with recursive decomposition.
Spliterators can invoke consumer callbacks on the elements
of the stream. It is also possible to decompose a Spliterator,
partitioning elements into another Spliterator that can be
consumed in parallel. The results of processing each Split-
erator can be combined inside of a Collector. Collection
implementations in the JDK have already been furnished
with a reasonable Spliterator implementation and most ap-
plication developers do not need to work directly with Split-
erators, instead writing to the Stream abstraction built on
top. The following is a query that computes the sum weight
of all red widgets in parallel,
Result
Data item
Accumulator
Container
Finisher
Combiner
Container
Data item
Supplier
Accumulator
Container
Container
Container
Container
Combiner
Figure 1: Parallel data reduction using a Collector.
The most general use of collect performs the mutable re-
duction using a Collector. A Collector is a mutable reduc-
tion operator that accumulates input elements into a muta-
ble result container, optionally transforming the accumulat-
ed result into a final representation after all input elements
have been processed. A Collector is defined by four func-
tions: a Supplier that creates and returns a new mutable
result container, an Accumulator that folds a value into a
result container, a Combiner that merges two result con-
tainers representing partial results, and an optional Finisher
that transforms the intermediate result into the final repre-
sentation. By default the finisher is an identity function.
With the new syntax of double colons for method reference
capture, the following code creates a collector for collecting
widgets into a TreeSet,
Collector.of(
TreeSet::new,
TreeSet::add,
(left, right) ->
{ left.addAll(right); return left; });
Collecting data with a Collector is semantically equivalent
to the following,
widgets
.parallelStream()
.filter(b -> b.getColor() == RED)
.mapToInt(b -> b.getWeight())
.sum();
Note that a query on a parallel stream over the same da-
ta collection is not guaranteed to perform better than on a
sequential stream. There are many factors that affect the
performance of parallel execution, including whether the un-
derlying data source can be efficiently split, how balanced
the splitting is, and what is the relative cost of the accumula-
tor and combiner functions. In order to achieve performance
improvement, one must carefully design the Spliterator as
well as the stream operators and collectors.
2.3
Challenges Not Addressed By Stream
The Java 8 Stream API provides expressive and efficient
ways of processing data inside a JVM, either sequentially or
in parallel. However, there are many challenges brought by
data sets that cannot fit in a single machine and thus require
distributed storage and processing. These challenges include
but are not limited to
• Querying against distributed data sources like HDFS,
Coherence, and distributed Databases;
• Shipping functions and results among multiple nodes;
• Storing results in distributed form other than single-
JVM Java Collections;
• Federating multiple compute engines for query pro-
cessing and global query optimization.
In the following sections, we present the distributable ver-
sion of the Stream API and describe how we address these
challenges in our design and implementation.3.
DISTRIBUTABLE STREAM
3.1
The Basic Idea
DistributableStream is a stream-like interface that pro-
vides similar filter/map/collect operations that can be ap-
plied to distributed data sets. At the high level, Distributa-
bleStream works by serializing the whole operation pipeline
and shipping them to worker nodes. Each worker node is
responsible for performing a local portion of the Distributa-
bleStream, usually from the data partitions that are locally
resident on this node. To do this, the stream pipeline is de-
serialized and applied directly to the local stream. Shipping
the stream pipeline to the place where data resides enables
efficient stream execution. Finally, these partial answers
from local streams are connected by some form of distribut-
ed collection to form a global view of the data. Additional
streams can be generated from such distributed collections
to be consumed by downstream applications. See Figure 2
for an illustration of this paradigm.
Compute Engine
Client Node
Computational
Stage
Distributed
job
optimizations
DistributableStream
Worker Node
Serialized ops
/ collectors
Deserialized ops
/ collectors
Runtime JVM
optimizations
Engine
specific
distributed
collection
Stream
Data
partitions
Local
collection
Data Storage
Figure 2: The workflow of DistributableStream.
We rely on serialization to achieve function shipping de-
scribed above. Serialization is the process of converting a
graph of object instances into a linear stream of bytes, which
can be stored to a file or sent over the network. A clone of the
original objects can be obtained by deserializing the bytes.
To facilitate this, we define Serializable subclasses of the Ja-
va 8 java.utils.function interfaces. By using these interfaces
as the arguments for DistributableStream, we ensure that
the lambda expressions can be serialized and the functions
can be shipped to the node where the data resides. We al-
so define a Serializable subclass for the Collector interface
as DistributableCollector and use this type in Distributa-
bleStream. A companion factory called DistributableCollec-
tors is provided to create commonly used collectors appro-
priate for DistributableStream.
3.2
The Engine Abstraction
An important feature of the DistributableStream frame-
work is its support for multiple compute engines and feder-
ated queries that over them. The Engine interface provides
a convenient abstraction for this purpose. Each implemen-
tation of DistributableStream has a corresponding instance
of the Engine interface that controls which compute engine
is to be used to execute the computation. Modern data cen-
ters have many clusters, supporting multiple engines. An
instance of Engine refers to one engine running on one clus-
ter. Calling the
DistributableStream.withEngine(Engine engine)
method switches the underlying compute engine from the
current one to the one specified in the argument, and re-
turns an instance of the DistributableStream associated with
the new engine. The Engine class is also used for passing
platform specific parameters, as well as providing an avenue
for two compute engines to negotiate the data movement
between them. The Engine abstraction makes the stream
computational model succinct and clean.
Currently we have support for three engines, LocalEngine,
MapReduceEngine, and CoherenceEngine. Supporting new
engines is convenient as the only requirement is to imple-
ment the Engine interface. We plan to add the support
for more distributed platforms such as Apache Spark and
Apache Tez in the future.
3.3
Extended Forms of Collect
In the standard Stream library, the collect() operation is a
terminal operation – it consumes a data set and produces a
summary result. However, more flexibility is needed in a dis-
tributed environment, so we extend the default terminating
collect in two ways in DistributableStream.
A method collectToStream() is added to the interface
that provides for a collecting operator that is blocking but
not terminating. CollectToStream() returns another Dis-
tributableStream so that we can build longer running op-
erations, reduce startup time and avoid materialization of
intermediate results. This approach also allows for opti-
mizations span beyond collect call boundaries.
We also define a subclass of Collector called ByKeyCol-
lector. It is introduced to instruct the implementation that
collection can be done independently for each key allowing
for less communication during the collection process. To do
this we insist that the result of such a collection is a ja-
va.util.Map. Instead of a more general combiner, used in
the base class, a merger is used to combine just the values
associated with the same key. By-key collect is especially
important when implementing the non-terminating collect-
ToStream() method on a distributed compute engine.
3.4
Engine Specific Distributed Collections
To facilitate the representation of distributed data sets
used for creating DistributableStreams and as the output of
a DistributableCollector, we introduce engine specific dis-
tributed collections. These are distributed materializations
of Java List and Map. Unlike lazily materialized Pig Latin
relations and Spark RDDs, they are always materialized. We
provide this distributed abstraction so that multiple engines
can be supported using the same Java 8 program. These col-
lections are not required to support normal operations such
as Map.get(), instead they are DistributableStream facto-
ries, providing access to the data they represent. These dis-
tributed collections can be named independently of the JVM
using engine specific naming scheme. Collections have a pre-
ferred engine, and by default, DistributableStreams created
from such a collection will use that engine. We also al-
low collections associated with one engine to be accessed by
another engine. If these engines are running on the same
cluster, as enabled by Apache Hadoop YARN [2], this can
be done without network transfers.
3.5
Creation of a DistributableStream
In summary, there are three ways to create a Distributa-
bleStream, (i) Use the Engine instance methods to createa DistributableStream over a persistent engine specific data
set, (ii) Build a DistributableStream from an engine specific
distributed collection created by DistributableStream.collect(),
(iii) Use the result of DistributableStream.collectToStream().
4.
DESIGN AND IMPLEMENTATION
In this section, we discuss our design and implementation
in greater detail. We will cover how we integrate Distributa-
bleStream with the native Java 8 Stream, and how we trans-
late stream computation into execution plans over Hadoop
MapReduce and Coherence, respectively.
4.1
DistributableStream and Stream
We considered three potential relationships for these two
interfaces, (i) Have Stream implement DistributableStream,
(ii) Have DistributableStream implement Stream, (iii) Pro-
vide efficient conversion operators. We chose (iii) but it is
instructive to consider the alternatives.
From a type-theory point of view (i) is the most attractive,
as Stream is logically a restriction of DistributableStream
that is bound to a local compute engine. This runs into a
series of technical problems. First of all Java does not sup-
port parameter contra-variance for method overrides. This
means that a method with a parameter restricted in the base
class, for example serializable functions and collectors, can-
not be overridden with a method that removes the param-
eter restriction. An alternative to contra-variance is poly-
morphism, two methods with the same name that accept
different parameters, for example Stream instead of overrid-
ing the operator methods taking serializable functions and
collectors, defines new methods that accept nonserializable
functions and collectors. When matching polymorphic func-
tions, Java picks the most restrictive function that the actual
argument can be coerced into. This means that potential-
ly serializable functions will match the DistributableStream
methods but not the Stream methods. Unfortunately the
Java implementation of Serializability adds additional over-
head to the creation of serializable instances which we do
not want to pay in the local case.
Alternative (ii) can be made to work but it has problems
because for a DistributableStream to implement Stream, it
must check that its parameters are Serializable at runtime.
If the parameter is not Serializable, it must either fail or
move to the local engine which does not require serializabil-
ity. Of course a DistributableStream can add polymorphic
operator methods that accept the serializable functions and
collectors, and as long as the arguments are Serializable,
these will be called in preference to the Stream methods.
But this is error prone as it will be easy for a programmer
to write a nondistributable computation thinking it is dis-
tributable, only to find out at runtime there is a problem.
This is why we ended up with the option (iii) of convert-
ing efficiently between DistributableStream and Stream. We
do this by defining an local engine called LocalEngine. Lo-
calEngine is a singleton class that represents an engine that
runs within this JVM and can be used to process Distributa-
bleStream computations. We provide a factory method in
LocalEngine that accepts a Stream and returns a Distributa-
bleStream associated with the local engine. The result-
ing DistributableStream is implemented by projecting each
method onto the local Stream hidden inside the LocalEngine
implementation of DistributableStream.
We handle the other direction by defining
DistributableStream.stream()
This pulls a local stream out of a distributable stream to
allow its result to be examined locally. If the Distributa-
bleStream is managed by LocalEngine, then it can just re-
turn the underlying Stream object directly. Otherwise it
can call withEngine(LocalEngine.getInstance()).stream() to
perform the equivalent operation.
Local Java 8 Streams are used to process both small and
large amounts of local data so we need to make both Stream
creation and per item processing very efficient. On the oth-
er hand, DistributableStreams need to be understood by
the distributed execution planner which must optimize the
movement of data and state between engines. Therefore, our
design supports the optimization of both use cases. The lo-
cal streams are structured to enable runtime JVM optimiza-
tions while the distributed tasks can be efficiently managed
by the DistributableStream library.
4.2
Mapping Streams into Job Plans
One interesting challenge in designing DistributableStream
is to translate stream computations into job plans. We break
stream computations into stages at the points where shuffle
is required. Shuffling data is generally needed in blocking
and terminal operators. For example, the following query
is mapped into two stages, “filter-flatMap-collectToStream”
followed by “map-collect”.
distributableStream
.filter(predicate)
.flatMap(firstMapper)
.collectToStream(firstCollector)
.map(secondMapper)
.collect(secondCollector);
We implement the translation lazily, similar to the way
Java 8 Stream implements operation pipelines. When an
intermediate operation is called on a DistributableStream,
the transformation is stored in a data structure referred to
by the new instance of DistributableStream returned. Only
when a terminal operation is called are both the transforma-
tions and the terminal operation serialized and shipped to
the appropriate engine’s worker nodes. This lazy approach
allows the execution of stream operations to be delayed until
a concrete result is needed, leaving opportunities for opera-
tion pipelining and job plan optimization.
For those compute engines that already support shuffle
operation, such as Hadoop MapReduce, we rely on it to
shuffle the data at the end of each stage. For underlying en-
gines without explicit shuffle operations, such as Coherence,
the Engine implementation will implement the shuffle itself.
4.3
MapReduceStream
MapReduceStream was our first attempt to implement
DistributableStream on a distributed engine. It supports
the execution of stream operations on Hadoop clusters. To
support MapReduceStream, there is no need to recompile
Hadoop source code with JDK8. The only requirement is to
install the Java 8 runtime environment (JRE8).
The first challenge we addressed in MapReduceStream
was function shipping. We provide three options, all using
Java serialization to serialize the pipeline and the functions
it references, to a sequence of bytes. (i) When the pipeline
being shipped does not reference large objects, we stringi-
fy the serialized pipeline and insert it into the Hadoop jobConfiguration object. (ii) When the pipeline does reference
large objects, this becomes inefficient so instead we store
the serialized pipeline into a file and add it to the Hadoop
DistributedCache. (iii) If the objects being referenced are
very large, we require the programmer to use a distributed
collection class with a smart serializer and deserializer so
that the reference to the distributed collection rather than
the actual data is included in the serialized pipeline.
Another challenge for MapReduceStream is dealing with
the strong typing in Hadoop. It is required that the in-
put and output key/value instances match the classes spec-
ified in the job configuration. While this is quite useful to
guarantee the correct job behavior, it is tedious and error-
prone to configure such parameters for jobs with multiple
rounds of map and reduce. Initially we provided a way for
users to embed these parameters to the Collector and made
the framework responsible for setting the job configuration.
This approach made stream application code less clean and
too specific to the MapReduceEngine – other engines may
not have such a strong typing requirement and we did not
want to pollute the DistributableCollector interface. We
decided to remove this Hadoop-specific constraint from the
interface and instead require users to specify the classes in
the Engine configuration at each step of the stream compu-
tation. For improved usability, we also provide a wrapper
class that can wrap any Serializable or Writable objects.
The framework can wrap and unwrap the data items if the
classes are set to this wrapper type. This saves effort in
configuring each computational stage at the cost of efficien-
cy due to wrapping and unwrapping.
Stream computations are broken into stages by blocking
and terminal operations. Each stage of MapReduceStream
is implemented by a single Hadoop job. Hadoop jobs are
only submitted when a terminal operation is called.
Hadoop Cluster
Reducer
Mapper
filter(predicate)
flatMap(mapper)
collector
Runtime JVM
optimizations
Stream
Container,
InputSplit
key/value
HDFS
collector
Shuffling
Combiner,
Merger
Local
collection
Container,
key/value
HDFS
Figure 3: Translating a stage to a Hadoop job.
Figure 3 shows how a single stage of a stream computation
is mapped to a Hadoop job in greater detail. In particular,
by assembling local stream from an InputSplit of a map-
per, we are able to apply the pipelined operations to data
items in one pass. This gives the same result as the native
Hadoop ChainMapper that runs several map phases inside
a single mapper, with the advantages of being inline-able,
avoiding configuring parameters for each map phase sepa-
rately, and providing in-memory partial merging opportuni-
ty before collecting records to the MapOutputBuffer. Note
that collecting into containers can happen at both mapper
and reducer side, with the distinction that the mapper side
never applies the optional finisher embedded in the collector.
We rely on the Hadoop framework to shuffle mapper output
to reducers. If the collector is a ByKeyCollector, individual
records are shuffled by key and efficient merging of values
associated with the same key is performed on the reducer
side; otherwise, all containers are sent to a single reducer
that combines them in pairs. At the end of this stage, all
local collections coming out of the reducers are logically put
together to form an engine specific distributed collection.
Then the next stage continues from there.
MapReduceEngine includes factories to generate MapRe-
duceStreams based on a Hadoop Configuration object, which
must contain properly configured parameters such as Input-
Format and InputPath. Three methods keyStream(conf),
valueStream(conf), and entryStream(conf) return a Dis-
tributableStream of keys, values, and entries, respectively.
4.4
CoherenceStream
Oracle Coherence [16] is an in-memory Java object store
equipped with computational capability. Our design and
implementation of CoherenceStream has benefited from the
lessons we have learned from MapReduceStream. We bor-
row ideas from MapReduceStream and adapt them to fit
requirements in the Coherence environment.
The first special treatment for CoherenceStream was to
add the support for streams without keys. The current
version of Coherence provides distributed in-memory map-
s but not lists. Therefore, storing stream elements into a
Coherence NamedCache requires that the framework gen-
erate unique keys for these items. One solution is to use
machine-specific auto-incrementing integers as the primary
key. The key structure we use contains an optional thread
id, a machine id, and an integer. Another approach is to use
a Coherence provided primary key generator that allows for
optimized data loading.
Another challenge comes from the fact that Coherence us-
es its own object format, the portable object format (POF),
to do object serialization. POF is language independent and
very efficient. Usually (de)serialization of POF object is sev-
eral times faster than the standard Java serialization and
the result is much smaller. Standard Java types have their
default serializer implemented in Coherence. However, user-
defined types have to either implement the PortableObject
interface themselves or register their own serializers that im-
plement the PofSerializer interface. This means that Writa-
bles that come with Hadoop and user-defined Serializables
that are not standard in Java cannot be serialized directly
into Coherence. We provide serializers for commonly used
Writable types and we also provide a wrapper class that
translates objects between the different serialization meth-
ods. However, we recommend that users write their own se-
rializers for their customized types if the performance penal-
ty caused by wrapping becomes significant.
Coherence does not have a similar framework like Hadoop
for accepting jobs, assigning mappers and reducers, and
shuffling data between them. Instead, we provide our own
implementation to achieve the same functionalities. This
framework is responsible for assigning an id to each job sub-
mission, starting necessary services for the job execution,
and keeping track of the job status and output.
The actual computation is implemented using distributed
Invocation Services. An invocation service allows the exe-
cution of single-pass agents called Invocable objects (i) on
a given node, (ii) in parallel on a given set of nodes, or(iii) in parallel on all nodes of the grid. We take a two-
step approach similar to MapReduceStream to implement
a single stage of the stream computation. For each stage,
the framework first starts an invocation service to run the
pipeline of intermediate operations and collect the results
into an intermediate MapperOutputCache. We call this step
the MapperService which has functionality corresponding to
a Hadoop Map task. Upon the completion of the Map-
perService, the framework starts another invocation service,
the ReducerService, to read from the intermediate cache and
write the final results into the output cache.
We have applied some optimization techniques to avoid
unnecessary data movement in CoherenceStream. A Named-
Cache in Coherence is partitioned and distributed across a
set of member nodes in the grid by the framework. A Map-
perService is started on the member nodes that own the
input cache, and we use a filter so that each Map task pro-
cesses only the locally available data from the input cache.
In Hadoop terminology, all Map tasks are data-local. The
OutputCollector accumulator is called by the MapperSer-
vice and buffers entries in local memory and flushes them to
the MapperOutputCache when a pre-configured threshold
is reached. The MapperOutputCache is implemented with
partition affinity so that all entries associated with the same
key are stored in the same partition, thus eliminating the
need to reshuffle the data before starting the ReducerSer-
vice. Like MapperService, the ReducerService is started on
member nodes storing the MapperOutputCache and, with
the help of a filter, each of them only processes its local
partitions. At the end of the ReducerService execution, the
final results are written to their local nodes with partition
affinity. See Figure 4 for an illustration of CoherenceStream
that corresponds to the MapReduceStream example in Fig-
ure 3. The ReducerService could also execute the map and
collect associated with the next stage of the computation
but we have not implemented this yet.
Coherence Grid
ReducerService
MapperService
filter(predicate)
flatMap(mapper)
collector
Runtime JVM
optimizations
Local
partitions
Stream
Container,
key/value
NamedCache
abstraction also provides an avenue for two compute engines
adjacent in the computation to negotiate the data movement
at their boundary. The two approaches for data movement
between engines is shown in Figure 5. By default, data
movement follows the push-based model where the upstream
engine directly writes the result to the downstream engine.
But in some cases, having the upstream engine materialize
the result itself and the downstream engine to pull from it
reduces the data transfer cost. Let us take a look at the
following example,
CoherenceEngine
.entryStream(conf)
.filter(predicate)
.withEngine(mapReduceEngine);
Instead of having Coherence to write entries into HDFS
files, we ask Hadoop to directly pull the data from the
NamedCache via the NamedCacheInputFormat. This strat-
egy saves two passes over the entire disk resident data set.
Upstream
Engine
Task
Downstream
Engine
Task
Push
Read
Data Storage
Data Storage
Upstream
Engine
Task
Write
Data Storage
Downstream
Engine
Task
Pull
Data Storage
Figure 5: Data movement between engines.
There is also an optimization called “short-circuiting”, as
illustrated by Figure 6, available to the special case where
the downstream engine simply pulls data from the upstream
engine without having the latter to do any preprocessing. In
this case, if the downstream engine is able to read directly
from the upstream storage, there is no need to run a separate
job in the upstream engine just for data movement.
collector
Partition
affinity
Combiner,
Merger
Container,
key/value
Local
collection
Upstream
Engine
Task
Downstream
Engine
Task
Pull
NamedCache
Data Storage
Data Storage
Figure 4: Translating a stage to invocation services.
We define a Configuration class to store job related config-
uration parameters. We also provide factory methods in Co-
herenceEngine to generate CoherenceStreams based on such
a Configuration object. Three methods keyStream(conf),
valueStream(conf), and entryStream(conf) return a Dis-
tributableStream of keys, values, and entries, respectively.
4.5
Changing Engines in Midstream
To support federated query, the DistributableStream frame-
work relies on the Engine abstraction to separate the engine-
specific details from the computational model. The Engine
Figure 6: Short-circuiting without upstream job.
A useful application of short-circuiting is the pulling of
external data specified by a Hadoop InputFormat directly
into the Coherence in-memory cache. This enables us to
pull data from Hadoop into memory similar to the creation
of a HadoopRDD in Spark.
MapReduceEngine
.entryStream(conf)
.withEngine(coherenceEngine);We have implemented factories for creating local streams
from various data sources, for example, InputFormatStream
from HDFS files or Hive tables and NamedCacheStream from
Coherence NamedCaches. We have the corresponding im-
plementations OutputFormatCollector and NamedCacheCol-
lector for collecting to Hadoop and Coherence. We use these
streams inside of our Engine implementations but they are
also useful for programmers writing nondistributed applica-
tions that want to access distributed data sets.
5.
EXAMPLE STREAM APPLICATIONS
5.1
Distributed Reservoir Sampling
Reservoir sampling is a family of randomized algorithms
for uniformly sampling k items from a base stream of n
items, where n is either very large or unknown in advance. A
simple O(n) algorithm maintains a reservoir list of size k and
runs in a single pass over the base stream. The algorithm
works by storing the first k items in the reservoir. For the
i-th item where k < i ≤ n, the algorithm includes it in the
reservoir with probability k/i. If this happens, a random
item from the reservoir is removed to keep the total as k.
To sample from a distributed stream in parallel, we extend
the basic algorithm and choose at most k samples uniform-
ly from each partition and then combine these samples by
choosing a subset so that each item from the base stream
has equal probability k/n of being chosen. To achieve this
goal, we implement a SampleContainer where the put(T
item) method implements the basic reservoir sampling algo-
rithm above and the putAll(SampleContainer<T> other)
method combines two containers by choosing elements from
the partitions with weights computed from their respective
sizes. The distributed reservoir sampling algorithm is im-
plemented as a collector where the supplier creates Sample-
Containers, the accumulator puts an item into a SampleCon-
tainer, the combiner combines two SampleContainers, and
the finisher extracts the reservoir list after the final combin-
ing. To facilitate the creation of such collectors, we provide
a factory SamplingCollector.get(k) to return a reservoir
sampler with the appropriate size reservoir.
Program 2 Distributed Reservoir Sampling
public class SamplingCollector {
public static <T> DistributableCollector<
T, SampleContainer<T>, List<T>> get(int k) {
return DistributableCollector
.of(() -> new SampleContainer<T>(k),
SampleContainer::put,
SampleContainer::putAll,
SampleContainer::getList); } }
public static <T> List<T> distReservoirSample (
DistributableStream<T> stream) {
return stream
.collect(SamplingCollector.get(k)); }
5.2
PageRank
PageRank is a graph algorithm for measuring the impor-
tance of webpages. On a directed graph, the algorithm itera-
tively updates the rank of a vertex by computing a weighted
sum of ranks from its incoming neighbors.
Program 3 PageRank
// Page is a class that represents the format
// <URL, <[neighbor-URL, neighbor-URL], rank>>
public static DistributableStream<Page> iterate(
DistributableStream<Page> stream, float damp) {
return stream
.flatMap(p -> {
final float rank = p.getRank();
p.setRank(1 - damp);
List<String> nbrs = p.getNeighbors();
final int size = nbrs.size();
return Stream.concat(
// contribution from incoming neighbors
nbrs.stream()
.map(nbr -> new Page(nbr,
emptyList, damp * rank / size)),
// contribution from damping factor
Stream.of(p)); })
.collectToStream(
DistributableCollectors
.toMap(
// clustering based on URL
p -> p.getURL(),
p -> p.getValue(),
(left, right) -> {
// reconstructing neighbor list
mergeNeighbors(left, right);
// updating rank
left.setRank(
left.getRank() + right.getRank());
return left; })); }
public static DistributableStream<Page> pageRank(
DistributableStream<Page> stream,
float damp, int iteration) {
return Stream
.iterate(stream, s -> iterate(s, damp))
.skip(iteration - 1).findFirst().get(); }
Assume a webpage is represented in the format <URL,
<[neighbor-URL, neighbor-URL], rank>>. In one itera-
tion of the algorithm, it parses every page so that an en-
try <neighbor-URL, <[], contribution>> is emitted for
each of its neighbors, where contribution is calculated as
the product of its rank and the damping factor, divided by
the neighbor size. A special entry <URL, <[neighbor-URL,
neighbor-URL], 1 - damp>> is also emitted for this page
so that the list of neighbors can be reconstructed for next
iteration. Entries are then by-key collected by their URLs
and their page ranks updated. Note how we take advantage
of the non-terminating collectToStream() method to build
larger multi-stage computations.
5.3
K-Means Clustering
K-means clustering is a popular clustering method in ma-
chine learning and data mining. Given n vertices in the
space, the algorithm assigns each vertex to the cluster whose
centroid is closest to it. At each iteration the centroids are
updated using the mean of the vertices assigned to it.
In each iteration, the stream supplier returns a stream of
all vertices in the space. The vertices are by-key collectedinto a map, where the key is the id of the cluster to which
the vertex is closest and the value is a pair consisting of the
vertex coordinate and 1. The merger merges two pairs re-
turning a new pair that represents the vector sum and the
vertex count. The clustering step is done with a distributed
compute engine. After that, the means for each cluster are
computed by dividing the vector sum by the vertex count
to create the new centroids of the k clusters. The new cen-
troids and the termination condition are evaluated by the
LocalEngine running inside the client JVM. This stage could
have been executed using a second reduce step on the itera-
tion engine, but this is less efficient if k is sufficiently small
and the client machine is sufficiently large.
To bootstrap this algorithm, we need to give it a stream
supplier and a list of initial centroids. Hadoop is used to
parse and filter the raw data. The filtered input is then
written into a Coherence NamedCache to take advantage
of the fast in-memory store for the iterative computations.
The initial k centroids are obtained using our distributed
reservoir sampling algorithm described in Section 5.1. The
driver for k-means clustering is written in Program 5.
Program 5 Driver for K-Means Clustering
List<Vertex> centroids =
MapReduceEngine.valueStream(hConf)
.map(parser)
.withEngine(CoherenceEngine.get(cConf))
.collect(SamplingCollector.get(k));
return kMeans(
() -> CoherenceEngine.valueStream(cConf),
centroids, maxIter, threshold);
6.
EVALUATION
We evaluate our implementation through a series of ex-
periments in an Oracle Big Data Appliance [15], where each
node is equipped with 2 × eight-core Intel Xeon processors,
64GB memory, and 12 × 4TB 7200RPM disks. Nodes are
connected by InfiniBand switches for high network through-
put. We use Cloudera CDH 5.0.2 Hadoop distribution, Or-
acle Coherence 12.1.2, and Java SE 8u5 as the testing envi-
ronment. Coherence cache server instances are started by a
long running map-only Hadoop job.
We first compare a DistributableStream program with a
native program for the same engine. In this test we com-
pare MapReduceStream with a native Hadoop job. We run
WordCount on 45GB Wikipedia dumps. We compare the
stream implementation using Java immutable types shown
in Program 1 and another using Hadoop Writable types.
For comparison, we compare both the standard WordCount
example in the Hadoop package and another using Java im-
mutable types. The four implementations are executed us-
ing the same hardware with equal amount of resources.
Program 4 K-Means Clustering
public static List<Vertex> kMeans(
Supplier<DistributableStream<Vertex>> supplier,
List<Vertex> centroids, int maxIter,
double threshold) {
do {} while (
supplier
.get() // generating vertex stream
.collect(DistributableCollectors
.toMap( // clustering by cluster id
v -> closestCentroid(v, centroids),
v -> new Pair(v, 1),
(left, right) -> {
left.setVert(
sum(left.getVert(),
right.getVert()));
left.setCnt(
left.getCnt() + right.getCnt());
return left; }))
.entrySet()
.parallelStream() // SMP processing
.collect( // updating means
Collector.of(
() -> new DoubleWritable(0),
(delta, entry) -> {
int id = entry.getKey();
Vertex centroid = mean(
entry.getValue());
delta.set(delta.get() +
distance(centroid,
centroids.get(id)));
centroids.set(id, centroid); },
(left, right) -> {
left.set(left.get()
+ right.get());
return left; },
delta -> delta.get())) > threshold
&& maxIter-- > 0 )
return centroids; }
10
9
8
7
6
5
4
3
2
1
0
MRStream
8.54
Hadoop
4.63
1
1.17
Writable
Immutable
Object Type
Figure 7: WordCount.
The normalized job completion times are shown in Fig-
ure 7. MapReduceStream outperforms Hadoop by 17% for
Writable types, and by 84% for immutable types. This is be-
cause MapReduceStream performs in-memory partial merg-
ing before collecting records to MapOutputBuffer. The sav-
ing is substantial for immutable types, which incur higher
serialization and garbage collection costs.
One of the most important features of DistributableStream
is the ability to perform federated queries on top of multiple
compute engines for query optimization. We run k-means
clustering algorithm shown in Section 5 to test the bene-
fit of using Coherence over Hadoop as the compute engine
for iterations. About 45GB of raw data representing onebillion vertices is first parsed and filtered by the Hadoop en-
gine, and gets written into HDFS or Coherence, depending
on the compute engine to be used for the iterative steps.
Then the compute engine chosen for iterations is used to
cluster vertices into one thousand clusters. At the end of
each iteration, the LocalEngine is used for updating the list
of centroids and evaluating the termination condition.
Previous work has shown that by avoiding disk IOs, in-
memory processing can outperform Hadoop by up to 20× in
iterative applications [19]. We take a different approach in
our evaluation: by caching the input data and all intermedi-
ate results in the OS cache, we avoid most disk IOs during
the job execution. The number of iterations is varied to see
how the job completion time is affected by the increased
workload. We configure the parameters so that Hadoop and
Coherence have equal amounts of available resources.
18
16
14
12
10
8
6
4
2
0
16.13
Hadoop + Coherence + Local
Hadoop + Hadoop + Local
11.67
6.98
6.92
4.87
4.79
3.1
2.88
1.86
1
1
5
10
20
Number of Iterations
30
Figure 8: K-means clustering.
The normalized job completion times are shown in Fig-
ure 8. We observe that for evaluating each iteration, Hadoop
is outperformed by Coherence by 2.3× to 3.1×, even with-
out the extra disk IOs. This is partly because accessing the
OS cache is slower than accessing the Java heap and also
because Coherence stores data in memory as Java objects
which saves deserialization cost.
7.
7.1
FURTHER DISCUSSIONS
Operation Pipeline Improvement
Consider again the example shown in Section 4.2. In our
current MapReduceStream implementation, two consecutive
Hadoop jobs are needed for the two stream stages, result-
ing in the “map-reduce-map-reduce” pattern. Spark and
Tez, with their ability to perform “map-reduce-reduce” op-
erations, can apply the second map operation in the same
reducer processes in the first stage, right after they finish re-
ducing on each key. This will allow intermediate operations
from the next stage of the computation to be pipelined with
the terminal operation in the previous stage, saving one pass
over the data set.
7.2
Job Planner and Optimizer
The ability to change engines in midstream offers great
flexibility in designing stream applications. Choosing the
right engine based on price, data locality, and available re-
sources is critical in performance optimization.
Small Data Sets: Some applications with small data sets
fit within a single JVM. Running such applications in a local
engine avoids unnecessary costs of starting distributed ser-
vices and worker instances and sometimes can be more effi-
cient than a distributed engine. As we provide Distributa-
bleStream with an efficient local implementation, applica-
tions written with this API can easily scale down for pro-
cessing small amounts of data efficiently.
Iterative Algorithms: Many graph and machine learning
algorithms apply computations iteratively until a conver-
gence condition is reached. Loading the input to the fast
in-memory store and running iterations there avoids expen-
sive IOs and can lead to a huge performance improvement.
Huge Data Sets or CPU-Bound Applications: Some
applications work on huge amount of data that just cannot
fit in memory. Other applications, such as natural language
processing and video encoding, are CPU-bound and benefit
more from increased computing resources rather than faster
IO. Memory is also much more expensive than disk: extra
money spent on memory might be better used to buy more
or faster processors.
Resource Availability: Data centers are generally not
built out as one uniform cluster. Data centers have a va-
riety of different clusters, purchased at different times, run-
ning different software, storing different data, with differ-
ent amounts of memory, disk and computational resources.
The ability to run a federated computation allows the pro-
grammer to exploit the available resources. For example, a
memory based engine, like Coherence, may be a cache layer
shared by multiple applications. It can make sense to ship
data and offload some work to another cluster which has
more resources available to it.
Automatic Engine Assignment: In the current imple-
mentation, we do not have a computation optimizer to auto-
matically choose the right engine for each stage in a stream
application. By including the withEngine() method in Dis-
tributableStream interface, we provide a convenient means
for an optimizer to take charge. Programmers now can man-
ually design the execution plan by explicitly calling the with-
Engine() method. We give such an example for the k-means
clustering algorithm in Section 5. Using our Engine frame-
work, programmers can write programs that do not hard
code the engine and instead read the engine class name and
configuration from the command line.
Without careful application design, the flexibility provid-
ed by our framework can lead to poor performance. Let us
assume some input data resides in a Hadoop cluster. Con-
sider the two examples shown in Figure 9.
The first approach first generates an InputFormatStream
that represents the input data, and then wraps it into a
LocalStream which is a DistributableStream with the Lo-
calEngine, and finally uses withEngine() call to switch back
to the MapReduceEngine.
LocalEngine
.of(InputFormatStream.valueStream(conf))
.withEngine(mapReduceEngine);
This approach is semantically identical to the second ap-
proach shown as the direct path in Figure 9,
MapReduceEngine.valueStream(conf);MapReduce
Engine
Task
Local
Engine
(3) Push
(4) Read
Task
MapReduce
Engine
Task
(2) Read
Read
(1) Pull
HDFS
Memory
HDFS
Figure 9: Improper engine assignment.
However, the first one suffers from the fact that all stream
elements go through the local JVM before being consumed
by the processing tasks in the Hadoop cluster, resulting in
a less efficient program. While this example might seem
artificial, given the flexibility our framework offers to the
programmers, we could end up seeing a few similar cases.
This is similar to what we have discussed in Section 2.2.4
on parallelism in local streams – making a stream parallel is
not always beneficial, and sometimes can be harmful.
Therefore, we emphasize here that, carefully choosing the
right source representation and the engine assignment is im-
portant in designing stream applications. The users need to
have a basic performance cost model in order to use the tools
effectively. While good visualization and monitoring tools
are useful for programmers to approach the right design, an
automatic job planner and optimizer is a more attractive
way of achieving the same goal.
7.3
Job Progress Monitoring and Workspace
Management
The chance of failure during a job execution grows with
the scale of the distributed environment. Each compute en-
gine usually has its own way of monitoring job progress and
tolerating task failures during the job execution. However,
as the execution of a DistributableStream application can
span across multiple compute engines, job progress moni-
toring also needs to go across their boundaries.
Assume a stream application consists of stages on several
engines. On the top level, a centralized service, ZooKeeper
[8] for example, can be responsible for keeping track of the
unique global job id. Engine-specific job information can
be stored under the global job id by the individual Engine
implementations. Task-level progress monitoring and fault
tolerance within each compute engine are still handled by
individual engines. When a portion of the job on a specific
engine fails entirely, the centralized service takes the charge
to recover or fail the job.
Consider data movement between two engines. The pull-
based model works by providing a means for tasks in the
downstream engine to read from the data storage in the up-
stream engine. Temporary results from failed tasks in the
upstream engine will not move across the engine boundary
because they are automatically handled by the first engine.
On the other hand, the push-based model usually works by
side-effects, instructing upstream engine to write directly in-
to downstream data storage. Without proper output com-
mitment, partial results from failed upstream engine tasks
could sneak into the downstream storage. One technique for
avoiding problems is to generate repeatable primary keys.
In this case, entries generated by a restarted task will over-
write the partial results from the failed task, even if they
have made through into the downstream storage.
Once the execution of the entire job is complete, inter-
mediate results inside individual compute engines need to
be cleaned up and temporary files deleted. The centralized
service can, in a similar way as Cascading and Tez, instruct
every compute engine to handle its own piece. It could also
leave the users to do the cleanup themselves, in case that
some of the results were named for later reuse.
Currently, in the MapReduceStream implementation, tem-
porary files are written under a temporary directory which
gets cleaned up periodically, while in the CoherenceStream
intermediate caches for MapperServices are destroyed upon
the completion of ReducerServices.
8.
RELATED WORK
Language Integrated Query: We like SQL, but not all
Java programmers use SQL and implementing certain ap-
plications is more natural in Java. Therefore, instead of
integrating SQL style declarative query as LINQ [14] and
OptiQL [13], DistributableStream takes a similar approach
to Dryad [11] and Spark [5] to adopt functional programming
style for the language integrated interface. This allows ap-
plication developers to live on the stream abstraction with
its well-defined data-parallel operations and still enjoy the
flexibility of procedural languages. While one could consid-
er building yet another SQL compiler on top of Distributa-
bleStream, like Hive [3] on Hadoop or Shark [17] on Spark,
we are more interested in extending existing SQL engines
with table functions defined using Java computations.
Cascading [9] is an application development platform for
building applications on Hadoop and most recently Tez [7].
Similar to DistributableStream, Cascading provides an ab-
straction layer between the compute engine APIs and devel-
opers, so that applications developed with Cascading APIs
can be reused without rewriting any business logic when the
underlying engine is replaced. However, it is not clear to us
that Cascading supports changing engines within the same
application. By reusing the patterns and concepts from the
standard Java 8 Stream interface and supporting changing
engines in midstream, DistributableStream could be a better
choice to help Java programmers quickly and conveniently
adopt the API for processing big data sets.
Massively Parallel Processing Systems: MapReduce
[10], Hadoop [1], Dryad [11], Spark [5], Storm [6], and Tez
[7] are examples of recent data-parallel MPP systems. These
systems store data as individual splits and run parallel tasks
responsible for processing their own data splits. Distributa-
bleStream, on the other hand, does not provide its own data-
parallel MPP infrastructure. Our focus here is to provide a
clean computational model and API for federating different
MPP systems both between and within a query. The goal is
to facilitate the development of simple, generic, and efficient
applications across an extensible set of compute engines to
process distributed data sets.
Distributed Streaming Computation: Apache Storm
[6] is a distributed realtime computation system for pro-
cessing streams of data. Computations in Storm are spec-
ified by topology transformations in DAGs. Storm definesa spout abstraction, which is responsible for feeding mes-
sages into the topology for processing, for integration with
new queuing systems. Spark Streaming [18] is another dis-
tributed stream processing system that has tight integra-
tion with Spark and combines streaming with batch and in-
teractive queries. Although the engine implementations we
currently have for DistributableStream do not support real-
time streaming, the DistributableStream API itself can be
extended with tee and window functions to support Storm
and Spark Streaming.
Resilient Distributed Datasets (RDDs): Combining
Coherence and Hadoop allows us to cache, store, and pro-
cess data that originally resides in HDFS in a similar way as
RDDs in Spark. Both Spark and Coherence allow storing da-
ta at different levels depending on whether they are stored as
Java objects or as serialized bytes, whether they are stored
in memory or on disk, and whether they are replicated or
not. RDDs also support storing the transformation recipe
to allow lineage-based recovery for fault tolerance, while the
current Coherence implementation does not include this fea-
ture. We have developed a preliminary implementation that
allows us to use RDDs as a data source and Spark as a com-
pute engine for stream computations. Detailed evaluations
are planned for future work.
9.
CONCLUSIONS
Java and other JVM-based languages play an important
role in the Hadoop and Big Data ecosystem. Java is a
more natural implementation language for certain applica-
tions and we have seen these JVM-based distributed systems
complement the SQL based analytical systems for process-
ing unstructured data and for algorithmic programming.
While the flexibility of Java has been appreciated in the
Hadoop ecosystem, there are some features in high demand.
First, expressiveness and conciseness brought by functional
and declarative languages are convenient for application de-
velopment, which were not well supported by older versions
of Java. Second, many JVM-based distributed systems have
their own distinct APIs. This forces companies and appli-
cation programmers to split their effort and investment a-
mong several frameworks, hoping that their bet on today’s
frameworks will not become obsolete tomorrow with newly
emerged systems. A single API that can be supported over
multiple engines can be seen as insurance against obsoles-
cence. Third, the demand for improved performance can
never be satisfied. It is preferred that a framework can be
organized so that it can easily benefit both from high-level
job planning and low-level JVM optimizations.
To address this demand, we present DistributableStream,
a Java computational model that enables programmers to
write generic, distributed and federated queries on top of
an extensible set of compute engines, allowing data to flow
between them so that different stages of the computation
can be carried out on their respective optimized engines.
By reusing the patterns and concepts from the standard Ja-
va Stream interface, DistributableStream provides a friendly
way of integrating queries into the programming language,
making Java a more expressive tool for Big Data processing.
With the Engine abstraction, the platform-specific param-
eters are separated from the computational model, freeing
programmers from low-level details as they design the al-
gorithmic part of their applications. The abstraction also
makes it convenient to run existing applications develop-
ed on the DistributableStream framework on new engines,
leveraging the investment in past development. The Dis-
tributableStream approach, translating computations into
stages and decomposing data into local streams, allows for
efficient execution that benefits from both job optimization
and intelligent Java runtimes.
We validate this API by showing three implementations
– a local SMP engine, a distributed file based engine and a
distributed in-memory engine – and demonstrate the useful-
ness of DistributableStream with examples and performance
evaluations. We are considering incorporating this work in
a JSR and hope that others will write applications using the
API, support the API on additional engines, and extend the
computational model supported by the API.
10.
ACKNOWLEDGEMENT
We thank the Oracle Java team for their work on the Java
8 Stream API and Michel Benoliel for his inspiring imple-
mentation of MapReduce on Coherence. We also thank the
anonymous referees for their useful suggestions that signifi-
cantly improved the quality of this work.
11.
REFERENCES
[1] Apache Hadoop. http://hadoop.apache.org/.
[2] Apache Hadoop YARN.
http://hadoop.apache.org/docs/current/
hadoop-yarn/hadoop-yarn-site/YARN.html.
[3] Apache Hive. http://hive.apache.org/.
[4] Apache Pig. http://pig.apache.org/.
[5] Apache Spark. http://spark.apache.org/.
[6] Apache Storm.
http://storm.incubator.apache.org/.
[7] Apache Tez. http://tez.incubator.apache.org/.
[8] Apache ZooKeeper. http://zookeeper.apache.org/.
[9] Cascading. http://www.cascading.org/.
[10] J. Dean and S. Ghemawat. MapReduce: Simplified
data processing on large clusters. In Proceedings of
OSDI, pages 137–150, 2004.
[11] The Dryad project. http:
//research.microsoft.com/en-us/projects/Dryad/.
[12] JDK 8 project. https://jdk8.java.net/.
[13] H. Lee, K. J. Brown, A. K. Sujeeth, H. Chafi,
T. Rompf, M. Odersky, and K. Olukotun.
Implementing domain-specific languages for
heterogeneous parallel computing. IEEE Micro,
31(5):42–53, 2011.
[14] LINQ: language integrated query. http://msdn.
microsoft.com/en-us/library/bb397926.aspx/.
[15] Oracle Big Data Appliance. http://www.oracle.com/
us/products/database/big-data-appliance/.
[16] Oracle Coherence. http://www.oracle.com/
technetwork/middleware/coherence/.
[17] Shark. http://shark.cs.berkeley.edu/.
[18] Spark streaming.
http://spark.apache.org/streaming/.
[19] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma,
M. McCauley, M. J. Franklin, S. Shenker, and
I. Stoica. Resilient distributed datasets: A
fault-tolerant abstraction for in-memory cluster
computing. In Proceedings of NSDI, pages 15–28, 2012.
Challenges and Opportunities with Big Data
A community white paper developed by leading researchers across the United States
Executive Summary
The promise of data-driven decision-making is now being recognized broadly, and there is
growing enthusiasm for the notion of ``Big Data.’’ While the promise of Big Data is real -- for example, it
is estimated that Google alone contributed 54 billion dollars to the US economy in 2009 -- there is
currently a wide gap between its potential and its realization.
Heterogeneity, scale, timeliness, complexity, and privacy problems with Big Data impede
progress at all phases of the pipeline that can create value from data. The problems start right away
during data acquisition, when the data tsunami requires us to make decisions, currently in an ad hoc
manner, about what data to keep and what to discard, and how to store what we keep reliably with the
right metadata. Much data today is not natively in structured format; for example, tweets and blogs are
weakly structured pieces of text, while images and video are structured for storage and display, but not
for semantic content and search: transforming such content into a structured format for later analysis is
a major challenge. The value of data explodes when it can be linked with other data, thus data
integration is a major creator of value. Since most data is directly generated in digital format today, we
have the opportunity and the challenge both to influence the creation to facilitate later linkage and to
automatically link previously created data. Data analysis, organization, retrieval, and modeling are other
foundational challenges. Data analysis is a clear bottleneck in many applications, both due to lack of
scalability of the underlying algorithms and due to the complexity of the data that needs to be analyzed.
Finally, presentation of the results and its interpretation by non-technical domain experts is crucial to
extracting actionable knowledge.
During the last 35 years, data management principles such as physical and logical independence,
declarative querying and cost-based optimization have led, during the last 35 years, to a multi-billion
dollar industry. More importantly, these technical advances have enabled the first round of business
intelligence applications and laid the foundation for managing and analyzing Big Data today. The many
novel challenges and opportunities associated with Big Data necessitate rethinking many aspects of
these data management platforms, while retaining other desirable aspects.
We believe that
appropriate investment in Big Data will lead to a new wave of fundamental technological advances that
will be embodied in the next generations of Big Data management and analysis platforms, products, and
systems.
We believe that these research problems are not only timely, but also have the potential to
create huge economic value in the US economy for years to come. However, they are also hard,
requiring us to rethink data analysis systems in fundamental ways. A major investment in Big Data,
properly directed, can result not only in major scientific advances, but also lay the foundation for the
next generation of advances in science, medicine, and business.Challenges and Opportunities with Big Data
1. Introduction
We are awash in a flood of data today. In a broad range of application areas, data is being
collected at unprecedented scale. Decisions that previously were based on guesswork, or on
painstakingly constructed models of reality, can now be made based on the data itself. Such Big Data
analysis now drives nearly every aspect of our modern society, including mobile services, retail,
manufacturing, financial services, life sciences, and physical sciences.
Scientific research has been revolutionized by Big Data [CCC2011a]. The Sloan Digital Sky Survey
[SDSS2008] has today become a central resource for astronomers the world over. The field of
Astronomy is being transformed from one where taking pictures of the sky was a large part of an
astronomer’s job to one where the pictures are all in a database already and the astronomer’s task is to
find interesting objects and phenomena in the database. In the biological sciences, there is now a well-
established tradition of depositing scientific data into a public repository, and also of creating public
databases for use by other scientists. In fact, there is an entire discipline of bioinformatics that is largely
devoted to the curation and analysis of such data. As technology advances, particularly with the advent
of Next Generation Sequencing, the size and number of experimental data sets available is increasing
exponentially.
Big Data has the potential to revolutionize not just research, but also education [CCC2011b]. A
recent detailed quantitative comparison of different approaches taken by 35 charter schools in NYC has
found that one of the top five policies correlated with measurable academic effectiveness was the use of
data to guide instruction [DF2011]. Imagine a world in which we have access to a huge database where
we collect every detailed measure of every student's academic performance. This data could be used to
design the most effective approaches to education, starting from reading, writing, and math, to
advanced, college-level, courses. We are far from having access to such data, but there are powerful
trends in this direction.
In particular, there is a strong trend for massive Web deployment of
educational activities, and this will generate an increasingly large amount of detailed data about
students' performance.
It is widely believed that the use of information technology can reduce the cost of healthcare
while improving its quality [CCC2011c], by making care more preventive and personalized and basing it
on more extensive (home-based) continuous monitoring. McKinsey estimates [McK2011] a savings of
300 billion dollars every year in the US alone.
In a similar vein, there have been persuasive cases made for the value of Big Data for urban
planning (through fusion of high-fidelity geographical data), intelligent transportation (through analysis
and visualization of live and detailed road network data), environmental modeling (through sensor
networks ubiquitously collecting data) [CCC2011d], energy saving (through unveiling patterns of use),
smart materials (through the new materials genome initiative [MGI2011]), computational social sciences
1(a new methodology fast growing in popularity because of the dramatically lowered cost of obtaining
data) [LP+2009], financial systemic risk analysis (through integrated analysis of a web of contracts to
find dependencies between financial entities) [FJ+2011], homeland security (through analysis of social
networks and financial transactions of possible terrorists), computer security (through analysis of logged
information and other events, known as Security Information and Event Management (SIEM)), and so
on.
In 2010, enterprises and users stored more than 13 exabytes of new data; this is over 50,000
times the data in the Library of Congress. The potential value of global personal location data is
estimated to be $700 billion to end users, and it can result in an up to 50% decrease in product
development and assembly costs, according to a recent McKinsey report [McK2011]. McKinsey predicts
an equally great effect of Big Data in employment, where 140,000-190,000 workers with “deep
analytical” experience will be needed in the US; furthermore, 1.5 million managers will need to become
data-literate. Not surprisingly, the recent PCAST report on Networking and IT R&D [PCAST2010]
identified Big Data as a “research frontier” that can “accelerate progress across a broad range of
priorities.” Even popular news media now appreciates the value of Big Data as evidenced by coverage in
the Economist [Eco2011], the New York Times [NYT2012], and National Public Radio [NPR2011a,
NPR2011b].
While the potential benefits of Big Data are real and significant, and some initial successes have
already been achieved (such as the Sloan Digital Sky Survey), there remain many technical challenges
that must be addressed to fully realize this potential. The sheer size of the data, of course, is a major
challenge, and is the one that is most easily recognized. However, there are others. Industry analysis
companies like to point out that there are challenges not just in Volume, but also in Variety and Velocity
[Gar2011], and that companies should not focus on just the first of these. By Variety, they usually mean
heterogeneity of data types, representation, and semantic interpretation. By Velocity, they mean both
the rate at which data arrive and the time in which it must be acted upon. While these three are
important, this short list fails to include additional important requirements such as privacy and usability.
The analysis of Big Data involves multiple distinct phases as shown in the figure below, each of
which introduces challenges. Many people unfortunately focus just on the analysis/modeling phase:
while that phase is crucial, it is of little use without the other phases of the data analysis pipeline. Even
in the analysis phase, which has received much attention, there are poorly understood complexities in
the context of multi-tenanted clusters where several users’ programs run concurrently. Many significant
challenges extend beyond the analysis phase. For example, Big Data has to be managed in context,
which may be noisy, heterogeneous and not include an upfront model. Doing so raises the need to track
provenance and to handle uncertainty and error: topics that are crucial to success, and yet rarely
mentioned in the same breath as Big Data. Similarly, the questions to the data analysis pipeline will
typically not all be laid out in advance. We may need to figure out good questions based on the data.
Doing this will require smarter systems and also better support for user interaction with the analysis
pipeline. In fact, we currently have a major bottleneck in the number of people empowered to ask
questions of the data and analyze it [NYT2012]. We can drastically increase this number by supporting
2many levels of engagement with the data, not all requiring deep database expertise. Solutions to
problems such as this will not come from incremental improvements to business as usual such as
industry may make on its own. Rather, they require us to fundamentally rethink how we manage data
analysis.
Fortunately, existing computational techniques can be applied, either as is or with some
extensions, to at least some aspects of the Big Data problem. For example, relational databases rely on
the notion of logical data independence: users can think about what they want to compute, while the
system (with skilled engineers designing those systems) determines how to compute it efficiently.
Similarly, the SQL standard and the relational data model provide a uniform, powerful language to
express many query needs and, in principle, allows customers to choose between vendors, increasing
competition. The challenge ahead of us is to combine these healthy features of prior systems as we
devise novel solutions to the many new challenges of Big Data.
In this paper, we consider each of the boxes in the figure above, and discuss both what has
already been done and what challenges remain as we seek to exploit Big Data. We begin by considering
3the five stages in the pipeline, then move on to the five cross-cutting challenges, and end with a
discussion of the architecture of the overall system that combines all these functions.
2. Phases in the Processing Pipeline
2.1 Data Acquisition and Recording
Big Data does not arise out of a vacuum: it is recorded from some data generating source. For
example, consider our ability to sense and observe the world around us, from the heart rate of an
elderly citizen, and presence of toxins in the air we breathe, to the planned square kilometer array
telescope, which will produce up to 1 million terabytes of raw data per day. Similarly, scientific
experiments and simulations can easily produce petabytes of data today.
Much of this data is of no interest, and it can be filtered and compressed by orders of
magnitude. One challenge is to define these filters in such a way that they do not discard useful
information. For example, suppose one sensor reading differs substantially from the rest: it is likely to
be due to the sensor being faulty, but how can we be sure that it is not an artifact that deserves
attention? In addition, the data collected by these sensors most often are spatially and temporally
correlated (e.g., traffic sensors on the same road segment). We need research in the science of data
reduction that can intelligently process this raw data to a size that its users can handle while not missing
the needle in the haystack. Furthermore, we require “on-line” analysis techniques that can process such
streaming data on the fly, since we cannot afford to store first and reduce afterward.
The second big challenge is to automatically generate the right metadata to describe what data
is recorded and how it is recorded and measured. For example, in scientific experiments, considerable
detail regarding specific experimental conditions and procedures may be required to be able to interpret
the results correctly, and it is important that such metadata be recorded with observational data.
Metadata acquisition systems can minimize the human burden in recording metadata. Another
important issue here is data provenance. Recording information about the data at its birth is not useful
unless this information can be interpreted and carried along through the data analysis pipeline. For
example, a processing error at one step can render subsequent analysis useless; with suitable
provenance, we can easily identify all subsequent processing that dependent on this step. Thus we need
research both into generating suitable metadata and into data systems that carry the provenance of
data and its metadata through data analysis pipelines.
2.2 Information Extraction and Cleaning
Frequently, the information collected will not be in a format ready for analysis. For example,
consider the collection of electronic health records in a hospital, comprising transcribed dictations from
several physicians, structured data from sensors and measurements (possibly with some associated
uncertainty), and image data such as x-rays. We cannot leave the data in this form and still effectively
4analyze it. Rather we require an information extraction process that pulls out the required information
from the underlying sources and expresses it in a structured form suitable for analysis. Doing this
correctly and completely is a continuing technical challenge. Note that this data also includes images
and will in the future include video; such extraction is often highly application dependent (e.g., what you
want to pull out of an MRI is very different from what you would pull out of a picture of the stars, or a
surveillance photo). In addition, due to the ubiquity of surveillance cameras and popularity of GPS-
enabled mobile phones, cameras, and other portable devices, rich and high fidelity location and
trajectory (i.e., movement in space) data can also be extracted.
We are used to thinking of Big Data as always telling us the truth, but this is actually far from
reality. For example, patients may choose to hide risky behavior and caregivers may sometimes mis-
diagnose a condition; patients may also inaccurately recall the name of a drug or even that they ever
took it, leading to missing information in (the history portion of) their medical record. Existing work on
data cleaning assumes well-recognized constraints on valid data or well-understood error models; for
many emerging Big Data domains these do not exist.
2.3 Data Integration, Aggregation, and Representation
Given the heterogeneity of the flood of data, it is not enough merely to record it and throw it
into a repository. Consider, for example, data from a range of scientific experiments. If we just have a
bunch of data sets in a repository, it is unlikely anyone will ever be able to find, let alone reuse, any of
this data. With adequate metadata, there is some hope, but even so, challenges will remain due to
differences in experimental details and in data record structure.
Data analysis is considerably more challenging than simply locating, identifying, understanding,
and citing data. For effective large-scale analysis all of this has to happen in a completely automated
manner. This requires differences in data structure and semantics to be expressed in forms that are
computer understandable, and then “robotically” resolvable. There is a strong body of work in data
integration that can provide some of the answers. However, considerable additional work is required to
achieve automated error-free difference resolution.
Even for simpler analyses that depend on only one data set, there remains an important
question of suitable database design. Usually, there will be many alternative ways in which to store the
same information. Certain designs will have advantages over others for certain purposes, and possibly
drawbacks for other purposes. Witness, for instance, the tremendous variety in the structure of
bioinformatics databases with information regarding substantially similar entities, such as genes.
Database design is today an art, and is carefully executed in the enterprise context by highly-paid
professionals. We must enable other professionals, such as domain scientists, to create effective
database designs, either through devising tools to assist them in the design process or through forgoing
the design process completely and developing techniques so that databases can be used effectively in
the absence of intelligent database design.
52.4 Query Processing, Data Modeling, and Analysis
Methods for querying and mining Big Data are fundamentally different from traditional
statistical analysis on small samples. Big Data is often noisy, dynamic, heterogeneous, inter-related and
untrustworthy. Nevertheless, even noisy Big Data could be more valuable than tiny samples because
general statistics obtained from frequent patterns and correlation analysis usually overpower individual
fluctuations and often disclose more reliable hidden patterns and knowledge. Further, interconnected
Big Data forms large heterogeneous information networks, with which information redundancy can be
explored to compensate for missing data, to crosscheck conflicting cases, to validate trustworthy
relationships, to disclose inherent clusters, and to uncover hidden relationships and models.
Mining requires integrated, cleaned, trustworthy, and efficiently accessible data, declarative
query and mining interfaces, scalable mining algorithms, and big-data computing environments. At the
same time, data mining itself can also be used to help improve the quality and trustworthiness of the
data, understand its semantics, and provide intelligent querying functions. As noted previously, real-life
medical records have errors, are heterogeneous, and frequently are distributed across multiple systems.
The value of Big Data analysis in health care, to take just one example application domain, can only be
realized if it can be applied robustly under these difficult conditions. On the flip side, knowledge
developed from data can help in correcting errors and removing ambiguity. For example, a physician
may write “DVT” as the diagnosis for a patient. This abbreviation is commonly used for both “deep vein
thrombosis” and “diverticulitis,” two very different medical conditions. A knowledge-base constructed
from related data can use associated symptoms or medications to determine which of two the physician
meant.
Big Data is also enabling the next generation of interactive data analysis with real-time answers.
In the future, queries towards Big Data will be automatically generated for content creation on websites,
to populate hot-lists or recommendations, and to provide an ad hoc analysis of the value of a data set to
decide whether to store or to discard it. Scaling complex query processing techniques to terabytes while
enabling interactive response times is a major open research problem today.
A problem with current Big Data analysis is the lack of coordination between database systems,
which host the data and provide SQL querying, with analytics packages that perform various forms of
non-SQL processing, such as data mining and statistical analyses. Today’s analysts are impeded by a
tedious process of exporting data from the database, performing a non-SQL process and bringing the
data back. This is an obstacle to carrying over the interactive elegance of the first generation of SQL-
driven OLAP systems into the data mining type of analysis that is in increasing demand. A tight coupling
between declarative query languages and the functions of such packages will benefit both
expressiveness and performance of the analysis.
2.5 Interpretation
Having the ability to analyze Big Data is of limited value if users cannot understand the analysis.
Ultimately, a decision-maker, provided with the result of analysis, has to interpret these results. This
6interpretation cannot happen in a vacuum. Usually, it involves examining all the assumptions made and
retracing the analysis. Furthermore, as we saw above, there are many possible sources of error:
computer systems can have bugs, models almost always have assumptions, and results can be based on
erroneous data. For all of these reasons, no responsible user will cede authority to the computer
system. Rather she will try to understand, and verify, the results produced by the computer. The
computer system must make it easy for her to do so. This is particularly a challenge with Big Data due to
its complexity. There are often crucial assumptions behind the data recorded. Analytical pipelines can
often involve multiple steps, again with assumptions built in. The recent mortgage-related shock to the
financial system dramatically underscored the need for such decision-maker diligence -- rather than
accept the stated solvency of a financial institution at face value, a decision-maker has to examine
critically the many assumptions at multiple stages of analysis.
In short, it is rarely enough to provide just the results. Rather, one must provide supplementary
information that explains how each result was derived, and based upon precisely what inputs. Such
supplementary information is called the provenance of the (result) data. By studying how best to
capture, store, and query provenance, in conjunction with techniques to capture adequate metadata,
we can create an infrastructure to provide users with the ability both to interpret analytical results
obtained and to repeat the analysis with different assumptions, parameters, or data sets.
Systems with a rich palette of visualizations become important in conveying to the users the
results of the queries in a way that is best understood in the particular domain. Whereas early business
intelligence systems’ users were content with tabular presentations, today’s analysts need to pack and
present results in powerful visualizations that assist interpretation, and support user collaboration as
discussed in Sec. 3.5.
Furthermore, with a few clicks the user should be able to drill down into each piece of data that
she sees and understand its provenance, which is a key feature to understanding the data. That is, users
need to be able to see not just the results, but also understand why they are seeing those results.
However, raw provenance, particularly regarding the phases in the analytics pipeline, is likely to be too
technical for many users to grasp completely. One alternative is to enable the users to “play” with the
steps in the analysis – make small changes to the pipeline, for example, or modify values for some
parameters. The users can then view the results of these incremental changes. By these means, users
can develop an intuitive feeling for the analysis and also verify that it performs as expected in corner
cases. Accomplishing this requires the system to provide convenient facilities for the user to specify
analyses. Declarative specification, discussed in Sec. 4, is one component of such a system.
3. Challenges in Big Data Analysis
Having described the multiple phases in the Big Data analysis pipeline, we now turn to some
common challenges that underlie many, and sometimes all, of these phases. These are shown as five
boxes in the second row of Fig. 1.
73.1 Heterogeneity and Incompleteness
When humans consume information, a great deal of heterogeneity is comfortably tolerated. In
fact, the nuance and richness of natural language can provide valuable depth. However, machine
analysis algorithms expect homogeneous data, and cannot understand nuance. In consequence, data
must be carefully structured as a first step in (or prior to) data analysis. Consider, for example, a patient
who has multiple medical procedures at a hospital. We could create one record per medical procedure
or laboratory test, one record for the entire hospital stay, or one record for all lifetime hospital
interactions of this patient. With anything other than the first design, the number of medical
procedures and lab tests per record would be different for each patient. The three design choices listed
have successively less structure and, conversely, successively greater variety. Greater structure is likely
to be required by many (traditional) data analysis systems. However, the less structured design is likely
to be more effective for many purposes – for example questions relating to disease progression over
time will require an expensive join operation with the first two designs, but can be avoided with the
latter. However, computer systems work most efficiently if they can store multiple items that are all
identical in size and structure. Efficient representation, access, and analysis of semi-structured data
require further work.
Consider an electronic health record database design that has fields for birth date, occupation,
and blood type for each patient. What do we do if one or more of these pieces of information is not
provided by a patient? Obviously, the health record is still placed in the database, but with the
corresponding attribute values being set to NULL. A data analysis that looks to classify patients by, say,
occupation, must take into account patients for which this information is not known. Worse, these
patients with unknown occupations can be ignored in the analysis only if we have reason to believe that
they are otherwise statistically similar to the patients with known occupation for the analysis
performed. For example, if unemployed patients are more likely to hide their employment status,
analysis results may be skewed in that it considers a more employed population mix than exists, and
hence potentially one that has differences in occupation-related health-profiles.
Even after data cleaning and error correction, some incompleteness and some errors in data are
likely to remain. This incompleteness and these errors must be managed during data analysis. Doing
this correctly is a challenge. Recent work on managing probabilistic data suggests one way to make
progress.
3.2 Scale
Of course, the first thing anyone thinks of with Big Data is its size. After all, the word “big” is
there in the very name. Managing large and rapidly increasing volumes of data has been a challenging
issue for many decades. In the past, this challenge was mitigated by processors getting faster, following
Moore’s law, to provide us with the resources needed to cope with increasing volumes of data. But,
8there is a fundamental shift underway now: data volume is scaling faster than compute resources, and
CPU speeds are static.
First, over the last five years the processor technology has made a dramatic shift - rather than
processors doubling their clock cycle frequency every 18-24 months, now, due to power constraints,
clock speeds have largely stalled and processors are being built with increasing numbers of cores. In the
past, large data processing systems had to worry about parallelism across nodes in a cluster; now, one
has to deal with parallelism within a single node. Unfortunately, parallel data processing techniques
that were applied in the past for processing data across nodes don’t directly apply for intra-node
parallelism, since the architecture looks very different; for example, there are many more hardware
resources such as processor caches and processor memory channels that are shared across cores in a
single node. Furthermore, the move towards packing multiple sockets (each with 10s of cores) adds
another level of complexity for intra-node parallelism. Finally, with predictions of “dark silicon”, namely
that power consideration will likely in the future prohibit us from using all of the hardware in the system
continuously, data processing systems will likely have to actively manage the power consumption of the
processor. These unprecedented changes require us to rethink how we design, build and operate data
processing components.
The second dramatic shift that is underway is the move towards cloud computing, which now
aggregates multiple disparate workloads with varying performance goals (e.g. interactive services
demand that the data processing engine return back an answer within a fixed response time cap) into
very large clusters. This level of sharing of resources on expensive and large clusters requires new ways
of determining how to run and execute data processing jobs so that we can meet the goals of each
workload cost-effectively, and to deal with system failures, which occur more frequently as we operate
on larger and larger clusters (that are required to deal with the rapid growth in data volumes). This
places a premium on declarative approaches to expressing programs, even those doing complex
machine learning tasks, since global optimization across multiple users’ programs is necessary for good
overall performance. Reliance on user-driven program optimizations is likely to lead to poor cluster
utilization, since users are unaware of other users’ programs. System-driven holistic optimization
requires programs to be sufficiently transparent, e.g., as in relational database systems, where
declarative query languages are designed with this in mind.
A third dramatic shift that is underway is the transformative change of the traditional I/O
subsystem. For many decades, hard disk drives (HDDs) were used to store persistent data. HDDs had far
slower random IO performance than sequential IO performance, and data processing engines formatted
their data and designed their query processing methods to “work around” this limitation. But, HDDs are
increasingly being replaced by solid state drives today, and other technologies such as Phase Change
Memory are around the corner. These newer storage technologies do not have the same large spread in
performance between the sequential and random I/O performance, which requires a rethinking of how
we design storage subsystems for data processing systems. Implications of this changing storage
subsystem potentially touch every aspect of data processing, including query processing algorithms,
query scheduling, database design, concurrency control methods and recovery methods.
93.3 Timeliness
The flip side of size is speed. The larger the data set to be processed, the longer it will take to
analyze. The design of a system that effectively deals with size is likely also to result in a system that can
process a given size of data set faster. However, it is not just this speed that is usually meant when one
speaks of Velocity in the context of Big Data. Rather, there is an acquisition rate challenge as described
in Sec. 2.1, and a timeliness challenge described next.
There are many situations in which the result of the analysis is required immediately. For
example, if a fraudulent credit card transaction is suspected, it should ideally be flagged before the
transaction is completed – potentially preventing the transaction from taking place at all. Obviously, a
full analysis of a user’s purchase history is not likely to be feasible in real-time. Rather, we need to
develop partial results in advance so that a small amount of incremental computation with new data can
be used to arrive at a quick determination.
Given a large data set, it is often necessary to find elements in it that meet a specified criterion.
In the course of data analysis, this sort of search is likely to occur repeatedly. Scanning the entire data
set to find suitable elements is obviously impractical. Rather, index structures are created in advance to
permit finding qualifying elements quickly. The problem is that each index structure is designed to
support only some classes of criteria. With new analyses desired using Big Data, there are new types of
criteria specified, and a need to devise new index structures to support such criteria. For example,
consider a traffic management system with information regarding thousands of vehicles and local hot
spots on roadways. The system may need to predict potential congestion points along a route chosen
by a user, and suggest alternatives. Doing so requires evaluating multiple spatial proximity queries
working with the trajectories of moving objects. New index structures are required to support such
queries. Designing such structures becomes particularly challenging when the data volume is growing
rapidly and the queries have tight response time limits.
3.4 Privacy
The privacy of data is another huge concern, and one that increases in the context of Big Data.
For electronic health records, there are strict laws governing what can and cannot be done. For other
data, regulations, particularly in the US, are less forceful. However, there is great public fear regarding
the inappropriate use of personal data, particularly through linking of data from multiple sources.
Managing privacy is effectively both a technical and a sociological problem, which must be addressed
jointly from both perspectives to realize the promise of big data.
Consider, for example, data gleaned from location-based services. These new architectures
require a user to share his/her location with the service provider, resulting in obvious privacy concerns.
Note that hiding the user’s identity alone without hiding her location would not properly address these
privacy concerns. An attacker or a (potentially malicious) location-based server can infer the identity of
the query source from its (subsequent) location information. For example, a user’s location information
can be tracked through several stationary connection points (e.g., cell towers). After a while, the user
10leaves “a trail of packet crumbs” which could be associated to a certain residence or office location and
thereby used to determine the user’s identity. Several other types of surprisingly private information
such as health issues (e.g., presence in a cancer treatment center) or religious preferences (e.g.,
presence in a church) can also be revealed by just observing anonymous users’ movement and usage
pattern over time. In general, Barabási et al. showed that there is a close correlation between people’s
identities and their movement patterns [Gon2008]. Note that hiding a user location is much more
challenging than hiding his/her identity. This is because with location-based services, the location of the
user is needed for a successful data access or data collection, while the identity of the user is not
necessary.
There are many additional challenging research problems. For example, we do not know yet
how to share private data while limiting disclosure and ensuring sufficient data utility in the shared data.
The existing paradigm of differential privacy is a very important step in the right direction, but it
unfortunately reduces information content too far in order to be useful in most practical cases. In
addition, real data is not static but gets larger and changes over time; none of the prevailing techniques
results in any useful content being released in this scenario. Yet another very important direction is to
rethink security for information sharing in Big Data use cases. Many online services today require us to
share private information (think of Facebook applications), but beyond record-level access control we do
not understand what it means to share data, how the shared data can be linked, and how to give users
fine-grained control over this sharing.
3.5 Human Collaboration
In spite of the tremendous advances made in computational analysis, there remain many
patterns that humans can easily detect but computer algorithms have a hard time finding. Indeed,
CAPTCHAs exploit precisely this fact to tell human web users apart from computer programs. Ideally,
analytics for Big Data will not be all computational – rather it will be designed explicitly to have a human
in the loop. The new sub-field of visual analytics is attempting to do this, at least with respect to the
modeling and analysis phase in the pipeline. There is similar value to human input at all stages of the
analysis pipeline.
In today’s complex world, it often takes multiple experts from different domains to really
understand what is going on. A Big Data analysis system must support input from multiple human
experts, and shared exploration of results. These multiple experts may be separated in space and time
when it is too expensive to assemble an entire team together in one room. The data system has to
accept this distributed expert input, and support their collaboration.
A popular new method of harnessing human ingenuity to solve problems is through crowd-
sourcing. Wikipedia, the online encyclopedia, is perhaps the best known example of crowd-sourced
data. We are relying upon information provided by unvetted strangers. Most often, what they say is
correct. However, we should expect there to be individuals who have other motives and abilities –
some may have a reason to provide false information in an intentional attempt to mislead. While most
11such errors will be detected and corrected by others in the crowd, we need technologies to facilitate
this. We also need a framework to use in analysis of such crowd-sourced data with conflicting
statements. As humans, we can look at reviews of a restaurant, some of which are positive and others
critical, and come up with a summary assessment based on which we can decide whether to try eating
there. We need computers to be able to do the equivalent. The issues of uncertainty and error become
even more pronounced in a specific type of crowd-sourcing, termed participatory-sensing. In this case,
every person with a mobile phone can act as a multi-modal sensor collecting various types of data
instantaneously (e.g., picture, video, audio, location, time, speed, direction, acceleration). The extra
challenge here is the inherent uncertainty of the data collection devices. The fact that collected data are
probably spatially and temporally correlated can be exploited to better assess their correctness. When
crowd-sourced data is obtained for hire, such as with “Mechanical Turks,” much of the data created may
be with a primary objective of getting it done quickly rather than correctly. This is yet another error
model, which must be planned for explicitly when it applies.
4. System Architecture
Companies today already use, and appreciate the value of, business intelligence. Business data
is analyzed for many purposes: a company may perform system log analytics and social media analytics
for risk assessment, customer retention, brand management, and so on. Typically, such varied tasks
have been handled by separate systems, even if each system includes common steps of information
extraction, data cleaning, relational-like processing (joins, group-by, aggregation), statistical and
predictive modeling, and appropriate exploration and visualization tools as shown in Fig. 1.
With Big Data, the use of separate systems in this fashion becomes prohibitively expensive given
the large size of the data sets. The expense is due not only to the cost of the systems themselves, but
also the time to load the data into multiple systems. In consequence, Big Data has made it necessary to
run heterogeneous workloads on a single infrastructure that is sufficiently flexible to handle all these
workloads. The challenge here is not to build a system that is ideally suited for all processing tasks.
Instead, the need is for the underlying system architecture to be flexible enough that the components
built on top of it for expressing the various kinds of processing tasks can tune it to efficiently run these
different workloads. The effects of scale on the physical architecture were considered in Sec 3.2. In this
section, we focus on the programmability requirements.
If users are to compose and build complex analytical pipelines over Big Data, it is essential that
they have appropriate high-level primitives to specify their needs in such flexible systems. The Map-
Reduce framework has been tremendously valuable, but is only a first step. Even declarative languages
that exploit it, such as Pig Latin, are at a rather low level when it comes to complex analysis tasks.
Similar declarative specifications are required at higher levels to meet the programmability and
composition needs of these analysis pipelines. Besides the basic technical need, there is a strong
business imperative as well. Businesses typically will outsource Big Data processing, or many aspects of
it. Declarative specifications are required to enable technically meaningful service level agreements,
12since the point of the out-sourcing is to specify precisely what task will be performed without going into
details of how to do it.
Declarative specification is needed not just for the pipeline composition, but also for the
individual operations themselves. Each operation (cleaning, extraction, modeling etc.) potentially runs
on a very large data set. Furthermore, each operation itself is sufficiently complex that there are many
choices and optimizations possible in how it is implemented. In databases, there is considerable work
on optimizing individual operations, such as joins. It is well-known that there can be multiple orders of
magnitude difference in the cost of two different ways to execute the same query. Fortunately, the user
does not have to make this choice – the database system makes it for her. In the case of Big Data, these
optimizations may be more complex because not all operations will be I/O intensive as in databases.
Some operations may be, but others may be CPU intensive, or a mix. So standard database optimization
techniques cannot directly be used. However, it should be possible to develop new techniques for Big
Data operations inspired by database techniques.
The very fact that Big Data analysis typically involves multiple phases highlights a challenge that
arises routinely in practice: production systems must run complex analytic pipelines, or workflows, at
routine intervals, e.g., hourly or daily. New data must be incrementally accounted for, taking into
account the results of prior analysis and pre-existing data. And of course, provenance must be
preserved, and must include the phases in the analytic pipeline. Current systems offer little to no
support for such Big Data pipelines, and this is in itself a challenging objective.
5. Conclusion
We have entered an era of Big Data. Through better analysis of the large volumes of data that
are becoming available, there is the potential for making faster advances in many scientific disciplines
and improving the profitability and success of many enterprises. However, many technical challenges
described in this paper must be addressed before this potential can be realized fully. The challenges
include not just the obvious issues of scale, but also heterogeneity, lack of structure, error-handling,
privacy, timeliness, provenance, and visualization, at all stages of the analysis pipeline from data
acquisition to result interpretation. These technical challenges are common across a large variety of
application domains, and therefore not cost-effective to address in the context of one domain alone.
Furthermore, these challenges will require transformative solutions, and will not be addressed naturally
by the next generation of industrial products. We must support and encourage fundamental research
towards addressing these technical challenges if we are to achieve the promised benefits of Big Data.
13Bibliography
[CCC2011a] Advancing Discovery in Science and Engineering. Computing Community Consortium.
Spring 2011.
[CCC2011b] Advancing Personalized Education. Computing Community Consortium. Spring 2011.
[CCC2011c] Smart Health and Wellbeing. Computing Community Consortium. Spring 2011.
[CCC2011d] A Sustainable Future. Computing Community Consortium. Summer 2011.
[DF2011] Getting Beneath the Veil of Effective Schools: Evidence from New York City. Will Dobbie,
Roland G. Fryer, Jr. NBER Working Paper No. 17632. Issued Dec. 2011.
[Eco2011] Drowning in numbers -- Digital data will flood the planet—and help us understand it
better. The Economist, Nov 18, 2011.
http://www.economist.com/blogs/dailychart/2011/11/big-data-0
[FJ+2011] Using Data for Systemic Financial Risk Management. Mark Flood, H V Jagadish, Albert
Kyle, Frank Olken, and Louiqa Raschid. Proc. Fifth Biennial Conf. Innovative Data Systems
Research, Jan. 2011.
[Gar2011] Pattern-Based Strategy: Getting Value from Big Data. Gartner Group press release. July
2011. Available at http://www.gartner.com/it/page.jsp?id=1731916
[Gon2008] Understanding individual human mobility patterns. Marta C. González, César A. Hidalgo,
and Albert-László Barabási. Nature 453, 779-782 (5 June 2008)
[LP+2009] Computational Social Science. David Lazer, Alex Pentland, Lada Adamic, Sinan Aral,
Albert-László Barabási, Devon Brewer,Nicholas Christakis, Noshir Contractor, James
Fowler, Myron Gutmann, Tony Jebara, Gary King, Michael Macy, Deb Roy, and Marshall
Van Alstyne. Science 6 February 2009: 323 (5915), 721-723.
[McK2011] Big data: The next frontier for innovation, competition, and productivity. James Manyika,
Michael Chui, Brad Brown, Jacques Bughin, Richard Dobbs, Charles Roxburgh, and Angela
Hung Byers. McKinsey Global Institute. May 2011.
[MGI2011] Materials Genome Initiative for Global Competitiveness. National Science and
Technology Council. June 2011.
[NPR2011a] Folowing the Breadcrumbs to Big Data Gold. Yuki Noguchi. National Public Radio, Nov.
29, 2011. http://www.npr.org/2011/11/29/142521910/the-digital-breadcrumbs-that-lead-to-big -
data
[NPR2011b]
The Search for Analysts to Make Sense of Big Data. Yuki Noguchi. National Public Radio,
Nov. 30, 2011.
http://www.npr.org/2011/11/30/142893065/the-search-for-analysts-to-make-sense-of-big-data
[NYT2012]
The Age of Big Data. Steve Lohr. New York Times, Feb 11, 2012.
http://www.nytimes.com/2012/02/12/sunday-review/big-datas-impact-in-the-world.html
14[PCAST2010] Designing a Digital Future: Federally Funded Research and Development in Networking
and Information Technology. PCAST Report, Dec. 2010. Available at
http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf
[SDSS2008]
SDSS-III: Massive Spectroscopic Surveys of the Distant Universe, the Milky Way Galaxy,
and Extra-Solar Planetary Systems. Jan. 2008. Available at
http://www.sdss3.org/collaboration/description.pdf
15About this Document
This white paper was created through a distributed conversation among many prominent researchers
listed below. This conversation lasted a period of approximately three months from Nov. 2011 to Feb.
2012. Collaborative writing was supported by a distributed document editor.
Divyakant Agrawal, UC Santa Barbara
Philip Bernstein, Microsoft
Elisa Bertino, Purdue Univ.
Susan Davidson, Univ. of Pennsylvania
Umeshwar Dayal, HP
Michael Franklin, UC Berkeley
Johannes Gehrke, Cornell Univ.
Laura Haas, IBM
Alon Halevy, Google
Jiawei Han, UIUC
H. V. Jagadish, Univ. of Michigan (Coordinator)
Alexandros Labrinidis, Univ. of Pittsburgh
Sam Madden, MIT
Yannis Papakonstantinou, UC San Diego
Jignesh M. Patel, Univ. of Wisconsin
Raghu Ramakrishnan, Yahoo!
Kenneth Ross, Columbia Univ.
Cyrus Shahabi, Univ. of Southern California
Dan Suciu, Univ. of Washington
Shiv Vaithyanathan, IBM
Jennifer Widom, Stanford Univ.
16
S PECIAL I SSUE : B USINESS I NTELLIGENCE R ESEARCH
B USINESS I NTELLIGENCE AND A NALYTICS :
F ROM B IG D ATA TO B IG I MPACT
Hsinchun Chen
Eller College of Management, University of Arizona,
Tucson, AZ 85721 U.S.A. {hchen@eller.arizona.edu}
Roger H. L. Chiang
Carl H. Lindner College of Business, University of Cincinnati,
Cincinnati, OH 45221-0211 U.S.A. {chianghl@ucmail.uc.edu}
Veda C. Storey
J. Mack Robinson College of Business, Georgia State University,
Atlanta, GA 30302-4015 U.S.A. {vstorey@gsu.edu}
Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners
and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary
business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research
first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A.
BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and
capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A
research and education are identified. We also report a bibliometric study of critical BI&A publications,
researchers, and research topics based on more than a decade of related academic and industry publications.
Finally, the six articles that comprise this special issue are introduced and characterized in terms of the
proposed BI&A research framework.
Keywords: Business intelligence and analytics, big data analytics, Web 2.0
Introduction
Business intelligence and analytics (BI&A) and the related
field of big data analytics have become increasingly important
in both the academic and the business communities over the
past two decades. Industry studies have highlighted this
significant development. For example, based on a survey of
over 4,000 information technology (IT) professionals from 93
countries and 25 industries, the IBM Tech Trends Report
(2011) identified business analytics as one of the four major
technology trends in the 2010s. In a survey of the state of
business analytics by Bloomberg Businessweek (2011), 97
percent of companies with revenues exceeding $100 million
were found to use some form of business analytics. A report
by the McKinsey Global Institute (Manyika et al. 2011) pre-
dicted that by 2018, the United States alone will face a short-
age of 140,000 to 190,000 people with deep analytical skills,
as well as a shortfall of 1.5 million data-savvy managers with
the know-how to analyze big data to make effective decisions.
Hal Varian, Chief Economist at Google and emeritus profes-
sor at the University of California, Berkeley, commented on
the emerging opportunities for IT professionals and students
in data analysis as follows:
MIS Quarterly Vol. 36 No. 4, pp. 1165-1188/December 2012
1165Chen et al./Introduction: Business Intelligence Research
So what’s getting ubiquitous and cheap? Data. And
what is complementary to data? Analysis. So my
recommendation is to take lots of courses about how
to manipulate and analyze data: databases, machine
learning, econometrics, statistics, visualization, and
so on. 1
The opportunities associated with data and analysis in dif-
ferent organizations have helped generate significant interest
in BI&A, which is often referred to as the techniques, tech-
nologies, systems, practices, methodologies, and applications
that analyze critical business data to help an enterprise better
understand its business and market and make timely business
decisions. In addition to the underlying data processing and
analytical technologies, BI&A includes business-centric
practices and methodologies that can be applied to various
high-impact applications such as e-commerce, market intelli-
gence, e-government, healthcare, and security.
This introduction to the MIS Quarterly Special Issue on
Business Intelligence Research provides an overview of this
exciting and high-impact field, highlighting its many chal-
lenges and opportunities. Figure 1 shows the key sections
of this paper, including BI&A evolution, applications, and
emerging analytics research opportunities. We then report
on a bibliometric study of critical BI&A publications,
researchers, and research topics based on more than a decade
of related BI&A academic and industry publications. Educa-
tion and program development opportunities in BI&A are
presented, followed by a summary of the six articles that
appear in this special issue using our research framework.
The final section concludes the paper.
BI&A Evolution: Key Characteristics
and Capabilities
The term intelligence has been used by researchers in
artificial intelligence since the 1950s. Business intelligence
became a popular term in the business and IT communities
only in the 1990s. In the late 2000s, business analytics was
introduced to represent the key analytical component in BI
(Davenport 2006). More recently big data and big data
analytics have been used to describe the data sets and ana-
lytical techniques in applications that are so large (from
terabytes to exabytes) and complex (from sensor to social
media data) that they require advanced and unique data
1
“Hal Varian Answers Your Questions,” February 25, 2008 (http://
www.freakonomics.com/2008/02/25/hal-varian-answers-your-questions/).
1166
MIS Quarterly Vol. 36 No. 4/December 2012
storage, management, analysis, and visualization technol-
ogies. In this article we use business intelligence and ana-
lytics (BI&A) as a unified term and treat big data analytics as
a related field that offers new directions for BI&A research.
BI&A 1.0
As a data-centric approach, BI&A has its roots in the long-
standing database management field. It relies heavily on
various data collection, extraction, and analysis technologies
(Chaudhuri et al. 2011; Turban et al. 2008; Watson and
Wixom 2007). The BI&A technologies and applications
currently adopted in industry can be considered as BI&A 1.0,
where data are mostly structured, collected by companies
through various legacy systems, and often stored in commer-
cial relational database management systems (RDBMS). The
analytical techniques commonly used in these systems,
popularized in the 1990s, are grounded mainly in statistical
methods developed in the 1970s and data mining techniques
developed in the 1980s.
Data management and warehousing is considered the foun-
dation of BI&A 1.0. Design of data marts and tools for
extraction, transformation, and load (ETL) are essential for
converting and integrating enterprise-specific data. Database
query, online analytical processing (OLAP), and reporting
tools based on intuitive, but simple, graphics are used to
explore important data characteristics. Business performance
management (BPM) using scorecards and dashboards help
analyze and visualize a variety of performance metrics. In
addition to these well-established business reporting func-
tions, statistical analysis and data mining techniques are
adopted for association analysis, data segmentation and
clustering, classification and regression analysis, anomaly
detection, and predictive modeling in various business appli-
cations. Most of these data processing and analytical tech-
nologies have already been incorporated into the leading com-
mercial BI platforms offered by major IT vendors including
Microsoft, IBM, Oracle, and SAP (Sallam et al. 2011).
Among the 13 capabilities considered essential for BI plat-
forms, according to the Gartner report by Sallam et al. (2011),
the following eight are considered BI&A 1.0: reporting,
dashboards, ad hoc query, search-based BI, OLAP, interactive
visualization, scorecards, predictive modeling, and data
mining. A few BI&A 1.0 areas are still under active devel-
opment based on the Gartner BI Hype Cycle analysis for
emerging BI technologies, which include data mining work-
benchs, column-based DBMS, in-memory DBMS, and real-
time decision tools (Bitterer 2011). Academic curricula in
Information Systems (IS) and Computer Science (CS) oftenChen et al./Introduction: Business Intelligence Research
Figure 1. BI&A Overview: Evolution, Applications, and Emerging Research
include well-structured courses such as database management
systems, data mining, and multivariate statistics.
BI&A 2.0
Since the early 2000s, the Internet and the Web began to offer
unique data collection and analytical research and develop-
ment opportunities. The HTTP-based Web 1.0 systems,
characterized by Web search engines such as Google and
Yahoo and e-commerce businesses such as Amazon and
eBay, allow organizations to present their businesses online
and interact with their customers directly. In addition to
porting their traditional RDBMS-based product information
and business contents online, detailed and IP-specific user
search and interaction logs that are collected seamlessly
through cookies and server logs have become a new gold
mine for understanding customers’ needs and identifying new
business opportunities. Web intelligence, web analytics, and
the user-generated content collected through Web 2.0-based
social and crowd-sourcing systems (Doan et al. 2011;
O’Reilly 2005) have ushered in a new and exciting era of
BI&A 2.0 research in the 2000s, centered on text and web
analytics for unstructured web contents.
An immense amount of company, industry, product, and
customer information can be gathered from the web and
organized and visualized through various text and web mining
techniques. By analyzing customer clickstream data logs,
web analytics tools such as Google Analytics can provide a
trail of the user’s online activities and reveal the user’s
browsing and purchasing patterns. Web site design, product
placement optimization, customer transaction analysis, market
structure analysis, and product recommendations can be
accomplished through web analytics. The many Web 2.0
applications developed after 2004 have also created an abun-
dance of user-generated content from various online social
media such as forums, online groups, web blogs, social net-
working sites, social multimedia sites (for photos and videos),
and even virtual worlds and social games (O’Reilly 2005). In
addition to capturing celebrity chatter, references to everyday
events, and socio-political sentiments expressed in these
media, Web 2.0 applications can efficiently gather a large
volume of timely feedback and opinions from a diverse
customer population for different types of businesses.
Many marketing researchers believe that social media
analytics presents a unique opportunity for businesses to treat
the market as a “conversation” between businesses and
customers instead of the traditional business-to-customer,
one-way “marketing” (Lusch et al. 2010). Unlike BI&A 1.0
technologies that are already integrated into commercial
enterprise IT systems, future BI&A 2.0 systems will require
the integration of mature and scalable techniques in text
mining (e.g., information extraction, topic identification,
opinion mining, question-answering), web mining, social
network analysis, and spatial-temporal analysis with existing
DBMS-based BI&A 1.0 systems.
MIS Quarterly Vol. 36 No. 4/December 2012
1167Chen et al./Introduction: Business Intelligence Research
Except for basic query and search capabilities, no advanced
text analytics for unstructured content are currently con-
sidered in the 13 capabilities of the Gartner BI platforms.
Several, however, are listed in the Gartner BI Hype Cycle,
including information semantic services, natural language
question answering, and content/text analytics (Bitterer 2011).
New IS and CS courses in text mining and web mining have
emerged to address needed technical training.
BI&A 3.0
Table 1 summarizes the key characteristics of BI&A 1.0, 2.0,
and 3.0 in relation to the Gartner BI platforms core capa-
bilities and hype cycle.
The decade of the 2010s promises to be an exciting one for
high-impact BI&A research and development for both indus-
try and academia. The business community and industry have
already taken important steps to adopt BI&A for their needs.
The IS community faces unique challenges and opportunities
in making scientific and societal impacts that are relevant and
long-lasting (Chen 2011a). IS research and education pro-
grams need to carefully evaluate future directions, curricula,
and action plans, from BI&A 1.0 to 3.0.
Whereas web-based BI&A 2.0 has attracted active research
from academia and industry, a new research opportunity in
BI&A 3.0 is emerging. As reported prominently in an
October 2011 article in The Economist (2011), the number of
mobile phones and tablets (about 480 million units) surpassed
the number of laptops and PCs (about 380 million units) for
the first time in 2011. Although the number of PCs in use
surpassed 1 billion in 2008, the same article projected that the
number of mobile connected devices would reach 10 billion
in 2020. Mobile devices such as the iPad, iPhone, and other
smart phones and their complete ecosystems of downloadable
applicationss, from travel advisories to multi-player games,
are transforming different facets of society, from education to
healthcare and from entertainment to governments. Other
sensor-based Internet-enabled devices equipped with RFID,
barcodes, and radio tags (the “Internet of Things”) are
opening up exciting new steams of innovative applications.
The ability of such mobile and Internet-enabled devices to
support highly mobile, location-aware, person-centered, and
context-relevant operations and transactions will continue to
offer unique research challenges and opportunities throughout
the 2010s. Mobile interface, visualization, and HCI
(human–computer interaction) design are also promising
research areas. Although the coming of the Web 3.0 (mobile
and sensor-based) era seems certain, the underlying mobile
analytics and location and context-aware techniques for
collecting, processing, analyzing and visualizing such large-
scale and fluid mobile and sensor data are still unknown. Several global business and IT trends have helped shape past
and present BI&A research directions. International travel,
high-speed network connections, global supply-chain, and
outsourcing have created a tremendous opportunity for IT
advancement, as predicted by Thomas Freeman in his seminal
book, The World is Flat (2005). In addition to ultra-fast
global IT connections, the development and deployment of
business-related data standards, electronic data interchange
(EDI) formats, and business databases and information
systems have greatly facilitated business data creation and
utilization. The development of the Internet in the 1970s and
the subsequent large-scale adoption of the World Wide Web
since the 1990s have increased business data generation and
collection speeds exponentially. Recently, the Big Data era
has quietly descended on many communities, from govern-
ments and e-commerce to health organizations. With an
overwhelming amount of web-based, mobile, and sensor-
generated data arriving at a terabyte and even exabyte scale
(The Economist 2010a, 2010b), new science, discovery, and
insights can be obtained from the highly detailed, contex-
tualized, and rich contents of relevance to any business or
organization.
No integrated, commercial BI&A 3.0 systems are foreseen for
the near future. Most of the academic research on mobile BI
is still in an embryonic stage. Although not included in the
current BI platform core capabilities, mobile BI has been
included in the Gartner BI Hype Cycle analysis as one of the
new technologies that has the potential to disrupt the BI
market significantly (Bitterer 2011). The uncertainty asso-
ciated with BI&A 3.0 presents another unique research
direction for the IS community. In addition to being data driven, BI&A is highly applied and
can leverage opportunities presented by the abundant data and
domain-specific analytics needed in many critical and high-
impact application areas. Several of these promising and
high-impact BI&A applications are presented below, with a
discussion of the data and analytics characteristics, potential
impacts, and selected illustrative examples or studies: (1) e-
commerce and market intelligence, (2) e-government and
politics 2.0, (3) science and technology, (4) smart health and
1168
MIS Quarterly Vol. 36 No. 4/December 2012
BI&A Applications: From Big
Data to Big ImpactChen et al./Introduction: Business Intelligence Research
Table 1. BI&A Evolution: Key Characteristics and Capabilities
BI&A 1.0
BI&A 2.0
BI&A 3.0
Key Characteristics
DBMS-based, structured content
• RDBMS & data warehousing
• ETL & OLAP
• Dashboards & scorecards
• Data mining & statistical analysis
Web-based, unstructured content
• Information retrieval and extraction
• Opinion mining
• Question answering
• Web analytics and web
intelligence
• Social media analytics
• Social network analysis
• Spatial-temporal analysis
Mobile and sensor-based content
• Location-aware analysis
• Person-centered analysis
• Context-relevant analysis
• Mobile visualization & HCI
•
•
•
•
•
Gartner BI Platforms Core
Capabilities
Ad hoc query & search-based BI
Reporting, dashboards & scorecards
OLAP
Interactive visualization
Predictive modeling & data mining
well-being, and (5) security and public safety. By carefully
analyzing the application and data characteristics, researchers
and practitioners can then adopt or develop the appropriate
analytical techniques to derive the intended impact. In addi-
tion to technical system implementation, significant business
or domain knowledge as well as effective communication
skills are needed for the successful completion of such BI&A
projects. IS departments thus face unique opportunities and
challenges in developing integrated BI&A research and
education programs for the new generation of data/analytics-
savvy and business-relevant students and professionals (Chen
2011a).
E-Commerce and Market Intelligence
The excitement surrounding BI&A and Big Data has arguably
been generated primarily from the web and e-commerce
communities. Significant market transformation has been
accomplished by leading e-commerce vendors such Amazon
and eBay through their innovative and highly scalable e-
commerce platforms and product recommender systems.
Major Internet firms such as Google, Amazon, and Facebook
continue to lead the development of web analytics, cloud
computing, and social media platforms. The emergence of
customer-generated Web 2.0 content on various forums,
newsgroups, social media platforms, and crowd-sourcing
systems offers another opportunity for researchers and prac-
•
•
•
•
Gartner Hype Cycle
Column-based DBMS
In-memory DBMS
Real-time decision
Data mining workbenches
• Information semantic
services
• Natural language question
answering
• Content & text analytics
• Mobile BI
titioners to “listen” to the voice of the market from a vast
number of business constituents that includes customers, em-
ployees, investors, and the media (Doan et al. 2011; O’Rielly
2005). Unlike traditional transaction records collected from
various legacy systems of the 1980s, the data that e-commerce
systems collect from the web are less structured and often
contain rich customer opinion and behavioral information.
For social media analytics of customer opinions, text analysis
and sentiment analysis techniques are frequently adopted
(Pang and Lee 2008). Various analytical techniques have also
been developed for product recommender systems, such as
association rule mining, database segmentation and clustering,
anomaly detection, and graph mining (Adomavicius and
Tuzhilin 2005). Long-tail marketing accomplished by
reaching the millions of niche markets at the shallow end of
the product bitstream has become possible via highly targeted
searches and personalized recommendations (Anderson
2004).
The Netfix Prize competition 2 for the best collaborative
filtering algorithm to predict user movie ratings helped gener-
ate significant academic and industry interest in recommender
systems development and resulted in awarding the grand prize
of $1 million to the Bellkor’s Pragmatic Chaos team, which
2
Netflix Prize (http://www.netflixprize.com//community/viewtopic.php?
id=1537; accessed July 9, 2012).
MIS Quarterly Vol. 36 No. 4/December 2012
1169Chen et al./Introduction: Business Intelligence Research
surpassed Netflix’s own algorithm for predicting ratings by
10.06 percent. However, the publicity associated with the
competition also raised major unintended customer privacy
concerns.
Much BI&A-related e-commerce research and development
information is appearing in academic IS and CS papers as
well as in popular IT magazines.
astrophysics and oceanography, to genomics and environ-
mental research. To facilitate information sharing and data
analytics, the National Science Foundation (NSF) recently
mandated that every project is required to provide a data
management plan. Cyber-infrastructure, in particular, has
become critical for supporting such data-sharing initiatives.
The 2012 NSF BIGDATA 3 program solicitation is an obvious
example of the U.S. government funding agency’s concerted
efforts to promote big data analytics. The program
E-Government and Politics 2.0
The advent of Web 2.0 has generated much excitement for
reinventing governments. The 2008 U.S. House, Senate, and
presidential elections provided the first signs of success for
online campaigning and political participation. Dubbed
“politics 2.0,” politicians use the highly participatory and
multimedia web platforms for successful policy discussions,
campaign advertising, voter mobilization, event announce-
ments, and online donations. As government and political
processes become more transparent, participatory, online, and
multimedia-rich, there is a great opportunity for adopting
BI&A research in e-government and politics 2.0 applications.
Selected opinion mining, social network analysis, and social
media analytics techniques can be used to support online
political participation, e-democracy, political blogs and
forums analysis, e-government service delivery, and process
transparency and accountability (Chen 2009; Chen et al.
2007). For e-government applications, semantic information
directory and ontological development (as exemplified below)
can also be developed to better serve their target citizens.
Despite the significant transformational potential for BI&A in
e-government research, there has been less academic research
than, for example, e-commerce-related BI&A research. E-
government research often involves researchers from political
science and public policy. For example, Karpf (2009) ana-
lyzed the growth of the political blogosphere in the United
States and found significant innovation of existing political
institutions in adopting blogging platforms into their Web
offerings. In his research, 2D blogspace mapping with com-
posite rankings helped reveal the partisan makeup of the
American political blogsphere. Yang and Callan (2009)
demonstrated the value for ontology development for govern-
ment services through their development of the OntoCop
system, which works interactively with a user to organize and
summarize online public comments from citizens.
Science and Technology
Many areas of science and technology (S&T) are reaping the
benefits of high-throughput sensors and instruments, from
1170
MIS Quarterly Vol. 36 No. 4/December 2012
aims to advance the core scientific and technological
means of managing, analyzing, visualizing, and ex-
tracting useful information from large, diverse, dis-
tributed and heterogeneous data sets so as to accel-
erate the progress of scientific discovery and innova-
tion; lead to new fields of inquiry that would not
otherwise be possible; encourage the development of
new data analytic tools and algorithms; facilitate
scalable, accessible, and sustainable data infrastruc-
ture; increase understanding of human and social
processes and interactions; and promote economic
growth and improved health and quality of life.
Several S&T disciplines have already begun their journey
toward big data analytics. For example, in biology, the NSF
funded iPlant Collaborative 4 is using cyberinfrastructure to
support a community of researchers, educators, and students
working in plant sciences. iPlant is intended to foster a new
generation of biologists equipped to harness rapidly ex-
panding computational techniques and growing data sets to
address the grand challenges of plant biology. The iPlant data
set is diverse and includes canonical or reference data,
experimental data, simulation and model data, observational
data, and other derived data. It also offers various open
source data processing and analytics tools.
In astronomy, the Sloan Digital Sky Survey (SDSS) 5 shows
how computational methods and big data can support and
facilitate sense making and decision making at both the
macroscopic and the microscopic level in a rapidly growing
and globalized research field. The SDSS is one of the most
ambitious and influential surveys in the history of astronomy.
3
“Core Techniques and Technologies for Advancing Big Data Science &
Engineering (BIGDATA),” Program Solicitation NSF 12-499 (http://www.
nsf.gov/pubs/2012/nsf12499/nsf12499.htm; accessed August 2, 2012).
4
iPlant Collaborative (http://www.iplantcollaborative.org/about; accessed
August 2, 2012).
5
“Sloan Digital Sky Survey: Mapping the Universe” (http://www.sdss.org/;
accessed August 2, 2012).Chen et al./Introduction: Business Intelligence Research
Over its eight years of operation, it has obtained deep, multi-
color images covering more than a quarter of the sky and
created three-dimensional maps containing more than 930,000
galaxies and over 120,000 quasars. Continuing to gather data
at a rate of 200 gigabytes per night, SDSS has amassed more
than 140 terabytes of data. The international Large Hadron
Collider (LHC) effort for high-energy physics is another
example of big data, producing about 13 petabytes of data in
a year (Brumfiel 2011).
Smart Health and Wellbeing
Much like the big data opportunities facing the e-commerce
and S&T communities, the health community is facing a
tsunami of health- and healthcare-related content generated
from numerous patient care points of contact, sophisticated
medical instruments, and web-based health communities.
Two main sources of health big data are genomics-driven big
data (genotyping, gene expression, sequencing data) and
payer–provider big data (electronic health records, insurance
records, pharmacy prescription, patient feedback and
responses) (Miller 2012a). The expected raw sequencing data
from each person is approximately four terabytes. From the
payer–provider side, a data matrix might have hundreds of
thousands of patients with many records and parameters
(demographics, medications, outcomes) collected over a long
period of time. Extracting knowledge from health big data
poses significant research and practical challenges, especially
considering the HIPAA (Health Insurance Portability and
Accountability Act) and IRB (Institutional Review Board)
requirements for building a privacy-preserving and trust-
worthy health infrastructure and conducting ethical health-
related research (Gelfand 2011/2012). Health big data ana-
lytics, in general, lags behind e-commerce BI&A applications
because it has rarely taken advantage of scalable analytical
methods or computational platforms (Miller 2012a).
Over the past decade, electronic health records (EHR) have
been widely adopted in hospitals and clinics worldwide.
Significant clinical knowledge and a deeper understanding of
patient disease patterns can be gleanded from such collections
(Hanauer et al. 2009; Hanauer et al. 2011; Lin et al. 2011).
Hanauer et al. (2011), for example, used large-scale, longi-
tudinal EHR to research associations in medical diagnoses
and consider temporal relations between events to better
elucidate patterns of disease progression. Lin et al. (2011)
used symptom–disease–treatment (SDT) association rule
mining on a comprehensive EHR of approximately 2.1
million records from a major hospital. Based on selected
International Classification of Diseases (ICD-9) codes, they
were able to identify clinically relevant and accurate SDT
associations from patient records in seven distinct diseases,
ranging from cancers to chronic and infectious diseases.
In addition to EHR, health social media sites such as Daily
Strength and PatientsLikeMe provide unique research oppor-
tunities in healthcare decision support and patient empower-
ment (Miller 2012b), especially for chronic diseases such as
diabetes, Parkinson’s, Alzheimer’s, and cancer. Association
rule mining and clustering, health social media monitoring
and analysis, health text analytics, health ontologies, patient
network analysis, and adverse drug side-effect analysis are
promising areas of research in health-related BI&A. Due to
the importance of HIPAA regulations, privacy-preserving
health data mining is also gaining attention (Gelfand 2011/
2012).
Partially funded by the National Institutes of Health (NIH),
the NSF BIGDATA program solicitation includes common
interests in big data across NSF and NIH. Clinical decision
making, patient-centered therapy, and knowledge bases for
health, disease, genome, and environment are some of the
areas in which BI&A techniques can contribute (Chen 2011b;
Wactlar et al. 2011). Another recent, major NSF initiative
related to health big data analytics is the NSF Smart Health
and Wellbeing (SHB) 6 program, which seeks to address
fundamental technical and scientific issues that would support
a much-needed transformation of healthcare from reactive and
hospital-centered to preventive, proactive, evidence-based,
person-centered, and focused on wellbeing rather than disease
control. The SHB research topics include sensor technology,
networking, information and machine learning technology,
modeling cognitive processes, system and process modeling,
and social and economic issues (Wactlar et al. 2011), most of
which are relevant to healthcare BI&A.
Security and Public Safety
Since the tragic events of September 11, 2001, security
research has gained much attention, especially given the
increasing dependency of business and our global society on
digital enablement. Researchers in computational science,
information systems, social sciences, engineering, medicine,
and many other fields have been called upon to help enhance
our ability to fight violence, terrorism, cyber crimes, and other
cyber security concerns. Critical mission areas have been
identified where information technology can contribute, as
suggested in the U.S. Office of Homeland Security’s report
“National Strategy for Homeland Security,” released in 2002,
including intelligence and warning, border and transportation
6
“Smart Health and Wellbeing (SBH),” Program Solicitation NSF 12-512
(http://www.nsf.gov/pubs/2012/nsf12512/nsf12512.htm; accessed August 2,
2012).
MIS Quarterly Vol. 36 No. 4/December 2012
1171Chen et al./Introduction: Business Intelligence Research
security, domestic counter-terrorism, protecting critical infra-
structure (including cyberspace), defending against catastro-
phic terrorism, and emergency preparedness and response.
Facing the critical missions of international security and
various data and technical challenges, the need to develop the
science of “security informatics” was recognized, with its
main objective being the
development of advanced information technologies,
systems, algorithms, and databases for security-
related applications, through an integrated techno-
logical, organizational, and policy-based approach
(Chen 2006, p. 7).
BI&A has much to contribute to the emerging field of security
informatics.
Security issues are a major concern for most organizations.
According to the research firm International Data Corpora-
tion, large companies are expected to spend $32.8 billion in
computer security in 2012, and small- and medium-size
companies will spend more on security than on other IT
purchases over the next three years (Perlroth and Rusli 2012).
In academia, several security-related disciplines such as
computer security, computational criminology, and terrorism
informatics are also flourishing (Brantingham 2011; Chen et
al. 2008).
Intelligence, security, and public safety agencies are gathering
large amounts of data from multiple sources, from criminal
records of terrorism incidents, and from cyber security threats
to multilingual open-source intelligence. Companies of dif-
ferent sizes are facing the daunting task of defending against
cybersecurity threats and protecting their intellectual assets
and infrastructure. Processing and analyzing security-related
data, however, is increasingly difficult. A significant chal-
lenge in security IT research is the information stovepipe and
overload resulting from diverse data sources, multiple data
formats, and large data volumes. Current research on tech-
nologies for cybersecurity, counter-terrorism, and crime-
fighting applications lacks a consistent framework for
addressing these data challenges. Selected BI&A technol-
ogies such as criminal association rule mining and clustering,
criminal network analysis, spatial-temporal analysis and
visualization, multilingual text analytics, sentiment and affect
analysis, and cyber attacks analysis and attribution should be
considered for security informatics research.
The University of Arizona’s COPLINK and Dark Web
research programs offer significant examples of crime data
mining and terrorism informatics within the IS community
(Chen 2006, 2012). The COPLINK information sharing and
1172
MIS Quarterly Vol. 36 No. 4/December 2012
crime data mining system, initially developed with funding
from NSF and the Department of Justice, is currently in use
by more than 4,500 police agencies in the United States and
by 25 NATO countries, and was acquired by IBM in 2011.
The Dark Web research, funded by NSF and the Department
of Defense (DOD), has generated one of the largest known
academic terrorism research databases (about 20 terabytes of
terrorist web sites and social media content) and generated
advanced multilingual social media analytics techniques.
Recognizing the challenges presented by the volume and
complexity of defense-related big data, the U.S. Defense
Advanced Research Project Agency (DARPA) within DOD
initiated the XDATA program in 2012 to help develop com-
putational techniques and software tools for processing and
analyzing the vast amount of mission-oriented information for
defense activities. XDATA aims to address the need for
scalable algorithms for processing and visualization of
imperfect and incomplete data. The program engages applied
mathematics, computer science, and data visualization com-
munities to develop big data analytics and usability solutions
for warfighters. 7 BI&A researchers could contribute signifi-
cantly in this area.
Table 2 summarizes these promising BI&A applications, data
characteristics, analytics techniques, and potential impacts.
BI&A Research Framework:
Foundational Technologies and
Emerging Research in Analytics
The opportunities with the abovementioned emerging and
high-impact applications have generated a great deal of
excitement within both the BI&A industry and the research
community. Whereas industry focuses on scalable and inte-
grated systems and implementations for applications in dif-
ferent organizations, the academic community needs to
continue to advance the key technologies in analytics.
Emerging analytics research opportunities can be classified
into five critical technical areas—(big) data analytics, text
analytics, web analytics, network analytics, and mobile
analytics—all of which can contribute to to BI&A 1.0, 2.0,
and 3.0. The classification of these five topic areas is intended
7
“DARPA Calls for Advances in ‘Big Data” to Help the Warfighter,” March
29, 2012 (http://www.darpa.mil/NewsEvents/Releases/2012/03/29.aspx;
accessed August 5, 2012).Chen et al./Introduction: Business Intelligence Research
Table 2. BI&A Applications: From Big Data to Big Impact
E-Commerce and
Market Intelligence
E-Government and
Politics 2.0 Science &
Technology Smart Health and
Wellbeing
Security and
Public Safety
Applications • Recommender
systems
• Social media
monitoring and
analysis
• Crowd-sourcing
systems
• Social and virtual
games • Ubiquitous
government services
• Equal access and
public services
• Citizen engagement
and participation
• Political campaign
and e-polling • S&T innovation
• Hypothesis testing
• Knowledge
discovery • Human and plant
genomics
• Healthcare
decision support
• Patient community
analysis • Crime analysis
• Computational
criminology
• Terrorism
informatics
• Open-source
intelligence
• Cyber security
Data • Search and user
logs
• Customer transac-
tion records
• Customer-
generated content • Government informa-
tion and services
• Rules and regula-
tions
• Citizen feedback and
comments • S&T instruments
and system-
generated data
• Sensor and
network content • Genomics and
sequence data
• Electronic health
records (EHR)
• Health and patient
social media •
•
•
•
Characteristics:
Structured web-
based, user-
generated content,
rich network informa-
tion, unstructured
informal customer
opinions Characteristics:
Fragmented informa-
tion sources and
legacy systems, rich
textual content,
unstructured informal
citizen conversations Characteristics:
High-throughput
instrument-based
data collection, fine-
grained multiple-
modality and large-
scale records, S&T
specific data formats Characteristics:
Disparate but highly
linked content,
person-specific
content, HIPAA, IRB
and ethics issues Characteristics:
Personal identity
information, incom-
plete and deceptive
content, rich group
and network infor-
mation, multilingual
content
Analytics • Association rule
mining
• Database segmen-
tation and
clustering
• Anomaly detection
• Graph mining
• Social network
analysis
• Text and web
analytics
• Sentiment and
affect analysis • Information integra-
tion
• Content and text
analytics
• Government informa-
tion semantic ser-
vices and ontologies
• Social media moni-
toring and analysis
• Social network
analysis
• Sentiment and affect
analysis • S&T based
domain-specific
mathematical and
analytical models • Genomics and
sequence analysis
and visualization
• EHR association
mining and
clustering
• Health social
media monitoring
and analysis
• Health text
analytics
• Health ontologies
• Patient network
analysis
• Adverse drug
side-effect
analysis
• Privacy-preserving
data mining • Criminal
association rule
mining and
clustering
• Criminal network
analysis
• Spatial-temporal
analysis and
visualization
• Multilingual text
analytics
• Sentiment and
affect analysis
• Cyber attacks
analysis and
attribution
Impacts Long-tail marketing,
targeted and person-
alized recommenda-
tion, increased sale
and customer
satisfaction Transforming govern-
ments, empowering
citizens, improving
transparency, partici-
pation, and equality S&T advances,
scientific impact Improved healthcare
quality, improved
long-term care,
patient empower-
ment Improved public
safety and security
Criminal records
Crime maps
Criminal networks
News and web
contents
• Terrorism incident
databases
• Viruses, cyber
attacks, and
botnets
MIS Quarterly Vol. 36 No. 4/December 2012
1173Chen et al./Introduction: Business Intelligence Research
Table 3. BI&A Research Framework: Foundational Technologies and Emerging Research in Analytics
(Big) Data Analytics
Foundational
Technologies
Text Analytics Web Analytics RDBMS
data warehousing
ETL
OLAP
BPM
data mining
clustering
regression
classification
association
analysis
anomaly detection
neural networks
genetic algorithms
multivariate
statistical analysis
optimization
heuristic search • information
retrieval
• document
representation
• query processing
• relevance feedback
• user models
• search engines
• enterprise search
systems • information
retrieval
• computational
linguistics
• search engines
• web crawling
• web site ranking
• search log analysis
• recommender
systems
• web services
• mashups • bibliometric
analysis
• citation network
• coauthorship
network
• social network
theories
• network metrics
and topology
• mathematical
network models
• network
visualization • web services
• smartphone
platforms
• statistical machine
learning
• sequential and
temporal mining
• spatial mining
• mining high-speed
data streams and
sensor data
• process mining
• privacy-preserving
data mining
• network mining
• web mining
• column-based
DBMS
• in-memory DBMS
• parallel DBMS
• cloud computing
• Hadoop
• MapReduce • statistical NLP
• information
extraction
• topic models
• question-answering
systems
• opinion mining
• sentiment/affect
analysis
• web stylometric
analysis
• multilingual
analysis
• text visualization
• multimedia IR
• mobile IR
• Hadoop
• MapReduce • cloud services
• cloud computing
• social search and
mining
• reputation systems
• social media
analytics
• web visualization
• web-based
auctions
• internet
monetization
• social marketing
• web privacy/
security • link mining
• community
detection
• dynamic network
modeling
• agent-based
modeling
• social influence
and information
diffusion models
• ERGMs
• virtual communities
• criminal/dark
networks
• social/political
analysis
• trust and reputation • mobile web
services
• mobile pervasive
apps
• mobile sensing
apps
• mobile social
innovation
• mobile social
networking
• mobile visualiza-
tion/HCI
• personalization and
behavioral
modeling
• gamification
• mobile advertising
and marketing
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Emerging
Research
to highlight the key characteristics of each area; however, a
few of these areas may leverage similar underlying tech-
nologies. In each analytics area we present the foundational
technologies that are mature and well developed and suggest
selected emerging research areas (see Table 3).
(Big) Data Analytics
Data analytics refers to the BI&A technologies that are
grounded mostly in data mining and statistical analysis. As
mentioned previously, most of these techniques rely on the
mature commercial technologies of relational DBMS, data
warehousing, ETL, OLAP, and BPM (Chaudhuri et al. 2011).
1174
MIS Quarterly Vol. 36 No. 4/December 2012
Network Analytics
Mobile Analytics
Since the late 1980s, various data mining algorithms have
been developed by researchers from the artificial intelligence,
algorithm, and database communities. In the IEEE 2006
International Conference on Data Mining (ICDM), the 10
most influential data mining algorithms were identified based
on expert nominations, citation counts, and a community
survey. In ranked order, they are C4.5, k-means, SVM
(support vector machine), Apriori, EM (expectation maximi-
zation), PageRank, AdaBoost, kNN (k-nearest neighbors),
Naïve Bayes, and CART (Wu et al. 2007). These algorithms
cover classification, clustering, regression, association analy-
sis, and network analysis. Most of these popular data mining
algorithms have been incorporated in commercial and open
source data mining systems (Witten et al. 2011). OtherChen et al./Introduction: Business Intelligence Research
advances such as neural networks for classification/prediction
and clustering and genetic algorithms for optimization and
machine learning have all contributed to the success of data
mining in different applications.
Two other data analytics approaches commonly taught in
business school are also critical for BI&A. Grounded in
statistical theories and models, multivariate statistical analysis
covers analytical techniques such as regression, factor analy-
sis, clustering, and discriminant analysis that have been used
successfully in various business applications. Developed in
the management science community, optimization techniques
and heuristic search are also suitable for selected BI&A prob-
lems such as database feature selection and web crawling/
spidering. Most of these techniques can be found in business
school curricula.
Due to the success achieved collectively by the data mining
and statistical analysis community, data analytics continues to
be an active area of research. Statistical machine learning,
often based on well-grounded mathematical models and
powerful algorithms, techniques such as Bayesian networks,
Hidden Markov models, support vector machine, reinforce-
ment learning, and ensemble models, have been applied to
data, text, and web analytics applications. Other new data
analytics techniques explore and leverage unique data charac-
teristics, from sequential/temporal mining and spatial mining,
to data mining for high-speed data streams and sensor data.
Increased privacy concerns in various e-commerce, e-
government, and healthcare applications have caused privacy-
preserving data mining to become an emerging area of
research. Many of these methods are data-driven, relying on
various anonymization techniques, while others are process-
driven, defining how data can be accessed and used (Gelfand
2011/ 2012). Over the past decade, process mining has also
emerged as a new research field that focuses on the analysis
of processes using event data. Process mining has become
possible due to the availability of event logs in various
industries (e.g., healthcare, supply chains) and new process
discovery and conformance checking techniques (van der
Aalst 2012). Furthermore, network data and web content have
helped generate exciting research in network analytics and
web analytics, which are presented below.
In addition to active academic research on data analytics,
industry research and development has also generated much
excitement, especially with respect to big data analytics for
semi-structured content. Unlike the structured data that can
be handled repeatedly through a RDBMS, semi-structured
data may call for ad hoc and one-time extraction, parsing,
processing, indexing, and analytics in a scalable and dis-
tributed MapReduce or Hadoop environment. MapReduce
has been hailed as a revolutionary new platform for large-
scale, massively parallel data access (Patterson 2008).
Inspired in part by MapReduce, Hadoop provides a Java-
based software framework for distributed processing of data-
intensive transformation and analytics. The top three com-
mercial database suppliers—Oracle, IBM, and Microsoft—
have all adopted Hadoop, some within a cloud infrastructure.
The open source Apache Hadoop has also gained significant
traction for business analytics, including Chukwa for data
collection, HBase for distributed data storage, Hive for data
summarization and ad hoc querying, and Mahout for data
mining (Henschen 2011). In their perspective paper, Stone-
braker et al. (2010) compared MapReduce with the parallel
DBMS. The commercial parallel DBMS showed clear advan-
tages in efficient query processing and high-level query
language and interface, whereas MapReduce excelled in ETL
and analytics for “read only” semi-structured data sets. New
Hadoop- and MapReduce-based systems have become
another viable option for big data analytics in addition to the
commercial systems developed for RDBMS, column-based
DBMS, in-memory DBMS, and parallel DBMS (Chaudhuri
et al. 2011).
Text Analytics
A significant portion of the unstructured content collected by
an organization is in textual format, from e-mail commu-
nication and corporate documents to web pages and social
media content. Text analytics has its academic roots in
information retrieval and computational linguistics. In infor-
mation retrieval, document representation and query pro-
cessing are the foundations for developing the vector-space
model, Boolean retrieval model, and probabilistic retrieval
model, which in turn, became the basis for the modern digital
libraries, search engines, and enterprise search systems
(Salton 1989). In computational linguistics, statistical natural
language processing (NLP) techniques for lexical acquisition,
word sense disambiguation, part-of-speech-tagging (POST),
and probabilistic context-free grammars have also become
important for representing text (Manning and Schütze 1999).
In addition to document and query representations, user
models and relevance feedback are also important in
enhancing search performance.
Since the early 1990s, search engines have evolved into
mature commercial systems, consisting of fast, distributed
crawling; efficient inverted indexing; inlink-based page
ranking; and search logs analytics. Many of these founda-
tional text processing and indexing techniques have been
deployed in text-based enterprise search and document
management systems in BI&A 1.0.
MIS Quarterly Vol. 36 No. 4/December 2012
1175Chen et al./Introduction: Business Intelligence Research
Leveraging the power of big data (for training) and statistical
NLP (for building language models), text analytics techniques
have been actively pursued in several emerging areas,
including information extraction, topic models, question-
answering (Q/A), and opinion mining. Information extraction
is an area of research that aims to automatically extract
specific kinds of structured information from documents. As
a building block of information extraction, NER (named
entity recognition, also known as entity extraction) is a
process that identifies atomic elements in text and classifies
them into predefined categories (e.g., names, places, dates).
NER techniques have been successfully developed for news
analysis and biomedical applications. Topic models are algo-
rithms for discovering the main themes that pervade a large
and otherwise unstructured collection of documents. New
topic modeling algorithms such as LDA (latent Dirichlet
allocation) and other probabilistic models have attracted
recent research (Blei 2012). Question answering (Q/A) sys-
tems rely on techniques from NLP, information retrieval, and
human–computer interaction. Primarily designed to answer
factual questions (i.e., who, what, when, and where kinds of
questions), Q/A systems involve different techniques for
question analysis, source retrieval, answer extraction, and
answer presentation (Maybury 2004). The recent successes
of IBM’s Watson and Apple’s Siri have highlighted Q/A
research and commercialization opportunities. Many pro-
mising Q/A system application areas have been identified,
including education, health, and defense. Opinion mining
refers to the computational techniques for extracting, classi-
fying, understanding, and assessing the opinions expressed in
various online news sources, social media comments, and
other user-generated contents. Sentiment analysis is often
used in opinion mining to identify sentiment, affect, subjec-
tivity, and other emotional states in online text. Web 2.0 and
social media content have created abundant and exciting
opportunities for understanding the opinions of the general
public and consumers regarding social events, political move-
ments, company strategies, marketing campaigns, and product
preferences (Pang and Lee 2008).
In addition to the above research directions, text analytics also
offers significant research opportunities and challenges in
several more focused areas, including web stylometric
analysis for authorship attribution, multilingual analysis for
web documents, and large-scale text visualization. Multi-
media information retrieval and mobile information retrieval
are two other related areas that require support of text
analytics techniques, in addition to the core multimedia and
mobile technologies. Similar to big data analytics, text
analytics using MapReduce, Hadoop, and cloud services will
continue to foster active research directions in both academia
and industry.
1176
MIS Quarterly Vol. 36 No. 4/December 2012
Web Analytics
Over the past decade, web analytics has emerged as an active
field of research within BI&A. Building on the data mining
and statistical analysis foundations of data analytics and on
the information retrieval and NLP models in text analytics,
web analytics offers unique analytical challenges and
opportunities. HTTP/HTML-based hyperlinked web sites and
associated web search engines and directory systems for
locating web content have helped develop unique Internet-
based technologies for web site crawling/spidering, web page
updating, web site ranking, and search log analysis. Web log
analysis based on customer transactions has subsequently
turned into active research in recommender systems. How-
ever, web analytics has become even more exciting with the
maturity and popularity of web services and Web 2.0 systems
in the mid-2000s (O’Reilly 2005).
Based on XML and Internet protocols (HTTP, SMTP), web
services offer a new way of reusing and integrating third party
or legacy systems. New types of web services and their
associated APIs (application programming interface) allow
developers to easily integrate diverse content from different
web-enabled system, for example, REST (representational
state transfer) for invoking remote services, RSS (really
simple syndication) for news “pushing,” JSON (JavaScript
object notation) for lightweight data-interchange, and AJAX
(asynchronous JavaScript + XML) for data interchange and
dynamic display. Such lightweight programming models
support data syndication and notification and “mashups” of
multimedia content (e.g., Flickr, Youtube, Google Maps)
from different web sources—a process somewhat similar to
ETL (extraction, transformation, and load) in BI&A 1.0.
Most of the e-commerce vendors have provided mature APIs
for accessing their product and customer content (Schonfeld
2005). For example, through Amazon Web Services, devel-
opers can access product catalog, customer reviews, site
ranking, historical pricing, and the Amazon Elastic Compute
Cloud (EC2) for computing capacity. Similarly, Google web
APIs support AJAX search, Map API, GData API (for
Calendar, Gmail, etc.), Google Translate, and Google App
Engine for cloud computing resources. Web services and
APIs continue to provide an exciting stream of new data
sources for BI&A 2.0 research.
A major emerging component in web analytics research is the
development of cloud computing platforms and services,
which include applications, system software, and hardware
delivered as services over the Internet. Based on service-
oriented architecture (SOA), server virtualization, and utility
computing, cloud computing can be offered as software as aChen et al./Introduction: Business Intelligence Research
service (SaaS), infrastructure as a service (IaaS), or platform
as a service (PaaS). Only a few leading IT vendors are cur-
rently positioned to support high-end, high-throughput BI&A
applications using cloud computing. For example, Amazon
Elastic Compute Cloud (EC2) enables users to rent virtual
computers on which to run their own computer applications.
Its Simple Storage Service (S3) provides online storage web
service. Google App Engine provides a platform for devel-
oping and hosting Java or Python-based web applications.
Google Bigtable is used for backend data storage. Microsoft’s
Windows Azure platform provides cloud services such as
SQL Azure and SharePoint, and allows .Net framework
applications to run on the platform. The industry-led web and
cloud services offer unique data collection, processing, and
analytics challenges for BI&A researchers.
In academia, current web analytics related research encom-
passes social search and mining, reputation systems, social
media analytics, and web visualization. In addition, web-
based auctions, Internet monetization, social marketing, and
web privacy/security are some of the promising research
directions related to web analytics. Many of these emerging
research areas may rely on advances in social network analy-
sis, text analytics, and even economics modeling research.
Network Analytics
Network analytics is a nascent research area that has evolved
from the earlier citation-based bibliometric analysis to include
new computational models for online community and social
network analysis. Grounded in bibliometric analysis, citation
networks and coauthorship networks have long been adopted
to examine scientific impact and knowledge diffusion. The
h-index is a good example of a citation metric that aims to
measure the productivity and impact of the published work of
a scientist or scholar (Hirsch 2005). Since the early 2000s,
network science has begun to advance rapidly with contri-
butions from sociologists, mathematicians, and computer
scientists. Various social network theories, network metrics,
topology, and mathematical models have been developed that
help understand network properties and relationships (e.g.,
centrality, betweenness, cliques, paths; ties, structural holes,
structural balance; random network, small-world network,
scale-free network) (Barabási 2003; Watts 2003).
Recent network analytics research has focused on areas such
as link mining and community detection. In link mining, one
seeks to discover or predict links between nodes of a network.
Within a network, nodes may represent customers, end users,
products and/or services, and the links between nodes may
represent social relationships, collaboration, e-mail exchanges,
or product adoptions. One can conduct link mining using
only the topology information (Liben-Nowell and Kleinberg
2007). Techniques such as common neighbors, Jaccard’s
coefficient, Adamic Adar measure, and Katz measure are
popular for predicting missing or future links. The link
mining accuracy can be further improved when the node and
link attributes are considered. Community detection is also an
active research area of relevance to BI&A (Fortunato 2010).
By representing networks as graphs, one can apply graph
partitioning algorithms to find a minimal cut to obtain dense
subgraphs representing user communities.
Many techniques have been developed to help study the
dynamic nature of social networks. For example, agent-based
models (sometimes referred to as multi-agent systems) have
been used to study disease contact networks and criminal or
terrorist networks (National Research Council 2008). Such
models simulate the actions and interactions of autonomous
agents (of either individual or collective entities such as
organizations or groups) with the intent of assessing their
effects on the system as a whole. Social influence and infor-
mation diffusion models are also viable techniques for
studying evolving networks. Some research is particularly
relevant to opinion and information dynamics of a society.
Such dynamics hold many qualitative similarities to disease
infections (Bettencourt et al. 2006). Another network
analytics technique that has drawn attention in recent years is
the use of exponential random graph models (Frank and
Strauss 1986; Robins et al. 2007). ERGMs are a family of
statistical models for analyzing data about social and other
networks. To support statistical inference on the processes
influencing the formation of network structure, ERGMs
consider the set of all possible alternative networks weighted
on their similarity to an observed network. In addition to
studying traditional friendship or disease networks, ERGMs
are promising for understanding the underlying network
propertities that cause the formation and evolution of
customer, citizen, or patient networks for BI&A.
Most of the abovementioned network analytics techniques are
not part of the existing commercial BI&A platforms. Signifi-
cant open-source development efforts are underway from the
social network analysis community. Tools such as UCINet
(Borgatti et al. 2002) and Pajek (Batagelj and Mrvar 1998)
have been developed and are widely used for large-scale
network analysis and visualization. New network analytics
tools such as ERGM have also been made available to the
academic community (Hunter et al. 2008). Online virtual
communities, criminal and terrorist networks, social and
political networks, and trust and reputation networks are some
of the promising new applications for network analytics.
MIS Quarterly Vol. 36 No. 4/December 2012
1177Chen et al./Introduction: Business Intelligence Research
Mobile Analytics
As an effective channel for reaching many users and as a
means of increasing the productivity and efficiency of an
organization’s workforce, mobile computing is viewed by
respondents of the recent IBM technology trends survey (IBM
2011) as the second most “in demand” area for software
development. Mobile BI was also considered by the Gartner
BI Hype Cycle analysis as one of the new technologies that
have the potential to drastically disrupt the BI market (Bitterer
2011). According to eMarketer, the market for mobile ads is
expected to explode, soaring from an estimated $2.6 billion in
2012 to $10.8 billion in 2016 (Snider 2012).
Mobile computing offers a means for IT professional growth
as more and more organizations build applications. With its
large and growing global install base, Android has been
ranked as the top mobile platform since 2010. This open
source platform, based on Java and XML, offers a much
shorter learning curve and this contributes to its popularity
with IT professionals: 70 percent of the IBM survey
respondents planned to use Android as their mobile develop-
ment platform, while 49 percent planned to use iOS and 35
percent planned to use Windows 7. The ability to collect fine-
grained, location-specific, context-aware, highly personalized
content through these smart devices has opened new possi-
bilities for advanced and innovative BI&A opportunities. In
addition to the hardware and content advantages, the unique
apps ecosystem developed through the volunteer community
of mobile app developers offers a new avenue for BI&A
research. The Apple App Store alone offers more than
500,000 apps in almost any conceivable category as of August
2012; 8 the number of Android apps also reached 500,000 in
August 2012. 9 Many different revenue models have begun to
emerge for mobile apps, from paid or free but ad-supported
apps to mobile gamification, which incentivizes participants
(e.g., users or employees) by giving rewards for contributions
(Snider 2012). For mobile BI, companies are considering
enterprise apps, industry-specific apps, e-commerce apps, and
social apps (in ranked order) according to the IBM survey.
The lightweight programming models of the current web
services (e.g., HTML, XML, CSS, Ajax, Flash, J2E) and the
maturing mobile development platforms such as Android and
iOS have contributed to the rapid development of mobile web
services (e.g., HTML5, Mobile Ajax, Mobile Flash, J2ME) in
8
Apple – iPhone 5 – Learn about apps from the App store (http://www.
apple.com/iphone/built-in-apps/app-store.html; accessed August 8, 2012).
9
AppBrain, Android Statistics (http://www.appbrain.com/stats/number-of-
android-apps; accessed August 8, 2012).
1178
MIS Quarterly Vol. 36 No. 4/December 2012
various mobile pervasive applications, from disaster manage-
ment to healthcare support. New mobile analytics research is
emerging in different areas (e.g., mobile sensing apps that are
location-aware and activity-sensitive; mobile social innova-
tion for m-health and m-learning; mobile social networking
and crowd-sourcing; mobile visualization/HCI; and personali-
zation and behavioral modeling for mobile apps). In addition,
social, behavioral, and economic models for gamification,
mobile advertising, and social marketing are under way and
may contribute to the development of future BI&A 3.0
systems.
Mapping the BI&A Knowledge
Landscape: A Bibliometric Study of
Academic and Industry Publications
In an effort to better understand the current state of BI&A
related research and identify future sources of knowledge, we
conducted a bibliometric study analyzing relevant literature,
major BI&A scholars, disciplines and publications, and key
research topics. A collection, transformation, and analytics
process was followed in the study, much like a typical BI&A
process adopted in other applications.
To discern research trends in BI&A, related literature from
the past decade (2000–2011) was collected. Relevant IT
publications were identified from several large-scale and
reputable digital libraries: Web of Science (Thomson
Reuters, covering more than 12,000 of the highest impact
journals in sciences, engineering, and humanities), Business
Source Complete (EBSCO, covering peer-reviewed business
journals as well as non-journal content such as industry/trade
magazines), IEEE Xplore (Institute of Electrical and Elec-
tronics Engineers, providing access to the IEEE digital
library), ScienceDirect (Elsevier, covering over 2,500 journals
from the scientific, technical, and medical literature), and
Engineering Village (Elsevier, used to retrieve selected ACM
conference papers because the ACM Digital Library interface
does not support automated downloading). These sources
contain high-quality bibliometric metadata, including journal
name and date, author name and institution, and article title
and abstract.
To ensure data consistency and relevance across our collec-
tion, we retrieved only those publications that contained the
keywords business intelligence, business analytics, or big
data within their title, abstract, or subject indexing (when
applicable). The choice of these three keywords was intended
to focus our search and analysis on publications of direct rele-
vance to our interest. However, this search procedure mayChen et al./Introduction: Business Intelligence Research
Keyword All
Years 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011
Business Intelligence 3,146 113 104 146 159 229 330 346 394 352 201 334 338
213 0 5 43 4 5 2 9 6 19 16 17 126
Business Analytics
Big Data
Total
243 0 1 0 0 7 4 3 26 11 41 44 95
3,602 113 110 149 163 241 336 358 426 382 358 356 560
Figure 2. BI&A Related Publication Trend from 2000 to 2011
also omit articles that use other BI&A relevant terms (e.g.,
data warehousing, data mining) but not the three specific
keywords in the title or abstract. This kind of limitation is
common in bibliometric studies. The collected data was
exported as XML records and parsed into a relational data-
base (SQL Server) for analysis. The number of records
initially retrieved totaled 6,187 papers. After removing dupli-
cates, the number of unique records totaled 3,602.
Figure 2 shows the statistics and growth trends of publications
relating to the three search keywords. Overall, business intel-
ligence had the largest coverage and the longest history. This
is consistent with the evolution of BI&A, as the term BI
appeared first in the early 1990s. In our collection, business
analytics and big data began to appear in the literature in
2001, but only gained much attention after about 2007. The
business intelligence related publications numbered 3,146,
whereas business analytics and big data publications each
numbered only 213 and 243, respectively. While the overall
publication trend for business intelligence remains stable,
business analytics and big data publications have seen a faster
growth pattern in recent years. Overall, the largest source of academic business intelligence
publications was academic conferences. The Conference on
Business Intelligence and Financial Engineering (#1) and
Conference on Electronic Commerce and Business Intelli-
gence (#3) are specialized academic conferences devoted to
business intelligence. One IS conference ranks #2 in the top-
20 list: Hawaii International Conference on Systems Sciences
(HICSS), with 370 publications. 10 IEEE holds the majority of
conferences on the list through various outlets; several are
related to emerging technical areas, such as data mining,
Internet computing, and cloud computing. The IEEE Inter-
national Conference on Data Mining (ICDM) is highly
regarded and ranks #5. ACM has two publications in the top-
20 list: Communications of the ACM and the ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining. Both are well-known in CS. Again, the data mining
community has contributed significantly to BI&A. Other
technical conferences in CS are also contributing to BI&A in
areas such as computational intelligence, web intelligence,
evolutionary computation, and natural language processing,
all of which are critical for developing future data, text, and
web analytics techniques discussed in our research frame-
Knowledge of the most popular publications, as well as pro-
lific authors, is beneficial for understanding an emerging
research discipline. Table 4 summarizes the top 20 journals,
conferences, and industry magazines with BI&A publications.
(The top 20 academic BI&A authors are identified in Table 6.) 10
Two major IS conferences, ICIS (International Conference on Information
Systems) and WITS (Workshop on Information Technologies and Systems)
may have also published significant BI&A research; however, their collec-
tions are not covered in the five major digital libraries to which we have
access and thus are not included in this analysis.
MIS Quarterly Vol. 36 No. 4/December 2012
1179Chen et al./Introduction: Business Intelligence Research
Table 4. Top Journals, Conferences, and Industry Magazines with BI&A Publications
Top
20
Academic Publication
Publications Top
20
Industry Publication
Publications
1 Conf. on Business Intelligence and Financial Engineering 531 1 ComputerWorld 2 Hawaii International Conf. on Systems Sciences 370 2 Information Today 258
3 Conf. on Electronic Commerce and Business Intelligence 252 3 InformationWeek 229
4 International Conf. on Web Intelligence and Intelligent Agent
Technology Workshops 151 4 Computer Weekly 199
5 IEEE International Conf. on Data Mining 150 5 Microsoft Data Mining 108
6 IEEE International Conf. on e-Technology, e-Commerce, and
e-Service 129 6 InfoWorld 86
7 IEEE Intelligent Systems 47 7 CIO 71
8 IEEE Cloud Computing 44 8 KM World 61
9 Decision Support Systems 39 9 CRN (formerly
VARBusiness) 59
10 IEEE Congress on Evolutionary Computation 39 10 Stores Magazine 56
11 Journal of Business Ethics 34 11 Forbes 45
12 Communications of the ACM 33 12 CRM Magazine 40
13 European Journal of Marketing 32 13 Network World 39
14 IEEE/ACM International Symposium on Cluster, Cloud, and
Grid Computing 31 14 Financial Executive 37
15 International Journal of Technology Management 29 15 Healthcare Financial
Management 33
16 ACM SIGKDD International Conf. on Knowledge Discovery
and Data Mining 28 16 Chain Store Age 40
17 International Symposium on Natural Language Processing 22 17 Strategic Finance 29
18 IEEE Internet Computing 21 18 Traffic World 28
19 International Conf. on Computational Intelligence and Software
Engineering 21 19 Data Strategy 27
20 IEEE Software 20 20 CFO 25
work. Journals are somewhat more limited in their publi-
cation volume, although it is notable that the IS journal
Decision Support Systems made the top 20 list (at #9). A few
business school journals also contain related BI&A research
in areas such as business ethics, marketing, and technology
management. Other major IS publications also published
business intelligence related articles, but at a lower rate than
the aforementioned sources (see Table 5). Relevant sources
from industry tend to be general IT publications, without a
specific BI focus (e.g., ComputerWorld at #1, Information
Today at #2, and InformationWeek at #3), as shown in
Table 4. However, there are some focused sources as well,
such as Microsoft Data Mining (#5), KM World (#8), and
CRM Magazine (#12), that are more relevant to the BI&A
related topics of data mining, knowledge management, and
customer relation management. KM and CRM have tradi-
tionally been topics of interest to IS scholars.
1180
MIS Quarterly Vol. 36 No. 4/December 2012
282
Table 6 summarizes the top-20 academic authors with BI&A
publications. Most of these authors are from IS and CS, with
several others from the related fields of marketing, manage-
ment, communication, and mathematics. Many of these
authors are close collaborators, for example, Hsinchun Chen
(#1), Jay F. Nunamaker (#18), Michael Chau (#11), and
Wingyan Chung (#18) through the University of Arizona
connection, 11 and Barabara H. Wixom (#5) and Hugh J.
Watson (#5) through the University of Georgia connection.
We also report the PageRank score (Brin and Page 1998), a
popular metric for data and network analytics, for the BI&A
authors based on the coauthorship network within BI&A
publications. A higher PageRank score captures an author’s
propensity to collaborate with other prolific authors. The
11
Readers are welcome to contact the authors for validation of our data set
and results or for additional analysis.Chen et al./Introduction: Business Intelligence Research
Table 5. Major IS Journals with BI&A Publications
Academic Publication
Publications
Decision Support Systems 41
Communications of the AIS 19
Journal of Management Information Systems 12
Management Science 10
Information Systems Research 9
Journal of the Association for Information Systems 5
INFORMS Journal on Computing 4
Management Information Systems Quarterly 2
Table 6. Top Academic Authors in BI&A
Rank
1
Name
Hsinchun Chen
Affiliation
Discipline
University of Arizona, U.S.
IS
Region Total PageRank
North America 19 7.471
4.276
2 Shenghong Li Zhejiang University, China Math Asia 16 3 Yong Shi University of Nebraska, U.S. CS North America 15 3.708
4 Kin Keung Lai City University of Hong Kong, China IS Asia 14 4.780
5 Barbara H. Wixom University of Virginia, U.S. IS North America 8 2.727
5 Hugh J. Watson University of Georgia, U.S. IS North America 8 2.485
5 Elizabeth Chang Curtin University, Australia IS Australia 8 2.381
5 Sheila Wright De Montfort University, U.K. Marketing Europe 8 2.859
5 Matteo Golfarelli University of Bologna, Italy CS Europe 8 1.785
5 Farookh Hussain University of Technology Sydney, Australia CS Australia 8 1.264
11 Michael Chau Hong Kong University, China IS Asia 7 1.788
11 Josef Schiefer Vienna University of Technology, Austria CS Europe 7 2.731
11 Craig S. Fleisher College of Costal Georgia, U.S. Management North America 7 1.042
14 Lingling Zhang Towson University, U.S. Communication North America 6 2.328
14 Olivera Marjanovic University of Sydney, Australia IS Australia 6 2.464
16 Xiaofeng Zhang Changsha University of Science and
Technology, China IS Asia 5 2.393
16 Stefano Rizzi University of Bologna, Italy CS Europe 5 1.683
18 Jay F. Nunamaker University of Arizona, U.S. IS North America 4 2.792
18 Wingyan Chung Santa Clara University, U.S. IS North America 4 1.761
18 Zahir Urabu Brunel University, U.K. Management Europe 4 2.241
analysis reveals broad and even contribution of authors from
North America, Asia, Europe, and Australia, reflecting the
diversity and international interest in the field of BI&A.
The last set of analyses investigated the content of BI&A
publications from 2000–2011. Mallet (McCallum 2002), a
Java-based open-source NLP text analytics tool, was used to
extract the top bigrams (two-word phrases) for each year. A
few bi-grams were combined to form more meaningful BI-
related trigrams such as “customer relation management” and
“enterprise resource planning.” These keywords were then
ranked based on their frequency, and the top 30 keywords
displayed using the tagcloud visualization. More important
keywords are highlighted with larger fonts as shown in
Figure 3. For example, competitive advantage, big data, data
warehousing, and decision support emerged as the top four
topics in the BI&A literature. Other BI&A related topics such
as customer relation management, data mining, competitive
MIS Quarterly Vol. 36 No. 4/December 2012
1181Chen et al./Introduction: Business Intelligence Research
Figure 3. Tagcloud Visualization of Major Topics in the BI&A Literature
intelligence, enterprise resource planning, and knowledge
management were also highly ranked. Overall, the topics
extracted were highly relevant to BI&A, especially for its
managerial and application values, although most of the
detailed technical terms, as described in the previous research
framework sections, were not present. This could be attri-
buted to the tendency of authors to use broad terminologies in
article titles and abstracts.
BI&A Education and Program
Development
BI&A provides opportunities not only for the research com-
munity, but also for education and program development. In
July 2012, Columbia University and New York City
announced plans to invest over $80 million dollars in a new
Center for Data Science, which is expected to generate
thousands of jobs and millions of dollars in tax revenues from
100 startup companies over the next 10 years (Associated
Press 2012). BI&A is data science in business. Job postings
seeking data scientists and business analytics specialists
abound these days. There is a clear shortage of professionals
with the “deep” knowledge required to manage the three V’s
of big data: volume, velocity, and variety (Russom 2011).
There is also an increasing demand for individuals with the
deep knowledge needed to manage the three “perspectives” of
business decision making: descriptive, predictive, and pre-
scriptive analytics. In this section, we describe BI&A educa-
tion in business schools, present the challenges facing IS
departments, and discuss BI&A program development oppor-
tunities. We also provide some suggestions for the IS disci-
pline in addressing these challenges (Chiang et al. 2012).
1182
MIS Quarterly Vol. 36 No. 4/December 2012
Education Challenges
BI&A focuses on understanding, interpretation, strategizing,
and taking action to further organizational interests. Several
academic disciplines have contributed to BI&A, including IS,
CS, Statistics, Management, and Marketing, as shown in our
bibliometric study. IS programs, in particular, are uniquely
positioned to train a new generation of scholars and students
due to their emphasis on key data management and infor-
mation technologies, business-oriented statistical analysis and
management science techniques, and broad business disci-
pline exposure (e.g., Finance, Accounting, Marketing, and
Economics).
Since its inception approximately 45 years ago, IS as an
academic discipline has primarily focused on business needs
in an era when the major challenges involved the management
of internal business and transaction data. In the age of big
data, these problems remain, but the emphasis in industry has
shifted to data analysis and rapid business decision making
based on huge volumes of information. Such time-critical
decision making largely takes place outside of the IS function
(i.e., in business units such as marketing, finance, and
logistics). Can IS programs serve the needs of these business
decision makers? Can we provide courses in data mining,
text mining, opinion mining, social media/network analytics,
web mining, and predictive analytics that are required for
marketing and finance majors? We should also ask ourselves
about the skill sets needed by students. Should we recruit
students with strong math and statistical skills, for example?
We contend that a new vision for IS, or at least for some IS
programs, should address these questions.
BI&A presents a unique opportunity for IS units in business
schools to position themselves as a viable option for edu-Chen et al./Introduction: Business Intelligence Research
cating professionals with the necessary depth and academic
rigor to tackle the increased complexity of BI&A issues. IS
programs housed within business schools have access to a
variety of business courses, as well as courses intended to
improve communication and presentation skills. It is also
common for business schools to house management science
and statistics faculty in the same IS unit.
BI&A Knowledge and Skills
BI&A education should be interdisciplinary and cover critical
analytical and IT skills, business and domain knowledge, and
communication skills required in a complex data-centric
business environment.
Analytical and IT skills include a variety of evolving topics.
They are drawn from disciplines such as statistics and
computer science for managing and analyzing both structured
data and unstructured text. Coverage of these topics ranges
from BI&A 1.0 to BI&A 3.0. The academic programs
intended to produce BI&A professionals should consider
these analytical and IT skills as suggested in Table 3 of our
research framework.
To provide useful insights and decision-making support, the
BI&A professionals must be capable of understanding the
business issues and framing the appropriate analytical solu-
tions. The necessary business knowledge for BI&A profes-
sionals ranges from general familiarity with the areas of
Accounting, Finance, Management, Marketing, Logistics, and
Operation Management, to the domain knowledge required in
specific BI&A applications, some of which are discussed
earlier and summarized in Table 2.
The importance of an organization-wide culture for informed
fact-based decision making for business analytics is empha-
sized by Davenport (2006). To support such a culture, BI&A
professionals need to know not only how to turn raw data and
information (through analytics) into meaningful and action-
able knowledge for an organization, but also how to properly
interact with and communicate this knowledge to the business
and domain experts of the organization.
Program Development
BI&A provides a unique opportunity for IS units in business
schools to develop new courses, certificate programs, and
degree programs charged with preparing the next generation
of analytical thinkers. There are many options for delivering
BI&A education. Because of the depth of knowledge
required, graduate programs are the obvious choice. Viable
program development options in delivering BI&A education
include
•
•
•
creating a Master of Science (MS) degree in BI&A
creating a BI&A concentration in existing MS IS
programs
offering a graduate BI&A certificate program
The first option requires the effort of developing a new
program. A few universities have embarked on this endeavor.
A nonexhaustive list includes North Carolina State Univer-
sity, Saint Joseph’s University, Northwestern University, the
University of Denver, Stevens Institute of Technology, and
Fordham University. New York University will launch its
new program in May 2013. New MS degree programs can be
designed explicitly to attract analytically strong students with
undergraduate degrees in areas such as mathematics, science,
and computer science, and to prepare these students for
careers, not only in the IS or IT groups in industry, but also in
functional areas such as research and development, marketing,
media, logistics, and finance.
The second option leverages existing MS IS programs with a
BI&A concentration that would supplement the already
existing curriculum in IT, data management, and business and
communication courses with additional analytics coverage.
This option has been adopted by a number of schools
including the IS departments at Carnegie Mellon University
and the University of Arizona. This option provides BI&A
knowledge and skills for students who will primarily find
careers in IS groups in industry.
For working IT professionals who wish to expand into BI&A,
a part-time MS or certificate program (the third option) offer
practical and valid alternatives. These certificate programs
can be delivered online or on-site and need to provide the
skills that will complement the current IT or business experi-
ence of IT professionals, and/or provide technical and analy-
tical skills to business professionals in non-IT areas. Online
programs that are currently available include Northwestern
University’s MS in Predictive Analytics and Stanford Univer-
sity’s Graduate Certificate on Mining Big Data. In addition,
IS programs can help design a BI&A concentration in MBA
programs to help train a new generation of data- and
analytics-savvy managers.
A key to success for a BI&A program is to integrate the
concept of “learning by doing” in the BI&A curriculum via
hands-on projects, internships, and industry-guided practicum.
Big data analytics requires trial-and-error and experimen-
tation. Strong relationships and partnerships between aca-
demic programs and industry partners are critical to foster the
experiential learning aspect of the BI&A curriculum.
MIS Quarterly Vol. 36 No. 4/December 2012
1183Chen et al./Introduction: Business Intelligence Research
Papers in this Special Issue
The idea for this special issue began in May 2009, when
Detmar Straub, the Editor-in-Chief of MIS Quarterly, soli-
cited suggestions for special issues from the editorial board
members. We submitted the special issue proposal on Busi-
ness Intelligence Research in August 2009, with the call-for-
papers approved and distributed at the 30 th Annual Interna-
tional Conference on Information Systems (ICIS) in Decem-
ber of that year. Submissions to this special issue needed to
relate to MIS Quarterly’s mission with strong managerial,
organizational, and societal relevance and impact. In addition
to the Design Science approach (Hevner et al. 2004; March &
Storey 2008), rigorous and relevant BI-related research using
management science (modeling, optimization), information
economics, and organizational and behavioral methodologies
(case studies, surveys) was also welcomed. A total of 62
manuscripts was received by October 2010. In the following
20 months, six of the manuscripts went through three or four
review rounds and were then accepted for this issue.
The six papers address various aspects of the BI&A research
framework presented in this introduction paper (see Table 7).
All six papers are based on BI&A 1.0, with three also based
on BI&A 2.0. The first three papers by Chau and Xu, Park et
al., and Lau et al. focus on BI&A 2.0 with applications on e-
commerce and market intelligence using text, web, and net-
work analytics. In the next two papers, both Hu et al. and
Abbasi et al. work in the category of BI&A 1.0 with a focus
on security, but Hu et al. use network analytics whereas
Abbasi et al. emphasize security analysis and data analytics.
Finally, Sahoo et al. also work in BI&A 1.0, with direct appli-
cation to e-commerce and market intelligence using web and
data analytics.
In “Business Intelligence in Blogs: Understanding Consumer
Interactions and Communities,” Michael Chau and Jennifer
Xu recognized the potential “gold mine” of blog content for
business intelligence and developed a framework for
gathering business intelligence by automatically collecting
and analyzing blog content and bloggers’ interaction net-
works. A system developed using this framework was
applied to two case studies, which revealed novel patterns in
blogger interactions and communities.
Sung-Hyuk Park, Soon-Young Huh, Wonseok Oh, and Sang
Pil Han in their paper, “A Social Network-Based Inference
Model for Validating Customer Profile Data,” argue that busi-
ness intelligence systems are of limited value when they deal
with inaccurate and unreliable data. The authors proposed a
social network-driven inference framework to determine the
accuracy and reliability of self-reported customer profiles.
The framework utilized the individuals’ social circles and
1184
MIS Quarterly Vol. 36 No. 4/December 2012
communication patterns within their circles. To construct the
specific inference and validation model, a combination of
methods was used, including query processing, statistical
inference, social network analysis, and user profiling. The
authors analyzed over 20 million actual mobile call trans-
actions and their proposed social network-based inference
model consistently outperformed the alternative approaches.
In “Web 2.0 Environmental Scanning and Adaptive Decision
Support for Business Mergers and Acquisitions,” Raymond
Lau, Stephen Liao, K. F. Wong, and Dickson Chiu analyzed
company mergers and acquisitions (M&A). Online environ-
mental scanning with Web 2.0 provides top executives with
opportunities to tap into collective web intelligence to develop
better insights about the socio-cultural and political-economic
factors that cross-border M&As face. Grounded on Porter’s
five forces model, this research designed a due diligence
scorecard model that leverages collective web intelligence to
enhance M&A decision making. The authors also developed
an adaptive business intelligence (BI) 2.0 system, which they
applied to Chinese companies’ cross-border M&A activities.
In their paper, “Network-Based Modeling and Analysis of
Systemic Risk in Banking Systems,” Daning Hu, J. Leon
Zhao, Zhimin Hua, and Michael Wong analyzed systemic risk
in banking systems by treating banks as a network linked with
financial relationships, leading to a network approach to risk
management (NARM). The authors used NARM to analyze
systemic risk attributed to each individual bank via simulation
based on real-world data from the Federal Deposit Insurance
Corporation. NARM offered a new means by which con-
tagious bank failures could be predicted and capital injection
priorities at the individual bank level could be determined in
the wake of a financial crisis. A simulation study showed
that, under significant market shocks, the interbank payment
links became more influential than the correlated bank
portfolio links in determining an individual bank’s survival.
Ahmed Abbasi, Conan Albrecht, Anthony Vance, and James
Hansen in their paper, “MetaFraud: A Meta-Learning Frame-
work for Detecting Financial Fraud,” employed a design
science approach to develop MetaFraud, a meta-learning
framework for enhanced financial fraud detection. A series
of experiments was conducted on thousands of legitimate and
fraudulent firms to demonstrate the effectiveness of the frame-
work over existing benchmark methods. The research results
have implications for compliance officers, investors, audit
firms, and regulators.
The paper by Nachiketa Sahoo, Param Vir Singh, and Tridas
Mukhopadhyay, “A Hidden Markov Model for Collaborative
Filtering,” reports on the analysis of making personalized
recommendations when user preferences are changing. TheChen et al./Introduction: Business Intelligence Research
Table 7. Summary of Special Issue Papers Within the BI&A Research Framework
Authors and Titles
Evolutions
Applications
Data
Analytics/ Research
Impacts
Chau and Xu, “Business
Intelligence in Blogs: Under-
standing Consumer Inter-
actions and Communities” BI&A 2.0 on
social media
& network
analytics Market intelligence
on consumers and
communities User-generated
content extracted
from blogs • Text and network
analytics
• Community detection
• Network visualization Increased sales
and customer
satisfaction
Park et al., “A Social
Network-Based Inference
Model for Validating
Customer Profile Data” BI&A 1.0 &
2.0 on social
network
analysis and
statistical
analysis Market intelligence
in predicting cus-
tomers’ profiles Self-reported user
profiles and mobile
call records • Network analytics
• Anomaly detection
• Predictive analytics Personalized
recommendation
and increased
customer
satisfaction
Lau et al., “Web 2.0
Environmental Scanning and
Adaptive Decision Support
for Business Mergers and
Acquisitions” BI&A 1.0 and
2.0 on
scorecards
and web
analytics Market intelligence
on environmental
scanning Business information
extracted from
Internet and
proprietary financial
information • Text and web analytics
• Sentiment and affect
analysis
• Relation mining Strategic decision
making in
mergers and
acquisitions
Hu et al., “Network-Based
Modeling and Analysis of
Systemic Risk in Banking
Systems” BI&A 1.0 on
statistical
analysis Systemic risk
analysis and
management in
banking systems U.S. banking infor-
mation extracted from
FDIC and Federal
Reserve Wire
Network • Network and data
analytics
• Descriptive and
predictive modeling
• Discrete event simulation Monitoring and
mitigating of
contagious bank
failures
Abbasi et al., “MetaFraud: A
Meta-Learning Framework
for Detecting Financial
Fraud” BI&A 1.0 on
data mining
and meta-
learning Fraud detection Financial ratios, and
organizational and
industrial-level context
features • Data analytics
• Classification &
generalization
• Adaptive learning Financial fraud
detection
Sahoo et al., “A Hidden
Markov Model for Col-
laborative Filtering” BI&A 1.0 on
statistical
analysis Recommender sys-
tems with changing
user preferences Blog reading data,
Netflix prize data set,
and Last.fm data • Data and web analytics
• Statistical dynamic model
• Collaborative filtering Personalized
recommendations
authors proposed a hidden Markov model based on collabo-
rative filtering to predict user preferences and make the most
appropriate personalized recommendations for the predicted
preference. The authors employed real world data sets and
simulations to show that, when user preferences are changing,
there is an advantage to using the proposed algorithm over
existing ones.
Summary and Conclusions
Through BI&A 1.0 initiatives, businesses and organizations
from all sectors began to gain critical insights from the
structured data collected through various enterprise systems
and analyzed by commercial relational database management
systems. Over the past several years, web intelligence, web
analytics, web 2.0, and the ability to mine unstructured user-
generated contents have ushered in a new and exciting era of
BI&A 2.0 research, leading to unprecedented intelligence on
consumer opinion, customer needs, and recognizing new
business opportunities. Now, in this era of Big Data, even
while BI&A 2.0 is still maturing, we find ourselves poised at
the brink of BI&A 3.0, with all the attendant uncertainty that
new and potentially revolutionary technologies bring.
This MIS Quarterly Special Issue on Business Intelligence
Research is intended to serve, in part, as a platform and
conversation guide for examining how the IS discipline can
better serve the needs of business decision makers in light of
maturing and emerging BI&A technologies, ubiquitous Big
Data, and the predicted shortages of data-savvy managers and
of business professionals with deep analytical skills. How
can academic IS programs continue to meet the needs of their
traditional students, while also reaching the working IT
professional in need of new analytical skills? A new vision
for IS may be needed to address this and other questions.
By highlighting several applications such as e-commerce,
market intelligence, e-government, healthcare, and security,
and by mapping important facets of the current BI&A
knowledge landscape, we hope to contribute to future sources
of knowledge and to augment current discussions on the
importance of (relevant) academic research.
MIS Quarterly Vol. 36 No. 4/December 2012
1185Chen et al./Introduction: Business Intelligence Research
Finally, the six papers chosen for this special issue are them-
selves a microcosm of the current state of BI&A research.
These “best of the best” research papers showcase how high-
quality academic research can address real-world problems
and contribute solutions that are relevant and long lasting—
exactly the challenge that our discipline continues to face.
Acknowledgments
We wish to thank the Editor-in-Chief of MIS Quarterly, Detmar
Straub, for his strong support for this special issue from its incep-
tion. He shared the belief that business intelligence and analytics is
an emerging and critical IS research area. We appreciate the
continued support from the incoming Editor-in-Chief, Paulo Goes,
and his feedback on an earlier version of this paper. We also thank
Janice DeGross and Jennifer Syverson from the MIS Quarterly
office for their professional editorial support and Cathy Larson for
her support and assistance in managing the manuscripts and
coordinating the review process.
We are grateful to our excellent group of 35 associate editors (listed
below) and the reviewers (too numerous to name) who carried out
the review process in a timely manner while still meeting MIS
Quarterly's high expectations of scholarly quality. We thank the
authors of these 62 submissions who chose to submit their research
to our special issue. We are especially indebted to the associate
editors who handled the six accepted papers of the special issue.
They and the reviewers they invited offered valuable critiques and
suggestions throughout the review process. This special issue would
not have been possible without their efforts.
The research reported in this article was partially supported by the
following sources: National Science Foundation (NSF CMMI-
1057624, CMMI-0926270, CNS-0709338), Defense Threat Reduc-
tion Agency (DTRA HDTRA-09-0058), J. Mack Robinson College
of Business at the Georgia State University, Carl H. Lindner College
of Business at the University of Cincinnati, and the Eller College of
Management at the University of Arizona. We also thank the
following colleagues for their assistance or comments: Ee-Peng
Lim, Ted Stohr, Barbara Wixom, Yukai Lin, and Victor Benjamin.
Special Issue Associate Editors
Gediminas Adomavicius, University of Minnesota
Sue Brown, University of Arizona
Michael Chau, University of Hong Kong
Cecil Chua, University of Auckland
Wendy Currie, Audencia, Ecole de Management
Andrew Gemino, Simon Fraser University
Paulo Goes, University of Arizona
Alok Gupta, University of Minnesota
1186
MIS Quarterly Vol. 36 No. 4/December 2012
Paul Jen-Hwa Hu, University of Utah
Hemant Jain, University of Wisconsin – Milwaukee
Robert Kauffman, Singapore Management University
Vijay Khatri, Indiana University
Gondy Leroy, Claremont Graduate University
Ting-Peng Liang, National Chengchi University
Ee-Peng Lim, Singapore Management University
Vijay Mookerjee, University of Texas at Dallas
Sridhar Narasimhan, Georgia Institute of Technology
Jeffrey Parsons, Memorial University of Newfoundland
H. Raghav Rao, The State University of New York at Buffalo
Raghu T. Santanam, Arizona State University
Balasubramaniam Ramesh, Georgia State University
Ramesh Sharda, Oklahoma State University
Matti Rossi, Aalto University School of Economics
Michael Jeng-Ping Shaw, University of Illinois, Urbana-Champaign
Olivia Sheng, University of Utah
Keng Siau, Missouri University of Science and Technology
Atish Sinha, University of Wisconsin – Milwaukee
Alexander Tuzhilin, New York University
Vijay Vaishnavi, Georgia State University
Doug Vogel, City University of Hong Kong
Chih-Ping Wei, National Taiwan University
Barbara Wixom, University of Virginia
Carson Woo, University of British Columbia
Daniel Zeng, University of Arizona
J. Leon Zhao, City University of Hong Kong
References
Adomavicius, G., and Tuzhilin, A. 2005. “Toward the Next
Generation of Recommender Systems: A Survey of the State-of-
the-Art and Possible Extensions,” IEEE Transactions on
Knowledge and Data Engineering (17:6), pp. 734-749.
Anderson, C. 2004. “The Long Tail,” WIRED Magazine (12:10)
(http://www.wired.com/wired/archive/12.10/tail.html).
Associated Press. 2012. “Columbia U Plans New Institute for Data
Sciences,” July 30 (http://www.cbsnews.com/8301-505245_162-
57482466/columbia-u-plans-new-institute-for-data-sciences/,
accessed August 3, 2012).
Barabási, A. 2003. Linked: How Everything Is Connected to
Everything Else and What it Means for Business, Science, and
Everyday Life, New York: Plume.
Batagelj, V., and Mrvar, A. 1998. “Pajek: A Program for Large
Network Analysis,” Connections (21), pp. 47-57.
Bettencourt, L. M.A., Cintrón-Arias, A., Kaiser, D. I., and Castillo-
Chávez, C. 2006. “The Power of a Good Idea: Quantitative
Modeling of the Spread of Ideas from Epidemiological Models,”
Physica A (364), pp. 513-536.
Bitterer, A. 2011. “Hype Cycle for Business Intelligence,” Gartner,
Inc., Stamford, CT.
Blei, D. M. 2012. “Probabilistic Topic Models,” Communications
of the ACM (55:4), pp. 77-84.
Bloomberg Businessweek. 2011. “The Current State of Business
Analytics: Where Do We Go from Here?,” Bloomberg Business-Chen et al./Introduction: Business Intelligence Research
week Research Services (http://www.sas.com/resources/asset/
busanalyticsstudy_wp_08232011.pdf).
Borgatti, S. P., Everett, M. G., and Freeman, L. C. 2002. UCInet
for Windows: Software for Social Network Analysis, Harvard,
MA: Analytic Technologies.
Brantingham, P. L. 2011. “Computational Criminology,” Keynote
Address to the European Intelligence and Security Informatics
Conference, Athens, Greece, September 12-14.
Brin, S.,and Page, L. 1998. “The Anatomy of a Large-Scale Hyper-
textual Web Search Engine,” Computer Network and ISDN
Systems (30), pp. 107-117.
Brumfiel, G. 2911, “High-Energy Physics: Down the Petabyte
Highway,” Nature (469), pp. 282-283.
Chaudhuri, S., Dayal, U., and Narasayya, V. 2011. “An Overview
of Business Intelligence Technology,” Communications of the
ACM (54:8), pp. 88-98.
Chen, H. 2006. Intelligence and Security Informatics for Inter-
national Security: Information Sharing and Data Mining, New
York: Springer.
Chen, H. 2009. “AI, E-Government, and Politics 2.0,” IEEE
Intelligent Systems (24:5), pp. 64-67.
Chen, H. 2011a. “Design Science, Grand Challenges, and Societal
Impacts,” ACM Transactions on Management Information
Systems (2:1), pp. 1:1-1:10.
Chen, H. 2011b. “Smart Health and Wellbeing,” IEEE Intelligent
Systems (26:5), pp. 78-79.
Chen, H. 2012. Dark Web: Exploring and Mining the Dark Side of
the Web, New york: Springer.
Chen, H., Brandt, L., Gregg, V., Traunmuller, R., McIntosh, A.,
Dawes, S., Hovy, E., and Larson, C. A. (eds.). 2007. Digital
Government: E-Government Research, Case Studies, and Imple-
mentation, New York: Springer.
Chen, H., Reid, E., Sinai, J., Silke, A., and Ganor, B. (eds.). 2008.
Terrorism Informatics: Knowledge Management and Data
Mining for Homeland Security, New York: Springer.
Chiang, R. H. L., Goes, P., and Stohr, E. A. 2012. “Business
Intelligence and Analytics Education and Program Development:
A Unique Opportunity for the Information Systems Discipline,”
ACM Transactions on Management Information Systems (3:3),
forthcoming.
Davenport, T. H. 2006. “Competing on Analytics,” Harvard
Business Review (84:1), p. 98-107.
Doan, A., Ramakrishnan, R., and Halevy, A. Y. 2011. “Crowd-
sourcing Systems on the World-Wide Web,” Communications of
the ACM (54:4), pp. 86-96.
Fortunato, S. 2010. “Community Detection in Graphs,” Physics
Reports (486:3-5), pp. 75-174.
Frank, O., and Strauss, D. 1986. “Markov Graphs,” Journal of the
American Statistical Association (81:395), pp. 832-842.
Freeman, T. 2005. The World is Flat: A Brief History of the
Twenty-First Century, New York: Farrar, Straus, and Giroux.
Gelfand, A. 2011/2012. “Privacy and Biomedical Research:
Building a Trust Infrastructure—An Exploration of Data-Driven
and Process-Driven Approaches to Data Privacy,” Biomedical
Computation Review, Winter, pp. 23-28 (available at
http://biomedicalcomputationreview.org/content/privacy-and-
biomedical-research-building-trust-infrastructure, accessed
August 2, 2012).
Hanauer, D. A. , Rhodes, D. R., and Chinnaiyan, A. M. 2009.
“Exploring Clinical Associations Using ‘-Omics’ Based
Enrichment Analyses,” PLoS ONE (4:4): e5203.
Hanauer, D. A., Zheng, K., Ramakrishnan, N., and Keller, B. J.
2011. “Opportunities and Challenges in Association and Episode
Discovery from Electronic Health Records,” IEEE Intelligent
Systems (26:5), pp. 83-87.
Henschen, D. 2011. “Why All the Hadoopla?” Information Week,
November 14, pp. 19-26.
Hevner, A., March, S. T., Park, J., and Ram. S. 2004. “Design
Science Research in Information Systems,” MIS Quarterly
(28:1), pp. 75-105.
Hirsch, J. E. 2005. “An Index to Quantify an Individual’s Scientific
Research Output,” Proceedings of the National Academy of
Sciences of the United States of America (102:46), pp.
16569-16572.
Hunter, D. R., Handcock, M. S., Butts, C. T., Goodreau, S. M., and
Morris, M. 2008. “ergm: A Package to Fit, Simulate and
Diagnose Exponential-Family Models for Network,” Journal of
Statistical Software (24:3) (http://www.ncbi.nlm.nih.gov/
pmc/articles/PMC2743438/).
IBM. 2011. “The 2011 IBM Tech Trends Report: The Clouds are
Rolling In...Is Your Business Ready?,” November 15
(http://www.ibm.com/developerworks/techntrendsreport;
accessed August 4, 2012)..
Karpf, D. 2009. “Blogsphere Research: A Mixed-Methods
Approach to Rapidly Changing Systems,” IEEE Intelligent
Systems (24:5), pp. 67-70.
Liben-Nowell, D., and Kleinberg, J. 20007. “The Link-Prediction
Problem for Social Networks,” Journal of American Society for
Information Science and Technology (58:7), pp. 1019-1031.
Lin, Y., Brown, R. A., Yang, H. J., Li, S., Lu, H., and Chen, H.
2011. “Data Mining Large-Scale Electronic Health Records for
Clinical Support,” IEEE Intelligent Systems (26:5), pp. 87-90.
Lusch, R. F., Liu, Y., and Chen, Y. 2010. “The Phase Transition of
Markets and Organizations: The New Intelligence and Entre-
preneurial Frontier,” IEEE Intelligent Systems (25:1), pp. 71-75.
Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh,
C., and Byers, A. H. 2011. “Big Data: The Next Frontier for
Innovation, Competition, and Productivity,” McKinsey Global
Institute (http://www.mckinsey.com/insights/mgi/research/
technology_and_innovation/big_data_the_next_frontier_for_in
novation; accessed August 4, 2012).
Manning, C. D., and Schütze, H. 1999. Foundations of Statistical
Natural Language Processing, Cambridge, MA: The MIT Press.
March, S. T., and Storey, V. C. 2008. “Design Science in the
Information Systems Discipline,” MIS Quarterly (32:4), pp.
725-730.
Maybury, M. T. (ed.). 2004. New Directions in Question
Answering, Cambridge, MA: The MIT Press.
McCallum, A. 2002. “Mallet: A Machine Learning for Language
Toolkit,” University of Massachusetts, Amherst
(http://mallet.cs.umass.edu/).
MIS Quarterly Vol. 36 No. 4/December 2012
1187Chen et al./Introduction: Business Intelligence Research
Miller, K. 2012a. “Big Data Analytics in Biomedical Research,”
Biomedical Computation Review (available at http://
biomedicalcomputationreview.org/content/big-data-analytics-
biomedical-research; accessed August 2, 2012).
Miller, K. 2012b. “Leveraging Social Media for Biomedical
Research: How Social Media Sites Are Rapidly Doing Unique
Research on Large Cohorts,” Biomedical Computation Review
(available at http://biomedicalcomputationreview.org/content/
leveraging-social-media-biomedical-research; accessed August
2, 2012).
National Research Council. 2008. Behavioral Modeling and Simu-
lation: From Individuals to Societies, Committee on Organiza-
tional Modeling: From Individuals to Societies, G. L. Zacharias,
J. MacMillan and S. B. Van Hemel (eds.), Board on Behavioral,
Cognitive, and Sensory Sciences, Division of Behavioral and
Social Sciences and Education, Washington, DC: The National
Academies Press.
O’Reilly, T. 2005. “What Is Web 2.0? Design Patterns and
Business Models for the Next Generation of Software,”
September 30, (http://www.oreillynet.com/pub/a/oreilly/tim/
news/2005/09/30/what-is-web-20.html).
Pang, B., and Lee, L. 2008. “Opinion Mining and Sentiment
Analysis,” Foundations and Trends in Information Retrieval
(2:1-2), pp. 1-135.
Patterson, D. A. 2008. “Technical Perspective: The Data Center Is
the Computer,” Communications of the ACM (51:1), p. 105.
Perlroth, N., and Rusli, E. M. 2012. “Security Start-Ups Catch
Fancy of Investors,” New York Times, Technology Section,
August 5.
Robins, G., Pattison, P., Kalish, Y., and Lusher, D. 2007. “An
Introduction to Exponential Random Graph (p*) Models for
Social Networks,” Social Networks (29:2), pp. 173-191.
Russom, P. 2011. “Big Data Analytics,” TDWI Best Practices
Report, Fourth Quarter.
Sallam, R. L., Richardson, J., Hagerty, J., and Hostmann, B. 2011.
“Magic Quadrant for Business Intelligence Platforms,” Gartner
Group, Stamford, CT.
Salton, G. 1989. Automatic Text Processing, Reading, MA:
Addison Wesley.
Schonfeld, E. 2005. “The Great Giveaway,” Business 2.0 (6:3), pp.
80-86.
1188
MIS Quarterly Vol. 36 No. 4/December 2012
Snider, M. 2012. “More Businesses Getting Their Game On,” USA
Today, July 30.
Stonebraker, M., Abadi, D., DeWitt, D. J., Madden, S., Pavlo, A.,
and Rasin, A. 2012. “MapReduce and Parallel DBMSs: Friends
or Foes,” Communications of the ACM (53:1), pp. 64-71.
The Economist. 2010a. “The Data Deluge,” Special Report on
Managing Information, Technology Section, February 25
(http://www.economist.com/node/15579717).
The Economist. 2010b. “All Too Much,” Special Report on
Managing Information, Technology Section, February 25
(http://www.economist.com/node/15557421).
The Economist. 2011. “Beyond the PC,” Special Report on
Personal Technology, October 8 (http://www.economist.com/
node/21531109).
Turban, E., Sharda, R., Aronson, J. E., and King, D. 2008. Business
Intelligence: A Managerial Approach, Boston: Pearson Prentice
Hall,
U.S. Office of Homeland Security. 2002. National Strategy for
Homeland Security, Washington, DC: Office of Homeland
Security.
van der Aalst, W. 2012. “Process Mining: Overview and
Opportunities,” ACM Transactions on Management Information
Systems (3:2), pp. 7:1-7:17.
Wactlar, H., Pavel, M., and Barkis, W. 2011. “Can Computer
Science Save Healthcare?” IEEE Intelligent Systems (26:5), pp.
79-83.
Watson, H. J., and Wixom, B. H. 2007. “The Current State of
Business Intelligence,” IEEE Computer (40:9), pp. 96-99.
Watts, D. 2003. Six Degrees: The Science of a Connected Age,
New York: W. W. Norton.
Witten, I. H., Frank, E., and Hall, M. 2011. Data Mining:
Practical Machine Learning Tools and Techniques (3 rd ed.), San
Francisco: Morgan Kaufmann.
Wu, X., Kumar, V., Quinlan, J. R., Ghosh, J., Yang, Q., Motoda, H.,
McLachlan, G. J., Ng, A., Liu, B., Yu, P.S., Zhou, Z.-H.,
Steinbach, M., Hand, D. J., and Steinberg, D. 2007. “Top 10
Algorithms in Data Mining,” Knowledge and Information
Systems (14:1), pp. 1-37.
Yang, H., and Callan, J. 2009. “OntoCop: Constructing Ontologies
for Public Comments,” IEEE Intelligent Systems (24:5), pp.
70-75.Copyright of MIS Quarterly is the property of MIS Quarterly & The Society for Information Management and
its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's
express written permission. However, users may print, download, or email articles for individual use.
Information, Communication & Society
ISSN: 1369-118X (Print) 1468-4462 (Online) Journal homepage: http://www.tandfonline.com/loi/rics20
CRITICAL QUESTIONS FOR BIG DATA
danah boyd & Kate Crawford
To cite this article: danah boyd & Kate Crawford (2012) CRITICAL QUESTIONS FOR BIG DATA,
Information, Communication & Society, 15:5, 662-679, DOI: 10.1080/1369118X.2012.678878
To link to this article: http://dx.doi.org/10.1080/1369118X.2012.678878
Published online: 10 May 2012.
Submit your article to this journal
Article views: 47886
View related articles
Citing articles: 179 View citing articles
Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=rics20
Download by: [Birla Institute of Technology and Science]
Date: 23 September 2015, At: 11:04danah boyd & Kate Crawford
CRITICAL QUESTIONS FOR BIG DATA
Provocations for a cultural,
technological, and scholarly
phenomenon
The era of Big Data has begun. Computer scientists, physicists, economists, mathemati-
cians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring
for access to the massive quantities of information produced by and about people, things,
and their interactions. Diverse groups argue about the potential benefits and costs of ana-
lyzing genetic sequences, social media interactions, health records, phone logs, govern-
ment records, and other digital traces left by people. Significant questions emerge.
Will large-scale search data help us create better tools, services, and public goods? Or
will it usher in a new wave of privacy incursions and invasive marketing? Will data ana-
lytics help us understand online communities and political movements? Or will it be used
to track protesters and suppress speech? Will it transform how we study human communi-
cation and culture, or narrow the palette of research options and alter what ‘research’
means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it
is necessary to critically interrogate its assumptions and biases. In this article, we offer
six provocations to spark conversations about the issues of Big Data: a cultural, techno-
logical, and scholarly phenomenon that rests on the interplay of technology, analysis, and
mythology that provokes extensive utopian and dystopian rhetoric.
Keywords Big Data; analytics; social media; communication studies;
social network sites; philosophy of science; epistemology; ethics; Twitter
(Received 10 December 2011; final version received 20 March 2012)
Technology is neither good nor bad; nor is it neutral . . . technology’s inter-
action with the social ecology is such that technical developments frequently
have environmental, social, and human consequences that go far beyond the
immediate purposes of the technical devices and practices themselves.
(Kranzberg 1986, p. 545)
Information, Communication & Society Vol. 15, No. 5, June 2012, pp. 662 –679
ISSN 1369-118X print/ISSN 1468-4462 online # 2012 Microsoft
http://www.tandfonline.com http://dx.doi.org/10.1080/1369118X.2012.678878CRITICAL QUESTIONS FOR BIG DATA
We need to open a discourse – where there is no effective discourse now –
about the varying temporalities, spatialities and materialities that we might
represent in our databases, with a view to designing for maximum flexibility
and allowing as possible for an emergent polyphony and polychrony. Raw
data is both an oxymoron and a bad idea; to the contrary, data should be
cooked with care. (Bowker 2005, pp. 183 – 184)
The era of Big Data is underway. Computer scientists, physicists, economists,
mathematicians, political scientists, bio-informaticists, sociologists, and other
scholars are clamoring for access to the massive quantities of information pro-
duced by and about people, things, and their interactions. Diverse groups
argue about the potential benefits and costs of analyzing genetic sequences,
social media interactions, health records, phone logs, government records, and
other digital traces left by people. Significant questions emerge. Will large-
scale search data help us create better tools, services, and public goods? Or
will it usher in a new wave of privacy incursions and invasive marketing? Will
data analytics help us understand online communities and political movements?
Or will analytics be used to track protesters and suppress speech? Will large quan-
tities of data transform how we study human communication and culture, or
narrow the palette of research options and alter what ‘research’ means?
Big Data is, in many ways, a poor term. As Manovich (2011) observes, it has
been used in the sciences to refer to data sets large enough to require supercom-
puters, but what once required such machines can now be analyzed on desktop
computers with standard software. There is little doubt that the quantities of data
now available are often quite large, but that is not the defining characteristic of
this new data ecosystem. In fact, some of the data encompassed by Big Data (e.g.
all Twitter messages about a particular topic) are not nearly as large as earlier
data sets that were not considered Big Data (e.g. census data). Big Data is less
about data that is big than it is about a capacity to search, aggregate, and
cross-reference large data sets.
We define Big Data 1 as a cultural, technological, and scholarly phenomenon
that rests on the interplay of:
(1) Technology: maximizing computation power and algorithmic accuracy to
gather, analyze, link, and compare large data sets.
(2) Analysis: drawing on large data sets to identify patterns in order to make
economic, social, technical, and legal claims.
(3) Mythology: the widespread belief that large data sets offer a higher form of
intelligence and knowledge that can generate insights that were previously
impossible, with the aura of truth, objectivity, and accuracy.
Like other socio-technical phenomena, Big Data triggers both utopian and dys-
topian rhetoric. On one hand, Big Data is seen as a powerful tool to address
663664
INFORMATION, COMMUNICATION & SOCIETY
various societal ills, offering the potential of new insights into areas as diverse as
cancer research, terrorism, and climate change. On the other, Big Data is seen as
a troubling manifestation of Big Brother, enabling invasions of privacy, decreased
civil freedoms, and increased state and corporate control. As with all socio-tech-
nical phenomena, the currents of hope and fear often obscure the more nuanced
and subtle shifts that are underway.
Computerized databases are not new. The US Bureau of the Census
deployed the world’s first automated processing equipment in 1890 – the
punch-card machine (Anderson 1988). Relational databases emerged in the
1960s (Fry & Sibley 1974). Personal computing and the Internet have made it
possible for a wider range of people – including scholars, marketers, govern-
mental agencies, educational institutions, and motivated individuals – to
produce, share, interact with, and organize data. This has resulted in what
Savage and Burrows (2007) describe as a crisis in empirical sociology. Data
sets that were once obscure and difficult to manage – and, thus, only of interest
to social scientists – are now being aggregated and made easily accessible to
anyone who is curious, regardless of their training.
How we handle the emergence of an era of Big Data is critical. While the
phenomenon is taking place in an environment of uncertainty and rapid
change, current decisions will shape the future. With the increased automation
of data collection and analysis – as well as algorithms that can extract and illus-
trate large-scale patterns in human behavior – it is necessary to ask which
systems are driving these practices and which are regulating them. Lessig
(1999) argues that social systems are regulated by four forces: market, law,
social norms, and architecture – or, in the case of technology, code. When it
comes to Big Data, these four forces are frequently at odds. The market sees
Big Data as pure opportunity: marketers use it to target advertising, insurance
providers use it to optimize their offerings, and Wall Street bankers use it to
read the market. Legislation has already been proposed to curb the collection
and retention of data, usually over concerns about privacy (e.g. the US Do
Not Track Online Act of 2011). Features like personalization allow rapid
access to more relevant information, but they present difficult ethical questions
and fragment the public in troubling ways (Pariser 2011).
There are some significant and insightful studies currently being done that
involve Big Data, but it is still necessary to ask critical questions about what all
this data means, who gets access to what data, how data analysis is deployed,
and to what ends. In this article, we offer six provocations to spark conversations
about the issues of Big Data. We are social scientists and media studies scholars
who are in regular conversation with computer scientists and informatics
experts. The questions that we ask are hard ones without easy answers, although
we also describe different pitfalls that may seem obvious to social scientists but are
often surprising to those from different disciplines. Due to our interest in and
experience with social media, our focus here is mainly on Big Data in socialCRITICAL QUESTIONS FOR BIG DATA
media context. That said, we believe that the questions we are asking are also
important to those in other fields. We also recognize that the questions we are
asking are just the beginning and we hope that this article will spark others to
question the assumptions embedded in Big Data. Researchers in all areas –
including computer science, business, and medicine – have a stake in the compu-
tational culture of Big Data precisely because of its extended reach of influence and
potential within multiple disciplines. We believe that it is time to start critically
interrogating this phenomenon, its assumptions, and its biases.
1. Big Data changes the definition of knowledge
In the early decades of the twentieth century, Henry Ford devised a manufactur-
ing system of mass production, using specialized machinery and standardized
products. It quickly became the dominant vision of technological progress.
‘Fordism’ meant automation and assembly lines; for decades onward, this
became the orthodoxy of manufacturing: out with skilled craftspeople and
slow work, in with a new machine-made era (Baca 2004). But it was more
than just a new set of tools. The twentieth century was marked by Fordism at
a cellular level: it produced a new understanding of labor, the human relationship
to work, and society at large.
Big Data not only refers to very large data sets and the tools and procedures
used to manipulate and analyze them, but also to a computational turn in thought
and research (Burkholder 1992). Just as Ford changed the way we made cars –
and then transformed work itself – Big Data has emerged a system of knowledge
that is already changing the objects of knowledge, while also having the power to
inform how we understand human networks and community. ‘Change the
instruments, and you will change the entire social theory that goes with
them’, Latour (2009) reminds us (p. 9).
Big Data creates a radical shift in how we think about research. Commenting
on computational social science, Lazer et al. (2009) argue that it offers ‘the
capacity to collect and analyze data with an unprecedented breadth and depth
and scale’ (p. 722). It is neither just a matter of scale nor is it enough to consider
it in terms of proximity, or what Moretti (2007) refers to as distant or close
analysis of texts. Rather, it is a profound change at the levels of epistemology
and ethics. Big Data reframes key questions about the constitution of knowledge,
the processes of research, how we should engage with information, and the
nature and the categorization of reality. Just as Du Gay and Pryke (2002) note
that ‘accounting tools . . . do not simply aid the measurement of economic
activity, they shape the reality they measure’ (pp. 12 –13), so Big Data stakes
out new terrains of objects, methods of knowing, and definitions of social life.
Speaking in praise of what he terms ‘The Petabyte Age’, Anderson, Editor-
in-Chief of Wired, writes:
665666
INFORMATION, COMMUNICATION & SOCIETY
This is a world where massive amounts of data and applied mathematics
replace every other tool that might be brought to bear. Out with every
theory of human behavior, from linguistics to sociology. Forget taxonomy,
ontology, and psychology. Who knows why people do what they do? The
point is they do it, and we can track and measure it with unprecedented fide-
lity. With enough data, the numbers speak for themselves. (2008)
Do numbers speak for themselves? We believe the answer is ‘no’. Significantly,
Anderson’s sweeping dismissal of all other theories and disciplines is a tell: it
reveals an arrogant undercurrent in many Big Data debates where other forms
of analysis are too easily sidelined. Other methods for ascertaining why people
do things, write things, or make things are lost in the sheer volume of
numbers. This is not a space that has been welcoming to older forms of intellectual
craft. As Berry (2011, p. 8) writes, Big Data provides ‘destablising amounts of
knowledge and information that lack the regulating force of philosophy’. Instead
of philosophy – which Kant saw as the rational basis for all institutions – ‘compu-
tationality might then be understood as an ontotheology, creating a new ontological
“epoch” as a new historical constellation of intelligibility’ (Berry 2011, p. 12).
We must ask difficult questions of Big Data’s models of intelligibility before
they crystallize into new orthodoxies. If we return to Ford, his innovation was
using the assembly line to break down interconnected, holistic tasks into
simple, atomized, mechanistic ones. He did this by designing specialized tools
that strongly predetermined and limited the action of the worker. Similarly,
the specialized tools of Big Data also have their own inbuilt limitations and
restrictions. For example, Twitter and Facebook are examples of Big Data
sources that offer very poor archiving and search functions. Consequently,
researchers are much more likely to focus on something in the present or
immediate past – tracking reactions to an election, TV finale, or natural disaster
– because of the sheer difficulty or impossibility of accessing older data.
If we are observing the automation of particular kinds of research functions,
then we must consider the inbuilt flaws of the machine tools. It is not enough to
simply ask, as Anderson has suggested ‘what can science learn from Google?’,
but to ask how the harvesters of Big Data might change the meaning of learning,
and what new possibilities and new limitations may come with these systems of
knowing.
2. Claims to objectivity and accuracy are misleading
‘Numbers, numbers, numbers’, writes Latour (2009). ‘Sociology has been
obsessed by the goal of becoming a quantitative science’. Sociology has never
reached this goal, in Latour’s view, because of where it draws the line
between what is and is not quantifiable knowledge in the social domain.CRITICAL QUESTIONS FOR BIG DATA
Big Data offers the humanistic disciplines a new way to claim the status of
quantitative science and objective method. It makes many more social spaces
quantifiable. In reality, working with Big Data is still subjective, and what it
quantifies does not necessarily have a closer claim on objective truth – particu-
larly when considering messages from social media sites. But there remains a
mistaken belief that qualitative researchers are in the business of interpreting
stories and quantitative researchers are in the business of producing facts. In
this way, Big Data risks re-inscribing established divisions in the long running
debates about scientific method and the legitimacy of social science and humanis-
tic inquiry.
The notion of objectivity has been a central question for the philosophy of
science and early debates about the scientific method (Durkheim 1895).
Claims to objectivity suggest an adherence to the sphere of objects, to things
as they exist in and for themselves. Subjectivity, on the other hand, is viewed
with suspicion, colored as it is with various forms of individual and social con-
ditioning. The scientific method attempts to remove itself from the subjective
domain through the application of a dispassionate process whereby hypotheses
are proposed and tested, eventually resulting in improvements in knowledge.
Nonetheless, claims to objectivity are necessarily made by subjects and are
based on subjective observations and choices.
All researchers are interpreters of data. As Gitelman (2011) observes, data
need to be imagined as data in the first instance, and this process of the imagin-
ation of data entails an interpretative base: ‘every discipline and disciplinary insti-
tution has its own norms and standards for the imagination of data’. As
computational scientists have started engaging in acts of social science, there is
a tendency to claim their work as the business of facts and not interpretation.
A model may be mathematically sound, an experiment may seem valid, but as
soon as a researcher seeks to understand what it means, the process of interpret-
ation has begun. This is not to say that all interpretations are created equal, but
rather that not all numbers are neutral.
The design decisions that determine what will be measured also stem from
interpretation. For example, in the case of social media data, there is a ‘data
cleaning’ process: making decisions about what attributes and variables will be
counted, and which will be ignored. This process is inherently subjective. As
Bollier explains,
As a large mass of raw information, Big Data is not self-explanatory. And yet
the specific methodologies for interpreting the data are open to all sorts of
philosophical debate. Can the data represent an ‘objective truth’ or is any
interpretation necessarily biased by some subjective filter or the way that
data is ‘cleaned?’. (2010, p. 13)
667668
INFORMATION, COMMUNICATION & SOCIETY
In addition to this question, there is the issue of data errors. Large data sets from
Internet sources are often unreliable, prone to outages and losses, and these
errors and gaps are magnified when multiple data sets are used together.
Social scientists have a long history of asking critical questions about the collec-
tion of data and trying to account for any biases in their data (Cain & Finch 1981;
Clifford & Marcus 1986). This requires understanding the properties and limits
of a data set, regardless of its size. A data set may have many millions of pieces of
data, but this does not mean it is random or representative. To make statistical
claims about a data set, we need to know where data is coming from; it is simi-
larly important to know and account for the weaknesses in that data. Further-
more, researchers must be able to account for the biases in their
interpretation of the data. To do so requires recognizing that one’s identity
and perspective informs one’s analysis (Behar & Gordon 1996).
Too often, Big Data enables the practice of apophenia: seeing patterns where
none actually exist, simply because enormous quantities of data can offer connec-
tions that radiate in all directions. In one notable example, Leinweber (2007)
demonstrated that data mining techniques could show a strong but spurious cor-
relation between the changes in the S&P 500 stock index and butter production
in Bangladesh.
Interpretation is at the center of data analysis. Regardless of the size of a
data, it is subject to limitation and bias. Without those biases and limitations
being understood and outlined, misinterpretation is the result. Data analysis is
most effective when researchers take account of the complex methodological
processes that underlie the analysis of that data.
3. Bigger data are not always better data
Social scientists have long argued that what makes their work rigorous is rooted
in their systematic approach to data collection and analysis (McCloskey 1985).
Ethnographers focus on reflexively accounting for bias in their interpretations.
Experimentalists control and standardize the design of their experiment.
Survey researchers drill down on sampling mechanisms and question bias. Quan-
titative researchers weigh up statistical significance. These are but a few of the
ways in which social scientists try to assess the validity of each other’s work.
Just because Big Data presents us with large quantities of data does not mean
that methodological issues are no longer relevant. Understanding sample, for
example, is more important now than ever.
Twitter provides an example in the context of a statistical analysis. Because it
is easy to obtain – or scrape – Twitter data, scholars have used Twitter to
examine a wide variety of patterns (e.g. mood rhythms (Golder & Macy
2011), media event engagement (Shamma et al. 2010), political uprisings
(Lotan et al. 2011), and conversational interactions (Wu et al. 2011)). WhileCRITICAL QUESTIONS FOR BIG DATA
many scholars are conscientious about discussing the limitations of Twitter data
in their publications, the public discourse around such research tends to focus on
the raw number of tweets available. Even news coverage of scholarship tends to
focus on how many millions of ‘people’ were studied (Wang 2011).
Twitter does not represent ‘all people’, and it is an error to assume ‘people’
and ‘Twitter users’ are synonymous: they are a very particular sub-set. Neither is
the population using Twitter representative of the global population. Nor can we
assume that accounts and users are equivalent. Some users have multiple
accounts, while some accounts are used by multiple people. Some people
never establish an account, and simply access Twitter via the web. Some accounts
are ‘bots’ that produce automated content without directly involving a person.
Furthermore, the notion of an ‘active’ account is problematic. While some users
post content frequently through Twitter, others participate as ‘listeners’ (Craw-
ford 2009, p. 532). Twitter Inc. has revealed that 40 percent of active users sign
in just to listen (Twitter 2011). The very meanings of ‘user’ and ‘participation’
and ‘active’ need to be critically examined.
Big Data and whole data are also not the same. Without taking into account
the sample of a data set, the size of the data set is meaningless. For example, a
researcher may seek to understand the topical frequency of tweets, yet if Twitter
removes all tweets that contain problematic words or content – such as refer-
ences to pornography or spam – from the stream, the topical frequency
would be inaccurate. Regardless of the number of tweets, it is not a representa-
tive sample as the data is skewed from the beginning.
It is also hard to understand the sample when the source is uncertain.
Twitter Inc. makes a fraction of its material available to the public through its
APIs. 2 The ‘firehose’ theoretically contains all public tweets ever posted and
explicitly excludes any tweet that a user chose to make private or ‘protected’.
Yet, some publicly accessible tweets are also missing from the firehose. Although
a handful of companies have access to the firehose, very few researchers have this
level of access. Most either have access to a ‘gardenhose’ (roughly 10 percent of
public tweets), a ‘spritzer’ (roughly one percent of public tweets), or have used
‘white-listed’ accounts where they could use the APIs to get access to different
subsets of content from the public stream. 3 It is not clear what tweets are
included in these different data streams or sampling them represents. It could
be that the API pulls a random sample of tweets or that it pulls the first few thou-
sand tweets per hour or that it only pulls tweets from a particular segment of the
network graph. Without knowing, it is difficult for researchers to make claims
about the quality of the data that they are analyzing. Are the data representative
of all tweets? No, because they exclude tweets from protected accounts. 4 But are
the data representative of all public tweets? Perhaps, but not necessarily.
Twitter has become a popular source for mining Big Data, but working with
Twitter data has serious methodological challenges that are rarely addressed by
those who embrace it. When researchers approach a data set, they need to
669670
INFORMATION, COMMUNICATION & SOCIETY
understand – and publicly account for – not only the limits of the data set, but
also the limits of which questions they can ask of a data set and what interpret-
ations are appropriate.
This is especially true when researchers combine multiple large data sets.
This does not mean that combining data does not offer valuable insights –
studies like those by Acquisti and Gross (2009) are powerful, as they reveal
how public databases can be combined to produce serious privacy violations,
such as revealing an individual’s Social Security number. Yet, as Jesper Anderson,
co-founder of open financial data store FreeRisk, explains: combining data from
multiple sources creates unique challenges. ‘Every one of those sources is error-
prone . . . I think we are just magnifying that problem [when we combine mul-
tiple data sets]’ (Bollier 2010, p. 13).
Finally, during this computational turn, it is increasingly important to recog-
nize the value of ‘small data’. Research insights can be found at any level, includ-
ing at very modest scales. In some cases, focusing just on a single individual can
be extraordinarily valuable. Take, for example, the work of Veinot (2007), who
followed one worker – a vault inspector at a hydroelectric utility company – in
order to understand the information practices of a blue-collar worker. In doing
this unusual study, Veinot reframed the definition of ‘information practices’ away
from the usual focus on early-adopter, white-collar workers, to spaces outside of
the offices and urban context. Her work tells a story that could not be discovered
by farming millions of Facebook or Twitter accounts, and contributes to the
research field in a significant way, despite the smallest possible participant
count. The size of data should fit the research question being asked; in some
cases, small is best.
4. Taken out of context, Big Data loses its meaning
Because large data sets can be modeled, data are often reduced to what can fit
into a mathematical model. Yet, taken out of context, data lose meaning and
value. The rise of social network sites prompted an industry-driven obsession
with the ‘social graph’. Thousands of researchers have flocked to Twitter and
Facebook and other social media to analyze connections between messages and
accounts, making claims about social networks. Yet, the relations displayed
through social media are not necessarily equivalent to the sociograms and
kinship networks that sociologists and anthropologists have been investigating
since the 1930s (Radcliffe-Brown 1940; Freeman 2006). The ability to represent
relationships between people as a graph does not mean that they convey equiv-
alent information.
Historically, sociologists and anthropologists collected data about people’s
relationships through surveys, interviews, observations, and experiments.
Using this data, they focused on describing people’s ‘personal networks’ – theCRITICAL QUESTIONS FOR BIG DATA
set of relationships that individuals develop and maintain (Fischer 1982). These
connections were evaluated based on a series of measures developed over time
to identify personal connections. Big Data introduces two new popular types of
social networks derived from data traces: ‘articulated networks’ and ‘behavioral
networks’.
Articulated networks are those that result from people specifying their con-
tacts through technical mechanisms like email or cell phone address books,
instant messaging buddy lists, ‘Friends’ lists on social network sites, and ‘Fol-
lower’ lists on other social media genres. The motivations that people have
for adding someone to each of these lists vary widely, but the result is that
these lists can include friends, colleagues, acquaintances, celebrities, friends-
of-friends, public figures, and interesting strangers.
Behavioral networks are derived from communication patterns, cell coordi-
nates, and social media interactions (Onnela et al. 2007; Meiss et al. 2008). These
might include people who text message one another, those who are tagged in
photos together on Facebook, people who email one another, and people who
are physically in the same space, at least according to their cell phone.
Both behavioral and articulated networks have great value to researchers,
but they are not equivalent to personal networks. For example, although con-
tested, the concept of ‘tie strength’ is understood to indicate the importance
of individual relationships (Granovetter 1973). When mobile phone data
suggest that workers spend more time with colleagues than their spouse, this
does not necessarily imply that colleagues are more important than spouses.
Measuring tie strength through frequency or public articulation is a common
mistake: tie strength – and many of the theories built around it – is a subtle
reckoning in how people understand and value their relationships with other
people. Not every connection is equivalent to every other connection, and
neither does frequency of contact indicate strength of relationship. Further,
the absence of a connection does not necessarily indicate that a relationship
should be made.
Data are not generic. There is value to analyzing data abstractions, yet
retaining context remains critical, particularly for certain lines of inquiry.
Context is hard to interpret at scale and even harder to maintain when data
are reduced to fit into a model. Managing context in light of Big Data will be
an ongoing challenge.
5. Just because it is accessible does not make it ethical
In 2006, a Harvard-based research group started gathering the profiles of 1,700
college-based Facebook users to study how their interests and friendships
changed over time (Lewis et al. 2008). These supposedly anonymous data
were released to the world, allowing other researchers to explore and analyze
671672
INFORMATION, COMMUNICATION & SOCIETY
them. What other researchers quickly discovered was that it was possible to de-
anonymize parts of the data set: compromising the privacy of students, none of
whom were aware their data were being collected (Zimmer 2008).
The case made headlines and raised difficult issues for scholars: what is the
status of so-called ‘public’ data on social media sites? Can it simply be used,
without requesting permission? What constitutes best ethical practice for
researchers? Privacy campaigners already see this as a key battleground where
better privacy protections are needed. The difficulty is that privacy breaches
are hard to make specific – is there damage done at the time? What about 20
years hence? ‘Any data on human subjects inevitably raise privacy issues, and
the real risks of abuse of such data are difficult to quantify’ (Nature, cited in
Berry 2011).
Institutional Review Boards (IRBs) – and other research ethics committees
– emerged in the 1970s to oversee research on human subjects. While unques-
tionably problematic in implementation (Schrag 2010), the goal of IRBs is to
provide a framework for evaluating the ethics of a particular line of research
inquiry and to make certain that checks and balances are put into place to
protect subjects. Practices like ‘informed consent’ and protecting the privacy
of informants are intended to empower participants in light of earlier abuses
in the medical and social sciences (Blass 2004; Reverby 2009). Although IRBs
cannot always predict the harm of a particular study – and, all too often,
prevent researchers from doing research on grounds other than ethics – their
value is in prompting researchers to think critically about the ethics of their
project.
Very little is understood about the ethical implications underpinning the Big
Data phenomenon. Should someone be included as a part of a large aggregate of
data? What if someone’s ‘public’ blog post is taken out of context and analyzed in
a way that the author never imagined? What does it mean for someone to be
spotlighted or to be analyzed without knowing it? Who is responsible for
making certain that individuals and communities are not hurt by the research
process? What does informed consent look like?
It may be unreasonable to ask researchers to obtain consent from every
person who posts a tweet, but it is problematic for researchers to justify their
actions as ethical simply because the data are accessible. Just because content
is publicly accessible does not mean that it was meant to be consumed by just
anyone. There are serious issues involved in the ethics of online data collection
and analysis (Ess 2002). The process of evaluating the research ethics cannot be
ignored simply because the data are seemingly public. Researchers must keep
asking themselves – and their colleagues – about the ethics of their data collec-
tion, analysis, and publication.
In order to act ethically, it is important that researchers reflect on the impor-
tance of accountability: both to the field of research and to the research subjects.
Accountability here is used as a broader concept than privacy, as Troshynski et al.CRITICAL QUESTIONS FOR BIG DATA
(2008) have outlined, where the concept of accountability can apply even when
conventional expectations of privacy are not in question. Instead, accountability
is a multi-directional relationship: there may be accountability to superiors, to
colleagues, to participants, and to the public (Dourish & Bell 2011). Academic
scholars are held to specific professional standards when working with human
participants in order to protect informants’ rights and well-being. However,
many ethics boards do not understand the processes of mining and anonymizing
Big Data, let alone the errors that can cause data to become personally identifi-
able. Accountability requires rigorous thinking about the ramifications of Big
Data, rather than assuming that ethics boards will necessarily do the work of
ensuring that people are protected.
There are also significant questions of truth, control, and power in Big Data
studies: researchers have the tools and the access, while social media users as a
whole do not. Their data were created in highly context-sensitive spaces, and it is
entirely possible that some users would not give permission for their data to be
used elsewhere. Many are not aware of the multiplicity of agents and algorithms
currently gathering and storing their data for future use. Researchers are rarely
in a user’s imagined audience. Users are not necessarily aware of all the multiple
uses, profits, and other gains that come from information they have posted. Data
may be public (or semi-public) but this does not simplistically equate with full
permission being given for all uses. Big Data researchers rarely acknowledge
that there is a considerable difference between being in public (i.e. sitting in a
park) and being public (i.e. actively courting attention) (boyd & Marwick 2011).
6. Limited access to Big Data creates new digital
divides
In an essay on Big Data, Golder (2010) quotes sociologist Homans (1974): ‘The
methods of social science are dear in time and money and getting dearer every
day’. Historically speaking, collecting data has been hard, time consuming, and
resource intensive. Much of the enthusiasm surrounding Big Data stems from the
perception that it offers easy access to massive amounts of data.
But who gets access? For what purposes? In what contexts? And with what
constraints? While the explosion of research using data sets from social media
sources would suggest that access is straightforward, it is anything but. As Man-
ovich (2011) points out, ‘only social media companies have access to really large
social data – especially transactional data. An anthropologist working for Face-
book or a sociologist working for Google will have access to data that the rest of
the scholarly community will not’. Some companies restrict access to their data
entirely; others sell the privilege of access for a fee; and others offer small data
sets to university-based researchers. This produces considerable unevenness in
the system: those with money – or those inside the company – can produce
673674
INFORMATION, COMMUNICATION & SOCIETY
a different type of research than those outside. Those without access can neither
reproduce nor evaluate the methodological claims of those who have privileged
access.
It is also important to recognize that the class of the Big Data rich is
reinforced through the university system: top-tier, well-resourced universities
will be able to buy access to data, and students from the top universities are
the ones most likely to be invited to work within large social media companies.
Those from the periphery are less likely to get those invitations and develop
their skills. The result is that the divisions between scholars will widen
significantly.
In addition to questions of access, there are questions of skills. Wrangling
APIs, scraping, and analyzing big swathes of data is a skill set generally restricted
to those with a computational background. When computational skills are posi-
tioned as the most valuable, questions emerge over who is advantaged and who is
disadvantaged in such a context. This, in its own way, sets up new hierarchies
around ‘who can read the numbers’, rather than recognizing that computer
scientists and social scientists both have valuable perspectives to offer. Signifi-
cantly, this is also a gendered division. Most researchers who have computational
skills at the present moment are male and, as feminist historians and philosophers
of science have demonstrated, who is asking the questions determines which
questions are asked (Harding 2010; Forsythe 2001). There are complex ques-
tions about what kinds of research skills are valued in the future and how
those skills are taught. How can students be educated so that they are equally
comfortable with algorithms and data analysis as well as with social analysis
and theory?
Finally, the difficulty and expense of gaining access to Big Data produce a
restricted culture of research findings. Large data companies have no responsi-
bility to make their data available, and they have total control over who gets
to see them. Big Data researchers with access to proprietary data sets are less
likely to choose questions that are contentious to a social media company if
they think it may result in their access being cut. The chilling effects on the
kinds of research questions that can be asked – in public or private – are some-
thing we all need to consider when assessing the future of Big Data.
The current ecosystem around Big Data creates a new kind of digital divide:
the Big Data rich and the Big Data poor. Some company researchers have even
gone so far as to suggest that academics should not bother studying social media
data sets – Jimmy Lin, a professor on industrial sabbatical at Twitter argued that
academics should not engage in research that industry ‘can do better’ (Conover
2011). Such explicit efforts to demarcate research ‘insiders’ and ‘outsiders’ –
while by no means new – undermine the research community. ‘Effective demo-
cratisation can always be measured by this essential criterion’, Derrida (1996)
claimed, ‘the participation in and access to the archive, its constitution, and
its interpretation’ (p. 4).CRITICAL QUESTIONS FOR BIG DATA
Whenever inequalities are explicitly written into the system, they produce
class-based structures. Manovich (2011) writes of three classes of people in the
realm of Big Data: ‘those who create data (both consciously and by leaving digital
footprints), those who have the means to collect it, and those who have expertise
to analyze it’. We know that the last group is the smallest, and the most privi-
leged: they are also the ones who get to determine the rules about how Big Data
will be used, and who gets to participate. While institutional inequalities may be
a forgone conclusion in academia, they should nevertheless be examined and
questioned. They produce a bias in the data and the types of research that
emerge.
By arguing that the Big Data phenomenon is implicated in some broad his-
torical and philosophical shifts is not to suggest it is solely accountable; the
academy is by no means the sole driver behind the computational turn. There
is a deep government and industrial drive toward gathering and extracting
maximal value from data, be it information that will lead to more targeted adver-
tising, product design, traffic planning, or criminal policing. But we do think
there are serious and wide-ranging implications for the operationalization of
Big Data, and what it will mean for future research agendas. As Suchman
(2011) observes, via Levi Strauss, ‘we are our tools’. We should consider
how the tools participate in shaping the world with us as we use them. The
era of Big Data has only just begun, but it is already important that we start ques-
tioning the assumptions, values, and biases of this new wave of research. As scho-
lars who are invested in the production of knowledge, such interrogations are an
essential component of what we do.
Acknowledgements
We wish to thank Heather Casteel for her help in preparing this article. We are
also deeply grateful to Eytan Adar, Tarleton Gillespie, Bernie Hogan, Mor
Naaman, Jussi Parikka, Christian Sandvig, and all the members of the Microsoft
Research Social Media Collective for inspiring conversations, suggestions, and
feedback. We are indebted to all who provided feedback at the Oxford Internet
Institute’s 10th Anniversary. Finally, we appreciate the anonymous reviewers’
helpful comments.
Notes
1
2
We have chosen to capitalize the term ‘Big Data’ throughout this
article to make it clear that it is the phenomenon we are discussing.
API stands for application programming interface; this refers to a set
of tools that developers can use to access structured data.
675676
INFORMATION, COMMUNICATION & SOCIETY
3
4
Details of what Twitter provides can be found at https://dev.Twitter.
com/docs/streaming-api/methods White-listed accounts were com-
monly used by researchers, but they are no longer available.
The percentage of protected accounts is unknown, although attempts
to identify protected accounts suggest that under 10 percent of
accounts are protected (Meeder et al. 2010).
References
Acquisti, A. & Gross, R. (2009) ‘Predicting social security numbers from public
data’, Proceedings of the National Academy of Science, vol. 106, no. 27,
pp. 10975–10980.
Anderson, C. (2008) ‘The end of theory, will the data deluge makes the scientific
method obsolete?’, Edge, [Online] Available at: http://www.edge.org/3rd_
culture/anderson08/anderson08_index.html (25 July 2011).
Anderson, M. (1988) The American Census: A Social History, Yale University Press,
New Haven, CT.
Baca, G. (2004) ‘Legends of Fordism: between myth, history, and foregone con-
clusions’, Social Analysis, vol. 48, no. 3, pp. 169–178.
Behar, R. & Gordon, D. A. (eds) (1996) Women Writing Culture, University of Cali-
fornia Press, Berkeley, CA.
Berry, D. (2011) ‘The computational turn: thinking about the digital humanities’,
Culture Machine, vol. 12, [Online] Available at: http://www.
culturemachine.net/index.php/cm/article/view/440/470 (11 July 2011).
Blass, T. (2004) The Man Who Shocked the World: The Life and Legacy of Stanley Milgram,
Basic Books, New York.
Bollier, D. (2010) ‘The promise and peril of big data’, [Online] Available at: http://
www.aspeninstitute.org/sites/default/files/content/docs/pubs/The_
Promise_and_Peril_of_Big_Data.pdf (11 July 2011).
Bowker, G. C. (2005) Memory Practices in the Sciences, MIT Press, Cambridge, MA.
Boyd, D. & Marwick, A. (2011) ‘Social privacy in networked publics: teens’ atti-
tudes, practices, and strategies’, paper given at Oxford Internet Institute,
[Online] Available at: http://papers.ssrn.com/sol3/papers.cfm?abstract_
id=1925128 (28 September 2011).
Burkholder, L. (ed.) (1992) Philosophy and the Computer, Westview Press, Boulder,
San Francisco, and Oxford.
Cain, M. & Finch, J. (1981) ‘Towards a rehabilitation of data’, in Practice and Progress:
British Sociology 1950 – 1980, eds P. Abrams, R. Deem, J. Finch & P. Rock,
George Allen and Unwin, London, pp. 105–119.
Clifford, J. & Marcus, G. E. (eds) (1986) Writing Culture: The Poetics and Politics of
Ethnography, University of California Press, Berkeley, CA.CRITICAL QUESTIONS FOR BIG DATA
Conover, M. (2011) ‘Jimmy Lin’, Complexity and Social Networks Blog, [Online] Avail-
able at: http://www.iq.harvard.edu/blog/netgov/2011/07/the_interna
tional_conference_o.html (9 December 2011).
Crawford, K. (2009) ‘Following you: disciplines of listening in social media’, Con-
tinuum: Journal of Media & Cultural Studies, vol. 23, no. 4, pp. 532–533.
Derrida, J. (1996) Archive Fever: A Freudian Impression, trans. Eric Prenowitz, Univer-
sity of Chicago Press, Chicago.
Dourish, P. & Bell, G. (2011) Divining a Digital Future: Mess and Mythology in Ubiqui-
tous Computing, MIT Press, Cambridge, MA.
Du Gay, P. & Pryke, M. (2002) Cultural Economy: Cultural Analysis and Commercial Life,
Sage, London.
Durkheim, E. (1895/1982) Rules of Sociological Method, The Free Press, New York,
NY.
Ess, C. (2002) ‘Ethical decision-making and Internet research: recommendations
from the aoir ethics working committee’, Association of Internet Researchers,
[Online] Available at: http://aoir.org/reports/ethics.pdf (12 September 2011).
Fischer, C. (1982) To Dwell Among Friends: Personal Networks in Town and City,
University of Chicago, Chicago.
Forsythe, D. (2001) Studying Those Who Study Us: An Anthropologist in the World of Arti-
ficial Intelligence, Stanford University Press, Stanford.
Freeman, L. (2006) The Development of Social Network Analysis, Empirical Press,
Vancouver.
Fry, J. P. & Sibley, E. H. (1996) [1974] ‘Evolution of database management systems’,
Computing Surveys, vol. 8, no. 1.1, pp. 7 –42. Reprinted in (1996) Great Papers
in Computer Science, ed. L. Laplante, IEEE Press, New York.
Gitelman, L. (2011) Notes for the Upcoming Collection ‘Raw Data’ is an Oxymoron,
[Online] Available at: https://files.nyu.edu/lg91/public/ (23 July 2011).
Golder, S. (2010) ‘Scaling social science with hadoop’, Cloudera Blog, [Online] Avail-
able at: http://www.cloudera.com/blog/2010/04/scaling-social-science-
with-hadoop/ (18 June 2011).
Golder, S. & Macy, M. W. (2011) ‘Diurnal and seasonal mood vary with work, sleep and
daylength across diverse cultures’, Science, vol. 333, no. 6051, pp. 1878–1881,
[Online] Available at: http://www.sciencemag.org/content/333/6051/1878.
Granovetter, M. S. (1973) ‘The strength of weak ties’, American Journal of Sociology,
vol. 78, no. 6, pp. 1360–1380.
Harding, S. (2010) ‘Feminism, science and the anti-Enlightenment critiques’, in
Women, Knowledge and Reality: Explorations in Feminist Philosophy, eds A. Garry
& M. Pearsall, Unwin Hyman, Boston, MA, pp. 298–320.
Homans, G. C. (1974) Social Behavior: Its Elementary Forms, Harvard University Press,
Cambridge, MA.
Kranzberg, M. (1986) ‘Technology and history: kranzberg’s laws’, Technology and
Culture, vol. 27, no. 3, pp. 544–560.
Latour, B. (2009) ‘Tarde’s idea of quantification’, in The Social after Gabriel Tarde:
Debates and Assessments, ed. M. Candea, Routledge, London, pp. 145–162,
677678
INFORMATION, COMMUNICATION & SOCIETY
[Online] Available at: http://www.bruno-latour.fr/articles/article/116-
TARDE-CANDEA.pdf (19 June 2011).
Lazer, D., Pentland, A., Adamic, L., Aral, S., Baraba ́si, A., Brewer, D., Christakis,
N., Contractor, N., Fowler, J., Gutmann, M., Jebara, T., King, G., Macy, M.,
Roy, D. & Van Alstyne, M. (2009) ‘Computational social science’, Science, vol.
323, no. 5915, pp. 721–723.
Leinweber, D. (2007) ‘Stupid data miner tricks: overfitting the S&P 500’, The
Journal of Investing, vol. 16, no. 1, pp. 15–22.
Lessig, L. (1999) Code: and Other Laws of Cyberspace, Basic Books, New York, NY.
Lewis, K., Kaufman, J., Gonzalez, M., Wimmer, A. & Christakis, N. (2008)
‘Tastes, ties, and time: a new social network dataset using Facebook.com’,
Social Networks, vol. 30, no. 4, pp. 330–342.
Lotan, G., Graeff, E., Ananny, M., Gaffney, D., Pearce, I. & boyd, D. (2011) ‘The
revolutions were tweeted: information flows during the 2011 Tunisian and Egyp-
tian revolutions’, International Journal of Communications, vol. 5, pp. 1375–1405,
[Online] Available at: http://ijoc.org/ojs/index.php/ijoc/article/view/1246.
Manovich, L. (2011) ‘Trending: the promises and the challenges of big social data’,
in Debates in the Digital Humanities, ed. M. K. Gold, The University of Minne-
sota Press, Minneapolis, MN, [Online] Available at: http://www.manovich.
net/DOCS/Manovich_trending_paper.pdf (15 July 2011).
McCloskey, D. N. (ed.) (1985) ‘From methodology to rhetoric’, The Rhetoric of
Economics, University of Wisconsin Press, Madison, pp. 20–35.
Meeder, B., Tam, J., Gage Kelley, P. & Faith Cranor, L. (2010) ‘RT @IWantPrivacy:
widespread violation of privacy settings in the Twitter social network’, paper
presented at Web 2.0 Security and Privacy, W2SP 2011, Oakland, CA.
Meiss, M. R., Menczer, F. & Vespignani, A. (2008) ‘Structural analysis of behavioral
networks from the Internet’, Journal of Physics A: Mathematical and Theoretical,
vol. 41, no. 22, pp. 220–224.
Moretti, F. (2007) Graphs, Maps, Trees: Abstract Models for a Literary History, Verso, London.
Onnela, J. P., Sarama ̈ki, J., Hyvo ̈nen, J., Szabo ́, G., Lazer, D., Kaski, K., Kerte ́sz, J.
& Baraba ́si, A. L. (2007) ‘Structure and tie strengths in mobile communi-
cation networks’, Proceedings from the National Academy of Sciences, vol. 104,
no. 18, pp. 7332–7336.
Pariser, E. (2011) The Filter Bubble: What the Internet is Hiding from You, Penguin Press,
New York.
Radcliffe-Brown, A. R. (1940) ‘On social structure’, The Journal of the Royal Anthro-
pological Institute of Great Britain and Ireland, vol. 70, no. 1, pp. 1–12.
Reverby, S. M. (2009) Examining Tuskegee: The Infamous Syphilis Study and Its Legacy,
University of North Carolina Press, Chapel Hill, NC.
Savage, M. & Burrows, R. (2007) ‘The coming crisis of empirical sociology’, Soci-
ology, vol. 41, no. 5, pp. 885–899.
Schrag, Z. M. (2010) Ethical Imperialism: Institutional Review Boards and the Social
Sciences, 1965 –2009, Johns Hopkins University Press, Baltimore, MD.CRITICAL QUESTIONS FOR BIG DATA
Shamma, D. A., Kennedy, L., and Churchill, E. F. (2010) ‘Tweetgeist: Can the
Twitter Timeline Reveal the Structure of Broadcast Events?,’ Paper presented
at the Computer-Supported Cooperative Work-2010, Association for Computing
Machinery, February 6 –10, Savannah, Georgia USA. Available at: http://
research.yahoo.com/pub/3041.
Suchman, L. (2011) ‘Consuming anthropology’, in Interdisciplinarity: Reconfigurations of
the Social and Natural Sciences, eds A. Barry & G. Born, Routledge, London,
[Online] Available at: http://www.lancs.ac.uk/fass/doc_library/sociology/
Suchman_consuming_anthroploogy.pdf.
Troshynski, E., Lee, C. & Dourish, P. (2008) ‘Accountabilities of presence: refram-
ing location-based systems,’ Proceeding of the twenty-sixth annual SIGCHI confer-
ence on Human factors in computing systems, April 5–10, Florence, Italy.
Twitter (2011) ‘One hundred million voices’, Twitter Blog, [Online] Available at:
http://blog.Twitter.com/2011/09/one-hundred-million-voices.html
(12
September 2011).
Veinot, T. (2007) ‘The eyes of the power company: workplace information practices
of a vault inspector’, The Library Quarterly, vol. 77, no. 2, pp. 157–180.
Wang, X. (2011) ‘Twitter posts show workers worldwide are stressed out on the
job’, Bloomberg Businessweek, [Online] Available at: http://www.businessw
eek.com/news/2011-09-29/Twitter-posts-show-workers-worldwide-are-
stressed-out-on-the-job.html (12 March 2012).
Wu, S., Hofman, J. M., Mason, W. A. & Watts, D. J. (2011) ‘Who says what to
whom on Twitter’, Proceedings of the International World Wide Web Conference
(WWW 2011), March 28-April 1, Hyderabad, India, pp. 705 – 714.
Zimmer, M. (2008) ‘More on the “Anonymity” of the Facebook dataset – it’s
Harvard College’, MichaelZimmer.org Blog, [Online] Available at: http://
www.michaelzimmer.org/2008/01/03/more-on-the-anonymity-of-the-face
book-dataset-its-harvard-college/ (20 June 2011).
danah boyd is Senior Researcher at Microsoft Research, Research Assistant
Professor at New York University, and Fellow at Harvard’s Berkman Center for
Internet & Society. Her work focuses on how people integrate social media into
their everyday practices, with a particular eye towards youth’s socio-technical
practices. Her next book is called It’s Complicated: The Social Lives of
Networked Teens (Yale University Press). Address: Microsoft Research, One Mem-
orial Drive, Cambridge, 02142 MA, USA. [email: danah-ics@danah.org]
Kate Crawford is Associate Professor at the University of New South Wales,
Sydney, and Principal Researcher at Microsoft Research New England. She
has conducted large scale studies of mobile and social media use, and has
been published widely on the cultural and political contexts of social media.
Her next book is the coauthored Internet Adaptations: Language, Technology,
Media, Power (Palgrave Macmillan). Address: Microsoft Research, One Memorial
Drive, Cambridge, 02142 MA, USA. [email: kate@katecrawford.net]
679
