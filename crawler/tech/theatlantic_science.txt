
On Thursday morning, Adam Gill stepped outside in a heavy, bright-yellow coat, bulky gloves, and a ski mask to brace himself against the blistering wind. He brought with him a metal teakettle full of boiling water. As he tipped the kettle over, the piping-hot liquid turned instantly into snow and blew away in the wind.
That’s how cold it was at the Mount Washington Observatory in New Hampshire, the highest peak in the northeastern United States. The video of Gill, a meteorologist at the observatory, conducting this little presentation received thousands of sympathetic likes on Facebook. The temperature that day at the observatory hit a bone-chilling low of -34 degrees Fahrenheit (-37 degrees Celsius)—and that was without accounting for wind chill. The day broke the previous record of -31 degrees Fahrenheit (-35 degrees Celsius), set in 1933.
The frigid weather in New Hampshire is part of an Arctic cold front that has settled over large swaths of the United States this week. Millions of people are bundling up as temperatures hit bone-chilling lows in the Northeast and the Midwest. Daily high temperatures on the East Coast have dipped into the teens and 20s in Fahrenheit, and highs in parts of the Midwest are well below zero, with some in the negative 20s and 30s. The National Weather Service warns of hypothermia and frostbite. In Toledo, Ohio, a dog was discovered frozen solid on a porch.
These conditions mean that in some parts of the United States, it’s actually colder than it is on Mars.
The latest weather data from the Curiosity rover on Mars recorded a peak temperature of -9 degrees Fahrenheit (-23 degrees Celsius) on the day of Sol 1910, which for us is December 20. The rover roams around Gale Crater, near the equator. There, the winter solstice has just passed and the cold season is getting started. Humans have yet to figure out if the red planet could be habitable, but right now it seems just as (in)hospitable as home, at least weather-wise. (Of course, Earth still has one thing that Mars doesn’t: breathable air.)
The comparison to Mars is indeed accurate, says Michael Mischna, a research scientist at NASA’s Jet Propulsion Laboratory in California where he studies the Martian climate—and where it will reach an enviable 80 degrees Fahrenheit (27 degrees Celsius) Friday. “Temperature is pretty much temperature wherever you are,” Mischna said.
There is one small difference. The atmosphere on Mars is about 100 times thinner than on Earth, which means a given air temperature—if you could stand outside on the planet, without a spacesuit—wouldn’t feel the same on both planets.
“If you were to jump into a pool that was 70 degrees, it would feel a lot colder to your body than standing in air at 70 degrees, and that’s because the water is able to suck the heat out of your body,” Mischna said. Like water molecules, air molecules can wick away warmth, but they’re much less efficient. On Mars, where there’s little water vapor and few air molecules, a person wouldn’t feel as cold. “Minus 100 degrees on Mars might only feel like minus 30,” he said.
There’s very little water on Mars, which means there’s no humidity; the heat is dry and the cold is bitter. Temperatures during summers on Mars, particularly at warmer regions like at the equator, can rise above freezing. At the height of the season, they can even reach 68 degrees Fahrenheit (20 degrees Celsius). Peak winter on Mars, however, is no match for the same season on Earth. Temperatures are -94 degrees Fahrenheit (-70 degrees Celsius) at the equator, and -200 degrees Fahrenheit (-130 degrees Celsius) at the poles.
Back on Earth, the American cold snap has prompted some doubters, including President Donald Trump, to surface their usual argument against climate change: that record-breaking, frigid temperatures are proof that human-driven climate change isn’t real.
The President Doesn't Care to Understand Global Warming
The president is too eager to leap to conclusions: A single week of cold weather in one part of the world does not disprove that the world, overall, is warming. The Arctic’s current temperature is more than five degrees Fahrenheit above average, and 2017, as a whole, will be one of the warmest years ever recorded worldwide.
But this week’s Arctic blast may actually be a symptom of climate change, albeit an unusual one. In recent years, several studies have found that as the Arctic warms, it seems to destabilize other parts of the winter climate system, bringing frigid air down to the Midwest and Northeast. Scientists aren’t sure why this is happening yet—it may have to do with changes to the jet stream or the high-altitude vortex of air over the North Pole—but for now, it’s a well-documented aspect of our warming world. (Alas, these frigid winters don’t prevent the United States from suffering through scorching, climate-addled summers in the same year.)
Forecasters warn that this week’s temperatures could stretch through the New Year’s holiday. Here’s a disheartening map from the National Weather Service:
Here's a look at the forecast wind chills for right around midnight on New Year's Eve. Brutally cold in some locations. #NewYearsEve pic.twitter.com/pXGBVb3zut
Maybe this holiday season will help prepare humanity for future New Years’ celebrations on Mars.


In the first novel ever written about Sherlock Homes, we learn something peculiar about the London detective. Holmes, supposedly a modern man and a keen expert in the workings of the world, does not know how the solar system works. Specifically he is unfamiliar with the heliocentric Copernican model, which, upon its slow acceptance in the 17th century, revolutionized Western thought about the place of our species in the universe.  
“What the deuce is it to me?” Holmes asks his sputtering soon-to-be sidekick, Dr. Watson. “You say that we go ’round the sun. If we went round the moon it would not make a pennyworth of difference to me or to my work.”
Brains are a kind of “little empty attic,” says the detective, and they should be filled only with furniture that’s useful to one’s line of work. Holmes doesn’t doubt the Copernican model; he simply has no use for it in solving murder cases. “Now that I do know it,” he adds, “I shall do my best to forget it.”
Thursday night, as record lows gripped most of the country’s northern half, President Trump clarified that he does not understand another revolution in our knowledge of the natural order of things: the theory of human-driven climate change.
In the East, it could be the COLDEST New Year’s Eve on record. Perhaps we could use a little bit of that good old Global Warming that our Country, but not other countries, was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up!
Trump is wrong about the science—I’ll get to that in a moment—but, first, let’s not mince words: The president is trolling here. Pointing to cold weather and asking Whither climate change? is, by this point, almost a Republican tradition. In February 2015, Senator James Inhofe of Oklahoma brought a snowball inside the Senate chambers to demonstrate that global warming was not real. “It’s very, very cold out. Very unseasonable,” said Inhofe, then the chairman of the Environment and Public Works Committee. “In case we have forgotten because we keep hearing that 2014 has been the warmest year on record.”
Despite the snowball, 2014 was the warmest year on record. It was followed by 2015, which broke 2014’s record and became the new hottest year on record; and 2016, which  became the new hottest year after it blasted away 2015’s record. Though this year is not yet over, NASA estimates that 2017 will also beat 2015’s record and become the second-hottest year ever recorded. And even if this late cold snap averts that dreadful streak, 2017 will still break 2014’s record. We have just lived through what would be, in any other decade, the warmest year ever measured.
Perhaps Trump has forgotten that his native New York suffered three heat waves by late July this year, or that a “normal” summer now would count as a “hot” summer during the 20th century. Scorching temperatures have vanished not because global warming has stopped, but because it is the winter, which every preschooler knows as the “cold season.” Six months after hurricane season ends, Floridians do not ask where the tropical cyclones have gone; cable-news pundits do not spend time every night debating whether the sun will rise the next morning, though that may be preferable to what is currently aired.
And there is, in fact, a connection between global warming and the current frigid weather across the United States. Spend some time clicking around the Climate Reanalyzer—a fantastic tool from the University of Maine—and you’ll see that the northern half of North America is the only part of the world where temperatures are significantly colder right now than normal. Moscow, Russia, is normally about 24 degrees Fahrenheit this time of year; it’s currently pushing 40. The Arctic as a whole is more than 5 degrees Fahrenheit above normal.
Nor is this phenomenon limited to exotic capitals. Los Angeles has an average high of 68 degrees Fahrenheit on December 29. Today it hit 82.
It’s not too much of an exaggeration to say it’s warmer than normal almost everywhere right now except for Canada and the northern United States. What no one should tell the president is that this pattern may not be due to random chance—it seems to arise from global climate change.
For the past decade or so, climate scientists have noticed that when the Arctic has an especially warm winter, the northern continents become especially cold and snowy. A plethora of studies in the past three years have seemed to confirm the connection: When the Arctic is extremely warm, it seems to loose cold air across the world, and northern North America suffers an extremely cold winter. Why? Scientists aren’t sure yet, but they think it may arise from a destabilized jet stream or a weakened stratospheric polar vortex.
Just this past July, a team of researchers found that frigid winters, driven by a warm Arctic, were already reducing the productivity of American agriculture. They estimated that this warming has already cost Texas a 20 percent decline in corn production for some years.
Suffice it to say: There is plenty of evidence that climate change is still at work, even in a week as chilling as this one. But does the president really need to understand all this stuff? Perhaps if he had remained in real-estate development, he could have stayed cheerfully ignorant. His line of work would never have touched the theory of human-driven climate change—outside of the occasional application for government money to protect a golf course from sea-level rise.  
But unfortunately for Americans—and quite unlike the good detective of 221B Baker Street—Trump’s job has brought him frequently and directly into contact with climate science. During his year in office, he has made decision after decision that turns on a nuanced understanding of global warming. He has withdrawn the United States from the Paris Agreement on climate change. He has revoked a slew of Obama-era policies meant to stem greenhouse-gas emissions. He has repealed the Clean Power Plan, a landmark rule that would have made the United States grid less reliant on coal. And he has declined to seek any kind of climate-protecting measure in the new GOP tax bill, despite the many alumni of the Reagan administration pleading for it.
People of good faith can disagree about the legality, effectiveness, and wisdom of any of these individual policies—and they do. But Trump has attacked the policies collectively and with great gusto, while declining to ever actually learn about the underlying scientific evidence.
Meanwhile, he sits atop the science agencies of the U.S. government, some of the finest scientific institutions ever constructed. Last month, his own administration released the Climate Science Special Report, a product of 13 federal agencies and itself the best scientific synopsis of climate change in years.
It seemed to address exactly his concern. “The number of high temperature records set in the past two decades far exceeds the number of low temperature records,” its authors said, with the highest confidence possible. “The frequency of cold waves has decreased since the early 1900s, and the frequency of heat waves has increased since the mid-1960s.” (The Dust Bowl period still holds the record for the most extreme temperatures.)
Trump has never expressed curiosity for this kind of fact finding or self-education. But neither he nor his administration has publicly fought climate change on its scientific basis. After some hand wringing this summer, the government released the unabridged Climate Science Special Report last month. And while much could still change, Scott Pruitt, the administrator of Trump’s EPA, seems unlikely to revoke the agency’s own endangerment finding, a 2009 memo that accepts the scientific foundations of climate change into federal policy. The Trump administration has hastily scaled back climate policy and barely touched climate science.
Indeed, this has characterized Trump’s approach: a rapid dismantling of law, and a lazy disregard for evidence. He has called climate change—in part a triumph of the American scientific enterprise—a “hoax” “created by and for the Chinese.” Trump seems confident in his belief that Earth scientists and the climate-concerned have invented a phenomenon out of whole cloth and that he needs to pay little attention to it. He seems sure, too, that the shambolic catastrophe of a destabilized climate—which will be a central preoccupation of the United States in the 21st century, whether its leaders recognize the reality of it or not—is a fable. In so doing, he underestimates the citizens whom he governs; and he conceives of the country over which he presides as being shallower, less curious, quicker to anger, more unwise, and altogether not as secure in its good understanding than it actually is. Pity him, and mourn for us.


Two years ago, China launched a space probe into orbit around Earth. Scientists nicknamed it Wukong, or Monkey King, after the hero of a 16th-century novel about a Buddhist monk’s long journey to India to secure religious texts. The probe’s job was to track and record cosmic rays, the streams of high-energy particles that constantly bombard Earth’s atmosphere from all corners of the universe.
In its first 530 days of operation, the probe recorded more than 2.8 billion cosmic rays. When scientists looked at data, they found something unusual. Some of the cosmic rays—at least 1.5 million of them—were recorded at a different and higher energy level than the others. Plotted on a chart, they appeared as a cluster of tiny outliers suspended above the curve.
Though they don’t look like much, this blip is incredibly important to astrophysicists around the world who are trying to solve one of the biggest mysteries in science: the existence of dark matter. Scientists believe that dark matter makes up about a quarter of everything in the universe, but the tricky thing is that we can’t see dark matter. In fact, we don’t even know what it’s made of. The​ existence of dark matter ​is inferred in​directly from observational data. It shapes some important phenomena in the cosmos—like why galaxies form in the first place, and stick together instead of flying apart—that the physics we already understand cannot.
The top theory for dark matter suggests that the mysterious stuff is made of WIMPs, weakly interacting massive particles. Wukong, known formally as the Dark Matter Particle Explorer (DAMPE), is designed to detect the signal that comes from WIMPs. WIMPs are lazy, slow-moving particles that collide rarely, but when they do, they could form pairs of electrons and positrons—the antimatter counterparts of electrons—some of the particles that make up cosmic rays. An uptick in the number of these pairs would be detected as a distinct bump in a survey of cosmic rays—like the one Wukong identified.
The DAMPE findings, which were released last month in Nature, line up nicely with other space-based experiments on dark matter in the last decade, an exciting prospect in a field that depends on measuring the same thing more than once. But “that is not to say that this is an indication of dark matter,” said Douglas Finkbeiner, an astrophysicist at Harvard who studies dark matter. There are many possible interpretations. The signal detected by DAMPE and others come from all directions in the sky, so scientists can’t pinpoint a source. The unusual cosmic rays could come from supernovae, the powerful deaths of stars, or pulsars, bright, fast-rotating stars.
For now, the DAMPE result, while tantalizing, is inconclusive. For scientists, it provides another data point in the search for dark matter. But for China in particular, the findings mean something more. DAMPE is the country’s first mission dedicated exclusively to astronomy and astrophysics, and within two years it has returned a promising result. The successful showing makes China a fierce participant—or, depending on whom you ask, competitor—in the field.
“The way I see it, we are making strides toward solving this big cosmic puzzle,” said Priyamvada Natarajan, a theoretical astrophysicist at Yale. “I don’t feel restricted by international borders in that sense. But it doesn’t go unnoticed that it is a Chinese satellite that did it.”
When people talk about China’s ambitions in space, the discussion usually focuses on activities that have some military implications—like the launch of spy satellites—rather than scientific ones. Headlines shout about a “space race” between China and the United States, and some American defense officials rattle their sabers alongside calls for increased capabilities in low-Earth orbit. The thought of Chinese spacecraft near U.S. assets in orbit is, from a national-security perspective, far more unnerving than a science mission for a particle that may not even exist.
“Unless you’re in a really intense Star Trek fantasy, it’s not going to lead you into a weapons capability,” said Joan Johnson-Freese, a national-security affairs professor at the U.S. Naval War College who has studied space security for 20 years.
China has invested heavily in space science in recent years. The country started building the world’s most powerful radio telescope in 2011, edging the famed Arecibo Observatory in Puerto Rico out of the top spot. In the fall of 2016, the telescope, the 500-Meter Aperture Spherical Radio Telescope (FAST) started making observations. It discovered two new pulsars in its first year of operations, and stands to be the leading instrument in the search for intelligent extraterrestrial life for years to come. In 2013, China landed a rover on the moon to poke around the surface. In June of this year, the Hard X-Ray Modulation Telescope (HXMT), a space observatory to study black holes and neutron stars, joined DAMPE in orbit.
The choice to invest in these particular fields have been very deliberate, according to Johnson-Freese. “China likes to be in the record books like everyone else,” she said, but the country can’t compete in areas of space exploration where the United States and other countries have long dominated. Instead, the Chinese have gone after realms in which no country has yet made a definitive triumph—like the search for dark matter.
In March 2016, a few months after DAMPE launched, Chang Jin, the mission’s chief scientist, said the search for the mysterious substance “tops the basic frontier projects of science listed by the United States, Europe, China, and Japan.”
“Any progress in dark-matter research will probably bring a breakthrough in physics,” Chang said.
While a breakthrough by the Chinese—a breakthrough by any group of scientists in any nation, really—would be cause of celebration in the astrophysical community, the merriment would feel thorny for some.“If China were to get a Nobel Prize in science, would that mean that the United States suddenly lost all of its lead? No,” Johnson-Freese said. “But I can see that there would be a lot of scientists who would say, well, this is going to become a Chinese matter of expertise. We’re going to depend on their science for us to do work.”
The isolation from a potential breakthrough likely will be felt most by American scientists, thanks to a law passed in 2011 that prohibits NASA from working with China’s space agency. There’s some irony there, given that one of the earlier experiments that noticed the same, strange blip in a survey of cosmic rays—the signal that scientists hope betrays the existence of dark matter—came from a collaboration between China, the United States, and other countries, the Alpha Magnetic Spectrometer, mounted on the International Space Station just one month after Congress approved the ban.
China has found opportunities for collaboration elsewhere. Scientists from institutions in Geneva and Italy are working on the DAMPE mission, and Chinese officials are in talks with the European Space Agency about building an outpost on the moon together. “These congressional restrictions presume that forbidding contact will slow the pace of Chinese progress,” Gregory Kulacki, a senior analyst and China project manager at the Union of Concerned Scientists, an American nonprofit group, said in an email. “Projects like FAST and DAMPE prove beyond a shadow of a doubt that presumption is mistaken.
Eventually, even China’s ambitions in particle physics will be subject to the scrutiny usually reserved for its more secretive, space-based military operations.“It seems no matter what China does in space, even if it is to make a significant scientific contribution to our understanding of the universe, some people in the United States will perceive it as a threat,” Kulacki said. But science is not a zero-sum game, he said, and the scientists themselves “understand who is to blame.”
After decades of looking for elusive dark-matter particles, the effort feels like it is at an impasse, Natarajan said. The hunt for WIMPs, the leading candidates, has repeatedly turned up empty, and astrophysicists are trying to broaden their search methods. Scientists are hopeful about more results from DAMPE, which is expected to last five years.
“We’ve all been thinking about this for so long, it’s such an embarrassment,” Natarajan said. “That anyone is making progress is super exciting.”


Among blind people, says Kim Charlson, asking if you’d prefer to see always starts a lively debate: “Every opinion is going to be different.” Charlson, who lost her sight at age 11 and now is president of the American Council of the Blind, says she would hold out for full color vision. Others might settle for seeing in blurry black and white. And yet other blind people might have no desire to see at all.
For a small number of blind people, this hypothetical question recently become a real one.
Last week, the Food and Drug Administration approved Luxturna, the first gene therapy to treat a specific form of inherited blindness called Leber’s congenital amaurosis. In fact, it’s the first gene therapy to treat any inherited disease at all. The news has been universally hailed as a scientific breakthrough. But its stratospheric cost—potentially $1 million per patient—has provoked hard questions about the value of the ability to see, especially if its effects are only partial and temporary, as may be the case with Luxturna.
Spark Therapeutics, the company that makes Luxturna, has yet to set an official cost. But the $1 million figure comes from a November earnings call, where the company’s CEO, Jeff Marrazzo, suggested the high cost is justified in part by the earnings patients and their caretakers could gain. He cited the National Federation of the Blind to note that 70 percent of working-age Americans who are blind are unemployed. “This economic reality of unemployment reminds us of a wonderful real-life example of the value of Luxturna,” said Marrazzo. One woman in the Luxturna trial regained enough vision to get her very first job at age 38.
The National Federation of the Blind took issue with being cited this way. Chris Danielsen, a spokesperson for the organization, says Spark was “relying on erroneous and harmful notions about the capacity of blind people to live the lives we want.” Research into specific eye diseases is well and good, he says, but the high unemployment rate among blind people is due to “society’s low expectations for the blind” and lack of training for skills like cane travel and reading Braille that would help them lead independent lives.
“It’s not like we’ve been sitting in rocking chairs for decades, and we’re waiting until we can go the hospital for the treatment,” says Stacy Cervenka, who was born blind due to an undeveloped optic nerve. “Blind people have lives that are as busy and chaotic and full as any sighted people,” she says. Cervenka is an executive officer for the California State Rehabilitation Council, and her husband, who was blinded by a gunshot wound, teaches blind people how to get around.
She credits such training for their independent lives. When we spoke, they were preparing to take their 4-year-old son on Amtrak for a holiday vacation. “From a purely fiscal standpoint of getting the most bang for your buck, the government could do so much more good for so much less money by providing vocational rehabilitation,” says Cervenka.
Cervenka is not opposed to gene therapy to treat blindness, but she wanted to emphasize the trade-offs. Aside from the potential seven-figure price tag, the therapy comes with risks from injecting into the eyeball, plus it takes time to travel to a hospital that offers it. In the meantime, she asks, “Who’s taking care of your kids? Who’s doing your job?” For her, it’s not worth the hassle for only partial vision, but she would consider it if her vision could be good enough to drive—the one place she feels like being blind actually impinges on her life.
How the Blind Are Reinventing the iPhone
For some people who are blind, particularly if they have been blind since birth, gaining sight can actually be a bizarre, disruptive experience.“It isn’t like you can turn on a switch and someone who hasn’t seen would be able to see, because their brain doesn’t know what vision is,” says Charlson. People who have regained sight, such as through a corneal transplant, report being unable to recognize objects until they pick them up in their hands—as they had been doing their whole life.
Charlson says Luxturna is very promising, and she hopes gene therapy can be developed to treat other forms of inherited blindness. For now, Luxturna can only help people who are born with one specific mutation (RPE65) for one specific disorder (Leber’s congential amaurosis), one of dozens of conditions that can cause blindness. It is for this reason that some blind advocates take such issue with Spark’s justification of a $1 million value. They don’t want it to be marketed using language that comes at the expense of blind people now, most of whom the treatment cannot help.
This has been a perennial tension. In 2016, the Foundation Fighting Blindness, which funds research into retinal degenerative diseases and backed some of the research behind Luxturna, released a series of videos for its #HowEyeSeeIt campaign. One video featured a scene of kids at the playground going dark, asking viewers to contemplate being unable to take kids to the park. An outcry ensued. In response, Cervenka filmed and uploaded a video of her and her husband taking their son to the park. “It’s the worst video—a blind person with an iPad,” she joked. But she made her point. She could take her kid to the park just fine.


In downtown Ulaanbaatar, on a pedestal in the Central Museum of Mongolian Dinosaurs, stands a 70-million-year-old Tarbosaurus bataar dinosaur from the southern Gobi Desert. In 2012, the Tarbosaurus was very nearly sold at auction in New York, despite such a sale violating Mongolian law as well as a temporary restraining order by a U.S. federal judge in Dallas. Five years and 6,000 miles later, that very same dinosaur fossil found itself back in Mongolia, now an icon symbolizing Mongolian and American efforts to combat the illicit fossil trade in Central Asia. As I walked through the dimmed entry hall backlit with the museum’s name in lights, it occurred to me that the long-dead and almost-trafficked dinosaur has a lot of life left to live.
Ever since the 1997 sale of Sue for a then-unprecedented $7.6 million, fossils have proven to be an extremely lucrative luxury market. For buyers interested in owning prehistoric natural objects, dinosaur fossils like skulls and complete skeletons can add an impressive bit of the Cretaceous to their portfolios. In the 21st-century high-end collectors’ market, fossils from Mongolia and China, in particular, are challenging the international community’s ethical response to fossil trafficking. Ever since the return of that first Tarbosaurus, thanks to the Herculean efforts of the Mongolian paleontologist Bolortsetseg “Bolor” Minjin, dozens and dozens of other dinosaur fossils have been seized by ICE and sent back to Mongolia.
“Sending the fossils back” is really just a new beginning for these repatriated fossils.
* * *
Through the efforts of Bolortsetseg and other fossil activists, the Institute for the Study of Mongolian Dinosaurs, a nonprofit organization in Ulaanbaatar, works with U.S. and Mongolian agencies to help return Mongolian fossils before they eventually go on display at the Central Museum of Mongolian Dinosaurs. Bolortsetseg and her colleagues consider education and outreach, as well as museum curation, to be an integral part of Mongolia’s successful fossil-repatriation program, whether through new dinosaur museums in Ulaanbaatar or driving a mobile museum with casts of fossils to rural parts of Mongolia.
At the museum, the fossils are important objects that teach visitors about Mongolia’s deep natural history and paleo past. According to UNESCO, the Gobi is the world’s largest “fossil reservoir.” Scientists have discovered something like 80 different dinosaur genera in the desert from over 60 known fossil sites. Although other parts of the world, like the Rocky Mountain region in the western United States and Canada, have spectacular fossils, those found in the Gobi are particularly prized because the fossils are so well preserved, allowing researchers to see tiny details on the fossils, like marks of blood vessels and nerves. The spectacular preservation also means that the Gobi can boast an inordinate number of fossil species, both large and small, plant and animal, all of which offer scientists the opportunity to study ancient ecosystems.
For decades, Russian, Polish, Chinese, and American paleontologists have come to the southern stretches of the Gobi’s arid Nemegt Basin to excavate dinosaur—and other—fossils. (The interest in the area traces back to the 1920s, when the American Museum of Natural History explorer Roy Chapman Andrews showed the world Mongolia’s fossil-rich deposits, including the first example of dinosaur eggs.) The first Tarbosaurus fossils were discovered in the 1940s and the species was officially named by Soviet paleontologist Evgeny Maleev in 1955; the Tarbosaurus bataar is an evolutionary cousin to the well-known Tyrannosaurus rex. By the time Tarbosaurus started hitting American auctions in the 21st century, however, it was long understood in the scientific community as a uniquely Mongolian dinosaur (despite some specimens being found in China) thanks to its uniquely Mongolian geological context.
The Tarbosaurus that sparked Mongolia’s repatriation revolution stands eight feet tall, measures 24 feet from tail to snout, and arrived back in Ulaanbaatar in 2013. Both the Tyrannosaurs and Tarbosaurs are apex predators, sporting fierce teeth and meme-ically comic forearms—both are some of the most charismatically identifiable species of the late Cretaceous, striking a perfect balance of awe and inspiration to museums and collectors the world over.
As with most countries, it is illegal to remove fossils and archaeological artifacts from Mongolia and sell them in private markets. Having legislation on the books that prohibits the sale of fossils is one thing. Being able to enforce that legislation is another. Money talks and there are immense amounts of it to be made with buyers willing to pay top dollar for something as stunning as a dinosaur skeleton. (“[The Tarbosaurus] can fit in all rooms 10 feet high,” the auctioneer added when the original Tarbosaurus fossil was going under the gavel in 2012. “So it’s also a great decorative piece.”) The repatriation of the Tarbosaurus bataar in 2013 gave Mongolia and the paleontological community some Tarbosaurus-like teeth to pursue the possibility of recovering other smuggled fossils.
For artifacts and fossils, repatriation has long been a way of ceding social authority and acknowledging a country’s right in assigning an object’s value, be it scientific or cultural. For the fossils to connect with their audiences, they need to be defined as something more than just “returned”—they need to be shown, studied, talked about, and valued.
When the Tarbosaurus bataar arrived in Mongolia in 2013, Bolortsetseg spent two weeks curating an exhibit that showcased the fossil and the efforts to have it returned to Mongolia. The exhibit was a sensation as visitors queued for hours to see the fossil. The exhibit piqued public curiosity and reintroduced dinosaurs to Mongolia’s national identity. The Central Museum of Mongolian Dinosaurs soon dedicated itself to bringing public awareness to repatriated Mongolian dinosaurs. The museum operates out of Ulaanbaatar’s old Lenin Museum, now repurposed and renovated with shiny new display cases, snazzy illustrations, detailed maps of paleontological sites, placards describing the science behind the species, and a fossil-themed gift shop. In 2013 alone, 22 more trafficked dinosaurs were repatriated—each dinosaur sent back home reinforced precedent for other fossils to follow. As visitors to the Central Museum of Mongolian Dinosaurs walk up the building’s old marble steps to the second floor, the plethora of trafficked fossils—and the scope of the smuggling problem—quickly becomes apparent. The fossils on display are simply the tip of the iceberg, in terms of the number of fossils smuggled out of Mongolia.
Since 2013, other dinosaur-museum spaces have been built to highlight Mongolia’s fossil heritage. At the Hunnu Mall on the outskirts of Ulaanbaatar, for example, visitors are welcomed to the small museum with the truly spectacular
“Amazing Dinosaurs From Mongolia” exhibition, sponsored by the Mongolian Academy of Sciences Institute of Paleontology and Geology. From my bench perch on the Hunnu Mall’s second floor, I watched visitors circle the full-size dinosaur displays and then follow the decal dinosaur footprints to the museum’s exhibition. Some of the Hunnu Mall’s dinosaurs are real fossils with museum catalog numbers clearly visible, like the giant plant-eating sauropod Opisthocoelicaudia skarzynski; some are replicas, like the Tarbosaurus bataar that stands in front of the first-floor escalator backlit by a Coca-Cola advertisement. Most shoppers pause, look at the dinosaurs, take some selfies, and go on about their business.
There is little doubt that these repatriated fossils have an important role to play in educating a new generation of local dinosaur experts. Mongolia does not currently have a graduate training program in paleontology—for either the science or curation of fossils. During summer months, Bolortsetseg and the ISMD volunteers drive a 40-foot Winnebago through remote parts of Mongolia to introduce kids to Mongolia’s deep geologic history. (The dino-mobile was previously owned and subsequently donated by the American Museum of Natural History, which also used it for fossil-outreach projects.)
In 2016 alone, more than 30 fossils were recovered from U.S. auctions, ranging from a nest of dinosaur eggs, to a Protoceratops skull (similar in size and anatomy to a Triceratops), to an entire turkey-sized Psittacosaurus skeleton. The fossils recovered from 2016 occupy the second floor of the Central Museum of Mongolian Dinosaurs’ exhibit hall, with plaques from ICE that commemorate the joint efforts to halt the illegal smuggling. Based on U.S.-Mongolian efforts, seven additional fossils from Europe joined the burgeoning collection of returned dinosaurs currently housed at the Central Museum of Mongolian Dinosaurs.
At the Central Museum, I watched visitors point out the ICE certificates to each other. As tourists tried to figure out the best angle to photograph the Tarbosaurus on its pedestal, museum guides told and retold the story of its triumphant return.


The year 2017 has turned out to be a good one for rocket science in the United States.
American companies made 29 successful rocket launches into orbit, the highest figure since 1999, which saw 31 launches, according to a comprehensive database maintained by Gunter Krebs, a spaceflight historian in Germany. The final launch of the year, by a SpaceX Falcon 9 rocket carrying a cache of commercial communications satellites, took place Friday night at Vandenberg Air Force Base in California.
SpaceX, Elon Musk’s private spaceflight company, is responsible for most of this year’s launches. After a brief hiatus following an explosion in September 2016 that destroyed a Falcon 9 and its $200 million commercial payload, SpaceX returned to the launchpad in mid-January. At the time, the success of the launch was imperative; SpaceX had lost another rocket in June 2015, about two minutes after takeoff, and its rocket-fueling process was receiving intense scrutiny by a NASA safety advisory group. NASA was entering its fifth year of using SpaceX rockets for resupply missions to the International Space Station, and future deals were on the line.
The launch went smoothly. Another successful attempt followed in February, and then two more in March. Observers held their breath as the months progressed, hopeful for a streak but unwilling to call attention to it, lest they jinx the whole thing. But the streak continued, and SpaceX launched a variety of payloads—commercial satellites, International Space Station cargo, even supersecret missions for the U.S. government—at a regular cadence throughout the year.
Despite an explosion of one of its rocket engines during a test at its facility in Texas in November, SpaceX ended the year without any in-flight failures or launchpad explosions. The company’s total comes to 18 launches in 2017—its most in a single year, and more than double that of the previous year.
The streak has made rocket launches by a commercial company from U.S. soil seem routine and almost ordinary—though someone probably should have warned the people of Los Angeles, many of whom were confused or terrified at the sight of the exhaust plume from Friday’s launch, which took the shape of a ghostly spaceship in the night sky.
It has been just two years since SpaceX launched a Falcon 9 into orbit and then returned the rocket’s first-stage booster back to Earth, landing it upright on the ground. And although there were a few spectacular fiery fails before and after this attempt, the recovery of a Falcon 9’s first stage, whether on land or on a drone ship at sea, seems, by now, a regular occurrence.
But landing a rocket stage after launch is only the first part of a truly reusable rocket regime. In March of this year, SpaceX made good on the second part when it used a refurbished first stage, previously flown in 2016, to launch a communications satellite. The first stage returned to Earth, touching down on a drone ship in the Atlantic Ocean. It was the first time SpaceX had launched a “flight-proven” rocket, the promise of which led Musk to create the company in the first place. In November, NASA agreed to let SpaceX use refurbished rockets in its resupply missions to the International Space Station. Earlier this month, a used rocket did just that, launching 5,000 pounds of crew supplies and science experiments.
The last year has placed commercial companies like SpaceX and Blue Origin, Amazon founder Jeff Bezos’s firm, squarely in the age of reusable rockets. While NASA builds an expendable heavy launch system for astronauts of the future, the rest of the market has decisively moved away from disposal rockets, and private firms are jockeying to be the top brand in the field. Blue Origin beat SpaceX to reusability in November 2015, when the company launched its New Shepard rocket into Earth’s atmosphere, breached the edge of space, and parachuted back down, landing vertically. Musk and others quickly, and correctly, pointed out that while Bezos’s rocket had made it to the Kármán line, the boundary that separates the atmosphere and outer space, it had not reached orbit. Blue Origin has tested the New Shepard several times this year, but hasn’t yet achieved orbital flight.
SpaceX said it plans to significantly increase the number of launches in 2018. “We will increase our cadence next year about 50 percent,” Gwynne Shotwell, the company’s president and COO, told SpaceNews in November. “We’ll fly more next year than this year, knock on wood, and I think we will probably level out at about that rate, 30 to 40 per year.” The company also said it would start construction next year on the BFR, a behemoth of a rocket that Musk said in September would someday replace its current fleet of spacecraft in sending payloads—and eventually people—into low-Earth orbit and beyond.
SpaceX will begin the new year as it began 2017, with another high-stakes launch. After some delay and much hype, the company plans to launch its Falcon Heavy rocket, a reusable, behemoth version of the Falcon 9 capable of lifting more than twice the payload of the Delta IV Heavy, the most powerful heavy-lift launch vehicle in operation. The Heavy is a throwback to the power of the Saturn V, the rocket that sent astronauts to the moon and Skylab, the first U.S. space station, into orbit in the 1960s and 1970s.
Musk has said he’s not sure if the flight will be a success. “I hope it makes it far enough away from the pad that it does not cause pad damage,” he said at a conference in July. “I would consider even that a win, to be honest.”
For its maiden flight, the Falcon Heavy will carry one of Musk’s cherry-red Tesla roadsters, in quite the show of cross-platform promotion. The rocket will blast off from a launchpad SpaceX leases from NASA, at Cape Canaveral in Florida—the historic site of launches for the Apollo program, the Space Shuttle program that built humans a home in orbit, and, in this case, an attempt to make big rocket launches a routine thing.


It’s not easy being a “theory of everything.” A TOE has the very tough job of fitting gravity into the quantum laws of nature in such a way that, on large scales, gravity looks like curves in the fabric of space-time, as Albert Einstein described in his general theory of relativity. Somehow, space-time curvature emerges as the collective effect of quantized units of gravitational energy—particles known as gravitons. But naïve attempts to calculate how gravitons interact result in nonsensical infinities, indicating the need for a deeper understanding of gravity.
String theory (or, more technically, M-theory) is often described as the leading candidate for the theory of everything in our universe. But there’s no empirical evidence for it, or for any alternative ideas about how gravity might unify with the rest of the fundamental forces. Why, then, is string/M-theory given the edge over the others?
The theory famously posits that gravitons, as well as electrons, photons, and everything else, are not point particles but rather imperceptibly tiny ribbons of energy, or “strings,” that vibrate in different ways. Interest in string theory soared in the mid-1980s, when physicists realized that it gave mathematically consistent descriptions of quantized gravity. But the five known versions of string theory were all “perturbative,” meaning they broke down in some regimes. Theorists could calculate what happens when two graviton strings collide at high energies, but not when there’s a confluence of gravitons extreme enough to form a black hole.
Then, in 1995, the physicist Edward Witten discovered the mother of all string theories. He found various indications that the perturbative string theories fit together into a coherent non-perturbative theory, which he dubbed M-theory. M-theory looks like each of the string theories in different physical contexts but does not itself have limits on its regime of validity—a major requirement for the theory of everything. Or so Witten’s calculations suggested. “Witten could make these arguments without writing down the equations of M-theory, which is impressive but left many questions unanswered,” explained David Simmons-Duffin, a theoretical physicist at the California Institute of Technology.
Another research explosion ensued two years later, when the physicist Juan Maldacena discovered the AdS/CFT correspondence: a hologram-like relationship connecting gravity in a space-time region called anti–de Sitter (AdS) space to a quantum description of particles (called a “conformal field theory”) moving around on that region’s boundary. AdS/CFT gives a complete definition of M-theory for the special case of AdS space-time geometries, which are infused with negative energy that makes them bend in a different way than our universe does. For such imaginary worlds, physicists can describe processes at all energies, including, in principle, black-hole formation and evaporation. The 16,000 papers that have cited Maldacena’s over the past 20 years mostly aim at carrying out these calculations in order to gain a better understanding of AdS/CFT and quantum gravity.
This basic sequence of events has led most experts to consider M-theory the leading TOE candidate, even as its exact definition in a universe like ours remains unknown. Whether the theory is correct is an altogether separate question. The strings it posits—as well as extra, curled-up spatial dimensions that these strings supposedly wiggle around in—are 10 million billion times smaller than experiments like the Large Hadron Collider can resolve. And some macroscopic signatures of the theory that might have been seen, such as cosmic strings and supersymmetry, have not shown up.
Other TOE ideas, meanwhile, are seen as having a variety of technical problems, and none have yet repeated string theory’s demonstrations of mathematical consistency, such as the graviton-graviton scattering calculation. (According to Simmons-Duffin, none of the competitors have managed to complete the first step, or first “quantum correction,” of this calculation.) One philosopher has even argued that string theory’s status as the only known consistent theory counts as evidence that the theory is correct.
The distant competitors include asymptotically safe gravity, E8 theory, noncommutative geometry, and causal fermion systems. Asymptotically safe gravity, for instance, suggests that the strength of gravity might change as you go to smaller scales in such a way as to cure the infinity-plagued calculations. But no one has yet gotten the trick to work.
This article appears courtesy of Quanta Magazine.


For most people, the thought of spending every waking hour with strangers in a metal capsule roughly the size of a studio apartment for weeks sounds like the stuff of nightmares.
For others, it’s a dream.
About 400 people applied this year to live, work, and sleep in NASA’s Human Exploration Research Analog, a three-story habitat built to mimic the confinement of space missions and study human behavior and teamwork dynamics. The space agency has spent the last several months shuffling groups of four volunteers in and out of the habitat, which sits inside a warehouse at the Johnson Space Center in Houston. The groups live in the habitat for 45-day stays designed to simulate a round-trip journey to an asteroid to collect and return soil samples. The latest group emerged this month, were greeted with sparkling fruit juice, and returned to their daily lives, with some much-needed privacy.
The participants are essentially lab rats, the test subjects that will inform the procedures and protocols necessary for future missions to Mars and deep space. Everything that happens to them in that metal tube—their physiological changes, mood swings, interpersonal interactions—will someday be folded into guidelines for keeping astronauts happy and healthy on long-term missions. Similar campaigns to study human behavior for space journeys are underway elsewhere, including a University of Hawaii program called HI-SEAS that put six people in a fake Mars habitat in Hawaii for eight months this year.
“I’ve built a career asking other people to be test subjects. I felt like I owed it to the science to be a guinea pig myself,” says Rick Addante, a psychology and neuroscience professor at California State University at San Bernardino. Addante and three others moved into the HERA habitat in August. “If we want to get to Mars, we have to use our brains, but we also have to understand our brains and what’s going to happen to them on the way to Mars,” he says.
For now, though, the goal of analogs like HERA and HI-SEAS is to get people to survive weeks of confinement in good health—without losing it or turning on each other. The HERA program is part space camp, part escape room, and pays $10 an hour. Crew members are all in it together—literally—so making it work is their only option.
“I had a lot of faith in the powers that be to select a good crew,” says Reinhold Povilaitis, a former HERA crew member and research analyst at Arizona State University who works on a NASA moon orbiter. “Before I went in, I reminded myself to keep an open mind about everything.”
NASA picks “astronaut-like” volunteers, people between ages 30 and 55 who have advanced degrees in science fields or some military experience and can pass medical, physical, and psychological screenings. Participants must also pass virtual-reality motion-sickness tests to prepare for simulations of space walks and sample collections using VR headsets. After that, assembling a crew is kind of like an admissions office pairing roommates in the first year of college. HERA staff tries to pick people who will get along. “They may or may not become best friends, but they work together,” says Lisa Spence, the flight-analog project manager for HERA.
The experience is meant to be as isolating as possible, far more extreme than the environment on the International Space Station. Unlike ISS astronauts, HERA crew members—or HERAnauts, as they’re called—had no internet access and get just 30 minutes a week to call family and friends. Their only connections to the outside world were the handful of NASA employees who monitor them and electronic copies of the Houston Chronicle and U.S.A. Today, delivered every weekday. Their workdays were scheduled to the minute, packed with sample collections, simulations, drills, and wellness tests and screenings.
“It’s weird not to see the sun and not hear the rain and not feel the wind,” says Tim Evans, a biology professor at Grand Valley State University in Michigan who stayed in the HERA habitat from May to June. “But you don’t dwell on that because you’re so busy doing other things.”
NASA closely monitored the crew’s health. They took surveys asking them about their emotional states and math tests that targeted their cognitive function. Their diets—consisting of freeze-dried or thermo-stabilized foods made for microgravity—were tightly controlled. No breath mints were allowed on board, since those would add an extra calorie or two to the daily intake. Participants wore sensors to track their vitals (and, during virtual-reality simulations, their brain activity) and regularly gave blood, urine, and fecal samples. For the blood draws, crew members stuck their arms through a hole in a curtain in the habitat’s airlock, where a “robot”—actually a HERA staffer—stuck a needle in their veins.
On top of that, crew members were deprived of sleep. To study how lack of adequate slumber affects humans, the HERA program kept participants awake for 19 hours every weekday. “We would stand around at 1:59 a.m. just waiting for the clock to go to 2 a.m. so we could go upstairs [to our bunks] and go to sleep,” says Shelley Cazares, a research scientist at the Institute for Defense Analyses in Virginia who stayed in the HERA habitat in August. On weekends, they were allowed a full eight hours of sleep a night.
But the deprivation was taxing. The books some crew members brought with them for entertainment instead put them to sleep. “I’d wake up tired and be tired all day,” Evans says.
The lack of sleep negatively affected their cognitive and motor skills—they made more mistakes when maneuvering a robotic arm on the habitat, for example—and sometimes made them irritable and grumpy. “We all joked about there being a kind of honeymoon phase, where we’d get up in the morning and say, ‘Hey, how are you doing, how’d you sleep?’” says John Kennard, a Green Beret in the U.S. Army who teaches cyberdefense at Fort Bragg in North Carolina. Kennard stayed in the HERA habitat from May to June. “And then after about a week, it was more like grunts than actual conversation. You figure out who’s less talkative in the morning, who needs their own space to fully wake up.” James Titus, Kennard’s fellow crew member, heartily agreed. “In the morning, in-depth conversation could not happen,” says Titus, who works for a nuclear-fusion start-up in California. He said he would dab some Tabasco sauce on his tongue when he started to feel himself dozing off.
The resulting crankiness naturally led to some misunderstandings among crew members. Tension, in general, is pretty unavoidable in such an environment. One crew member likened the experience to a long family car ride across the country, where people are bound to get on each other’s nerves at some point. The key difference is that they can’t leave the HERA habitat for a walk, and must instead talk it out.
Time slows down inside the habitat. A day could feel like an entire week, some crew members say. To pass the time when they weren’t working, the crew played board games and watched a ton of movies. One crew watched every installment of Star Wars, The Lord of the Rings, and Harry Potter.
Sometimes, when the Houston Chronicle delivered a big story, like the firing of FBI director James Comey in May, they’d talk about politics. Crew members often had different political leanings, but they say their discussions remained respectful and productive. “It was, in a lot of ways, the types of discussions you have when you’re in college and you would have that time to sit there and actually hash something out,” says Mark Settles, a plant-cell and molecular biology professor at the University of Florida, who was in the habitat when the Comey news broke.
The isolation had some perks, like the lack of email and all its anxiety-inducing qualities. “It was pretty freeing,” Settles says. Of course, when he returned to the real world, “it took me months to catch up on the things that I had missed,” he says.
The outside world crept into the habitat in a very big way in August, when Hurricane Harvey arrived in Houston, wreaking havoc on the city. A crew was about halfway through their mission at the time, and they tracked the storm’s developments through their daily newspaper deliveries. When Harvey moved to front-page news, NASA started calling the crew member’s emergency contacts, just in case. When tornado warnings were issued in the middle of the night, HERA’s mission-control staff roused the crew members and told them to huddle together on the first floor of the habitat. On the morning of August 27, as Houston’s roads and highways swelled with floodwaters, NASA decided to abort the mission. The rising water made it difficult and dangerous for HERA staff to come in, and the HERAnauts couldn’t be left alone.
The crew had about 20 minutes to pack up their stuff and grab their astronaut food. “We stepped outside and I asked the first person I saw, ‘Why did we cancel?’ And he said, ‘Well, just go outside and you’ll see,’” says Paul Haugen, one of the crew members and a NASA engineer. They stepped out of the habitat and found a whole city underwater. The crew piled into a van and drove carefully to a hotel across the street.
Spence, HERA’s project manager, doesn’t know yet whether NASA will use the data gathered during the shortened mission. The crew that got Harveyed, as Spence puts it, was, naturally, disappointed about such an abrupt end. The crews that completed the full 45 days had a slightly different outlook when they stepped out of the habitat. The members had become friends, and would stay in touch after they left the habitat—texting about inside jokes and even how grouchy they could get—but when that airlock popped open, revealing the world they had pretended to leave, they were thrilled. Their work was done. The astronauts of the future will benefit from what NASA learns from this experiment—but they’ll need to wait far longer to feel the relief of coming home.


Good news for the environment comes from California today, and from a part of the state very near the hillsides that have suffered the economic and environmental devastation of the recent wildfires. A renowned tract of undeveloped California coastal land totaling more than 24,000 acres, or about 38 square miles, has been purchased by The Nature Conservancy (TNC) for permanent preservation, thanks to a $165 million donation by a wealthy tech-industry couple. The donation, the largest single gift TNC has ever received, is significant in its immediate effects, and it has the potential to matter even more through the longer-term example it aspires to set.
The tract includes hills and canyons, grasslands and brush, 2,000 acres of coastal live-oak stands containing perhaps 1 million trees, a creek, parts of the Santa Ynez mountain range—and a full eight miles of the bluffs and beaches that make up the coastline around Point Conception, west of Santa Barbara. The benefactors are Jack and Laura Dangermond, who founded and still run the Esri mapping company in the small southern California town of Redlands where they both grew up. (For the record: I have known the Dangermonds for many years, having grown up in the same town at about the same time.)
The tract will be called the Jack and Laura Dangermond Preserve. It has been informally known as the Bixby Ranch, after the family that owned it and a lot more of Southern California starting in the late 1800s. (More recently the land has been known as the Cojo and Jalama ranches.) Cattle have grazed here since the ranch’s founding, but the land still very much has the look and the ecological characteristics of the wild. Crucially, it has never been subdivided, converted to malls or mansions, or otherwise commercially developed. With today’s announcement, it never will be—a different outcome than many conservationists feared over the past decade, when the land was owned by a real-estate hedge fund from Boston that has specialized in coastal-development projects.
The TNC representatives I spoke with about this nature preserve, along with Jack Dangermond himself, were careful to say nothing whatsoever about these previous owners. But the group’s identity and track record are easy to figure out from online sources. When it bought the ranch for some $136 million 10 years ago, just before the worldwide financial and real-estate crash, its lawyer gave a non-denial denial to local citizens concerned about commercial development. Under terms of the sale, the land could have been broken into more than 100 parcels. But according to Ethan Stewart of the Santa Barbara Independent, the hedge fund’s lawyer said everyone should calm down. After all, he said, his clients “absolutely do not have any specific plans yet for the land.” In the financial and regulatory conditions of the decade that followed, it turned out that they were not able to pursue any.
* * *
Point Conception is evident on a map as the place where the California coast takes a 90-degree turn. On one side of it, stretching toward Santa Barbara and Ventura and beyond them to Los Angeles, the coast runs nearly east-west. On the other side, heading up toward Monterey and San Francisco, the coast runs mainly north-south. (Immediately north and west of the tract is Vandenberg Air Force Base, still in active use as a launching site for rockets and missiles.) The headland of Point Conception juts out into the Pacific at the corner where the coast makes its turn, with one of the earliest lighthouses built along the California coast.
“There’s no place like it on this Earth,” a county supervisor named Joni Gray told Ann Herold and Dan Harder of the Los Angeles Times 10 years ago, after the hedge-fund sale. “It’s more beautiful than Yosemite or Yellowstone.” Another man who had grown up in the region told Herold and Harder, “The footprint of man is very light out here. It’s where you understand what California was all about before people ruined it.”
Before the arrival of the Spanish and then the Americans, this was the land of the Chumash people, who considered the point a portal through which spirits entered the next world. Point Conception’s positioning has long made it perilous for mariners—and precious to scientists and naturalists. “I like to say it’s where Northern California and Southern California meet,” Michael Sweeney, executive director of The Nature Conservancy in California, told me. “That is why it is a zone of such ecological diversity. You have plants and animals you’d expect to find in the north, along with those you’d expect to find in the south. It is uniquely diverse.”
The melting-pot aspect extends even to the waters off the point. “On the marine side, it is one of the most valuable places on the California coast, because it’s where the cold currents coming down from the north meet the warm currents from the south,” Jack Dangermond told me in a phone interview. “With this huge mixing, you have a rookery of seals, big whales, all the elements of a hugely diverse marine resource.” A California state marine reserve, established over the past decade, protects fish, sea mammals, and other wildlife in a 22-square-mile area of the waters off Point Conception. “From the seafloor to the ridgetops you have a big protected area, and that is pretty special in the world,” Sweeney said.
“This is a conservation project of historic significance,” Henry Yang, the chancellor of the University of California at Santa Barbara, told me via email when I asked him about the new preserve. As part of this project, the Dangermonds are also establishing a $1 million endowed Chair in Conservation Studies at UCSB. “The area is recognized globally for its rich biological diversity and ecological significance,” Yang said. “As a transition zone from warmer southern species to cooler northern species both on land and in the coastal ocean, it provides a unique place to study and learn how climate affects the ecosystem.”
“Laura and I ... became deeply attached to this land a long time ago,” Jack Dangermond said in an all-hands messages to the several thousand Esri employees in announcing the purchase. Fifty years earlier, on the Dangermonds’ honeymoon in the late 1960s, they’d driven along the coast in a little car and camped at nights in a pup tent. “We were just kids, and that was our first connection to realizing it was a special place,” he told me.
* * *
The Dangermonds’ gift is significant in two ways, the immediate and the longer term.
The first effect is what it does directly, to preserve habitat. The simple fact is that without this move, sooner or later this ecologically, aesthetically, and even spiritually important land would have been bulldozed away. Now that will not occur.
Jack Dangermond told me that his friend E.O. Wilson, the famed biologist, had advanced the idea that “we are innately all ‘bio-philiacs’”—that people are drawn to nature even in unconscious ways. “It’s why people keep a little philodendron in their apartment, or have an aquarium, or dogs. People want to feel some connection to nature and the natural living world.” The innate importance of preserving parts of the natural matched the Dangermonds’ own sensibility. Jack grew up in a gardening culture—his father, an immigrant from Holland, was a gardener and ran a small nursery business, and Jack’s original training was in landscape architecture—and he and Laura have made the property around their modest home in California essentially one big arboretum. But they argue that preservation has a larger consequence.
“These natural areas, particularly pristine and intact areas like this one, are so very important, and they are disappearing like crazy,” Jack told me. The Dangermonds’ Esri company specializes in mapping software that, among other functions, allows long-term analysis of geographical trends. “We did a study with Clark University, forecasting out 50 years, and making maps with our software about natural areas in that time. And in 50 years, the areas that remain will become very fragmented. If any normal person would see that, they would get very disturbed, but the process is well underway. These models make it clear that the fundamental fabric of nature is being altered, and these areas are going to disappear.”
Thus the urgency, he argued, of “grabbing places like this and putting them into conservation, while you still can.” The vistas of Point Conception may not have the same iconic impact as the Grand Canyon or Yosemite, he said. “But ecologically it is just as important”—because of the diversity of flora and fauna, because of its nearly unspoiled aspect. “We have the splendor of the national parks because they were set aside 100 years ago in a park system. It gives me a kind of solace to think that more of these areas will be protected.”
Beyond solace, the Dangermonds hope that UCSB and The Nature Conservancy will together make Point Conception a unique digital-research center for the identification and protection of similarly important and endangered ecological hotspots around the world. They’ll have help from Esri’s software, which they believe will allow them to measure and analyze trends, good and bad, more extensively than in any comparable site.
“This really is a pivotal point to be envisaging how new technologies might be employed to gather the data in support of research,” Chancellor Yang wrote to me. “Many new Earth-observing satellites have been launched in recent years, with the ability to scan the surface more frequently and in more detail. We have a wide range of sensors that can be installed on, below, and above the land surface; we have new computer models that can take these data inputs and process them to make accurate predictions of the impacts of conservation strategies.”
* * *
The second significant aspect of the purchase is the example the Dangermonds hope to set for their fellow rich people. This donation is unusual not simply in its scale but also because the Dangermonds are publicizing it, something they usually take pains not to do.  
For a long time, Jack and Laura Dangermond have been the wealthiest family in their small community of Redlands, and its leading philanthropic donors. Since their Esri company is still privately held, the precise extent of the Dangermonds’ wealth is also private. Jack told me that published estimates, in the low to mid billions, are “exaggerated.” Whatever the details, they are people of means. But their previous gifts have been unpublicized and often anonymous. Their names don’t appear at all, or only in fine print, at many of the local institutions they support.
In contrast, this tract of land will be the Jack and Laura Dangermond Preserve, and the professor at UCSB will hold the Jack and Laura Dangermond Chair. Why?
“We’re very intentionally setting out a model that we hope other people with money will follow,” Jack Dangermond said. “We’d like people to think, ‘Let’s do what the Dangermonds did.’ We’d like them to copy us.”
“There are lots of wealthy people in the tech industry in California and elsewhere,” he said. “America’s wealthy people are flush with money. They’re wondering where to put it”—and the favored emphases of this era’s tech leaders include education and public health. “Those are great! But we haven’t yet seen that kind of commitment to nonpolitical conservation issues.” He gave the example of the Rockefeller family a century ago, who devoted some of their assets to preserving land in New York, in Maine, in Wyoming, in many other places that eventually adjoined or became part of national or state parks.
He is aware of the complications of this private-philanthropy model for conservation: the baronial overtones, the theoretical superiority of having the government take the lead with truly national parks, like the Grand Canyon or Yellowstone. But to put it mildly, that’s not what the federal government is doing these days. In the weeks before the Point Conception announcement, Donald Trump ordered the the Bears Ears and Grand Staircase–Escalante national monuments reduced in size. When I asked Michael Sweeney why the state or federal government hadn’t bought the ranch long ago—including through the defense budget, since Vandenberg Air Force Base is next door—he said, “Because it just would never happen. The price is an obstacle. There are too many agencies to coordinate.”
Jack Dangermond makes a more positive case. “If you look back historically, the national parks were of course a public undertaking. But families like the Rockefellers played a direct or indirect part for many years. America has a long history of private philanthropy for public causes that sometimes gets overlooked.”
Much as the Rockefellers’ example is remembered now—or Carnegie’s with his libraries, or the Mellons’ and Fricks’ with their museums—the Dangermonds hope that in their smaller-scale way they can set an example others can refer to. “We would love to have 100 Dangermond Reserves,” he said. “But I’m not Carnegie. We’re not in the oil business. We can’t do this by ourselves. We’re telling the story to set an example of what others could do.”
Examples set by people of wealth and influence can have their ripple effects through society. Andrew Carnegie established some 2,500 libraries largely with his own money, but ultimately their success and survival depended on support from the communities where they were built. Indeed, Carnegie conditioned his grants on guarantees of long-term local taxpayer support for each library, because otherwise “the public ceases to take interest in it, or, rather, never acquires interest in it.” Jack Dangermond said that, beyond its potential effect on other rich families, the new preserve would encourage citizens more generally to look at conservation projects in their own neighborhoods. “People could buy a vacant lot and turn it from ‘gray infrastructure’ to ‘green infrastructure,’” he told me. “As cars become automated, a lot of parking-lot space will open up, and people in neighborhoods can collectively or individually create parks and make cities more livable.”
Will individuals and families make the connection between this large act of philanthropy and the smaller-scale opportunities immediately around them? It’s a lot to expect. But even if the impact of today’s news were, despite the donors’ aspirations, confined to one evocative tract along the California coast, the announcement would be a significant one. It is a gift to the planet and to the future—a dramatically positive move from people with wealth, when there’s been so much movement the opposite way.


Can unexpected weather make a war or a failed state more likely? It’s a question that could define the 21st century.
A new study, published Thursday in Science, finds a link between temperature variation and forced migration.
When unusually hot or cold weather strikes the growing region of an agricultural country, more people living in that country seek asylum protection in the European Union. Those people are then, in turn, more likely to be accepted as permanent residents by the EU.
Because asylum applicants must be fleeing conflict or persecution—and because their acceptance seems to validate the severity of their claims—the study’s authors say they’ve found an underlying connection between weather, agriculture, and failed governance.
“It’s pretty much like a medical trial of a new drug. There are many impacts that affect health, just like there are many impacts that affect asylum applications. But we’ve set up the trial and those are not correlated, so I have faith that we’ve established a relationship between weather and conflict,” said Wolfram Schlenker, an environmental economist at Columbia University. Schlenker and Anouch Missirian, another environmental economist at Columbia, conducted the study.
The study began by examining two databases: asylum applications to the EU between 2000 and 2014, and average temperatures across 103 countries.
The authors omitted asylum data from 2015 and 2016, when refugees were fleeing the Syrian Civil War and other conflicts. More than 1 million people applied for asylum annually during those two years, a spike well above the average 350,000 annual applicants from 2000 to 2014.
Schlenker and Missirian found an early correlation between weather and migration, but they waded through the data, trying to account for as much statistical noise as possible. They removed one-year shocks from events like the onset of the global financial crisis. They also factored in the difference between hot and cold countries, as a naturally colder country might be able to deal with a few extra degrees more easily than a hot country.
Ultimately, they found that the entire effect in asylum increases was attributable to temperature shocks in maize-growing countries that hit during the growing season, in the area where crops are grown.
Though the research seems to examine the relationship between climate change and migration, it’s actually getting at the deeper question of climate and governmental collapse or oppression. That’s because of the definition of forced migration: Only refugees from conflict or persecution can apply for permanent asylum in another country. The 1951 UN Refugee Convention, which every nation in the EU has signed, does not define a refugee as someone fleeing a country for economic reasons.
If a country suffered a temperature shock, then its refugees were three times more likely to be accepted for asylum than the average applicant to the EU. To Schlenker, this points to a deep connection between climatic crises and military conflict or persecution.
“The mechanism through which this works is conflict,” he told me. “We don’t know why they get accepted or not, but people in destination countries in the EU find them to be worthy of protection.”
The paper also projected its findings forward, by examining when various climate models believe weather shocks in growing regions could become more likely. They found modest increases. In a world that gets carbon-dioxide pollution under control and holds global warming to roughly 2 degrees Celsius, the number of asylum applications to the EU could rise by about 28 percent.
But if carbon pollution continues unabated, and global temperatures rise by about 5 degrees Celsius, annual applications could rise by 188 percent by 2070. More than 650,000 people could seek protection in the European Union annually.
Schlenker made it clear that these numbers should be used as tools for thinking and not final projections. “This is the best estimate we can do with current data, but there’s lots of asterisks,” he said. The projections could err too high, because they don’t account for global adaptation to warmer weather. But they could also still be too low, because they can’t anticipate political tipping points like civil wars or regime change.
“If a country switches from a democracy to a dictatorship—which would cause many more asylum applications—that would have a huge effect and we don’t account for it in the model,” he said. Schlenker noted that while these would be huge increases, the number of migrants who fled the Syrian Civil War was “much larger than the predicted impact we’re seeing with climate change.”
Researchers who study the intersection of climate and politics said that the historical connection was an extremely sturdy finding, but they shared in Schlenker’s skepticism about the future projections.
“This is a very strong study showing a robust relationship between temperature and forced migration,” said Claire Adida, a political scientist at the University of California at San Diego, by email. “The authors were meticulous in their analysis.”
“They show that the relationship holds only when you look at temperature deviations over the growing season and in the crop area ... This, by the way, is consistent with a lot of other research in agricultural economics showing that climate change and conflict are likely to be related via the effect of climate change on agricultural income,” she said.
Elizabeth Chalecki, a political scientist at the University of Nebraska at Omaha, agreed that the core finding was strong, but she identified a few more variables that she said could be related. Forced migration could also depend on whether commodity crops were available to relieve a food shortage or whether there were broader non-climatic events—like a genocide—happening in the region.
“The researchers admit that their correlation is ceteris paribus, meaning all things being equal, but that never is the case in real life,” she said. “If nationalist politics are on the rise across the EU and elsewhere, then doing something about climate change might be good national-security policy, not just good environmental policy.”
Giovanni Bettini, a political theorist at Lancaster University, said it was important to be careful about discussing climate migration as fait accompli.
“The inference that correlation equates causation is very problematic,” he said, adding that the link between climate change and political mobility was a “political open question.”
And he said it was important to treat projections not as settled science but as one of many tools for thinking about the future. It was crucial, too, he said, to talk about migrants with nuance and care not to fall into racial tropes. “Climate refugees are never white, in the discourse,” he said. “The dangerous flocks of people who are set to be uprooted in the future, creating dangerous security threats for ‘us,’ are always somewhere else.”


Emma Wren Gibson, frozen as an embryo in 1992, was born a few days after Thanksgiving in 2017, more than 25 years later. It’s the longest an embryo is known to have been frozen before being born as baby.
In fact, the embryo that became Emma is only a year younger than the woman who gave birth to her, Tina Gibson. “This embryo and I could have been best friends,” Gibson, now 26, told CNN. Tina and her husband “adopted” the frozen embryo after learning he was likely infertile. It came from an anonymous couple who went through in vitro fertilization (where sperm and egg are united in a lab) and donated their remaining frozen embryos, which have remained suspended in time for more than two decades.
Today, IVF and the cryopreservation of resulting embryos are so commonplace that it’s easy to overlook how disruptive the process once seemed. “All hell will break loose, politically and morally, all over the world,” James Watson once said about IVF. Others warned of the slippery slope to surrogate pregnancies, designer babies, and artificial wombs.
When The New York Times wrote about cryopreservation in 1974, experts envisioned a catalogue of “one-day-old frozen embryos, guaranteed free of all genetic defects, with sex, eye color, probably IQ, and other traits described in detail on the label. Just thaw and implant.” But what no one quite considered was how much the technology could eventually shift time—resulting in births like Emma’s.
In the beginning, embryos created through IVF were implanted in the womb right away. The first IVF baby, Louise Brown, was born in 1978. It took six more years before doctors perfected the freezing and unfreezing technique. Zoe Leyland, the first baby from an embryo frozen after IVF, was born in Australia in 1984. The embryo had been frozen for what we might now consider a brief two months. Doctors in the United States, Britain, Israel, the Netherlands, and West Germany quickly picked up the procedure; over the next two years, more than two dozen babies were born from frozen embryos.
The ability to stop time, even briefly, offered practical advantages. Before cryopreservation, embryos had to be implanted all at once or discarded; now extra embryos could be saved for later, reducing the chance of twins and triplets. Doctors could wait for the moment a mother’s body was most ready to accept an embryo. And it eventually became possible, as predicted, to use the extra time to test embryos for genetic disease. (Actual designer babies are still far from reality, though.)
The IVF Panic: 'All Hell Will Break Loose, Politically and Morally, All Over the World'
Freezing also bought time for families who were uneasy about discarding their extra embryos, but who were not sure about donation, either. Today, as many as a million frozen embryos are stored in tanks of liquid nitrogen. They remain even when their genetic parents die or divorce, inevitably raising legal questions. There are ethical questions, too: The National Embryo Donation Center, where the Gibsons “adopted” Emma, is a Christian fertility center whose purpose includes “protect[ing] the lives and dignity of frozen embryos.”
To make it even more complicated, no one knows exactly how long frozen embryos can remain suspended in time. The only way to know is to keep waiting. There are no official records on how long eggs are frozen before being implanted, but before Emma, the oldest known embryo that resulted in a live birth had been frozen for 20 years. A woman in New York gave birth to the boy in 2010. And in 2013, a woman gave birth from a 19-year-old embryo.
Theoretically, the reproductive specialist Barry Behr once told Scientific American, frozen embryos might last as long as a century or two. The real hitch, he said, is how cryopreservation could scramble ideas of family and generations:
The nature of reproduction and building families does not really foster an environment that would allow you to wait 50 years before you thaw your embryos out. Unless your daughter wanted to carry her sibling, for example, which in theory is possible: A person born from IVF may still have sibling embryos frozen and when they reach 30 and are infertile could technically thaw out the embryos that were created at the same time they were created, gestate them, and deliver their sibling. That hasn’t happened yet but it could be possible.
There may another practical consideration, too. Embryo-freezing technology has changed considerably in the last 25 years, and different freezing techniques require different thawing techniques. Deborah Wachs, a doctor at the Reproductive Science Center of the Bay Area, where the 19-year-old embryo was thawed and implanted, says that her center was transitioning from an older slow-freezing technique to a new vitrification technique at the time. Their longtime lab director had been there since the embryo was first frozen.
“If you look at a new embryologist coming out, they will likely never slow freeze in their entire careers because all they will know is vitrification,” Wachs says.  In 10 years, she notes, everyone will be much less familiar with how to thaw slow-frozen embryos. The limiting factor on how long frozen embryos can remain suspended in time may not be physics or biology, but the half-life of human knowledge.


Hundreds of millions of miles away, in the orbit of Saturn, on the surface of Titan, the planet’s largest moon, rests a piece of human-made technology. Huygens, a nine-foot-wide, saucer-shaped probe, was dropped by the passing Cassini spacecraft and parachuted down to the surface in 2005. For a precious 72 minutes after it landed, Huygens transmitted data back to Earth, including image after image of its surroundings. There they were—gullies, the kind that on Earth are etched into rocky terrain by flowing water. Scientists watched, enthralled, as views of an alien land flashed across computer screens, marking humanity’s first look at the surface of a moon that wasn’t their own.
Then Huygens, out of battery power, went dead; its demise was, as grim as it seems, part of the plan. Huygens remains in the spot where it landed, a dusty monument to the desire to glimpse, even briefly, the worlds in our solar system.
But a new piece of technology may be on its way. Not to land on Titan, but to hover—getting just close enough to reveal secrets.
A mission to send a drone-like spacecraft to study Titan received approval and funding from NASA Wednesday. Dragonfly, a dual-quadcopter, would launch in the mid-2020s and, upon arrival, hop from one spot over Titan’s surface to the next, searching for signs of life.
The funding comes from NASA’s New Frontiers program, a competition for exploration proposals that has produced several well-known and successful robotic missions in the solar system: New Horizons, which flew past Pluto in 2015; Juno, which currently orbits Jupiter; and OSIRIS-REx, which is on its way to an asteroid called Bennu, where it will carve out some surface material and return the sample back to Earth. The Dragonfly mission, led by Elizabeth Turtle, a planetary scientist at Johns Hopkins University’s Applied Physics Laboratory, now has $4 million and one year to complete its concept. So does the second mission NASA approved today, the Comet Astrobiology Exploration Sample Return (CAESAR) that would target a comet that approaches the sun about every six-and-a-half years. But only one of these missions will actually launch—NASA will choose between them in 2019.
Titan, the largest of Saturn’s moons, has long mystified scientists. The moon is wrapped in a dense, planetlike atmosphere mostly made of nitrogen. Spacecraft like Voyager 1 and Cassini have detected a plethora of complex and organic molecules in the atmosphere that are also found on Earth. Robotic missions have also revealed Titan has a similar liquid process to the water cycle on Earth, but with a different chemical compound: methane, the main ingredient of natural gas. On Titan, methane clouds release methane rains that feed methane lakes and seas and streams that can erode the rocky landscapes. This makes Titan, like Earth, an ocean world. All together, these features mean Titan is one of the best candidates for life in our solar system.
But, aside from Huygens, the imaging instruments on spacecraft that have visited Saturn’s orbit have been unable to penetrate Titan’s hazy atmosphere, and the composition of its surface remains largely unknown.
Dragonfly, should it take flight, will carry equipment capable of identifying chemical components important to biological processes. It will search for signatures of hydrocarbon-based life-forms and water and check for signs of progress in prebiotic chemistry—the transformation of simple molecules into complex life. It will carry cameras that will help the copter choose future landing sites and, of course, send home potentially beautiful, high-resolution images. “We’re not exactly looking for five-legged creatures running around or something like that,” Peter Bedini, the program manager for Dragonfly, said during a talk in July. “Although if we did confront one of those, we’d be sure to take a selfie and send it home.”
The support for the Dragonfly mission is a boon to astrobiologists seeking signs of life on other ocean worlds, like Enceladus, another Saturnian moon, and Europa, a moon of Jupiter. On these moons, potential oceans lurk under thick crusts of ice, perhaps teeming with microbes and other life-forms.
The drone-like design of Dragonfly is unusual given the previous roster of robots sent to explore the solar system, like the suite of landers and rovers that have traveled to Mars since the 1970s. But “the dense, calm atmosphere and low gravity [of Titan] make flying an ideal means to travel to different areas of the moon,” the Dragonfly team explains. Aerial probes can cover more ground than rovers. Mars rovers like Curiosity and Opportunity have a top speed of less than one mile per hour, and are not built to maneuver over particularly rough terrain. After five years on Mars, Curiosity’s wheels show some significant damage from crunching over rock. As NASA pushes forward with the exploration of worlds whose surfaces are less understood than that of Mars, scientists and engineers may need to rethink the hardware best suited for exploration.
“In just a few flights, Dragonfly will be able to go farther than the Opportunity rover on Mars has in the last 12 years,” Bedini said.


The Republican tax-reform bill, which passed Congress Wednesday, makes some big changes to the federal government. It repeals Obamacare’s health-insurance mandate, temporarily expands the child tax credit, and permanently cuts taxes on corporations and the wealthiest Americans.
What it doesn’t do: impose a new tax on carbon dioxide, the heat-trapping gas that warms the globe and acidifies the oceans.
This might not come as a surprise, as only some GOP politicians accept that global warming—which a carbon tax is meant to slow—is real. But passage of the tax bill will end a year of prodding, cajoling, and storytelling from leaders in both parties—including sitting senators and Reagan-administration alumni—that tried to force senior Republican leadership into considering some kind of plan to soften the blow of global warming.
As the GOP passes its largest legislative package in years, with no carbon price to be found, it’s clear that those rhetorical efforts have failed. Even as wildfires and hurricanes ransack the coasts, and record-breaking temperatures stack up, there’s still little appetite among the party’s leaders to address climate change through tax policy.
By rejecting a carbon tax without proposing an alternative, Republicans may have relinquished their best chance to shape climate policy this decade, continuing to cede the issue to Democrats at a national level.
Some background: A carbon tax is one of several ways that economists have proposed dealing with climate change. Right now, it costs nothing for American polluters to release carbon dioxide and other greenhouse gases into the atmosphere. Those gases trap heat and aggravate global warming, leading to hotter days, rising seas, more intense storms, and a host of other environmental problems.
A carbon tax aims to make it expensive to release carbon pollution. Under a carbon tax—also called a “carbon fee” or “price”—the federal government would charge companies for each ton of carbon dioxide they release into the atmosphere. Economists and politicians disagree about how to spend the cash raised by such a policy: Some want to rebate it directly back to Americans; others say it should be used to help out-of-work coal miners and renewable-energy development.
Economic models of the idea have made big promises. If the United States imposed a tax of $25 per ton of carbon pollution, it could raise $1.1 trillion over 10 years while preventing 12.4 billion tons of carbon emissions by 2030, according to Resources for the Future, an independent economics research group. A $50 tax could raise $1.9 trillion and prevent almost 20 billion tons.
A carbon tax of $15 would allow the United States meet its goals under the Paris Agreement, according to Resources for the Future. In those negotiations, President Obama promised that the United States would reduce its annual carbon emissions by about 1.6 billion tons by 2030.
At this time last year, Republican opposition to carbon taxes seemed like a sure thing. In May 2016, as he was securing the party’s presidential nomination, Donald Trump tweeted: “I will not support or endorse a carbon tax!” One month later, House Republicans unanimously voted for a nonbinding resolution that rejected the idea of a carbon tax.
But it seemed possible that the party’s stated goal of bringing the U.S. corporate income tax in line with other countries’ revenue schemes could have pushed them to adopt a carbon tax had things gone a little differently. Ireland, which boasts famously low corporate tax rates, also imposes a carbon price of roughly $23 per ton. France, Japan, Ukraine, and more than 35 other countries have either implemented or passed a carbon price, according to the World Bank.
After Trump’s victory and inauguration, some of the Republican Party’s elder statesmen said it was time for a conservative climate policy.
In February, a group of GOP-affiliated economists and former Reagan cabinet officials announced a plan to gut EPA regulations while imposing a a new tax of $40 per ton of carbon pollution. The proposal’s supporters included James A. Baker III and George P. Shultz, secretaries of state to Presidents George H.W. Bush and Ronald Reagan. (In the interest of disclosure: Laurene Powell Jobs, a co-owner of The Atlantic, was also a founding supporter of the plan.)
“Crazy as it may sound, this is the perfect time to enact a sensible policy to address the dangerous threat of climate change,” wrote the economists Martin Feldstein and Gregory Mankiw in The New York Times in February. “Republicans are in charge of both Congress and the White House. If they do nothing other than reverse regulations from the Obama administration, they will squander the opportunity to show the full power of the conservative canon, and its core principles of free markets, limited government, and stewardship.”
Both men brought solid Republican bonafides: Feldstein spent three years as President Reagan’s chief economic adviser; Mankiw played the same role for George W. Bush. They warned that “a repeal-only climate strategy would prove quite unpopular,” citing polling that shows six out of 10 Americans are worried about global warming.
Mitt Romney tweeted approvingly about the plan, its leaders had a press conference in D.C., and some of them even met with the White House economic adviser Gary Cohn—and that was about as far as it got. Congressional Republicans never took up the proposal. President Trump withdrew the United States from the Paris Agreement in June, and he revoked the bulk of President Obama’s climate regulations in October.
Speaking by email on Tuesday, Feldstein did not sound contrite about the work he did to support the Republican carbon-tax plan. “I wouldn’t have done it if I didn't think it might be legislation,” he said.
But the carbon tax’s failure didn’t cloud his view of the overall Republican tax bill. “On balance,” he told me, “I like it.”
Republicans were not the only ones to make a pitch. In August, Senators Brian Schatz of Hawaii and Sheldon Whitehouse of Rhode Island, both Democrats, proposed their own carbon-tax compromise. They hoped to impose a new price of $49 per ton of carbon pollution, using the trillions of dollars in new revenue to cut the top corporate income tax rate. Their plan also went nowhere.
“I don’t think there was a serious prospect of it happening, seeing as how the whole tax package is mostly a way to reward the rich—and especially campaign contributors, a significant portion of whom are in the fossil-fuel business,” says Michael Gerrard, the director of the Sabin Center for Climate Change Law at Columbia University.
“If the Republican leadership was actually serious about the deficit, a carbon tax would’ve been a marvelous opportunity,” he adds. “But obviously there were several things going against it, mostly the words ‘carbon’ and ‘tax’ ... and ‘Trump.’”
“It’s clear that much of the sales pitch for a carbon tax that has been aimed at Republicans has fallen flat,” says Joseph Majkut, a geoscientist and the director of climate policy at the Niskanen Center, a libertarian think tank that supports a carbon tax. He says it’s time to focus on “bring[ing] Republicans toward the mainstream on climate science” while still advocating for a hands-off carbon price.
Oren Cass, a senior fellow at the Manhattan Institute and the domestic policy director of Mitt Romney’s 2012 presidential campaign, said he was grateful that a carbon tax never made it into the bill.
“The long and the short of it is, I don’t think a carbon tax is either good climate policy or good fiscal policy,” he told me. “Even the pretty strong proponents of the carbon tax, they don’t try to assert that it will do anything about climate change. And if you ask them to provide any benefit estimate that they could put into analysis, they won’t—you’ll get vague talk about leadership, or how it’s going to spur innovation.”
But he also resented that the Republican tax bill in its final form increased the deficit by more than $1 trillion. “I dislike strongly that the tax cut is not revenue neutral,” he told me. “I think if you want to cut taxes, you should find other taxes you want to have to pay for it. But if you were to plot out a list of all the revenue raisers you could have, I think a carbon tax should be very far down the list.”
Why? Because carbon-dioxide emissions make for a “terrible tax base,” he said. If a carbon tax succeeded in reducing the use of fossil fuels, or forcing people to move to renewable energy, then it would erode its own tax base over time. “You’ve shifted onto an unstable tax base that you’re hoping will go away, and you’ll wind up having to raise other taxes up anyway” He also said a carbon tax imposed regressive penalties on sectors and regions already struggling in the current economy—such as energy-intensive manufacturing in the Midwest—while rewarding “higher-income coastal knowledge work.”
The massive unpopularity of the Republican plan—41 percent of Americans believe it is a “bad idea,” according to a Wall Street Journal/NBC News poll—may now give Democrats the opportunity to pass their own tax bill. Adam Looney, who led tax analysis at the U.S. Treasury Department for the last three years of the Obama administration, predicted at an event last month that the tax code would continue to change in the years to come.
“It also seems like this is not going to be the last word on tax reform. There will be a lot of changes yet to come in the tax system,” he said. “The carbon tax will always be right there on the shelf ready for the right moment.”
Gerrard, the Columbia University professor, agreed, saying that the Republican carbon-tax plan from February isn’t dead yet. “If, for instance, we have Democratic control of Congress [in 2020 or 2024], then that proposal could attract the moderate Republicans who are now keeping their heads down in the foxhole,” he told me. (Though that assumes that Democrats would reintroduce a Republican-invented plan.)
But even if the pipe dream of a bipartisan carbon tax has ended for now, the effort to bring Republicans over to the fold will never die. The Climate Solutions Caucus, a bipartisan group of House legislators, recognizes the existence of global warming and “explores policy options” to research it, slow it, and prepare for it. Membership must be kept even between Democrats and Republicans—meaning, in essence, that a Democrat can only join when they woo a Republican to hop in, too.
The caucus’s membership has swelled this year; there are now 31 House Republicans who belong to the group. Their votes alone could have nixed the GOP tax bill: Only 23 House Republicans would have needed to vote no to kill the bill (or at least force changes to it).
Carlos Curbelo, the cochair of the Climate Solutions Caucus and a Republican whose district includes the Florida Keys, seemed to exemplify the tensions faced by moderate, climate-concerned Republicans this week as he voted “yes” for the tax bill—right after calling President Trump’s refusal to recognize global warming as a national-security threat “unacceptable.”
Six Republicans in the Climate Solutions Caucus did vote against the tax bill; they made up half of all GOP House dissenters.
On Tuesday afternoon, I contacted all 31 Republicans in the Climate Solutions Caucus, asking them if they had threatened to withhold their support for the bill for its lack of a carbon tax. I also asked them whether they anticipated this Congress, led by Speaker Paul Ryan and Senate Majority Leader Mitch McConnell, ever passing climate legislation.
None of them got back to me.


Growing up, the writer Clay Bonnyman Evans had heard all sorts of stories about his mother’s father, First Lieutenant Alexander Bonnyman Jr. Evans’s grandfather had joined the Marines’ officer ranks during World War II, and in 1943, he learned he’d be taking part in an assault on the remote island of Betio (BAY-sho) in the South Pacific. The goal was to drive out Japanese defenders so the United States could use the island to launch attacks on other Japanese-occupied territories.
After Bonnyman and his troops stormed the beach at Betio, they dropped TNT where Japanese service members were hiding out, using flamethrowers to light the explosives. That flushed out more than a hundred Japanese defenders so that American forces could take control of the island, despite heavy losses. Bonnyman was felled by Japanese fire on November 22, 1943, and the military awarded him the Medal of Honor after his death. “By his dauntless fighting spirit,” the citation read, “Bonnyman had inspired his men to heroic effort, enabling them to beat off the counterattack.”
Though 56-year-old Evans, of Hilton Head Island, South Carolina, had never met his grandfather, it was his greatest wish to find Bonnyman’s remains and bring him home to the United States. No one seemed to know where he’d ended up, though. A family member said Bonnyman was buried in the National Memorial Cemetery of the Pacific in Hawaii, but that was never confirmed. Relatives eventually decided to have his tombstone inscribed with the words “Buried at Sea.”
Evans’s mission to find his grandfather got a major assist when he struck up a conversation with a man named Mark Noah. A commercial pilot with a consuming interest in lost U.S. service members from World War II, Noah had founded a private nonprofit organization called History Flight. He had made multiple trips to the South Pacific to unearth the bodies of lost Marines—one of a variety of efforts, both public and private, to recover missing service members around the world.
The U.S. government’s Defense POW/MIA Accounting Agency (DPAA) estimates that the remains of more than 80,000 U.S. service members are still undiscovered, and the total of undiscovered remains is many times that when fallen troops from other nations are included. Using a combination of traditional excavation methods and newer ones like ground-penetrating radar, History Flight and similar organizations around the world have found hundreds of service members once deemed unrecoverable. The Association for the Recovery of the Fallen in Eastern Europe, for example, has retrieved the remains of World War II service members in a variety of countries, including Germany, Poland, and Russia. With government programs sometimes falling short, privately funded recovery efforts and public-private ventures have come to play a central role in efforts to return missing service members to their home countries.
It was History Flight’s Noah who first supplied Evans with an important clue about his grandfather’s whereabouts. According to a World War II–era casualty card Noah unearthed from military archives, Bonnyman had been buried in a grave at Cemetery 27 on Betio—a site that had been given up for lost in the 1940s after U.S. personnel carried out a search for it and came up empty.
Knowing Noah’s sights were set on locating this grave site and others, in 2013 Evans decided to enlist as a volunteer on History Flight’s Betio project. “I went down for the first time to help dig—I was just a grunt. I’ll do anything and everything they want me to do.”
Evans returned to Betio multiple times over the years, sifting sand, cleaning skeletal remains, digging out burial trenches, and shooting photos and video. The more time he spent there, the more committed he became to Noah’s mission, even though the excavation team hadn’t yet found his grandfather. “Mark and I were in very close touch,” Evans recalls. “We always talked about Cemetery 27 and that finding my grandfather might be a needle in a haystack.”
In March 2015, Evans got a call from Noah. He said History Flight’s team might be zeroing in on Cemetery 27. Would Evans like to join the excavation and document the recovery? Noah kept Evans updated on the team’s progress, and it wasn’t long before Evans went ahead and booked himself a flight.
The island of Betio—a side jewel in the necklace-shaped Tarawa Atoll—has an area of less than one square mile, but it played an outsize role in 20th-century history. For centuries, Betio, part of the Republic of Kiribati, has been home to a small population of seafaring Pacific Islanders who fished to feed their families. Their descendants, more than 16,000 of them, still inhabit the island today and speak the traditional Kiribati language.
In late 1943, the U.S. command decided it needed to capture the nearby Marshall Islands from the Japanese in order to secure enough territory to launch air attacks from a land base. But to claim the Marshalls, the United States would have to capture a series of other islands first, including the Tarawa Atoll.
The United States assembled a massive invasion force comprised of a dozen battleships, 17 aircraft carriers, more than 60 destroyer ships, and over 18,000 Marines to take Tarawa and nearby islands. The frontline Marines who landed on Betio, Tarawa’s largest island, saw a tropical beach that was swiftly transforming into a hellscape. Battle strategists had failed to account for the low tide, so Japanese gunners on high ground were able to pick off many men as they struggled up the exposed beach. Others fell later on in fierce hand-to-hand combat. Altogether, more than 1,000 U.S. troops died in the 76-hour battle, and afterward, the waves around Betio were tinted red with blood.
These men’s sacrifices were not in vain—U.S. forces defeated the Japanese at Tarawa, setting up a string of other crucial wins in the Pacific. But in the heat of war, many service members killed in action had to be hastily buried on Betio. A body-recovery sweep in 1946 turned up the remains of 436 Marines, fewer than half the battle’s total casualties, and in 1949, the Office of the Quartermaster General designated the rest of Tarawa’s missing “unrecoverable.” That meant hundreds of families thought they’d never be able to bury their loved ones. Families were told, variously, that service members had been buried at sea or in unmarked graves.
That lack of closure was what got to Noah. As a commercial pilot and aviation-history expert, he had lent his knowledge to missing-persons recovery cases involving planes. Seeing the grief families felt for their missing relatives, who were often service members, Noah resolved to use his flying skills to aid the recovery cause. “We started doing airplane rides and using [them] to fund the search for the missing,” says Noah, who founded History Flight in 2003. “While we started doing that, it became very apparent there was a tremendous need for this kind of work.”
Though Noah eventually stopped selling plane rides, his recovery mission continued, taking on momentum as donations poured in. Since 2006, History Flight has sent more than 100 recovery teams to World War II battle sites around the world, including Allmuthen, Belgium; Buschhoven, Germany; and Mili Atoll in the Marshall Islands. The organization has raised more than $6.5 million, much of it privately donated, to fund these searches, and Noah himself logs 40 to 60 hours of searching each week on a volunteer basis. In 2015, History Flight entered a public-private partnership with the government-run DPAA, which helps identify the service members History Flight finds.
In Tarawa, Noah recognized a peerless recovery opportunity. World War II–era military records showed that hundreds of service members were likely still buried on Betio. That meant a well-targeted long-term search effort could grant peace and closure to thousands of relatives. “The reuniting of the person’s remains with their family is extremely important,” Noah says. “Instead of being buried underneath a garbage dump or a parking lot, they can be buried in their home country in a cemetery.”
As Noah ramped up his Tarawa excavation plan, he got in touch with a number of archaeologists, including Agamemnon Gus Pantel, a Puerto Rico–based archaeologist who’d worked on other History Flight projects in Europe. Pantel had experience using ground-penetrating radar to identify burial locations, an invaluable asset to any team searching for lost service members. At Noah’s urging, Pantel joined the Tarawa team as a volunteer in 2012. He was moved by the chance to help bring service members home with the honor they deserved. “When you deal with MIAs, it’s very real, and these are almost contemporaries,” he says. “Most of these Marines we find are basically just kids—people who gave up their lives and didn’t really have a chance to enjoy life.”
From the start, the team—which also included History Flight forensic archaeologist Kristen Baker—knew they were in for a tough slog, since others had explored Betio in the past and come up empty. But because History Flight’s team came armed with ground-penetrating radar, they had an advantage. This type of radar uses microwaves to map different kinds of material below the soil surface. Unusual objects in the ground send different patterns of energy waves back to the surface than the surrounding soil, and the radar’s antenna picks up and records these differences. With this information—considered alongside things like historical records and the findings of cadaver-dog searches—researchers can assemble a rough map of where bodies may be located, which allows them to focus on excavation sites where they are most likely to have success.
Beginning in 2008, teams of History Flight researchers had gone to Betio periodically to conduct radar scans of the island. On some of these trips, they brought along surveyors and historians to help them get the lay of the land, as well as a cadaver dog named Buster, who had been trained to sniff out human remains. As Noah and his team compared the evidence they’d gathered to World War II–era burial-location notes and maps, they grew fairly sure they’d found Cemetery 27. According to U.S. military records, at least 40 Marines—including Bonnyman—had been buried in this cemetery after the Battle of Tarawa, but no one had been able to find it since.
In early 2015, with maps and radar results in hand, Pantel, Baker, Noah, and the rest of the History Flight team returned to Betio to uncover Cemetery 27. The team sweated for long hours under the scorching tropical sun to dig in the most promising spots. The work was even more brutally difficult than usual, since thick concrete pads—the remains of wartime installations—covered much of the cemetery. To dismantle the pads, the team had to unleash an arsenal of heavy equipment, including concrete saws, jackhammers, and a backhoe. “Our people work 14 to 16 hours a day,” Noah says. “We went through about 50 to 70 centimeters of concrete, and about 50 centimeters of crushed coral rock below.”
But chewing through the rock-hard barriers proved worthwhile. As the concrete and coral fragments gave way to finer grains and rich dirt, Pantel knew just how close they were getting. “You had a very clear whitish coral sand, and then all of a sudden you had this dark soil literally defining the burial trench. It just jumped out at you,” he says.
Soon after that, the team started turning up telltale dog tags, long bones, and even uniforms. One by one, Marines killed at Betio emerged from the soil, many of them almost perfectly preserved. As difficult as it had been to demolish, the thick concrete had kept Cemetery 27’s occupants largely intact. To figure out the identity of each man, the team compared still-intact sets of teeth to the wartime dental records of known service members. Other clues like identification tags and inscribed cigarette lighters also helped. Ultimately, the History Flight team retrieved the bodies of at least 35 Marines from the cemetery’s trenches—the largest war-related recovery ever completed at a single site.
Evans, his video-recorder light on, was there to document the whole thing. Based on wartime burial records, the History Flight team had determined that Bonnyman was most likely in grave number 17. Baker, the recovery team’s leader, was very confident Bonnyman would be found exactly where the record stated. As they inched closer to the grave, Evans’s nerves threatened to get the best of him. “I had a lot of time to really think about it: ‘Wow, I can’t even believe we’re coming this close.’”
The first indication that the crew had found Bonnyman was a glint of precious metal. “My grandfather had this extremely distinctive dental work,” Evans says. “Kristen saw the gold on this tooth—a kind of unusual-looking filling. She just said, ‘It’s gold.’” As it dawned on Evans that he was actually seeing Bonnyman face-to-face, he dropped his camera, completely overcome. “I had to try to regroup.” (Evans’s book about his search, Bones of My Grandfather: Reclaiming a Lost Hero of WWII, is slated to come out in July 2018.)
After History Flight makes preliminary identifications like these, the U.S. Department of Defense (DOD) and the DPAA step in for the next phase: DNA testing to confirm each service member’s identity. Most often, the DOD contacts a known living family member to request a DNA sample. If there are maternal relatives, technicians extract the sample’s mitochondrial DNA—long-lasting genetic codes that are passed down from mother to child—and read the genetic codes using a machine called a sequencer. When a relative’s mitochondrial DNA sample matches that of a recovered service member, the DOD can state with confidence that the person belongs to a particular family line. (If necessary, other types of DNA testing, like autosomal DNA or Y-chromosome DNA analysis, can also be used to confirm service members’ identities.)
Even as the identities of the Cemetery 27 service members were being confirmed, History Flight’s excavation team was forging ahead in its search for the hundreds of Marines still missing. In the two years since the Cemetery 27 find went public, the team has found dozens of other Marines in various locations on Betio, including in garbage pits and pigsties. This year, project researchers are wrapping up their second-largest find—another cemetery, where 24 service members have so far been uncovered. Wartime documents mention other nearby burial sites that have not yet been excavated, so Pantel predicts other recoveries in the near future.
Still, as they search for the remaining service members on Betio, the researchers face ongoing logistical challenges. The Tarawa Atoll has a population density rivaling Hong Kong’s, and Betio’s landscape looks much different than it did 74 years ago. Wartime bunkers have been turned into latrines and animal pens, and residents frequently dig large pits to dispose of trash, which can disturb remaining burial sites. The research team is in a race against time to recover what they still can—especially since climate change threatens to bring on rising waters that could one day overtake the island.
To secure access to interred remains, the team often engages in delicate negotiations with residents whose thatched-roof homes line Betio’s shores. The locals have a deep appreciation for the island’s history and are often willing to assist researchers. “We always go into the communities—we meet with the mayor, the police department,” Pantel says. “We don’t want to come in invading their privacy or their cultural tranquility.” If the team needs to remove a row of plants or dismantle a fence to uncover a burial site, they either restore the site themselves or compensate the homeowners. History Flight says it’s also working with Tarawa’s cultural-heritage department to put up displays about the significance of bunkers, gun emplacements, and other battle relics that remain.
Beyond adding to the historical record, what matters most to the History Flight team is granting families the peace they have sought for generations. “Closure is more than just putting someone to rest. The unknown is very disconcerting,” Pantel says. “You have comrades, nephews, nieces, grandchildren who still don’t know what happened to their relatives. These are very real pains that families still feel.”
Family members say their loved ones’ homecoming has helped ease some of that pain. Betsy Gore Bond of Pembroke, Kentucky, found out through a surprise phone call that History Flight had located her missing uncle, Private First Class Ben Hadden Gore, in Cemetery 27. Her uncle’s return and burial, she says, brought family members closer together and strengthened their sense of their own history. “It’s like something you never even imagined happening to you.”
Evans, too, says the impact of a physical reunion can’t be overstated. In the hours after Bonnyman’s body was first uncovered on Betio, Evans was mostly able to keep it together—he didn’t feel comfortable letting his emotions out with so many people around. It wasn’t until the day after the recovery, as the sun rose over the quiet excavation grounds, that he let his guard down completely. “I went back over there by myself, and I sat down. It was between me and my grandfather, and that’s when I decided to have my moment in private.”
This post appears courtesy of Sapiens.


Although it’s impossible to say for sure, Trofim Lysenko probably killed more human beings than any individual scientist in history. Other dubious scientific achievements have cut thousands upon thousands of lives short: dynamite, poison gas, atomic bombs. But Lysenko, a Soviet biologist, condemned perhaps millions of people to starvation through bogus agricultural research—and did so without hesitation. Only guns and gunpowder, the collective product of many researchers over several centuries, can match such carnage.
Having grown up desperately poor at the turn of the 20th century, Lysenko believed wholeheartedly in the promise of the communist revolution. So when the doctrines of science and the doctrines of communism clashed, he always chose the latter—confident that biology would conform to ideology in the end. It never did. But in a twisted way, that commitment to ideology has helped salvage Lysenko’s reputation today. Because of his hostility toward the West, and his mistrust of Western science, he’s currently enjoying a revival in his homeland, where anti-American sentiment runs strong.
Lysenko vaulted to the top of the Soviet scientific heap with unusual speed. Born into a family of peasant farmers in 1898, he was illiterate until age 13, according to a recent article on his revival in Current Biology. He nevertheless took advantage of the Russian Revolution and won admission to several agricultural schools, where he began experimenting with new methods of growing peas during the long, hard Soviet winter, among other projects. Although he ran poorly designed experiments and probably faked some of his results, the research won him praise from a state-run newspaper in 1927. His hardscrabble background—people called him the “barefoot scientist”—also made him popular within the Communist party, which glorified peasants.
Officials eventually put Lysenko in charge of Soviet agriculture in the 1930s. The only problem was, he had batty scientific ideas. In particular, he loathed genetics. Although a young field, genetics advanced rapidly in the 1910s and 1920s; the first Nobel Prize for work in genetics was awarded in 1933. And especially in that era, genetics emphasized fixed traits: Plants and animals have stable characteristics, encoded as genes, which they pass down to their children. Although nominally a biologist, Lysenko considered such ideas reactionary and evil, since he saw them as reinforcing the status quo and denying all capacity for change. (He in fact denied that genes existed.)
Instead, as the journalist Jasper Becker has described in the book Hungry Ghosts, Lysenko promoted the Marxist idea that the environment alone shapes plants and animals. Put them in the proper setting and expose them to the right stimuli, he declared, and you can remake them to an almost infinite degree.
To this end, Lysenko began to “educate” Soviet crops to sprout at different times of year by soaking them in freezing water, among other practices. He then claimed that future generations of crops would remember these environmental cues and, even without being treated themselves, would inherit the beneficial traits. According to traditional genetics, this is impossible: It’s akin to cutting the tail off a cat and expecting her to give birth to tailless kittens. Lysenko, undeterred, was soon bragging about growing orange trees in Siberia, according to Hungry Ghosts. He also promised to boost crop yields nationwide and convert the empty Russian interior into vast farms.
Such claims were exactly what Soviet leaders wanted to hear. In the late 1920s and early 1930s Joseph Stalin—with Lysenko’s backing—had instituted a catastrophic scheme to “modernize” Soviet agriculture, forcing millions of people to join collective, state-run farms. Widespread crop failure and famine resulted. Stalin refused to change course, however, and ordered Lysenko to remedy the disaster with methods based on his radical new ideas. Lysenko forced farmers to plant seeds very close together, for instance, since according to his “law of the life of species,” plants from the same “class” never compete with one another. He also forbade all use of fertilizers and pesticides.
Wheat, rye, potatoes, beets—most everything grown according to Lysenko’s methods died or rotted, says Hungry Ghosts. Stalin still deserves the bulk of the blame for the famines, which killed at least 7 million people, but Lysenko’s practices prolonged and exacerbated the food shortages. (Deaths from the famines peaked around 1932 to 1933, but four years later, after a 163-fold increase in farmland cultivated using Lysenko’s methods, food production was actually lower than before.) The Soviet Union’s allies suffered under Lysenkoism, too. Communist China adopted his methods in the late 1950s and endured even bigger famines. Peasants were reduced to eating tree bark and bird droppings and the occasional family member. At least 30 million died of starvation.
Because he enjoyed Stalin’s support, Lysenko’s failures did nothing to diminish his power within the Soviet Union. His portrait hung in scientific institutes across the land, and every time he gave a speech, a brass band would play and a chorus would sing a song written in his honor.
Outside the U.S.S.R., people sang a different tune: one of unwavering criticism. A British biologist, for instance, lamented that Lysenko was “completely ignorant of the elementary principles of genetics and plant physiology ... To talk to Lysenko was like trying to explain differential calculus to a man who did not know his 12-times table.” Criticism from foreigners did not sit well with Lysenko, who loathed Western “bourgeois” scientists and denounced them as tools of imperialist oppressors. He especially detested the American-born practice of studying fruit flies, the workhorse of modern genetics. He called such geneticists “fly lovers and people haters.”
Unable to silence Western critics, Lysenko still tried to eliminate all dissent within the Soviet Union. Scientists who refused to renounce genetics found themselves at the mercy of the secret police. The lucky ones simply got dismissed from their posts and were left destitute. Hundreds if not thousands of others were rounded up and dumped into prisons or psychiatric hospitals. Several got sentenced to death as enemies of the state or, fittingly, starved in their jail cells (most notably the botanist Nikolai Vavilov). Before the 1930s, the Soviet Union had arguably the best genetics community in the world. Lysenko gutted it, and by some accounts set Russian biology back a half-century.
Lysenko’s grip on power began to weaken after Stalin died in 1953. By 1964, he’d been deposed as the dictator of Soviet biology, and he died in 1976 without regaining any influence. His portrait did continue to hang in some institutes through the Gorbachev years, but by the 1990s, the country had finally put the horror and shame of Lysenkoism behind it.
Until recently. As the new Current Biology article explains, Lysenko has enjoyed a renaissance in Russia over the past few years. Several books and papers praising his legacy have appeared, bolstered by what the article calls “a quirky coalition of Russian right-wingers, Stalinists, a few qualified scientists, and even the Orthodox Church.”
There are several reasons for this renewal. For one, the hot new field of epigenetics has made Lysenko-like ideas fashionable. Most living things have thousands of genes, but not all those genes are active at once. Some get turned on or off inside cells, or have their volumes turned up or down. The study of these changes in “gene expression” is called epigenetics. And it just so happens that environmental cues are often what turn genes on or off. In certain cases, these environmentally driven changes can even pass from parent to child—just like Lysenko claimed.
But even a cursory look at his work reveals that he didn’t predict or anticipate epigenetics in any important way. Whereas Lysenko claimed that genes didn’t exist, epigenetics take genes as a given: They’re the things being turned on or off. And while epigenetic changes can occasionally (and only occasionally) pass from parent to child, the changes always disappear after a few generations; they’re never permanent, which contradicts everything Lysenko said.
Epigenetics alone, then, can’t explain Lysenko’s revival. There’s something more going on here: a mistrust of science itself. As the Current Biology article explains, Lysenko’s new defenders “accuse the science of genetics of serving the interests of American imperialism and acting against the interests of Russia.” Science, after all, is a major component of Western culture. And because the barefoot peasant Lysenko stood up to Western science, the reasoning seems to go, he must be a true Russian hero. Indeed, nostalgia for the Soviet era and its anti-Western strongmen is common in Russia today. A 2017 poll found that 47 percent of Russians approved of Joseph Stalin’s character and “managerial skills.” And riding on the coattails of Stalin’s popularity are several of his lackeys, including Lysenko.
On the one hand, this rehabilitation is shocking. Genetics almost certainly won’t be banned in Russia again, and the rehabilitation effort remains a fringe movement overall. But fringe ideas can have dangerous consequences. This one distorts Russian history and glosses over the incredible damage Lysenko did in abusing his power to silence and kill colleagues—to say nothing of all the innocent people who starved because of his doctrines. The fact that even some “qualified scientists” are lionizing Lysenko shows just how pervasive anti-Western sentiment is in some circles: Even science is perverted to promote ideology.
On the other hand, there’s something depressingly familiar about the Lysenko affair, since ideology perverts science in the Western world as well. Nearly 40 percent of Americans believe that God created human beings in their present form, sans evolution; nearly 60 percent of Republicans attribute global temperature changes to nonhuman causes. And while there’s no real moral comparison between them, it’s hard not to hear echoes of Lysenko in Sarah Palin’s mocking of fruit-fly research in 2008. Lest liberals get too smug, several largely left-wing causes—GMO hysteria, the “blank slate” theory of human nature—sound an awful lot like Lysenko redux.
Like the Soviet Union itself, the “science” of Trofim Lysenko has been consigned to the dustbin of history. Yet the dangers of Lysenkoism—of subsuming biology to ideology—continue to lurk.



Today, you can find a huge variety of breads on supermarket shelves, only a few of which are called “sourdough.” For most of human history, though, any bread that wasn’t flat was sourdough—that is, it was leavened with a wild community of microbes. And yet we know surprisingly little about the microbes responsible for raising sourdough bread, not to mention making it more nutritious and delicious than bread made with commercial yeast. For starters, where do the fungi and bacteria in a sourdough starter come from? Are they in the water or the flour? Do they come from the baker’s hands? Or perhaps they’re just floating around in the foggy air, as the bakers of San Francisco firmly believe? This episode, Cynthia and Nicky go to Belgium with two researchers, 15 bakers, and quite a few microbes for a three-day science experiment designed to answer this question once and for all. Listen in for our exclusive scoop on the secrets of sourdough.
No one knows how or when humans first figured out that if you mixed mashed-up grains and water and let them sit for a while, you got a bubbling beige goo that you could use to make beer or raise bread. (No one even knows whether humans went for the booze or the bread first, although Nicky is firmly on Team Booze.) Historians and archaeologists speculate that the first bread would have been fluffy and spongy, something like the injera that is still eaten in Ethiopia today. Despite the microbes, it would likely have still been quite flat, because the fermented ground-grain-and-water mix would have been cooked simply by pouring it on a rock—there’s evidence that humans were grinding sorghum, an African grain, long before pottery was invented in which to bake the shaped dough into a loaf.
But once Louis Pasteur explained yeast’s role in fermentation using his new compound microscope in the 1850s, sourdough’s popularity slumped. Sourdough was finicky, unreliable, and slow; commercial baker’s yeast, which was first sold by the Fleischmann brothers just 11 years after Pasteur first published his discoveries, worked well enough, and it provided the same rapid results every time. By the 1960s, sourdough had all but been forgotten. In this episode, we trace its revival, but also reveal how science, which initially gave us boring baker's yeast, is now uncovering the secrets of sourdough.
Microbiologists Rob Dunn and Anne Madden allowed us to accompany them to Belgium for their groundbreaking experiment to discover where the microbes in a sourdough culture come from. While there, we visit the world’s first and only Sourdough Library, learn how the microbes in sourdough improve the texture, flavor, and nutritional quality of bread, and eat our body weight in baked goods. And then, the results! Why do bakers have “sourdough paws”? What is so special about Australian starters? And where do all those microbes actually come from? All that and more this episode: Listen in now!
This post appears courtesy of Gastropod.


To telescopes, ‘Oumuamua, the interstellar asteroid that made itself known to Earth in October, looks like a point of light in the dark, much like a star in the night sky—a perhaps underwhelming picture of a significant discovery.
But for astronomers, the tiny speck—the sunlight reflected by the asteroid—can reveal a trove of information. They can break down the light from an object into a spectrum of individual wavelengths, from which they can infer the object’s shape, chemical composition, and other properties. As astronomers like to say, if a picture is worth a thousand words, then a spectrum is worth a thousand pictures.
The astronomy community has spent weeks sorting through these pictures of ‘Oumuamua, captured by telescopes around the world as the asteroid sped away from the sun and faded from view. The earliest analysis of the light from ‘Oumuamua, conducted by its discoverers in Hawaii, revealed a strange, fast-spinning, cigar-like object unlike anything they’ve ever seen. The latest analyses continue to produce tantalizing results, further challenging long-standing predictions for the first visitor to our solar system.
‘Oumuamua has a thick crust of carbon-rich material, hardened by years of exposure to cosmic radiation in interstellar space, that could be protecting an icy interior, according to a new analysis in Nature Astronomy of the object in visible and near-infrared wavelengths. The coating could explain why ‘Oumuamua shows no signs of being a comet, the kind of object scientists long expected would coast into our solar system.
When the asteroid was first spotted by the Pan-STARRS telescope in Hawaii, ‘Oumuamua was already speeding away from the sun and quickly fading from view. Scientists had only about two weeks to deploy telescopes to soak up the reflected light before the interstellar visitor got too far away. They looked for a coma, a stream of evaporated particles that trails comets as they pass near the sun and their icy contents become heated. ‘Oumuamua had made a fairly close pass to the sun—about a quarter of the distance between the sun and Earth—and telescopes were prepared to spot as little as a sugar cube’s worth of material flying off the object every second.
“We had data that matched pretty closely with what we’d expect for a body out there,” said Alan Fitzsimmons, an astronomer at Queen’s University Belfast who led the new analysis. “And yet we saw no sign of ice being heated and ejected into space.”
‘Oumuamua didn’t show signatures of ice or minerals found in rock, which means it’s neither icy nor rocky, at least not exactly. But it did show signs of carbon compounds. Fitzsimmons said previous studies have revealed that when carbon-rich, comet-like objects are exposed to the radiation that would be found in interstellar space, the material forms a crust that acts as insulation. If ‘Oumuamua has ice, as a comet would, it may be hiding beneath a mantle half a meter thick, formed after hundreds of millions—perhaps even billions—of years of bombardment by high-energy particles.
Fitzsimmons and his colleagues say ‘Oumuamua’s crust may have been able to prevent heat from the sun from penetrating the surface and vaporizing ice particles. According to their thermal models, any ice buried 30 centimeters (12 inches) deep would have remained intact even as the surface of the asteroid reached temperatures of about 600 degrees Kelvin (620 degrees Fahrenheit) during its pass of the sun.
“Comets are absolutely excellent insulators,” said Karen Meech, one of ‘Oumuamua’s discoverers at the University of Hawaii, who was not involved in this new analysis.“They’re very fluffy and porous, like a down jacket.”
Scientists will likely never know for sure what lies inside ‘Oumuamua. When astronomers study the light reflected from the asteroid, they’re only examining the top few microns of its surface, a width smaller than that of a human red blood cell. “We can infer only so much about these little dots of light,” said Michele Bannister, an astronomer at Queen’s University Belfast and a coauthor on the new study.
Astronomers also have a finite amount of data to work with. Most of their ground-based telescope observations were carried out in the two weeks after ‘Oumuamua was discovered, when it was still visible enough to get good measurements. The majority of research on ‘Oumuamua—the attempts to explain its weird shape, its nonstop spinning, its unexpected hardiness—will come out of this data set.
The rest may come from yet-to-be published observations by space telescopes like the Hubble, which has tracked ‘Oumuamua to help astronomers better understand its trajectory and where it came from. The asteroid is now about twice the distance between the Earth and the sun from our planet. Even to Hubble, the producer of countless, radiant images of distant stars and galaxies, ‘Oumuamua, a small object about 400 meters long, will look like a small point of light.
“It could be icy inside, and we’ll never know,” Meech said.


Forty years ago, the great tropical ecologist Dan Janzen noticed something funny about the plants in Costa Rica. Many species invested a lot of their energy in producing huge fruits with tough seeds and seed pods, which no animals seemed to eat. With nothing to consume them, the seeds and pods just fell to the jungle floor and rotted, or died in the shade of their parent trees.
In a now legendary paper cowritten with Paul S. Martin (“Neotropical Anachronisms: The Fruits the Gomphotheres Ate”), Janzen speculated that there was a good reason for this: The jungle plants’ original partners had all gone extinct. They identified a long list of plants, such as the jicaro and guanacaste in South America and honey locust, pawpaw, persimmon, and Osage orange in North America, which seem to have lost their original dispersal agents. These species had evolved over millions of years to have their seeds eaten and spread around by ground sloths, glyptodonts, gomphotheres, (a family of mastodon-like creatures from South America), extinct horses, and other vanished megafauna. The great extinctions at the end of the Pleistocene left these plants as orphans.
They’re not alone. In many ways, we all live in an orphaned world. Extinct megafauna like the mammoth and ground sloth weren’t just hapless prey or passive victims of climate change. They were engineers, actively shaping their environment to suit their needs. In doing so, they created evolutionary niches in which plants and predators could thrive. It’s these patterns of coevolution and mutual dependence that create a functioning ecosystem.
Since the Quaternary extinction event in which the world lost some 50 percent of its large mammal species, many crucial links in the food chain have gone missing. Figuring out exactly what this has done to the world we’re living in now is something paleoecologists have been trying to figure out for a good half-century. Like anything to do with ecology, the problem of how missing megafauna affect modern-day habitats is a complex puzzle, with multiple lines of evidence. First you have to figure out who ate what and where, in what season and in what quantity. And if you want to do that, the best place to look is ancient poop.
In the past few years, a group of researchers in New Zealand led by the paleoecologist Jamie Wood have succeeded in using ancient droppings to reconstruct the world of the giant moa, one of a group of large flightless birds that includes the ostrich, emu, cassowary, and Madagascar’s extinct elephant birds. From the fossil record, we know there were at least nine species of moa on New Zealand. The smallest, the little bush moa, stood a little over four feet tall. At 12 feet with its neck outstretched, the largest, the giant moa, may have been the tallest bird that ever lived. In between these two extremes, moas came in a range of sizes and forms, adapted for a range of habitats.
Before the arrival of human beings, New Zealand was a paradise of birds. When it started drifting away from its parent supercontinent of Gondwana some 85 million years ago, it only carried with it a few primitive mammals. In time, they went extinct, as did the dinosaurs. With 2,000 miles between it and the next nearest landmass, the only group that could repopulate New Zealand were the birds. Over millions of years, they evolved to occupy most of the available ecological niches (they were joined later by bats, who took the place mice occupy in most terrestrial ecosystems). Birds functioned as the only major predators and herbivores. They were New Zealand’s antelopes, cheetahs, and giraffes.
New Zealand’s plants evolved in concert with its birds. An unusual number of the islands’ bushes and trees have what’s called a divaricating pattern of growth. The branches of these plants grow at wildly offset angles, creating an impenetrable mesh of interwoven twigs. Growing this way costs a lot of energy—the plants lose precious sunlight by creating their own shade—but it makes for an effective defense against a large, toothless herbivore like the moa. But now, with the moa gone and replaced by mammalian herbivores (mostly sheep), they find themselves defenseless.
New Zealand is a perfect place to study the effects of megafauna on their landscape. For one, the megafauna stayed around until very recently. Giant moas were happily foraging for tree-fern buds while the Magna Carta was being signed and the Florentines were building Brunelleschi’s dome. Two, it was quite varied: Nine species of moa coexisted on the islands, each (presumably) with its own habits and ecological niche. And three, all those moas left a lot of poop. People have been finding it in caves since the 1870s. Natural-history museums have over 2000 specimens on file, from 30 localities, with more waiting to be discovered.
Woods’s lab used some of this abundant resource to settle a few mysteries about New Zealand’s lost ecosystems. Using a combination of ancient DNA, plant macrofossils, and pollen, his team reconstructed the diet of four moa species. They found that the heavy-footed moa preferred to graze in open fields and grasslands. The bush moa preferred to munch on forest understory. The giant moa was more of a generalist, moving between these two habitats at will.
Figuring out moa diet is only the tip of the dungheap when it comes to ancient poop studies. Prehistoric dung has a wealth of uses for science, from tracking the demise of the mammoth to deciphering the peopling of the Americas. The spores of Sporormiella, a type of mold that loves nothing better than a nice pat of dung to grow over, have been used as a proxy to track the abundance of megafauna across the millennia. Carbon dating DNA-fingerprinted coprolites from the Paisley Caves in Oregon helped prove the presence of pre-Clovis humans (and, as a bonus, testing feces for DNA doesn’t raise the same ethical quandaries as testing ancient skeletal remains).
The climate history of the American Southwest was established in large part thanks to a deposit of sloth dung discovered in Arizona in the 1950s. The dung was left by the Shasta ground sloth, a small species as ground sloths go, more bearlike than the more famous Megatherium, which grew to the size of an elephant. For around 30,000 years, these sloths used Rampart Cave, a hollow in the side of the Grand Canyon, as their latrine. Pollen in the accumulated droppings recorded the shifts in vegetation that accompanied the arrival and departure of past glacial maxima. Crucially, they proved that the shift in vegetation and temperature that came with the end of the last Ice Age wasn’t particularly new or extreme. It was something that had happened multiple times in previous millennia.
Tragically, the precious dung deposit caught fire in 1976. The National Park Service spent tens of thousands of dollars to save it, but to no avail. It made the nightly news. Walter Cronkite joked about “endangered feces,” but to Paul Martin, a geoscientist who devoted much of his career to reconstructing the environment of the ancient Southwest (and came up with the overkill hypothesis in the process) compared it to losing the Library of Alexandria.
Martin felt better a few years later, when two zoologists discovered an unusually big sphere of chewed-up grass in a cave in southern Utah. Subsequent visits turned up several more dung balls of surprising size, which radiocarbon dating showed to be about 12,000 years old. Martin guessed they came from America’s second-largest extinct mammal, the Columbian mammoth.
Larger than any living elephant, the Columbian mammoth, was, like them, a prodigious maker of dung. The Utah cave, named Bechan, from the Navajo word for “big shit,” showed just how prodigious. Excavations in the mid-’80s revealed a layer of dung 16 inches thick covering a surface of several tennis courts (that’s 14,000 cubic feet of dung total). This July, a team at McMaster University in Ontario reported that it had successfully sequenced DNA from the dung boluses, proving that they really did come from mammoths.
The most spectacular dung find of recent years comes from the Page-Ladson site on the Aucilla River in the Florida Panhandle. The site is a sinkhole in the middle of the Aucilla River, on one of the few stretches where it flows above ground, but it used to be a spring-fed pond. Fourteen thousand years ago, when Florida was much cooler and drier than it is today (and also much larger, thanks to lower sea levels), various animals would come to this pond to drink. That made it a great spot for visiting predators, including humans. Underwater excavations at Page-Ladson turned up clear signs of human activity, including mastodon butchery, making it the oldest confirmed habitation in the American Southeast.
But the Aucilla mastodons weren’t just sitting ducks for human hunters. Mostly, they used the pool as a wallow, the way elephants use drinking holes in Africa today. In the process, they left behind a heck of a lot of dung. At some point, the water level rose, burying the wallow in sediment, and preserving this priceless fecal Pompeii for posterity.
Archaeologists have found hundreds of cubic meters of mastodon droppings in this ancient latrine. Since the early 1990s, they’ve been patiently sifting through the only partially digested remains. From it, they’ve been able to piece together a picture of the mastodon in a vanished landscape. The main thing they found was cypress twigs—the mastodons ate vast quantities of cypress, particularly young branches, and particularly in autumn. They also found traces of dozens of other plant species, including some of Janzen’s orphans, such as honey locust, persimmon, Osage orange, and wild gourd. The wild gourd is especially interesting. These days, wild gourds are very rare in the wild. Their extremely bitter rind keeps most living herbivores from snacking on their fruit. It took thousands of years of patient cultivation by Native Americans to turn them into today’s pumpkin and squash.
So next time you have pumpkin pie at Thanksgiving, spare a thought for the wild Cucurbita, the mastodon, which long ago spread its seeds, and all the other ghosts that live on in our orphaned land. And once the meal is over, tell your family about all the amazing things you could learn by stepping back in time to stand in front of a mastodon’s butt.


Updated on December 18 at 3:51 p.m. ET
A few years ago, Michelle Hebl attended the latest in a series of talks hosted by her department at Rice Univeristy. The speaker was a man, and Hebl realized that she hadn’t heard any female speakers in that series for a while. “Maybe I’m just not thinking about them,” she thought. “Or maybe it’s something we should look at.”*
Colloquium talks, where academics are invited to discuss their research, give speakers a chance to publicize their work, build collaborations with new colleagues, and boost their reputations. The talks can lead to promotions or job offers. They are big opportunities. But as Hebl’s student Christine Nittrouer eventually found, they are opportunities that are predominantly extended to men.
Nittrouer and her team scanned the websites of the top 50 U.S. universities, as ranked by U.S. News, to build a database of every colloquium speaker from six departments: biology, bioengineering, political science, history, psychology, and sociology. They chose those six to represent a breadth of disciplines, and to exclude departments with either a very low or very high proportion of women. And they found that men gave more than twice as many talks as women: 69 percent versus 31 percent.
That result should not be too surprising. Several studies have shown that men outnumber women among the speakers of several scientific conferences. There’s even a site that collates examples of all-male panels.
Why does this happen? Hebl accounted for several of what she calls “yeah-but explanations,” which underplay these figures as the result of anything other than discriminatory biases. For example, some might argue that men outnumber women in many fields, and so any equitable selection process would naturally lead to more male speakers. But the team estimated the full pool of available speakers by counting every professor in their six chosen fields at each of the top 100 U.S. universities. And even after adjusting for the relative numbers of men and women in the various fields or ranks, they found that men are still 20 percent more likely to be invited to give colloquium talks than women.
Skeptics might also argue that the problem is a generational one: Science, for instance, has historically been skewed toward men, and when colloquia committees decide whom to invite, they’re prisoners of that history. But if that were true, and the arc of academia was slowly bending toward equality, then when assistant and associate professors—who are younger and more junior than full professors—are selected to give talks, the gender difference should be narrower. Hebl’s team found no such trend. “The people in whom we should see more parity aren’t showing us more parity,” she says.
“People sometime say: You know what? Maybe it’s the women,” says Hebl. “Maybe they don’t want to give talks, or they’re declining because they’re staying home with their kids.” That’s not what she found when she surveyed 186 professors who didn’t give colloquium talk at prestigious universities, but were in the same departments as those who did. Their answers clearly showed that women don’t decline colloquium invitations more than men, that they feel just as strongly that these talks are important for their careers, and that they’re no more likely to decline such talks because of family obligations.
“This dispels the widely held myth that women are less frequent speakers because they travel less,” says Jo Handelsman, from the University of Wisconsin at Madison. “Clearly, we need to test such assumptions before we absolve ourselves of culpability in creating biased slates.”
“Despite their presence in departments, women are not being asked to contribute to the intellectual development of their fields in the most coveted ways,” says Robin Nelson, from Santa Clara University, who has studied the prevalence of harassment in science. “This gendered discrimination minimizes women’s visible contributions to their fields, validating the idea that the greatest intellectual contributions are made by a few brilliant men.”
“We can account for all the yeah-buts,” Hebl says, “but we still have this bias, and we need to do something about it.”
One solution is to give women more power over inviting colloquium speakers. The team found that when those committees are chaired by women, half of the invited speakers are women; that’s compared to just 30 percent when the committees are chaired by men. “I’m not sure if these are explicit bias, where male chairs are saying we don’t want women,” Hebl says. “It’s more about the people who they think about, who are in their networks. And maybe women just know other women in the field.”
But in male-dominated fields, “if we take the few women we have and we put them on all the committees, we’re overwhelming them with experiences that aren’t necessarily helpful to their own progress,” Hebl says. Ultimately, “the burden falls on male allies. We need to train men to be aware of these biases. And we should put women on the important committees—the ones that decides who’s going to be our new board member, not the one that decides where we have our holiday party.”
Yael Niv, from Princeton, who created a site to monitor gender biases at neuroscience conferences, says that her department solved the problem of all-male colloquia by requiring faculty to nominate a certain base rate of women as possible speakers. “In this way, we encourage everyone to think a bit deeper about their nominations, and cast a slightly wider net,” she says. “The results have been colloquia with equal numbers of men and women in recent years, and a wider, more interesting array of scientific ideas that we’re exposed to.” And even if committee members struggle to think of female speakers, several scientists have now compiled lists of possible names in microbiology, astronomy, physics, evolution, political science, neuroscience, and more.
“While finding more women gatekeepers may help get more women colloquium speakers, will it actually solve the whole problem?” asks Kelly Ramirez from Colorado State University, cofounder of 500 Women Scientists. “Probably not. There are so many things that can make science a toxic or difficult environment for women.” When seeking jobs, women are viewed as being less competent than identical male applicants. They’re offered lower salaries and fewer opportunities for mentorship, and they’re given shorter letters of recommendation with more hedging words. When teaching, they’re rated more negatively. When simply trying to work, they face high levels of sexual harassment and assault.
The solution, Ramirez says, is to “build a strong and large network so whatever the challenge, women scientists will have a network to turn to. And we need to speak up, set examples, and hold people and institutions accountable.”
All of these barriers are particularly profound for women of color, who face a double whammy of discrimination because of both their race and gender. It’s telling that Hebl’s team wanted to look at whether ethnicity deepens the gender disparity among colloquium speakers, but with the universities they looked at, they couldn’t find enough professors of color to get a statistically strong sample.
* This article originally attributed quotes from Michelle Hebl to Christine Nittrouer. We regret the error.


Over the past half-century, climate scientists have learned that the weather leaves behind a hidden history of itself. Through evidence preserved in tree rings, in the gunk at the bottom of lakes, and in towering stalagmites that rise from cave floors, researchers have learned how to read thousands of years of weather history, inferring the existence of long-forgotten rainstorms, hurricanes, and mega-droughts.
Recent research suggests that there may be a similar account hidden in popular music, too. Thursday, at the annual meeting of the American Geophysical Union, the meteorologist Paul Williams presented evidence that a spate of intense hurricanes imprinted themselves on the American songbook.
“In the same way that we have a climatological record of temperature from ice cores, it seems there’s also a kind of climate record in music as well,” said Williams, a professor of atmospheric science at the University of Reading.
His and his colleagues’ main finding turns on two spans of hurricane activity. In the 1950s and 1960s, many large and damaging hurricanes made landfall in the United States, including Hurricanes Dora, Donna, and Camille. Throughout the 1970s and 1980s, there were many fewer high-profile hurricanes, especially in the United States.
Drawing on a karaoke database, Williams and his colleagues found more than 750 pop songs with weather-related lyrics from those four decades.
When times were stormy in the real world, the weather in pop music was darker, too. Almost three-quarters of the weather-themed songs from the 1950s and 1960s had lyrics that emphasized storminess, with frequent use of words like rain, wind, and hurricane. But during the next two decades, only 46 percent of the weather-related songs featured stormy themes. The difference is pronounced enough to be statistically significant.
“That climatological difference over that four-decade period was represented in the songs that were being written,” said Williams. “It does seem to be the case that songwriters are writing about the weather that they’re experiencing on the day they write the song.”
That seems much too neat to me—Paul Jabara and Paul Shaffer didn’t compose “It’s Raining Men” when a cold-front squall line precipitating adult human males gusted into lower Manhattan—but it does point to how the cultural prominence of different weather patterns changes across time periods. Songwriters are just as likely to turn to weather metaphors that are in the news or the cultural conversation as they are to write about what they’re experiencing.
This kind of artistic discovery has informed climate science in the past. From about 1600 to 1850, a series of cold periods afflicted much of the world, including Western Europe. One of the worst winters occurred in 1665—the same year that a spate of winter scenes popped up across European art, including Bruegel the Elder’s The Hunters in the Snow.  Indeed, a kind of “Little Ice Age” is captured in thousands of European paintings from the period.
Williams and his colleagues also conducted a very small unscientific survey of a smattering of classical works from across the years, and from around Europe, that feature explicit depictions of the weather. He found that British composers were much more likely to focus on the weather than composers of any other nationality. “The authors of the study are British, but I don’t think there’s a bias in the study from that,” he said. “I think it’s fitting the national stereotype of British people as being obsessed by the weather.”
Often, he found, composers used stormy weather as a musical stand-in for “emotional turbulence,” he said. In his better-known work, Williams is something of an expert on turbulence: Earlier this year, he published a study finding that climate change will increase the amount of turbulence experienced by air travelers.
But the feedback can work the other way too. Many classical musicians turn to climate science—and sometimes even climate data—as a direct inspiration for their work. Matthew Burtner, an Alaska-born composer who teaches music at the University of Virginia, also presented at the American Geophysical Union, describing his music written about—and sometimes with—glaciers.
Burtner is an “eco-acoustician,” meaning he brings both the sounds and the data signatures of the environment into his work. Sometimes, he’ll take climate data and convert it into musical sound, a process known as “sonification.” (Think of it as the aural equivalent of data visualization.)
His work also includes the sound of nature itself: Burtner has traveled to Matanuska Glacier, a 27-mile-long ice run near Anchorage, Alaska, to bring the object into his composition. In 2013, he scattered 19 microphones across the glacier: Some mics sat on top of the ice, listening for the wind; others were lowered into openings into its interior; and a few sat beneath the glacier’s melt pools, catching the drip-drip-drip of its lost mass.
The goal, Burthner says, is to create “a single audio representation of the glacier and bring that into the studio, the gallery, the concert hall.”
Burtner isn’t the only musician who has worked on environmental themes—or with environmental data specifically. He told me that Iannis Xenakis, the Greek-French composer, influenced his work; Xenakis once wrote a piece that represented certain statistical qualities of gases. But other composers—including Pauline Oliveros, R. Murray Schafer, and John Luther Adams—have written music about the environment, sometimes using environmental recordings.
Burtner’s work—and Williams’s discovery of the climatological pop record—suggests that music and science about the environment may share some deep connections. Both rely on qualitative experience quantified; both require patience and exhaustive focus. Recently, the literary critic Amitav Ghosh wondered why so few novelists today are writing fiction about climate change: “Why does climate change cast a much smaller shadow on literature than it does on the world?” Perhaps we can answer: People are writing music about it instead.


Can training the mind make us more attentive, altruistic, and serene? Can we learn to manage our disturbing emotions in an optimal way? What are the transformations that occur in the brain when we practice meditation? In a new book titled Beyond the Self, two friends—Matthieu Ricard, who left a career as a molecular biologist to become a Buddhist monk in Nepal, and Wolf Singer, a distinguished neuroscientist—engage in an unusually well-matched conversation about meditation and the brain. Below is a condensed and edited excerpt.
Matthieu Ricard: Although one ﬁnds in the Buddhist literature many treatises on “traditional sciences”—medicine, cosmology, botanic, logic, and so on—Tibetan Buddhism has not endeavored to the same extent as Western civilizations to expand its knowledge of the world through the natural sciences. Rather it has pursued an exhaustive investigation of the mind for 2,500 years and has accumulated, in an empirical way, a wealth of experiential ﬁndings over the centuries. A great number of people have dedicated their whole lives to this contemplative science.
Modern Western psychology began with William James just over a century ago. I can’t help remembering the remark made by Stephen Kosslyn, then chair of the psychology department at Harvard, at the Mind and Life meeting on “Investigating the Mind,” which took place at MIT in 2003. He started his presentation by saying, “I want to begin with a declaration of humility in the face of the sheer amount of data that the contemplatives are bringing to modern psychology.”
It does not sufﬁce to ponder how the human psyche works and elaborate complex theories about it, as, for instance, Freud did. Such intellectual constructs cannot replace two millennia of direct investigation of the workings of mind through penetrating introspection conducted with trained minds that have become both stable and clear.
Wolf Singer: Can you be more speciﬁc with this rather bold claim? Why should what nature gave us be fundamentally negative, requiring special mental practice for its elimination, and why should this approach be superior to conventional education or, if conﬂicts arise, to psychotherapy in its various forms, including psychoanalysis?
Ricard: What nature gave us is by no means entirely negative; it is just a baseline. Few people would honestly argue that there is nothing worth improving about the way they live and the way they experience the world. Some people regard their own particular weaknesses and conﬂicting emotions as a valuable and distinct part of their “personality,” as something that contributes to the fullness of their lives. They believe that this is what makes them unique and argue that they should accept themselves as they are. But isn’t this an easy way to giving up on the idea of improving the quality of their lives, which would cost only some reasoning and effort?
Modern conventional education does not focus on transforming the mind and cultivating basic human qualities such as lovingkindness and mindfulness. As we will see later, Buddhist contemplative science has many things in common with cognitive therapies, in particular with those using mindfulness as a foundation for remedying mental imbalance. As for psychoanalysis, it seems to encourage rumination and explore endlessly the details and intricacies of the clouds of mental confusion and self-centeredness that mask the most fundamental aspect of mind: luminous awareness.
Singer: So rumination would be the opposite of what you do during meditation?
Ricard: Totally opposite. It is also well known that constant rumination is one of the main symptoms of depression. What we need is to gain freedom from the mental chain reactions that rumination endlessly perpetuates. One should learn to let thoughts arise and be freed to go as soon as they arise, instead of letting them invade one’s mind. In the freshness of the present moment, the past is gone, the future is not yet born, and if one remains in pure mindfulness and freedom, potentially disturbing thoughts arise and go without leaving a trace.
Singer: What you have to learn then is to adopt a much more subtle approach to your internal emotional theater, to learn to identify with much higher resolution the various connotations of your feelings.
Ricard: That’s right. In the beginning, it is difﬁcult to do it as soon as an emotion arises, but if you become increasingly familiar with such an approach, it becomes quite natural. Whenever anger is just showing its face, we recognize it right away and deal with it before it becomes too strong.
Singer: It is not unlike a scientiﬁc endeavor except that the analytical effort is directed toward the inner rather than the outer world. Science also attempts to understand reality by increasing the resolving power of instruments, training the mind to grasp complex relations, and decomposing systems into ever-smaller components.
Ricard: It is said in the Buddhist teachings that there is no task so difﬁcult that it cannot be broken down into a series of small, easy tasks.
Singer: Your object of inquiry appears to be the mental apparatus and your analytical tool, introspection. This is an interesting self-referential approach that differs from the Western science of mind because it emphasizes the ﬁrst-person perspective and collapses, in a sense, the instrument of investigation with its object. The Western approach, while using the ﬁrst-person perspective for the deﬁnition of mental phenomena, clearly favors the third-person perspective for its investigation.
I am curious to ﬁnd out whether the results of analytical introspection match those obtained by cognitive neuroscience. Both approaches obviously try to develop a differentiated and realistic view of cognitive processes.
What guarantees that the introspective technique for the dissection of mental phenomena is reliable? If it is the consensus among those who consider themselves experts, how can you compare and validate subjective mental states? There is nothing another person can look at and judge as valid; the observers can only rely on the verbal testimony of subjective states.
Ricard: It is the same with scientiﬁc knowledge. You ﬁrst have to rely on the credible testimony of a number of scientists, but later you can train in the subject and verify the ﬁndings ﬁrsthand. This is quite similar to contemplative science. You ﬁrst need to reﬁne the telescope of your mind and the methods of investigations for years to ﬁnd out for yourself what other contemplatives have found and all agreed on. The state of pure consciousness without content, which might seem puzzling at ﬁrst sight, is something that all contemplatives have experienced. So it is not just some sort of Buddhist dogmatic theory. Anyone who takes the trouble to stabilize and clarify his or her mind will be able to experience it.
Regarding cross-checking interpersonal experience, both contemplatives and the texts dealing with the various experiences a meditator might encounter are quite precise in their descriptions. When a student reports on his inner states of mind to an experienced meditation master, the descriptions are not just vague and poetic. The master will ask precise questions and the student replies, and it is quite clear that they are speaking about something that is well deﬁned and mutually understood.
However, in the end, what really matters is the way the person gradually changes. If, over months and years, someone becomes less impatient, less prone to anger, and less torn apart by hopes and fears, then the method he or she has been using is a valid one.
An ongoing study seems to indicate that while they are engaged in meditation, practitioners can clearly distinguish, like everyone who is not distracted, between pleasant and aversive stimuli, but they react much less emotionally than control subjects. While retaining the capacity of being fully aware of something, they succeed in not being carried away by their emotional responses.
Singer: How do you do this? What are the tools?
Ricard: This process requires perseverance. You need to train again and again. You can’t learn to play tennis by holding a racket for a few minutes every few months. With meditation, the effort is aimed at developing not a physical skill but an inner enrichment.
In extreme cases, you could be in a simple hermitage in which nothing changes or sitting alone always facing the same scene day after day. So the outer enrichment is almost nil, but the inner enrichment is maximal. You are training your mind all day long with little outer stimulation. Furthermore, such enrichment is not passive, but voluntary, and methodically directed. When you engage for eight or more hours a day in cultivating certain mental states that you have decided to cultivate and that you have learned to cultivate, you reprogram the brain.
Singer: In a sense, you make your brain the object of a sophisticated cognitive process that is turned inward rather than outward toward the world around you. You apply the cognitive abilities of the brain to studying its own organization and functioning, and you do so in an intentional and focused way, similar to when you attend to events in the outer world and when you organize sensory signals into coherent percepts. You assign value to certain states and you try to increase their prevalence, which probably goes along with a change in synaptic connectivity in much the same way as it occurs with learning processes resulting from interactions with the outer world.
Let us perhaps brieﬂy recapitulate how the human brain adapts to the environment because this developmental process can also be seen as a modiﬁcation or reprogramming of brain functions. Brain development is characterized by a massive proliferation of connections and is paralleled by a shaping process through which the connections being formed are either stabilized or deleted according to functional criteria, using experience and interaction with the environment as the validation criterion. This developmental reorganization continues until the age of about 20. The early stages serve the adjustment of sensory and motor functions, and the later phases primarily involve brain systems responsible for social abilities. Once these developmental processes come to an end, the connectivity of the brain becomes ﬁxed, and large-scale modiﬁcations are no longer possible.
Ricard: To some extent.
Singer: To some extent, yes. The existing synaptic connections remain modiﬁable, but you can’t grow new long-range connections. In a few distinct regions of the brain, such as the hippocampus and olfactory bulb, new neurons are generated throughout life and inserted into the existing circuits, but this process is not large-scale, at least not in the neocortex, where higher cognitive functions are supposed to be realized.
Ricard: A study of people who have practiced meditation for a long time demonstrates that structural connectivity among the different areas of the brain is higher in meditators than in a control group. Hence, there must be another kind of change allowed by the brain.
Singer: I have no difﬁculty in accepting that a learning process can change behavioral dispositions, even in adults. There is ample evidence of this from reeducation programs, where practice leads to small but incremental behavior modiﬁcations. There is also evidence for quite dramatic and sudden changes in cognition, emotional states, and coping strategies. In this case, the same mechanisms that support learning—distributed changes in the efﬁciency of synaptic connections—lead to drastic alterations of global brain states.
Ricard: You could also change the ﬂow of neuron activity, as when the trafﬁc on a road increases signiﬁcantly.
Singer: Yes. What changes with learning and training in the adult is the ﬂow of activity. The ﬁxed hardware of anatomical connections is rather stable after age 20, but it is still possible to route activity ﬂexibly from A to B or from A to C by adding certain signatures to the activity that ensure that a given activation pattern is not broadcast in a diffuse way to all connected brain regions but sent only to selected target areas.
Ricard: So far, the results of the studies conducted with trained meditators indicate that they have the faculty to generate clean, powerful, well-deﬁned states of mind, and this faculty is associated with some speciﬁc brain patterns. Mental training enables one to generate those states at will and to modulate their intensity, even when confronted with disturbing circumstances, such as strong positive or negative emotional stimuli. Thus, one acquires the faculty to maintain an overall emotional balance that favors inner strength and peace.
Singer: So you have to use your cognitive abilities to identify more clearly and delineate more sharply the various emotional states, and to train your control systems, probably located in the frontal lobe, to increase or decrease selectively the activity of subsystems responsible for the generation of the various emotions.
An analogy for this process of reﬁnement could be the improved differentiation of objects of perception, which is known to depend on learning. With just a little experience, you are able to recognize an animal as a dog. With more experience, you can sharpen your eye and become able to distinguish with greater and greater precision dogs that look similar. Likewise, mental training might allow you to sharpen your inner eye for the distinction of emotional states.
In the naïve state, you are able to distinguish good and bad feelings only in a global way. With practice, these distinctions would become increasingly reﬁned until you could distinguish more and more nuances. The taxonomy of mental states should thus become more differentiated. If this is the case, then cultures exploiting mental training as a source of knowledge should have a richer vocabulary for mental states than cultures that are more interested in investigating phenomena of the outer world.
Ricard: Buddhist taxonomy describes 58 main mental events and various subdivisions thereof. It is quite true that by conducting an in-depth investigation of mental events, one becomes able to distinguish increasingly more subtle nuances.
Take anger, for instance. Often anger can have a malevolent component, but it can also be rightful indignation in the face of injustice. Anger can be a reaction that allows us to rapidly overcome an obstacle preventing us from achieving something worthwhile or remove an obstacle threatening us. However, it could also reﬂect a tendency to be short-tempered. If you look carefully at anger, you will see that it contains aspects of clarity, focus, and effectiveness that are not harmful in and of themselves. So if you are able to recognize those aspects that are not yet negative and let your mind remain in them, without drifting into the destructive aspects, then you will not be troubled and confused by these emotions.
Another result of cultivating mental skills is that, after a while, you will no longer need to apply contrived efforts. You can deal with the arising of mental perturbations like the eagles I see from the window of my hermitage in the Himalayas. The crows often attack them, even though they are much smaller. They dive at the eagles from above trying to hit them with their beaks. However, instead of getting alarmed and moving around to avoid the crow, the eagle simply retracts one wing at the last moment, letting the diving crow pass by, and extends its wing back out. The whole thing requires minimal effort and is perfectly efﬁcient. Being experienced in dealing with the sudden arising of emotions in the mind works in a similar way. When you are able to preserve a clear state of awareness, you see thoughts arise; you let them pass through your mind, without trying to block or encourage them; and they vanish without creating many waves.
Singer: That reminds me of what we do when we encounter severe difﬁculties that require fast solutions, such as a complicated trafﬁc situation. We immediately call on a large repertoire of escape strategies that we have learned and practiced, and then we choose among them without much reasoning, relying mainly on subconscious heuristics. Apparently, if we are not experienced with contemplative practice, we haven’t gone through the driving school for the management of emotional conﬂicts. Would you say this is a valid analogy?
Ricard: Yes, complex situations become greatly simpliﬁed through training and the cultivation of effortless awareness. When you learn to ride a horse, as a beginner you are constantly preoccupied, trying not to fall at every movement the horse makes. Especially when the horse starts galloping, it puts you on high alert. But when you become an expert rider, everything becomes easier. Riders in eastern Tibet, for instance, can do all kinds of acrobatics, such as shooting arrows at a target or catching something on the ground while galloping at full speed, and they do all that with ease and a big smile on their face.
One study with meditators showed that they can maintain their attention at an optimal level for extended periods of time. When performing what is called a continuous performance task, even after 45 minutes, they did not become tense and were not distracted even for a moment. When I did this task myself, I noticed that the ﬁrst few minutes were challenging and required some effort, but once I entered a state of “attentional ﬂow,” it became easier.
Singer: This resembles a general strategy that the brain applies when acquiring new skills. In the naïve state, one uses conscious control to perform a task. The task is broken down into a series of subtasks that are sequentially executed. This requires attention, takes time, and is effortful. Later, after practice, the performance becomes automatized. Usually, the execution of the skilled behavior is then accomplished by different brain structures than those involved in the initial learning and execution of the task. Once this shift has occurred, performance becomes automatic, fast, and effortless and no longer requires cognitive control. This type of learning is called procedural learning and requires practice. Such automatized skills often save you in difﬁcult situations because you can access them quickly. They can also often cope with more variables simultaneously due to parallel processing. Conscious processing is more serialized and therefore takes more time.
Do you think you can apply the same learning strategy to your emotions by learning to pay attention to them, differentiate them, and thereby familiarize yourself with their dynamics so as to later become able to rely on automatized routines for their management in case of conﬂict?
Ricard: You seem to be describing the meditation process. In the teachings, it says that when one begins to meditate, on compassion, for instance, one experiences a contrived, artiﬁcial form of compassion. However, by generating compassion over and over again, it becomes second nature and spontaneously arises, even in the midst of a complex and challenging situation.
Singer: It would be really interesting to look with neurobiological tools at whether you have the same shift of function that you observe in other cases where familiarization through learning and training leads to the automation of processes. In brain scans, one observes that different brain structures take over when skills that are initially acquired under the control of consciousness become automatic.
Ricard: That is what a study conducted by Julie Brefczynski and Antoine Lutz at Richard Davidson’s lab seems to indicate. Brefczynski and Lutz studied the brain activity of novice, relatively experienced, and very experienced meditators when they engage in focused attention. Different patterns of activity were observed depending on the practitioners’ level of experience.
Relatively experienced meditators (with an average of 19,000 hours of practice) showed more activity in attention-related brain regions compared with novices. Paradoxically, the most experienced meditators (with an average of 44,000 hours of practice) demonstrated less activation than the ones without as much experience. These highly advanced meditators appear to acquire a level of skill that enables them to achieve a focused state of mind with less effort. These effects resemble the skill of expert musicians and athletes capable of immersing themselves in the “ﬂow” of their performances with a minimal sense of effortful control. This observation accords with other studies demonstrating that when someone has mastered a task, the cerebral structures put into play during the execution of this task are generally less active than they were when the brain was still in the learning phase.
Singer: This suggests that the neuronal codes become sparser, perhaps involving fewer but more specialized neurons, once skills become highly familiar and are executed with great expertise. To become a real expert seems to require then at least as much training as is required to become a world-class violin or piano player. With four hours of practice a day, it would take you 30 years of daily meditation to attain 44,000 hours. Remarkable!

This article has been adapted from Matthieu Ricard and Wolf Singer’s book, Beyond the Self: Conversations Between Buddhism and Neuroscience.


Sex at the zoo is a highly managed affair.
When zookeepers do not want a species to reproduce, birth control is in order. “Chimps take human birth-control pills, giraffes are served hormones in their feed, and grizzly bears have slow-releasing hormones implanted in their forelegs,” writes The New York Times. When zookeepers do want a species to reproduce—especially an endangered or threatened one—the couplings must be carefully arranged. An animal might travel 1,500 miles to meet a partner.
But after all this meticulous planning, zookeepers can hit a wall of uncertainty: It’s sometimes quite hard to know whether a female is pregnant. In the case of pandas, their keepers might not be entirely certain until the baby pops out.
There are animals where it’s easier, sure. Great apes, for example, are related enough to humans that regular old pregnancy tests can work. The problem is getting individual apes to pee on a stick. To get around this, the St. Louis Zoo built special gutters where the great apes slept, which would route the urine outside. In the morning, someone would go out to collect the urine. “But the female has to be alone to do that,” says Cheryl Asa, a former director of reproductive research at the zoo. The system worked well for great-ape species like orangutans, which are more solitary, but not so well for chimps or gorillas, which prefer to sleep in groups and inevitably pee together.
Poop is easier to differentiate by individual. In some cases, says Asa, you can feed different animals food studded with beads of different colors. Or you can just watch as they go. Since mating animals are intensely surveilled anyway, staff members take note when females poop. “If they see a female defecate, they map it,” Asa says. A keeper later uses the map to retrieve the samples. The feces are tested for levels of the hormone progesterone, which rises with pregnancy.
Animals closely related to domestic pets also benefit from veterinary medicine. Wolf pregnancies, for example, can be tested using commercial dog pregnancy kits that detect a hormone called relaxin. The kits require a little bit of blood, which is easy enough to draw from dogs, and actually isn’t that difficult to get from wolves, either. “We all think of wolves as being really fierce, and they can be,” says Asa, “but most of the time when they’re cornered, they’ll just give up and you can hold them down and pull a leg free to get a quick blood sample.”
Some species, however, go through a strange phase called a pseudopregnancy, which looks just like a real pregnancy except there is no fetus. Pandas do this. After a mating, their progesterone levels may rise, and they may start acting like they’re about to give birth. “They build a nest. They get lethargic. They don’t really eat,” says Laurie Thompson, the assistant curator of giant pandas at Smithsonian’s National Zoo.
The only way to know for sure is to see a fetus on an ultrasound, which zookeepers can coax pandas into doing with honey water or fruit. But baby pandas are so tiny even at birth—only four ounces—that an unborn fetus is easy to miss. Worse, “sometimes the pandas don’t even want to participate and they’re just in their den,” says Thompson. The Smithsonian team has only successfully detected a panda pregnancy via an ultrasound once.
Then there are cheetahs, another species difficult to breed in captivity. Like pandas, they go through pseudopregnancies. They aren’t terribly cooperative with ultrasounds or blood draws, either. They do, however, poop.
Recently, researchers at the Smithsonian found a protein in cheetah feces called immunoglobulin J chain that rises during a true pregnancy. “The great thing about feces is that it’s noninvasive,” says Adrienne Crosier, a cheetah biologist at Smithsonian Conservation Biology Institute and an author on the study. “We can get feces any day of the week from a cat. It’s very easy. Any facility can collect it.”
The Life, Times, and Departure of Bao Bao the Panda
The study looks at 26 female cheetahs living in seven different zoos. Crosier also relied on the Smithsonian’s own archive of freeze-dried cheetah feces, which goes back 12 years. In fact, the zoo stores frozen feces, urine, blood, sperm, oocytes, embryos, ovarian tissue, uterine tissue, and testicular tissue for dozens of species. “We keep absolutely everything so we can use it for research,” says Crosier. “We don’t know what we may be interested in looking at or need to look at in five to 10 years.”
At the San Diego Zoo Institute for Conservation Research, which famously has its own “frozen zoo,” there is talk of resurrecting species using tissue from long-dead animals. The zoo has sperm, for example, from northern white rhinos, of which only three remain on Earth. So even in death, reproduction can be a highly managed affair.


Days before his return to Earth in 2008, NASA astronaut Daniel Tani told reporters he couldn’t wait to do something very ordinary after spending four months in space.
“I’m looking forward to putting food on a plate and eating several things at once, which you can’t do up here,” Tani said.
Plates are pretty useless on the International Space Station, where food—along with everything else—floats. Mealtime in microgravity usually consists of thermo-stabilized or freeze-dried entrees and snacks in disposable packages and pouches. Astronauts heat them up in an oven or add water before chowing down with a fork straight out of the package. The space station doesn’t have refrigerators or freezers to keep food fresh, so there’s no such thing as leftovers.
Despite the almost alien process of eating, astronauts consume many of the foods they would find back home: scrambled eggs, spaghetti, chicken teriyaki, broccoli au gratin, oatmeal with raisins. During the holidays, they have turkey, candied yams, cornbread dressing, and other seasonal foods. The current menu includes about 200 foods and beverages. Some items can be eaten in their natural form, like nuts and cookies. But most of the food has to be prepared in a laboratory and carefully tested over and over, to ensure it’s fit for consumption but can also last for two years before opening. Some of the prep veers into Food Network territory: The lab gets volunteers to judge food items on appearance, color, flavor, texture, and aroma.
The process of developing a microgravity-friendly food item can take months or years, says Vickie Kloeris, the food scientist who runs the ISS food-systems lab. I spoke with Kloeris about eating in space, how to pack food for a mission to Mars, and the myth of astronaut ice cream. Our conversation has been edited for length and clarity.
Marina Koren: So you got to Johnson Space Center as a food scientist in 1985. What was the state of astronaut food back then?
Vickie Kloeris: It really wasn’t all that different than it is now. Everything was shelf-stable, just like it is now. We had thermo-stabilized, freeze-dried, natural-form food, irradiated food, powdered beverages—just like we do now. But we didn’t have nearly as much variety during the Space Shuttle program because the missions were short, so we really didn’t need a whole lot of variety. When I came to work here, we were flying entrees from the MREs from the military. We don’t do that anymore because the MRE entrees are way too high in salt and fat for what we want for our long-duration crew members. The military has good reasons to have that salt and fat in there, but they are negatives for our crew members.
Koren: How do you transform a a terrestrial recipe into something that’s fit for consumption in microgravity?
Kloeris: Many terrestrial recipes, especially entrees, are not shelf-stable. The end product requires refrigeration. We don’t have a dedicated refrigerator or freezer for food on the space station, so everything that we send to orbit has to be shelf-stable. So we convert standard recipes into shelf-stable foods through freeze-drying and thermo-stabilization. Thermo-stabilization is basically canning—except we don’t do it in cans, we do it in pouches. Pouches are much lighter in weight and more efficient to stow. The tricky thing is, you can’t just take a traditional recipe and thermo-stabilize it or freeze-dry it and have it work. If only it were that simple. When we go to create a new item, it often takes multiple attempts, multiple adjustments, to end up with something that actually works.
Koren: Does microgravity affect the taste buds? Does food taste the same on the space station?
Kloeris: That depends on who you talk to. There is no scientific evidence that microgravity alters the taste of food. There is anecdotal evidence from crew members that they feel like their taste buds are somewhat dulled in orbit. Other crew members say it’s all in their head and there is no difference. But they are probably getting less aroma from the food when they eat in orbit than when they consume those same items on the ground. They’re eating out of packages rather than off a plate, so that can hinder the amount of aroma they’re getting. Plus, when you heat food on the ground, a lot of the heat rises and the aroma goes with it. When you heat stuff in microgravity, the heat can dissipate in different directions, so that has the potential to spread out the odor and have it be less intense. So that could be it. Just like when you and I are congested down here and we’re not getting as much aroma—the food’s not going to taste exactly the same.
Koren: Which foods are the most difficult to prepare for space?
Kloeris: Anything that creates a lot of crumbs. Crumbs are very difficult to deal with in microgravity because they’re just messy. When they get loose, they can make it into the air filtration system. You have to find a way to clean them up, and that usually involves a vacuum cleaner. Anything that requires refrigeration to remain microbiologically stable is going to be impossible to send up there. We occasionally get to send ice cream because they’ll have a freezer for medical samples that’s empty on the uphill trip. When that happens, we can send some frozen ice-cream treats and they have to eat those pretty much as soon as the vehicle docks, because they’re going to have to fill up that freezer with medical samples.
Koren: Have you tried to develop a microgravity-friendly recipe that just didn’t work?
Kloeris: We’ve had it happen more than once. We tried a thermo-stabilized cheesecake and we were never, never happy with the results. So we gave up on that.
Koren: How about carbonated drinks like soda? Can astronauts drink that?
Kloeris: Not unless they’re packaged under pressure, like in a whipped-cream can. In microgravity, the carbonation will not remain with the beverage. It will separate. Coke and Pepsi flew in pressure vessels back in the ’80s on one flight, and at that time, they didn’t have a way to chill it. So it was like, okay, we had hot Coke and hot Pepsi, so what? You’re probably not going to want a lot of carbonation in your diet when you’re in microgravity anyway, because when you burp down here, it’s dry burp. When you burp in microgravity, it’s probably not going to be a dry burp.
Koren: What ... what kind of burp would it be?
Kloeris: Wet. You’re gonna have food coming with it. When you burp, you’re burping through that sphincter at the top of your stomach. That is not a full closure. So in microgravity, when you eat, the food floats high in your stomach. Burping in microgravity is probably not something you want to do a lot of.
Koren: Have you been thinking about what kind of meals NASA would need to prepare for longer missions, like a trip to Mars or into deep space?
Kloeris: The research team in our lab is trying to figure that out right now. For Mars, the food that they eat on the return trip will be somewhere between five and seven years old, so that is a huge challenge. We can actually make food that is microbiologically safe to eat for that period of time. But there’s very few foods in our current food system that would maintain sufficient quality after that long. Even though we can stop microbial changes in these products by preserving them, we can’t stop the chemical changes. The color, texture, and flavor are going to change, and the nutritional content is going to degrade. We’re looking into which items are most susceptible to degradation. A particular nutrient will be more stable in one food than in another. For instance, vitamin C is not very stable in thermo-stabilized products, but it’s very stable in powdered beverages.
Koren: What recipes are you working on right now?
Kloeris: We aren’t developing any new foods right at this moment. We have some new products that have been developed over the last couple of years that we’re just now introducing into the food supply to see how the acceptability goes. We have a new freeze-dried roasted-brussels-sprouts dish, a couple of thermo-stabilized fish casseroles—to try to get some more omega-3s into the food system—a freeze-dried fruit salad.
Koren: I’m sure people have asked you this a million times, but how did that chalky, Neapolitan astronaut ice cream become a thing?
Kloeris: During [the Apollo program], one of the crew members did request ice cream, but what they flew doesn’t look like anything that they sell at the museums or the visitor centers. I think that just took off because it was something the kids liked and a commercial company made it. What actually flew during Apollo was a synthetic cube that was dairy-based. That’s about as close to ice cream as they got.
Koren: Does your job change the way you look at preparing food at home?
Kloeris: My family thinks I overreact sometimes because I worry about food safety. I’m not one to leave the turkey sitting out on the table for hours after dinner.


This article is edited from a story shared exclusively with members of The Masthead, the membership program from The Atlantic. Find out more.

One hundred miles above the Earth’s surface, orbiting the planet at thousands of miles per hour, the six people aboard the International Space Station enjoy a perfect isolation from the chaos of earthly conflict. Outer space has never been a military battleground. But that may not last forever. The debate in Congress over whether to create a Space Corps comes at a time when governments around the world are engaged in a bigger international struggle over how militaries should operate in space. Fundamental changes are already underway. No longer confined to the fiction shelf, space warfare is likely on the horizon.
While agreements for how to operate in other international domains, like the open sea, airspace, and even cyberspace, have already been established, the major space powers—the United States, Russia, and China—have not agreed upon a rulebook outlining what constitutes bad behavior in space. It’s presumed that International Humanitarian Law would apply in outer space—protecting the civilian astronauts aboard the International Space Station—but it’s unclear whether damaging civilian satellites or the space environment itself is covered under the agreement. With only a limited history of dangerous behavior to study, and few, outdated guidelines in place, a war in space would be a war with potentially more consequences, but far fewer rules, than one on Earth.
Although there has never been a military conflict in space, the history of human activity above our atmosphere is not entirely benign. In 1962, the United States detonated a 1.4 megaton nuclear weapon 250 miles above the Earth’s surface. The blast destroyed approximately one third of satellites in orbit and poisoned the most used region of space with radiation that lasted for years. Although the United States, Russia, and others soon agreed to a treaty to prevent another nuclear test in space, China and North Korea never signed it. In 2007, China tested an anti-satellite weapon, a conventionally-armed missile designed to target and destroy a satellite in orbit. In the process, it annihilated an old Chinese weather satellite and created high-velocity shrapnel that still threatens other satellites. Even though demonstrations like this have consequences for everyone, countries are free to carry them out as they see fit. No treaties address this kind of test, the creation of space debris, or the endangerment of other satellites.
The U.S. has the most to lose in a space-based conflict
With by far the most satellites in orbit, the U.S. has the most to gain by establishing norms, but also the most to lose. Almost half of all operational satellites are owned and operated by the United States government or American commercial companies. That’s twice as many as Russia and China, combined. Space may seem distant, but what happens there affects our everyday lives on the ground. When we use our phones to plan a trip, we depend on American GPS satellites to guide us. When the U.S. military deploys troops overseas, satellite communications connect forces on the ground to control centers. When North Korea launches an intercontinental ballistic missile, the U.S. and its allies depend on early-warning satellites to detect it.
On one hand, if the global space powers agreed to put limits on space-based weapons and other related technologies, it could make space safer for everyone. But because the U.S. may have spent time and resources developing exactly the type of weapons that a code of conduct would ban, it could also curtail the most advanced space-based developments, erasing years of research and progress.
There are more players in space—and less consensus
In the first space age, from the launch of the first human-made satellite in 1957 through the fall of the Soviet Union, the United States and the USSR were responsible for over 90 percent of all satellites. Their race to perfect space technology, dominated by both national security interests and scientific discovery, far outpaced everyone else. The second space age, from 1990 to today, looks remarkably different. Now, more satellites are operated by private companies than militaries, and more space launches and new satellites come from countries other than the United States and Russia. More players in space—particularly more unpredictable players—means more opportunities for aggressive behavior, like developing anti-satellite technologies or hacking satellite communications. Countries like Iran or North Korea that are newer to space can choose to operate in a way we’ve never seen before. And if their nuclear programs on Earth are any guide, they could pose serious threats if left unchecked.
Efforts have been made to create a modern-day space rulebook, but so far none have gained traction. In 2008, when Russia and China both proposed norms of behavior, the United States refused to sign on. Similarly, when the United States supported a 2014 European Union proposal to govern the use of conventional weapons in orbit, Russia and China didn’t agree with the terms.
Since the congressional debate about a Space Corps, people have been taking the prospect of a war in space seriously, in a way we haven’t seen before. Now we should start talking about how to avoid that war. To prevent conflict in the upper atmosphere, all potential adversaries—the United States, China, North Korea, Iran, Russia, the EU—need to align, and agree on norms of behavior. They need rules.


Astronomers have completed their first round of telescope observations of ‘Oumuamua, the first known interstellar object to enter our solar system, to check the asteroid for signs of alien technology.
So far, they have found no evidence of artificial signals coming from the asteroid, they said Thursday—but the search isn’t over yet.
“Indeed, nothing has popped up, but we’re busy churning through the data we’ve collected so far,” said Andrew Siemion, the director of the Berkeley SETI Research Center who leads its Breakthrough Listen Initiative, a $100 million effort in the search for intelligent extraterrestrial life.
The decision to check ‘Oumuamua for artificial signals came from Yuri Milner, the Russian billionaire and tech investor who is spending $100 million over 10 years to fund SETI efforts.
‘Oumuamua was first detected by Hawaiian astronomers in October. The asteroid, named for a Hawaiian word meaning “messenger,” puzzled the astronomy community. The properties of the mysterious space rock were unusual, particularly its extremely elongated, cigar-like form, a shape difficult to create through the natural, known processes of the universe. Somewhere along the way, some astronomers began to wonder whether this space rock could be a probe of some kind, dispatched by an advanced civilization.
Aliens are, of course, at the very bottom of the list of explanations for new and confounding astronomical discoveries. But for Milner, it was worth checking when astronomers had the chance, before ‘Oumuamua zoomed out of reach of even the most powerful telescopes.
The observations began Wednesday at 3:45 p.m. Eastern Time. For six hours, a Breakthrough Listen instrument on the Green Bank Telescope in West Virginia scanned ‘Oumuamua across four radio bands. The instrument is capable of scanning billions of individual channels at once, listening for pings between one and 12 gigahertz—a range of frequency that on Earth includes signals from technology like cellphones and microwave ovens.
Astronomers have so far only analyzed the data from one of the bands. “I’m certainly anxious to see if anything pops up from searches of the other bands,” Siemion said. When astronomers analyze data in the range of one to two gigahertz, they can look for evidence of the molecule hydroxyl, which would suggest the presence of water on ‘Oumuamua.
Software developed for the Breakthrough Listen project will search for narrow-bandwidth signals drifting in the frequencies detected by Green Bank. “By matching the rate at which these signals drift to the expected drift due to the motion of ‘Oumuamua ... the software attempts to identify any signals that might be coming from ‘Oumuamua itself,” the team said Thursday. The software will also weed out signals coming from human technology on or around Earth.
Siemion said the analysis will take some time. “We’re looking for signals from weakly transmitting or very distant technologies, bathed in a sea of signals from our own technology,” he said. “SETI is not an especially good field for the easily discouraged.”
‘Oumuamua is currently about twice the distance between the Earth and the sun from our planet, and barreling through the solar system at a rate of 38.3 kilometers per second. At this distance, Green Bank can still detect signals as faint as the radio waves from a cellphone. The next round of observations at Green Bank will likely begin Friday.
Astronomers predict many more interstellar objects will be detected in the solar system in the years to come. For Milner and SETI astronomers, it’s worth checking each one for signs of artificial technology.
“More and more SETI is going to be done on those types of objects over time, because I think this is not the only one that we will detect,” Milner said. “We need to start doing this, to practice.”


In August 1976, a 44-year-old headmaster named Mabalo Lokela arrived back in the town of Yambuku in the Democratic Republic of the Congo, after two weeks spent touring with a local mission. A few days after his return, he checked into the local hospital with nosebleeds, dysentery, and a fever. The doctors treated him for malaria, but to no avail. Lokela got worse. In early September, two weeks after his first symptoms, he died. And meanwhile, other people who had come into contact with him started getting sick.
Over the next three months, 318 people became infected, and 280 of them died. That outbreak, and another that took place simultaneously in South Sudan, alerted the world to the existence of a lethal new disease, which eventually took the name of the waterway on which Yambuku is situated—the Ebola River.
Ebola is famously deadly, but not inevitably so. Around 12 percent of those who were infected in the Yambuku outbreak survived their brush with the disease, and many of them are still around today. They’ve lived through seven more documented Ebola outbreaks in the DRC, the latest of which took place this May, less than 350 kilometers away from Yambuku. They’ve watched from afar as the biggest Ebola outbreak in history ravaged West Africa.
Compared to that epidemic, “the DRC’s outbreaks have been smaller and more isolated,” says Anne Rimoin at the University of California, Los Angeles, “and so too have these survivors. They’ve had no contact or follow-up.” They still bear the scars and social stigma of their experience with the virus. But they also carry defenses against it.
Rimoin has shown that the original survivors’ blood still contains antibodies against Ebola. In some cases, people had antibodies that can destroy the virus outright, even after 41 years. “They should be immune to Ebola,” Rimoin says.
Simply finding the survivors was a Herculean task. Medical records from that 1976 outbreak were nowhere to be found, so Rimoin had to ask the researchers who were on the scene to rummage through their files. Once she had a list, her team took several trips to Yambuku to search for the people behind the names. And since the town is so remote, every trip involved a chartered flight and a grueling drive. “It took maybe 9 hours in the dry season, and 20 hours in the rainy season,” Rimoin says.
The team eventually tracked down 14 survivors who, according to Rimoin, were eager to take part in a new study. “They were very happy that there were people out there interested in hearing their stories and understanding what they had been through,” she says. “It may have been a long time ago, but they are still living the consequences of what they’ve suffered. Most of them lost family members. And when they emerged from the hospital, having narrowly survived a terrifying near-death experience, they found their homes had been burned to the ground for fear of contamination. Everything they owned had disappeared.”
Even now, they have to live with the social stigma of having once had Ebola four decades ago. Such is the fear surrounding the virus that the hospital in Yambuku was initially reluctant to let Rimoin bring her volunteers in to take blood samples. “To this day, if you say you’re an Ebola survivor, people will recoil,” she says.
Previously, another team found that Ebola patients retain some immunity against the virus after 14 years, but Rimoin’s team have shown that this protection extends for decades more. All of the 14 people they studied still carry antibodies that recognize at least one of the Ebola virus’s proteins, and four had antibodies that could completely neutralize the virus. “Those are the kinds of responses you’d like to see in a vaccine—long-lasting and robust,” says Rimoin, “which means that these antibodies are of great value to science.”
It’s clear that the Ebola virus can stick around long after symptoms abate, by hiding out in unusual places like eyeballs. One man still carried the virus in his semen 565 days after he recovered. Ebola’s tenacity might explain why survivors continue to produce antibodies against it, long after they’ve finally cleared it from their bodies.
“It’s well-documented that acute viral infections such as measles and smallpox provide lifelong immunity,” says Mohan Natesan from the U.S. Army Medical Research Institute for Infectious Diseases. Ebola infections can clearly lead to similarly long-lasting antibodies, but “it is difficult to predict whether these antibodies will protect from reinfections, or infection with another Ebola species. Further studies with these survivors are warranted to answer these questions."
It’s also hard to say why four of the 14 people developed such potent antibodies. They might have had naturally powerful immune systems. They might have already been exposed to similar viruses in the past, and confronted Ebola with partial immunity. It’s hard to say why with such a small group, but “it is remarkable that 14 suspected survivors were tracked and found in the first place,” says Lauren Cowley from Harvard University. “The 1976 outbreak was much smaller than the recent West African outbreak so a much larger pool of survivors will now be available for future studies on the long-term effects of Ebola.”
Rimoin is now planning more trips to the DRC to continue studying the survivors, including those who made it through later epidemics. She’s on the clock: The average life expectancy in the DRC is 62 years for women and 58 years for men. Many people who were teenagers during the first epidemic are now nearing the end of their lives. Indeed, Rimoin originally identified 16 survivors from the 1976 outbreak, but two of them died before she got a chance to take their blood. “The aperture for studying these people is closing,” she says.


The journalist was not having it. He tailed the museum director out the door. “Are you looting the museum for your own personal means?” he demanded. “I totally saw you slip something into your car earlier today.”
Obviously, the emergency evacuation of the state museum of “Smithsonia” was not going according to plan.
This scene played out at the Smithsonian on a recent Wednesday afternoon, during an exercise in which a group of cultural-heritage professionals and emergency responders tried to evacuate the fictional Smithsonia museum after a pretend cyclone. They had all come to Washington, D.C., for the weeklong Heritage Emergency and Response Training, or HEART, hoping to learn how museums can plan for natural disasters or even war.
The HEART organizers did not make it easy: The museum’s collection was scattered and uncatalogued, the staff were largely absent, the aforementioned reporter was snooping around for dirt, and the museum’s director was, well, you already know. To successfully evacuate the museum, the team would have to navigate egos, bureaucracies, and public opinion—plus move dozens of fragile objects.
These included a cabinet of ceramics (to be individually Bubble Wrapped), several framed drawings (also to be Bubble Wrapped), and a giant, crumbling flag (to be carefully rolled up and supported atop a bed of cut-up boxes). All of these had to be lifted down a set of stairs, through a narrow tunnel, back up the stairs, out the door, down an outdoor path, and back into another building. All in an hour.
“I would do 200 things differently,” Megan Salazar-Walsh, one of the HEART participants, later said.
HEART is a program of the Smithsonian’s Cultural Rescue Initiative, which deploys experts to sites of conflict or natural disaster around the world. Corine Wegener, one of the organizers of HEART, previously recovered artifacts looted from museums during Operation Iraqi Freedom. In 2010, she went to Haiti after the earthquake and worked on restorations at the Centre d’Art and the Holy Trinity Episcopal Cathedral’s historic murals—that project eventually led to the formal creation of the Cultural Rescue Initiative.
Since then, the group has worked to preserve heritage sites and museums in Iraq, Syria, post-earthquake Nepal, and most recently Puerto Rico.
Large museums have detailed emergency-response plans—in some cases, built in their very architecture. The Getty Museum, recently threatened by wildfires around LA, has a million-gallon water tank and an air-filtration system that keeps out smoke. The Whitney Museum in New York City is protected by a 15,500-pound flood door “designed by engineers who build watertight latches for the U.S. Navy’s destroyers.”
Not all museums are impenetrable fortresses, though, and the state museum of Smithsonia was considerably more modest. In fact, it was woefully understaffed, which is why it had to rely on volunteers, played by the HEART participants, to evacuate it. They had divided into five teams of five, each with a team leader who in turn reported to the incident commander, Megan Salazar-Walsh.
In their real lives, the participants were geographically diverse, spanning Alaska to Puerto Rico. Salazar-Walsh is an art conservator at the John and Mable Ringling Museum of Art in Florida.
The 25 of them met at the beginning of the week, so they were not quite strangers by the evacuation exercise Wednesday afternoon, but not yet at ease, either. Inevitably, when their careful plans to label items A through D and pack them up in a specific box gave way to improvisation, that lack of underlying trust showed. “Different people are giving out different directions, commander,” one woman said, clearly exasperated, as Salazar-Walsh made her way through the museum’s rooms.
It did not help that the museum’s director had waltzed in and was issuing his own orders. Brian Daniels is a director at the Penn Cultural Heritage Center, but for the scenario, he had donned a blue suit and pink bowtie to play the aloof director of the Smithsonia state museum—the kind of director prone to digressions about collection items even in the middle of an emergency evacuation. (In last year’s evacuation exercise, Daniels wore a black suit and played an irate politician. “It was very out of character,” he says. “For the rest of the class everyone was in fear of me.”)
Daniels strode over to the cabinet and plucked a ceramic poodle off the shelf. “Fifi!” he exclaimed, before launching into the story of the beloved white poodle who belonged to the children of Smithsonia’s governor. Salazar-Walsh gently tried to move him along. “If only you’d grown up on the island ...” Daniels continued. He implored the evacuators to wrap Fifi with care.
Fifi the ceramic poodle had come from a garage or estate sale—like all the other items that made up the mock museum. (“No museum artifacts are put at risk in the course of this exercise,” Wegener clarified.) But the mock artifacts had to be treated as real ones, and even as Daniels hammed it up, he was getting at something fundamental: Why save museums in war or natural disaster?
“I always liken it to your own home,” Wegener later said. Of course, you want to get your family out first, but as soon as it’s safe, the fire department is authorized to salvage personal belongings, too. “For people going back in their own homes after disasters, they’re looking for those irreplaceable items that stand for their personal and their family’s identities, their memories, their hope for recovery in the future. Scale that up to whole communities and countries,” she says. One of her colleagues in Haiti had summed it up this way: “The dead are dead, we know that. But without our culture, the rest of us can’t go on living.” For Smithsonia, one had to remember, Fifi represented far more than a random dog.
In the next room, Daniels found a team starting to pack an old, crumbling flag laid out on the floor. “I don’t want it to be rolled crooked,” he said. He strode over, almost stepping on the flag in several places. The team exchanged nervous glances. But Daniels soon found a mug that he said was his personal mug, and they told him to take it, relieved to have finally dispatched him. (“They’re doing really well despite me trying my best to distract them,” Daniels said.)
What the team did not all realize, though, is that while Daniels was wandering around, so was the keen-eyed journalist—played by Wegener’s husband, Paul. (Naturally, he wore a flak jacket.) And when the journalist saw Daniels putting the mug in his car, he confronted him. The resulting make-believe headline: “Objects Looted From Museum.”
The point was not to make everyone suspicious of each other, but to inject the kind of unexpected situation that always comes up in emergencies. “If you can do it here, where you just met these people, etcetera, and you’re under time constraint, think how much better you’ll be able to do it when you get back to your institution when you have a plan,” says Wegener. “And that’s the aha moment people have. Yeah, it’ll be lot easier.” Later in the week, HEART had a whole session on planning for the media during emergencies.
Most of all, HEART’s goal over the week was getting museums and emergency planners to talk to one each other. “Those two universes do not coincide very much,” says Wegener. The goal is to get more cities and towns to integrate cultural heritage in emergency planning. “We’re miles ahead of where we were several years ago, miles ahead with Hurricane Katrina,” she adds.
At times during the evacuation, the gap between these two worlds was on display. The group got stuck in a debate over what to do with several pieces of furniture they would not have time to evacuate. In the scenario, the evacuated museum was being turned into an emergency-operations center, or EOC—not exactly a place where protecting antique furniture would be a top priority. Someone suggested telling the EOC people to keep away from the museum furniture because it might be unsafe.
One of the emergency planners balked. “I can’t in good conscience tell them it’s not safe if I think it is,” he said.
“Can we just tell them not to use the collection items?”
“EOC will use everything,” another emergency planner said.
Okay, what about shrink-wrapping the furniture, which would at least give them some protection?
This time Salazar-Walsh, a conservator, hesitated. “It also could create a microclimate in that space,” she said, referring to how the plastic could trap humidity inside wooden furniture and cause mold. But she reconsidered: The furniture didn’t look wet.
The EOC people were marching up the steps, Wegener said. It was time to make a decision.
They decided to shrink-wrap.


Updated on December 13 at 6:30 p.m.
Hydraulic fracturing, or fracking, may pose a significant—but very local—harm to human health, a new study finds. Mothers who live very close to a fracking well are more likely to give birth to a less healthy child with a low birth weight—and low birth weight can lead to poorer health throughout a person’s life.
The research, published Wednesday in Science Advances, is the largest study ever conducted on fracking’s health effects.
“I think this is the most convincing evidence that fracking has a causal effect on local residents,” said Janet Currie, an economist at Princeton University and one of the authors of the study.
The researchers took the birth records for every child born in Pennsylvania from 2004 to 2013—more than 1.1 million infants in total—and looked at the mother’s proximity to a fracking site, using the state of Pennsylvania’s public inventory of fracking-well locations. They used private state records that showed the mother’s address, allowing them to pinpoint where every infant spent its nine months in utero.
They found significant, but very local, consequences. Infants born to mothers who lived within two miles of a fracking well are less healthy and more underweight than babies born to mothers who lived even a little further away. Babies born to mothers who lived between three and 15 miles from a fracking well—that is, still close enough to benefit financially from the wells—resembled infants born throughout the rest of the state.
While birth weight may seem like just a number, it can affect the path of someone’s life. Children with a low birth weight have been found to have lower test scores, lower lifetime earnings, and higher rates of reliance on welfare programs throughout their lives. In a previous study, a different team of researchers examined twins in Norway whose birth weight diverged by 10 percent or more. The lighter twin was 1 percent less likely to graduate from high school and earned 1 percent less than their sibling through their life.
“Hydraulic fracturing has widely dispersed benefits—we are all paying lower natural-gas bills for heating, we’re all paying lower electricity prices, we’re all paying less for cheaper gasoline at the pump. And even if health was all that you care about, we’re all benefitting from decreased air pollution thats widely dispersed, because coal plants are closing,” said Michael Greenstone, a professor of economics at the University of Chicago and another authors of the paper.
But all those benefits, he said, were borne by the local communities that lived extremely close to hydraulic fracturing wells. “There’s this interesting trade off between the greater good and what are the costs and benefits for local communities,” he told me.
Oil and gas lobbying groups rushed to criticize the study. “This report highlights a legitimate health issue across America that has nothing to do with natural gas and oil operations.  It fails to consider important factors like family history, parental health, lifestyle habits, and other environmental factors and ignores the body of scientific research that has gone into child mortality and birthweight,” said Reid Porter, a spokesman for the American Petroleum Institute, a trade organization that represents the oil and gas industry.
In the fracking study, researchers tried to separate the costs of fracking and socioeconomic status and parental health in several ways. First, they compared baby birthweight near fracking wells to those babies immediately around them, which they believe accounts for the wealth of various communities.
Second, they found that the connection held for siblings who were or were not exposed to a fracking well. “We follow the same mother over time and ask whether on average, children born after fracking starts have worse outcomes than their siblings born before fracking starts,” Currie told me. “In this case, since we follow the same woman over time, we are controlling for her underlying characteristics.”
Babies who gestated near a well had a reliably lower birth weight than their siblings who were not exposed to the well.
The researchers don’t yet know why this link between fracking and low birth weight exists, though they suggest that air pollution could be a possible contributor. The process of fracking may release chemicals into the air, for one, but many wells also run multiple diesel engines at once, and they can be a hub of local activity, with trucks regularly commuting to the sites.
While environmental activists and some researchers have proposed that fracking chemicals may leak into groundwater, most studies have failed to find lasting and widespread water pollution near wells. The birth-weight study seems to suggest that air, not water, pollution may instead be the threat that fracking sites pose to human health.
Greenstone believes the next step for this research is to figure out exactly what is driving the babies’ low birth weight. “Is it the trucks? Is it the diesel generators?” he said. “If you knew the channel, you might be able to devise a light-touch regulatory approach.”
But he and Currie also believe more research is needed to figure out how fracking affects people outside the womb and later in their life. Such connections will be harder to distill, but may become easier as this kind of broad, data-based approach to environmental economics becomes more widespread.


In 1799, the Italian scientist Alessandro Volta fashioned an arm-long stack of zinc and copper discs, separated by salt-soaked cardboard. This “voltaic pile” was the world’s first synthetic battery, but Volta based its design on something far older—the body of the electric eel.
This infamous fish makes its own electricity using an electric organ that makes up 80 percent of its two-meter length. The organ contains thousands of specialized muscle cells called electrocytes. Each only produces a small voltage, but together, they can generate up to 600 volts—enough to stun a human, or even a horse. They also provided Volta with ideas for his battery, turning him into a 19th-century celebrity.
Two centuries on, and batteries are everyday objects. But even now, the electric eel isn’t done inspiring scientists. A team of researchers led by Michael Mayer at the University of Fribourg have now created a new kind of power source that ingeniously mimics the eel’s electric organ. It consists of blobs of multicolored gels, arranged in long rows much like the eel’s electrocytes. To turn this battery on, all you need to do is to press the gels together.
Unlike conventional batteries, the team’s design is soft and flexible, and might be useful for powering the next generation of soft-bodied robots. And since it can be made from materials that are compatible with our bodies, it could potentially drive the next generation of pacemakers, prosthetics, and medical implants. Imagine contact lenses that generate electric power, or pacemakers that run on the fluids and salts within our own bodies—all inspired by a shocking fish.
To create their unorthodox battery, the team members Tom Schroeder and Anirvan Guha began by reading up on how the eel’s electrocytes work. These cells are stacked in long rows with fluid-filled spaces between them. Picture a very tall tower of syrup-smothered pancakes, turned on its side, and you’ll get the idea.
When the eel’s at rest, each electrocyte pumps positively charged ions out of both its front-facing and back-facing sides. This creates two opposing voltages that cancel each other out. But at the eel’s command, the back side of each electrocyte flips, and starts pumping positive ions in the opposite direction, creating a small voltage across the entire cell. And crucially, every electrocyte performs this flip at the same time, so their tiny voltages add up to something far more powerful. It’s as if the eel has thousands of small batteries in its tail; half are pointing in the wrong direction but it can flip them at a whim, so that all of them align. “It’s insanely specialized,” says Schroeder.
He and his colleagues first thought about re-creating the entire electric organ in a lab, but soon realized that it’s far too complicated. Next, they considered setting up a massive series of membranes to mimic the stacks of electrocytes—but these are delicate materials that are hard to engineer in the thousands. If one broke, the whole series would shut down. “You’d run into the string-of-Christmas-lights problem,” says Schroeder.
In the end, he and Guha opted for a much simpler setup, involving lumps of gel that are arranged on two separate sheets. Look at the image below, and focus on the bottom sheet. The red gels contain saltwater, while blue ones contain freshwater. Ions would flow from the former to the latter, but they can’t because the gels are separated. That changes when the green and yellow gels on the other sheet bridge the gaps between the blue and red ones, providing channels through which ions can travel.
Here’s the clever bit: The green gel lumps only allow positive ions to flow through them, while the yellow ones only let negative ions pass. This means (as the inset in the image shows) that positive ions flow into the blue gels from only one side, while negative ions flow in from the other. This creates a voltage across the blue gel, exactly as if it was an electrocyte. And just as in the electrocytes, each gel only produces a tiny voltage, but thousands of them, arranged in a row, can produce up to 110 volts.
The eel’s electrocytes fire when they receive a signal from the animal’s neurons. But in Schroeder’s gels, the trigger is far simpler—all he needs to do is to press the gels together.
It would be cumbersome to have incredibly large sheets of these gels. But Max Shtein, an engineer at the University of Michigan, suggested a clever solution—origami. Using a special folding pattern that’s also used to pack solar panels into satellites, he devised a way of folding a flat sheet of gels so the right colors come into contact in the right order. That allowed the team to generate the same amount of power in a much smaller space—in something like a contact lens, which might one day be realistically worn.
For now, such batteries would have to be actively recharged. Once activated, they produce power for up to a few hours, until the levels of ions equalize across the various gels, and the battery goes flat. You then need to apply a current to reset the gels back to alternating rows of high-salt and low-salt. But Schroeder notes that our bodies constantly replenish reservoirs of fluid with varying levels of ions. He imagines that it might one day be possible to harness these reservoirs to create batteries.
Essentially, that would turn humans into something closer to an electric eel. It’s unlikely that we’d ever be able to stun people, but we could conceivably use the ion gradients in our own bodies to power small implants. Of course, Schroeder says, that’s still more a flight of fancy than a goal he has an actual road map for. “Plenty of things don’t work for all sorts of reasons, so I don’t want to get too far ahead of myself,” he says.
It’s not unreasonable to speculate, though, says Ken Catania from Vanderbilt University, who has spent years studying the biology of the eels. “Volta’s battery was not exactly something you could fit in a cellphone, but over time we have all come to depend on it,” he says. “Maybe history will repeat itself.”
“I’m amazed at how much electric eels have contributed to science,” he adds. “It’s a good lesson in the value of basic science.” Schroeder, meanwhile, has only ever seen electric eels in zoos, and he’d like to encounter one in person. “I’ve never been shocked by one, but I feel like I should at some point,” he says.


When Charles Konsitzke and Dhanu Shanmuganayagam first met, they were both just trying to get some peace and quiet. It was early 2014, and they were representing the University of Wisconsin-Madison at a fancy event to promote the university’s research to local politicians. After hours of talking to senators, Shanmuganayagam was fried, and went for a walk to clear his head. That’s when he bumped into Konsitzke, an administrator at the University of Wisconsin’s Biotechnology Center. They introduced themselves, and discussed their work. Shanmuganayagam said that he ran a facility that rears miniature pigs, which are genetically engineered to carry mutations found in human genetic disorders. Scientists can study the mini-pigs to better understand those diseases.
“And I said: I have a project for you,” Konsitzke recalls.
Konsitzke’s son Mason, now aged 7, was born with little brown birthmarks on his buttocks. Many kids have one or two of these café-au-lait spots and at first, Konsitzke thought they were cute. But after more appeared, he did some research and found that such spots are a common symptom of neurofibromatosis type 1 (NF-1)—an incurable inherited disease. Around Mason’s first birthday, a pediatrician confirmed the diagnosis.
NF-1 is an incredibly varied disease with many possible symptoms. The spots are the least of them. Some patients, Mason included, develop learning disabilities. Others develop bone and heart problems. Most commonly, patients get tumors on their skin and nerves; Mason already has one on the side of his face. These tumors are usually benign, but even so they can still disfigure. “It continues to grow, and if it runs out of space, it will deform his face outward,” says Konsitzke.
Konsitzke isn’t a scientist by training, but through his job he’s well connected to the scientific world. Once Mason’s diagnosis was in, he started asking around about NF-1 research. In particular, he wanted to know where the bottlenecks are. What was the single thing he could do that would most accelerate research into his son’s condition? And the answer that he kept hearing was: Find better animals to experiment on.
When studying diseases, scientists often turn to laboratory animals like mice and zebrafish. They can use these so-called model organisms to work out how mutations cause diseases, and to find and test possible treatments. But the usual lab animals aren’t a good fit for NF-1. They’re too small, and they don’t react in the same way to the mutations that cause the disease in humans. For example, studies in mice suggested that a drug called lovastatin might help to address the learning and attentional problems that accompany NF-1. But when the drug was tested on actual children, in a large clinical trial, it did nothing.
To better understand NF-1, Konsitzke learned, you need a species that’s closer in both size and biology to a person, and yet is still relatively easy to raise and study. That is, you need pigs. “Pigs closely represent humans,” says Neha Patel, who directs the UW neurofibromatosis clinic. “People with NF-1 have varied cognitive deficits, from severe learning issues to subtle problems. If you imagine studying those in a rat, you’d only get a crude picture of how that translates to humans. But pigs are intellectual animals.”
That’s why, in a quiet corner of the Wisconsin State Capitol, Konsitzke was so excited to meet Shanmuganayagam. Here was someone with experience in raising, engineering, and studying pigs. Here was just the guy he needed to give NF-1 research a boost.
For his part, Shanmuganayagam was keen to take on a new challenge. “I know a lot of diseases,” he says, “but when Chuck told me [about NF-1], I thought, ‘I don’t know what this is.’ And I can’t believe I didn’t know because it’s not that rare.” Indeed, NF-1 affects at least one in every 2,500 babies, making it more common than other better-known genetic disorders like cystic fibrosis. And that figure is probably an underestimate, because many cases don’t present with obvious symptoms. “It’s under-recognized, and kids are not getting the best care,” says Patel. She’ll often see parents whose children clearly meet all the clinical criteria for the disease, and yet hadn’t been diagnosed for over a decade.
The disease has a low profile partly because its symptoms can be so disfiguring. Patients with NF-1 often “become closeted and sheltered,” says Konsitzke. “People don’t look at them or listen to them, so there’s no strong speaking voice for the disease.” The man whom Pope Francis embraced in 2013, whose face was covered in growths, had NF-1. Joseph Merrick, the so-called Elephant Man of 19th-century England, was once suspected of having had NF-1. “Our neuro-oncologist told us to take pictures of Mason now, and be prepared for his face to change,” says Konsitzke.
For different reasons, NF-1 has a low profile in the scientific community, too. It’s caused by mutations in a gene called neurofibromin 1 (confusingly also shortened to NF1), which is daunting to study because of its large size and unusual variability. “People fear it in the research realm,” says Konsitzke. There are more than 4,000 variants of the gene, each of which changes the symptoms of NF-1 in subtle ways.
Again, the pigs can help. Konsitzke and Shanmuganayagam aren’t just planning to develop pigs that can model the symptoms of NF-1. They want to use the revolutionary gene-editing technique known as CRISPR to create pigs that have the specific mutations of a particular individual. Each child with NF-1 would get their own personalized piglet, whose version of the NF1 gene matched their own. The piggy proxy could be monitored to see how the kid’s condition might progress, especially since they mature faster than humans do.
The pigs could also be used to test possible treatments. At the moment, people with NF-1 “are their own guinea pigs,” says Konsitze. “It takes years of tests and side effects to find something that works. My son is on a cocktail of four different meds.” Having an animal that mirrors the particulars of a person’s condition might make it quicker to narrow down the best treatments. And Konsitze argues that this should save enough money to compensate for the costs of creating a customized pig.
Shanmuganayagam and his colleagues started actively trying to make the gene-edited pigs in 2014, with a shoestring budget of $50,000 from the Neurofibromatosis Network, which they bolstered through their own fund-raising efforts. (Two companies also developed pig models of NF-1 around the same time: Recombinetics, based in St. Paul, Minnesota; and Exemplar Genetics, in Sioux Center, Iowa.)
Jennifer Meudt, one of Shanmuganayagam’s colleagues, led the work. “Dhanu said: ‘We’re going to gene edit pigs,’” she says. “And I said: Okay, ha ha, how are we going to do this? My background is in botany.”
The easy way to do it would be to buy pig eggs from slaughterhouses, fertilize them in the lab, do any necessary gene editing, and inject them back into sows. But UW’s pig-rearing facility is kept free of disease-causing microbes, so the team can’t just implant their pigs with commercially sourced embryos. Instead, they have to artificially inseminate their own sows, collect the embryos, inject them with CRISPR ingredients, and put them back into other pigs.
The embryos themselves are hard to work with, because they’re not translucent like those of many other animals. “They look like little black balls, which makes it more difficult to inject when you don’t know where you’re poking,” says Meudt. There are logistical complications too. Meudt has to do the CRISPR injections in a building that’s 30 minutes from the pig facility, so the animals have to get shuttled back and forth. From start to finish, the process to make a single embryo starts at 4 in the morning, and ends at around 10 at night. “Those are well-traveled embryos,” Meudt says.
The team delivered their first edited animal in November 2016, which carried a set of NF-1 mutations that had been reported in an earlier published study. They have since engineered three more animals, each with a different cluster of mutations. All of these are essentially personalized—they represent someone’s case of NF-1, but those patients are anonymized.
The next step will be to create pigs that are openly personalized to specific people—and the team is in the midst of getting approval for that. “We slowed ourselves down for some ethical reasons,” says Shanmuganayagam. For example, if a personalized pig starts to show symptoms, and their child counterpart has not, how should the team communicate that information to a family? What happens if the pig dies early? Should a family even get to know which pig is “theirs”? “We’re trying to resolve that,” says Shanmuganayagam. “We might blind ourselves to whose pig is whose with the option of revealing at a later date. I’ve also had my group read Flowers for Algernon.”


The moment of birth is the moment we transform from an individual into an entire world. We leave the sterility of the womb, pass through a mother’s vagina, and become lathered in her microbes, taking them into our skin, our mouths, our guts. We begin our life as we will always live it: as a community of trillions, enclosed within a single body.
Microbes help their hosts to build their bodies, digest food, and defend against disease, so animals have evolved a multitude of ways for bestowing these tiny partners onto their offspring. Many insects do so at the earliest possible opportunity, adding bacteria directly to egg cells, so that their young are accompanied by microbes from conception. There is literally no part of their life cycle where they are sterile. Others do so in the womb. The tsetse fly, which spreads sleeping sickness, nourishes its grub inside a bizarrely mammalian uterus and feeds it with a milklike fluid—one that’s laden with microbes.
Yet other species have ways of provisioning their young as they greet the world. Humans do so automatically. The Japanese stinkbug coats her jelly-bean-like eggs in a bacteria-rich icing so the hatchlings become colonized when they emerge. Koala moms package the bacteria that allow them to digest tough and toxic eucalyptus leaves into a special kind of poop called pap for their joeys to ingest.
And perhaps the strangest technique is used by the beewolf—a powerful, bee-killing wasp that daubs the walls of her infant’s nursery with a living plaster that she squeezes out of her own head. You can learn more about its bizarre life, and the scientist who studies it, in the video below—the third in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.


The history of antibiotics is a history of running in place. Two years after the first of these life-saving drugs—penicillin—was mass-produced, bacteria that resisted the drug became widespread, too. With grim inevitability, the same events have unfolded for every other drug. Every time scientists identify a new substance that can hold back the tide of infectious disease, resistant superbugs surge over that barrier in a matter of years.
The evolution of drug-resistant microbes is unavoidable, but it’s not instantaneous. And one might reasonably wonder why. Microbes have been around for billions of years. They have had, quite literally, all the time in the world to invent every possible biochemical trick, including ways of defusing antibiotics that they themselves use to kill and suppress each other. So why aren’t all microbes already resistant to all drugs?
“The reason is that resistance, like any superpower, comes at a cost,” says Nina Wale, from the University of Michigan. For example, microbes could create pumps that flush out any killer drugs, but those pumps cost energy to build and maintain. These costs mean that, under normal conditions, resistant microbes grow more slowly than their susceptible peers, and are almost always outcompeted. But antibiotics tip the balance of this competition by finally giving the resistant microbes a huge advantage; their susceptible rivals die off, and they can finally take over.
“That’s our in,” says Wale. “Competition is the force that keeps resistance down in nature. Maybe we can harness that competition to keep them down before they even get going.” She and her colleagues, led by Andrew Read at Pennsylvania State University, have devised a way of preventing the evolution of drug-resistant microbes, by putting them at a competitive disadvantage even when antibiotics are around.
The team proved this concept by studying mice infected with malarial parasites. When Wale and her colleagues treated the mice with the drug pyrimethamine, resistant parasites emerged as expected. But these parasites have a weakness: They’re uniquely hungry for a substance called PABA, which they convert into folate, an essential nutrient. Normally, malarial parasites have other ways of making folate. But these alternatives are shut down by the same mutations that make the parasites resistant to pyrimethamine. So when the parasites evolve to resist the drug, they also become uniquely dependent on PABA for their folate-making needs.
When Wale deprived them of PABA, she not only delayed the emergence of resistant parasites, but completely prevented it. “I was bowled over,” she says. “I plotted the data, and I was sitting on my bed, shaking slightly.”
It’s not that the lack of PABA starves the resistant parasites outright; instead, it makes them less competitive than the susceptible ones. When Wale infected the mice only with resistant parasites, they still became sick. But whenever she infected them with both resistant and susceptible ones, the latter always took over, allowing the pyrimethamine to do its job. That’s encouraging, Wale says, especially because she used tens of thousands of resistant parasites in these competitive experiments—far more than would normally exist when they first emerge in the real world. “Even when the horse has bolted and resistance is already here, by intensifying competition, we can buy ourselves more time,” she says.
Here’s the future that Wale envisions. Rather than simply seeing parasites as targets, we view them as organisms in their own right. We work out the nutrients they need, and how those requirements change as they evolve resistance to drugs. We then identify chemicals that deprive them of said nutrients. These “resource limiters” aren’t meant to kill the parasites, but to put the resistant ones at a perpetual disadvantage so a standard antibiotic can finish off the rest. It’s like “developing anti-spinach” to stop Popeye from becoming strong, Wale says.
“It’s promising,” says Heather Hendrickson, from Massey University. And it’s clearly very effective in this particular setup involving mice and malaria. Whether it would work for other superbugs, including bacteria like staph or E. coli, is a matter of detail. “It will really rely on the strength of competition between the resistant and susceptible versions, and the degree to which we can tip the scale in favor of the drug-susceptible ones,” she says.
Of course, it’s possible that microbes will evolve to resist the resource limiters too. But Wale thinks that’s unlikely. Usually, microbes evolve to resist drugs by getting rid of them, neutralizing them, or changing the molecules that they are designed to attack. But those solutions “wouldn’t work against not being given something,” Wale says.
The Plan to Avert Our Post-Antibiotic Apocalypse
If the approach pans out more broadly, it might give us more options for controlling infectious diseases, beyond just developing more antibiotics. That task has become increasingly difficult. Only a few new antibiotics are in development, and no major new types have emerged for decades. But there are plenty of potential resource-limiting drugs around. They’re often ignored because they don’t kill microbes outright, but they don’t need to be lethal to thwart the emergence of resistance. If scientists can identify these substances, and pair them with existing antibiotics, they could prolong the usefulness of our current arsenal.
“It’s going to be very challenging to find these types of [resource-limiting] compounds,” says Tara Smith, from Kent State University. For example, scientists have long talked about using substances that soak up the metals that microbes require, but that idea “is still more theoretical than practical.” Still, “it’s a good example of the outside-of-the-box thinking we need to preserve antimicrobials.”
“This idea of having something that goes along with an antibiotic and reduces the likelihood of resistance is a very productive field,” says Ramanan Laxminarayan, from the Center for Disease Dynamics, Economics, and Policy. Some groups are working on substances that stop antibiotics from reaching the gut, and fomenting the evolution of resistant superbugs there. Others are developing chemicals that attack resistance genes directly, transforming superbugs back into their civilian alter egos. “These are all different strategies to just coming up with new antibiotics, and I think this is obviously the right way to go,” Laxminarayan says.
“This should certainly be at the forefront of the strategies we test and employ,” says Pamela Yeh from the University of California, Los Angeles. “I think our long battle with antibiotic resistance show us that we'll continue to lose ground against resistant pathogens if we don't consider the environment of the pathogen along with the pathogen itself.”


The email about “a most peculiar object” in the solar system arrived in Yuri Milner’s inbox last week.
Milner, the Russian billionaire behind Breakthrough Listen, a $100 million search for intelligent extraterrestrial life, had already heard about the peculiar object. ‘Oumuamua barreled into view in October, the first interstellar object seen in our solar system.
Astronomers around the world chased after the mysterious space rock with their telescopes, collecting as much data as they could as it sped away. Their observations revealed a truly unusual object with puzzling properties. Scientists have long predicted an interstellar visitor would someday coast into our corner of the universe, but not something like this.
“The more I study this object, the more unusual it appears, making me wonder whether it might be an artificially made probe which was sent by an alien civilization,” Avi Loeb, the chair of Harvard’s astronomy department and one of Milner’s advisers on Breakthrough Listen, wrote in the email to Milner.
A day later, Milner’s assistant summoned Loeb to Milner’s home in Palo Alto. They met there this past Saturday to talk about ‘Oumuamua, a Hawaiian word for “messenger.” Loeb ran through the space rock’s peculiarities, particularly its elongated shape, which looks like a cigar or needle—an odd shape for a common space rock.
For Milner, the object was becoming too intriguing to ignore. So he’s decided to take a closer look.
Breakthrough Listen announced Monday that the program will start checking ‘Oumuamua this week for signs of radio signals using the Green Bank Telescope in West Virginia. The interstellar asteroid is now about twice the distance between the Earth and the sun from our planet, moving at a brisk clip of 38.3 kilometers per second. At this close distance, Green Bank can detect the faintest frequencies. It would take the telescope less than a minute to pick up something as faint as the radio waves from a cellphone. If ‘Oumuamua is sending signals, we’ll hear them.
The chance of an alien detection is, as always, small. But it’s not zero. And Milner thinks we should check—just in case—before ‘Oumuamua is gone for good. The object will pass the orbit of Jupiter next year, and by the 2020s will be hurtling beyond Pluto.
“Whether it’s artificial or not, we will definitely know more about this object,” Milner told me, in a video interview last week.
The new observations will likely be welcomed by the many astronomers who have been scratching their heads for weeks over this space rock. ‘Oumuamua seems to smash many of their predictions about fast-moving interstellar objects, and the more scientists delve into the data, the more puzzles they find.
* * *
The glint of ‘Oumuamua was first spotted by the Pan-STARRS survey telescope in Hawaii during its nightly scan of near-Earth objects like comets and asteroids. Its speed and orbit suggested the object was not bound by the sun’s gravity, and was not of this solar system.
At first, astronomers thought ‘Oumuamua must be a comet, based on decades of scientific literature that predicted its arrival. When our solar system was young, the biggest planets wreaked havoc as they swirled into shape and settled into their orbits. Their movements could jostle nearby material so violently that bits of rock and ice would go flying way out into the universe. The easiest objects to eject were those orbiting at the edge of the solar system, where escaping from the sun’s gravity would be easier. In our solar system, there are far more comets than asteroids lurking near the boundary before interstellar space. Astronomers expected these to be the first interstellar objects they saw.
And so astronomers checked ‘Oumuamua for a coma, a tail of evaporated material that trails comets as they pass near the sun and become heated up. They used telescopes that can detect a sugar cube’s worth of material flying off the object every second. But ‘Oumuamua showed no signs of a coma.
This was the first surprise of many.
Unlike the lumpy, potato-shaped asteroids of our solar system, the 400-meter-long ‘Oumuamua is perhaps 10 times as long as it is wide, an extreme aspect ratio that trumps any of the known asteroids. Astronomers don’t know how the universe could have produced an object such as this. Most natural interactions between an object and its surrounding medium favor the creation of rounded objects, Loeb said, like pebbles on a lakeshore made smooth by lapping water.
Further observations of ‘Oumuamua revealed it carried no traces of water ice, which suggests the asteroid is made of rock or perhaps metal. Whatever it is, the material is certainly sturdy. ‘Oumuamua rotates about every seven hours, a rate that would likely cause some rocky objects, nicknamed “rubble piles,” to crumble. ‘Oumuamua even survived a close pass with the sun in September, before it was detected, without breaking apart.
Thanks to its nonspherical shape, the asteroid is tumbling uncontrollably. “If you take an object that isn’t round and you throw it up in the air, it’ll make this complicated spinning motion,” said Jason Wright, an astronomer at Penn State University. “It just doesn’t just spin nicely along one axis.” Wright said a long journey across the cosmos can slow an object’s tumbling, but ‘Oumuamua has shown no signs of stopping its spinning.
“I’m not saying that any of that is necessarily a smoking gun or super exciting,” Milner said of ‘Oumuamua’s unusual properties. “But I think it warrants thorough investigation from a SETI standpoint.”
Karen Meech, an astronomer at the University of Hawaii Institute for Astronomy whose team discovered ‘Oumuamua, has said their observations are “entirely consistent with it being a natural object.” Analysis of the light reflected by the asteroid shows ‘Oumuamua is red, a color that would be expected for rocky bodies exposed to the cosmic radiation of interstellar space for long periods of time.
There are indeed some natural explanations for some of ‘Oumuamua’s weird properties. Some astronomers say ‘Oumuamua could be a contact binary—two objects that drift closer until they touch and fuse at one end—like our solar system’s Kleopatra, a metallic, dog-bone-shaped asteroid.
They suggest any ice on the asteroid’s surface was zapped away by high-energy particles on its journey between stars. Perhaps the asteroid is so hardy because it formed in the inner regions of a solar system, where rock and metal are more commonly found than ice. This would be tricky, since most exoplanets discovered so far orbit extremely close to their parent star, preventing them from flinging debris beyond the star’s pull. But they may have siblings, like our Jupiter and Neptune, lurking in the darkness, doing the work for them.
If ‘Oumuamua has anything exciting to tell us, it’s that our understanding of planet formation needs some work, said Gregory Laughlin, an astronomer who studies exoplanets at Yale University.“We know that planetary systems are extremely common, but the way that their process unfolds seems to be richer than anticipated,” he said.
* * *
The thought of a spaceship being dropped into planetary systems like a reconnaissance mission may sound like the stuff of science fiction. But for Milner, it’s the future. Milner is spending $100 million over 10 years to develop spacecraft technology capable of sending a tiny probe hurtling at one-fifth the speed of light toward Alpha Centauri, the nearest star system to Earth. If Milner succeeds, the 25-trillion-mile trip would be cut from tens of thousands of years, based on our current technology, to a breezy 20 years. Hundreds of these miniature probes would be deployed into the darkness in the hopes that at least one might complete the journey. Perhaps another civilization already had the same idea.
If that kind of technology were available today, Milner said he would send some kind of probe after ‘Oumuamua. “We need some new propulsion technologies to be able to do this,” he said.
The possibility that ‘Oumuamua is an artificial artifact from an advanced civilization is not spoken in whispers in the astronomy community. But there’s a healthy dose of hesitation in their discussions. Scientists must, after all, exhaust every other plausible explanation before considering ET.
“It’s sometimes mentioned in a half-joking way that people say things when they’re not quite sure whether they want you to take them seriously or not,” said Ed Turner, an astrophysicist at Princeton University. He’s intrigued by a potential SETI observation of ‘Oumuamua, but, like most astronomers, he’s not holding his breath.
“If you were betting your house, I wouldn’t bet it on this,” Turner said.
When they speak of ‘Oumuamua, astronomers recall the tale in Arthur C. Clarke’s 1973 novel Rendezvous with Rama. The year is 2131, and astronomers have detected a mysterious object beyond the orbit of Jupiter and classified it as an asteroid. Their observations reveal the object, named Rama, doesn’t follow the orbit of the sun and must have come from outside the solar system. They send a space probe to photograph Rama and find it’s in the shape of a perfect cylinder. A crewed mission is dispatched. When they land, humans discover the asteroid is an alien spaceship, carrying odd, machinelike beings that pay them little attention. There are no signs of the alien civilization that made them. After some tinkering, the human crew disembarks, leaving Rama to speed out of the solar system.
The story shares some tantalizing similarities with the current circumstances. The most skeptical astronomers point out that, aside from its vessel-like shape, ‘Oumuamua doesn’t have any of the characteristics one would imagine for an alien spacecraft. The object has followed an easy-to-predict trajectory through the solar system. Astronomers have accurately plotted its course forward and backward. Wouldn’t an alien spaceship travel at a fraction of the speed of light, and wouldn’t it slow down to take a look at things as it swept by?
“The explanation that this is a directed probe is, in my view, comically unlikely,” said Konstantin Batygin, a planetary astrophysicist at the California Institute of Technology. “This is just a chunk of debris. I think there’s nothing more to it than that.”
Astronomers predict many more interstellar asteroids, perhaps thousands, are coasting through our solar system, out of view of our telescopes. Pan-STARRS, a survey telescope designed to observe the entire visible sky night after night, found ‘Oumuamua after only four years of operation. Turner suspects the discovery—a fairly quick one—is not a case of pure luck, but a sign of more to come.
Some interstellar asteroids may be hiding, overlooked, in the archival data of Pan-STARRS. Many more will be spotted as other powerful survey telescopes, like the Large Synoptic Survey Telescope in Chile, come online in the next few years.
For Milner, it’s worth examining every interstellar object for signs of artificial technology. They could all be nothing more than space rocks, mindlessly plowing ahead. Or they could be the needle in the haystack. “It would be difficult to work in this field if you thought that every time you looked at something, you weren’t going to succeed,” said Andrew Siemion, the director of the Berkeley SETI Research Center who leads the center’s Breakthrough Listen Initiative.
And so, starting Wednesday at 3 p.m. Eastern Time, the Green Bank Telescope will aim at the first known interstellar object in our solar system. The telescope will observe the asteroid for 10 hours across four bands of radio frequency. The results may be made public within a matter of days.
Milner knows the odds are against him. But as he spoke from a conference table in his home, flanked by screens filled with radiant telescope images stretching from floor to ceiling, he smiled in excitement.
“If you look more, everywhere, I think chances are that eventually you will find something,” he said.


When one person asks another a question, it takes an average of 200 milliseconds for them to respond. This is so fast that we can’t even hear the pause. In fact, it’s faster than our brains actually work. It takes the brain about half a second to retrieve the words to say something, which means that in conversation, one person is gearing up to speak before the other is even finished. By listening to the tone, grammar, and content of another’s speech, we can predict when they’ll be done.
This precise clockwork dance that happens when people speak to each other is what N.J. Enfield, a professor of linguistics at the University of Sydney, calls the “conversation machine.” In his book How We Talk, he examines how conversational minutiae—filler words like “um” and “mm-hmm,” and pauses that are longer than 200 milliseconds—grease the wheels of this machine. In fact, he argues, these little “traffic signals” to some degree define human communication. What all human languages have in common, and what sets our communication apart from animals, is our ability to use language to coordinate how we use language.
I hopped into the conversation machine with Enfield for a very meta chat about the big impacts of tiny words and pauses on human interaction. An edited and condensed transcript of our interview is below.
Julie Beck: Can you explain what the “conversation machine” is and why it’s unique among animals?
N.J. Enfield: When we’re having a conversation, because of the entirely cooperative nature of language, we form a single unit. Certain social cognitions that humans have—the capacity to read other people’s intentions and the capacity to enter into true joint action—allow us to connect up to each other in interaction and ride along in this machine.
Obviously, animals communicate in a range of interesting and complex ways. But where I draw the line is the moral accountability that humans have in interaction. If one person doesn’t do the appropriate thing, for example not answering a question when it’s being asked, we can be held to account for that. We don’t see that in animals. [In humans], one individual can say: “Why did you say that?” Or “Please repeat that.” You don’t see animals calling others out for their failures, asking why did they say that, or could they repeat that. [What’s unique in humans] is the capacity for language to refer back to itself.
Beck: It seems like conversation is always operating on two levels. One is we’re talking about whatever it is we’re talking about, and at the same time, on this more meta level, we’re monitoring the conversation itself and steering it in the direction we want it to go.
Enfield: Exactly. In the book, I mention a psychologist by the name of Herb Clark, at Stanford. He’s made the point for years that language is a tool for coordinating joint action. Let’s say you and I are moving house. All day we’re going to be using language to coordinate our activity. When we lift up a table, we’ll say, like, “One, two, three, lift.” We’re using language to coordinate our physical activity. Herb Clark points this out, and then he says language is used in exactly that way to coordinate the very activity of using language. We might be talking about a subject like what are we going to do on the weekend or whatever, but at the same time we’re using all these traffic signals to coordinate the activity of talking itself. We’re sending little signals like, “Wait, I’m not ready to finish my turn yet,” “What was that? I didn’t catch what you said,” “Yes, I’m still paying attention to you.” Language regulates itself.
Beck: One of the ways we do that is how quickly people respond to each other, right? You write that people usually respond to each other in a conversation within 200 milliseconds. What if you take longer to respond? What signal does that send?
Enfield: It could mean a few things. The fact that this is average, 200 milliseconds, suggests people are aiming for that. So if you are late, it suggests you were not able to hit that target because of some trouble in finding the words you wanted. Or maybe you didn’t hear what was said, or maybe you were distracted in some way. That delay is caused directly by some kind of processing problem. And if you ask people difficult questions, their answers will tend to be delayed
One of the big traffic signals that manages that is these hesitation markers like “um” and “uh,” because they can be used as early as you like. Of course, they don’t have any content, they don’t tell you anything about what I’m about to say, but they do say, “Wait please, because I know time’s ticking and I don’t want to leave silence but I’m not ready to produce what I want to say.”
There’s another important reason for delay, and that is because you are trying to buffer what we call a “dis-preferred response.” A clear example would be: I say “How about we go and grab coffee later?” and you’re not free. If you’re free and you say, “Yeah, sure, sounds good,” that response will tend to come out very fast. But if you say “Ah, actually no, I’m not really free this afternoon, sorry,” that kind of response is definitely going to come out later. It may have nothing to do with a processing problem as such, but it’s putting a buffer there because you’re aware saying “No” is not the thing the questioner was going for. We tend to deliver those dis-preferred responses a bit later. If you say “no” very quickly, that often comes across as blunt or abrupt or rude.
The way we play with those little delays, others are very sensitive to what that means. A full second is about the limit of our tolerance for silence. Then we will either assume the other person’s not going to respond at all, and we just keep speaking, or we might pursue a response.
Beck: Maybe I shouldn’t tell you this, but one of the things that they tell you to do if you’re doing an interview is to just wait. If they’re not responding, just sit there quietly, because people get uncomfortable and then they just keep talking.
Enfield: Exactly. The interesting thing about it is you as an interviewer have to suppress quite a strong tendency to jump into that space. It’s a skill you’ve got to learn to do. I think people naturally don’t feel comfortable with that silence. Once you’ve got that one second going by, somebody’s got to do something. Unless it’s a situation where you’re with your loved ones in your house or you’re on a long car drive or something like that. Obviously, we can lapse into silence and that’s not a problem, but if we’re in the middle of a to-and-fro conversation, we’re generally not going to let that happen.
Beck: So I’m going to transcribe this Q&A later, and I’m going to edit all of those filler words like “um” and “uh” and “well” out of this interview, as I always do. But you write that these words are actually extremely important to conversation. What am I going to lose by cutting all of that out of this transcript?
Enfield: I think it’s the right thing to do, to edit it out when you write things down. You’re not going to lose anything too significant, and the reason is you’ve changed the context completely in which people are going to consume those words. At the moment, the words I’m producing are being interpreted by you in real time. Things never come out perfectly, and we have to edit on the fly. That’s what these words do. What they’re doing is telling you, “No, that word is not what I meant, I’ve doubled back and I’m now going to replace that word with this word.” Or, “Wait a second, I’m about to get the word I’m looking for.” But as soon as you transcribe those, people are not consuming the words at the same time and place as I’ve created them. Those “ums” and “uhs” just become superfluous.
Beck: So you don’t need the words that you use to edit yourself anymore because I’m literally editing you?
Enfield: Exactly. The thing about my book is that as a reader, you don’t know how many times I’ve rephrased a sentence. But you can’t hide that from someone in interaction because you’ve got the time pressure of turn taking. What we’re doing, it’s messy, there’s no getting around that. And that is completely hidden from view when you write something down and publish it because no one’s going to get access to all the drafts. But conversation is all draft.
Beck: Another thing you mentioned that I thought was super interesting was the way people use “um” as a way to claim more conversational space for themselves. Can you talk about that?
Enfield: In any form of interaction, we don’t have access to each other’s minds. It’s the classic problem of human life in a way. Things like “ums” and “uhs” signal there’s some delay in processing. But as a speaker, what I can do is exploit those kinds of signals. I can use them dishonestly. I can use something like “um” to give the overt signal that I’m having some sort of trouble with processing, but in reality, all I’m doing is trying to claim more ground and get you to keep waiting for me to finish.
All words can be used to lie. Whether they’re nouns and verbs, or whether they’re traffic signals, we can exploit them in dishonest ways. If you want to game the system, and all you want to do is hold the floor, then words like “um” can be exploited in that way. Obviously, there are limits to it. People are sensitive to these things, and after a while if you’re trying to dominate the floor, people will either wise up and grab it back or they will just get sick of you.
Beck: Another thing I do in interviews all the time, that I’m doing right now and I’m also going to cut out of the transcript, is I say “mm-hmm” a lot while the person is talking. It makes sense; it’s me just signaling that I’m still listening. But how important is that to our experience of conversation? If I wasn’t “mm-hmm”-ing, would that make a difference to how our conversation goes?
Enfield: Yeah, it would make a big difference. When you’re saying “Mm-hmm, uh-huh,” you’re really playing an important role in the smooth operation of this conversation machine. In the book, I talk about a study done by Janet Bavelas in Canada, with her colleagues. They brought people into the lab, they asked them to get into pairs, and they’d just randomly nominate one of them and say, “Think of a near-miss scenario you had and tell that to the other person.” The listener will look at them, they’ll nod, say “Uh-huh, mm-hmm,” and when the person gets to the punchline, they’ll say things like “Wow.”
Then they had a special condition where they tried to distract the listener. They said, “You have to press this button underneath the desk every time the person who’s telling their near-miss scenario uses a word that begins with the letter T.” It completely distracted the listeners from actually following along the content of the story. They produced many fewer of those “uh-huhs” and “mm-hmms.” It also meant the timing of them was kind of out of whack, and they didn’t really recognize when the speaker had reached the climax of the story—the moment when they’re supposed to say “Oh, wow.” They showed that when you distract the listener, then the storyteller tends to circle back and repeat themselves. They essentially become a less proficient and less fluent storyteller. It was a powerful demonstration of precisely the importance of those types of feedback markers for the performance of the person who is telling the story itself.
Beck: You talk a lot in the book about the “moral architecture” of conversation. Explain what that means in the context of these little traffic signals. What does using words like “um” have to do with morality?
Enfield: Morality’s a strong word. When you use that word, people think, “Oh you’re talking about is it okay to have sex with animals” or whatever. Thinking about grand moral questions. I’m talking about a much simpler code. In general, morals tell us how we should live. In the moral architecture of language, they tell us how we should talk. What the moral code does is it licenses us to hold other people accountable to that code. Like, “Hey I asked you a question,” that would be an example. I might not be saying it explicitly but I’m implying, “That’s bad. You shouldn’t be silent when I’m asking you a question, you should respond.”
When it comes to little words, I produce “um” and “uh” as a signal to you that I know I should be speaking right now. The right thing to do is to be speaking fluently, moving the conversation forward. The whole motivation for my producing those little traffic signals is to make clear that, despite current appearances, I am aware of and I’m following the basic stipulations of what it takes to produce an appropriate conversation. It’s that whole moral architecture that human beings have, it’s the root of so much of our cultural life and our social life: the defining of what’s appropriate, what’s inappropriate, and policing those things and judging others on the basis of those things. And in these extremely subtle ways it’s right there in every conversation that we have.



Olive oil is not what you think it is. According to Tom Mueller, the author of Extra Virginity: The Sublime and Scandalous World of Olive Oil, an olive is a stone fruit like a plum or cherry—meaning that the green-gold liquid we extract from it “is, quite literally, fruit juice.” And, while we’re blowing your minds, have you ever stopped to wonder what “extra virgin” means? “It’s like extra dead or semi-pregnant,” Mueller said. “I mean, it doesn’t make any sense at all.” This episode we visit two groves—one in the Old World, one in the New—to get to the bottom of olive oil’s many mysteries. Listen in this episode as we find out why the ancient Romans rubbed it all over their bodies, and whether the olive oil on our kitchen counters really is what it says on the label.
Olive oil’s original home lies along the shores of the Mediterranean, where its wild ancestor, the oleaster, can still be found today. Somehow, people realized that the bitter berry from these hardy trees tasted excellent when brined in salt and, even better, could be crushed to produce a liquid fat that was not only delicious but, Mueller says, burns as hot as benzene and has twice the heat-energy content of carbon. By the seventh century BCE, olive-oil production was taking place at industrial scale: Olive presses excavated at Ekron, in modern-day Israel, were capable of producing 500,000 liters of oil a year. The demand was equally enormous: Olive oil powered lamps and preserved and enhanced food, and it was used as an all-purpose medicine, a contraceptive—even an aphrodisiac. Olive oil was so critical to Greek and Roman culture that wars were fought over it and fortunes made, much like the petroleum sheikhs of today.
Today, olive oil is more popular in the kitchen than in a lamp, but it still enjoys a superior status to its fellow fats—particularly since the 1980s, when it began to be promoted by medical researchers as a key component of the heart-healthy Mediterranean diet. But all is not well in the olive groves. In Italy, millions of olives have already been killed by Xylella fastidiosa, an insect-borne pathogen that was detected in 2013. The disease can cause mature trees to die of thirst within two years. We speak to Rodrigo Almeida, a professor of environmental science at the University of California, Berkeley, to find out how worried we should be about the future of Italian olive oil. Meanwhile, olive-oil fraud is rampant in general, but especially in the United States, as profits from fraudulent oil can be more lucrative than dealing cocaine.
But fear not: We won’t leave you on this depressing note. Instead, olive-oil growers Anna Casadei of the Castello del Trebbio in Tuscany, and Kathryn Tomajan and Robin Sloan of Fat Gold in Sunol, California, lead us through harvesting, milling, and, most importantly, tasting, in order to equip us to buy, use, and love olive oil nearly as much as they do. Listen in now, and then enjoy a big glug of oil over your veggies—heck, we won’t judge if you want to rub it on your skin, too!


When Mike Brown first proposed that a hidden, massive planet lurks in the outer reaches of our solar system, he was confident someone would prove him wrong. “Planet Nine,” as the hypothetical world was nicknamed, was his explanation for the strange movements of half a dozen distant, icy planetoids that are farther away and smaller than Pluto: In theory, this huge, somehow-undiscovered planet could sway their orbits. But surely astronomers would be quick to find a more obvious explanation.
“Shockingly, in a year and a half, nobody has,” says Brown, an astronomer at the California Institute of Technology. “There have been so many claims of planets in the last 170 years, and they were always wrong. But I’m clearly a true believer at this point.”
Brown, the self-titled “Pluto Killer” who led the campaign that demoted the dwarf planet, and Konstantin Batygin, his coauthor at Caltech and a young star who plays in his own rock band, know how to spark debate. Since their proposal about Planet Nine, the lack of definitive evidence for or against its existence has divided the planetary community. Other astronomers have put forth alternative explanations, and some contend Brown and Batygin’s data are biased. Until someone clearly spots the new mystery planet in a telescope, they’ve come to an impasse.
The peculiar clustering of the six faraway objects that Brown and Batygin’s initial hypothesis highlighted is extremely unlikely to happen just by chance. According to the duo’s mathematical arguments, it would be naturally explained, though, by a planet about 10 times as big as Earth in the region known as the Kuiper Belt. Batygin has come up with more recent evidence, too: The orbits of other distant solar-system objects yoked to Neptune have gotten “detached” as well, and other objects’ orbits have gotten tilted to the side or reversed, so the solar system as a whole no longer resembles a thin record or CD with the sun at the center. Planet Nine, if it exists, could explain all of those phenomena.
“When all of these things come together, it’s becoming evident that without Planet Nine, the solar system has these weird puzzles and features that stand out,” Batygin says. His approach echoes that of astrophysicists when they inferred the existence of dark matter—which still can’t be seen—based on the rapid motions of stars in the outer realms of galaxies, and then clinched the idea with a more diverse array of evidence.
But other experts remain skeptical. Even the most basic facts are in dispute. Scientists with the Outer Solar System Origins Survey, or OSSOS, argue that Brown and Batygin’s data are actually biased by factors like bad weather and their telescope’s location, which could influence what’s seen and what’s missed, thereby introducing a spurious trend. If this is true, then there’s nothing weird about the little balls of rock and ice in the first place. The OSSOS researchers say these objects mostly seem randomly oriented and not aligned by some unseen force. If there’s no clustering, then Planet Nine’s linchpin disappears.
“While my research is skeptical of this planet, that’s not at all to say there isn’t a planet out there,” says Cory Shankman, an astronomer at the University of Victoria who led the research. He advocates for continuing the search for these hard-to-detect objects and understanding the biases while doing it. It’s slow, painstaking work.
Shankman’s survey only covered one-twentieth of the sky, however. Other astronomers, such as those affiliated with the Dark Energy Survey, question their findings, just like Shankman questions Brown and Batygin’s. “The more objects you can find that bear on the story, the easier it is to talk about them as a population rather than a small handful of curiosities” says David Gerdes, a University of Michigan astrophysicist. One way or another, he believes, the answer will be clear within the next year or two.
A theory’s more powerful if it doesn’t just explain what’s already known, but also makes successful predictions about things that haven’t been seen yet. If scientists find more objects throughout the Kuiper Belt and these objects are hardly clustered at all, it will deal a blow against Planet Nine. If the objects are similarly clustered as Brown and Batygin expect, it will strengthen their case.
And yet, another possibility remains, which is that the only solutions people find are the ones they have access to, like the proverbial story of the person who lost their keys at night and only looks for them under the streetlight. “Scientists are often good at doing contrastive assessments, like a Sherlock Holmes–style argument: Here’s my suspects, and here’s the one most likely to have done it,” says Christopher Smeenk, a philosopher of science at the University of Western Ontario. “But do you have the right list of suspects?”
There have been many planetary misses in history, Smeenk points out, such as 17th-century claims of a moon orbiting Venus, which better data demonstrated not to exist. Two centuries later, astronomers attributed Mercury’s slightly peculiar orbit to the gravitational forces from an unseen inner planet, dubbed Vulcan. But when Albert Einstein’s theory of general relativity came along, it explained the orbit, debunking Vulcan claims.
In the case of Planet Nine, Ann-Marie Madigan, an astrophysicist at the University of Colorado Boulder, believes everyone has missed a key suspect. The gravitational forces in the outer solar system could be more complicated, and the unlikely alignment of those icy bodies could all be a temporary coincidence.
She argues that there are millions, if not billions, of planetoids—more than previously thought—orbiting in that distant, dusty disk of material around our solar system. Most astronomers have assumed that the forces of these tiny objects are so small that they can be ignored in models, and it’s difficult to model their behavior. But Madigan includes them all in her models, and has found that, as they orbit over and over again, the nebulous pull of their gravity gently and gradually clumps some objects together over time. This “self-gravity” mechanism, as she calls it, could explain the other lines of evidence brought forth by Batygin as well.
“People think gravity’s dominated by Jupiter, Neptune, Uranus, and Saturn, and they’re not really thinking of the collective effects of all the smaller bodies,” Madigan says. “The main criticism I get from Planet Nine advocates is that there’s no evidence for such a mass of small bodies. But I don’t pay too much attention to that, because they haven’t seen Planet Nine yet either.”
Incidentally, both Batygin and Madigan invoke the principle of Occam’s razor, the notion that the simplest explanation is likely the correct one. But they come to completely different conclusions, highlighting that this seemingly straightforward principle is actually rather complicated, with no clear answer yet in sight.


On Wednesday, a team of scientists unveiled a newly discovered dinosaur that had the body and sickle-clawed feet of Velociraptor, the head and snout of a swan, and weird arms that were somewhere between grasping limbs and flattened flippers. This bizarre murder-swan, which the team christened Halszkaraptor, was so odd that when they first saw it, they suspected that it was a fake—a Frankensaur that had been assembled from parts of different dinosaurs. “All of us thought, when we first saw it: Oh come on now,” says Philip Currie from the University of Alberta.
But after using a particle accelerator to scan the animal, and the rock in which it is still encased, Currie and his colleagues are convinced that it’s the real deal.
Apparently This Is What a Swimming Dinosaur Looks Like
Not everyone is so sure, though. When Steve Brusatte from the University of Edinburgh first saw a picture of Halszkaraptor, his spidey sense also started tingling. Its posture, from the curve of its tail to the way its claws were almost perfectly fanned out, looked strange, as if it was ready for display. Its body looked like that of Velociraptor, but its skull looked like one from a different dinosaur group—the alvarezsaurs. And most worryingly of all, the specimen has a convoluted history. It was poached from Mongolia (as many dinosaurs are), and smuggled into Japan and Britain, before ending up in a private collection in France—a meandering route that offered few reassurances and many chances for tampering.
“This new discovery has thrown a lot of us for a loop,” he says. “It’s either really a new dinosaur, which would be awesome, or it’s been tampered with and I really hope that’s not the case. The authors worked really hard to demonstrate that it hasn’t, using the best tools at their disposal. But I wonder if we have the right tools, since the fakes are so sophisticated.”
No matter their views on Halszkaraptor, everyone I spoke to agreed that fossil forgery is a surprisingly serious problem. There’s a long history of hoax fossils, like the infamous Piltdown Man—a human skull that was glued to an orangutan’s jaw, and presented as a missing link in human evolution. That was in 1912, but a century on, fake or doctored specimens still abound in fossil shows, private collections, and even in small regional museums.  
These bogus specimens aren’t cries for attention. “In my experience, I’ve never seen a single specimen that was made by the scientific community to get more publicity,” says Mark Norell, a curator at the American Museum of Natural History. Instead, they’re almost always the work of people trying to earn a buck—often poor farmers in China.  
There is a lucrative market in fossils, which owes much of its existence to Sue, a beautifully preserved Tyrannosaurus that was unearthed in 1990 and auctioned for $7.6 million. After her sale, “a modern gold rush began, and it has not let up,” wrote Paige Williams in The New Yorker. Fossils became status symbols, attracting wealthy buyers and unscrupulous vendors. An extensive black market arose, connecting fossil-rich sites like China and Mongolia and anyone with money to burn and a penchant for trophies from another age.  
The fakes are sometimes created from whole cloth—or rather, from whole plaster. These are usually crude and easy to recognize. Currie remembers seeing photos of an alleged Archaeopteryx—a pivotal species which helped to show that birds evolved from other dinosaurs. It was in Beijing, but after Currie flew over, “within three seconds of seeing it, I knew it was 100 percent fake,” he says. The weight of it was wrong. The texture of the bone was wrong. No actual paleontologist would be fooled, but Currie found several such fakes in tourist shops, all with expensive price tags.
In many ways, the market in forged fossils is similar to the one in forged artworks, says Norell, who has a foot in both worlds. When he attends the gargantuan annual fossil show in Tuscon, he takes the same equipment that he uses to examine fine art, like a special ultraviolet light. “I have a carbide needle that I can heat up with a lighter and push onto something,” he says. “If it’s real, guys will let you do it. If it has epoxy, they won’t.”
Some phonies are harder to spot. Currie recalls seeing a Tarbosaurus—a Mongolian relative of Tyrannosaurus—that “looked pretty good and was sold for hundreds of thousands of dollars.” When his team analyzed it with a medical scanner, they showed that just a single bone in the jaw was real. “It was convincing enough that we couldn’t be sure how much was real and how much wasn’t.”
More often, actual fossils are touched up to make them more dramatic. Another Tarbosaurus, which was famously seized by the American government in 2012, was mostly real, but its teeth, fingers, and toes had all been reconstructed. Feathered dinosaurs are all the rage, so artisans will sometimes paint the outlines of feathers on their fossils. One dinosaur had a crest added to its skull by fossil-poachers, who wanted to make it look more dramatic and complete; scientists spent so much time removing the extra material that, in a bit of questionable victim-blaming, they named the beast Irritator. And by some estimates, around 80 percent of the marine reptile specimens that are on display in local and regional Chinese museums have been “altered or artificially combined to varying degrees.”
The vast majority of sham fossils are chimeras—two or more actual specimens that have been glued together to make them seem new. Sometimes, the work is ... not well-researched. Currie once saw a Confuciusornis—a crow-sized Cretaceous bird—that was clearly fake because its foot had been glued directly to its knee, with no shinbone in the middle. It was on display in a Chinese museum.
But these pseudosaurs can be far more convincing. In the Gobi desert of Mongolia, dinosaurs are often found in soft sandstone. Fakers have been known to crumble this rock and then re-harden it to glue separate specimens together. “It can be really hard to tell if that’s fake,” says Brusatte. “Is that original sandstone or sandstone that’s been put back together?”
Mostly, these counterfeits have little impact on what we know about dinosaurs. They don’t feature in high-profile papers, and they stay out of the limelight. But there are exceptions. In 1998, the owners of a small Utah museum acquired a specimen from China. The beast was never described in a scientific journal, but that didn’t stop National Geographic from hosting a press conference about it, or featuring it in the magazine as Archaeoraptor—a “true missing link in the complex chain that connects dinosaurs to birds.”
“The specimen was suspicious right from the beginning,” says Currie, who was one of the scientists called in to examine it. The tail didn’t seem to connect to the body. The feet were exactly symmetrical in a way that real limbs just aren’t, as if one had been made using the other as a template.
It turned out that Archaeoraptor was the dinosaur equivalent of a horse costume, with the front and back halves played by different actors. Someone had glued the head and upper body of Yanornis (a primitive bird) to the tail of Microraptor (a four-winged Velociraptor cousin) and the legs and feet of an unidentified animal. Some missing parts had been filled in with a paste that was made from ground-up bone. It looked like bone at first glance because it was, but a medical CT scanner revealed the trickery: The fakes lacked the complex internal structures of real bones.
“The Archaeoraptor fiasco really brought the issue home to dinosaur workers,” says Brusatte. “There hasn’t been a big forgery like that in dinosaurs in the last 15 years. But we’re due for something like this to come up again and I just hope that it’s not me who’s fooled by it.”
He has reason to worry because by his own admission, some of the fakes are “really, really good”—good enough to fool even the trained eye. And the wider problem is that no one really knows which techniques are good enough to distinguish fossils from faux-sils. CT scans helped to pierce the Archaeoraptor illusion, but “looking at a CT scan is oftentimes like looking at a Rorschach test, or at tea leaves,” says Brusatte. “You can see many things in there.” Would it be good enough to detect sandstone that had been joined together with reconsolidated sandstone? Who knows?
That’s why Currie and his colleagues scanned Halszkaraptor using a synchrotron, producing scans that are much higher in resolution. “You can look at the continuity of the bones and the rocks, and whether there’s any gluing or infilling,” he says. They could even show that the chemical composition of the rock is consistent across the fossil. To fake that, someone would have had to find two specimens from the same site, and join them together using rock that also had the same origins.
Currie is highly respected among his peers, and his name gives weight to the claims around Halszkaraptor, even among the skeptics. I asked him how certain he is that the specimen is real, on a scale of 1 to 10. “I’m at least a 9,” he tells me. “You can never say that you are 100 percent sure, because some of these people are really incredible artisans.”
“You’re running to catch up with the forgers,” says Brusatte. “I don’t know what the definitive evidence would be either way, or what sort of burden of proof would be enough. The only thing that can prove that it’s genuine is another team finding another skeleton and digging it up themselves. And I hope that happens.”


I never thought we were going to die. Even when the canyon air filled with smoke, when the flames came rushing up, when darkness fell and the sky glowed red both behind and ahead of us. So, okay, it was a little scary. But we were just a short drive from Portland, Oregon, on a well-traveled trail my family had hiked a dozen times in the last 10 years. No one dies in a forest fire when they’re that close to home. We weren’t outdoorsy enough to die in a forest anywhere. Or so it seemed to me.
On the West Coast, the 2017 onslaught of forest fires has been widespread and relentless—a char stretching from South Cariboo, British Columbia, last summer to the Caravaggio exhibit in the Getty Center just above west Los Angeles Thursday. Blazes are striking with growing regularity in the region, sparked in part by drought and record-breaking heat. Seven of California’s 10 largest modern wildfires have come in past 14 years.
The news coverage of these fires plays like a disaster movie. Forested hills wrapped in a devil’s fiery cloak. Well-tended homes reduced to scorched concrete and melted bikes. By this point, Americans are used to watching with a mix of horror and curiosity. But as the frequency of wildfires increases, it’s also more likely for people on the West Coast to find themselves in their paths—and not always because they’re away from home.
My experience began on a sunny Saturday afternoon in early September. Labor Day was the first time my wife and I had all three of our busy, nearly grown kids with us since Christmas. Figuring we’d spend the day together romping around Oregon’s natural playground, we drove 40 miles east to the Eagle Creek Trail, a path that follows a waterfall-clotted river on an uphill climb toward the richly forested Bull Run watershed.
When we arrived just after noon, the parking lot was so crowded that we had to double back and bootleg a spot on the side of the road. Setting out through a thicket of multigenerational tourist families, taut hikers, cooler-toting beer dudes, toddler-chasing couples, and dozens of other Oregon types, we continued for three breezy miles, had a shady lunch at the High Bridge, and after an hour or so headed to the Punch Bowl Falls swimming area for a cooling dip. We were back on the trail at 4 p.m. for the gentle two-mile stroll down to the car. A sweet end to a lovely afternoon, right until one of my sons, Teddy, came sprinting back from walking a few hundred feet ahead of us.
The trail was on fire, he shouted. In fact, the entire hillside was ablaze.
Thinking dad thoughts, I made a few steps forward to check it out, but Teddy put his hand up to my chest. You do not want to get closer.
Twenty minutes earlier, a few feckless teenagers had tossed a smoke bomb from the trail, and hell had been unleashed. Our cheerful family outing had sent us into the maw of a deadly siege. But that was absurd. So I sighed and girded myself for something more reasonable: an unexpected pain in the ass.
Back at Punch Bowl Falls, we spread the word among the 100-plus hikers, swimmers, and out-of-town visitors and joined them all in standing meekly on the riverbank watching the thin yellow haze of smoke grow into a seething black curtain. Forest Service choppers buzzed in to drop water on the blaze, made no impact, went off to reload. A smaller helicopter fluttered in to drop a note, but the instructions were vague: WE SEE YOU ... STAY PUT ... DANGER! Then that chopper was gone for good.
We stayed put on the riverside. Some teenagers tossed a Frisbee. Parents played with their kids. A few well-put-together women stood together and fretted. One asked me: Is this one of those fires that sucks the oxygen away and suffocates us right here?
I made a face. Of course not.
How do you know?
Because it just isn’t.
Denial is definitely not a smoke-wreathed river in Oregon, but I had to maintain some level of control of the situation. Not far away a ball of fire shot through the dense black smoke.
I kept peering past the tree line, waiting for some kind of cavalry to come galloping in. Instead, a 30-ish fellow with short dark curls and a beard climbed up on a boulder and called out for attention. He was Technical Sergeant Robert Dones of the U.S. Air Force, a veteran of two tours in Afghanistan. After scoping out the movement of the fire, he had some ideas. The flames were too close for us to stay on the riverbank. We needed to get out now, and no matter how fast, how slow, how young or old, we were all going together. We would stay together until we were all safe.
The cavalry, it turned out, had been among us the whole time.
Off we went, away from one fire and bound for another trail that the map said would lead to a road where rescuers would be waiting. There was one problem: The path would lead us to the hem of the Indian Creek Fire, a blaze that had been burning out of control since July. There was no way of knowing if the fire would consume the trail before we got there, but given no other options, we crossed our fingers and kept moving.
Through the gloaming and into the darkness, we trooped lit by cellphones across vertiginous shelves of cliffside scree. Five, eight, a dozen miles. Eventually, the canyon air thickened with smoke and the sky just above the western trees went red. The Indian Creek Fire was just over the hill, so close that its embers drifted around us—some tickling new flames from the underbrush. A Forest Service ranger hiked down to help us, and when she came near me, I pointed to a fresh blaze taking hold 100 feet from where we had stopped. “That doesn’t matter,” she replied in a taut monotone.
Two miles beyond the embers, we stopped again to rest for a few hours. My family had no clothes beyond our T-shirts and shorts, and only 10 carrot sticks left over from lunch. Hungry and exhausted, we huddled together in the dirt and tried to sleep. The plan was to stay until dawn, but the Eagle Creek fire picked up speed in the night, and the Indian Creek blaze was closing in, too. We were rousted an hour before dawn into a thick, scratchy fog of smoke. After an hour or two of hiking, we started to meet Forest Service firefighters carrying their axes and water jugs to the front of the Indian Creek fire.
Three hours later, we got to Wahtum Lake and trudged up the officially named Stairway to Heaven, a long, steep set of wood-and-dirt stairs, to find two dozen rescuers, a table full of military MREs, and school buses bound for safety.
Too exhausted to feel much of anything beyond relief, we located places on the bus and sat numbly through the hour-long ride back to where we’d left our cars less than 24 hours earlier. We were home 90 minutes after that, finding everything just as we had left the morning before. I took off my boots and hosed them down, trying to wash the smoke and ash and filth, and left them on the back porch to dry.
The next morning, I woke up to see smoke hanging above our yard. Taking my cup of coffee out to the back porch, I found my boots where I’d left them, just as ash-covered as they’d been before I’d hosed them out the day before. The winds had changed direction. We’d made it home, but the fire was still coming for us.


When Mark DePristo and Ryan Poplin began their work, Google’s artificial intelligence did not know anything about genetics. In fact, it was a neural network created for image recognition—as in the neural network that identifies cats and dogs in photos uploaded to Google. It had a lot to learn.
But just eight months later, the neural network received top marks at an FDA contest for accurately identifying mutations in DNA sequences. And in just a year, the AI was outperforming a standard human-coded algorithm called GATK. DePristo and Poplin would know; they were on the team that originally created GATK.
It had taken that team of 10 scientists five years to create GATK. It took Google’s AI just one to best it.
“It wasn’t even clear it was possible to do better,” says DePristo. They had thrown every possible idea at GATK. “We built tons of different models. Nothing really moved the needle at all,” he says. Then artificial intelligence came along.
This week, Google is releasing the latest version of the technology as DeepVariant. Outside researchers can use DeepVariant and even tinker with its code, which the company has published as open-source software.
DeepVariant, like GATK before it, solves a technical but important problem called “variant calling.” When modern sequencers analyze DNA, they don’t return one long strand. Rather, they return short snippets maybe 100 letters long that overlap with each other. These snippets are aligned and compared against a reference genome whose sequence is already known. Where the snippets differ with the reference genome, you probably have a real mutation. Where the snippets differ with the reference genome and with each other, you have a problem.
GATK tries to solve the problem with a lot of statistics. DNA-sequencing machines sometimes make mistakes, so the GATK team studied where the machines tend to made mistakes. (The letters GTG are particularly error-prone, to give just one example.) They thought long and hard about things like “the statistical models underlying the Hidden Markov model,” per DePristo. GATK then gives its best guess for the actual letter at a certain location in DNA.
DeepVariant, on the other hand, still does not know anything about DNA-sequencing machines. But it has digested a lot of data. Neural networks are often analogized as layers of “neurons” that deal in progressively more complex concepts—the first layer might respond to light, the second shapes, the third actual objects. As DeepVariant is trained with data, it learns which connections between “neurons” to strengthen and which to ignore. Eventually, it can sort the actual mutations from the errors.
To fit the DNA-sequencing data to an image-recognition AI, the Google team came up with a work-around: Just make it an image! When scientists want to investigate a mutation, they’ll often pull up the aligned snippets, like so:
“If humans are doing this as a visual task, why not present this as a visual task?” says Poplin. So they did. The letters—A, T, C, or G—got assigned a red value; the quality of the sequencing at that location a green value; and which strand of DNA’s two strands it is on a blue value. Together, they formed an RGB (red, green, blue) image.
And then it was simply a matter of feeding the neural network data. “It changes the problem enormously from thinking super hard about the data to looking for more data,” says DePristo.
Between publishing a preprint about DeepVariant last December and the release this week, the team continued improving the tool. Instead of three layers of data—represented by red, green, and blue—at any location in the genome, DeepVariant now considers seven. It would no longer make any sense as an image to the human eye. But to a machine, what’s just a few more layers of numbers?
To be clear, DeepVariant itself is unlikely to change genetics research. It is better than GATK, but only slightly so—and it is half as fast depending on the conditions. It does, however, lay the groundwork for AI’s influence in future genetics research.
“The test will really be how it can translate to other technologies,” says Manuel Rivas, a geneticist at Stanford. New sequencing technologies like Oxford Nanopore are becoming popular. If DeepVariant can quickly learn variant calling under these new conditions—remember the humans took five years with GATK—that could speed up the adoption of new sequencing technologies.
DePristo says that the idea of layering data on top of each location in the genome could easily be applied to other problems in genetics—the more important of which is predicting the effects of a mutation. You might imagine layering on, for example, data on when genes are active or not. DeepVariant started off with just three layers of data. Now it has seven. Eventually it might be dozens. It won’t make much sense to a human brain anymore, but to an AI, sure.


Massive wildfires are raging across Southern California, threatening thousands of homes and cultural landmarks like the Getty Museum in Los Angeles. Some of the largest fires were still barely contained by Wednesday afternoon.
It’s been an unusually bad year for the state—amid an unusually bad year for the West at large. Fires in California have destroyed more than 6,000 structures and incinerated hundreds of thousands of acres. Montana and British Columbia both also had some of their worst wildfire seasons ever.
Of course, most years are bad wildfire years now. Seven of California’s 10 largest modern wildfires have occurred in the last 14 years. (The state began keeping reliable records in 1932.) Given the scale of the blazes, and their increasing regularity, it makes sense to ask: Does global warming have anything to do with this?
The answer isn’t as clear-cut as it was this summer, when drought- and heat-stoked fires raged across the Rockies and Pacific Northwest. Instead, a mix of forces are driving the fires in Southern California, and only some of them have a clear connection to global warming.
“These fires are not immediately emblematic of climate change,” said John Abatzoglou, an associate professor of geography and climate at the University of Idaho, in an email. “Yes, California did have the warmest summer on record. But the big anomaly here is the delay in the onset of precipitation for the southland that has kept the vegetation dry and fire-prone.”
In other words, late-fall and winter rains would normally end California’s fire season in November. Because those rains haven’t yet arrived, the blazes continue.
“At least in Southern California right now, we are largely seeing textbook wildfires,” said Alexandra Syphard, a senior research scientist at the Conservation Biology Institute who studies fires. “Wind-driven fire events occur most typically in the fall, but can also occur like this, later in the year with fast-spreading, ember-driven fires under Santa Ana wind conditions.”
Here are some of the biggest factors that are shaping the wildfires in California—and how global warming is or isn’t changing them:
The Santa Ana winds
Blame for the wildfires in Ventura and Los Angeles counties lies first and foremost with the Santa Ana winds, famously hot and desiccating gusts that blow from the desert to the coast. The Santa Anas also set the stage for the massive wildfires in Napa and Sonoma earlier this year.
Fires depend on two variables—an ignition source and fuel to burn—and the Santa Ana winds increase the availability of both. First, they dry out vegetation, creating more fuel across the landscape. Second, they blow trees and other debris against power lines, providing the source of a spark.
When the Santa Anas blow this late in the year, they can start fires. In fact, writes Abatzoglou, “all December fires in the southland since 1948 have been associated with Santa Ana wind.”
But there are few signs—at least so far—that the Santa Ana winds are becoming more prevalent or that they’re systematically moving later in the year. The peak of Santa Ana season usually comes in September or October. There is no trend toward more or fewer Santa Ana fires—or Santa Ana winds generally—in the historical record, Abatzoglou told me.
A 2006 study from researchers at the University of California, Berkeley, and the Lawrence Berkeley National Laboratory suggested that by the end of the century, Santa Ana winds may become more common. They may also form later in the year, including in December.
La Niña
There’s currently a weak La Niña in the tropical Pacific, which means that global temperatures are cooler than they would be otherwise.
The same phenomenon is also keeping storms from making landfall in Southern California. Normally, California’s wet season would have started by this time of year. “Once [autumn] rains hit the region, fuel moistures recover and make the landscape fire-resistant, thus reducing the odds that a power-line failure or vehicle will start a fire,” said Abatzoglou.
But the rains haven’t yet appeared, he told me. “So far this autumn, much of the southern half of California is pitching a shutout in terms of rainfall to date. Some of this is characteristic of La Niña ... as the southern tier of the United States sees less precipitation during La Niña winters.”
It’s still unclear how climate change will affect the Pacific’s yearly dance between El Niño, La Niña, and a neutral state. A 2015 study in Nature Climate Change found that the Pacific Ocean may careen between extreme states—from an intense El Niño to a monster La Niña—by the end of the century, but more research on the question still needs to be done.
A very cold U.S. East Coast
Even as the West Coast remains warm and dry, the Eastern Seaboard is settling into some of its first cold weather of the season. This pattern—a warm West, a frigid East—is known as the North American winter dipole.
It’s caused when the jet stream—which both ferries storms into the continent and generally divides warm air from cold air—gets especially twisted across North America. It rises far into the Canadian Northwest, keeping most of the western United States warm and dry; then it cascades down across the middle of the country, bringing cold air well into the U.S. Southeast.
This phenomenon prolonged California’s drought during the first part of this decade, keeping any kind of storm system offshore. It also brought the infamous “polar vortex” down into the continental United States.
There are a number of theories about how this pattern comes to form, and most of them revolve around climate change, as Jason Samenow writes at The Washington Post.
Daniel Swain, a climate scientist at the University of California, Los Angeles, argues that the ridge forms in part because the West is warming up much faster than the East. If this is the case, then scientists might expect to see the phenomenon fade in decades to come, as the East Coast catches up to the West.
But a paper published this week in Nature Communications takes another view. It finds that the disappearance of sea ice over the Arctic Ocean could change the circulation of the Pacific Ocean, encouraging the jet stream to veer north. In other words, climate change will make something like the North American winter dipole keep reappearing.


Albert Einstein said that the “most incomprehensible thing about the universe is that it is comprehensible.” He was right to be astonished. Human brains evolved to be adaptable, but our underlying neural architecture has barely changed since our ancestors roamed the savannah and coped with the challenges that life on it presented. It’s surely remarkable that these brains have allowed us to make sense of the quantum and the cosmos, notions far removed from the “commonsense,” everyday world in which we evolved.
But I think science will hit the buffers at some point. There are two reasons why this might happen. The optimistic one is that we clean up and codify certain areas (such as atomic physics) to the point that there’s no more to say. A second, more worrying possibility is that we’ll reach the limits of what our brains can grasp. There might be concepts, crucial to a full understanding of physical reality, that we aren’t aware of, any more than a monkey comprehends Darwinism or meteorology. Some insights might have to await a post-human intelligence.
Scientific knowledge is actually surprisingly “patchy”—and the deepest mysteries often lie close by. Today, we can convincingly interpret measurements that reveal two black holes crashing together more than a billion light-years from Earth. Meanwhile, we’ve made little progress in treating the common cold, despite great leaps forward in epidemiology. The fact that we can be confident of arcane and remote cosmic phenomena, and flummoxed by everyday things, isn’t really as paradoxical as it looks. Astronomy is far simpler than the biological and human sciences. Black holes, although they seem exotic to us, are among the uncomplicated entities in nature. They can be described exactly by simple equations.
So how do we define complexity? The question of how far science can go partly depends on the answer. Something made of only a few atoms can’t be very complicated. Big things need not be complicated either. Despite its vastness, a star is fairly simple—its core is so hot that complex molecules get torn apart and no chemicals can exist, so what’s left is basically an amorphous gas of atomic nuclei and electrons. Alternatively, consider a salt crystal, made up of sodium and chlorine atoms, packed together over and over again to make a repeating cubical lattice. If you take a big crystal and chop it up, there’s little change in structure until it breaks down to the scale of single atoms. Even if it’s huge, a block of salt couldn’t be called complex.
Atoms and astronomical phenomena—the very small and the very large—can be quite basic. It’s everything in between that gets tricky. Most complex of all are living things. An animal has internal structure on every scale, from the proteins in single cells right up to limbs and major organs. It doesn’t exist if it is chopped up, the way a salt crystal continues to exist when it is sliced and diced. It dies.
Scientific understanding is sometimes envisaged as a hierarchy, ordered like the floors of a building. Those dealing with more complex systems are higher up, while the simpler ones go down below. Mathematics is in the basement, followed by particle physics, then the rest of physics, then chemistry, then biology, then botany and zoology, and finally the behavioral and social sciences (with the economists, no doubt, claiming the penthouse).
“Ordering” the sciences is uncontroversial, but it’s questionable whether the “ground-floor sciences”—particle physics, in particular—are really deeper or more all-embracing than the others. In one sense, they clearly are. As the physicist Steven Weinberg explains in Dreams of a Final Theory (1992), all the explanatory arrows point downward. If, like a stubborn toddler, you keep asking “Why, why, why?” you end up at the particle level. Scientists are nearly all reductionists in Weinberg’s sense. They feel confident that everything, however complex, is a solution to Schrödinger’s equation—the basic equation that governs how a system behaves, according to quantum theory.
But a reductionist explanation isn’t always the best or most useful one. “More is different,” as the physicist Philip Anderson said. Everything, no matter how intricate—tropical forests, hurricanes, human societies—is made of atoms, and obeys the laws of quantum physics. But even if those equations could be solved for immense aggregates of atoms, they wouldn’t offer the enlightenment that scientists seek.
Macroscopic systems that contain huge numbers of particles manifest “emergent” properties that are best understood in terms of new, irreducible concepts appropriate to the level of the system. Valency, gastrulation (when cells begin to differentiate in embryonic development), imprinting, and natural selection are all examples. Even a phenomenon as unmysterious as the flow of water in pipes or rivers is better understood in terms of viscosity and turbulence, rather than atom-by-atom interactions. Specialists in fluid mechanics don’t care that water is made up of H2O molecules; they can understand how waves break and what makes a stream turn choppy only because they envisage liquid as a continuum.
New concepts are particularly crucial to our understanding of really complicated things—for instance, migrating birds or human brains. The brain is an assemblage of cells; a painting is an assemblage of chemical pigment. But what’s important and interesting is how the pattern and structure appears as we go up the layers, what can be called emergent complexity.
So reductionism is true in a sense. But it’s seldom true in a useful sense. Only about 1 percent of scientists are particle physicists or cosmologists. The other 99 percent work on “higher” levels of the hierarchy. They’re held up by the complexity of their subject, not by any deficiencies in our understanding of subnuclear physics.
In reality, then, the analogy between science and a building is really quite a poor one. A building’s structure is imperiled by weak foundations. By contrast, the “higher-level” sciences dealing with complex systems aren’t vulnerable to an insecure base. Each layer of science has its own distinct explanations. Phenomena with different levels of complexity must be understood in terms of different, irreducible concepts.
We can expect huge advances on three frontiers: the very small, the very large, and the very complex. Nonetheless—and I’m sticking my neck out here—my hunch is there’s a limit to what we can understand. Efforts to understand very complex systems, such as our own brains, might well be the first to hit such limits. Perhaps complex aggregates of atoms, whether brains or electronic machines, can never know all there is to know about themselves. And we might encounter another barrier if we try to follow Weinberg’s arrows further down: if this leads to the kind of multidimensional geometry that string theorists envisage. Physicists might never understand the bedrock nature of space and time because the mathematics is just too hard.
My claim that there are limits to human understanding has been challenged by David Deutsch, a distinguished theoretical physicist who pioneered the concept of “quantum computing.” In his provocative and excellent book The Beginning of Infinity (2011), he says that any process is computable, in principle. That’s true. However, being able to compute something is not the same as having an insightful comprehension of it. The beautiful fractal pattern known as the Mandelbrot set is described by an algorithm that can be written in a few lines. Its shape can be plotted even by a modest-powered computer:
But no human who was just given the algorithm can visualize this immensely complicated pattern in the same sense that they can visualize a square or a circle.
The chess champion Garry Kasparov argues in Deep Thinking (2017) that “human plus machine” is more powerful than either alone. Perhaps it’s by exploiting the strengthening symbiosis between the two that new discoveries will be made. For example, it will become progressively more advantageous to use computer simulations rather than run experiments in drug development and materials science. Whether the machines will eventually surpass us to a qualitative degree—and even themselves become conscious—is a live controversy.
Abstract thinking by biological brains has underpinned the emergence of all culture and science. But this activity, spanning tens of millennia at most, will probably be a brief precursor to the more powerful intellects of the post-human era—evolved not by Darwinian selection but via “intelligent design.” Whether the long-range future lies with organic post-humans or with electronic superintelligent machines is a matter for debate. But we would be unduly anthropocentric to believe that a full understanding of physical reality is within humanity’s grasp, and that no enigmas will remain to challenge our remote descendants.
This post appears courtesy of Aeon.



At first, the fossil was smuggled out of Mongolia, as many dinosaurs are. It found its way to Japan, then Britain, then France. In 2015, the private collector who finally bought it contacted the paleontologist Pascal Godefroit to get his opinion on the specimen.
Godefroit’s opinion was: This is one weird dinosaur.
The creature was clearly a small predator, much like Velociraptor. Its feet even had the distinctive sickle-shaped claws that clinked across the kitchen floor in Jurassic Park. But its long neck and tapering snout resembled those of a swan. Its arms and hands also had unusual proportions—something halfway between the grasping limbs of other raptors and the flattened flippers of modern penguins. It looked like a Velociraptor that had adapted for life in the water—that is, if it was even an actual dinosaur.
“It was so strange that we suspected that it might have been a chimera—a mix of different skeletons glued together. It wouldn’t be the first time,” says Andrea Cau from the University of Bologna, who joined Godefroit’s investigation. “We had to be sure that it was a real dinosaur and not a fake.”
Since most of the animal was still encased in a 15-inch block of stone, the team took it to Grenoble, and scanned it using a particle accelerator. The scans showed that the block was a solid mass that hadn’t been assembled from separate pieces. And the parts of the skull still within it were identical to those on the outside, as were the hidden arm bones. This swan-necked, duck-snouted, almost-paddle-limbed, sickle-clawed creature was assuredly weird—but it was a real dinosaur. (Cau notes that the scans are all openly available in case any other paleontologists want to check them.)
The team called the creature Halszkaraptor escuilliei. The name’s first half, pronounced “hull-shka-raptor,” honors Halszka Osmólska, a Polish paleontologist who discovered more than a dozen Mongolian dinosaurs and has at least four species named after her. The second half honors François Escuillié, the French collector who bought the fossil, alerted Godefroit, and worked to return the poached specimen to its rightful home in Mongolia. It currently sits in Brussels and will remain there for a year or so while the team finishes studying it.
Halszkaraptor is one of the theropods—a group of mostly meat-eating dinosaurs that count Tyrannosaurus and Velociraptor among their ranks. But unlike its kin, Halszkaraptor’s odd features suggest it was a strong swimmer that perhaps chased fish underwater, much like modern cormorants do. Outside of birds, “this is the first time we see that in a dinosaur,” says Cau. (Other ancient reptiles like paddle-limbed plesiosaurs and the dolphin-esque ichthyosaurs are not actually dinosaurs.)
Like many other fish-eating specialists, Halszkaraptor had a lot of teeth in the front of its snout—more than twice the number of a typical dinosaur. “The first time I saw that, I asked my colleagues to repeat the analysis because I wasn’t convinced,” says Cau. The snout also contained branching bony chambers that would have once housed a large network of blood vessels and sensory nerves. Such features are common in modern crocodiles, giving them an exquisite sense of touch.
Halszkaraptor’s neck made up half its length from snout to hip, reminiscent of plesiosaurs, several groups of freshwater turtles, and birds like swans and herons—all of which use their long necks to catch fish. Halszkaraptor’s neck bones also had more side-to-side mobility than those of the average theropod. “That might be an adaptation to swimming, or it may indicate that the animal used rapid sweeping motions of the neck to capture some sort of small prey,” says Michael Habib, from the University of Southern California.
The arms “are the most problematic part,” says Cau, because they’re not quite like anything else. The long bones are flattened, and the fingers get progressively longer from the outside in—the opposite pattern to most theropods. They’re closest in proportion to the limbs of swimming birds like puffins, murres, and penguins. But they’re not flippers. “I prefer not to say if [Halszkaraptor] used its arms propulsively,” says Cau. “We don’t have information on the shoulder girdle, which would be important to determine if it swam like a penguin.”
Despite all these adaptations for swimming, Halszkaraptor’s back half is that of a typical landlubbing theropod. It had long legs, although it wasn’t well adapted for running. It had a longish tail, although one that was too thin to effectively counterbalance the exceptionally long neck. To compensate, Halszkaraptor probably stood upright, more so than other raptors, but not quite as erect as a penguin. Cau thinks it lived in an unstable environment, with cycles of freshwater and drought. Halszkaraptor evolved to cope with both worlds.
Halszkaraptor is the first amphibious dinosaur that we know of—and that’s strange. With mammals, you have digging moles, gliding squirrels, flying bats, swimming otters, oceangoing dolphins, running gazelles, climbing monkeys, and swinging gibbons. Where’s that diversity of lifestyles among the dinosaurs? Until recently, it seemed that aside from birds, “nearly all dinosaurs are considered to be typical ground-living animals,” says Xing Xu, a prolific dino discoverer from the Chinese Academy of Sciences. That’s surprising, given how diverse they were, and how thoroughly they dominated the planet for hundreds and millions of years.
But that view has changed in recent years, thanks to new discoveries. In 2014, scientists reimagined Spinosaurus, the sail-backed giant that menaced the cast of Jurassic Park 3, as a semiaquatic, crocodile-like fish hunter. They also revealed the complete skeleton of Deinocheirus, a “horse-headed, humpbacked dinosaur that looks like something out of a bad sci-fi movie.” A year later, Xu described Yi, a small predator which had both feathers and bat-like wings, and may have glided between trees. The year after that, another team described Limusaurus, a beaked plant eater that loses its teeth as it grows up. All of these species are theropods. And all of these differ wildly in their lifestyles, diets, physiques, and more. Halszkaraptor, the first amphibious theropod, is part of that revolution.
“It’s exhilarating and yet almost embarrassing thinking back on how typecast these beasties were just a few short decades ago,” says Lindsay Zanno, from the North Carolina Museum of Natural Sciences. In particular, “the theropod section of the dinosaur family tree continues to be in flux as specimens like this one keep popping up.”
“There’s great potential for future dinosaur-fossil hunting,” Xu adds.


Scientists searching for astronomical objects in the early universe, not long after the Big Bang, have made a record-breaking, two-for-one discovery.
Using ground-based telescopes, a team of astronomers have discovered the most distant supermassive black hole ever found. The black hole has a mass 800 million times greater than our sun, which earns it the “supermassive” classification reserved for giants like this. Astronomers can’t see the black hole, but they know it’s there because they can see something else: A flood of light around the black hole that can outshine an entire galaxy. This is called a quasar, and this particular quasar is the most distant one ever observed.
The light from the quasar took more than 13 billion years to reach Earth, showing us a picture of itself as it was when the universe was just 5 percent of its current age. Back then, the universe was “just” 690 million years old. The hot soup of particles that burst into existence during the Big Bang was cooling rapidly and expanding outward. The first stars were starting to turn on, and the first galaxies beginning to swirl into shape. Quasars from this time are incredibly faint compared to the nearest quasars, the light from some of which takes just 600 million light years to reach the Earth.
“It’s like finding the needle in a haystack,” said Eduardo Bañados, an astronomer at the Carnegie Institution for Science who led the international research team. Their double discovery is described in a study published Wednesday in Nature.
Black holes, mysterious as they are, are among the most recognizable astronomical phenomena in popular science. They’re pretty straightforward: Black holes are spots in space where the tug of gravity is so strong that not even light can escape. They gobble up gas and dust and anything that comes near, growing and growing in size. A supermassive black hole sits in the center of virtually all large galaxies, including the Milky Way. Astronomers can infer their existence by watching fast-moving stars hurtle around a seemingly empty, dark region.
Quasars, meanwhile, are a little trickier to understand, and you’d be forgiven for thinking they sound like something out of Star Trek. A quasar is, to put it simply, the product of a binge-eating black hole. A black hole consumes nearby gas and dust inside a galaxy with intense speed, and the violent feast generates a swirling disk of material around it as it feeds. The disk heats up to extreme temperatures on the order of 100,000 degrees Kelvin and glows brightly. The resulting light show is what we call a quasar, and what a light show it is.
“A quasar emits more light than an entire galaxy’s worth of stars, and it’s actually just a glowing disk of material that is the size of our solar system,” said Daniel Mortlock, an astrophysicist at Imperial College London and Stockholm University. In 2011, Mortlock and his colleagues reported their discovery of the most distant quasar found at the time.
The more material a black hole consumes, the bigger it becomes. Eventually, the black hole drains the surrounding area of material and has nothing to eat. The luminous disk around it shrinks and fades, and the quasar is extinguished. In this way, quasars—and the black holes that power them—are like volcanoes, erupting under one set of conditions and settling into dormancy under another.
Quasars were first detected in 1963 by the Dutch astronomer Maarten Schmidt with California’s Palomar Observatory. Astronomers thought these newly discovered points of light were stars because of their extreme brightness. But when they studied the spectrum of their light, they were stunned to find the “stars” were more than a billion light-years away. When light travels through space, it gets stretched thanks to the constant expansion of the universe. As it moves, it shifts toward redder, longer wavelengths. Astronomers can measure this “redshift” to figure out how long the light took to reach Earth, which indicates how far a certain object is. Schmidt and his fellow astronomers knew that for stars to appear so luminous to Earth from such great distances was impossible. They were dealing with completely new phenomena.
“They’re not something that anyone predicted at all,” Mortlock said. “Occasionally you get astronomical objects like [stars known as] brown dwarfs, where people had predicted that they would exist and waited for astronomy to find them. No one predicted anything like quasars. It’s one of those cases where our imaginations weren’t up to what nature turned out to provide.”
To find the latest record-breaking quasar, Bañados and his colleagues used computer algorithms to search through databases of large sky surveys. They selected points of light they suspected could turn out to be quasars and observed them with the telescopes at Las Campanas Observatory in Chile. One night in March of this year, they all gathered to look at the data, one quasar candidate at a time. Quasars, astronomers have found, are easily recognizable when raw data is plotted on a chart. The spectrum of a quasar—a plot of brightness against the wavelength of light—has a very distinctive shape. Features known as emission lines appear broad, rather than sharp, thanks to the Doppler effect, which means the object emitting the light it traveling at high speeds.
“These objects are so bright that basically in 10 minutes, I can know from the raw data if it’s a quasar or not,” Bañados said. They found a quasar in their search, and when they calculated its distance from Earth, they couldn’t believe what they’d found. The next day, Bañados started drafting proposals to get observation time on powerful telescopes around the world to further study this quasar.
From the data for the quasar, astronomers can infer the size of the black hole responsible for powering it. “To get a bright quasar like this, you have to build up a supermassive black hole,” Mortlock said.
Astronomers studied the galaxy where the black hole and its quasar reside using radio telescopes in the French Alps and New Mexico. They found that the galaxy, at a mere 690 million years, had “already formed an enormous amount of dust and heavy chemical elements. This means it must already have formed a large amount of stars.” Astronomers say they’ll need to rethink some existing models for the evolution of galaxies to explain how a young galaxy could accumulate so much matter so fast. The findings about the galaxy are published in a separate study in the Astrophysical Journal Letters.
Quasars are some of the best targets for studying the early universe. Like flashlights, they illuminate a cosmic time astronomers are still struggling to understand. The newly discovered quasar comes from a period in the universe’s history know as “the epoch of re-ionization,” when a mysterious source of radiation ionized hydrogen and transformed the gas in the universe from an indiscernible fog into something transparent. About this time, the first objects to radiate light also formed. The exact process, as well as which phenomenon happened first, remains poorly understood.
Mortlock said he feels some sense of ownership of the quasar he discovered, which is now the second-farthest ever spotted. To feel that way about an object billions of light-years away is “completely ridiculous,” he said with a laugh. “And it’s especially ridiculous because there was no way that the object we discovered was going to be the end of this process. As we get more data and observe larger areas of the sky and look more deeply, we’re always going to find more objects like this.”
Someday, Bañados’s discovery will be relegated to second place, too. “There must be more out there, especially fainter ones,” Bañados said. “I’m still searching for them.”


While dozens of people have lived there over the years—including six right now—the International Space Station is unlike any other home. Its residents sleep zipped into bags tethered to the wall so they don’t float away. They pee into a plastic hose that suctions urine into a processor and then turns it into drinking water. Their showers require squeezing globs of water out of pouches.
But just like in homes found on Earth, the residents of the International Space Station share their space with thousands of invisible roommates: bacteria.
A team of microbiologists at the University of California recently sought to examine the population of microbes on the station. They wanted to see how the ecosystem orbiting 200 miles above Earth would resemble that of homes—and the people who inhabit them—below. They collected swab samples from more than a dozen surfaces on the station and compared them to microbes that were taken from surfaces in terrestrial households and different spots on the human body.
The researchers say the ISS turned out more species-rich than they expected. They identified 12,554 species of microbes, most of them harmless. They found that the composition of microbes of the space staton and a typical household was significantly different, but the microbes on the station more closely resembled home surfaces than human ones.
These results are both surprising and unsurprising. The researchers had expected to find differences in the microbial ecosystems between the space station and terrestrial homes. “Unlike the ISS, homes on Earth are exposed to a variety of sources of microbes, including the outdoor air, tracked-in soil, plants, pets, and human inhabitants,” the study authors write. On the ISS, you can’t exactly crack open a window to let some fresh air in.
These conditions would seem to suggest that the microbes living on the surfaces of the station would have more in common with the ones residing on the humans inside it. And yet, the researches found the microbes to be “more similar to the surfaces of human homes on Earth than it is to human bodies.” So the microbes on the ISS don’t match up with the ones found in houses or on bodies, but they’re still more like ones living on your kitchen counter than on your skin.
The findings were published Tuesday in the journal PeerJ. In May 2014, Koichi Wakata, an astronaut from Japan and the crew commander at the time, swabbed 15 surfaces around the station, including telephones, laptop keyboards, handrails, and air vents. In the station’s microgravity environment, microbe-carrying dust tends to accumulate in air filters rather than surfaces.
The samples were stored in a lab freezer, sent back to Earth on a SpaceX spacecraft, and then shipped to a lab, where the researchers extracted DNA to build a census of the microscopic inhabitants. The researchers compared them to three databases: the Human Microbiome Project, which characterizes human microbes; the Wildlife of Our Homes project, which tracks microbe samples taken from household surfaces like kitchen counters, toilet seats, pillowcases, and door handles; and Project MERCCURI, a microbial experiment on the ISS.
The researchers tried to target surfaces on the ISS that may be similar to those used in the Wildlife of our Homes project, but they hit a few snags. “The kitchen surfaces aboard the ISS are in the Russian module, which we did not have permission to access,” they write. And “swabbing the toilet seat was deemed inappropriate due to biosafety concerns.”
Studying the specific microbial environment on the ISS is crucial for future space missions beyond low-Earth orbit. When humans leave the comfort of Earth’s protective bubble for more dangerous, extreme worlds, they won’t be going alone—they’ll take with them thousands, perhaps millions, of these microorganisms, tiny beings just as unprepared as they are to face environments they didn’t evolve in.
The microbes of the ISS recently popped into headlines in November when the Russian news agency TASS reported that cosmonauts had scraped living bacteria off the outside of the station during a spacewalk. The article quoted a cosmonaut saying the bacteria “have come from outer space and settled along the external surface,” prompting some on the internet to think it was of extraterrestrial origin. He didn’t add much else, but as science journalists have since pointed out, it’s more probable that the bacteria was delivered there by terrestrial activities, like repairs and maintenance by astronauts and cosmonauts during spacewalks. The likeliest culprits behind mystery bacteria in low-Earth orbit are always going to be humans. Beyond that, however, who knows?


When the botanist Richard Deakin examined Rome’s Colosseum in the 1850s, he found 420 species of plant growing among the ruins. There were plants common in Italy: cypresses and hollies, capers, knapweed and thistle, plants “of the leguminous pea tribe,” and 56 varieties of grass. But some of the rarer flowers growing there were a botanical mystery. They were found nowhere else in Europe.
To explain this, botanists came up with a seemingly unlikely explanation: These rare flowers had been brought as seeds on the fur and in the stomachs of animals like lions and giraffes. Romans shipped these creatures from Africa to perform and fight in the arena. Deakin takes care to mention in Flora of the Colosseum of Rome that the “noble and graceful animals from the wilds of Africa ... let loose in their wild and famished fury, to tear each other to pieces”—along with “numberless human beings.” As the animals fought and died in the arena, they left their botanical passengers behind to flourish and one day overtake the building itself.
This incredible piece of conjecture is hard to prove, but it shows just how much of the story of a ruined place can be found growing in the cracks between broken stones. Deakin, an Englishman from Royal Tunbridge Wells, opens his volume by calling the plants growing in the Colosseum “a link in the memory” that “flourish in triumph upon the ruins.” He lists their ancient medicinal properties, notes the species of fern that must have grown around the Fountain of Egeria, and pauses at one point over a particular species of grass that might have ringed the banks of Nero’s fish pond. The grasses that grow “in matted tufts,” he writes, “seem to be perennial weepers ... mourning over the vast destruction that reigns around them.”
Today, the Colosseum stands bald and bare. But for centuries, it was a wild and overgrown place, and its lost history as a primeval garden ruin has left traces in the art and poetry of countless generations that walked among its stands.
* * *
By the time the artists and painters of the Romantic Age began to turn their interest to the ruins of ancient Rome, the Colosseum had suffered greatly. Since the days of Rome’s flourishing, the great arena had been a cemetery, a quarry of quicklime, a rich family’s fortress, and a bullring. In the 16th century, it was even explored as a site for a wool factory staffed by former sex workers. It had been damaged by fire and struck by earthquakes five times, including one in 1349 that caused the entire outer south side to collapse.
Due to the belief that Christian martyrs had once been fed to the lions in the arena, the Colosseum was also a popular pilgrimage spot. Despite little evidence that Christians were ever actually killed in the arena, in 1749, Pope Benedict XIV endorsed the view that the Colosseum was a sacred site, outlawing the use of its stones in other buildings. By that point, the arena was a crumbling ruin. But it had become the home for a great variety of sumptuous greenery.
Plant life was so abundant in the ruined arena that at certain times in history, peasants had to pay for permission to collect the hay and herbs that grew there. It had become its own miniature landscape, and formed a perfect microclimate for biodiversity: dry and warm on its south side, cool and damp in the north. Pink dianthus grew down in the lower galleries, while white anemones dotted the stands during spring.
When the Italian scholar Poggio Bracciolini visited in 1430, he mourned over the site of the ruins. “This spectacle of the world, how it is fallen!” he exclaimed. “How changed! How defaced! The path of victory is obliterated by vines.” But others saw an alluring beauty in the arena’s greenery. Charles Dickens, in his 1846 Pictures From Italy, talks about his impressions on first seeing the Colosseum, and mentions the plant life there in particular detail, as a natural force reclaiming the site of past glory:
To see it crumbling there, an inch a year; its walls and arches overgrown with green; its corridors open to the day; the long grass growing in its porches; young trees of yesterday, springing up on its ragged parapets, and bearing fruit: chance produce of the seeds dropped there by the birds who build their nests within its chink and crannies; to see its Pit of Fight filled up with earth ... is the most impressive, the most stately, the most solemn, grand, majestic, mournful sight conceivable.
By the 19th century, countless painters and artists had visited the Colosseum and painted vistas of the arena’s overgrown stands and crumbling stones. The Scottish painter William Leighton Leitch (1804–1883) painted the Colosseum as an imposing empty space, a monumental hanging garden reminiscent of Babylon. Joseph Mallord William Turner’s “The Colosseum, Rome, by Moonlight” (1819) shows an almost tropical garden growing in the arena’s shadows:
For Ippolito Caffi, meanwhile, the ruins became an almost nightmarish vision, a primeval jungle ruin where strange lights dance between the stones wreathed in matted vines. The paintings that perhaps most effectively portray the strange, paradoxical atmosphere of the Colosseum in those days are those of Danish painter Christoffer Wilhelm Eckersberg (1783–1853), who captures the abundant greenery growing in the Colosseum’s stands as well as its lost history as a place of Christian worship, but does so with a faultlessly modern eye, never giving in to the Romantic excesses of some of his forebears. His paintings are by turns infused with a solemn quiet, and full of the life of common people.
Paintings like these inspired writers as well. In 1833, the then-unknown Edgar Allan Poe was able to publish a poem called “The Coliseum,” though he had never set foot in Italy. The poem mentions the flora of the arena particularly, with the lines, “Here, where the dames of Rome their yellow hair / Wav’d to the wind, now wave the reed and thistle.”
Earlier, in 1818, Percy Bysshe Shelley wrote lyrically of the way that the arena had itself become a natural entity, a landscape rather than a building.
It has been changed by time into the image of an amphitheater of rocky hills overgrown by the wild olive, the myrtle, and the fig tree, and threaded by little paths which wind among its ruined stairs and immeasurable galleries: The copsewood overshadows you as you wander through its labyrinths, and the wild weeds of this climate of flowers bloom under your feet.
Lord Byron, in the fourth canto of his “Childe Harold’s Pilgrimage,” also speaks of the “The garland-forest, which the gray walls wear, / Like laurels on the bald first Caesar’s head.” For him, the gaps and blanknesses in the ruins form portals to another time, bringing back the dead and unsettling the ruin-gazer.
But the abundant green life of the Colosseum was soon to come to an end. In 1870, Rome was captured by Italian nationalist forces, the final defeat of the Papal States in the war to unify the Italian peninsula. The Pope became an exile in his own palace, surrounded by armed soldiers who stopped him from appearing in public. A year later, Rome became the capital of the Kingdom of Italy, a new nation that was to be modern, democratic, and secular. As revenge for the Pope’s intransigence, Catholic convents, churches, and hospitals were seized for barracks. The Colosseum, too, was swept up in this upheaval.
As nation-states were being born around Europe and the rest of the world in the late 19th century, many of these new states reached back into their pasts to find a foundation on which to build their identities. The new Italian nation found this opportunity in the lost splendor of the Colosseum. The Italian government soon handed control of the arena over to archaeologists, who set about removing religious icons installed by the Christians and clearing it of its invasive greenery.
The work of these early archaeologists continued into the 20th century, and the Colosseum was finally totally denuded and its arena excavated under Mussolini’s fascist government, who sought even further to associate the modern Italian state with the monuments of ancient Rome.
* * *
Despite this loss, the Colosseum is today still a haven for plants. A study carried out between 1990 and 2000 found 243 distinct species still growing there, although this number is scarcely half what Deakin observed in the 19th century.
Plants growing today in the Colosseum include very rare species like Asphodelus fistulosus and Sedum dasyphyllum, which scientists believe can only survive when sheltered by the arena, a sanctuary from the urban environment outside. Due to increased pollution and the rising temperature of the city, the flora inside the ruined walls are beginning to change: Plants suited to a warmer and more arid climate are beginning to proliferate at the expense of those more used to cool and damp.
From the wild and magical ruins of the Romantic Age to the haunted and skeletal ruins of Dresden and Stalingrad, the place ruins hold in the collective imagination has always been as unstable as the crumbling structures themselves. No one sees the same ruin. Certain parts of a crumbling building, the ancient stones and arches, for instance, are deemed “proper.” Other parts, like the plant life and the later historical stages of the ruin, are deemed “improper” and are removed.
Increasing attention is being paid to these intangible elements of a building’s story, and archaeologists and conservationists alike are having lively debates about how the atmosphere of these sites—the inspiration for so many artists—can be preserved. Deakin believed that the wild spirit of a place can “teach us hopeful and soothing lessons, amid the sadness of a bygone age.” Today if you walk around the Colosseum, you might even see the occasional rebellious caper plant growing there, in defiance of security.


The headline on the big new gender survey from the Pew Research Center begins, “On Gender Differences, No Consensus”— and that could have been the report’s entire conclusion, too. The survey, released today, reveals deep divides in Americans’ perspectives on gender norms, including by political affiliation:
Of course, the recent wave of sexual-harassment allegations in politics have shown that people of all parties are capable of mistreating women. But this Pew survey and others reveal how, even though society’s ideas about gender are changing rapidly, Republicans are less likely to endorse those changes.
A poll conducted by PRRI and The Atlantic between October 5 and October 9 of last year found Trump supporters were more likely than Clinton supporters to feel that society punishes men just for acting like men. In that poll, Republicans, conservatives, and Trump supporters were also far more likely than liberals, Democrats, or Clinton supporters to think that society was becoming “soft.”
Social-science research suggests gender beliefs are an inherent part of what it means to be a liberal or conservative. As NPR’s Hidden Brain team reported, Republicans tend to have a “strict father” view of the world, in which strong figures decide what is best for the family (or the country). Democrats, meanwhile, tend to support the “nurturant parent” model, in which parents (and leaders) “feel their job is to empathize with their child, to know what their child needs, and to have open two-way discussions with their child,” the NPR reporters write. Those fundamental beliefs might later map onto more positive views of masculinity, in Republicans, and more free-flowing ideas about rules for men and women, in the case of Democrats. In academic studies, people are even more likely to describe the GOP in masculine terms, and the Democrats in feminine ones.
The 2016 election revealed a desire for better wages and conditions for working-class people of both parties. But on cultural issues—including gender—there still seems to be “no consensus.”


Once upon a time, the sun and moon argued about who would light up the sky. They fought, as anthropomorphic celestial bodies are meant to do, but after the moon proves to be as strong as the sun, they decide to take shifts. The sun would brighten the day, while the moon would illuminate the night.
This is one of several stories told by the Agta, a group of hunter-gatherers from the Philippines. They spend a lot of time spinning yarns to each other, and like their account of the sun and moon, many of these tales are infused with themes of cooperation and equality. That’s no coincidence, says Andrea Migliano, an anthropologist at University College London.
Storytelling is a universal human trait. It emerges spontaneously in childhood, and exists in all cultures thus far studied. It’s also ancient: Some specific stories have roots that stretch back for around 6,000 years. As I’ve written before, these tales aren’t quite as old as time, but perhaps as old as wheels and writing. Because of its antiquity and ubiquity, some scholars have portrayed storytelling as an important human adaptation—and that’s certainly how Migliano sees it. Among the Agta, her team found evidence that stories—and the very act of storytelling—arose partly as a way of cementing social bonds, and instilling an ethic of cooperation.
At first, Migliano wasn’t actually interested in storytelling. She wanted to know what qualities the Agta most value in their peers, given that they are nomadic and their camps continuously shift. So, her students asked 300 Agta to name the five people they’d most want to live with. They also asked the volunteers to nominate the strongest people they knew; the best hunters, fishers, and foragers; the ones whose opinions are most respected; and the ones with most medical knowledge. And finally, almost as an afterthought, they asked the volunteers to name the best storytellers. That, they assumed, was something relatively unimportant, and would make for an interesting contrast against the other more esteemed skills.
In fact, the Agta seemed to value storytelling above all else. Good storytellers were twice as likely to be named as ideal living companions as more pedestrian tale spinners, and storytelling acumen mattered far more all the other skills. “It was highly valued, twice as much as being a good hunter,” says Migliano. “We were puzzled.”
Fortunately, she had been working with Agta Aid, a nonprofit organization that had been trying to preserve the Agta’s oral stories in written forms. “We asked them if we could have a look at the stories they were collecting, and we realized that most of the content was about cooperation, egalitarianism, and gender equality.” The male sun and female moon divvy up the sky. A pig helps its injured friend—a sea cow—into the ocean so they can race side by side. A winged ant learns that she is not above her other wingless sisters.
These themes aren’t unique to the Agta. They’re also present in around 70 percent of the stories that Migliano compiled from work with other hunter-gatherer groups. “Hunter-gatherers move around a lot and no one has particular power,” she explains. “You need ways of ensuring cooperation in an egalitarian society, and we realized that you could use stories to broadcast the norms that are important to them.” People can use religion to achieve a similar end, enforcing good behavior through fear of a punitive deity. But Migliano points to research suggesting that high gods are a relatively recent invention, which emerged once human societies became large. Small communities like the Agta don’t have them. Instead, they use stories for the same purpose.
Migliano’s team asked Agta volunteers from various camps to play a simple game, in which they could share rice with their camp-mates. And they found that such sharing was more likely in camps with a higher proportion of good storytellers.
That’s just a correlation, though. It’s possible that the storytellers were actively fostering more generosity among their peers. Alternatively, Migliano says, “if you live in a more cooperative camp, perhaps you have more time and you just tell more fun stories.” But if that’s true, she adds, it wouldn’t explain why so many of the actual stories feature leitmotifs of cooperation, rather than other happy and positive themes. And it certainly doesn’t explain why storytelling skill is so beneficial for those who wield it.
Skilled Agta storytellers are more likely to receive gifts, and they’re not only more desirable as living companions—but also as mates. On average, they have 0.5 more children than their peers. That’s a crucial result. Stories might help to knit communities together, but evolution doesn’t operate for the good of the group. If storytelling is truly an adaptation, as Migliano suggests, it has to benefit individuals who are good at it—and it clearly does.
“It’s often said that telling stories, and other cultural practices such as singing and dancing, help group cooperation, but real-world tests of this idea are not common,” says Michael Chwe, a political scientist at the University of California, Los Angeles, who studies human cooperation. “The team’s attempt to do this is admirable.”
Still, it’s hard to know if it’s the specific act of storytelling that matters. As others have noted, “creativity comes with its own suite of personality traits, which may make [people] more attractive sexual partners,” says Lisa Zunshine, an English professor at the University of Kentucky.
And all of Migliano’s results hinge on the Agta accurately naming the best storytellers in their midst. Did they? Could they just have named people they were close to, or venerated celebrities who sprang readily to mind? Wouldn’t that explain both the fecundity and desirability of these supposed storytelling Jedi? Migliano thinks not. If the survey had been a mere popularity contest, the Agta should have also nominated the same people as exceptional hunters, gatherers, child minders, and so on. They didn’t. They singled out particular people for particular skills, including storytelling.
“It suggests that hunter-gatherers track this ability and perceive it as beneficial,” says Michelle Scalise Sugiyama from the University of Oregon, who has studied the origins of storytelling. Other societies, like the Tsimane of Bolivia, do the same, which “indicates that storytelling contributes something of adaptive value to human life.” That something might well be the reinforcement of norms and ethics. “As attested by the universality of the trickster figure, telling stories about rule breakers who get caught and punished is an effective means of persuading individuals to conform to group norms,” Scalise Sugiyama adds.
But “stories also contain valuable cultural knowledge, and accomplished storytellers are repositories of this knowledge,” she notes. Hunter-gatherers use their tales to pass down information about food, weather, and more—and often in ways that outsiders can miss.
For example, Andamanese people have a story about two quarreling weather gods, who eventually split up their wind-creating duties. You could see that as a story about avoiding conflict, or as a way of encoding information about the strong winds that buffet the Andaman Islands. A similar creation story talks about a monitor lizard that went into the jungle to hunt pigs, got stuck in a tree, and was helped down by a civet cat (whom it married). On the face of it, that’s a story of cooperation between the sexes. But Scalise Sugiyama notes that it also encodes information about the habitat, diet, and range overlaps between the local lizards, pigs, and civets. “These profiles may aid in the prediction of animal behavior, which is critical to locating, tracking, and stalking game,” she says.
This is not to say that people deliberately or consciously tell stories to pass down knowledge or to keep their communities together. “My guess is that they would say it’s fun,” says Migliano. That’s why individuals choose to tell stories on a moment-to-moment basis—it’s what biologists call the “proximate cause” of a behavior. But it’s the broader benefits—the “ultimate causes” like transmission of knowledge or inculcation of values—that might explain why storytelling arose in the first place.
The origin of storytelling doesn’t necessarily reflect its later uses, though. “Our very human love of stories has become adapted for different ends during later phases of human history,” says Sarah Blaffer Hrdy, an anthropologist from the University of California, Davis. “The Maya-speaking people I used to study in southern Mexico told tales about a winged, super-sexed demon with a six-meter-long, death-dealing penis, who reinforced proper sex roles for men and women, including proscriptions for postures during sex, menstrual taboos, freedom of movement. Rather than promoting sexual equality, these served to constrain women.”
“Alas, our wonderfully human universal of loving stories can also become an all-too-human vulnerability, fostering enmity as readily as amicable relations,” she adds.


They are known, in almost every local indigenous language, as “Bears Ears,” and when you look at photos, you can see why. The two buttes, ruddy and gentle-sloped, rise above the scrub and canyon land that surrounds them.
Thousands of years ago, now-forgotten indigenous peoples scribbled pictograms of men and animals on their rusty walls. A century and a half ago, Navajo leaders took refuge in those same caves and canyons as the U.S. government forcibly evicted them from their territory.
On Monday, their history gained another chapter. President Donald Trump has significantly shrunk the size of two national monuments in Utah, completing the largest rollback of public-land protections in U.S. history and opening a legal battle that could determine the fate of conservation in the United States.
One of these monuments includes the land surrounding Bears Ears, which President Barack Obama protected in a poetic and immediately controversial declaration during the final days of his administration. Obama extended federal protection to 1.3 million acres of land, roughly the area of Delaware; Trump has shrunk that to 220,000 acres, roughly the size of Dallas.
Grand Staircase–Escalante, another large monument created by President Bill Clinton in 1996, will be nearly halved in size.
“Some people think that the natural resources of Utah should be controlled by a small handful of very distant bureaucrats located in Washington. And guess what? They’re wrong,” said Trump, speaking in front of the Utah state capitol building.
Bears Ears was created after half a decade of lobbying by five indigenous tribes: the Hopi, the Navajo, the Ute, the Ute Mountain Tribe, and the Zuni. The nations had long sought special status for the land, which all five consider sacred. But when talks broke down with Utah lawmakers several years ago, they pressed Obama to protect the buttes through a national monument.
The five tribes have promised to sue the Trump administration, asserting that the president does not have the right to shrink national monuments. Environmental groups, united in anger at the president’s actions, have also said they will sue to preserve Grand Staircase–Escalante at its current size.
National monuments make for a strange kind of federal land designation. Locals can generally hike, camp, hunt, and fish on national monuments. They cannot mine coal or drill for oil. Grand Staircase–Escalante is known to contain coal reserves. Ryan Zinke, the secretary of the interior, said before the speech that that didn’t play into the president’s decision.
The Antiquities Act was passed in 1906 to enable presidents to protect indigenous archaeological or cultural sites. The law clearly grants the president the right to create new national monuments. But some conservative legal scholars argue that it also contains an implicit right to revoke or revise the size of national monuments.
Presidents have slashed the size of national monuments twice before. During World War I, Woodrow Wilson slashed land from Mount Olympus National Monument to secure lumber rights. The monument was later returned to its largest size and protected as a national park. Franklin Roosevelt also cut the size of Grand Canyon National Monument to open up more land for ranching.
But neither of those reductions were tested in court—and neither occurred during the modern era of environmental law.
Since its passage last century, presidents have activated the law 157 times to protect Native sites, natural wonders, historical landmarks, and vast stretches of seafloor. Congress later enshrined dozens of those monuments—including Grand Canyon, Bryce, Zion, Acadia, and Grand Teton—as national parks, which have an added level of protection.
“This law requires that only the smallest necessary area be set aside for special protection as national monuments,” said Trump, referring to the Antiquities Act.
“Unfortunately, previous administrations have ignored the standard and used the law to lock up hundreds of millions of acres of land under government control. It did so over the loud objections of the people of this state and their elected representatives,” he said.
Polling on the issue is conflicted. A slim majority of Utahans think Bears Ears is too big, according to a recent poll conducted by The Salt Lake Tribune and the University of Utah. But about the same number of Utahans also oppose changing the size of Grand Staircase–Escalante, which President Bill Clinton created in 1996.
Environmental groups fear that the president’s undoing of public-land protections would add public-land protection to the standard back-and-forth that accompanies every partisan changeover of the White House. If Trump wins in court, a future president could undo any swath of national monument, no matter how long ago it had been protected. A strong court precedent could strip the presidency of its powers to confidently and permanently protect public land.
“As a matter of public policy, it does not make sense for protective public-lands designations to be subject to partisan uncertainty every presidential election cycle. That is a poor management strategy,” said Sarah Krakoff, a professor of law at the University of Colorado Boulder, in an email.
“The president’s authority to act unilaterally is appropriately narrow—to act to increase federal lands protection. Otherwise, Congress retains the constitutional authority to make a broader range of decisions about the nation’s lands,” she added.
“When a monument is used to prevent, rather than protect, the president is right to take action,” Zinke told a reporter before the president’s speech. The state’s governor and its full Congressional delegation supported the size reduction, as did a public-land commissioner who represents some Navajo districts, Zinke said.
Indigenous nations had previously celebrated Bears Ears as a historic occasion because it represented a change in the history of the Antiquities Act. Where the act had once been written to paternalistically protect abandoned Native sites, it had now been extended at their behest.
“It actually brought tears to my face,” said Eric Descheenie, a Navajo leader who had fought for protection for Bears Ears and a Democratic member of the Arizona House of Representatives. “It’s so hard to even try to add up what this really means. At the end of the day, there’s only a certain place in this entire world, on earth, where we as indigenous peoples belong.”
“The regulators thousands and thousands of miles away—they don’t know your land and truly they don’t care about your land like you do,” said Trump. After he signed the two proclamations—above intermittent chants of “four more years!”—a Rolling Stones song that had often played at the president’s campaign rallies came on the AV. “You can’t always get what you want,” it said.


Even if countries take moderate action on climate change, by the end of this century, Phoenix is expected to have an extra month of days above 95 degrees Fahrenheit, while Washington, D.C., is expected to have another three weeks of these sweltering days, as the Climate Impact Lab and New York Times reported.
A new study suggests that even days that are an average of 90 degrees Fahrenheit, or 32 Celsius, might have long-term, negative impacts on developing fetuses. The stress of the hot weather might show up as reduced human capital once those fetuses reach adulthood.
Maya Rossin-Slater, a health-policy professor at Stanford University, said she and her team wanted to understand the long-term consequences of climate change on people. For the study, published today in the Proceedings of the National Academy of Sciences, she and other researchers looked at data on births, weather, and earnings in half the states in the United States. For a given county, on a given day, they measured how many days above 90 degrees a child born that day would have experienced during gestation and during their first year of life. They then compared that person’s salary as an adult to someone born in that same county on that same day in other years.
It turned out fetuses and infants exposed to a single extra 90-plus degree day made $30 less a year, on average, or $430 less over the course of their entire lifetimes. Right now, the average American only experiences one such day a year. (This study looked at the average temperature throughout the entire day, not the highest temperature that day.) By the end of the century, there will be about 43 such days a year.
In addition to birthday and county, the researchers also controlled for gender and race. Rossin-Slater said it is unlikely the difference in earnings could be explained by something other than heat.
“It’s really hard to figure out what else it could be. They set up a really good study design,” said Kathryn Grace, a professor of geography, environment, and society at the University of Minnesota, who was not involved in the study.
What’s more, the study used data from the 1970s, when more and more people were installing air conditioners in their houses. The researchers found the difference in earnings went away in areas where most people got air conditioners installed.
It’s not entirely clear how hot temperatures would be causing this dip in earnings. Fetuses and infants are especially sensitive to heat because they don’t yet have the ability to self-regulate their body temperatures. Rossin-Slater said there are three potential pathways by which being too hot could impact the fetus: The heat could overstress it, which could affect the child’s health. Heat could also affect how nutrients are delivered to the fetus, or harm its cognitive development, and thus potentially things like focus or self-control.
Nathaniel DeNicola, an obstetrician with George Washington University and an expert on environmental health, said it’s well established that extreme temperatures can affect fetuses, for example by restructuring proteins that are involved in organ formation, and extreme heat raises the risk of preterm birth and low birth weight. Those issues, in turn, can sometimes cause cognitive impairment.
Still, he said, the authors could be missing some other variables that could be contributing to adult salaries, and it’s not clear that adult salaries are a marker of good health. The overall message from the study, to him, should be that “there are clear health risks to extreme heat, and those risks are worse during the critical periods of development.” (In the study, there was no effect on earnings for children older than 12 months either way.)
Women shouldn’t get too worried if they are pregnant and living in a warm area, Rossin-Slater cautioned. The difference in earnings was small, after all, and nothing happened to the air-conditioned babies.
But that also doesn’t mean we should look to AC to solve all our climate-change woes. Air conditioning isn’t free, and people in the developing world—which is hotter—are more likely not to have access to it.
“In poor countries, we can sometimes be like, ‘Oh, it’s food insecurity, it’s drought, it’s not temperature,’” Grace said. “This is a place for us to start thinking about what physical environments do to our health.”


The viruses, Jeremy Barr realized, were in the wrong place.
Barr and his colleagues at San Diego State University had grown a layer of gut cells in a dish, much like those that line the surface of our own twisting intestines. The cells formed such tight connections with each other that bacteria couldn’t sneak past them. Even a dye couldn’t get through. The layer was meant to be impermeable, until the team infused the water on one side of it with viruses called phages.
After a few hours, they found a few of these phages on the other side. The cells had absorbed them at one end, and shoved them out the other. “It took us a while to realize what we were seeing, but when we did, it was really exciting,” Barr says.
Barr believes that the same process happens in our bodies, frequently and relentlessly. If he’s right, it means that our guts are absorbing billions of viruses every day, sending a steady stream of them into our bloodstream and the rest of our organs.
That’s not something to worry about. Phages don’t infect human cells and they don’t cause disease. Their full name, bacteriophages, means “eaters of bacteria,” and as that suggests, they infect and destroy bacteria. In doing so, Barr says, they could act as part of our immune system. They anchor themselves in the layer of mucus that lines our gut. By infecting the bacteria that also thrive there, they keep these microbial populations in check, and could determine which species get to live in our bodies.
This relationship is likely an ancient one. Mucus is universal to animals from corals to fish to humans, and phages are universal to mucus. Perhaps this was how the very first animals defended themselves against infections. They developed mucus to concentrate phages that were plentiful in their environment, and the viruses in turn helped their hosts to control the microbial multitudes around them. It was a mutually beneficial relationship between animal and virus, and one that continues today.
But the latest experiments from Barr’s team, many of which were done by his colleague Sophie Nguyen, suggest that this relationship between animals and phages is even more intimate. The phages aren’t just sitting atop human gut cells, acting as bouncers. They are actually being trafficked through the cells themselves. The team even used powerful microscopes to confirm the presence of phages within the cells. “A cell is enormous compared to a phage,” says Barr. “It’s like finding a cup of coffee by sectioning a skyscraper.”
In the experiment, just 0.1 percent of the total phages made it through. But based on their rate of travel, and the staggering number of them in the average human gut, the team estimated that our gut cells absorb around 31 billion phages every day. “The percentage feels like it can’t be that important but when you turn that percentage into absolute numbers, it feels biologically relevant,” says Corinne Maurice from McGill University, who also studies phages and was not involved in this study.
The team only did experiments using lab-grown cells, but Barr says there’s good reason to think that the same viral journeys take place in living bodies. For over 70 years, scientists have been “finding phages in parts of the body where they shouldn’t be,” he says, including supposedly sterile organs like the lungs. Microbiologist René Dubos found hints of this in 1943, by injecting phages into the guts of mice and finding those same viruses in the rodents’ brains.
“Phages can be detected outside the gastrointestinal tract, but there hasn’t been any real proof of how they get there,” says Lori Holtz from Washington University School of Medicine in St Louis. Many scientists believed that they were just leaking through gaps between the cells, but Nguyen’s work suggests that they are actually going through the cells themselves. In her experiments, the phages could traverse cells that line the kidneys, lungs, liver, and even the brain. “That’s absolutely astonishing in my view,” says Barr. The brain is separated from other organs by the blood-brain barrier—one of the most tightly controlled borders in the body. It’s incredibly hard for scientists to get small molecules through it. And yet, phages seem to do so.
This isn’t an infection in any meaningful way. The phages aren’t hijacking human cells to make more copies of themselves, as viruses like influenza, Zika, or Ebola might. Instead, Barr thinks that the cells are in control. They’re actively engulfing phages, and shuttling them from one end to the other. Why?
For a start, this would suffuse our bodies with a sparse but continuous stream of phages, which might then protect our organs against wayward, opportunistic bacteria. But Barr speculates there are more unexpected purposes at work. By sensing and studying the phages they absorb, cells could fine-tune the production of the mucus that houses these viruses, or the chemicals that feed the microbes that the phages then infect. If the cells break down some of the absorbed phages, they could access and use the viruses’ genetic material. All of this is possible, and none of it is certain. Scientists are only starting to eavesdrop on the three-way conversations between bacteria, phages, and our own cells. “It’s a big unknown,” says Barr.
Maurice also notes that phages aren’t just a homogenous group. They are incredibly diverse in their own right, and they exist in large communities. “Do they compete with each other?” she wonders. “Could some of them facilitate the entrance of others into human cells? I have no idea.”
These discoveries could have potential medical implications. For a century, scientists have looked to phages as a way of curing bacterial diseases, without having to resort to antibiotics. Although phage therapy fell out of favor in Western countries, research continued to blossom in Eastern Europe and Russia. And in recent years, there have been some spectacular successes, in which patients were pulled back from death’s door by infusions of these viruses
Martha Clokie from the University of Leicester notes that several infectious bacteria, including those that cause tuberculosis and Lyme disease, can enter and infect human cells. “If we want to treat these diseases, having phages that can cross into human cells would be very useful,” she says. “This is a neglected research field.”


Of the many catchy quotes attributed to Albert Einstein, this may produce perhaps the most anxiety among the scientists who have come after him: “A person who has not made his great contribution to science before the age of 30 will never do so.”
The exact origins of the oft-cited statement are murky, so it’s difficult to determine whether the great theoretical physicist said it in seriousness or jest. Whatever the intention, research on the connections between age and scientific output have frequently shown that Einstein’s claim was wrong—or at least, not exactly true for everyone. The study of these connections is far from new, and the results are usually tricky to extrapolate to larger populations. An effect found for top performers in one field may not necessarily apply for high achievers in another, for example. But the topic has long fascinated researchers and writers, including Helmut Abt, an astronomer and former longtime editor at The Astrophysical Journal.
Abt has been studying trends in the professional output of scientists since the early 1980s. He has found, as others have, that a specific category of researchers—the top performers, the Nobel Prize winners, the geniuses—seem to produce the most significant contributions to their fields during their 30s, with some exceptions (what is it about classical composers?). In his most recent study, published this fall in Publications of the Astronomical Society of the Pacific, Abt sought to explore the professional output of “average” individuals, specifically those in astronomy.
Abt examined the work of a small group of astronomers who died between last October and this June. He picked as his measure of productivity—a tremendously subjective thing—the number of times their research papers had been cited in other papers over the course of their lifetimes. “Of course we know that astronomers do many useful things, such as teaching, public education, service on committees both within their institutions and nationally, etc.,” Abt wrote in the paper. “This study concentrates only on research results that directly aided research by others.”
The sample included 22 men and three women, nearly all of whom began publishing papers in their 20s. Abt sifted through the citations for each astronomer, which can be found on a public database operated by the Smithsonian Astrophysical Observatory. He found that, by this measure, average astronomers peaked in their careers at the age of 45. His analysis also suggested they do half of their most important work after the age of 50.
The findings line up nicely with earlier research on the subject. In 1874, a study on age and scientific output found that peak performance in the science and creative arts usually occurred between the ages of 35 and 40. (That could be attributed, in some part, to the fact that these individuals, er, ran out of time to peak after that; the life expectancy for men—the people most likely to be carrying out research at the time—born in 1800 was about 44 years old at birth.)
An analysis in 1953 of the most successful performers in fields like art, science, music, literature, and others found the peak ranged from 34 years in math to 44 in astronomy. The psychology professor Dean Keith Simonton, well-known for his studies on creativity and age, concludes that creativity ramps up in the beginning of one’s career, peaks at about 40 or 45, and then gradually declines.
In 2011, researchers analyzed more than 400 Nobel Prizes in physics, chemistry, and medicine between 1900 and 2008 and found, with a few exceptions, most researchers were older than 30 when they produced their most important work. Nobel Prizes, however, have long been criticized as a flawed form of recognition of scientific contributions because they overlook many of the individuals who made them happen, as my colleague Ed Yong has written about here.
The reasons for why people seem to peak at these ages have remained just as fascinating and difficult to pin down as the effect itself. The researchers behind the 2011 analysis suggest a shift in the last century from theoretical work, where they say younger people tend to be more successful, toward experimental work, which usually requires a mature foundation of knowledge to carry out. Other factors include health and access to expensive equipment, which would be easier for older individuals to get. Abt speculates older astronomers may take high-level positions at universities and scientific institutions that remove them from the daily grind of research.
According to his own research, Abt has already peaked. He turned 92 this year. In 1952, when he was 27, Abt received the first doctoral degree in astrophysics the California Institute of Technology awarded, for his work on pulsating stars.
“I don’t think what I did in the next three years was necessarily some of my best research,” he said with a laugh. “So no, what Einstein said did not apply to me. But I am not one of the top prizewinners in astronomy. I’m just an average astronomer who just loves to do research.”


Consider two American children, one rich and one poor, both brilliant. The rich one is much more likely to become an inventor, creating products that help improve America’s quality of life. The poor child probably will not.
That’s the conclusion of a new study by the Equality of Opportunity project, a team of researchers led by the Stanford economist Raj Chetty. Chetty and his team look at who becomes inventors in the United States, a career path that can contribute to vast improvements in Americans’ standard of living. They find that children from families in the the top 1 percent of income distribution are 10 times as likely to have filed for a patent as those from below-median-income families, and that white children are three times as likely to have filed a patent as black children. This means, they say, that there could be millions of “lost Einsteins”—individuals who might have become inventors and changed the course of American life, had they grown up in different neighborhoods. “There are very large gaps in innovation by income, race, and gender,” Chetty told me. “These gaps don’t seem to be about differences in ability to innovate—they seem directly related to environment.”
The discrepancy in who gets patents is not the result of innate abilities, Chetty and his team, Alex Bell of Harvard, Xavier Jaravel of the London School of Economics, Neviana Petkova of the U.S. Treasury Department, and John Van Reenen of MIT, conclude. Children from many different backgrounds excel in math and science tests in third grade, for instance. But it’s the wealthy children who do well in math and science that end up getting patents. Why? Because they have more exposure to innovation in their childhood, the researchers say. This exposure comes mostly from interacting with people who are themselves inventors. If young kids know people who are inventors, or hear conversations at the dinner table about research and innovation, they’re more likely to become interested in pursuing careers in that field, Chetty told me. “Opportunity broadly, and exposure to innovation in particular, are really the keys to increasing innovation,” he said. Chetty, for instance, grew up in a family of academics, and overheard conversations about science and making discoveries, which, he says, influenced his decision to pursue a career in academia.
Inventors Are More Likely to Come From High-Income Families
Aaron Hertzmann, who is now a principal scientist at Adobe Systems, has eight patents. He grew up in Palo Alto, where he had a computer before he was 10 years old. He had a lot of exposure to inventors as a child—his father was a “tinkerer,” Hertzmann told me, and his stepfather was an academic in the field of computer science. “He exposed me to approaching math as something to explore, rather than it just being a homework assignment,” Hertzmann said, about his stepfather. Hertzmann and his mother would tag along to his stepfather’s conferences in places like Greece, meeting other academics and hearing them talk about their work. From that, the idea bloomed in Hertzmann’s mind that academia, and specifically math and science, were areas where he could have an impact. When he graduated from college, his stepfather guided him along the process of applying to study for a Ph.D. Hertzmann, now 43, has a Ph.D. in computer science, and develops new ideas and algorithms at the intersection of art and computer science.
Indeed, exposure to certain specific fields makes children more likely to pursue a career, and a patent, in those fields, the researchers found. This is how they know that exposure, in addition to neighborhoods, is important to innovation: It would be unlikely that growing up in a good neighborhood would inspire many children to patent in the same small field.  People who grew up in Minneapolis, where there are many medical-device manufacturers, were especially likely to get patents in medical devices, for instance. Among people living in Boston as adults, those who grew up in Silicon Valley were especially likely to patent in computers. Children whose parents have patents in a specific field—say, antennas—are also likely to patent in exactly the same field as their parents did.
The Place Where the Poor Once Thrived
Women who grew up in an area where women held a higher share of patents in a certain field were more likely to themselves get patents in that area when they grew up. Strikingly, it was especially important for children to see people who looked like them as innovators for them to pursue the same career path—girls in an area with a lot of male innovators wouldn’t necessarily envision themselves in the same career, while boys would. If girls were as exposed to female inventors as boys are to male inventors, the gender gap between male and female inventors would fall by half, the researchers estimate. (They find that 82 percent of 40-year-old inventors today are male.)
These findings have big implications for the state of the U.S. economy, which has seen innovation decline in recent decades. Innovation is often measured by what’s known as “total factor productivity,” which essentially tracks advances that have been made in using existing resources to increase output. Prior to 1973, total factor productivity increased at an annual rate of 1.9 percent—but since then, that growth rate has fallen to 0.7 percent, according to the Brookings Institution. Innovation is central to economic growth, Chetty says. About half of U.S. annual GDP growth is attributed to innovation. Innovation, says Chetty, is what allows people to live richer, healthier, and more productive lives.
Much of the past work out of the Equality of Opportunity project has been motivated by ideas about justice, and the idea that everyone, regardless of where they are raised, should have a fair shot at the American Dream, Chetty told me. But these results indicate that equality of opportunity is important for another reason too: It makes the economy stronger. “Opportunity might be vital for economic growth even if you don’t care about inequality or fairness concerns,” Chetty said. “If you give kids from lower-income families better training and better opportunities, maybe they would end up contributing more to the economy and that would help everyone essentially.” Chetty and his team estimate that if women, minorities, and children from low- and middle-income families invented at the same rate as white men from high-income families, there would be four times as many inventors in America as there are today.
Chetty and his team came up with these results by linking patent applications in the U.S. between 1996 and 2014 to federal income tax returns to create a dataset of 1.2 million inventors. (The study uses patents as a way to measure an individual’s contribution to innovation.) They tracked inventors’ lives from birth to adulthood to determine who becomes an inventor. They then linked data on math test scores from third to eighth grade from children who attended New York City public schools to see if the differences in who got a patent could be related to innate ability (they aren’t). They then showed that children who grew up in commuting zones with higher patent rates are “significantly” more likely to become inventors than children who did not. They also showed that children from both low-income and high-income families who attend universities like MIT go on to patent at relatively similar rates, suggesting that it’s factors in a child’s earlier life that determine whether they go on to patent.
This follows on earlier research from the Equality of Opportunity project that shows that growing up in an impoverished area can hurt a child’s chances of achieving many of the pieces of the American Dream. Living in certain neighborhoods makes it less likely that a child will attend college, that they’ll earn more than their parents did, and that they’ll postpone having children until they marry.
This paper’s results suggest that policies that increase exposure to innovation childhood could go a long way in stimulating economic growth. Internships or mentorship programs could link children interested in math and science with innovators, for example, which might make them more likely to pursue careers in that field. Integration could also help—if children have exposure to more types of people, the people they think of as their peer group changes, and they might be more likely to pursue a career that is dominated by people who don’t look like them. That will help them succeed individually, and it could have a positive effect on the economy as a whole.


At the time, news of the breakthrough on December 2, 1942, was conveyed  only in code: “The Italian navigator has landed in the New World.”
Our “Italian navigator” was Enrico Fermi, the physicist who had escaped fascist Italy for America. The “New World” was not a place but a time: the atomic age. On that day 75 years ago, Fermi’s team set off the first controlled and sustained nuclear chain reaction.
It all happened under the bleachers of University of Chicago’s Stagg Field. Fermi’s nuclear reactor was a pile of graphite, henceforth known as Chicago Pile-1. It produced all of a half-watt of power. But it proved that a neutron emitted by a splitting uranium atom could indeed split another uranium atom, which could split another and another, releasing energy with each reaction. With enough atoms, the chain reaction could unleash inconceivable amounts of energy. It proved, in other words, that an atomic bomb could exist.
The rest of the story is well-known: Bombs were made. Bombs were dropped. Hundreds of thousands of people died. A war was won.
As all of this receded into history, the anniversary of Fermi’s experiment has became a time to reflect on the legacy of nuclear science. “It’s always been a complicated story,” says Rachel Bronson, president of the Bulletin of the Atomic Scientists, the journal founded by former Manhattan Project scientists concerned about atomic weapons. Over the past 75 years, as the specter of nuclear annihilation has grown and waned and grown again, newspapers reporting on the anniversary have tried to grapple with that legacy.
* * *
The first time an anniversary of Chicago Pile-1 was commemorated publicly appears to be its fourth in 1946, and that was by proclamation of the War Department. In an October press release, Lieutenant General L.R. Groves, the commanding general of the Manhattan Project, suggested December 2 as the “birthday” of atomic energy.
The War Department helpfully released a packet of materials for journalists who were not present at the once secretive Chicago Pile experiment. Two public-information officers interviewed more than a dozen of the 50 scientists, and many of the small but colorful details that would be retold in later commemorations originated in their report.
Details like the bottle of Chianti wine, brought in secretly by the Hungarian-born theoretical physicist Eugene P. Wigner. When the experiment succeeded, Wigner opened the bottle. The participants drank out of paper cups and signed their names on the bottle’s straw wrapper.
And details like the graphite dust that blanketed everyone. (Graphite was used as a “moderator,” to slow down neutrons so they could split uranium atoms.) Albert Wattenberg, one of the young physicists that helped build the pile, told his interviewers:
“We found out how coal miners feel. After eight hours of machining graphite, we looked as if we were made up for a minstrel. One shower would remove only the surface graphite dust. About a half-hour after the first shower the dust in the pores of your skin would start oozing. Walking around the room where we cut graphite was like walking on a dance floor. Graphite is a dry lubricant, you know, and the cement floor covered with graphite dust was slippery.”
The Chicago Pile was a genuine scientific breakthrough, but other, more famous milestones like the Trinity test and the Hiroshima bombing have also been pegged as the beginning of the atomic age. Perhaps the War Department chose December 2, 1942, as the birthday of “atomic energy”—note: not “atomic bomb,” a phrase that never appears in the press release—because it represented a purer scientific achievement. Nuclear science had not yet been used for destruction; it could just as well power our homes and save lives through medicine.
When The New York Times covered the fourth anniversary in December, science writer William L. Laurence hinted only vaguely at “incalculable potentialities for good and for evil.” Laurence is credited with coining the term “atomic age” and he is a controversial figure in journalism. During the war, he worked for the Manhattan Project as its historian. Then he returned to the Times to continue reporting on the very project for which he worked, even winning a Pulitzer for his dispatches from Nagasaki. In 2004, journalists argued his Pulitzer Prize should be revoked because of his “uncritical parroting of propaganda.” He dismissed, for example, Japanese reports that people were dying from radiation days after the bombings.
“We will probably never know the true extent to which William Laurence was co-opted, compromised, or corrupted by his military and governmental connections and involvements. It appears that in many ways, he was never really certain himself,” Mark Wolverton recently wrote in Undark. But from the very beginning, the story of the birth of the atomic age was being written by the very people who ushered it in.
In 1952—now the 10th anniversary of the experiment—the Kentucky New Era quoted Arthur Compton, the physicist who oversaw Fermi’s work, speaking at a luncheon of the Chicago Association of Commerce and Industry. (Compton was the one who spoke the words: “The Italian navigator has landed in the New World.”) Compton defended the use of the bomb, but he was more eager to stress the civilian impacts of the experiment, emphasizing energy as the War Department’s press release did:
As a scientific tool, the importance of the nuclear reactor is comparable with that of the cyclotron. As a means of improving health, it may reasonably be compared with the betatron, a new type of supervoltage instrument for producing X-rays and beta rays ... As a means of defense, I would rate the atomic weapons as comparable in importance with the airplane. But the great significance of nuclear energy seems to be as a source of useful power.
When the 25th anniversary came around in 1967, World War II was receding from memory and the Cold War had come startlingly close to turning hot. It was atomic weapons that Americans were thinking about again. Volney Wilson, another physicist who worked on the Chicago Pile, speaking to the Schenectady Gazette, was far less optimistic: “It’s been a big disappointment to me ... I would have thought that the development of this horrible weapon would have been more of a force to bring the world together.” Wilson was a pacifist who was always ambivalent about building a bomb, but his words now had a note of bitterness.
The 50th anniversary came at a more optimistic time: 1992. The Soviet Union had dissolved. The United States was the world’s only superpower. The Soviet Union was not only dismantling its warheads, it was selling them to the United States for electricity. “Highly enriched uranium from former Soviet weapons once targeted on our cities will be used to light and heat those same cities as fuel in American nuclear power plants,” William S. Lee, president of Duke Power, said at a November 1992 meeting of the American Nuclear Society.
But, it was not lost on journalists that this was still the atomic age.  Articles written for the 50th anniversary note that Russia and the United States still had enough nuclear weapons to kill millions, and several other countries were pursuing their own. “Fifty years later, the legacy of the Chicago Pile remains mixed,” Earl Lane wrote in Newsday.
Which brings us to the75th anniversary of the Chicago Pile. Nuclear power is on the decline in the United States today. Nuclear weapons are ever present in the news again. Yet nuclear science has also produced real breakthroughs in science and medicine. The legacy of the Chicago Pile is mixed, and it probably always will be—until, and such is the nature of nuclear weapons, the day it is clearly not.


When Bernadette Demientieff was in high school, she gave up her heritage. Demientieff is a member of the Gwich’in, an indigenous tribe of roughly 9,000 people that spans north-central Alaska and northern Canada. “The ways of living in this world that are being pushed on our people” got to her, she told me. She moved south to Fairbanks, Alaska, and grew disconnected from her people and their land. She had kids. She grew up.
And then, one day in 2014, something called to her, she says. She was in Arctic Village, a small Gwich’in settlement at the edge of Alaska’s wilderness. She felt the urge to step out onto the tundra. She started walking, up and out of the center of town—and then she turned around and looked: In front of her stretched the Arctic National Wildlife Refuge, the largest area of untouched wilderness in the United States. The land, an open expanse of peaks and rivers, spanned hundreds of miles past the horizon to the unseen, icy flat of the Arctic Ocean.
“I started crying and crying,” she said. “And I asked the Creator for forgiveness.”
Now 42, Demientieff is the executive director of the Gwich’in Steering Committee. She has spent years trying to protect the Arctic National Wildlife Refuge, or ANWR (pronounced AN-wahr), from oil and gas exploration. That fight suffered a major loss Saturday, the result of lawmakers voting on an expansive and quickly written bill several thousand miles away.
The Tax Cuts and Jobs Act, which the Senate passed early Saturday morning, will change federal law on a matter that has little to do with the tax code. The bill authorizes the sale of oil and gas leases in a section of the ANWR on Alaska’s North Slope, the coastal plain that faces the Arctic Ocean. Soon, energy companies will be able to search for—and extract—oil and gas from the frozen tundra.
The Senate bill will now be reconciled with the House version in conference and go to President Donald Trump’s desk for his signature.
It brings a quiet end to the battle over whether to drill in the ANWR, one of the longest-running and most acrimonious battles in U.S. environmental history. The question has been embedded in federal law for 40 years, nearly as long as Alaska has been a state.
No one will be more affected by the opening of ANWR than Alaska’s indigenous people, who will live among—and work on—the rigs, drills, and pipelines that would follow the discovery of any oil or gas reserve. The discovery of oil or gas in the region could bring an economic windfall to the subsistence tribes that live on Alaska’s North Slope, the coastal plain that faces the Arctic Ocean. But if a major disaster—like an oil spill or gas leak—were to occur in the area, it would devastate their only homeland.
The issue still divides villages, counties, and Native nations in Alaska. It also sets tribes with differing claims to Alaska’s North Slope against each other. And both sides tend to assert that the overall public sides with them.
“The majority of Alaskans and majority of Alaksa Natives express their support for [drilling in ANWR]. It’s an issue of economic self-determination for our community,” said Richard Glenn, a member of the Inupiat tribe and the executive vice president of the Arctic Slope Regional Corporation, which owns nearly 5 million acres across Alaska’s northern coast. “This has been the unchanging position of the majority of the residents of our region for more 30 years.”
No recent polling data seems to be available on the issue. But even beyond public opinion, there’s a basic conflict.
The Inupiat live across the North Slope, including within the part of the ANWR that would soon be opened for drilling. Oil exploration already brings jobs and funds infrastructure in their communities. And the Arctic Slope Regional Corporation holds mineral rights to pockets of private land within the ANWR. If oil is discovered there, the corporation and its shareholders—roughly 13,00 members of the Inupiat tribe—could profit from the wealth.
The Gwich’in people, meanwhile, live hundreds of miles south in west-central Alaska. Their regional corporation does not own land on the North Slope. But the Gwich’in are spiritually connected to the porcupine caribou, a herd of more than 160,000 creatures who migrate annually across the U.S. and Canadian tundra. The herd’s calving grounds, the most sacred space to the Gwich’in, lies within the area which could soon be open to drilling. To many of them, drilling in the calving ground isn’t just an attack on the Gwich’in way of life. It’s an attack on the Gwich’in.
The immediate stakes of the fight go back to 1980, when Congress passed the Alaska National Interest Lands Conservation Act. The law protected more than 67,000 square miles of land (174,000 square km) across Alaska by establishing new national parks, national monuments, and wilderness areas. One part of the law, Section 1002, set aside 2,300 square miles of land (6,070 square km) on Alaska’s North Slope. Though this parcel of land—dubbed the “Section 1002 area”—was made part of the ANWR, Congress did not endow it with full wilderness protections and reserved the right to open it to gas exploration in the future.
Every decade or so, the question of whether to open the 1002 area has made it onto Congress’s agenda. In 2005, the Senate nearly opened ANWR to drilling before a Democratic filibuster turned the tide of public opinion. Ten years earlier, Bill Clinton vetoed a proposal to open ANWR. The closest Congress ever came to completing a deal was 1989, but the Exxon Valdez oil spill intervened, rendering the loosening of fossil-fuel rules in Alaska politically impossible.
While public opinion has always previously halted the opening of the ANWR, in today’s supersaturated news environment, Senate Republicans have slipped the drilling provision into the tax-reform bill without attracting the same outcry. Senator Lisa Murkowski, a Republican of Alaska, made drilling in ANWR a condition of her support for the tax bill, and it has been a de facto part of the legislative package since October.
It’s unclear how Americans feel about the ANWR proposal. A Morning Consult poll in 2014 found that only 50 percent of voters want to drill in the area, even though many more—61 percent—support increasing oil extraction in the United States. Neither a majority of Democrats nor Independents supported drilling.*
The modern history of the Gwich’in people is inseparable from drilling in the ANWR question.
In 1988, Congress began exploring how to open the refuge to drilling. The Gwich’in could not abide the thought. They held their first conference in 150 years, an unprecedented gathering of their people from across the United States and Canada. Demientieff remembers those early meetings—sitting on the floor as a little girl, hearing the fear and anguish of the adults. The elders gave her little tasks to keep her involved, like passing out crackers and dried fish to the members, as angry, passionate arguments raged around them.
No one ever sat her down and told her about the preciousness of ANWR. But just by growing up among the Gwich’in, she learned the importance of her people’s generations-old circuit through the land, in pursuit of the porcupine-caribou herd. Every year, more than 160,000 porcupine caribou move across the high Arctic tundra. Their journey starts in the meadows around the Porcupine River, which flows through modern-day Yukon and Northwest Territories in Canada. Then the caribou come north, through the vast, peak-rimmed plains of the ANWR, until they arrive at their calving ground on Alaska’s coastal plane, at the foot of the Arctic Ocean—and in the Section 1002 area.
The Gwich’in have followed the caribou across much of this odyssey for tens of thousands of years. But they knew not to enter the calving ground, which is called Iizhik Gwats’an Gwnadaii Goodlit, “the sacred place where life begins.” Even in the famines which followed first contact with the West, the Gwich’in did not trespass on the calving ground, Demientieff told me. “The porcupine caribou herd and the Gwich’in people are one,” she said. “I’m not just making up numbers when I say that we migrated with them for 20,000 years. These caribou have been here for 2 million years.”
Caribou aren’t the only animals that live in the ANWR. Polar bears, brown bears, and black bears all trundle through its streams and meadows. Lynx, moose, Arctic fox, walrus, and ringed seal lounge on the Arctic coast. And migratory birds—including merlins, sandpipers, and peregrine falcons—summer in the reserve before returning to the continental United States for the winter.
Even though the bill passed, Demientieff and a group of Gwich’in and Inupiat people who oppose the drilling plan to visit Washington, D.C., later this month, on the 57th anniversary of the creation of ANWR. They will drum and dance and sing and visit with members of Congress.
But they will shun Lisa Murkowski, their senator. The Murkowski family has pushed to open the ANWR as long as they’ve worked in politics. Frank Murkowski served in the U.S. Senate for more than 20 years. He led successive efforts to open ANWR to oil drilling, all of which failed. When he was elected governor in 2002, he resigned from the Senate and named the majority leader of the Alaska House of Representatives—his daughter, Lisa—to his old seat. She now chairs the Energy and Natural Resources Committee.
The ANWR provisions were widely seen as the chit Murkowski needed to support the tax bill. “For Murkowski to just turn as soon as they put that in there—it’s like they’re doing that just for her vote, and she’s falling for it,” Demientieff argues.
Protest isn’t the only way she is marking the anniversary. Demientieff is expecting a fourth grandchild due December 6, the anniversary of ANWR’s creation. She has five children, all Gwich’in or Gwich’in-Yupik.** Her 9-year-old daughter, Lexine, is part of Our Children’s Trust, the group of kids suing the federal government for its lack of a climate policy. Demientieff calls her “my little Gwich’in warrior.”
I asked Demientieff what it would feel like to know drilling would go through at ANWR. “Just when you said that right now, I got a big lump in my throat,” she told me. “We shouldn’t have to be fighting for our human rights. We’re not asking for anything, we’re not asking for money, we just want to continue our identity as Gwich’in. And that identity—a big, huge part of it is the porcupine-caribou herd.”
The history of drilling in ANWR arises from the history of the state of Alaska. When Alaska was a territory, the federal government owned virtually all of its land. Upon granting it statehood in 1959, the government transferred about a quarter of Alaska’s total area to the state government. The next year, President Dwight Eisenhower also established federal protections on the area that would become ANWR.
Eleven years later, the U.S. government transferred more federal land—about a third of the state’s area—to Native tribes in Alaska. But it didn’t transfer the land to them directly. Instead, it established regional corporations to hold the land for tribal shareholders.
Glenn manages land and natural resources for one of those companies, the Arctic Slope Regional Corporation, which owns acreage across the northern coast. The corporation has fought for the right to drill for years, and he marveled at how speedily it had been approved now. “It’s a faster-moving issue than it has been in the past 30 years,” he said.
The Republican tax bill assumes that drilling in ANWR will generate $1 billion in federal revenue over the next 10 years. During its last survey of the region, the U.S. Geological Survey said that 12 billion barrels of recoverable crude oil may lie beneath the reserve.
Glenn brushed away the estimates. “The USGS has published potential reserves, and I’m sure the industry explorers have their own number,” he told me. “I’m a geologist by training, and the only thing that proves a reserve is a drill bit.”
What was important, he said, was that the process of looking for oil was allowed to continue. It was this search for oil that drove the Inupiat communities on the North Slope. “Our region basically depends on continued safe, responsible exploration and development,” he told me. “It’s improved the quality of life in our community only because we’ve been able to tax the presence of the oil industry in our region.”
He pointed to Point Hope, a town of roughly 630 people on the western tip of Alaska’s Lisburne peninsula. “The runway there, the roads, the schools, and the ability to flush a toilet are only because of the presence of taxable revenue in our region. We’re all native folks, and we depend on our environment for our sustenance. But we also depend on having communities to come home to,” he said.
Glenn grew up in the small city of Utqiaġvik, commonly known as Barrow, on Alaska’s North Slope. The 54-year-old now lives in Anchorage, but remains active in Inupiat life on the North Slope: He is a subsistence hunter and fisher, and a whaling captain.
He described how he’ll often encounter human skeletons on his hunt—a relic of the time when aging members of a community might wander away from a village or tent in the winter, because they had become too much of a burden to the community. Seeing the skeletons, he said, “means our whole region is sacred land.” But he takes a decidedly pragmatic approach to the holiness of the land. “The environment is just as sacred to us as it is to everyone else, but you know what? We need an airstrip,” he said. “Even the place where we built a sewage lagoon for our village is sacred land—but we needed a sewage lagoon.”
These pieces of infrastructure were not just niceties, but the basis of their community, he told me. “When you get dropped off on the tundra in our region in winter or summer, you’ll never be so happy to find a little bit of infrastructure.”
Glenn believes the region can endure industry in part because it’s done so before. From the 1950s to the 1980s, a chain of U.S. Air Force bases lined the North Slope of Alaska. Together they formed the Distant Early Warning system, a Christmas-light string of installations that linked Alaska to central Greenland, scanning the sky for missiles. Each DEW base brought “power plants and landing strips and people,” says Glenn, but the region persevered. “The animals were there; the animals survived. The people were there; the national psyche survived,” he said. “There’s been industry and Natives all across the coastal plain.” (Glenn is, in part, a product of the DEW line: His dad, a white American, met his Inupiat-Eskimo mom while stationed in the high Arctic.)
I asked Glenn how he, as a geologist who has studied Arctic sea ice, thinks about the climate consequences of extracting oil from the North Slope. “I think about it all the time,” he said. “But the reality is that our region depends on oil and gas development. If we stop exploration, our communities dry up. And [by stopping exploration] we don’t change the climate one bit—it just means someone’s gonna open up the valve somewhere else.”
Not all Inupiat approve of drilling in ANWR, however.
“For me, it’s disappointing. We should have an area that we’re able to have added protections, where we don’t change what’s natural to our environment,” said Rosemary Ahtuangaruak, an Inupiat woman and a public-health advocate who lives in Nuiqsut, Alaska.
Nuiqsut, a village of 500, sits on the North Slope, to the west of Prudhoe Bay. It is surrounded by the oil and gas industry. Ahtuangaruak told me that some nights the particulate-matter pollution from the natutal-gas flares gets so bad that she has to stay up all night, tending to people in respiratory distress.
She also mourned what the drilling would do to the caribou. “There are four major herds in our community. Three of the herds are in severe declines. The only herd that isn’t is the porcupine-caribou herds,” she said.
Ahtuangaruak doubted that ASRC always acted in the best interests of the tribe. They were a corporation, she said, and not a government. “Their priority is profitability at all costs. But when we’re tribal people, our priority is our way of life and who we are in the future.”
Sharon Lord also opposes the ANWR plan. She operates a bed-and-breakfast in Kaktovik, Alaska, a village of several hundred people that is the sole settlement within the Section 1002 area. Lord’s father, Robert Thompson, is a famous anti-drilling activist. “This land is beautiful,” she says. “And I like our lifestyle the way it is. I don’t want it to change. If the oil company comes in here, they’ll turn it into an industrial area. ”
She adds: “There’s always a potential for an oil spill. There’s absolutely no way an oil spill can be cleaned up here. It would create an environment of irreversible damage.”
Most of the community, she said, opposed the drilling.
If that’s true, then it hasn’t reached Matthew Rexford, the Inupiat tribal administrator of Kaktovik. In November, he endorsed the drilling plan in testimony to Congress. “The Arctic Inupiat will not become conservation refugees,” he said at the time. “We do not approve of efforts to turn our homeland into one giant national park, which literally guarantees us a fate with no economy, no jobs, reduced subsistence and no hope for the future of our people.”
He delivered those remarks sitting next to Richard Glenn. Weeks later, back in Anchorage, Glenn encouraged me to “find something to help you sort the wheat from the chaff” on the emotion behind the ANWR drilling plan.
“The habit of journalists tends to be to find Native folks on both sides of the issue, and leave it to the reader to decide,” he said. “It would be nice if we could rise above that and say, what do those folks closest to the issue think?”
* This article previously misstated the proportion of Democrats and Independents that opposed drilling.
** This article previously misstated the tribal identity of Demientieff’s children. We regret the errors.


Earlier this May, Gregory Holt had just finished doing the morning rounds at Miami’s Jackson Memorial Hospital, when he got a call about a new patient in the emergency room. He went down with seven colleagues to find an unconscious 70-year-old man with breathing problems and signs of septic shock. He was alone and had no identification. His blood was full of alcohol, and its pressure was dropping. And when the doctors peeled back his shirt, they found a tattoo, running along his collarbones.
It said: DO NOT RESUSCITATE.
The NOT was underlined. There was a signature under the final word.
Holt was shocked. “We’ve always joked about this, but holy crap, this man actually did it,” he says. “You look at it, laugh a little, and then go: Oh no, I actually have to deal with this.”
By default, doctors would treat patients in this man’s condition as if they were “full code”—that is, they’d want everything possible done to prolong their life. “When faced with uncertainty, you pick the choice that’s not irreversible,” Holt explains. “So let’s do everything we can and when the dust settles, we can determine what the patient wanted if it wasn’t clear from the beginning. The tattoo threw a monkey wrench into the decision.”
In Florida, patients can ask not to be resuscitated by filling in an official form and printing it on yellow paper. (Yes, it has to be yellow.) Only then is it legally valid. Clearly, a tattoo doesn’t count. And yet, the patient had clearly gone through unusual effort to make his wishes known. The team members debated what to do, and while opinions differed, “we were all unanimous in our confusion,” says Holt.
They decided to temporarily ignore the tattoo, at least until they could get advice. In the meantime, they gave the man basic treatments—antibiotics, an IV drip, an oxygen mask to help him to breathe, and adrenaline for his plummeting blood pressure. But they avoided putting a tube down his throat and hooking him up to a ventilator. “It would have hurt to see a man with a DNR tattoo having a tracheal tube hanging out of him,” Holt says.
All of this bought them enough time to get a hold of Ken Goodman, the codirector of the University of Miami’s ethics programs. “My view was that someone does not go to the trouble of getting such a tattoo without forethought and mindfulness,” Goodman says. “As unorthodox as it is, you do get a dramatic view of what this patient would want.”
But tattoos are permanent and desires are fleeting, so the team pondered whether the words actually represented the man’s desires. And there’s good reason to be cautious. Back in 2012, Lori Cooper at the California Pacific Medical Center was caring for a (conscious) patient who was going to have a leg amputated, when she noticed a “DNR” tattoo on his chest. The man revealed that he got the tattoo after losing a poker bet many years ago, and actually, he would very much like to be resuscitated if the need arose. “It was suggested that he consider tattoo removal to circumvent future confusion about his code status,” Cooper wrote. “He stated he did not think anyone would take his tattoo seriously and declined tattoo removal.”
Holt’s unconscious patient couldn’t weigh in, but social workers used his fingerprints to track down his identity. He had come from a nursing facility, and to everyone’s immense relief, they had an official DNR form for him, printed on the requisite yellow paper. The man’s condition deteriorated, and he passed away in the night.
The team did the right thing, says Nancy Berlinger from the Hastings Center. They provided basic care to buy time, called for an ethics consult, and got social workers involved. “Even if the records weren’t there, it was right to honor the patient’s preferences,” she says. “Paper gets lost, and some people do not trust paper. This man may have been trying to safeguard against that, and [the tattoo] might have been the most reliable way to make his voice heard. It was right to take it seriously.”
But Lauris Kaldjian, an ethicist at the University of Iowa, says he wouldn’t have honored the tattoo without finding the official form. A DNR order isn’t an end in itself, he says. It’s a reflection of a patient’s goals—how they want their life to end. Patients are meant to discuss those goals with a physician so they can hear all the options available to them, and make an informed decision; the physician must then sign the order. “That’s not meant to be a paternalistic move,” Kaldjian says. “It’s meant to give evidence that a rational discussion was had, and I don’t think tattoo parlors are a place to have to have a code-status discussion.”
It’s the discussion that matters, not the words on the form (or the tattoo), says Joan Teno from the University of Washington, who studies end-of-life wishes. And in many cases, those discussions don’t happen, or aren’t respected. In a study of bereaved family members, she found that one in 10 say that something was done in the last month of a patient’s life that went against their wishes. “The fact that someone has to resort to a tattoo to have their wishes honored is a sad indictment of our medical system,” Teno says. “We need to create systems of care where patients have the trust and confidence that their wishes will be honored. That’s the important message from this case.”


By our third day at sea, we’d found it: a dozen bare and jagged piles of rock surrounded by ocean the color of Windex. It was smaller than I had imagined, all told about twice the size of a soccer field. There was no white sand, no volcanic peaks, no palm trees, none of the trappings of other tropical island chains at this latitude, just razor-sharp umber peaks iced in a thousand years of bird shit—the whole of it resembling a kind of sinister Gilligan’s Island.
But we didn’t motor from the coast of Brazil more than a thousand kilometers (620 miles) across the Atlantic on a beach holiday or three-hour tour. We came to explore the waters deep below the sunlit surface. We came to collect clues from this place, known as the Saint Peter and Saint Paul Archipelago, about how life on Earth first began—and how alien life might evolve on other planets in the solar system.
These are big, serious questions, and we’ve brought a big, serious team to investigate them, including a crew of more than 40 geologists, microbiologists, geophysicists, biologists, engineers, deep-ocean divers, and deckhands from a dozen nations. The team will spend the next two weeks aboard the MV Alucia—a 56-meter (184-foot) research vessel operated through the Dalio Ocean Initiative—scanning the ocean floor, sampling rocks, analyzing water samples, and diving research submarines a thousand meters beneath the surface.
Nobody has ever explored these deep waters, and no one on the team knows what we’ll find.
“It’s a unique area, and so it might host some unique life systems,” says Frieder Klein, a marine geologist, who’s leading the scientific team from Woods Hole Oceanographic Institution. Klein is standing barefoot on Alucia’s top deck in cargo shorts and a faded MC5 T-shirt, squinting in the noonday sun. A few hundred meters to the north, waves crash and fizz on the shores of the 15 bite-sized rock islets.
Klein tells me that below us, millions of years ago, the tectonic plates of the Mid-Atlantic Ridge began splitting apart. This gap has widened by about a finger’s width every year since, which is why Europe and North America are now separated by nearly 7,000 kilometers (4,350 miles) of ocean. Over the course of this very slow process, mantle rock, which usually lies hidden 6 kilometers (3.7 miles) below the crust, has been forced to the surface.
Mantle rock isn’t particularly rare; it covers wide swaths of the deep seafloor around the globe. Here, though, it’s much shallower, much more accessible—and it’s continuing to evolve as it interacts with seawater. “There is really no place like this in the world,” Klein says, wiping his forehead of sweat.
The rock here could also be harboring entirely new forms of life. Klein explains that a chemical reaction between seawater and the iron in mantle minerals creates hydrogen molecules. Microbes, single-celled or multicelled microorganisms, feed off this hydrogen. These organisms are similar to those that existed on Earth billions of years ago and may be closely related to our planet’s earliest life forms. Klein and his team will seek out microbes in the deep and analyze the chemical processes within the mantle rocks as they occur. In doing so, the scientists hope to catch a glimpse of early life systems—a sort of window back in time to our most primitive selves, and perhaps to our alien counterparts.
“Icy moons of Saturn and Jupiter, Europa and Enceladus, have water below their surfaces; we know that,” says Klein. “And these moons contain the same rocks that are on these islands.” If distant moons in our solar system have the same rock, and the same water, they could have the chemical processes that feed the same basic forms of life here on Earth.
Klein and I peer over the handrail and stare down into the ocean depths that plummet 4,000 meters (13,100 feet), the equivalent of 10 Empire State Buildings stacked on top of one another. Suddenly, it feels less like we’re on a boat looking into the surface and more like we’re on a spacecraft hovering over some alien world.
“We’re going where nobody has gone before,” says Klein. The captain cuts the motor and the Alucia gently drifts toward the southernmost islet. Klein gathers his phone and a water bottle, excuses himself, and hustles downstairs. After a year and a half of planning, it’s time to go deep.
* * *
It’s a hard thing to fathom, the concept that you, me, the birds, and the bees—all life that is and has ever been—came from a few chemical reactions on some ugly rocks a few billion years ago. Proposing such a theory in the 1500s would have likely gotten you beheaded for heresy. Even 50 years ago, it might have gotten you canned from a tenure-track teaching job, or at least ostracized by the scientific community.
That all changed in 1977 when an Oregon State University marine geologist named Jack Corliss chartered a research vessel off the coast of Ecuador and steamed out some 320 kilometers (200 miles) to the Galapagos Trench. Corliss suspected that a volcano, what marine scientists call a hydrothermal vent, was erupting on the deep seafloor in the area. Corliss and his crew deployed a remotely operated vehicle rigged with a camera named ANGUS to investigate. In one particular spot, at a depth of around 2,500 meters (8,200 feet), ANGUS’s temperature gauge registered a significant spike. After several hours, the team lugged ANGUS back on deck, cracked open the camera, and developed the film.
The 13 grainy photographs that ANGUS had captured when the temperature spike occurred revealed something extraordinary. There was life down there—crabs, mussels, lobsters, worms—all flourishing in complete darkness around a toxic plume of seawater hot enough at its source to melt lead. The incredible pressure, 250 times that on the surface, kept the water from turning to steam. Corliss had found a pressure cooker of life. And not only were all of the animals in this pressure cooker new to science; but, even stranger, they survived in an entirely different biological system.
Unlike surface life, which requires the sun’s light to survive, these life forms lived off the chemical energy in these superhot toxic plumes—a process called chemosynthesis. Corliss called the place the “Garden of Eden.”
In the years that followed, researchers would find more chemosynthetic communities on seafloors around the world. The deep ocean, it appeared, was not a wasteland but a sort of galaxy composed of independent biospheres, each orbiting around its own life-giving chemical “sun.” And the animals and microbes there had been flourishing for billions of years, perhaps longer than life in the terrestrial world.
Hydrothermal vent discoveries spurred geologists and microbiologists to dig even deeper, into even more extreme environments, on a quest to find the absolute limit of deep life. They drilled 3,600 meters (nearly 12,000 feet) through Antarctic ice and discovered an underground lake twice the size of Delaware that has likely been sealed off from the surface for 15 million years. In a single half-liter of water, they discovered thousands of bacteria that could survive in nearly every imaginable environment: extreme heat up to 122 degrees Celsius (252 Fahrenheit), extreme cold to -20 degrees Celsius (-4 Fahrenheit), acidic, alkaline, aerobic, anaerobic, and everything in between.
Then researchers plumbed beneath the seafloor of the world’s deepest ocean—nearly 11,000 meters beneath the surface to one of the most inhospitable environments on the planet. They found twice the amount of microbial life that had been discovered at milder, shallower depths.
They dug into the terrestrial surface as well, more than 4 kilometers (2.4 miles) through the crust, to find life forms steaming in water and sulfur that fed not off of the sun, or chemicals, but on radiation from the surrounding rocks. And this stuff had been living there for millions of years.
These discoveries suggest that there is virtually no limit to life. Even at the Earth’s most vicious extremes—from the edges of volcanic calderas to the black water pressurized to 15,000 pounds per square inch to radioactive waste sites—life finds a way. Life persists.
It turns out that in many ways, rocks at the bottom of the deep sea, buried under a mile of Earth’s crust, or covered in bird crap on Saint Peter and Saint Paul Archipelago aren’t inanimate objects at all. They are undulating, “breathing” systems crammed with organisms so tiny and metabolizing so slowly that nobody ever noticed. Until a few hardy scientists started looking.
Most researchers never bothered. Seeking out extreme life requires traveling to some of Earth’s most far-flung and miserable environments. Only a handful of microbiologists and geologists have had the will, fortitude, and resources to endure weeks in the triple-digit heat within African mines, or months in the frozen expanses of Antarctica, or years sifting through the polluted oil fields of Dagestan to find answers.
Which makes it all so much more guilt-inducing for our team to be lounging aboard the Alucia. While the dangers and discomforts of bobbing around in the middle of the Atlantic 600 miles from the nearest hospital are real and many, we’re at least comforted by the borderline luxury of our living and work quarters. Here every square inch of interior space is acclimatized to a refreshing and humidity-free 72 degrees Fahrenheit—so brisk that some of us futz around wearing sweaters and socks while the outside temperature climbs toward 100.
Tonight’s dinner, served in the mess hall, includes quinoa, steak, chicken, sautéed green beans, roasted potatoes, farmer’s salad, and homemade crème brûlée for dessert.
I grab a plate and scoot next to the two other lead researchers on the scientific team. Diva Amon is a deep-sea biologist from the Natural History Museum of London. She grew up swimming off the shores of Trinidad and Tobago and at an early age was fascinated with the diversity of sea life, especially the animals that dwelled beneath the curtain of permanently black waters. She’s come on the expedition with the hope of finding large-scale chemosynthetic life, such as crabs, tube worms, shrimp, or whatever else may lie beneath.
“We really have not even a basic understanding of many animals in the ocean, especially chemosynthetic life—how they live, where they live, and why,” she says.
While the deep ocean below 200 meters (650 feet) represents 70 percent of the habitable space of the planet, Amon tells me that less than 1 percent of it has been explored. The largest animal communities and the majority of biomass on the planet live there. And the threats they face are many. Pollution, trawl fishing, mining, and climate change all put this environment and the estimated 750,000 undiscovered species down there at risk.
“We could be destroying the deep-ocean habitat, and its inhabitants, before we even know what’s there,” says Amon. “I think it’s imperative to document it all while we still can.”
Sitting beside Amon is Florence “Flo” Schubotz, a geochemist from MARUM Center for Marine Environmental Sciences at the University of Bremen, Germany. She’s come here for the same reasons as Amon, but her interests are smaller: microscopic life living within the mantle rock.
“You think about it, the organisms that live at hydrothermal vents could be some of the earliest [forms of] life on the planet—around way before life on land,” says Schubotz, who is wearing a T-shirt from a past expedition with the Japan Agency for Marine-Earth Science and Technology. “These are ancient systems.”
Schubotz explains that 3.8 billion years ago, there was scant oxygen in the atmosphere. Life relied on other chemicals to survive, including hydrogen, carbon dioxide, and methane. As these primitive organisms flourished, some of them—the cyanobacteria—evolved a type of metabolism that produced oxygen as a waste product. Around 2.4 billion years ago there was enough “waste gas” oxygen to support new forms of aerobic “oxygen-consuming” life. These oxygen-fueled life forms grew more complex, eventually evolving into plants and animals, which eventually became us.
To see these microbes in action, Schubotz hopes to collect samples of deepwater mantle rock and feed it different chemical “foods” such as hydrogen, carbon dioxide, and methane to try to spark to life dormant organisms that those samples might contain.
In a sense, she’s hoping to create a few test-tube Jurassic Parks, but instead of a human-eating T. rex, she’ll be reanimating ancient microbes.
* * *
The following day, Schubotz, Amon, and Klein are standing in Alucia’s mission control, a dimly lit room plastered in wall-to-wall flashing monitors. Everyone is staring wide-eyed at a gargantuan video display of what looks like a rainbow pie with a few pieces missing.
With every passing second, a few more pixelated lines appear on the image and the pie is a little more complete. Klein is entranced, subconsciously oohing and aahing like a stockbroker watching the ticker of an initial public offering.
What the scientists are looking at is a high-resolution bathymetric map (the underwater equivalent of a topographic map) of the seafloor below us—a rendering of data gathered by the Multibeam Echo Sounder System, a sophisticated sonar device, mounted to the bottom of the Alucia. For the next two days the ship will circle the archipelago, moving farther away with every round, like a needle on a vinyl record playing in reverse. As we pass over the ocean floor the Multibeam will scan every nook and cranny down to a resolution of about 3 meters (9.8 feet) and down to a depth of 1,200 meters (4,000 feet).
“Nobody has ever seen any of this before,” says Klein. “It’s all very exciting.” He is looking for anachronisms on the otherwise rather featureless underwater cliff. If active hydrothermal vents are here, they’ll likely be marked by telltale spires of carbonate.
Carbonate is a common substance that can be forged from many different processes. Calcium carbonate, which makes up the shells of marine organisms, covers more than half of the ocean floor. The remains of these dead organisms makes up the white stuff in the toothpaste you brushed your teeth with this morning and are in the concrete of the sidewalk you walked on outside your front door.
But the carbonate Klein is looking for is likely not created by biological activities but from minerals coming out of solution when scalding hydrothermal fluids meet cold seawater.
“There’s something promising here,” says Klein. He points to a peculiar outcropping on the western slope of the island chain. He says it seems unlikely that a rock just tumbled down from above and landed in this spot. It looks as if the structure emerged from the rocks below. “At minimum,” says Klein, “this area is worth exploring.”
* * *
We spend the next few days battling high winds and strong currents. Still, the geochemists are able to take a half-dozen water samples from around the outcropping Klein identified on the map, and deeper on the ocean floor. The water in the area has elevated levels of methane, well above what is considered normal. It’s a promising sign.
Some of our planet’s oldest life forms survived on methane and are still found around hydrothermal vents. While some organisms fed off hydrogen and carbon dioxide and expelled methane as a waste product, others fed off methane and expelled carbon dioxide. The combinations and uses may vary, but what we do know is that the presence of carbon dioxide, hydrogen, and methane usually signifies an environment that can support primitive life forms.
Which is what made it so exciting to microbiologists when, in April 2015, NASA’s recently retired Cassini spacecraft flew by Saturn’s icy moon, Enceladus, and discovered huge amounts of hydrogen spewing from its surface. Not only that, but the plumes also contained carbon dioxide and other organics, and enough energy to support huge colonies of microbial life—what one geochemist described as “the caloric equivalent of 300 pizzas per hour.”
The chemicals on Enceladus are believed to be produced continuously by vent systems similar to those on our planet, and possibly right below us at Saint Peter and Saint Paul Archipelago.
By midmorning, Klein and Amon are hoping to find out. On the aft deck the Alucia crew rolls out Nadir, a three-person submarine rigged with half a dozen cameras and lights. We watch from the outside of the sub’s acrylic pressure sphere as Amon, the submarine pilot, and a videographer squirm in their seats, unpack water bottles, and ready themselves for takeoff.
Behind them, Klein is seated in Deep Rover, a smaller, more agile two-person submarine. The plan is for Klein to grab as many samples as he can with Deep Rover’s mechanical arms while Amon makes observations of the ecosystem and any chemosynthetic animals that might live there.
Carefully, slowly, a crane lifts Nadir, then Deep Rover, over the deck and plops them into the water. A plume of bubbles, a few goodbye waves, and the subs dip below the surface, growing smaller and fuzzier until they disappear.
For the next six hours we’ll sit around, staring at the sonar readouts, waiting, watching, and listening for signs of life.
* * *
By evening, the Alucia is once again a flurry of activity. The crew is hosing off the subs, Klein is bustling around buckets of rock samples, and geochemists Sean Sylva and Jeff Seewald are cooking up water samples that Deep Rover sucked up from around the seafloor.
On deck, Sylva places a water sample in a gas chromatograph, what looks like a steampunk version of a mid-1980s microwave. Sprouting from the chromatograph’s sides is a rat’s nest of wires, tubes, and knobs all held together by wood clamps. The tubes and wires, of course, have a purpose. As the water heats up in the oven, the compounds in the water will travel through the tubes at different speeds, depending on their size. A computer rigged to the device will analyze the speed at which the compounds move, allowing the team to gauge the proportion of methane and other chemicals in the water from that site.
Meanwhile, in an adjoining makeshift laboratory, Klein and Schubotz are inspecting rock samples that Deep Rover grabbed from a depth of more than 500 meters (1,640 feet). “I got three ugly rocks, and one big freaking rock,” says Klein, wiping his hands on an old Amoeba Records T-shirt.
Klein puts a rock in my hand and points out a web of tiny veins. He explains that when olivine, a common mineral made up of magnesium, iron, silicon, and oxygen, comes into contact with seawater, it destabilizes, allowing the water to penetrate more deeply into the rock. These little veins act as rivers for life forms within the rock, delivering energy and nutrients and removing waste. Over time, the olivine slowly dissolves and other minerals form within the veins. This process creates a marbleized rock that the ancient Romans called verde antico, or what geologists like Klein call serpentinite.
“What this rock says is, yes, the process of serpentinization has occurred on Saint Peter and Saint Paul Archipelago,” says Klein. “But are we just looking here at an archive of some past process? That’s what we need to figure out.”
As Schubotz and Klein slice and pulverize the rock samples and Sylva and Seewald vaporize seawater, I step outside to the roof deck to get a breath of fresh air. It’s twilight and the night sky is already so wet with stars it looks as if there’s more light than black.
I read a scientific study hours earlier in which researchers described collecting and comparing microbes from some of the most disparate and remote areas of the planet. Nineteen of those microbes were genetically identical, regardless of where they were collected.
Microbes can’t just get up and walk, or fly, or swim from one location to another. And even though some of these identical 19 microbes were separated by more than 16,000 kilometers (nearly 10,000 miles), they metabolized food in the same way, replicated in the same way, and shared the exact same DNA. How did all these identical life forms find themselves in these far-flung locations? It would be a bit like finding members of the Osmond family on every planet in our solar system, and beyond.
Standing beneath the canopy of stars and moons and planets I can’t help but wonder: Since we all start with the same basic building blocks, might all life follow the same path? Billions of alien habitats above and below where I stand are made of the same rocks, the same water, and susceptible to the same chemical reactions that first sparked life here on Earth—and eventually evolved into the hands that are scribbling down these words and the eyes that are staring at those stars.
How many other eyes made from the same stuff, the same reactions may be staring back at us now?
This thought is quaint, for sure, reminiscent of late-night, freshman-year philosophy rants, and most likely fueled by the three cans of cheap Brazilian beer I guzzled at dinner. I get that. But later that night, as I’m lying in my bunk, staring out a port window into a sky dusted with a billion distant stars, I can’t seem to shake it.
* * *
It’s our 13th day at the edge of Saint Peter and Saint Paul Archipelago, hovering over the Mid-Atlantic Ridge. This is the morning I’ve been both anxiously anticipating and subconsciously dreading since I first signed up for this assignment months ago.
We’ve lost a week or so due to strong currents that have kept the subs onboard the Alucia, but today, the skies are clear, the sun is shining, and the ocean is glass. But I’m also starving and my throat is parched. I haven’t had a sip of water in the past 14 hours and probably won’t eat or drink until later this afternoon. Amon suggested that this all-out fast is the best way to ensure my well-being. “The last thing you want is to, you know,” she paused and shot a knowing a nod. “You just don’t [want to] feel uncomfortable down there.”
By “down there” Amon is referring to the hundreds of meters below the surface I’ll be exploring aboard Nadir over the next several hours. For any adventurer or scientist, or reasonable citizen of the world, this excursion would be an absolute dream. But, sadly, all I’ve been able to think about is what will happen if I suddenly need to relieve myself, or feel claustrophobic, or suddenly feel the urge to stretch my legs, or arms, or back. There are no windows to open a thousand meters below the surface, no bathrooms, no pulling off to the side of the road. I’ll be stuck in a child-sized seat with my legs tucked to my chest for the time it takes to watch The Godfather, Part I. Twice. Including credits.
“You should lighten up,” says Colin Wollermann, a crop-haired American who will be piloting Deep Rover. He’s sitting at a mess-hall table across from me, shoveling bacon, buttered bread, and eggs into his mouth, washing it all down with liberal gulps of coffee. “My personal approach is just to get as gassy as possible,” he laughs, and takes another bite.
Alan Scott, the pilot for Nadir and Alucia’s submarine team leader, is beside Wollermann, packing handfuls of candy bars and potato chips into a backpack should we get hungry along the way. “It’s easy, mate,” he says in a thick Scottish brogue. “It’ll go by so fast you won’t even know what happened.”
Strangely, one thing that hasn’t crossed my mind this morning are the dangers involved in cruising around along an unexplored seafloor of the Atlantic in a pressurized bubble, 1,000 meters (3,280 feet) below the ocean surface and a thousand kilometers from the nearest hospital, or airport, or doctor. When I asked Klein if he ever worried about doing this kind of research, he demurred. “The only thing dangerous about it is to have spent a year and a half planning this trip and come home empty-handed,” he chuckled. “The rest of it, riding in subs, sailing out here? That’s the fun part.”
Amon was a tad less sanguine. Days earlier she had told me a story about the Johnson Sea Link, a four-person submarine. In the summer of 1973, the same year the sub was first launched, a team of two pilots, an ichthyologist, and dive master headed out on what was considered a routine dive 24 kilometers (15 miles) off the coast of Key West, Florida. The mission was to recover a fish trap from a sunken destroyer 100 meters (330 feet) below the surface.
While attempting to ascend, the Sea Link got caught on a cable extending from the sunken ship. The passengers sat back, relaxed as best they could, and waited for help. With the emergency oxygen reserves onboard, the pilots estimated they had about 42 hours before they suffocated.
Hours passed. The temperature plummeted to 42 degrees Fahrenheit. Soon the passengers were suffering from hyperthermia. Worse, their calculations for fresh air were far too optimistic. The concentration of carbon dioxide in the air began to rise to dangerous levels.
Eight hours after the pilots called for help a Navy support ship arrived and made several attempts to disentangle the sub. Nothing worked. The passengers began to lose consciousness.
The Sea Link was finally freed 32 hours after it had been launched. Two members of the sub team were dead of carbon-dioxide poisoning; the other two were treated immediately and would live.
While the Johnson Sea Link was an extremely rare disaster and happened more than 40 years ago, it’s impossible to overlook the fact that diving hundreds of meters deep in a transparent acrylic sphere the size of a phone booth has implicit risks. Motors can fail, electronics can short, lost fishing nets can entangle. Fortunately, though, the new generation of submarines are built with so many levels of redundancy and fail-safes, the chances of anything bad happening are remote. Of the hundreds of dives Deep Rover and Nadir have made, the crew members here have never experienced a problem.
“There are risks with any research, for sure,” Amon says. “But to me, the rewards so far outweigh any of that. It’s incredible to be out here doing this kind of field research.”
A half-hour later, I’m about to experience those rewards for myself. At 10 a.m., I’m standing in socks at the foot of a steel staircase. Below me is Nadir’s open top hatch. Scott is sitting inside the sub guiding me in. “Okay, now, go slow,” he says. With a few torso twists and some sloppy footwork, I manage to wriggle into the passenger seat. Following me is Susan Humphris, a geologist who will be surveying the underwater terrain and biology during the dive.
Scott seals shut Nadir’s hatch, gives the deckhands a thumbs-up, and we slowly creep out along the ship’s aft deck toward open water. Another thumbs-up and a crane lifts us from the deck until we’re swinging a dozen feet in the air like the pendulum of an old clock. I peer between my feet and see the Alucia crowded with crew and researchers. Between them, Klein and Wollermann roll out in Deep Rover. In front of us, there is nothing but horizonless blue ocean.
We lower down into the water, splash at the surface, detach from the guide rope, and float away from the ship. “Okay, all clear,” says Scott into the sonar radio. With a gurgle of bubbles we sink below until there’s nothing but gradients of blue water all around. It’s stunning.
These bands of color aren’t a distortion from the acrylic sphere and we’re not imagining them. What we’re seeing is the spectrum of sunlight being absorbed by water molecules. Long wavelengths of light—reds, oranges, and yellows—are absorbed first, so they disappear near the surface. As we sink lower, past 15 meters (50 feet), I notice that my beige pants, my shirt, my skin, and notepad have all faded into the same bluish-gray metallic tone.
Deeper still, we sink until there is no blue, no gray, no purple—no light at all. Nothing but black. Scott flicks on Nadir’s lights. We have reached 500 meters. At this depth, photosynthesis is no longer possible. The ocean world around us now is almost entirely animal and mineral.
“Affirmative, Deep Rover, I see you,” says Scott. In the distance, two pinprick white lights emerge from the blackness. It’s Deep Rover. Even though we are only a hundred or so meters away, Wollermann and Klein will need to wait about four seconds before they receive our transmission. Radio waves can’t transmit through water, so the submarines must communicate through sound waves via a sonar system. Each audio transmission we send travels through the water column all the way up to the Alucia where it is then sent back down to Deep Rover.
About 10 seconds after our transmission, we hear an echoey squawk layered in reverb and noise come through Nadir’s speaker. Scott tells me that understanding sonar transmission takes a trained ear and time, the same kind of listening skills dentists use to translate the open-mouth garble of patients.
The sub pilots exchange a few more commands, then we turn to face Deep Rover head-on. While moving a machine around in the terrestrial world may take a few seconds, down here in the deep ocean even simple maneuvers can take minutes due to the water’s resistance and the limited power of the subs, which crawl at a maximum of about four knots (4.6 miles per hour).
The syrupy slowness of our movements combined with the rising humidity inside the pressure hull gives the whole scene a dreamlike, meditative quality. After a while, it feels as though our bodies are slowing down too—seconds turn to minutes, minutes turn to hours.
“Carbonate, very interesting,” says Humphris. She’s been recording our every move like a stats fan at a baseball game ever since we hit the seafloor. The presence of carbonate rocks, Humphris says, is an indication that there was likely, or still is, hydrothermal activity in the area. “Promising, for sure,” she says, penciling another acronym onto the sheet.
Meanwhile, in front of us, Klein has extended a mechanical arm from Deep Rover and is attempting to grab some of what looks like carbonate rocks on the seafloor. Operating a mechanical arm is difficult, and the work is slow going.
Scott takes the opportunity to hand out our lunch boxes. We munch on chips and cheer when Klein manages to coerce a rock into the sample bucket; and we boo when samples slip through the mechanical fingers and are swallowed by the blackness below.
This goes on for an hour, or two. It occurs to me how bizarre the pursuit of deep life has become. Here we are in a hollowed-out plastic marble, sitting in perfect comfort 500 meters below the surface of the Atlantic Ocean, nibbling on Flamin’ Hot Cheetos and dark-chocolate Kit Kats, watching the little steel fingers of a robotic arm probe holes into million-year-old microscopic bones. If we told our ancestors a hundred years ago we’d be doing this, nobody would have ever believed us. As I sit here, experiencing this, I’m not even sure I believe it myself.
Scott takes another handful of Cheetos, grabs the control stick, and leans back. The oxygen gauge in the sub is reading about 20 percent. Although we have dozens of hours of reserves, it’s always best to play it safe.
“Okay, that’s it,” says Scott into the receiver. “Heading up.” He flicks a switch, the electric motors hum, and we begin to rise upward. Inky blackness bleeds to deep purple which bleeds to neon blue and finally, at the surface, blinding, glorious yellow sunlight.
“Easy, huh?” says Scott, squinting in the magnified sunlight. I look at my phone. Five hours have passed since we left the surface. I nod to Scott, “Easy.” My only regret is it went by so fast I didn’t even know what happened.
* * *
That evening Schubotz is in a makeshift laboratory jostling between cases of sparkling water, condiments, and cases of Brazilian beer. She is organizing test tubes on a cutting board covered with what looks like black dust. Over the past few days, Schubotz has been soaking the samples in hydrogen, carbon dioxide, and methane, hoping to spark some kind of reaction. She’s also been trying to “feed” them heavy carbon. If there are microbes on the rocks, they’ll likely consume the carbon and become measurably heavier.
Cells in the human body, such as those in the small intestine, can replicate, or “turn over,” in just a matter of days. The turnover of some deep microbes, however, can take weeks to months to years or even decades. “It’s a lot of detective work, which makes it so fascinating,” she says. “You’re dealing with just such a crazy dimension.”
Schubotz and the rest of the scientific team are certain that hydrothermal activity occurred here at Saint Peter and Saint Paul Archipelago, but suspect that the activity occurring now, if it’s occurring, is likely lower-temperature, subtler, and slower than at most other vent systems.
Over the next several months, Schubotz will take the samples back to the laboratory in Bremen, Germany, and try to determine if the heavy carbon has been consumed, which will prove that microorganisms are active—that the rocks here are harboring hydrothermal life. “Only time will tell,” she says, shooting a smile.
* * *
On our last day here, we finally have the opportunity to set foot on dry land. Not as if there’s much of it, and not as if there’s much to see. Nothing really grows on Saint Peter and Saint Paul Archipelago; there is no sand, no shade. The only people who live here are a crew of mostly shirtless Brazilian Navy sailors who rotate every fortnight. As we approach in a smaller tender boat, the sailors wave us in. We tie up, climb a rusty ladder over a sheer wall of rock, and gather on an elevated wooden boardwalk.
Charles Darwin came to these little islets in 1832 while on a five-year cruise around the world aboard the HMS Beagle. Upon landing, he described being surrounded by two species of pelicans and gulls, so “gentle and stupid” they remained perfectly calm, and perfectly still in his presence, surely because they had never seen humans before.
Those days are, sadly, gone. As the crew and I scurry past, the few hundred brown boobies (Sula leucogaster) that call the archipelago their home lunge at our ankles, shins, and knees. We manage to escape them and enter the porch of the plywood Brazilian Navy bunkhouse. We exchange a few bom dias, sip some water, and take a seat. The tour of Saint Peter and Saint Paul Archipelago is over.
While the rest of our group heads out for a swim, I excuse myself, hop off the walkway to explore the unpaved crevices, and discover a little secluded cove frothed in spindrift. From this vantage there are no crap-covered satellite disks, broken outhouses, bunkhouses, or plastic bottles. No sign of human presence, just bare mantle rock surrounded by blue ocean that goes on forever.
This is how these rocks looked when they were first pushed up from the seafloor, when they cracked and fissured and coupled with the seawater to give birth to primitive life.
And here we are, a few billions of years later, back together again, the descendants of that rock and water, staring at each other, still piecing together our family tree, trying to find a way back home.
This article appears courtesy of BioGraphic.
Portions of the scientific history and some facts in this article also appeared in Nestor's book Deep.


Sometime in the mid-2020s, the United States plans to launch a new member of its fleet of space observatories, one with a field of view 100 times more powerful than the Hubble Space Telescope. The Wide-Field Infrared Survey Telescope, or WFIRST, will spend six years scanning the universe. It will scour the Milky Way in search of hundreds of more exoplanets, and it will soak up the light from distant stars in the hope of understanding, even just a little, the great mystery that is dark energy.
But first, NASA has to figure out how to foot the bill.
The estimated cost of WFIRST has ballooned steadily since astronomers proposed their preliminary designs for the mission in 2011, from less than $2 billion to more than $3 billion. NASA officially began working on WFIRST in February 2016, and staff has spent the months since looking for ways to reduce its price tag. Their efforts went into overdrive in November, thanks to a troubling report from an independent committee that NASA established to help the space agency manage the mission.
The report, provided to WFIRST in October but released publicly for the first time last week, had some sobering words for NASA: The mission, as is, is “not executable” unless they find even more funding. NASA headquarters, in response to the warning, sent a memo to WFIRST folks in October instructing them to find a few hundred millions of dollars in their budget and cut it.
“It’s not fun for anybody,” said Jeffrey Kruk, the project scientist for WFIRST. “It’s very stressful. We’re trying to come up with the right answer, the best answer we can.”
The new report, as well as other official warnings about WFIRST’s rising costs, have provided a rare glimpse into the complicated, nitty-gritty operation behind funding some of NASA’s biggest missions, a process that usually doesn’t spill over into public view. Talk of saving money isn’t unusual in the earliest stages of any mission, but the level of scrutiny the WFIRST project has received so far is.
WFIRST will be the next great space telescope after the James Webb, which is scheduled to launch in spring 2019. Webb, itself the scientific successor to Hubble, will be the most powerful space telescope ever built, charged with spotting the faint light of the earliest stars and galaxies. It’s also been really, really expensive to build. In 2010, NASA estimated the Webb would cost about $5 billion. A year later, the space agency said it would take more than $8 billion. NASA has repeatedly asked Congress for money to cover cost overruns over the years, and the mission has eaten away at resources for other science programs. Officials don’t want a repeat with WFIRST.
“WFIRST, as the next in line, will be subject to scrutiny that 20 years ago it may not have gotten,” said Alan Boss, an astronomer at the Carnegie Institution for Science and one of the many members of the committee that first brought WFIRST to life.
The mission emerged from a decadal survey by the National Research Council, which outlines priorities in astronomy and astrophysics for the United States. WFIRST “presents relatively low technical and cost risk, making its completion feasible within the decade, even in a constrained budgetary environment,” the report said in 2010. This year, the cost reached $3.6 billion, and NASA headquarters is now trying to get it down to about $3.2 billion. The independent review board, however, says that figure is “not realistic for the scope, complexity, and expectations” of the mission, and suspects it’s going to cost more like $3.9 billion.
The particulars of WFIRST’s budget are not publicly available, so it’s not known where exactly all the money is going. But the report attributes the growing cost to several decisions made by NASA, which have created a mission “more complex than probably anticipated.”
Among them is the use of a 2.4-meter telescope, which the National Reconnaissance Office, an agency within the U.S. Department of Defense, donated to NASA for free. WFIRST will use the telescope to feed its main science instruments. One would think a giant gift like this would help considerably, since the WFIRST team wouldn’t have to build their own from scratch. But the telescope is bigger and heavier than the one the team initially envisioned, so it requires more heater power, a bigger spacecraft to hold it, and then a bigger launch vehicle to send it into space. Engineers also have to tweak it to suit their needs for this particular mission. “Having to work with an existing design forces a variety of small engineering choices in ways that can’t be optimized the way one would if starting from a blank sheet of paper,” Kruk said.
Another decision involves an instrument called a coronagraph, which would directly image and study the chemical compositions of exoplanets. At first, the WFIRST team planned to treat the coronagraph as a technology demonstration. This meant that it would do enough science to show its usefulness for future generations of space observatories, but not enough to classify as a full-fledged science instrument. Somewhere along the way, the scientists decided to push the boundaries a little bit—imagine the potential findings of such advanced technology!—and added to its capabilities. That move set the mission up for more planning and testing and eventually got too expensive.
And so the WFIRST team has followed the report’s recommendation to reduce the scope of the coronagraph’s capabilities. “When the gun’s at your head, you realize maybe we could do this a simpler way,” said David Spergel, a Princeton University astrophysicist and cochair of the WFRIST science team. “This pressure, while painful, is actually good.”
Removing the coronagraph altogether would chop $400 million from the total mission cost, but NASA headquarters is intent on keeping it. WFIRST’s main science instrument, a wide-field imager that will investigate dark energy, can’t do detailed analysis of the exoplanets it finds. “The nice thing about the coronagraph is it makes the entire package much more palatable to the public and to congresspeople that are interested in exoplanets,” Boss said. “Getting an image of a nearby planet, to many people, is a lot more interesting than seeing some strange number about dark energy.”
Kruk said most of the cost-cutting this month has come from constant reshuffling of the design, development, and testing schedules. The more things they can do at the same time, the less money they spend. The project is also nearing agreements with other nations, like Canada and Japan, to participate and contribute hardware—$50 million here, another $50 million there, Spergel said.
The WFIRST mission has a busy few months ahead. Next week the House Committee on Science, Space, and Technology will hold a hearing on NASA’s next generation of large telescopes, including Webb and WFIRST, and the recent report on the latter will surely come up. In 2011, Congress placed an $8 billion cap on development costs for Webb. The mission has faced frequent delays, and its launch date has slipped steadily, from the proposed 2014 to 2019. Jeremy Kasdin, a Princeton professor who is leading the coronagraph’s development on WFIRST, is wary of the public making comparisons between the financial histories of the telescopes. “It’s important that people understand it’s not a runaway cost problem like the [Webb telescope] was, he said.”
In February, WFIRST scientists will present to headquarters their latest designs and science objectives, and, if approved, will move the project onto the next phase of development. By that time, they need to hit the $3.2 billion target, too. So far, the scientists I spoke with seem confident about cutting enough costs. They just know it’s going to hurt.
“The problem is astronomers always want to make the best possible instrument they can and engineers are happy to oblige them because engineers get their jollies making something work really well in space,” Boss said. “But unfortunately, they also charge you for it.”


In 2019, if everything goes according to plan, the much-delayed James Webb Space Telescope will finally launch into orbit. Once assembled, it will use an array of 18 hexagonal mirrors to collect and focus the light from distant galaxies. This segmented-mirror design was developed in the 1980s, and it has been so successful that it will feature in almost all the large telescopes to be built in the near future.
But as always, nature got there first. For millions of years, scallops have been gazing at the world using dozens of eyes, each of which has a segmented mirror that’s uncannily similar to those in our grandest telescopes. And scientists have just gotten a good look at one for the first time.
Yes, those scallops—the pan-seared pucks of white flesh that grace our dinner plates. Those pucks are just the muscles that the animals use to close their beautiful shells. Look at a full, living scallop, and you’ll see a very different animal. And that animal will be looking right back at you, using dozens of eyes that line the fleshy mantle on the inner edges of its shell. Some species have up to 200 eyes. Others have electric-blue ones.
Inside the eyes, the weirdness deepens. When light enters a human eye, it passes through a lens, which focuses it onto the retina—a layer of light-sensitive cells. When light enters a scallop eye, it passes through a lenslike structure, which ... doesn’t seem to do anything. It then passes through two retinas, layered on top of each other. Finally, it hits a curved mirror at the back of the eye, which reflects it back onto the retinas. It’s this mirror, and not the lens, which focuses the incoming light, in much the same way that those in segmented telescopes do.
Michael Land from the University of Sussex discovered much of this in the 1960s, by carefully eyeballing the eyes under a microscope, and tracing the path that light must take within them. He identified the mirror, he showed that it consists of layered crystals, and he suggested that the crystals are made of guanine—one of the building blocks of DNA. “It’s very impressive how Land was right about pretty much everything from some pretty simple approaches,” says Daniel Speiser from the University of South Carolina, who also studies scallop eyes. “But no one has gotten a good look at an intact mirror before.”
The problem is that powerful microscopes tend to dehydrate samples in the process of analyzing them, and that would ruin the placement of the mirror’s crystals. Now, Lia Addadi from the Weizmann Institute of Science has found a way around this problem. Her team, including Benjamin Palmer and Gavin Taylor, used a microscope that rapidly freezes samples, so everything within stays in the right place. They’ve finally reconstructed the structure of the mirror in glorious detail, confirming many of Land’s ideas, and fleshing others out.
The mirror consists of flat, square guanine crystals, each a millionth of a meter wide. They tessellate together into a chessboard-like grid. Between 20 and 30 of these grids then stack on top of each other, with a liquid-filled gap between them. And the layers are arranged so that the squares in each one lie directly beneath the squares in the one above. The crystals and the gaps between them are respectively 74 and 86 billionths of a meter thick, and these exacting distances mean that the mirror as a whole is great at reflecting blue-green light—the color that dominates the scallop’s underwater habitat.
The whole structure is a master class in precision engineering. “When there is an elegant physical solution, the evolutionary process is very good at finding it,” says Alison Sweeney, a physicist at the University of Pennsylvania who studies animal vision.
This precision is all the more remarkable because guanine crystals don’t naturally form into thin squares. If you grow them in the lab, you get a chunky prism. Clearly, the scallop actively controls the growth of these crystals, shaping them as they form. Guanine crystals grow in layers, and Addadi thinks that the scallop somehow shifts the orientation of each layer by 90 degrees relative to the ones above and below it. As the layers grow outward, they do so in only four directions, creating a square. How it does that is a mystery, as is everything else about the way the mirrors form.
Also, the mirror is not an inanimate structure within the eye. It’s a living thing. The square crystals grow inside the cells of the scallop’s eye, filling them up. It’s the cells that then tessellate together to form the layers. “The cells can’t be dead,” Addadi says, “or the whole thing would break apart.” So not only must the cells control the growth of the crystals inside them, but they also have to communicate with each other to arrange themselves just so. “How do they do that? I really don’t know,” she adds.
Whatever their trick, it clearly produces results. Scallop vision isn’t going to rival ours anytime soon, but it’s far sharper than you might expect for an animal that’s basically a fancy clam. Speiser demonstrated this a decade ago by putting scallops in little seats and playing movies of drifting food particles. Even when the particles were just 1.5 millimeters wide, the scallops would open their shells, ready to feed. “The idea that these animals are forming really nice images with their eyes feels very solid to me,” Speiser says.
Addadi’s team also noticed that the scallop’s mirror is slightly tilted relative to its retinas. As a result, the mirror focuses light from the center of the animal’s visual field onto the upper retina, and light from the periphery onto the lower one. Perhaps that’s why the creature has two retinas: They allow it to focus on different parts of its surroundings at the same time.

“It’s a really amazing study,” says Jeanne Serb from Iowa State University, who has also studied scallop eyes. It helps to solve the mystery of the double retinas—something that scientists have long tried to address, with no success.
But Speiser isn’t completely convinced. He says that the eyes get easily deformed when they’re dissected, and even a gentle squish could change the orientation of the mirror and retinas. Still, he doesn’t have a better explanation, despite testing several possible ideas over the last 12 years. “Nothing checked out, and this is as good a hypothesis as any,” he says.
The next big goal for scallop aficionados, he adds, is to work out why scallops have quite so many eyes. They probably allow it to scan a wide area, but does it consider the information from each eye separately, or combine them all into a single image? After centuries of study, scientists finally know how each individual eye sees. But “we still have no idea what the animal as a whole is perceiving,” he says.


Millennials don’t always buy cars. But when they do, they apparently buy SUVs.
“The floor at this year’s Los Angeles Auto Show will look a lot like America's roads: full of SUVs,” the AP wrote this week. Car sales overall are actually slightly down this year, but SUV sales are up 6 percent.
The AP speculated that the boom in Wranglers and Explorers is the result of “a combination of low gas prices, growing millennial families, and a host of new models.”
But according to the authors of The Spirit Level, a book about the human costs of social inequality by epidemiologists Kate Pickett and Richard Wilkinson, skyrocketing SUV sales could also be tied to declining levels of social trust in U.S. society.
Fewer Americans now agree with the statement “most people can be trusted” than at any point in the past 40 years, and plummeting social trust tends to motivate individuals to make decisions that will protect their own family, class, or tribe.
The rise of SUVs in the 1980s and ’90s, Wilkinson and Pickett write, coincided with other markers of suspicion, like a growing number of gated communities and the increasing sales of home alarm systems. They point out how some of these cars’ names—Outlander, Pathfinder, and Crossfire—seem geared toward tough, suburban loners. With their rugged boxiness, SUVs are much manlier than small, socialist-approved minivans, which outsell SUVs in gentle, trusting Canada by two to one, they write. (The ratio is reversed in ballsy, do-it-yourself America.)
The main evidence for this theory is a 2005 paper by media-studies professor Josh Lauer. Lauer points out that SUVs are neither more spacious nor safer than minivans and station wagons. Instead, he argues, SUVs reflect Americans’ growing fear of others and our desire to sequester ourselves and our families from them. In other words, he writes, the “space” people actually seek in SUVs is personal space, and the “safety” is not road safety but personal safety.
Crime did not rise in the United States during the 1980s and ’90s, but fear of crime persisted, and so did sales of giant cars. Around that time, Americans also began buying mace and pepper spray, reflecting the “individualization of social risk”—or the idea that it’s on every individual to protect themselves from harm, Lauer writes.
He quotes Keith Bradsher as writing, in The New York Times in 2000, that:
The United States is in some ways becoming a medieval society, in which people live and work in the modern equivalent of castles—gated communities, apartment buildings with doormen, and office buildings with guards—and try to shield themselves while traveling between them. They do this by riding in sport-utility vehicles, which look armored, and by trying to appear as intimidating as possible to potential attackers.
Bradsher later wrote a book about SUV ownership, and, relying on market research, noted that their owners do, in fact, tend to be “insecure, vain, self-centered, insecure, and ... frequently nervous about their marriages.”
SUV ads hint at weekends full of mud-splattered adventures—the iconic image is of a Jeep Grand Cherokee perched on a desolate cliff—while dog-whistling to the buyer that the car is actually for protection back home. Lauer quotes one Toyota 4Runner ad, which showed the car parked in front of a city townhouse, as saying “It’s the only four-wheel drive to have in this neck of the woods” and that it’s “the ideal way to make tracks in the urban jungle.”
“The image of circled Conestoga wagons comes to mind,” Lauer writes.
Other ads suggest SUVs are dramatically roomier than other cars: “Imagine taking your favorite room wherever you go,” one read.
“These are not ads targeted at working-class outdoorsmen or seasoned campers,” Lauer concludes, “but insecure cosmopolitan drivers less concerned with actually roughing it than with being roughed up.”
In The Spirit Level, Wilkinson and Pickett contend that, by making people more anxious about their place in the world, inequality contributes to all manner of social ills, including violence, poor health, and, yes, reduced social trust.
SUVs, if they are an emblem of this problem, are probably a lesser one. (More significant data points might include that CEOs make 271 times the salary of the typical worker, or that people without college degrees are dying of despair.) Aggressive-looking cars are just the canaries in the coal mine—or, shall we say, the Sorento on the cliff.


In astronomy, an observation method called spectroscopy extends humanity’s reach into the cosmos. Through spectroscopy, astronomers can study different wavelengths of light coming from very distant objects in the universe, from single stars to massive galaxies, and determine their chemical composition. The technology may, one day, uncover life-giving molecules in the atmosphere of an exoplanet.
It’s a very cool thing, so it’s unfortunate that spectroscopy’s name makes it sound like an uncomfortable medical procedure. Here’s another way to think of the method: It’s the Leonardo DiCaprio of astronomy instruments, the Inception version, who insists, again and again, that we go deeper into the unknown.
Recently, one international team of astronomers spent two years heeding Leo’s call. The team, led by Roland Bacon of the Centre de Recherche Astrophysique de Lyon in France, used a spectroscopy instrument called MUSE, installed on European Southern Observatory’s Very Large Telescope in Chile. They pointed the instrument at a small patch of sky known as the Hubble Ultra-Deep Field. The field is our deepest view of the cosmos, a photograph of the universe as it was 13 billion years ago. The Hubble Space Telescope captured the view in 2004 after spending several months absorbing the light from the earliest galaxies.
The survey ended up measuring the properties of 1,600 faint galaxies, which the astronomers say is 10 times as many as have been recorded using other ground-based telescopes over the years. The survey includes 72 galaxies that have never been detected before, not even by Hubble. Altogether, the data have produced the deepest spectroscopic observations ever made, according to the team. Their findings are described in 10 papers published Wednesday in Astronomy & Astrophysics.
“When we started the project, I did not expect that we would be so successful to detect so many galaxies,” Bacon said in an email.
The 72 galaxies were hidden from earlier spectroscopy studies because they shine in only one color of light called Lyman-alpha. These galaxies, known as Lyman-alpha emitters, are young, star-producing factories. The dynamics of Lyman-alpha emitters are still poorly understood, and Bacon said studying them “must tell us something about the star formation in the early universe, a key ingredient for galaxy formation.”
The survey also found that the presence of halos of hydrogen gas around galaxies is a pretty common phenomenon in the early universe. Observing these halos is key to understanding the fundamentals of galaxy formation, Bacon said.
ESO has produced a visual tour of this deeper view of the cosmos, constructed using MUSE measurements of the distances of the galaxies from Earth. The video captures the data better than a single composite image could. Traveling through the field of view feels like staring up at falling snow:

The new survey demonstrates how spectroscopy can spice up the usual ways astronomers study distant galaxies, said Massimo Stiavelli, an astronomer at the Space-Telescope Science Institute in Baltimore. Instruments like MUSE have the capacity to provide information about the chemical compositions of every galaxy in their path, whether they can be detected in visible wavelengths of light or not.
“We tend to take an image, identify objects that we think are promising, and then take spectra of these objects,” Stiavelli said. “By being prejudiced by images, we would be missing some objects.”
The MUSE survey provides a tiny hint of what’s to come, when NASA launches its next space telescope, the James Webb, in 2019, kicking off a veritable spectroscopy party. The Webb, an infrared-light observatory, will be capable of measuring the spectra of some of the most distant exoplanets, stars, and galaxies in the universe.
Stiavelli said he has no doubt Webb will find the 72 galaxies the MUSE team discovered, and see them in even greater clarity. When that happens, Webb will unseat MUSE—as well as pretty much every other similar tool on or around the planet—as the Leonardo DiCaprio of astronomy instruments.
“This is a very meaningful appetizer of things we should be able to do with James Webb,” Stiavelli said.


New York City is a place where rats climb out of toilets, bite babies in their cribs, crawl on sleeping commuters, take over a Taco Bell restaurant, and drag an entire slice of pizza down the subway stairs. So as Matthew Combs puts it, “Rats in New York, where is there a better place to study them?”
Combs is a graduate student at Fordham University and, like many young people, he came to New York to follow his dreams. His dreams just happened to be studying urban rats. For the past two years, Combs and his colleagues have been trapping and sequencing the DNA of brown rats in Manhattan, producing the most comprehensive genetic portrait yet of the city’s most dominant rodent population.
As a whole, Manhattan’s rats are genetically most similar to those from Western Europe, especially Great Britain and France. They most likely came on ships in the mid-18th century, when New York was still a British colony. Combs was surprised to find Manhattan’s rats so homogenous in origin. New York has been the center of so much trade and immigration, yet the descendants of these Western European rats have held on.
When Combs looked closer, distinct rat subpopulations emerged. Manhattan has two genetically distinguishable groups of rats: the uptown rats and the downtown rats, separated by the geographic barrier that is midtown. It’s not that midtown is rat-free—such a notion is inconceivable—but the commercial district lacks the household trash (aka food) and backyards (aka shelter) that rats like. Since rats tend to move only a few blocks in their lifetimes, the uptown rats and downtown rats don’t mix much.
When the researchers drilled down even deeper, they found that different neighborhoods have their own distinct rats. “If you gave us a rat, we could tell whether it came from the West Village or the East Village,” says Combs. “They’re actually unique little rat neighborhoods.” And the boundaries of rat neighborhoods can fit surprisingly well with human ones.
Combs and a team of undergraduate students spent their summers trapping rats—beginning in Inwood at the north tip of Manhattan and working their way south. They got permission from the New York City Department of Parks and Recreation, which gave them access to big green spaces like Central Park as well as medians and triangles and little gardens that dot the city. And they asked local residents. “More often than not, they were very, very happy to show us exactly where they had rats.” says Combs. A crowdsourced map of rat sightings also proved very helpful.
Rats, although abundant, are not easily fooled into traps. They’re wary of new objects. To entice them, the bait was a potent combination of peanut butter, bacon, and oats. And the team placed their traps near places where rats had clearly crawled. They looked for rat holes, droppings, chew marks on trash cans, and sebum marks—aka the grease tracks rats leave when they traverse the same path to the garbage over and over again.
For the DNA analysis, Combs cut off an inch or so of the rats’ tails. (Over 200 of these tails are still saved in vials in a lab freezer.) The team also took tissue samples for other researchers interested in studying how rats spread diseases through the urban environment. And some of the rats they skinned and stuffed for the collections of the Yale University Peabody Museum of Natural History, where they will join stuffed rats from 100 years ago.
Combs is now writing his dissertation on the ecology of New York’s rats. He’s looking at how a number of characteristics—natural features like parks, social factors like poverty, physical infrastructure like the subway system—account for the spatial distributions of rats in Manhattan.
How Portland Lives With, Not Against, Its Rats
The point of all this, ultimately, is to help New York manage its rat problem, which is annoying as well as a genuine public-health hazard due to rat-borne diseases. In July, New York Mayor Bill de Blasio announced a $32 million war plan against the varmints. The New York Times noted wryly that when it came to rats, “There have been 109 mayors of New York and, it seems, nearly as many mayoral plans to snuff out the scourge. Their collective record is approximately 0-108.”
After two years of trapping rats, Combs has come to respect the enemy. At the end of our conversation, he launched into an appreciation of rats—their ability to thrive on nearly anything, their prodigious reproduction, and their complex social structure, in which female rats will give birth all at the same time and raise their offspring in one nest. “They are, quote-unquote, vermin, and definitely pests we need to get rid of,” he says, “but they are extraordinary in their own ways.”


When Margaret Rubega first read about how hummingbirds drink, she thought to herself: That can’t possibly be right.
Hummingbirds drink nectar using tongues that are so long that, when retracted, they coil up inside the birds’ heads, around their skulls and eyes. At its tip, the tongue divides in two and its outer edges curve inward, creating two tubes running side by side. The tubes don’t close up, so the birds can’t suck on them as if they were straws. Instead, scientists believed that the tubes are narrow enough to passively draw liquid into themselves. That process is called capillary action. It’s why water soaks into a paper towel, why tears emerge from your eyes, and ink runs into the nibs of fountain pens.
This explanation, first proposed in 1833, was treated as fact for more than a century. But it made no sense to Rubega when she heard about it as a graduate student in the 1980s. Capillary action is a slow process, she realized, but a drinking hummingbird can flick its tongue into a flower up to 18 times a second. Capillary action also is aided by gravity, so birds should find it easier to drink from downward-pointing flowers—and they don’t. And capillary action is even slower for thicker liquids, so hummingbirds should avoid supersweet nectar that’s too syrupy—and they don’t.
“I was in this very odd position,” says Rubega. “I was only a graduate student and all these really well-known people had done all this math. How could they be wrong?”
Even while she turned her attention to other birds, the hummingbird dilemma continued to gnaw at her. And decades later, as a professor at the University of Connecticut, she hired a student named Alejandro Rico-Guevara who would help her solve the mystery.
Born in Colombia, Rico-Guevara remembers spotting a hermit hummingbird on a fateful field trip in the Amazon. In the jungle, most animals are heard rather than seen, but the hermit flew right up and hovered in front of his face. “It was just there for a split second but it was clear that it had a completely different personality than other birds in the forest.” He fell in love, and started studying the birds. And when he read the capillary-action papers, he felt the same pang of disbelief that Rubega did. “We decided to go after it,” says Rubega. “Is it capillary action? And if not, what’s going on? We just wanted to know.”
Rico-Guevara handcrafted artificial flowers with flat glass sides, so he could film the birds’ flickering tongues with high-speed cameras. It took months to build the fake blooms, to perfect the lighting, and to train the birds to visit these strange objects. But eventually, he got what he wanted: perfectly focused footage of a hummingbird tongue, dipping into nectar. At 1,200 frames per second, “you can’t see what’s happening until you check frame by frame,” he says. But at that moment, “I knew that on my movie card was the answer. It was this amazing feeling. I had something that could potentially change what we knew, between my fingers.”
Here’s what they saw when they checked the footage.
As the bird sticks its tongue out, it uses its beak to compress the two tubes at the tip, squeezing them flat. They momentarily stay compressed because the residual nectar inside them glues them in place. But when the tongue hits nectar, the liquid around it overwhelms whatever’s already inside. The tubes spring back to their original shape and nectar rushes into them.
The two tubes also separate from each other, giving the tongue a forked, snakelike appearance. And they unfurl, exposing a row of flaps along their long edges. It’s as if the entire tongue blooms open, like the very flowers from which it drinks.
When the bird retracts its tongue, all of these changes reverse. The tubes roll back up as their flaps curl inward, trapping nectar in the process. And because the flaps at the very tip are shorter than those further back, they curl into a shape that’s similar to an ice-cream cone; this seals the nectar in. The tongue is what Rubega calls a nectar trap. It opens up as it immerses, and closes on its way out, physically grabbing a mouthful in the process.
“This has been going on literally under our noses for the entire history of our association with hummingbirds and there it was,” says Rubega. “We were the first to see it.”
This same technique is also how the hummingbird swallows. Every time it extends its tongue, it presses down with its beak, squeezing the trapped nectar out. And since there’s limited space inside the beak, and the tongue is moving forward, there’s nowhere for that liberated nectar to go but backward. In this way, the tongue acts like a piston pump. As it pulls in, it brings nectar into the beak. As it shoots out, it pushes that same nectar toward the throat. The tongue even has flaps at its base, which fold out of the way as it moves forward but expand as it moves backwards, sweeping the nectar even further back.
The thing that really astonishes Rico-Guevara about all of this is that it is passive. The bird isn’t forcing its tongue open—that happens automatically when the tip enters liquid, because of the changing surface tension around it. Rico-Guevera proved that by sticking the tongue of a dead hummingbird into nectar—sure enough, it bloomed on its own. Likewise, the tongue closes automatically. It releases nectar automatically. It pushes that nectar backward automatically. The bird flicks its tongue in and out, and all else follows.
In hindsight, the surprising reality of the hummingbird tongue should have been entirely unsurprising. Almost everything about these animals is counterintuitive. Hummingbirds are the bane of easy answers. They’re where intuition goes to die.
Consider their origins. Today, hummingbirds are only found in the Americas, but fossils suggest that they originated in Eurasia, splitting off from their closest relatives—the scythe-winged swifts—around 42 million years ago. These ancestral hummingbirds likely flew over the land bridge that connected Russia and North America at the time. They fared well in the north, but they only thrived when they got to South America. In just 22 million years, those southern pioneers had diversified into hundreds of species, at least 338 of which are still alive today. And around 40 percent of those live in the Andes.
As evolutionary biologist Jim McGuire once told me, “the Andes are kind of the worst place to be a hummingbird.” Tall mountains mean thin air, which makes it harder to hover, and to get enough oxygen to fuel a gas-guzzling metabolism. And yet, the birds flourished. Their success shows no sign of stopping, either. By comparing the rates at which new species have emerged and old species go extinct, McGuire estimated that the number of hummingbird species will probably double in the next few million years.
As they evolved, they developed one of the most unusual flying styles of any bird—one that’s closer to insects. The wings of medium-sized species beat around 80 times a second, but probably not in the way you think. When I ask people to mimic a hummingbird’s wingbeats, they typically stick their hands out to the side and flap them up and down as fast as they can. That’s not how it works. Try this, instead. Press your elbows into your sides. Keep your forearms parallel to the ground and swing them in and out. Now, rotate your wrists in figure eights as you do it. Congratulations, you look ridiculous, but you’re also doing a decent impression of hummingbird flight.
That unusual wingbeat allows them to hover, but it also allows for more acrobatic maneuvers. Hummingbirds use that aerial agility to supplement their nectar diet with insects, which they snatch from the air. While many birds can do that, they typically have short beaks and wide gapes. Hummingbirds, by contrast, have long flower-probing bills and narrow gapes. “It’s like flying around with a pair of chopsticks on your face, trying to catch a moving rice grain,” says Rubega.
But once again, she has shown that there’s more to these birds than meets the eye. Another of her students, Gregor Yanega, found that as the birds open their mouths, they can actively bend the lower half of their beaks, giving it a pronounced kink and getting it out of the way. Then, the hummingbirds essentially ram insects with their open mouths.
High-speed cameras again revealed their trick. “The moment Gregor first saw a bird fly into frame and open its beak, he stopped, and said: Hey, can you look at this?” says Rubega. She walked in and he played the footage. She asked him to play it again, and he did. Just one more time, she said. He played it again.
“That is wild, and you should know that nobody has ever seen that before you,” she told him.


In the fall of 2013, Charlotte Lindqvist got a call from a film company making an Animal Planet documentary about the yeti, the mythical apelike creature that roams the Himalayas. So, not the kind of thing scientists usually like to mess with. “Friends or colleagues were saying, ‘Oh, watch out. Don’t get into this whole area,’” she recalls with a laugh. But she said yes.
Lindqvist said yes because she is a geneticist who studies bears, and the rare Himalayan brown bear is one possible origin of the yeti legend. The team from Icon Films wanted to use science to investigate whether the yeti is real; Lindqvist wanted to investigate the enigmatic bears of the Himalayas.
Wild bear DNA is not easy to come by. Over the years, Lindqvist, a professor at the University at Buffalo, has built up a network of wildlife-biologist contacts in Alaska, who send her samples that have helped illuminate the evolution of polar bears. Scientists know much less about bears that live around the Himalayas. But if a film-production company was going to pay a crew to travel around the mountain range collecting possible samples of fur and bone, then she just might get a scientific project out of it, too.
The results of that unusual collaboration were published Tuesday in the Proceedings of the Royal Society B. Lindqvist and her colleagues used DNA to identify nine “yeti” samples.
These include: a thigh bone found by a spiritual healer in a cave that turned out to be from a Tibetan brown bear; hair from a mummified animal in a monastery that turned out to be from a Himalayan brown bear; a tooth from a stuffed animal collected by Nazis in the 1930s that turned out to be from a dog. The rest of the samples turned up five more Tibetan brown bears and an Asian black bear. For comparison with verified bear samples, Lindqvist also reached out to her network of research contacts in museums, zoos, and Pakistan’s Khunjerab National Park, who provided her with bear hair, bone, and scat to sequence.
Altogether, this search for the yeti yielded a surprising portrait of bears living around the Himalayas. The Tibetan brown bear and Himalayan brown bear, long considered to be subspecies, are quite distinct genetically. The latter diverged from all other brown bears about 650,000 years ago, when the formation of glaciers may have isolated a population that became the first Himalayan brown bears. Today, this ancient lineage of bears is critically endangered.
Lindqvist focused her analysis on DNA in the mitochondria—structures in the cell that have their own small pieces of DNA separate from the DNA in chromosomes. Mitochondria DNA is only passed down the maternal line, but when it comes to sequencing, it has the advantage of being more abundant in cells. This is especially important when working with degraded and decades-old samples. Her team eventually sequenced, for the first time, the entire mitochondrial genome of the rare Himalayan brown bear.
Other scientists have sequenced supposed yeti samples before—notably Bryan Sykes, a geneticist at Oxford who actually appeared in a previous yeti film by the same documentary team that aired on the United Kingdom’s Channel 4 in 2013. (Interest in the yeti never dies, apparently.) In it, Sykes says the hair matched no modern bears but an ancient 40,000-year-old polar bear, suggesting the yeti is actually an unknown, perhaps hybrid bear. Sykes later published the results in a scientific journal, but other scientists criticized him for extrapolating too far from a fragment of a single mitochondrial gene.
Lindqvist thinks she has resequenced one of the same samples, and based on the whole mitochondrial genome, the purposed yeti hair indeed came from a Himalayan brown bear. Ross Barnett, a paleogeneticist at the University of Durham, praised the methods in the new study. It’s the first time, he says, that he knows of a study using whole mitochondrial genomes to place bears in their evolutionary and geographic context.
The Animal Planet film eventually aired in May 2016 as Yeti or Not? Near the end, Lindqvist appears to reveal the last of the DNA-sequencing results. The show has been building up to this moment, hinting at possibilities like a new hybrid bear or maybe even an undiscovered hominid. “When I had to reveal to them that okay, these are bears, I was excited about that because it was my initial motive to get into this,” says Lindqvist. “They obviously were a little disappointed.”


Many people visit the fossil hall at Chicago’s Field Museum for the dinosaurs; but a certain kind of art lover goes for the murals. Originally painted by the famed wildlife artist Charles R. Knight in the late 1920s, each of the hall’s 28 murals presents an elegantly composed moment in time: armored squid tossed onto a desolate Ordovician beach, a duel between Tyrannosaurus and Triceratops, saber-toothed cats snarling at flocks of giant vulture-like Teratornis. There’s a dreamy quality to the images, impressionistic landscapes blending with vibrant animal figures. It doesn’t quite matter that the renderings are now scientifically out of date; they’re convincingly alive.
Such works of paleoart—a genre that uses fossil evidence to reconstruct vanished worlds—directly shape the way humans imagine the distant past. It’s an easy form to define but a tricky one to work in. Paleontological accuracy is a moving target, with the posture and life appearance of fossil species constantly reshuffled by new discoveries and scientific arguments. Old ideas can linger long after researchers have moved on, while some artists’ wild speculations are proved correct decades after the fact. Depictions of extinct animals exist in the gap between the knowable and the unknowable, and two recent books, Paleoart: Visions of the Prehistoric Past and Dinosaur Art II: The Cutting Edge of Paleoart, probe the different ways creators have tried to bridge that divide.
The Artists Who Paint Dinosaurs
As The Atlantic’s Ross Andersen wrote in a piece about paleoart in 2015, “To contemplate a dinosaur is to slip from the present, to travel in time, deep into the past, to see the Earth as it was tens, if not hundreds, of millions of years ago.” Paleoart, published by Taschen this fall, is primarily focused on how this past appeared to artists starting in the 19th century, when the genre first took root. A lavishly reproduced gallery of 160 years of prehistory-themed art, the book includes a series of short contextual essays from its author, the journalist Zoë Lescaze. Many of the animals presented in Paleoart may look odd to the modern eye: bloated, skeletal, or dragging their tails in the scientific fashion of the time. Lescaze doesn’t spend much time reflecting on the changing paleontological ideas that informed the drawings and paintings, though. “I came at the artwork through a more cultural lens,” Lescaze told me. “How they might reflect the political events of that period, or events in that artist’s own personal biography, and other techniques that any art historian would bring to a work of fine art.”
The oldest entries in the genre, in particular, illuminate how paleoart can reflect both political and aesthetic movements, Lescaze said. The first formal reconstructions of extinct animals appeared in the 1800s, around the time the first Mesozoic fossils came under scientific study. Europe was in tumult, with empires wrangling over colonial territory, and discoveries around biodiversity, extinction, and evolution were coming at a blinding pace. As such, reconstructions often took on an allegorical cast. The French artist Édouard Riou depicted marine reptiles such as Plesiosaurus and Ichthyosaurus squaring off like warships on the high seas, perhaps reacting to the naval battles of the Napoleonic wars, according to Lescaze. In the apocalyptic watercolors of John Martin, nightmarish beasts writhed and flailed in the antediluvian ooze. The artist Benjamin Waterhouse Hawkins thrilled Victorian Britain with paintings and sculptures of dinosaurs presiding as regal monarchs over tropical kingdoms full of lesser reptiles.
But paleoart didn’t really come into its own until the arrival of Knight. An American painter who began his career in the late 19th century and reached his peak in the early 20th, Knight worked closely with scientists such as Henry Fairfield Osborn and Barnum Brown to portray his subjects as accurately as possible, given the assumptions at the time. (In keeping with Osborn’s ideas, Knight gave his dinosaurs reedy, lizardy limbs, rather than the beefy, bird-like legs the fossils actually suggested.) Nearly blind by the time he was in his 30s, Knight opted for a naturalistic style full of heft and movement, with complementary colors, soft palettes, and expansive scenery. By Knight’s death in 1953, Lescaze said, his creations had directly influenced films like King Kong and Fantasia, writers such as Ray Bradbury, and a plethora of young paleontologists and artists.
During Knight’s life—and for some time afterward—paleoart remained a fairly loose field. Painters came from an assortment of backgrounds; some were trained illustrators, and others were enthusiastic amateurs. While they adhered to the larger paleontological views of the time, not everyone was necessarily concerned with anatomical rigor. In the 1930s and ’40s, European artists like Mathurin Méheut sought romance in prehistory with Art Nouveau designs and evocative watercolors, setting his bat-winged pterodactyls and drooping long-necked dinosaurs among asymmetrical arabesques. The Soviet paleontologist Konstantin Konstantinovich Flyorov (a great fan of Knight’s, Lescaze said) escaped the enforced artistic realism of the USSR by depicting the ancient world as a series of off-kilter fairy tales filled with dragon-like dinosaurs.
Toward the end of the 20th century, however, overt metaphor and experimentation were largely replaced by rehashes of Knight’s style, and artists drifted further away from the genre’s scientific underpinnings. The majority of those illustrating extinct animals were commercial artists without much knowledge of paleontology. A lack of accurate references encouraged large amounts of plagiarism; any one artist’s whim—a pose, a speculative anatomical detail—often became the de rigueur way of picturing an animal for decades afterward. (Knight’s dinosaurs, for example, have had a long and productive career in books, in movies, and on lunch boxes since his death.) There were exceptions, Lescaze said, such as the moody forests and skeletal dinosaurs that Ely Kish began painting in the 1970s. Paleoart ends its survey with her work. In doing so, it misses out on one of the most transformative periods in the genre’s history.
* * *
A major reassessment of dinosaurs that began in the 1960s, and finally took hold in the 1980s, positioned them not as dull evolutionary failures but as active, warm-blooded animals. Researcher-illustrators like Gregory Paul and painters like Mark Hallett began developing a rigorous anatomical style in accordance with new findings, slimming their animals down to lean creations of muscle and bone. In 1993, Jurassic Park tapped into this momentum, setting a new baseline for what dinosaurs should look like and sparking a popular craze that never quite faded.
The internet had a fundamental effect on paleoart, too. It became easier to find technical information on prehistoric animals’ anatomy, or the latest theories about their behavior. Image-hosting sites like DeviantArt, curated websites like The Dinosauricon, and dedicated blogs served as hubs for a growing paleoart community. Email listservs and the rise of social media meant researchers, professional artists, and amateurs could collaborate with each other on a wider scale. The field, in the 2010s, has become more accessible, accurate, and forward-looking than ever before—as well as more stylistically constrained.
Dinosaur Art II: The Cutting Edge of Paleoart is a dispatch from this internet age of paleontology, and is in some ways a revealing companion to Taschen’s Paleoart. Published in October by Titan Books, it compiles in-depth interviews and curated work from modern paleoartists across the globe, as collected by Steve White, a U.K. comics artist. (The book is a sequel to 2012’s Dinosaur Art: The World’s Greatest Paleoart.) Some of the featured illustrators, like Brazil’s Julio Lacerda, create digital images that look like photographic collages, while the artist Andrey Atuchin works in a clean, detailed style akin to that of classic National Geographic drawings. All the animals in Dinosaur Art II conform closely to modern scientific convention; most of the profiled artists work in the hyper-realistic mode that has come to define the genre. Compared to the breadth of approaches contained within Lescaze’s book, the results can look a little standardized and tame.
Today, the field is seeing a growing tension between a more cautious approach to paleoart and an urge for experimentation. In an attempt to make paleoart more academically credible, artists of the last few decades have often emphasized skeletal fidelity over all else. This proved to be a bit of an overcorrection: Compare a cat skull and a living cat, and it’s easy to see that skeletons aren’t always a good reflection of an animal’s flesh-and-blood appearance. Dinosaurs and prehistoric reptiles illustrated in the modern era have a tendency to look like skin shrink-wrapped over bone. A certain amount of cultural inertia and cliché also lingers, even in more carefully reconstructed art. Predatory dinosaurs in particular are still often depicted in relentless battle, mouths open in frozen roars.
In the 2010s, paleontologists and artists have been pushing for more radically imaginative approaches to soft-tissue anatomy and behavior, and less reliance on standard tropes. The “All Yesterdays” campaign—named after a provocative paleoart book published in 2012—challenged artists to think more broadly about prehistoric animals as living creatures, with sleep habits, social interactions, and foraging behaviors. All Yesterdays–style dinosaurs might have humps, or extravagant inflatable sacs, or unsuspected feathers. “There’s a nihilistic aspect to [the movement],” Mark Witton, a British paleontologist and one of the artists in Dinosaur Art II, told me. “We don’t really know what’s right or wrong about our [soft-tissue] reconstructions, so we might as well be as bold with them as our science will allow. … It’s more just about being honest, and exploring many possible truths rather than one tried-and-tested take on a subject species.”
Only traces of this new approach appear in Dinosaur Art II. Artists like Brian Engh, David Orr, and Rebecca Groom are exploring a wider range of styles, including conscious homage, fine art, and Pixar-inflected designs. As long as the art is grounded by a scientific understanding of the animal in question, Witton said, there’s still a lot of room for inventiveness. “Certain styles distort reality by necessity, so if we simplify the form of our subjects into basic geometries … or apply surreal color palettes, are we still making paleoart?” Witton asked. “We’re still scratching the surface of paleoart’s potential diversity.”
* * *
While paleoart is a form of scientific art, its value doesn’t always lie in its level of accuracy. According to Lescaze, while researching Paleoart, she met a Smithsonian paleontologist who showed her an original Knight dinosaur painting he had in his office. He’d fished it out of a dumpster after a new director disposed of outdated art to make space in the collections. “They’re complex artifacts, and vulnerable in a way that other works of natural history illustration aren’t,” Lescaze said of vintage pieces of paleoart. “Nobody’s going to throw out the John James Audubon, but works of paleoart that are rendered obsolete regularly get discarded. … It’s really important to look back at some of these and say, yeah, they’re not scientifically accurate anymore, but who cares? What else can they teach us?”
Whatever the influences or techniques, paleoart is fundamentally an attempt to glimpse something that can never be fully seen. Anybody who tries to reconstruct prehistory fills in the gaps with their own preoccupations, turning real animals into symbols of obsolescence, savagery, or martial power. Many modern artists are trying to strip these projections out of their art, but changing cultural ideas and paleontological consensus can make doing so difficult. “Evolution is a brush, not a ladder,” the artist Emily Willoughby notes in Dinosaur Art II: not a direct route going anywhere, but, rather, a messy bundle of approaches. It’s only fitting that the art depicting its sweep should be similarly difficult to pin down.



From The Flintstones to Focus on the Family, the stereotype has long been that men hunt and provide, while women just stir the pot. Thankfully, today many women—and men—reject both that biological essentialism and the resulting division of labor. But what can science tell us about the role our earliest female ancestors played in providing food for themselves and their communities? Meanwhile, given the fact that women have been confined to the kitchen for much of recent Western history, how have they used food as a tool of power and protest, escape, and resistance? Just in time for the holiday season, this episode we dive into two books that take on the science and history of women’s relationship with food. First, science journalist Angela Saini helps us upend conventional wisdom on “women’s work” and biological differences between the sexes; then food historian Laura Shapiro reveals an entirely new side to six well-known women through their culinary biographies. Join us this episode as we hunt, gather, and cook with women throughout history, from feral pigs to shrimp wiggle.
The idea that men and women are fundamentally different—that women are physically weaker, less rational, and equipped with smaller brains—was accepted as fact by scientists and enforced by culture for centuries. Charles Darwin himself claimed that women were at a lower stage of evolution. One widely believed argument for male superiority was that early men were the hunters—and that, through hunting, they not only provided food for their families, they also invented the first tools and the earliest forms of language, and thus, by extrapolation, everything that made humans distinctively human. In her new book, Inferior: How Science Got Women Wrong—and the New Research That’s Rewriting the Story, Saini exposes the flaws in that hypothesis to offer an inspiring revision of gender equality among early humans.
For the heroines of the culinary historian Shapiro’s newest book, What She Ate: Six Remarkable Women and the Food That Tells Their Stories, food has also proved itself an instrument of power, if not exactly equality. In this episode, Shapiro helps us explore how the disgusting food served in the Roosevelt White House—widely regarded as the worst in American history—offers a new perspective on the tensions that underlay Eleanor Roosevelt’s marriage. And she tells us how the delightful novels of British author Barbara Pym use food to explore gender, class, and character—but also help rehabilitate the appalling reputation of English cuisine. Together, these books reinforce the power of food to reshape the stories we tell about ourselves. Listen in this holiday season, then read—and then eat!
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


In the summer of 1922, as Simeon Burt Wolbach and Marshall Hertig slid their scalpels into 13 common house mosquitoes, they had no idea that they were about to stumble across one of the most successful microbes on the planet, nor that a century later, their discovery could potentially save millions of lives.
In those dissected mosquitoes, the duo found a new bacterium, and 14 years later, Hertig christened it Wolbachia after his colleague. Having named it, they ignored it. But in the subsequent decades, long after Wolbach’s death in 1954, scientists started to realize that his bacterial namesake is omnipresent, devious, and powerful.
It infects around four in 10 species of insects and other arthropods, which are themselves the most diverse and numerous animals in the world. It manipulates the sex lives of its hosts, changing some from males to females and allowing others to clone themselves, all so that it can spread quickly into the next generation. It even contains multitudes of its own: Nestled within its genome is a virus, and hidden within that virus are genes that originally came from spiders.
Wolbachia also makes some of its hosts resistant to viruses—a quality that a team of Australian scientists, led by Scott O’Neill, have spent decades trying to harness. When they implant the bacterium into the mosquito Aedes aegypti, the insect can no longer transmit the viruses behind dengue fever, yellow fever, Chikungunya, and Zika. And best of all, the bacterium is so good at manipulating its hosts that it can rapidly spread through a wild population. Release Wolbachia-infected mosquitoes into the wild, and within months, all the local bloodsuckers change from carriers of disease into culs-de-sac.
This approach, which I’ve written about before, is now being tested in 10 tropical countries around the world. It’s testament to the value of research for the sake of it: There was no way Wolbach and Hertig could have predicted where their arcane explorations of mosquito carcasses could have led.
The project also shows just how long it takes to translate basic discoveries into life-changing tools: It took decades for O’Neill’s team to successfully inject Wolbachia into mosquito embryos, to show exactly how the bacterium acts against viruses, to simulate its spread among wild insects to show that their approach could work in the field, and to get all-important community support for their work. You can watch their quest in the video below—the fourth in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.


In caves and rock walls of the southern Utah desert, pictographs have been painted, added to the backs of clamshell-shaped sandstone enclosures. Many are noted to have acoustic properties, meaning these ancient, indigenous images seem to be correlated with the way sound reflects around them. I’ve spoken in a normal voice back and forth from one sheltered rock-art panel to another an eighth of a mile down the canyon. The way sound spreads and is refocused, we could hear each other’s every word.
James Farmer, from the Utah Rock-Art Research Association, wrote that panels from the ghostly and enigmatic Barrier Creek tradition in Utah contain what he sees as thunderstorm motifs. At one of these Barrier Creek panels, he witnessed a cloudburst with thunder, waterfalls, and falling boulders. He wrote about the intensification of sound from the storm around the rock art, “it seems inconceivable to me that any ancient archaic hunter-gatherers witnessing a similar event would not have been just as astonished as me, and would have naturally invested the location with divine, supernatural powers.”
The nascent field of “archaeoacoustics” studies the way sound and archaeological sites interact. I look at this as not just an ancient feature, but one that we walk through every day. Cathedrals and capital domes have been noted for the way they capture and amplify sound. By happenstance or not, resonance is part of the way we relate to architecture, whether human-made or carved by nature.
Have you ever walked through an airport or the lobby of a building and noticed a sudden change in the acoustics? Even in a crowd, you hear your own footsteps as if you’d walked across a microphone. Like the acoustics of Barrier Creek panels, this is something I’ve explored in modern human environments. A friend who goes looking for them with me calls them “vocalizers.”
Once you start looking and listening, you find them all over, outdoor gardens, entrances to skyscrapers. One of the best I’ve found is a dome of focused sound created by the ceiling of Terminal C of the George Bush Intercontinental Airport, near the turn to gates 24–27. It’s like stepping through an invisible veil into a secret space. These are architectural simplicities, a circle or cupola pleasing to the eye, maybe with benches or planters or sculptures. The center is often marked with some small feature, a compass star, an intersection of lines, a mosaic of a circle or globe, or simply a drainpipe if not too fancy.
If you happen to pass over that center, or pause in conversation, the effect is immediate. You’ll hear your own voice reflected back on you with startling and encompassing clarity, louder than any other sound in the vicinity.
A related phenomenon is the “whispering wall” or “whispering gallery.” This is where a curved surface carries the slightest vocalization to another, distant location. Grand Central Station in New York has a famous one. On the lower concourse, just outside of the Oyster Bar, the voice of a person standing in one corner will travel up and over the crowd and land on the ears of a person standing in the opposite corner 30 feet away.
The “vocalizer” is slightly different, perhaps more ubiquitous. These are places that broadcast yourself back to you. I’ve taken to calling them “whispering wells.”
Circular is best, though a semicircle will do. Step into the center and say something or make a hiss. Whatever sound you create, even a clearing of your throat if you don’t want to attract attention, will come back from surrounding walls at exactly the same instant, none of the messy humdrum of baffles and angles, no single echo point any closer or father away than the others.
Tell somebody about it, a stranger walking by. I’ve done this; it generally works once they get past their understandable suspicion. Lead them to the center (many of these are in very public and nonthreatening locations) and when they speak to ask what they should do, they shut their mouth instantly, then say something else like hello or echo and look back at you amazed. Some people become giddy with excitement. Try the front of the Transamerica Pyramid in San Francisco—not the finest example, but an easy one to find. It just takes a few seconds. Step into the middle of one of its outdoor circles. An acoustic bubble envelopes you. You can hear the traffic and noise around you, but if you start singing quietly, which generally draws only mild attention, you are singing back to yourself, audience of one.
I move through the city the same way I move through the desert, looking for shaded alcoves that might hold rock art, hissing or clucking my tongue to hear the sound bounce back. Buildings become canyons, and the rounded architecture of lobbies, capital domes, or outdoor sitting areas where professionals eat their lunches become natural shelters, sites of acoustic reflections. And there I stand humming out loud, apparently imbued with supernatural powers.
This post appears courtesy of The Last Word on Nothing.


In the winner-takes-all game of fertilization, millions of sperm race toward the egg that’s waiting at the finish line. Plenty of sperm don’t even make it off the starting line, thanks to missing or deformed tails and other defects. Still others lack the energy to finish the long journey through the female reproductive tract, or they get snared in sticky fluid meant to impede all but the strongest swimmers. For the subset of a subset of spermatozoa that reach their trophy, the final winner would be determined by one last sprint to the end. The exact identity of the sperm was random, and the egg waited passively until the Michael Phelps of gametes finally arrived. Or so scientists have thought.
Joe Nadeau, principal scientist at the Pacific Northwest Research Institute, is challenging this dogma. Random fertilization should lead to specific ratios of gene combinations in offspring, but Nadeau has found two examples just from his own lab that indicate fertilization can be far from random: Certain pairings of gamete genes are much more likely than others. After ruling out obvious alternative explanations, he could only conclude that fertilization wasn’t random at all.
“It’s the gamete equivalent of choosing a partner,” Nadeau said.
His hypothesis—that the egg could woo sperm with specific genes and vice versa—is part of a growing realization in biology that the egg is not the submissive, docile cell that scientists long thought it was. Instead, researchers now see the egg as an equal and active player in reproduction, adding layers of evolutionary control and selection to one of the most important processes in life.
“Female reproductive anatomy is more cryptic and difficult to study, but there’s a growing recognition of the female role in fertilization,” said Mollie Manier, an evolutionary biologist at George Washington University.
* * *
The idea of sexual selection is as old as Charles Darwin himself. In On the Origin of Species, he wrote of the peacock’s showy tail and the elk’s giant antlers as examples of traits that evolved to help males show off their appeal as mates to females. For the next century, biologists focused on all the aspects of sexual selection that operated in the events leading up to copulation. After mating, the female had made her choice, and the only competition was among the sperm swimming to the egg.
This male-oriented view of female reproductive biology as largely acquiescent was pervasive, argued Emily Martin, an anthropologist at New York University, in a 1991 paper. “The egg is seen as large and passive. It does not move or journey but passively ‘is transported’ ... along the fallopian tube. In utter contrast, sperm are small, ‘streamlined,’ and invariably active,” she wrote.
Beginning in the 1970s, however, the science began to undermine that stereotype. William Eberhard, now a behavioral ecologist at the Smithsonian Tropical Research Institute, documented all the ways that females can affect which males fertilize their eggs even after mating. It’s a long list, and scientists still can’t say for sure whether they’ve documented everything. The belatedness of these discoveries wasn’t all due to sexism. Two walruses dueling with their tusks is easy to observe; games of hide-and-seek with sperm inside the female reproductive tract are much less so.
“As soon as you have eggs and sperm, you have sexual selection. There are incredible things that eggs and seminal fluid can do,” explained Andrea Pilastro, an evolutionary biologist at the University of Padova in Italy.
In those species in which fertilization happens outside the body, the females often coat their eggs with a thick, protein-rich ovarian fluid. Experiments in 2013 by Matthew Gage of the University of East Anglia in England showed that this fluid contains chemical signals to help attract the correct species of sperm. When they exposed eggs from salmon and trout to mixtures of sperm from both species, the eggs’ own species successfully fertilized 70 percent of the time, significantly more than to be expected by chance.
“The sperm behaved differently in different ovarian fluids. They actually swam straighter in their own fluid,” Gage said.
Internal fertilizers have their own methods of what Eberhard dubbed “cryptic female choice.” Some female reproductive tracts are labyrinthine, complete with false starts and dead ends that can stymie all but the strongest sperm. Some females, including many species of reptiles, fish, birds, and amphibians, that copulate with more than one male (which biologists estimate are a vast majority of species) can store sperm for months, even years, altering the storage environment to stack the odds to favor one male over another. Many female birds, including domestic chickens, can eject sperm after mating, which lets them bias fertilization in favor of the best male.
All these strategies, however, provide females with opportunities only to select the sperm of different males. Within an ejaculate, which sperm fertilized the egg still seemed to be left to chance.
In fact, the randomness of fertilization is implicit in the principle of segregation—the first law of genetics going back to Gregor Mendel. Parents carry two copies of each gene, which are divided randomly into gametes that carry only one copy. It’s what gives rise to many of the probabilities students learn in high-school biology. If both parents are heterozygotes—meaning they carry two alternate versions of the same gene—then half their offspring would also be heterozygotes. A quarter of the offspring would be homozygotes carrying two copies of one version, and the remaining quarter would be homozygotes with the other version.
“It’s one of the most broadly applicable rules in biology,” Nadeau said.
Yet these probabilities work out only if fertilization is random. If the egg or the sperm can somehow influence the identity of the other gamete involved in fertilization, then those ratios could be very different. This striking difference was what caught Nadeau’s attention back in 2005. When he started looking at the inheritance of two particular genes in mice, the probabilities were all off. In his Seattle lab, he began to wonder: Could Mendel have been wrong?
* * *
Nadeau hadn’t set out to question Mendel. Instead, he wanted to know how interactions between two genes (Apobec1 and Dnd1) affected risks for testicular cancer, one of the most heritable forms of cancer. When Nadeau and his doctoral student Jennifer Zechel bred female mice carrying one normal and one mutant copy of Dnd1 with heterozygote Apobec1 males, everything appeared to follow Mendel’s rules. So far, so good. But when they reversed the breeding (a female Apobec1 heterozygote mated with a male Dnd1 heterozygote), things got weird: They found that only 27 percent of the expected offspring carried copies of mutant Apobec1, mutant Dnd1, or both, compared with the 75 percent they expected to see.
As a researcher who had spent several decades studying heredity, Nadeau was aware of myriad factors that could affect Mendel’s ratios. If a fertilized egg ended up with two mutated copies of a recessive gene, the resulting embryo might die early in development. Such embryonic lethal mutations would alter the ratio of homozygotes to heterozygotes, but it would also reduce the average number of mouse pups in each litter. Yet all of Zechel and Nadeau’s mice had standard litter sizes, and they found no evidence that embryos were dying early after fertilization.
Perhaps, Nadeau reasoned, the problem lay in the sperm, not the egg. He therefore bred male mice with and without the mutation to healthy mutation-free females and found no differences in the males’ fertility—something that would have become obvious if the mutation were affecting sperm formation. Step by step, Nadeau and his team eliminated every possible cause of these wonky ratios of offspring genotypes ... except one: that during fertilization, the egg and sperm were genetically biased against the mutant genotype.
Surely, someone else must have already seen this, Nadeau reasoned, so he searched the scientific literature. Although he could find plenty of examples of unexplained offspring ratios, no one had seriously pursued genetically biased fertilization as an answer.
“Everyone just interpreted it as embryonic lethality because we see what we look for and we explain it using what we know,” Nadeau said.
One of those examples Nadeau found was from the lab of the cancer researcher Roseline Godbout at the University of Alberta. Godbout studied the role of a protein called DDX1 in the development of retinoblastoma, a highly heritable childhood cancer. Mice that were missing one functional copy of the DDX1 gene (but with another, fully functional gene as backup) seemed normal and healthy. When Godbout and Devon Germain, now a postdoctoral fellow at the Max F. Perutz Laboratories in Vienna, bred such heterozygote males and females, they found that none of the offspring lacked both copies of DDX1, even though simple Mendelian math would suggest 25 percent of them should. Given the gene’s importance to DNA replication, however, this wasn’t surprising: The homozygotes without DDX1 presumably died after conception. Godbout and Germain also found lower-than-expected numbers of homozygote offspring with two copies of DDX1. A complicated series of mating experiments led the scientists to propose that their results came from a rare mutation that had occurred in the DDX1 gene during their experiments.
Nadeau wasn’t convinced. He wrote to Godbout to ask how her lab had verified that the “knockout” homozygotes without DDX1 genes had died as embryos. They hadn’t. He also asked whether they had considered genetically biased fertilization, wherein the egg preferred to fuse with a sperm of the opposite DDX1 genotype.
“We really thought it was just a weird pattern of inheritance,” Germain recalled. “We hadn’t thought about nonrandom fertilization.”
Later, on a whim, Germain decided to review all the raw data from his experiments. As he looked over the results, he remembered Godbout’s questions that had been prompted by Nadeau’s email. The more he looked at the data, the more that genetically biased fertilization looked like “the most plausible explanation,” he said.
Frustrated at how few scientists had seriously considered genetically biased fertilization as an explanation for their results, Nadeau wrote up his hypothesis in “Can Gametes Woo?,” an article published in October in Genetics. His goal, he said, was to spur more research into this area and determine if and how egg-and-sperm interactions can alter fertilization.
“We’ve been blinded by our preconceptions. It’s a different way to think about fertilization with very different implications about the process of fertilization,” Nadeau says.
Other scientists, such as Manier at George Washington University, say that Nadeau’s hypothesis is intriguing and even plausible, but they point out that no one has any evidence about how it could happen. Nadeau agrees and points to two possibilities.
The first involves the metabolism of B vitamins such as folic acid, which form important signaling molecules on sperm and eggs. Research in Nadeau’s lab has shown that these molecules play an outsize role in fertilization, and he believes abnormalities in certain signaling genes may alter how much sperm and egg attract each other.
A competing hypothesis builds on the fact that sperm are often present in the female reproductive tract before the final set of cell divisions that produce the egg. Signals from the sperm could influence these cell divisions and bias the identity of the cell that becomes the egg.
Whatever the mechanism might be, this work challenges the standard view of female physiology as passive during fertilization. “Females were seen as passive objects with no choice, but females are going to have a vested interest in the outcome of fertilization,” said Renee Firman, an evolutionary biologist at the University of Western Australia. “We still have a long way to go to understand this process, but I don’t think we still really appreciate how common this is and how often it happens.”
Finding data to support or refute this hypothesis could be challenging, Manier said. It will depend on showing that genes within the sperm affect their surface molecules, and that the egg can sense these differences. Such results will require detailed biochemical studies of individual sperm cells and sequencing information about their genome.
Nadeau is prepared for skeptics—he’s encountered many at conferences when he presents the results of his mouse studies and his hypothesis for what’s going on. Critics often approach him after the talk and begin asking him questions. Whether they walk away convinced is unclear, but Nadeau feels they are much less certain that biased fertilization doesn’t happen. To Harmit Malik, a geneticist and virologist at the Fred Hutchinson Cancer Research Center, the situation is the ultimate Sherlock Holmesian solution.
“If you’ve eliminated the impossible, then what remains, however unlikely, must be the truth,” he quipped.


Dolly the sheep was the first animal to be cloned from an adult cell, and like many firsts, she came to stand in for all of her kind.
So when scientists suspected she had short telomeres—stretches of DNA that normally shorten with age—people wondered if it was because she was cloned from an adult cell. When she started to limp at age five, headlines said that her arthritis “dents faith in cloning.” And when she died at age six—as the result of a common lung virus that also killed other sheep in her barn—her short life again became a parable about cloning. A certain narrative took hold.
Then last year, Kevin Sinclair, a developmental biologist at the University of Nottingham, published a paper about several clones including Dolly’s four “sisters,” who were created from the same cell line as Dolly and lived to the old age of eight (about 70 in human years). They were quite healthy for their age. So of course, he kept getting questions, like if these animals are so healthy, then why was Dolly so unhealthy? It was Dolly that everyone cared about.
Sinclair would point out that Dolly was not so unhealthy. But the questions inspired his team to go looking for Dolly’s health records from the early 2000s. The records, however, were lost. “Everything has moved on. People had moved away, and people are doing other things,” says Sinclair. But after her death in 2003, Dolly’s bones were turned over to the National Museum of Scotland. Sinclair’s team got permission to study them—along with the bones of Megan and Morag, two sheep cloned from non-adult cells who were prototypes for Dolly, and Dolly’s naturally conceived daughter Bonnie.
A team of veterinarians scored X-rays of the bones for signs of arthritis. Megan and Bonnie, who had died at the ripe old ages of 13 and nine, respectively, did indeed have signs of arthritis, which was normal for their age. Megan, who had died at age four in an earlier outbreak of same lung virus that killed Dolly, did not. Even Dolly’s knee did not show signs of arthritis.
Arthritis also affects the soft tissue in a joint, so Sinclair notes the bones alone do not point to a conclusive diagnosis or lack thereof. (And Dolly really did limp.) But the overall set of data from Megan, Morag, and Bonnie as well as Dolly’s elderly sister clones suggest arthritis is no more common among clones than ordinary sheep. Fears about prematurely aging clones may be greatly exaggerated. “We felt the record needed to be set straight,” says Sinclair.
Even the fears about Dolly’s too-short telomeres haven’t quite borne out. Telomeres are repetitive DNA sequences at the ends of chromosomes, and they are shortened every time a cell divides. In 1999, scientists published data suggesting Dolly’s telomeres were too short for her age. Since then, scientists have cloned a whole menagerie of animals: mice, horses, cattle, pigs, dogs, and so on. Studies of their telomere lengths have turned up every possible result: Clones have shorter telomeres, clones have longer telomeres, and clones have normal telomeres—depending on the species or cloning technique.
Clones do have unique health problems, just not the ones that dominated headlines about Dolly. Clones are less likely to make it to term in pregnancy, and when they are born, they are more likely to be a little maladjusted. “You have to baby them—give them oxygen, give them glucose until they normalize,” says George Seidel, who studies animal-reproduction technologies at Colorado State University. The clones that make it to adulthood are generally pretty normal. But that makes it extremely expensive to clone livestock, on the order of $20,000 each. Despite much ado about cloned meat, cloning is not an economical way to produce cattle for meat or milk.
It does have niche applications in the livestock industry, though, like recreating million-dollar bulls with extremely valuable genes. Seidel, who also has an cattle ranch, buys semen from Final Answer 2, the clone of the famed sire Final Answer who died in 2014. Semen from the right bull is also incredibly valuable to the dairy industry. “You’ve probably had some cheese from the offspring of a clone,” says Seidel. But actual clones are not sold for food in the United States.
Of Clones and Clowns
For all the attention Dolly once attracted, cloning itself has not radically altered the world. The research that created her did teach scientists a lot about how embryos can be manipulated, which has inspired new lines of research into stem cells.
Dolly’s four sister clones—Daisy, Debbie, Denise, and Dianna—were actually euthanized last year. Sinclair’s group is still studying their preserved cells and tissues for answers to question like the unresolved debate over telomere length. But it’s the end of a certain era. “You get to know them very well,” says Sinclair about Daisy, Debbie, Denise, and Dianna. “Even though they’re clones they have their own personalities, and this is an important point to make.” They may have started with the same DNA as Dolly, but they were not the same as her.


In September, Bill Snape and his family took the 90-minute drive from their home near Washington, D.C., to the grounds of Shenandoah National Park. It was a trip they had made many times before, but this time Snape was taking it to check out a rumor. He had heard that something unusual was on sale at Skyland Lodge, a hotel on the park’s premises.
At the gift shop of the lodge, Snape found what he was looking for: multiple cases of Trump wine, produced at the Trump family’s nearby winery.
“At first, it just annoyed me. And then I thought, what is the law?” he told me on Wednesday. Snape is a senior counsel at the Center for Biological Diversity, an environmental advocacy organization.
Snape worries that the sale poses a major conflict of interest and may even violate the Constitution’s emoluments clause. “Emoluments means advantage. You cannot use your public office for your personal advantage,” he said.
The company behind Skyland Lodge has now confirmed that it sold Trump wine at Shenandoah National Park this year. This week, Snape and the Center for Biological Diversity filed a request under the Freedom of Information Act to learn how Trump wine came to be sold at Shenandoah in the first place.
The sale gets at a wrinkle in how the federal parks system operates: While Trump wine was on sale at Shenandoah National Park, the National Park Service wasn’t actually the organization that was selling it. Delaware North, a company based in Buffalo, New York, operates the hotels and gift shops on the grounds of Shenandoah, including Skyland Lodge. It also manages attractions in Yosemite National Park and Grand Canyon National Park.
“At Shenandoah National Park we offer wines from several different Virginia vintners,” said Glen White, a spokesman for Delaware North.
Up until September, he continued, the company had been offering wines from Trump Winery, as well as Kluge Estate (the name of the vineyard before Trump bought it) “because they are locally produced.” (Snape said Trump wine was on sale until at least October.)
“This was only at Shenandoah, where it is a local product, and not at any other parks,” White said. “The National Park Service did not request or require us to carry it.”
Jeremy Barnum, a spokesman for the National Park Service, also told me that Delaware North had been selling Kluge Estate wine for years. He said the park service “does not specify what brands of these products should be sold” by concessioners like Delaware North.
Delaware North is one of more than 500 companies with a contract to sell goods or services within a national park. The National Park Service says that concessioners across the country employ more than 25,000 people, who are not federal employees, and generate more than $1 billion in receipts.
But the wine was definitely still on sale at Shenandoah at the time that Donald Trump promoted it during a presidential press conference. After his statement on the white-supremacist march in Charlottesville, Virginia, in August, Trump told reporters: “I own actually one of the largest wineries in the United States—it’s in Charlottesville.”
The winery itself says this is not true: Donald Trump turned ownership of the property over to his son, Eric, in 2011. Town & Country magazine also disputes that Trump’s winery is one of the largest in the country.
Snape says that even if a concessioner sold the wine—and not the park service—it still doesn’t pass muster.
“It’s like a Rubik’s Cube, it looks bad from every angle,” he told me. “Why is the concessioner behaving that way—is the concessioner looking for special favors? And why is the park service allowing it to be sold?”
“Whatever you want to say about George Bush or Jimmy Carter or Bill Clinton, we weren’t eating Jimmy Carter peanuts, or buying George Bush oil, or using Bill Clinton condoms,” he said. “Where is the line between the public duty and his personal profit seeking?”
Trump family investments and the National Park Service do not only overlap at Shenandoah National Park. The observation deck of Trump’s new hotel in Washington—the Trump International, on Pennsylvania Avenue, just a few blocks from the White House—is operated and overseen by park-service rangers.


Left undisturbed by brushing and flossing, the bacteria in your mouth will form a sticky film called plaque. Left further undisturbed, plaque will turn hard and yellow, calcifying on teeth as dental calculus, also known as tartar.
At this point, the tartar is very durable. Just ask these Neanderthals, whose 40,000-year-old tartar scientists recently analyzed to figure out the real paleo diet. Tartar grows in layers—almost like tree rings—entombing DNA from tiny bits of food as well as bacteria in the mouth. Forty thousand years later, scientists can analyze that DNA to reconstruct what was going on in the mouths of long-dead Neanderthals.
Neanderthal Dental Plaque Shows What a Paleo Diet Really Looks Like
Having traveled so far back in time using ancient tartar, some of the same scientists have embarked on a more ambitious project: using the DNA from the bacteria in tartar to figure out how humans settled the 10 million square miles of Polynesia.
Polynesia has confounded the traditional ways of tracing human migration—archaeology, linguistic analyses, even human DNA—because large parts of it were settled so fast. Humans first reached the Society Islands, in the center of the Polynesian Triangle, perhaps around 1,000 AD. Then in the span of just a couple hundred years, they took canoes across vast tracts of open ocean to find specks of inhabitable rock as far-flung as Hawaii, New Zealand, and Easter Island. How Polynesians navigated these waters in the 11th century is a subject of considerable fascination. But even more basically, archaeologists are not sure exactly when the islands were settled and in what order. That’s where the tartar comes in.
Because bacteria are constantly dividing, they accumulate frequent mutations in their DNA. By comparing the mutations in tartar found in one island and the next, the researchers hope to figure out whether early settlers systematically hopscotched across Polynesia or skipped certain islands. “The traditional means of looking at human migrations might be too coarse. Hopefully, the rapid rate of evolution in that bacteria will allow us to answer some of the questions,” says Raphael Eisenhofer, a Ph.D. candidate at the University of Adelaide, and coauthor of a recent paper proposing the use of tartar to track human migrations.
He and a group of DNA researchers, as well archaeologists, have since begun sampling teeth from Polynesia. Removing ancient tartar is not that different from modern dentistry, says Laura Weyrich, an ancient-DNA researcher at the University of Adelaide and Eisenhofer’s adviser. She pops it off with a dental pick. The group has collected hundreds of samples from collections of the Natural History Museum in London and other museums.
It’s much easier, Weyrich says, to convince museums to give up some tartar than to sacrifice a bone for human DNA analysis. In fact, museums traditionally cleaned the gunk from teeth—to better analyze their shape and to make them look better on display. “From my perspective, it’s incredibly frustrating. It’s like, ‘Noooo, what did they do?’” says Weyrich. More often than not, the most famous specimens have been cleaned. “We’re looking for the skulls that were left in the closet and still dusty and nobody cleaned,” she says.
Back in Adelaide, Eisenhofer is sequencing the samples in a lab built specifically for ancient DNA. Working with ancient DNA is tricky because samples are usually small and degraded, easily swamped by the modern DNA shed by bacteria living in and on our bodies. To prevent contamination, he wears a suit, a mask, and three pairs of gloves.
The team has picked out dozens of strains of common mouth bacteria that bind to the tooth surface and are commonly found in plaque. And they hope to sequence specific genes and track mutations in them over time. Scientist have analyzed specific pathogens—such as Helicobacter pylori in the gut—as proxies for human migration before, but Weyrich and Eisenhofer hope to extract more information by studying the overall community of mouth bacteria. There could be a lot of information hidden in the gunk that museums once threw away.
When Weyrich samples tartar from teeth, she’s careful to pick jaws that have more than one tooth and to always leave some tartar behind. You don’t want to destroy it all, she explains, because you never know what techniques might come along in the future.


When she was in high school, Madhavi Colton was known as Miss Enthusiasm. “I’ve always been a die-hard optimist,” she says. “I tend to be perky. In my family, I was always the one who thought that everything was going to be fine, that we can do this.”
Recent years have tested her optimism. Colton is now a director at Coral Reef Alliance, a nonprofit dedicated to protecting coral reefs. And corals need all the help they can get. A third of reef-building corals are in danger of extinction, and their growth rates have plummeted by 40 percent since the 1970s. They have been pummeled by hurricanes, disease, and pollution. Acidifying water makes it harder for them to create their rocky reefs. Rampant overfishing kills off the grazing fish that keep competitors like seaweed and algae in line. Rising temperatures force them to expel the symbiotic algae in their tissues, which normally provide them with both food and vivid colors. Without these partners, the corals starve and whiten. Once-lush ecosystems full of kaleidoscopic fish become spectral wastelands, where scuzzy green algae grows over the bleached white skeletons of dead and dying corals.
The continuing desecration has taken an immense toll on the mental health of people like Colton who have devoted their lives to studying and saving these ecosystems. How do you get up and go to work every day when every day brings fresh news of loss? When everything you are working to save is collapsing, how do you stop yourself from collapsing, too? Maybe everything isn’t going to be fine, after all. Maybe we can’t do this. “Are we going to lose an entire ecosystem on my watch? It’s demoralizing. It’s been really hard to find the optimism,” she says. “I think Miss Enthusiasm has gone away.”
There was a time, just a few decades ago, when this crisis seemed unimaginable, when reefs seemed invincible. Phil Dustan, from the College of Charleston, similarly remembers being fresh out of grad school and telling the famed explorer Jacques Cousteau that “reefs are so big that humans couldn’t hurt them.” Those words seem hopelessly naïve now. Dustan recently dove at Dancing Lady Reef in Jamaica—a place that he had studied as a graduate student in the 1970s, and where scientists “first became intimate with the science of reefs,” he says. “I dropped into the water and I just choked. It was like someone going through their home after a forest fire has gone through, picking through the ashes.” Elsewhere in the Caribbean, he took his son snorkeling at Carysfort Reef, another site of once-legendary beauty. “He stayed real close to me and he wouldn’t range around because he was fearful,” Dustan says. “Finally, he said: Dad, we have to leave this place. It creeps me out. It’s all dead.”
This catastrophe has unfolded slowly. Nancy Knowlton, from the Smithsonian Institution, says that when it comes to corals, the bad news is usually incremental, and only obvious to those who work in the affected places. “But what happened in the Great Barrier Reef was so spectacularly bad that you didn’t need to work there to know it was bad,” she says.
What happened was this: In 2015, the world experienced a mass-bleaching event, where heat waves started killing corals in all three oceans where they thrive—the Pacific, Atlantic, and Indian. Two such global crises have happened before, in 1997 and 2010, but the 2015 one was unprecedented in its severity, and in its implications. Just 9 percent of the individual reefs that make up the 1,400-mile-long Great Barrier Reef, off the coast of northeastern Australia, were unscathed. All told, more than a quarter of the corals have died there, with a much higher proportion in the northern sections. No one had seen anything like it before. When the coral researcher Terry Hughes revealed the scale of the devastation to his students, they all reportedly wept.
“The news has been especially upsetting, because of the scale of the event, the iconic nature of the Great Barrier Reef, and the fact that there were gifted filmmakers on site to document it,” says Knowlton. “It was a perfect storm of attention.”
For many coral aficionados, it was also a tipping point for despair. One year of bleaching would be bad enough, but the Great Barrier Reef experienced a second in 2016 and there are signs that a third wave might hit by the end of this year. Corals bounce back, but consecutive blows could take even these resilient animals down for the count. “Our models said that wouldn’t happen for a long time, and I’m worried that we’ve underestimated the pace of change,” says Colton. “Things are even worse than we thought, and that’s been hard to cope with.” For her, that gnawing unease has led to sleepless nights, and curtailed her ability to deal with other worrying world events. “My barrel is full,” she tells me. “One more drop and it spills out. My resilience is gone.”
The feeling is ironic, because that’s exactly the problem that corals are facing. “I’m a coral reef,” Colton says. “I’m also failing to cope.”
But she also recognizes that she and other scientists are privileged. They care about reefs, but they’re not like the 450 million people around the world who rely on reefs for tourism revenue, food from fish, and protection against storms. For them, the losses are a daily reality. The last time Bette Willis, from James Cook University, went out to the Great Barrier Reef, the woman who ran her boat “would alternate between rants and depression,” she recalls. “She’s out there several times a week. She knows each coral. She could see her whole livelihood go down the drain.”
Everyone I spoke to talked about becoming very good at compartmentalizing—at acknowledging the scale of the tragedy, but also putting it aside to focus on their work. “I don’t find it productive to be angry or depressed all the time. It’s corrosive, and it isn’t going to solve the problem,” says Knowlton. She is perhaps the poster child for ocean optimism, having created a movement called ... Ocean Optimism. “I started that because there was no way to get people engaged in doing something about these problems, if they didn’t realize that there was something they could do.”
There is reason to hope, she says. Large protected areas have been established around coral reefs, which will protect them from overfishing and pollution, and make them more resilient. That won’t ward off bleaching events, which will continue as long as the climate keeps changing. But Knowlton thinks that the rise of electric cars and the increasingly competitive costs of renewable energy will reduce the pace of global warming, even if the current administration refuses to enact policies to mitigate it.
Willis also notes that corals have been around for millions of years, and are less fragile and more adaptive than people give them credit for. They might be able to swap the algae in their tissues for more heat-resistant varieties, or alter the composition of microbes on their surface to improve their health. Willis’s coping mechanism is to believe in the corals’ coping mechanisms. “It’s almost a faith,” she says.
Ruth Gates, from the Hawaii Institute of Marine Biology, finds solace in action. She remembers what her therapist told her several years ago, for reasons unrelated to corals: You can’t really control what happens around you; you can only control your response. “That was a profound statement,” she tells me. “My response to the gloom and doom is to ask what we can do about it.” She is now trying to breed “super-corals” that can better withstand a warming world, either because they’ve partnered with heat-tolerant algae, or been conditioned from an early age to take the heat.
Whatever the eventual strategy, Gates at least wants to try something—and she fears that the cautious inertia of academic science will stop her peers from pursuing interventionist approaches. “In the last five years, I’ve come to terms with the fact that we’ll have to do something to help reefs get through 2050,” she says. “I find it worrying that people think we have time to plan, and the research is telling us that we have to act.”
Phil Dustan agrees. When he started studying corals as a young scientist, he was interested in questions of basic ecology, like how the reef community responds to light. But now, “we really don’t have the time to be interested in how light structures the community because it’s going to be gone,” he says. Instead, he has shifted his attention to working with communities in Bali, teaching them to care for their reefs and helping them to set up their own protected areas.
This sense of shared purpose is perhaps the greatest vaccine against looming despair. Every four years, coral-reef researchers gather for the International Coral-Reef Symposium. The latest meeting took place last June. It was a rough five days, full of talk of decline and death. But “it was the most level playing field we’ve ever had,” says Gates. “There was a greater sense of community than I’ve ever seen—a sense that we’re going to have to bring our skill sets together to solve the problem.”
She was also relieved to see that younger scientists were not, as she once feared, dissuaded from studying ecosystems that could conceivably die out within their lifetimes. “It’s a very weighty thing to take on at the start of one’s career, but the young people coming into the field are extraordinarily driven to solve problems,” she says. “That’s very heartening.”


Nobody saw it coming.
The rocky object showed up in telescope images the night of October 19. The Pan-STARRS1 telescope, from its perch atop a Hawaiian volcano, photographed it during its nightly search for near-Earth objects, like comets and asteroids. Rob Weryk, a postdoctoral researcher at the University of Hawaii Institute for Astronomy, was the first to lay eyes on it, as he sorted through the telescope’s latest haul. The object was moving “rapidly” across the night sky. Weryk thought it was probably a typical asteroid, drifting along in the sun’s orbit.
“It was only when I went back and found it [in the data from] the night before that it became obvious it was something else,” he said. “I’d never expected to find something like this.”
Weryk and his colleagues scrambled to secure more telescope time to study this mysterious, fast-moving object. They called in reinforcements in the astronomy community. Initial observations suggested the space rock was a comet. When new data showed the object lacked some important properties of comets, they decided it did in fact have to be an asteroid. But it wasn’t acting like any asteroid they’d ever seen.
When astronomers examined and measured the object’s movements, they were stunned. The object didn’t originate in our solar system. It had come from somewhere else, and had traveled through interstellar space for who knows how long to get here.
Astronomers announced the discovery of the object October 26, calling it A/2017 U1. The University of Hawaii team eventually gave it a permanent name of Hawaiian origin, ‘Oumuamua, “a messenger from afar arriving first.” After weeks of follow-up observations, they have released more information about the finding in a new paper, published Monday in Nature, that confirms ‘Oumuamua is the first known interstellar object in our solar system.
‘Oumuamua is a cigar-shaped, 400-meter-long asteroid, red in color, with a surface similar to comets and organic-rich asteroids found elsewhere in our solar system, according to the astronomers. Little is known about its composition. But its existence is, for now, exciting enough.
Astronomers have long predicted this event could happen. Our solar system, in its adolescence, was a turbulent place. As the planets swirled into shape, some of the bigger ones jostled nearby material, sending some of it flying toward the edge of the solar system and beyond. Some of the rejected material could even make its way to another star. Since planet formation is quite uniform across the universe, astronomers believe ‘Oumuamua is one of these outcasts, tossed out of its home system. By this logic, there are likely pieces of our own solar system coasting somewhere in interstellar space or past another star.
Astronomers only had about two weeks after the discovery to observe ‘Oumuamua before it disappeared from the view of optical telescopes. “Because the object is moving fast, and the light we get from it is reflected sunlight, the faster it moves away from both the sun and the Earth, the faster it fades in brightness,” said Karen Meech, an astronomer at the University of Hawaii Institute for Astronomy and the lead author of the paper. She and her colleagues condensed weeks or months of work into days and raced to apply for observation time at the world’s most powerful telescopes, which is competitive and tightly scheduled. Observatories squeezed them in and other colleagues donated time out of their own projects.
Astronomers found that the properties of ‘Oumuamua are unlike any of the approximately 750,000 asteroids or comets known to humanity. “In our simulations, you can see that this could not have been from our solar system—it’s simply going too fast,” said Davide Farnocchia, a navigation engineer at NASA’s Jet Propulsion Laboratory who was responsible for figuring out ‘Oumuamua’s trajectory.
Its orbit was completely different, too, Meech said. Scientists can figure out the shape of the orbit of objects that move around our sun, a measurement known as eccentricity. The eccentricity of all objects bound to the gravity of the sun falls between 0 and 1. The highest known eccentricity, 1.058, belongs to a comet that was discovered in 1980, but astronomers interpret this, along with other measurements that stray from the norm, as the result of objects getting jostled as they moved past giant planets like Jupiter. The eccentricity of the interstellar visitor is nearly 1.2. The difference looks small on paper, but it’s big enough to confirm that ‘Oumuamua doesn’t play by our rules.
Naming the space rock posed an interesting challenge. Comets are usually named after their discoverers, while asteroids are named only after their orbits have been accurately computed and established. The International Astronomical Union, the organization in charge of naming these objects, didn’t have guidelines for christening an interstellar rock; in its designations, the IAU uses the letter C for comet and A for asteroid, and this thing wasn’t either, not really. “This object is only going by once,” said Paul Chodas, the manager of NASA’s Center For Near-Earth-Object Studies. The IAU eventually came up with a new designation: I, for interstellar.
Ground-based telescopes in Chile and Hawaii have already lost sight of ‘Oumuamua. The Hubble and Spitzer space telescope are observing the rock this week, and may be able to track it until December. ‘Oumuamua is now outside the orbit of Mars. It will pass the orbit of Jupiter next May, then Neptune in 2022, and Pluto in 2024. By 2025, it will coast beyond the outer edge of the Kuiper Belt, a field of icy and rocky objects. It will take many more years for the object to reach the Oort cloud, another region of floating objects, at the edge of the solar system.
The arrival of ‘Oumuamua had ignited the astronomical community, particularly asteroid researchers like Andy Rivkin, a planetary astronomer at Johns Hopkins University. Its departure feels just as abrupt. Rivkin put his own twist on an old refrain to describe how he felt about ‘Oumuamua fading from view. “Don’t be sad that it’s over. Be happy that you saw it,” he said. “Because it is really amazing that we saw it.”


Fox News anchors sometimes remind viewers that: We report. You decide.
The company took an especially broad outlook on what kind of information merited a decision on Monday, when it appeared to question the reality of the Apollo missions and the moon landing.
“You be the judge: Skeptics say picture debunks moon landing,” the network tweeted. The accompanying story cited an anonymous YouTube conspiracy theorist’s video that claimed to show discrepancies in a photo from the Apollo 17 mission. The story does not have a byline. A spokeswoman for the network did not respond to a request for comment about whether it considers the moon landing up for debate.
By the afternoon, the story had also made it on the front page of Google News. Newsweek and The Daily Mail also amplified the conspiracy theory.
The story may seem like a giant leap for Fox in terms of what sources it deems newsworthy. The network—whose reporters have questioned the existence of climate change and advanced sketchily sourced conspiracy theories—has given airtime to many viewpoints that challenge the established evidence in the past. But even so, the fact that NASA went to the moon is a particularly strange story to pick up. The Apollo landing sites are not only a product of American scientific and technological leadership; they’re also an easily confirmable fact. Probes from Japan, China, and India have all seen the landing sites, as have later NASA missions. A preponderance of evidence from the period also testifies to the reality of the landing.
But questioning the veracity of the moon landing in particular is not actually a new exercise for television journalists. In 2001, an hour-long special aired on Fox that questioned whether the moon landings were fake. Fox News has hosted a moon-landing conspiracist at least twice since that year. Not to be outdone, CNN covered Bart Sibrel, the same conspiracy theorist who produced the Fox documentary, in an online story in 2009, as well as NASA’s response to the Fox documentary in 2001.
And The New York Times covered Sibrel as well as other conspiracy theorists in 2009—though it also added that polls suggested only 6 percent of Americans believed the rumors.
One of those Americans—as it happens—is Roger Stone, a confidante of President Trump’s who once insisted (or perhaps joked) that the moon landing was fake.
Likely the Fox News tweet achieved such stardom because it darkened journalists’ timelines on the waning hours of a preholiday Monday. It seemed to confirm that Fox occasionally caters to the fringe. But the same network also amplified a poorly sourced conspiracy theory about Seth Rich, a murdered Democratic staffer, for weeks, before suddenly abandoning the story. Other journalists from the network have misreported stories about climate change and asked how global warming can be real if it also sometimes snows. Which coverage is more harmful? We report, you decide.


After a nearly decade-long fight over its construction—which grew to include three states, two provinces, several indigenous tribes, tens of thousands of activists, and two U.S. presidents—the Keystone XL pipeline seemed set to clear its final major hurdle on Monday morning.
By a vote of 3-2, the Nebraska Public Service Commission voted to allow the pipeline to pass through the state. The commission’s vote was the last significant regulatory approval that Keystone XL required before construction could begin. Montana, South Dakota, and the U.S. federal government already okayed the 1,100-mile-long project this year.
But—in a twist—the Nebraska commission’s vote might have merely opened a new chapter in the saga. While the commissioners approved Keystone XL, they also ordered that the pipeline take an alternate route through the state. The new route—which adds a 63-mile detour and parallels a preexisting pipeline—increases the cost and legal difficulties of an already expensive and delayed project.
The commissioners said they were forcing the new route because Keystone XL must “take advantage of any opportunity” to run along the preexisting pipeline corridor.
TransCanada, the pipeline’s developer, did not seem to celebrate the approval. Russ Girling, its president and chief executive officer, said in a statement that the company was “assessing how the decision would impact the cost and schedule of the project.” He also assured investors that the company was pursuing other improvements.
And as the day went on, environmentalists and indigenous groups opposed to Keystone XL sounded upbeat about the possibility of postponing its construction further.
“It’s complicated—indeed, wack—but the lawyers are sounding increasingly cheerful,” tweeted Bill McKibben, a writer and activist who first galvanized opposition to the pipeline. “Lots of room to fight.”
“By pushing Keystone XL onto a new route, the commission all but guaranteed more delays and hurdles for TransCanada to work through. We’ll be there with our allies pushing back on them every step of the way,” said May Boeve, the director of 350.org, a climate-activism organization. She also said that activists should prepare to peacefully assemble in the pipeline’s path.
Sarah Krakoff, a professor of natural-resources law at the University of Colorado, agreed that pipeline’s new route would probably complicate its completion. “I think at a minimum that this slows things down a bit compared to a straight-up approval. Given the recent massive spill in South Dakota, this might buy time to galvanize opposition,” she told me in an email.
Monday wasn’t the first time TransCanada made headlines this month. The Keystone pipeline—the longer and older sister project of the Keystone XL proposal—leaked more than 210,000 gallons of crude oil in South Dakota last week, leaving a spindly black splotch on the farmland and setting off a large cleanup effort. TransCanada said it caught the leak within about 15 minutes. (The Nebraska Public Service Commission is prohibited by law from considering the risk of oil leaks when approving pipelines.)
The pipeline project commands so much attention largely because it has grown into a symbol of the climate fight.
In 2011, McKibben seized on the pipeline, arguing that taking global warming seriously would mean no longer issuing permits for any fossil-fuel infrastructure. A year later, James Hansen, then the director of the NASA Goddard Institute for Space Studies, wrote in The New York Times that fully exploiting oil beneath the Canadian tar sands—as Keystone XL is meant to do—would spell “game over for the climate.”
President Barack Obama first blocked the pipeline in early 2012, saying it posed too much of a risk to the Sand Hills of Nebraska. Analysts said that he would likely approve the pipeline after his reelection. But his State Department slow-balled the approval process, and—in late 2015, a month before the UN negotiations on climate change in Paris—Obama formally blocked its approval.
“America is now a global leader when it comes to taking serious action to fight climate change,” he said at the time. “And frankly, approving this project would have undercut that global leadership.”
President Donald Trump reversed this decision in the first week of his administration, ordering the federal government to reexamine Keystone XL. “The regulatory processes in this country have become a tangled-up mess, and very unfair,” he told members of the press from behind the Resolute Desk. The government approved the pipeline’s passage within months.
Yet much has changed about the energy industry since the Keystone XL was first proposed in the late 2000s. Cheap natural gas has flooded the market, knocking down the price of oil and turning the United States into a net energy exporter. Canada has also improved the energy-export infrastructure around the Tar Sands, reducing the need for a new massive pipeline. While some analysts have wondered whether the Keystone XL might no longer make economic sense, TransCanada says that it has secured enough early reservations for the project to move forward.
The new route could change that. The Omaha World-Herald reported that some landowners along the route may not even know that Keystone XL now affects them. TransCanada will have to notify them and seek to persuade them to give up chunks of their land. And the company still must secure a permit to cross federal waterways in the United States—the same regulatory hurdle that nearly broke the Dakota Access pipeline last year.
And the climate fight itself has altered the energy market, too. Fossil-fuel infrastructure projects like Keystone XL can now unpredictably transform into national news stories: a new challenge for their investors and backers. By 3 p.m., McKibben was feeling triumphant enough to tweet a story from the National Journal.
“Insiders: Obama Will Approve Keystone XL Pipeline This Year,” it crowed, citing a poll of energy experts. The year it was published? 2011.


No matter how many words you can define, your vocabulary isn’t like a dictionary. Your mind stores language not as a list of words, but as a network of categories, properties, and meanings, with stronger connections between related words, like newspaper and magazine, than unrelated ones, like wallet and avalanche.
At six months old, a baby probably doesn’t know what wallet or avalanche means—but even at such a young age, months before children start talking, they do understand some basic nouns, like ball and dog. And a new study suggests that the few words infants know are structured in their minds the same way as an adult’s vocabulary, in a complex web of related concepts. The evidence: When words have similar meanings, babies can get confused. That confusion hints that babies know more about language, at a younger age, than scientists have found before.
In the study, which was published Monday in the Proceedings of the National Academy of Sciences, infants were shown images of two different common objects at a time: a blanket and a dog, a book and a diaper, a stroller and a car, and so on. The babies’ parents would name one of the images, and researchers would then track where the babies looked by reflecting infrared light off their eyes. When the images depicted related words, like nose and mouth, the infants spent more time looking at the wrong picture than when the images depicted unrelated words, like nose and bottle.
The fact that the children were more confused by related images reflects that they somehow understand that the concepts are related, said Elika Bergelson, an assistant professor of psychology and neuroscience at Duke University and the study’s lead author. The same is true of adults, who have shown such a lag on a smaller scale when they do similar experimental tasks. If you were supposed to look at the nose in Bergelson’s experiment, it would take your brain a few milliseconds to make sure that the mouth isn’t what you’re supposed to be looking at, because the entries for nose and mouth are more closely connected in your mental vocabulary. It would take less time to rule out the bottle.
Researchers already knew that toddlers and older children showed the same pattern as adults, but this is the first study to find the effect in six-month-olds. It’s part of a growing body of research on very young infants that suggests they have a deeper understanding of language than previously thought.
Janet Werker, a professor of psychology at the University of British Columbia who was not involved in the study, called the results “just amazing.” If further research shows that babies have a deep understanding of the similarity between the meanings of nose and mouth, and aren’t just used to seeing mouths and noses in the same places, it would reflect that they are driven by a “search for meaning,” she said. That would mean “our whole approach to infant cognition is going to be really turned on its head.”
Outside of the lab, the study’s authors analyzed the infants’ exposure to common objects in their own homes. They found that certain circumstances, like objects being present when parents talk about them with their children, appeared to help the infants look more at the correct objects during the in-lab task. But Bergelson emphasized that parents shouldn’t think her study is a how-to guide for improving their kids’ vocabularies. What they should take away is that “babies are listening, and you should treat them as conversational partners,” she said.
Both Werker and Bergelson acknowledge the study’s shortcomings, especially with respect to the group of babies that participated. For one, it’s relatively small—only 51 children—because recruiting families for both in-lab and in-home studies is, as Bergelson put it, “a huge pain in the butt.” Bergelson was also careful to point out that the babies came mostly from white, middle-class, well-educated families, which means more work needs to be done to figure out how generalizable her results are. Werker would like to see bilingual babies included in future studies.
The more researchers know about the typical development of young infants’ language skills, the earlier they can identify when something might be wrong. Bergelson noted that most parents and doctors can’t say whether a child has a language delay until they start talking, at around one-and-a-half or two years. But “if we can know enough about what a six-month-old’s vocabulary ‘should’ be like,” she said, language delays can be identified earlier. What doctors and families do from there is hard to say, since the origins of language delay can range from poverty to being on the autism spectrum. But no matter the cause, Bergelson said, “the earlier you intervene, the better the outcomes are.”


If you were an elephant, you might be puzzling over human behavior this week. On Monday, the animal-rights attorney Steven Wise filed a writ of habeas corpus on behalf of three privately owned Asian elephants, arguing that the animals are “legal persons” who have a right to bodily liberty and should be free to live in a sanctuary. Then, on Thursday, the U.S. Fish and Wildlife Service announced that the remains of elephants legally hunted in Zimbabwe and Zambia could now be legally imported to the United States as trophies.
This new policy overturned a ban put in place by the Obama administration in 2014. African elephants are considered “threatened” under the U.S. Endangered Species Act, a step below being endangered. The animals’ numbers have plunged from around 10 million 100 years ago to around 400,000 today, largely because of poaching and habitat loss. The Fish and Wildlife Service has not changed the elephants’ status; instead, it now argues that supporting “legal, well-managed hunting programs” will help provide “much-needed conservation dollars to preserve habitats and protect wild herds” in Zimbabwe and Zambia, the agency’s principal deputy director, Greg Sheehan, said in a news release.
But then, to further complicate matters, President Donald Trump tweeted Friday evening that nothing would actually change until he “reviews all conservation facts.”
The idea that killing more elephants will help save the species is counterintuitive, and its line of reasoning is difficult for many conservation organizations to support: Let rich hunters pay hefty sums to shoot elephants, and use the money to help conservation efforts and local communities. Supposedly, the villagers won’t then need to poach elephants to feed their families and pay their kids’ school fees. Still, the International Union for Conservation of Nature, or IUCN, a respected organization that sets the conservation status for all species, supports the notion.
But the evidence that “hunting elephants saves them” is thin. The hunting-safari business employs few people, and the money from fees that trickles down to the villagers is insignificant. A 2009 report from the IUCN revealed that sport hunting in West Africa does not provide significant benefits to the surrounding communities. A more recent report by an Australian economic-analysis firm for Humane Society International found that trophy hunting amounts to less than 2 percent of tourism revenue in eight African countries that permit it.*
And then, there is a larger moral question: How does hunting affect male elephants, especially the “big tuskers” that hunters want, and the overall population?
If elephants are recognized as legal persons, a term the U.S. courts have granted corporations and a New Zealand court gave to a river (elsewhere the term has been extended to chimpanzees, a bear, and the environment), it would be more difficult to hunt them at all—let alone import their body parts. Wise’s lawsuit cites extensive scientific studies that have established elephants’ cognitive abilities, emotional and empathetic natures, complex social lives, lifelong learning, and memory skills. “Taken together, the research makes it clear elephants are autonomous beings who have the capacity to choose how to live their lives as elephants,” he tells me.
One thing elephants would not choose, Wise and elephant researchers agree, is to be hunted. “It doesn’t matter to elephants if they are killed by poachers or trophy hunters,” says Joyce Poole, who has studied African elephants in the wild in Kenya and Mozambique for more than 40 years and is the codirector of ElephantVoices, a conservation organization. “Either way, you’re a killer. And if elephants understand that about you, they change their behavior.”
Elephants aren’t considered game animals in most African countries with substantial populations of these animals. But trophy hunters after large male elephants can seek their prey in South Africa, Namibia, Cameroon, Zambia, Zimbabwe, Tanzania, Gabon, and Mozambique. Kenya banned the sport in 1973, while Tanzania continued to permit legal hunting. That caused problems for the elephants of Kenya’s Amboseli National Park, says Poole, who was studying the large males in the park at the time. The park borders Tanzania, and after the Tanzanian government opened a hunting block on the opposite side, the Amboseli male elephants who wandered across became prized targets.
“It was an awful time,” Poole recalled, “because on one side, the elephants learned to trust tourists—generally white people—in cars. From our studies, we know they can smell the difference between whites and local people. They also distinguish us by our languages. They know people who speak Maa, the language of the local Maasai people, may throw spears at them; those who speak English don’t.” However, the tables were turned on the Tanzanian side of the border. There, white people in cars who drove up close to see an elephant might lean out with a camera—or a rifle.
“The elephants didn’t run because they didn’t expect to be shot,” Poole said. Two of the large males she was studying were lost this way to trophy hunters. She and others protested to the Tanzanian government, and these particular hunting blocks were eventually closed.
Poole does not know how the loss of these big males, who’d fathered many calves, affected the other elephants. Female elephants, though, do mourn family members who die, and are especially troubled when the matriarch, their leader, passes. In 2003, for instance, researchers in Kenya’s Samburu National Reserve watched as Eleanor, an elephant family’s matriarch, died from natural causes. When Eleanor fell heavily to the ground, Grace, a matriarch from another family, used her tusks to lift her friend and helped her to her feet. Despite Grace’s efforts, Eleanor died that night. She had a tiny, six-month-old calf who never left her side. In a photograph, the calf stands like a small sentinel beside her mother’s body, while the rest of the family bunches together, grieving.
Researchers have rarely seen similar moments among male elephants, who as adults, live away from the female herds they grew up in, and return only to mate. That behavior led to a “myth that males are far less social than females,” said George Wittemyer, a conservation biologist at Colorado State University in Fort Collins who has studied elephants in Kenya for more than 20 years. His new research contradicts this notion. “Actually, the males are always in groups and have preferences for certain companions. They’re not the loners they’ve been made out to be,” he said.
“The death of a bull will cause less disruption than the death of a family member,” said Iain Douglas-Hamilton, a zoologist who founded the organization Save the Elephants. “If a bull is shot while associating with a family the others will normally run away.” But he noted: “Bulls will defend or help each other sometimes, when one is down.”
From a population standpoint, “older male elephants are very important to the health and genetic vitality of a population,” said Cynthia Moss, who has led the Amboseli Elephant Research Project in Kenya since 1972. While hunters in the past have used the belief that older males are reproductively senile as an argument for killing them for their ivory, research has revealed that they are in fact an elephant population’s primary breeders. “By living to an older age, [older males show that] they have the traits for longevity and good health to pass on to their offspring,” Moss said. “Killing these males compromises the next generation of the population.”
It’s not clear if the Fish and Wildlife Service will consider how trophy hunting affects individual elephants or their families. The agency didn’t comment on Trump’s tweet when contacted, but later issued a public statement confirming that permits would be put on hold. “President Trump and I have talked and both believe that conservation and healthy herds are critical,” Interior Secretary Ryan Zinke said in the statement.
Wise believes that the emotional and psychological suffering the elephants endure from this sport is obvious. “One day it will be seen for the moral outrage that it is,” he said.
Before Trump’s tweet, the Fish and Wildlife Service had intended to begin issuing permits for importing elephant trophies on Friday. The new policy would apply to elephants hunted in Zimbabwe between January 21, 2016, and December 31, 2018, as well as elephants hunted in Zambia from 2016 to 2018. Regardless of how hunting affects elephants, if the policy goes through, some hunters will have trophies waiting for them in those countries.
* This article originally misstated the portion of tourism revenue contributed by trophy hunting. We regret the error.


In the early 1880s, the French astronomer Étienne Léopold Trouvelot published a dreamy illustration of Jupiter based on his telescope observations. Back then, the gas giant looked, through telescopes, like a fuzzy, gray marble, a dust particle hanging in the night sky. Trouvelot, who in his life created 7,000 astronomical drawings, sought to add a little more detail to the picture to enhance the planet’s features.
“My intent is ... to represent the celestial phenomena as they appear to the trained eye and to an experienced draughtsman through the great modern telescopes provided with the most delicate appliances,” he wrote at the time. He drew, with distinct lines and colors, Jupiter’s bands of swirling clouds and its trademark blotch, the Great Red Spot. “My aim is to combine ... accuracy in details ... with the natural elegance and delicate outlines peculiar to the objects depicted,” he explained.
More than a century later, the desire to share Jupiter with the public in this way lives on. Since July of last year, NASA’s Juno spacecraft orbiting Jupiter has returned batch after batch of grainy, raw images. When a new set arrives, the images are quickly uploaded to a public website, where a band of space enthusiasts, sprinkled around the world, grab them and get to work. They stitch the images together, make a few color corrections, and start sprucing. Some adjustments to contrast here, a little brightening there. They try to show Jupiter as Trouvelot did—the planet as it is, yes, but also as it is if humanity could get even closer, peer a little deeper, and see the wondrous details of a neighbor in the solar system.
The process has led to a trove of stunning photos of Jupiter, unlike anything other space missions have ever produced. The detail in stunning. Zoomed in, Jupiter’s clouds look like cream swirling in coffee, or like the textured brushstrokes of a Van Gogh. They look like art.
Some of the amateur processors behind these pictures hang out on Unmanned Spaceflight, a no-frills internet forum where users can share and discuss spacecraft imagery. “It’s time to start a new topic for Juno’s Perijove-09,” wrote Gerald Eichstädt in a post on the forum last week. Eichstädt is a mathematician who works in software and lives in Stuttgart, Germany. Juno had recently completed its ninth flyby of Jupiter—a close approach known as a perijove—and Eichstädt was waiting for the pictures to show up online. Juno returns photos about every 53 days, thanks to an elongated orbit that brings the spacecraft toward the planet for a few hours before flinging it back out.
“More power to you Gerald,” wrote back Seán Doran, a designer in London. “Next week is going to be busy and fun!”
Doran wished his fellow image processor luck because translating data from Juno’s camera into something usable is no easy feat. JunoCam, as it’s called, doesn’t take pictures like the camera on a smartphone, or even like the camera on other spacecraft. It photographs in hundreds and hundreds of narrow strips through red, green, and blue filters—all while spinning around about every 30 seconds. These strips, called framelets, have to be arranged and stitched together to create coherent, composite photographs.
While Juno cruised to Jupiter, Eichstädt developed a computer program that automates this assembly, and he’s still tweaking it today. When a fresh batch of pictures becomes available, Eichstädt dumps the raw images into the program and lets it run. When that’s done, he shares the composite images with his fellow processors, and everyone jumps over to Photoshop to tinker with the images, produce their own takes, and create animations.
Their final photos, shared widely on social media, transform Jupiter from an abstract, distant planet into a dynamic world swirling with stormy weather. “You can see the clouds—that’s something we can wrap our minds around,” Doran said. “All of a sudden, the planet becomes real.”
The adjustments make the colors of Jupiter—all kinds of shades of orange and red and blue—pop. Without tinkering, Jupiter would look muted. For example, here’s a true-color version, from Björn Jónsson, a software engineer in Iceland who has also processed images from the Cassini, Galileo, and Voyager missions.
Most processed photos are usually not “true color” images of Jupiter—but that’s not a bad thing.
“I’m not looking for ‘true/natural color’ in my images, because enhanced color can show us a little more,” explains Roman Tkachenko, an amateur astronomer and music producer in Kursk, Russia. Tkachenko assembles the many raw images manually before taking a Photoshop brush to them. “I just want to show people something more than they can see in unprocessed images,” he said.
Doran agrees. “It’s like peeling back a curtain,” he said. “You just want to reveal what’s there. That’s what motivates me—and I try not to upset the scientists too much.”
The people behind the Juno mission see it that way, too. Candy Hansen, a senior scientist at the Planetary Science Institute who leads the JunoCam team, said using artificial hues makes features in Jupiter’s atmosphere stand out. “We don’t turn up our noses at artificial color,” Hansen told me for an earlier story about JunoCam’s photos. “We love artificial color.”
The Juno mission doesn’t have a dedicated staff to process raw images from JunoCam, and usually relies on the group of processors to produce photos—for free—to share with the public and include in press materials. JunoCam wasn’t designed for scientific purposes—its sole mission is, quite literally, to take pretty pictures—but the Juno team has used the images to better understand the meteorology of gas giants, in our solar system and beyond. “Jupiter can be considered as representing a population of gas giants, likely a widespread population of celestial bodies in the observable universe,” Eichstädt said. “Understanding Jupiter means understanding non-negligible portions of our universe.”
The first rule of Juno club is that image processors be up-front about the changes they make, said Emily Lakdawalla, an editor and scientist at The Planetary Society and one of the administrators of Unmanned Spaceflight. “You need to be very clear about what you’ve done in order to create the image so you aren’t saying it’s a true representation.”
The Juno processors seem like a supportive and collaborative bunch, even though they’ve never met. There’s a hint of competition, but that’s to be expected in a community of artists, said Jason Major, a graphic designer in Rhode Island. “It’s a very friendly competition—who finds what interesting feature, who can work the quickest, who can get the best result.”
Major said they’ve been at this for enough time that it’s possible to guess which processor produced a particular Juno picture. “Everybody has their own eye, their own aesthetic sense as far as what looks good, and that’s what makes it interesting,” he said. “If you were to send 10 photographers out into the world and have them come back a day later and show you their results, you’re going to be seeing very, very different things.”
Take, for example, processed photos of the Great Red Spot, a storm that has been raging perhaps for centuries. Juno came within 5,600 miles of the spot in July, closer than any spacecraft before it. The raw images from that flyby were some of the most exciting shots the Juno processors had seen, and they raced to process and post them online. The depictions, each of them certainly dazzling, varied. The color of Major’s Great Red Spot turned out a soft orange, while Eichstädt’s was a fierce red.
The image processors have a few more weeks until the next perijove in December. The flyby will produce, again, hundreds and hundreds of strips ready for stitching, but they won’t look the same as last time. In fact, no photo of Jupiter, raw or processed, can ever look the same. The planet’s storms are always brewing, always shifting. A gorgeous filigree of clouds captured during one flyby may be gone in the next.
“Jupiter's clouds are constantly changing,” Jonsson said. “Each day it looks in some way at least slightly different from what it has ever done before.”


One afternoon in May 2008, a graduate student named Pat Kramer was in northwestern Pennsylvania catching purple martins. The bird, a large swallow that nests in artificial birdhouses across North America, is a well-studied species. But one particular purple martin Kramer and some fellow researchers from York University caught was about to revolutionize ornithology.
Kramer let an exclamation mark creep into his otherwise staid field notebook when he found it: “At 2:45p.m., Evelina captured Yellow 2551 in WH-43!”
Yellow 2551, the identification code assigned to this martin, was wearing a geolocator, a small device that uses a light sensor to calculate latitude and longitude and track a bird’s movement over time. The geolocator had traced this female martin’s migratory journey to Brazil and back via the Yucatan Peninsula. In doing so, it provided the first data on what had been a massive blind spot in the scientific understanding of the otherwise familiar purple martin: Where, specifically, does the bird go during migration? And what route does it take?
In the decade since Yellow 2551’s pioneering journey, scientists have used geolocators and a variety of newer technologies to gain an increasingly sophisticated understanding of how migratory songbirds move across the globe. As a result, a much more nuanced picture has emerged of how conditions on wintering grounds and along migration corridors affect birds’ survival. And very soon, with the deployment of some cutting-edge gadgetry on the International Space Station, ornithologists will finally be able to delve into the most disturbing mystery of all: why half of the migratory songbird species in North America are disappearing at alarming rates.
* * *
As so often happens, all this began with a coincidence. At a conference in Mexico, Bridget Stutchbury, an ornithologist at York University, stumbled across a poster on geolocators, which the British Antarctic Survey first developed to study seabirds. She realized that at 1.5 grams, the geolocators were small enough to put on very large songbirds like the purple martin. The following year, Stutchbury and her students deployed 20 of them, strapped to the birds with a backpack-style harness. Though they recovered just two from this initial batch—geolocators are “archival,” meaning they don’t transmit data remotely—they revealed an immediate surprise: Yellow 2551 had flown north in the spring much quicker than expected, covering the thousands of miles from Brazil to Pennsylvania in just two weeks.
“It was the first time anyone had been able to track songbirds from start to finish in their annual migration,” recalls Stutchbury. “We know conceptually that they do it, but to see it on the map ... it’s like, ‘Yes, the bird did this. Here’s the proof.’”
The research made a splash after it came out in Science in 2009, and soon the journals were regularly publishing geolocator studies of migrating songbirds. Through September 2017, a total of 121 such papers have been published, according to a forthcoming review study by Emily McKinnon, a postdoctoral fellow at the University of Windsor. Many of them tell remarkable stories. After recovering geolocators from two Connecticut warblers in Manitoba in 2016, for instance, McKinnon was astonished to discover that the tiny birds’ journey south to Bolivia the previous fall had included a nonstop, two-day haul over the Atlantic Ocean.
“One of the things that really continues to be amazing is that these small songbirds are doing things that we did not think that they are capable of,” she says.
This geolocator science is filling in fundamental gaps in the natural histories of entire species. Stutchbury eventually tracked about 400 purple martins with geolocators, and showed that the species’ core wintering range is in the Brazilian Amazon, rather than more densely populated areas of the country much further south and east, as scientists had previously thought.
In 2017, understanding of purple-martin migration took another leap forward thanks to archival GPS tags, which log birds’ locations down to just 10 meters, as opposed to geolocators’ several-hundred-kilometer margin of error. The research, led by Kevin Fraser at the University of Manitoba, showed that purple martins spend significant amounts of time on low-lying river sandbars in the Amazon. At this level of precision, specific conservation implications begin to emerge. Protecting this sandbar habitat, for example, could be key to maximizing purple martins’ winter survival.
Another technology out there is called Motus, which uses radio telemetry and is non-archival: Special towers simply record the presence of Motus-tagged birds that pass within range (though that range is small and there aren’t many towers out there). There are now satellite tracking tags small enough to use on large songbirds, although weight still prevents them from being widely applied in songbird research. And laboratory techniques like stable-isotope analysis of bird feathers also make valuable contributions by showing, for example, where birds pause during the year to molt new feathers.
The Space Station Is Becoming a Spy Satellite For Wildlife
Together, it all furthers the increasingly nuanced understanding of songbirds’ full annual cycles from breeding grounds to winter hideouts and back again, and the intricate cause-and-effect relationships between them that are only just now becoming clear. “We’re basically rewriting the textbooks,” says Pete Marra, head of the Smithsonian Institute’s Migratory Bird Center.
* * *
One final frontier remains, however: Where and how do birds die? Recovering data from archival tracking devices depends on birds surviving migration—and researchers catching them again afterward. That means that the many, many migratory songbirds that don’t survive their journeys simply disappear into oblivion, telling no tales about the circumstances of their demise. “If we can’t figure out where they die, then we can’t figure out why they’re dying, and we can’t then implement conservation strategies to stop those declines,” says Marra.
That’s what could make the ICARUS initiative—short for “International Cooperation for Animal Research Using Space”—the next ornithological game changer. Scheduled to come online next year, after Russian cosmonauts install a new antenna on the International Space Station, ICARUS will remotely track tagged birds’ movements with such precision that it will be able to tell when they stop breathing, says Martin Wikelski, the project leader at the Max Planck Institute for Ornithology.
When ICARUS launches, the lightest tags will weigh about 3.5 grams—still too heavy for most songbirds—and they’ll only communicate with the orbiting antenna once a day. But Wikelski predicts a one-gram tag will be available in two to three years, and plans are in the works to put more antennae in orbit, improving ICARUS’s coverage and allowing more frequent data transmissions. The idea is to allow scientists to follow individual birds across the globe while keeping tabs on an enormous amount of data—speed, altitude, temperature, heading, acceleration, and so forth—much like airplane passengers now track their flights on the seat-back display. As tags get smaller and satellite coverage improves, the birds they track can be smaller and the data they receive gets closer and closer to real time.
This means that any interested party should soon be able to follow a dot on a computer screen representing an individual purple martin en route from Pennsylvania to some tiny speck of a sandbar in the Amazon. It is, obviously, an exciting prospect for the ornithologists who study this stuff, as well as for the wider bird-watching public. Wikelski emphasizes the educational angle: Imagine an elementary-school science unit built around a single purple martin’s journey to Brazil and back again.
Perhaps that purple martin makes that journey just fine. But then again, maybe the bird fatigues and plunges, Icarus-like, into the sea. Or maybe it goes winging gracefully into an office tower’s plate-glass windows. Maybe some other fate befalls it. That might upset those elementary students, but, with ICARUS beaming back all the gruesome details, this dead bird would no longer be another scientifically useless tick mark in the mortality column. Instead, it would do its part in revolutionizing scientists’ understanding of songbird migration yet again, by showing exactly what happens to the many migratory songbirds who don’t make it back home—and by pointing to ways to keep them migrating in the future.


MIAMI—When Hurricane Irma sprinted toward Miami–Dade County, Jeff Ransom couldn’t sleep. He wasn’t just worried about gusts shattering windows, or sheets of rain drowning highways—that’s far from unusual near his home in Broward County, where extreme weather verges on routine, and patches of US-1 are regularly submerged.
Ransom, the county archaeologist, was preoccupied with an oak tree and its 350-year-old roots. If the tree capsized with enough intensity, he worried, the flailing roots could dislodge human remains.
On a blazing blue morning in early November, weeks after the storm, we trek to the site of the Tequesta Native American burial mound that kept Ransom awake.
“All night long, I was just thinking about that oak tree flipping over,” he says. “The big roots are growing right into the burial mound. That would’ve just blown human bone everywhere.”
Irma’s winds shaved canopies off the trees at the Deering Estate, a historic homestead that contains the burial mound and other fossil sites and is managed by the Miami-Dade County Department of Parks, Recreation and Open Space. Under those bald branches, growth was rapid as vines and chutes—nourished by seaweed deposits—scrambled for sunlight. The result has been a second spring: bright, young leaves, greedy for purchase among the gumbo-limbo and strangler figs. Ransom knocks a path for us with a machete, which he carries slung in a holster. Two thwacks splinter the Brazilian pepper branches—but that’s only because the machete is dull, he tells me. Usually, a single smack is enough to slice straight through, like butter.
Ransom is 52, with a GI Joe jawbone and black aviator sunglasses. At one point, these vanish into the carpet of leaf litter, gone shaggier since the storm, and Ransom spends a few minutes poking around for them beneath the slashed fronds before remembering that he has a nearly identical backup pair.
The burial ground was—is—fine. The oak’s trunk is sturdy and thick; the roots are sunk deep into the soil. We sit for a moment on benches nearby, guzzling water in the shade while Ransom uses his machete’s blunted edge to scrape burrs off his pants and shoes.
The storm didn’t bear down on the city with all its might: In general, southeast Florida was spared the breadth of damage that forecasters had conjured. A half-mile of mangroves buffered the Cutler Midden, another archaeological site on the Deering Estate, against damage wrought by crashing waves. Ancient shell tools and pottery fragments survived intact.
Irma could have bitten harder. But in isolated pockets, the storm was ravenous. We pass fragments of a historic boardwalk, which the archaeologists had laboriously documented and annotated. The structure “had been chunked up” in the storm, explains Mallory Fenn, the public-archaeology coordinator at the southeast/southwest Florida branch of the Florida Public-Archaeology Network (FPAN). The network is a project of the University of West Florida; the southeast/southwest division operates out of Florida Atlantic University.
Fenn’s earrings are made from gator teeth, and the boardwalk looks masticated and spit out, its component parts hardly visible. An orange-and-white barrier marches across the crumpled walkway, as if it wasn’t patently clear that there’s trouble ahead.
* * *
Before I fly down to Miami to trail her and Ransom through the swamp, Sara Ayers-Rigsby sends me a packing list. Ayers-Rigsby is the southeast/southwest regional director of FPAN, and the trunk of her car is stocked with supplies, from bug netting to single-serving bags of pretzels. She’ll have ample bug spray and sunscreen to share, she writes, but I’ll want to wear long sleeves, long pants, and the most waterproof boots I’ve got. We’ll be wading into the height of the king tides; the water might rise up to our knees. Heat and mugginess can have a scrambling effect. Ayers-Rigsby later describes it as “brain-meltingly hot.”
“The weather in South Florida is inhospitable,” she warns.
Writ broadly, that’s precisely the problem. Numerous projections forecast a future of extreme weather and persistent flooding that is incompatible with many elements of life as it’s known on the peninsula. Of all of the U.S. states, Florida is the most vulnerable to sea-level rise, and Miami-Dade is at particular risk.
As the plane drifts toward descent, water is everywhere: in green-blue pools that reach for the horizon, in mud-colored eddies, in staid intercoastals studded with white yachts. From the air, many of these basins look overfull, ready to spill with the slightest top-off.
Sooner or later, the water will swallow the shoreline. When it comes to the magnitude, severity, and timetable, there are shades and gradations of apocalyptic hues. In 2015, a working group comprised of officials from across southeast Florida set out to get on the same page about the threats and to strategize about mitigation efforts. Their projection draws from local tide measurements and is aligned with estimates from the U.S. Army Corps of Engineers and the National Oceanic and Atmospheric Administration. By 2030, they anticipate a sea-level rise of 6 to 10 inches from a 1992 baseline; they predict a rise of up to 26 inches by 2060, and 61 inches by 2100.
Even if the water doesn’t crawl quite that high, damage could still be widespread and devastating. Twenty-five percent of land in Miami-Dade County sits less than three feet above current sea level, according to the World Resources Institute. Ten percent is nearly flush with the sea.
And if water does splash to the maximum level, the results could be cataclysmic. In a recent report, the real-estate company Zillow estimated that, if the sea level were to rise by six feet, 24 percent of Miami’s housing stock would be drenched.
Troublingly for Ransom and Ayers-Rigsby, a sea-level rise of just half that height could destroy as many as 16,095 archaeological sites across the state. As the terrain goes soggier or washes away, how do you protect objects embedded in it?
“You can’t wrap an archaeological site in bubble wrap and put it on a high shelf,” Ayers-Rigsby told me via phone soon before Irma swept past. Some sites can be stabilized or buffered with mangroves or oyster beds, but when it comes to safeguarding them from the pummeling rain or surging waves of a hurricane-strength storm, options are limited. “Other than building a massive construction around it,” Ayers-Rigsby said, “there’s not that much you can do.”
Among officials in Miami-Dade, “there’s no sugarcoating or backtracking” about the threat of climate change, Ransom tells me. Its consequences play out in real time, in flooded streets and waterlogged basements, and voters throw their weight behind mitigation efforts at the polls. After his landslide victory in this month’s elections, the incoming City of Miami Mayor Francis Suarez told the local ABC affiliate that “Miami should be and must be the most resilient city in the world.” That same day, voters approved a bond measure that directed $192 million to pumps, walls, drains, and other projects to keep the city drier. Meanwhile, Ransom, Ayers-Rigsby, and their colleagues work to keep thousands of years of history from being lost to the sea.
* * *
If you wonder what archaeology Florida can boast of, you’d hardly be the first. In a carpool from the airport, I told two Australian businessmen what had brought me to the city. They cocked their heads. Miami, to them, evoked beaches, surgically altered bodies, and hefty Cuban sandwiches. What else was there?
I recount this to Ayers-Rigsby while we sit on a choked concrete freeway, inching from Fort Lauderdale to Biscayne Bay. She groans and slumps her head toward the steering wheel. Ayers-Rigsby, 34, relocated to Florida from the mid-Atlantic, and is now somewhat evangelical about the region’s overlooked merits. Around her neck, she wears a pendant with the state’s silhouette.
For as long as people and creatures have inhabited present-day Florida, they’ve been shedding traces of their lives. Fenn says the flitting snowbirds and rotating crop of transplants can be afflicted with a virulent case of historical amnesia. But the scattered sites testify to millennia before the shores were dotted with high-rises fashioned from glass and steel.
The Cutler Fossil Site is a watering hole into which all manner of Pleistocene beasts toppled. Sandwiched between the limestone layers of the sinkhole, some 16 feet above the current sea level of nearby Biscayne Bay, were bones of dire wolves, mastodons, camels, llamas, saber-toothed tigers, and the American lion. Though the site is protected, the city has sprawled around it in the intervening 10,000 years. Looking down into the ancient pit from the ridge, you can hear the rumble of nearby cars. But the site is hidden and sheltered from the road and the water, protected by its isolation and its elevation.
Other sites sit more uneasily with the present. In the late 1990s, archaeologists discovered a circle of postholes cut into the limestone bedrock at the mouth of the Miami River. Carbon dating of wood fragments helped identify the site as the home of a structure built nearly 2,000 years ago by the Tequesta Indians. “People have been partying in Miami for thousands of years,” Fenn jokes, as she shows me around the site. Archaeologists, Native activists, and a galvanized public sparred with a developer, who had purchased the property as the future site of luxury condos. (A flurry of controversy swirled at the time, when some scholars wondered whether the pattern was, more simply, the drain site for a septic system. Archaeology magazine solicited input from other archaeologists, scholars, and a master septic-tank contractor, the latter of whom summarily dismissed the possibility.)
The Miami Circle was designated a National Historic Landmark in 2009. Today, the site is a grassy expanse shaded by towering condos and hotels that have sprung up around it, overlooking cruise ships and cargo freight lumbering in the distance. It’s a rare green space in a vertiginous corner of the city—and that means it sometimes becomes a place for dogs to lift their legs. A fluffy white dog squats nearby as Fenn describes working on an archaeological site just across the narrow river, where archaeologists unearthed additional Tequesta artifacts in 2014 in the prospective footprint of a massive mixed-use development. These excavations are a trippy mash-up of the ancient and the dizzyingly modern. “When you look down, you think it’s the 1850s, with a sifter and a trowel,” she says. “Then you look up and see skyscrapers, and the Metromover going by.”
During Irma, water breached the walls just below the Miami Circle site. It rushed onto the grass, carrying palm fronds washed in from the river. Fenn, who lives nearby, “ran out pretty much the second we were allowed to be outside” to check in on it. The water soon receded, leaving no apparent damage. This particular spot, loaded with infill, has been shored up to withstand exactly this type of barrage.
Other sites, which lack these preventive measures, are more vulnerable. But studying them can reveal important data about the rising sea—and how long scholars have to hatch a plan.
* * *
Ransom and Ayers-Rigsby pick through a dense thicket and a floor carpeted with spiky bromeliads. They know what they’re looking for—orange-capped rebar that they sank into the bank of the Oleta River—but Irma blew down the trees onto which they’d tied yellow ribbon to help them identify the sites at a distance. Those orange markers have been coated with dirt.
This squishy portion of the riverbank is the site of a prehistoric midden containing traces of shell tools, pottery, and other daily items that would have been used by Native American tribes who lived on the shore.
“If any site is going to erode, it’s going to be this one,” Ransom says, sloshing through the muck.
The midden, or ancient trash heap, is nearly flush with the water level, which makes this site an ideal candidate for tracking inundation and water rise before and after storm events and king tides. By obtaining a baseline measurement and a set of comparisons, the archaeologists can document both accumulation and erosion—noting which events seem to pile more sediment on the top of the site, and which strip it, ultimately threatening to haul the artifacts out to sea.
The notion of using this area as a proxy for fluctuations in the water level dates back decades. In the late 1970s and early ’80s, when he was working as the country archaeologist, Robert Carr found evidence of ancient charcoal buried about two feet below the surface. Since a fire needs to be dry, Carr reasoned that that portion of the site was once above water. At the time, climate change “certainly wasn’t on anybody’s radar” in the archaeology community, he tells me via phone. There was “no particular movement or focus going on.” Carr advocated for using soil inundation, radiocarbon dating, and water levels as firm evidence for past and future variations. His work laid the foundation for what Ransom and Ayers-Rigby are doing.
On a recent afternoon, the mangrove roots are flecked with odd pieces of very modern garbage: foggy glass bottles, a boogie board speckled with barnacles, a black DVD case, a wrinkled bag of Ruffles chips. These aren’t the signs of someone sneaking in to use the forest as a dump, Ayers-Rigsby says—the refuse has been carried in on waves.
She and Ransom slog through the sucking mud, brushing biting ants from their backs and shoulders, to measure the distance from the rebar to the waterline. They jot down the measurements in a yellow notebook, its pages warped by wetness. In some spots, the sediment is piled higher than it was the last time they measured, before Irma blew in. That accumulation suggests that the water level breached a good chunk of the shoreline during the storm, Ransom says.
Carr explains that’s not unequivocally dangerous—there’s not yet sufficient clarity about whether inundation is an impediment to preserving sites in the same way that erosion is. Conceivably, he says, a site “could be better preserved underwater than it is above ground, if sea-level rise is gradual, not a result of pounding waves hitting shoreline and tearing up and removing soils.”
Through her work at FPAN, Ayers-Rigsby has also helped recruit a team of citizen scientists to fan out across the state and conduct regular monitoring of at-risk sites. Inspired by a U.K. program, Scotland’s Coastal Heritage at Risk, the Heritage Monitoring Scouts, a brigade more than 200 people strong, survey publicly accessible sites—not the more sensitive ones, like unmarked burial grounds—and upload their impressions onto a website form. They look out for signs of flooding, erosion, or wave action, or any artifacts that may have been dredged to the surface, and flag any places that need urgent attention.
Sixty-two-year-old volunteer Paula Streeter surveys the shell midden on Calusa Island, a dot of land off the state’s southwest coast once inhabited by Calusa Indians. Streeter has a wide-ranging background—her resume includes “a zillion, million, trillion things,” she tells me via phone. Since retiring from the city clerk’s office, she’s begun assisting archaeologists. “I only started this,” she says via phone. “It was the most amazing thing in my life, and it only happened two years ago.”
Already, the Calusa shoreline is being eaten by waves and wind action, Streeter says. Artifacts are surfacing in the midden, relics of the tribe’s use of shells for tools and weapons—but the average beachgoer might not notice them. “If you’ve been trained, you know that’s an ancient form of a hammer made from a whelk shell or a horse conch,” Streeter says.
The Calusa Island site is only accessible via boat or kayak—“you can’t just zip out there,” Streeter says. Before the recent hurricanes and king tides, the team intended to survey once a month. (The site is also monitored by researchers from the University of Florida.) When toppled trees exposed these artifacts, the team upped the frequency to once per week—and instead of leaving all of the artifacts in situ, the volunteers diagram the original locations and bag some of them, so they’re not tugged out to sea. Heritage Monitoring Scouts use rebar installations to measure the distance from the midden edge to the beach. Even without their precise computations, it’s easy to see the effect of the waves and wind in exposed roots and a dramatically angled ledge of sand.
Some of these sites contain clues to enriching or correcting the historical record. One example is the dwindling island of Egmont Key, off of the Tampa coast.
A few years ago, the U.S. Army Corps of Engineers reached out to the Seminole to ask about the dwindling island. It was eroding heavily—shrunk to 280 acres, half of its size—and they were wondering whether to replenish it with sand. Was the tribe interested in preserving it?
The imminent threat to the landmass was the impetus to uncover the site’s history. With his colleagues, Paul Backhouse, the director of the Ah-Tah-Thi-Ki Museum and tribal historic-preservation officer for the Seminole Tribe of Florida, pursued some research and learned that, during skirmishes with the U.S. Army in the mid-1800s, the island functioned as a detainment site for Seminoles who were caught evading the ships deployed to remove them out west. Judging by contemporary accounts, conditions were grim: There were no sources of fresh water, and the captives were trapped.
The island sits no more than six feet above sea level. Did the tribe want to keep it above the waves? Among the Seminole community, “the overwhelming response was yes,” Backhouse says via phone. Archaeologically, there was much to learn from the site and the 19th-century artifacts that accumulated there—but it could also function as a place of catharsis and education. “Youth can come and remember the struggle their ancestors went through to remain in Florida,” Backhouse says. “This history is a hidden history—it’s not one that’s in any of the textbooks, because it’s an embarrassment to normal American history.”
Egmont Key is on the front lines. With enough elevation or distance from foot traffic, many other sites will be safe for a relatively long time, by virtue of staying dry or hidden. But as the sea creeps higher, choices will have to be made.
* * *
This fall has been an expensive one at the Deering Estate. Hurricane Irma and the October king tides packed a double punch, explains Jennifer Tisthammer, the estate’s director.
During that first king tide, storm surge swamped the service road with ankle-deep water and flooded the back lawn, where many of the estate’s special events take place. Irma’s gales ripped off 80 percent of the tree canopy; 6,000 cubic yards of seaweed washed ashore. Tisthammer’s long-term vision is to raise the back lawn—but in the meantime, the staff looked for prophylactic measures to mitigate the aesthetics and promote drainage. Sod is best, Tisthammer says, but white rock looks better than soggy, brown grass. When the staff spread out truckloads of drain rock and sand, the puddles that had been taking weeks to drain were siphoned off within a few days.
Even if the fully underwater future is far off on the horizon, the king tides offer a regular reminder—and a kind of trial run. On a page devoted to king tides and climate change, the Environmental Protection Agency notes, “Sea-level rise will make today’s king tides become the future’s everyday tides.”
Places like the Deering Estate are already factoring preventative and adaptive strategies into line items on the budget. “You’re gonna have some loss,” Tisthammer says. “Do you put $3 million into something you know will eventually go under, or allocate it differently?”
The kind of data that Ayers-Rigsby and Ransom are collecting can be used to inform broader city planning and budgeting—and this December, Miami-Dade and three surrounding counties are taking archaeological sites into account, adding provisions to the updated action plan from the Southeast Florida Regional Climate-Change Compact. The document isn’t binding, but it encourages local officials to work with historic-preservation specialists to map and rank at-risk sites; to appeal to FEMA, local emergency-management offices, and other agencies for financial resources; and to implement sustainable preservation tactics such as planting mangroves and cordgrass, or “hard armoring” sites with rocks or concrete. These strategies aren’t without drawbacks. “Hard methods may negatively impact sites by the weight and shifting of large rocks, not to mention the cost of acquiring and moving these to remote places,” Ransom says.
The solution is also not as simple as plucking artifacts from the ground and shuttling them to museum collections, where they might be preserved behind plexiglass vitrines. For the Seminole tribe, as for many other indigenous groups, Backhouse says the prevailing philosophy is that items discarded over the centuries should be left in place. He acknowledges that this mantra of noting objects, “working around them, planning around them, and not thinking of those objects as just research vehicles” might “go completely against the grain of what most people think archaeology is.” But Ayers-Rigsby and Ransom likewise consider excavation to be something of a last resort.
In the Seminole culture, Backhouse says, there’s a difference between something being upturned by an earthquake, versus pulled to the surface by human hands. The underlying philosophy is seeking harmony and balance with nature, he says—and “indigenous cultures don’t have an idea that nature’s always nice.”
Last spring, my colleague Linda Poon reported that the vast majority of states lacked any mention of historic resources in their disaster-management plans. Up until this point, that’s been the case in Miami-Dade, says Ayers-Rigsby. “One of the reasons I was so happy we had some language put into the draft of the Climate Action Fund was just to get it on people’s radar,” she adds. “Before, it was not even included at all at any level.” There’s momentum in this direction: Earlier this fall, the city of Annapolis, Maryland, hosted a conference called Keeping History Above Water, dedicated to solutions for historic preservation and cultural resources. In August, Backhouse and the Seminole tribe participated in the Tidally United Summit, cosponsored with FPAN and the Florida International University Global Indigenous Forum, which focused on the relationship between climate science and historic resources.
Meanwhile, Ayers-Rigsby is sensitive to the emergent, unfolding toll that storms and flooding can wreak on people and property. “You have to put the human aspect in the present first,” she says. “You have to prioritize people’s safety and people’s livelihoods. Archaeology and historic resources are obviously necessarily secondary to that, but they should still be discussed.”
It’s painful enough to put a price tag on property—homes, cars, neighborhoods—that we will lose in the reckoning with the waves. And it can be an uphill battle to nudge residents and officials toward the level of abstraction required to dwell in the realm of forecasts and best guesses. “A risk in the future feels a lot less scary than a risk that’s presented right now,” the risk-perception expert David Ropeik told my colleague Laura Bliss in 2015. Even in Florida, where volatile weather is undeniable, it requires a few mental acrobatics to tumble toward an understanding of the sites that are at stake—sometimes literally below the surface.
But if the goal of archaeology is to preserve and interpret the past for the future, there’s plenty of work to be done—careful and quick, down in the muck and in legislative offices—before traces of that past slip away. In those strata are testaments to lives lived, forgotten, and remembered over the course of millennia: a record of what it has meant to be human.
No matter what they do, Ayers-Rigsby says, the time capsule will be incomplete. “Some things will be lost forever.”
This post appears courtesy of CityLab.


The Keystone pipeline was temporarily shut down on Thursday, after leaking about 210,000 gallons of oil into Marshall County, South Dakota*, during an early-morning spill.
TransCanada, the company which operates the pipeline, said it noticed a loss of pressure in Keystone at about 5:45 a.m. According to a company statement, workers had “completely isolated” the section and “activated emergency procedures” within 15 minutes.
Brian Walsh, a state environmental scientist, told the local station KSFY that TransCanada informed the South Dakota Department of Environment and Natural Resources about the spill by 10:30 a.m.
TransCanada estimates that the pipeline leaked about 5,000 barrels of oil at the site, Walsh said. A barrel holds 42 U.S. gallons of crude oil.
The Keystone pipeline system is nearly 3,000 miles long and links oil fields in Alberta, Canada, to the large crude-trading hubs in Patoka, Illinois, and Cushing, Oklahoma. It was completed in 2011. The entirety of its northern span—which travels through North Dakota, South Dakota, Nebraska, Kansas, Oklahoma, Missouri, and Illinois—would stay closed until the leak was fixed, the company said.
TransCanada said it was still operating the pipeline’s southern span, which connects Oklahoma to export terminals along the Gulf Coast.
The pipeline’s better-known sister project—the Keystone XL pipeline—was proposed in 2008 as a shortcut and enlargement of the Keystone pipeline.
In 2011, climate activists seized upon the Keystone XL pipeline, warning that its completion would allow the exploitation of much of Alberta’s tar sands and lock in too much future carbon pollution. James Hansen, then the director of the NASA Goddard Institute for Space Studies, warned in The New York Times that exporting oil from the Albertan tar sands would mean “game over for the climate.”
In 2015, President Barack Obama blocked the pipeline as part of his administration’s preparation for the UN climate-change talks in Paris. But less than a week after his inauguration, President Donald Trump ordered that decision reversed.
TransCanada faces one final approval—from the Nebraska Public Service Commission—before it can finish building Keystone XL. In a fluke of timing, that all-important meeting will take place this Monday, November 20.
TransCanada has already completed two spans of new pipeline around the trading hub in Oklahoma. Those projects make up about 40 percent of the Keystone XL project, and oil is already flowing through them. But even if the company wins approval next week, some analysts argue that the falling price of oil has made Keystone XL uneconomical.
The long-term environmental costs of an oil spill can vary greatly by location and the length of time the spill goes unnoticed. In 2006, BP spilled 267,000 gallons of crude oil in Prudhoe Bay, on Alaska’s northern coast. Though that is about the same amount that leaked from Keystone on Thursday, it flowed from the BP pipeline over at least five days. BP ultimately paid more than $100 million to the federal government, the state of Alaska, and Alaskans for damages related to the incident.

* This article originally misstated that the spill occurred in North Dakota. We regret the error.


On September 1, 1914, an old, trembling passenger pigeon named Martha died at Cincinnati Zoo. With her demise, her entire species slid into extinction. But in many ways, the species was already gone, for a solitary passenger pigeon is almost not a passenger pigeon at all. This is an animal that existed in gestalt. Its essence was in the flock.
Passenger pigeons were once the most abundant bird in North America, and quite possibly the world. At their peak, there were a few billion of them, traversing the continent in gargantuan, nomadic flocks that would blacken the sky for hours as they passed overhead. Simon Pokagon, a Potawatomi author and leader, described them as “the grandest waterfall of America” and their sound as that of “distant thunder” or “an army of horses laden with sleigh bells.”
And then, people started shooting them. They poisoned them, netted them, gassed them, hit them with sticks. In a matter of decades, the continent’s most common bird has been completely wiped out, down to the last individual. “It’s always astounded me how something could have that large a population and entirely disappear,” says Beth Shapiro from the University of California, Santa Cruz. “Why didn’t tiny populations survive somewhere in refugia? I mean, we are pretty good at murdering things, but how did we kill every one of them?”
These questions have been debated for decades. But Shapiro and her colleagues Gemma Murray and André Soares have found some new twists to the old answers by collecting bits of skin from around 200 passenger pigeons, whose century-old, taxidermied bodies sit in museums around the world. Using these samples, they sequenced the full genomes of four individuals, and compared them to the genome of the band-tailed pigeon—a close relative that still exists but lives in considerably smaller flocks.
At first, nothing jumped out. On average, the passenger pigeon’s genome looked to be extremely diverse—two to three times more so than that of any other bird that had been sequenced thus far. That made sense, given how many of them there were.
But averages are deceptive. DNA is packaged in chromosomes, and the team found that the genetic diversity at the ends of these chromosomes was exceptionally high, while the diversity in the middle was exceptionally low. The band-tailed pigeon doesn’t share the same pattern; its genome has roughly the same level of diversity throughout. Indeed, Shapiro had never seen anything like this before. This pattern—and the evolutionary forces that produced it—have important implications for understanding both why the passenger pigeon died out, and whether it'll be possible to bring it back.
Here’s why the pattern exists. When animals reproduce, their chromosomes mix and mingle, shuffling their genes into new groups. This process, known as recombination, breaks up blocks of genes, allowing natural selection to weed out the worst mutations and keep only the best ones. But in birds, recombination happens more often at the ends of chromosomes than in the middle.
Imagine that you’re going through your wardrobe trying to chuck out any clothes you hate, while keeping only the ones you love. Unfortunately, you find that some miscreant has stitched all the shirts, skirts, and pants together. If you want to keep a particular shirt, you’re forced to keep everything else that goes with it. That’s what happens in the middle of the pigeon’s chromosomes. Recombination is low, so genes stick together in large blocks, making it hard to select for one without getting all the hangers-on. But in your wardrobe, the hats, scarves, socks, and shoes are still free and loose, allowing you to consider each item individually and choose the best ones for your ensemble. Same goes for the ends of the passenger pigeon’s chromosomes: That’s why they’re more diverse than the middle.
It’s commonly assumed that animals with massive populations should be genetically diverse. But “the passenger pigeon’s genome is that of both a low-diversity species and a high-diversity one,” says Ben Novak, who worked on the study. “In either case, it was well-adapted for its preferred lifestyle.”
Indeed, that’s why its genome is so weird. Genomes can evolve either through drift, in which DNA changes randomly, or through natural selection, in which genes become more or less common because they affect their owner’s ability to survive and reproduce. Typically, both forces are important. But the passenger pigeon was so absurdly abundant that natural selection dominated. “There was almost no portion of the passenger pigeon’s genome that was evolving neutrally,” says Shapiro.
This might all sound pretty wonky, but it matters when thinking about why passenger pigeons died.
In 2014, Chih-Ming Hung and colleagues from National Taiwan Normal University used the genomes of passenger pigeons to reconstruct their historical population size. They concluded that the pigeon was a boom-and-bust bird, whose numbers cycled dramatically between incredible highs and stark lows. That created a natural vulnerability, which humans inadvertently exploited by persecuting the pigeons during a bust phase.
But Shapiro says that Hung’s team made a mistake. They used a technique which assumes that genomes are evolving neutrally—and the passenger pigeon’s largely isn’t. (Hung stands by the conclusions of his study.) Instead, it had been crafted by natural selection to an extent that most other species are not. “Passenger pigeons were not a naturally vulnerable species,” as has been repeatedly suggested over the last five decades, says Novak. “It was a superspecies in its natural element.”
So, why did this superspecies die out? Shapiro thinks it’s because the bird specifically evolved to live in mega-flocks, and developed adaptations that became costly when their numbers suddenly shrank at human hands. “Maybe they were simply not adapted to being in a small population, and didn’t have time to recover,” she says. Maybe they hit a tipping point when there were just too few of them to survive, regardless of whether they were being hunted.
That’s a bit of a leap from the data, but it’s an idea that’s worth entertaining, says Erich Jarvis from Rockefeller University, who studies bird genomes. “It makes one think: Just because humans leave a small population behind without killing off the rest of the species, it does not mean that the species will survive. It would be good to see if there are other species like this.” Shapiro agrees, and wants to see if the same patterns exist in the genomes of other animals that live in massive groups—perhaps herring, bristlemouth fish, or red-billed queleas.
Meanwhile, Novak cautions that we don’t know if the pigeon actually did suffer at low densities, or the minimum number that would have been necessary to prevent extinction. Indeed, he says that there’s evidence the passenger pigeon would have fared reasonably well at low numbers, if we had just left them alone. Records show that they were breeding just as efficiently when there were only a few hundred left as when there were billions.
The fact is that “human persecution was relentless right up until the very end,” he says. “The rarer the birds became, harvesting efforts only grew more intense. Whatever maladaptive trade-offs may have existed for the passenger pigeon, their decline was simply too rapid for these trade-offs to show symptoms.”
Novak is part of a group called Revive and Restore, which wants to use modern genetic tools to resurrect the passenger pigeon. These birds were crucial parts of America’s eastern forests; their huge numbers and voracious appetites created a patchwork of destruction and regeneration that many animals and plants actually depend on. Novak’s team wants to bring those cycles back, and they need passenger pigeons to do so. But if the bird can’t survive at low numbers, their project seems doomed to failure unless it simultaneously resurrected a flock of millions.
Not so, says Novak. His team isn’t trying to raise passenger pigeons, Lazarus-style. Instead, they are trying to turn the band-tailed pigeon into something passenger-ish, tweaking its genes so it becomes far more social. If their flocks get big enough, they can step into the role that the passengers once played. And over time, their genomes should evolve into the same pattern—diverse ends, uniform middles. “This isn’t something we will engineer, it is something that will happen due to selection and recombination in nature, just as it happened to passenger pigeons originally,” Novak says.
It’ll be a long process, and he won’t be around to see its conclusion. But he wants to see its beginning.


The first thing that hit me about Zealandia was the noise.
I was a 15-minute drive from the center of Wellington, New Zealand’s capital city, but instead of the honks of horns or the bustle of passersby, all I could hear was birdsong. It came in every flavor—resonant coos, high-pitched cheeps, and alien notes that seemed to come from otherworldly instruments.
Much of New Zealand, including national parks that supposedly epitomize the concept of wilderness, has been so denuded of birds that their melodies feel like a rare gift—a fleeting thing to make note of before it disappears. But Zealandia is a unique 225-hectare urban sanctuary into which many of the nation’s most critically endangered species have been relocated. There, they are thriving—and singing. There, their tunes are not a scarce treasure, but part of the world’s background hum. There, I realized how the nation must have sounded before it was invaded by mammals.
Until the 13th century, the only land mammals in New Zealand were bats. In this furless world, local birds evolved a docile temperament. Many of them, like the iconic kiwi and the giant kakapo parrot, lost their powers of flight. Gentle and grounded, they were easy prey for the rats, dogs, cats, stoats, weasels, and possums that were later introduced by humans. Between them, these predators devour more than 26 million chicks and eggs every year. They have already driven a quarter of the nation’s unique birds to extinction.
Many species now persist only in offshore islands where rats and their ilk have been successfully eradicated, or in small mainland sites like Zealandia where they are encircled by predator-proof fences. The songs in those sanctuaries are echoes of the New Zealand that was.
But perhaps, they also represent the New Zealand that could be.
In recent years, many of the country’s conservationists and residents have rallied behind Predator-Free 2050, an extraordinarily ambitious plan to save the country’s birds by eradicating its invasive predators. Native birds of prey will be unharmed, but Predator-Free 2050’s research strategy, which is released today, spells doom for rats, possums, and stoats (a large weasel). They are to die, every last one of them. No country, anywhere in the world, has managed such a task in an area that big. The largest island ever cleared of rats, Australia’s Macquarie Island, is just 50 square miles in size. New Zealand is 2,000 times bigger. But, the country has committed to fulfilling its ecological moonshot within three decades.
Beginning as a grassroots movement, Predator-Free 2050 has picked up huge public support and official government backing. Former Minister for Conservation Maggie Barry once described the initiative as “the most important conservation project in the history of our country.” If it works, Zealandia’s fence would be irrelevant; the entire nation would be a song-filled sanctuary where kiwis trundle unthreatened and kakapos once again boom through the night.
By coincidence, the rise of the Predator-Free 2050 conceit took place alongside the birth of a tool that could help make it a reality—CRISPR, the revolutionary technique that allows scientists to edit genes with precision and ease. In its raw power, some conservationists see a way of achieving impossible-sounding feats like exterminating an island’s rats by spreading genes through the wild population that make it difficult for the animals to reproduce. Think Children of Men, but for rats. Other scientists, including at least one gene-editing pioneer, see the potential for ecological catastrophe, beginning in an island nation with good intentions but eventually enveloping the globe.
In 2007, a retiree named Les Kelly returned to New Zealand after 25 years of working in Australia, and marked his homecoming with a four-month walking tour. And during that time, he realized that something had gone horribly wrong. The birds he remembered from his youth were gone. Learning that introduced pests were responsible, he conceived a bold plan to purge them and championed it through a self-created lobby group called Predator-Free New Zealand. Word got around, and in 2011, a regionally famous physicist named Paul Callaghan mentioned the idea in a rousing speech at Zealandia. “It can be done,” he said. “It’s crazy but it’s ambitious, and I think it might be worth a shot. I think it’s our great challenge.”
Callaghan died a few months later, but those words, delivered by a well-liked celebrity, kept gathering momentum. They certainly lit a fire in James Russell, a young ecologist who was born and raised in New Zealand. “I grew up in suburban Auckland with kakariki—these really rare parakeets that my mother raised,” he tells me. “Now, rats kill most of them, and it breaks my heart.” In 2015, he and three colleagues wrote a paper in which they laid out the benefits of eradicating pests nationwide, and estimated that a 50-year scheme would cost 9 billion NZD ($6 billion).
From there, the idea became a movement. “It stopped being aspirational,” Russell says. The government got on board, setting up a limited company to administer an initial $28 million NZD worth of funds. The public embraced the idea, too. People who had been individually trying to control invasive predators on their own land found common cause behind a unifying theme.
There are, of course, naysayers. Some accuse the initiative of ecological xenophobia, unfairly persecuting creatures that didn’t hail from New Zealand but sure as hell are part of it now. But Russell notes that these displaced predators are still wreaking havoc. “Something is going to die,” he says. “Either a bird is going to be killed by a rat that we brought here, or we’re going to kill the rat. And I would rather humanely kill the rat than have the rat inhumanely kill a bird.”
Other skeptics say that the task is simply too huge. So far, conservationists have successfully eradicated mammals from 100 small islands, but these represent just 10 percent of the offshore area, and just 0.2 percent of the far larger mainland. It’s one thing to cull pests on small, waterlocked pimples of land whose forests are almost entirely owned by the government. It’s quite another to repeat the feat in continuous stretches of land, dotted by cities and private homes.
But Russell, ever the optimist, notes that the daunting ascent ahead shouldn’t distract people from the path already climbed. In 1963, after decades of unsuccessfully trying to save birds from invasive predators, the legendary conservationist Don Merton finally divested a tiny island of its rats, by poisoning them by hand. In later decades, when the Department of Conservation started dropping poisoned bait by helicopter, larger islands became rat-free. Heavily visited islands just off the coast of Auckland were cleared. The mainland is a much bigger challenge but one that could be tackled gradually, by creating large sanctuaries like Zealandia and slowly expanding them. “This is a 2050 aspiration,” says Russell. “It’s not going to be solved in 3 to 5 years.”
“It has become less about technical feasibility but about cost,” he adds. “We could just use the tech today but it would be infinitely expensive. We need new control techniques that would be orders of magnitude cheaper. And that’s when we get into questions about CRISPR.”
In 2014, Kevin Esvelt, a biologist at MIT, drew a Venn diagram that troubles him to this day. In it, he and his colleagues laid out several possible uses for gene drives—a nascent technology for spreading designer genes through groups of wild animals. Typically, a given gene has a 50-50 chance of being passed to the next generation. But gene drives turn that coin toss into a guarantee, allowing traits to zoom through populations in just a few generations. There are a few natural examples, but with CRISPR, scientists can deliberately engineer such drives.
Suppose you have a population of rats, roughly half of which are brown, and the other half white. Now, imagine there is a gene that affects each rat's color. It comes in two forms, one leading to brown fur, and the other leading to white fur. A male with two brown copies mates with a female with two white copies, and all their offspring inherit one of each. Those offspring breed themselves, and the brown and white genes continue cascading through the generations in a 50-50 split. This is the usual story of inheritance. But you can subvert it with CRISPR, by programming the brown gene to cut its counterpart and replace it with another copy of itself. Now, the rats’ children are all brown-furred, as are their grandchildren, and soon the whole population is brown.
Forget fur. The same technique could spread an antimalarial gene through a mosquito population, or drought-resistance through crop plants. The applications are vast, but so are the risks. In theory, gene drives spread so quickly and relentlessly that they could rewrite an entire wild population, and once released, they would be hard to contain. If the concept of modifying the genes of organisms is already distasteful to some, gene drives magnify that distaste across national, continental, and perhaps even global scales.
Esvelt understood that from the beginning. In an early paper discussing gene drives, he and his colleagues discussed the risks, and suggested several safeguards. But they also included a pretty Venn diagram that outlined several possible applications, including using gene drives to control invasive species—like rats. That was exactly the kind of innovation that New Zealand was after. You could spread a gene that messes with the rodent’s fertility, or that biases them toward one sex or the other. Without need for poisons or traps, their population would eventually crash.
Please don’t do it, says Esvelt. “It was profoundly wrong of me to even suggest it, because I badly misled many conservationists who are desperately in need of hope. It was an embarrassing mistake.”
Through mathematical simulations conducted with colleagues at Harvard, he has now shown that gene drives are even more invasive than he expected. Even the weakest CRISPR-based gene drives would thoroughly invade wild populations, if just a few carriers were released. They’re so powerful that Esvelt says they shouldn’t be tested on a small scale. If conservationists tried to eliminate rats on a remote island using gene drives, it would only take a few strongly swimming rodents to spread the drive to the mainland—and beyond. “You cannot simply sequester them and wall them off from the wider world,” Esvelt says. They’ll eventually spread throughout the full range of the species they target. And if that species is the brown rat, you’re talking about the entire planet.
Together with Neil Gemmell from the University of Otago, who is advising Predator-Free 2050, Esvelt has written an opinion piece explicitly asking conservationists to steer clear of standard gene drives. “We want to really drive home—ha ha—that this is a technology that isn’t suitable for the vast majority of potential applications that people imagine for it,” he says. (The only possible exceptions, he says, are eliminating certain diseases like malaria and schistosomiasis, which affect hundreds of millions of lives and have proven hard to control.)
It’s not ready yet, either. Even if gene drives were given a green light today, Gemmell says it would take at least 2 to 3 years to develop carrier animals, another 2 years to test those individuals in a lab, and several years more to set up a small field trial. And these technical hurdles pale in comparison to the political ones. Rats are vermin to many cultures, but they’re also holy to some, and they’re likely to be crucial parts of many ecosystems around the world. Eradicating them is not something that any single nation could do unilaterally. It would have to be a global decision—and that’s unlikely. Consider how much effort it has taken to reach international agreements about climate change—another crisis in which the actions of certain nations have disproportionately reshaped the ecosystems of the entire world. Genetic tools have now become so powerful that they could trigger similar changes, but faster and perhaps more irreversibly.
“In a global society, we can’t act in isolation,” says Gemmell. “Some of these tools we’re thinking about developing will cross international borders. New Zealand is an island nation relatively isolated from everyone else, but what if this was a conversation happening in the United States about eradicating rodents? What if Canadians and Mexicans had a different view? This is something that should be addressed.”
Russell agrees with a precautionary approach but he isn’t ready to dismiss gene drives yet. For a start, he feels that Esvelt’s simulations overestimate the risk that such drives would establish themselves in the wild. Yes, rats are very good at traveling and colonizing new lands, but they’re surprisingly bad at invading places where other rats already exist. “Rats have a strong incumbent advantage,” he says. “You really have to introduce a lot of individuals” for them to successfully invade an already-established population.
Esvelt thinks that people would do exactly that. Gene-drive rats may not be able to swim or stow away in sufficient numbers to occupy new lands, but people could carry them. There is precedent for this: In 1997, farmers illegally smuggled a hemorrhagic virus into New Zealand to control rabbit pests. They could just as easily smuggle gene-drive rats in the other direction, to control the rodents in their own particular corners of the world. “New Zealand has very good biosecurity but it’s mostly focused on stopping things from getting in,” says Gemmell. “I’m not sure we’re that good at stopping things from getting out.”
If gene drives are deployed, it’s not unreasonable to imagine a black market in genetic rodenticide, which is exactly the kind of deliberate malfeasance that Esvelt says scientists rarely anticipate. “We don’t consider everything that will happen when technology gets in touch with reality,” he says.
All of this assumes that genes drives would be used to spread genes that kill or suppress pests outright. Instead, conservationists could use them to spread genes that are tied to particular ecosystems. “Imagine giving all rats in New Zealand a peanut butter allergy, and then we feed them all peanut butter,” Russell says. “Well sure,” Esvelt counters, “but then you’ve just converted all the rats in the world into GMOs without asking other countries.” The same problem remains: How do you keep the modification from spreading beyond New Zealand?
Esvelt is working on a couple of tricks for corralling the awesome power of gene drives. In a basic gene drive, a chosen gene has all the components it needs to spread itself. But you could split those components between several genes that are connected in a daisy chain, so that gene C is driven by gene B, B is driven by A, and A is driven by nothing. If rats with these genes were released into the wild, the A-carriers would initially spread the B and C genes, but would eventually disappear themselves. After they go, B would follow. Ultimately, so would C. These “daisy drives,” as Esvelt calls them, are self-exhausting. They’re designed to run out of steam. If they work, they are tools that countries could justifiably use without involving the entire world.
To be clear, despite the buzz around gene drives in New Zealand’s conservation circles, there are no concrete plans to actually use them. “There is currently no research being conducted in New Zealand to develop gene drives for Predator-Free targets, nor are there any plans for such research in the near future,” says Andrea Byrom, director of New Zealand’s Biological-Heritage National Science Challenge. Indeed, Predator-Free 2050’s research strategy mentions only the most exploratory of steps, such as sequencing the genomes of local rats, talking to international experts like Esvelt, and running mathematical simulations. Genuine research into the drives themselves wouldn’t begin any earlier than 2020, and would depend on “technological hurdles being surmounted, supportive policy, and New Zealand/international appetite to proceed.”
The group has also funded social research looking into how New Zealanders feel about using genetic technologies to control pests. That’s the right order, Byrom says: Work out what people want, and act accordingly. The first results, published this week, showed that 32 percent of the 8,000 people surveyed were comfortable with technologies like gene drives, 18 percent felt that they should never be used, and 50 percent were undecided or wanted strong controls.
Much of this work has been done in consultation with Māori scientists and tribal leaders. But “the conversation happens in pockets, around networks that scientists have,” says Maui Hudson from the University of Waikato, who studies Māori research ethics. That’s good for working out the Māori perspective on gene drives, but not for actually engaging those communities in the debate about the risks. Aroha Te Pareake Mead, a political scientist who has studied indigenous perspectives on biotechnology, agrees that there hasn’t yet been a robust and far-ranging discussion with Māori groups (iwi). “The idea of a predator-free New Zealand is widely endorsed throughout Māoridom,” she says. It fits with the concept of kaitiaki or guardianship—the imperative to protect one’s biological heritage. But the means of achieving that goal are more contentious.
“We’ve had many initiatives over the years that have sought to address environmental concerns, with unintended detrimental consequences,” Mead says. “Māori tend to have a precautionary approach because we’ve already had many cases of wrongdoing for the right reasons. Generally speaking, we are suspicious of any kind of genetic modification.”
Despite those reservations, she enjoyed meeting Esvelt two months ago, when he spoke about daisy drives at a community meeting. “I found him to be refreshing as a scientist,” she says. “He wasn’t defensive and he thought that questioning the risks was essential. That gave the Māori who were present a lot of comfort because we’re used to a very different type of geneticist who comes in, says this is the best thing since sliced bread, and if you question it, you’re ignorant and you don’t know the science. We want to be given a range of tools and to make an informed decision about the best one for the purpose.”
Gene drives are not the only game in town. The people behind Predator-Free 2050 are also working on ways of upgrading tried-and-tested technology. The most commonly used traps, for example, are simple one-use devices that must be manually checked and reset. But some companies have made self-resetting traps that can repeatedly kill dozens of rats with a gas-powered piston to the head, or traps that can spray 100 stoats with toxins before needing to be reset. Others are developing sensors that will tell trappers when their snares have snagged an animal, so they don’t have to laboriously check every one.
These traps are typically baited with food, but food goes off in the field and must be frequently restocked. Ironically, it also becomes less effective in well-protected areas where actual prey are common. But stoats, it turns out, are far more attracted to the scent of ferrets—a fellow species of weasel—than they are to food smells. Scientists are now trying to isolate the chemicals that make Eau de Ferret so enticing, to turn them into a super-lure.
Aerial drops of 1080 poison, which have freed so many islands from predators, will almost certainly be part of any mainland campaign. Its use is controversial: It can harm the playful kea parrot, and the occasional unwary pet dog. But conservationists could deploy poisons more effectively if they had better ways of detecting pests, like footpad sensors that could track a rodent’s footfalls, or cameras whose images are automatically analyzed by artificial intelligence. One team is also trying to develop more specific toxins, by analyzing the genome of possums to find chemicals that will affect them alone.
And Russell believes that for Predator-Free 2050 to succeed, it has to marshal the most effective tool around: human enthusiasm. Thousands of volunteer groups already exist around the country, monitoring for invasive species and setting traps. That kind of fervor has to spread, especially if mammals are to be exiled from cities. Any pockets of resistance or apathy would create strongholds where pests could thrive. “Conservation must be something that happens not just in national parks and the backcountry, but in people’s backyards,” Russell says. “They not only allow it but participate in it.”
Regardless of the technology that Predator-Free 2050 eventually settles on, there’s no question that such measures are needed. Consider the kakapo—New Zealand’s endearing, bumbling, giant, flightless parrot. In the 1960s, people thought it was extinct. Now, after the discovery of a surviving population and three decades of intense work, the population stands at 153.
The adults have been relocated to predator-free islands, but “in terms of large sites that would hold a decent population, we’ve saturated the market,” says Deidre Vercoe, a manager at the Kakapo Recovery program. Her team will have to start releasing the birds into places where stoats and rats are still a threat. If Predator-Free 2050 achieved its goal, they could do so with relaxed smiles rather than gritted teeth. Even if Stewart Island, New Zealand’s third-largest island, could be stripped of predators, “it would be an answer for kakapo for many, many years,” she says.
New Zealand is far from the only country grappling with these issues. Over the last seven centuries, 60 percent of the vertebrates that have disappeared from the planet have disappeared from islands—and in half of those cases, invasive species are the culprits. If Predator-Free 2050 makes the right choices, it can indeed change the world—but not with an unstoppable wave of gene-drive rodents. Instead, it’ll show other nations that islands can be protected, that invasive pests can be eradicated, that vanishing wildlife can be saved—even at scales once thought impossible.
“Even if we don’t get to the finish line, the fact that we ran most of the marathon will be pretty damn impressive,” says Russell.


Updated on November 15 at 12:37 p.m ET
On Monday, 44-year-old Brian Madeux spent three hours hooked up to an IV and made scientific history.
The clear liquid that dripped into his arm set off a chain of events that is supposed to end with the precise insertion of a gene that Madeux has lacked since birth into the DNA of his liver cells. With that, he might be cured of Hunter syndrome, a genetic disorder that causes a range of symptoms including joint stiffness,  breathing problems, and developmental delay. Madeux has had 26 surgeries to deal with it all.
If it works, the change will be permanent. If the gene gets inserted in the wrong place, that will likely be permanent too. Doctors may not know for several months.
The Associated Press broke the news of Madeux’s infusion Wednesday. He is the first patient treated in a clinical trial from Sangamo Therapeutics, which is using microscopic gene-editing tools called zinc-finger nucleases to alter DNA inside the bodies of patients. So invested is Sangamo in this technology that the company is concurrently recruiting patients with hemophilia B and Hurler syndrome for two other clinical trials using zinc-finger nucleases.
Gene therapy, in which a missing gene is inserted into a patient’s DNA, has been around since 1989. What makes Sangamo’s treatment different is precision. Past gene therapies have made use of certain viruses, which insert genes in somewhat random places in the genome—sometimes in an appropriate place, but sometimes in places that can cause cancer, which makes the treatment  risky.
Sangamo’s zinc-finger nucleases are engineered to find a specific stretch of DNA, where a new gene can be safely inserted. “You know exactly where you’re going in the genome. It’s not like using a shotgun hoping you’re hitting a bird. It’s like using a rifle,” says Chester Whitley, a principal investigator on Sangamo’s clinical trial and a pediatrician at the University of Minnesota. Madeux’s infusion contained billions of copies of the gene he is missing as well as zinc-finger nucleases.
Zinc-finger nucleases are also distinct from CRISPR, an even newer gene-editing tool that can also find precise stretches of DNA. CRISPR for gene editing in the body has not reached clinical trials in the U.S. yet.
Sangamo had previously used zinc-finger nucleases to edit immune cells taken from HIV patients. Once those gene-edited cells were injected back into the patient, they were resistant to HIV infection. But now, Sangamo is inserting zinc-finger nucleases directly into the body to do their editing. It’s a new level of risk but also a new level of possible reward.
A Dying Boy Gets a New, Gene-Corrected Skin
Madeux’s treatment will not be able to reverse all of the damage in his body. But it could halt the progression of the disease. It could also eliminate the need for expensive and time-consuming enzyme-replacement therapies, which many Hunter-syndrome patients currently use to manage their disorder.
But what Sangamo really hopes to do in the future is to cure kids. When I spoke to Sangamo’s president Sandy Macrae earlier this year, he told me the first trials have to be in adults because that’s the prudent thing to do. “Once we’re sure the medicine is safe and effective, our intention is to get it as quickly as possible into children,” he said.
So if it is safe and effective—still big ifs—then Madeux’s infusion could kick off a new era for genetic disorders, one where kids never have to suffer their effects in the first place.


There’s a wrinkle in how the United States talks about climate change in 2017, a tension fundamental to the issue’s politics but widely ignored.
On the one hand, Democrats are the party of climate change. Since the 1990s, as public belief in global warming has become strongly polarized, the Democratic Party has emerged as the advocate of more aggressive climate action. The most recent Democratic president made climate policy a centerpiece of his second term, and the party’s national politicians now lament and oppose the undoing of his work. Concern for the climate isn’t just an elite issue, either: Rank-and-file Democrats are more likely to worry about global warming than the median voter.
On the other hand, the Democratic Party does not have a plan to address climate change. This is true at almost every level of the policy-making process: It does not have a consensus bill on the issue waiting in the wings; it does not have a shared vision for what that bill could look like; and it does not have a guiding slogan—like “Medicare for all”—to express how it wants to stop global warming.
Many people in the party know that they want to do something about climate change, but there’s no agreement about what that something may be.
This is not for lack of trying. Democrats have struggled to formulate a post-Obama climate policy because substantive political obstacles stand in their way. They have not yet identified a mechanism that will make a dent in Earth’s costly, irreversible warming while uniting the many factions of their coalition. These problems could keep the party scrambling to face the climate crisis for years to come.
* * *
The roots of this crisis go back to 2009, when Democrats held unified control of the White House and Congress. The end of the last decade was a unique moment in climate politics: Thanks to a string of intense hurricane years, and the unexpected success of Al Gore’s documentary An Inconvenient Truth, public support for addressing climate change through legislation was higher than it had ever been.
Democrats responded with the American Clean Energy and Security Act, widely known as “Waxman-Markey,” after its two sponsors, Congressmen Henry Waxman of California and Edward Markey of Massachusetts. The bill proposed creating a carbon-emissions trading market across the United States. Under its terms, the government would have distributed a number of “right to emit carbon” credits to companies, which they could then have bought and sold to each other. As the years passed, the government would allot fewer credits, forcing the price of emitting carbon to increase, which would—in theory—ultimately decrease the amount of carbon dioxide released into the atmosphere.
Though more than a little technical, a pollution market was a proven idea in U.S. environmental law: George H.W. Bush established a similar “cap-and-trade” system during his presidency to reduce the pollutants that create acid rain.
In June 2009, Waxman-Markey passed the House. But as that summer wore on, the bill’s prospects floundered. By August, the Tea Party rose to command more media attention, and public opinion turned against Democrats. Senate Majority Leader Harry Reid—focused on passing what would become the Affordable Care Act—declined to take the climate bill to the Senate floor. By the middle of the next summer, Waxman-Markey was effectively dead. Only a few years after it opened, the window to pass climate legislation had already shut.
Even in defeat, Waxman-Markey cost the party dearly. More than two dozen congressional Democrats who had supported the cap-and-trade bill lost in the 2010 midterm election. The casualties included Rick Boucher, a 14-term veteran of Congress whose district included much of southwest Virginia’s coal country. Boucher had negotiated concessions for local coal companies into Waxman-Markey, but this could not save his seat. Ten House Democrats, including Boucher, voted for Waxman-Markey and against the Affordable Care Act. Six of them lost their seats in 2010.
Indeed, Democrats seemed to prevail only when they ran against the climate bill. Joe Manchin, then the Democratic governor of West Virginia, won a special election that year to serve in the Senate, but only after he ran an ad that showed him shooting a pile of paper with a rifle. “I sued EPA, and I’ll take dead aim at the cap-and-trade bill,” he said in the commercial, which received wide media coverage.
So Waxman-Markey failed. And then, freed from a unified campaign, American progressives took climate policy in two different directions.
First, the Obama administration pressed ahead with its plans to use the Clean Air Act to limit carbon dioxide. (This power predated his presidency: In 2007, the Supreme Court told the EPA it must consider regulating greenhouse gases under that law.) This push eventually produced the Clean Power Plan in 2015, a set of rules meant to reduce greenhouse-gas emissions from the power sector by 30 percent from their historic peak.
Though it dominated press coverage of Obama’s climate policy, the Clean Power Plan never actually took effect. In February 2016, the Supreme Court blocked it from gaining the force of law. But the administration got many other rules aimed at drawing down carbon pollution on the books by the end of Obama’s term.
Second, a swath of environmentalists abandoned the hope of passing climate legislation and returned to a more grassroots, project-by-project approach. Instead of reducing greenhouse-gas emissions through law, these activists hoped to deprive oil companies of their social license to operate altogether.
Chief among these new groups was 350.org, led by the author Bill McKibben. 350 opposed the Keystone XL pipeline, a mega-infrastructure project that linked the Canadian tar sands to U.S. export terminals, warning its completion would spell “game over for the climate.”  In 2010, the Sierra Club also began to use environmental litigation and public pressure to shut down hundreds of coal-burning power plants as part of its Beyond Coal campaign.
This climate antagonism was one aspect of the left’s general resurgence during this period. Inspired in part by Naomi Klein’s 2014 book This Changes Everything, many activists came to see climate change as one more symptom of a dehumanized and extractive capitalism.
These two sides even had something of a proxy battle during the 2016 Democratic primary. During that race, Hillary Clinton opposed a carbon tax and endorsed the Obama administration’s ongoing regulatory efforts. Sanders endorsed a carbon tax and called for more aggressive investment in climate mitigation. Unlike Clinton’s policies, Sanders would surely have required a Democratic Congress to enshrine his policies.
In some ways, though, the 2016 primary was an imperfect battleground for climate policy. Sanders did not seek an energy economist’s ideal climate policy. He wedded climate-hawk positions—like his opposition to natural-gas fracking—to the 1970s’ classically green opposition to nuclear power. (Even though his proposed carbon tax would have been the greatest boon to the nuclear-power industry in decades, as nuclear plants emit no greenhouse gases.)
And there likely would have been little substantive policy difference between either candidate’s presidency, at least at first. Had Clinton or Sanders won the election, their EPA would have dutifully defended the Clean Power Plan in court. And their administration would have benefited from one more liberal seat on the Supreme Court to help enshrine more aggressive climate protections into law.
But Trump won. And that brings us to the present.
* * *
On October 10, Scott Pruitt formally repealed the Clean Power Plan, the first phase of a planned disembowelment of Obama’s environmental legacy.
Pruitt is not assured to succeed in this effort. Many environmental lawyers argue that his legal arguments are not particularly well-supported. To push his repeal through, Pruitt had to muck with the EPA’s internal cost-benefit calculations. His EPA changed how it estimates the social harms of carbon dioxide, calculating a number more than 50 times smaller than what the Obama administration used; it also changed how it values the threat that air pollution poses to the American public.
It may all be for naught. Some environmentalists argue that the Clean Power Plan already accomplished its main goal, which was sending an anti-coal price signal to utility managers. Michael Bloomberg, a UN special climate envoy and the former New York mayor, has argued that the falling cost of renewable energy and the availability of cheap, bountiful natural gas will help the United States meet its carbon-reduction goals even without the federal policy.
But Pruitt does not need to win a total victory to succeed. The Clean Power Plan repeal will now be litigated in court, with the EPA lobbying on the side of deregulation. The fight will last years. It may even wind up in front of a Supreme Court more conservative than it is now.
No one knows the future, but it’s not hard to sketch possibilities. Pruitt may very well secure a ruling that effectively keeps the Clean Air Act from ever regulating greenhouse gases. Even if he fails, a future Trump EPA may succeed in permanently limiting the law’s regulatory power.
And when a climate-concerned Democrat next enters the White House, whether in 2020, 2024, or 2028, he or she may find that the Obama administration’s main policy implement for fighting carbon emissions has been dulled into uselessness. Attorneys at some future EPA will, at that point, probably improvise some new way to address climate change in regulation. They will have no other choice. But there will likely be a need, too, for new legislation. What will Democrats do?
* * *
Democrats face at least three major problems in trying to formulate a climate policy.
First, the relationship between environmentalists and labor groups has disintegrated since 2009. Once upon a time, unions widely supported Waxman-Markey. The bill funded assistance for workers put out of work during the transition away from fossil fuels and launched “green-job” retraining programs. It also provided tens of billions in funding for “carbon capture and sequestration,” an experimental technology that would possibly have allowed coal plants to keep running. But when it failed, the pan-Democratic consensus fell apart with it.
Neither of the two climate strategies that have succeeded Waxman-Markey offer particular appeal to organized labor. The Clean Power Plan essentially made nonprofits and already-strapped state budgets (in GOP-governed states) responsible for softening the economic blow of carbon regulation. In large part, they declined.
The antagonistic approach to pipelines and other fossil-fuel projects has also alienated labor. Construction-union workers still spend much more time building pipelines than installing renewables. Divorced from a unified legislative campaign—and the promise of federal funding—the new environmental antagonism can seem to run directly counter to worker interests.
This isn’t necessarily the fault of the antagonists alone: Unionization in the renewable industry is also lagging. Renewable-energy companies are often as anti-union as any other California-based tech firm. In October, for instance, Tesla—which swears it is trying to hire more employees as fast as it can—fired 400 to 700 workers in what many analysts understood as a union-busting move. United Auto Workers has filed a complaint with the National Labor-Relations Board.
Green-energy companies don’t only harbor an anti-regulatory, Silicon Valley–style dislike for organized labor. They also see themselves as fighting a desperate battle on price with oil and gas companies. Every cent the union might add to production cost, they argue, is a cent that disadvantages them against fossil fuels.
The green-labor breakdown is the party’s biggest political obstacle. But its second problem is that Democratic voters still don’t care about climate change very much. Like other Americans, most of the party’s electorate experience it as a “low-intensity” issue. Though a majority of Americans in every state believe in climate change, very few people use climate policy to decide whom to vote for. Even Democrats say that a candidate’s proposed climate policy matters less when making a voting decision than his or her proposed policies about jobs, health care, the economy, education, income inequality, and terrorism.
As it happens, Trump is helping solve this problem: Due to his steady siege on Obama-era climate policy, he seems to be motivating rank-and-file Democrats to address the issue. His withdrawal from the Paris Agreement was much more unpopular than the treaty itself. But it remains unclear whether the passage of climate policy motivates its supporters more than it aggravates and alienates the issue’s “anti-constituency” (such as coal and oil workers).
Finally, dealing with climate change through any policy is just hard. Most of the good news in climate change lately has come from the power sector, where emissions have dropped by 18 percent over the last five years. But electricity production only makes up about 29 percent of total U.S. greenhouse-gas emissions. For the United States to decarbonize—which even the newest and most optimistic projections say must proceed at a historically unprecedented pace—it must also tackle carbon pollution from other sectors.
Those other sectors will be tougher to crack. Most of the emissions from the transportation sector (which itself makes up more than a quarter of U.S. greenhouse-gas pollution) come from cars and trucks. In order to draw those emissions down by 2050, consumers will have to opt in to electric vehicles en masse and service stations will have to erect electric chargers across the country. Analysts say that’s unlikely to occur without large public investment.
And there are few ideas about how to tackle the 21 percent of U.S. emissions that come from the industrial and manufacturing sector.
* * *
These questions are more pressing than they seem. Eighteen months from now, Democratic policy advisers will meet to develop their chosen candidate’s primary policies. If Democrats win unified control of Congress and the White House in, say, 2020, history suggests they will get a sliver of time to commit any kind of new policy to statute before public opinion turns against them. During that window, dozens of issues will compete for law makers’ attention.
In that light, consider how Democrats have treated health care over the past 10 months. While there’s no party-wide consensus, small groups of voters around the country have organized around the issue, calling their senators every day for months. More than a third of Democratic senators have supported a bill to introduce a single national health insurer. And Bernie Sanders, the party’s 2020 frontrunner, has debated Trumpcare (in its many forms) on CNN several times.
In sum, many Democrats have coalesced around a single phrase (if not quite a policy to accompany it) and promised to deliver it the next time they’re in power. And even if some Democrats see “Medicare for All” as a base play—as a slogan more likely to be deployed in Vermont than Colorado—it remains a policy-focused promise about future governance. The party has similar promises for other issues, too: On immigration, it can promise the DREAM Act; on LGBT rights, the Equality Act.
I suspect that many voters (including most rank-and-file Democrats) believe that there’s a similar strategy on climate change. They think there’s some bill waiting in the wings that would address the issue. They trust that Democrats have a legislative plan to resolve climate crisis, and that the party only needs to be granted control of Congress to pass it.
But nothing of a similar scale exists, and some of the Senate’s most vocal Democrats on the issue resist formulating one. Sheldon Whitehouse, who delivers a weekly climate-themed haranguing on the Senate floor, recently told Vox’s Jeff Stein that way to politically solve global warming is to convert Republicans to the cause.
There are only two bills that come close to serving as a flagship bill. The first is the 100 by ’50 Act, released in April by Senators Jeff Merkley of Oregon and Bernie Sanders of Vermont. “100 by ’50” is an ambitious economic-planning package that would require 100 percent of American electricity to come from clean or renewable energy by 2050.
The bill’s release was timed to the People’s Climate March in Washington, D.C., and McKibben attended its unveiling. It represents the triumph of the 350.org wing of the environmental movement, blocking future fossil-fuel investment and directing plenty of funding to help historically at-risk and marginalized communities. But the 100 by ’50 Act debuted to a fizzle and Sanders, its more prominent cosponsor, spends little time discussing it publicly.
The only other bill is Merkley’s the Keep It in the Ground Act, which would prohibit new oil and gas leases on federal lands. Though cosponsored by Kirsten Gillibrand and Elizabeth Warren, it receives little attention beyond Merkley’s own press statements.
Labor sources have also told me that both of these bills would face problems if Democrats tried to run with them. By design, the “100 by ’50” Act includes no economy-wide mechanism to phase down carbon emissions, like a carbon tax or cap-and-trade market. (Many of the more leftist environmentalist leaders reject these arrangement as failed technocratic policies.)
Indeed, most Democrats told me that climate change could only make it to the legislative docket through some other kind of bill. The days of a Waxman-Markey climate-only bill are seen to have passed, but it’s possible that climate could be addressed in a tax or jobs bill.
If Republicans succeed in overhauling the U.S. tax code, then Democrats could slide a carbon tax into a progressive rewrite of the system. Aging Reaganite Republicans proposed just such a carbon tax-and-rebate scheme earlier this year. Such a policy would bump up the cost of gas by several dozen cents per gallon, discouraging carbon pollution (in theory)—but it would also send every American family a check for $1,500 four times a year. In the eyes of its supporters, this would reduce carbon emissions while also testing an anti-poverty universal basic income.
Democrats could also institute a carbon tax that funds renewable-energy development, as Vox’s Dave Roberts has proposed and as some national polling supports. Some economists worry that such a mechanism would be less politically durable at the national level.
Or Democrats could choose another route and pass a “green jobs and infrastructure” law that subsidizes renewable-energy construction across the country. They could underwrite the electrification of steel production and impose an import tariff on Chinese steel. They could also compel renewable companies to respect unionization drives—a feasible goal, since the short-term success of the solar and wind industry depends in large part on Democratic victories.
The party could do any of these things. But a glance around the infrastructure of the Democratic establishment reveals that little of this planning work is actually getting done. There is no consensus about whether a carbon tax is a good idea. There is no ideal policy embraced by Democrats in lieu of a carbon price. There is, as far as I could find, no think tank putting a bill together or thinking through legislative language. I could barely find professional Democrats planning how a future offensive on the issue would look.
Meanwhile, as Kate Aronoff writes at The Intercept, climate change has come to dominate headlines in the past few months like never before. Three historic hurricanes have wreaked havoc across the United States, leaving the worst blackout in American history (which, at writing, is ongoing). Wildfires have destroyed neighborhoods across the West. And 2017 is almost certain to be the second-hottest year ever measured.
Against this background, the Trump administration has waged an assault on environmental policy and science with little precedent in U.S. history. In response to this, the Democrats have admonished Trump, have lamented the downfall of Obama-era policy, and have sworn their allegiance to the Paris Agreement’s goals—but have promised no substantive alternative. Waxman-Markey is dead, and the Clean Power Plan is writhing on the floor. What’s next, Democrats?


Discovering an exoplanet isn’t what it used to be. Since the first detection of a planet around another star in 1995, astronomers have found thousands more, thanks in large part to the Kepler Space Telescope, which—truly an overachiever—has discovered nearly 5,000 potential worlds and verified about half of them. Even the discovery of the most exciting kind of exoplanet—an Earth-sized world orbiting inside a star’s habitable zone—has become routine: Kepler has confirmed the existence of more than 30 of them.
But that doesn’t mean the discoveries are boring.
A rocky planet similar in size and temperature to Earth is orbiting a nearby star called Ross 128, astronomers announced Wednesday. The star is a red dwarf, a type of star that is smaller and cooler than stars like our sun, located just 11 light-years from our solar system.
This makes the planet, which has been named Ross 128 b, the second-closest temperate, Earth-sized planet to our little corner of the universe. The closest is Proxima b, which resides four light-years away in the habitable zone of Proxima Centauri, our nearest star, and was discovered last year.
Ross 128 b was found by ESO’s High-Accuracy Radial-Velocity Planet Searcher (HARPS), an instrument located at the La Silla Observatory in Chile, and the discovery is described in a paper in Astronomy and Astrophysics. Astronomers at HARPS spent 10 years gathering and refining data, which included more than 160 measurements of Ross 128, before they found the signal that turned out to be an exoplanet, said Xavier Bonfils, the paper’s lead author, an astronomer at Institut de Planétologie et d’Astrophysique de Grenoble in France. “To detect such a small signal requires a lot of data,” Bonfils said.
Stars like Ross 128 move ever so slightly because of the gravitational tug of nearby objects. These tiny movements can be seen as changes in a star’s light spectrum; when the star moves toward an observer—in this case, the HARPS instrument—its light looks slightly bluer, and when it moves away, it turns redder. If these shifts occur in regular patterns, that means there’s an exoplanet lurking. Astronomers can analyze this information to determine some of the planet’s properties. Nicola Astudillo-Defru, an astronomer at the University of Geneva and Bonfils’s coauthor, said he reacted with “a big wow” when the data showed a rocky and temperate world. “I feel so excited that I mailed Xavier minutes later with a clear URGENT in the subject,” he said in an email.
Astronomers say Ross 128 b completes an orbit around its star every 9.9 days. They estimate the exoplanet has an equilibrium temperature somewhere between -60 degrees and 20 degrees Celsius (-76 degrees to 68 degrees Fahrenheit).

Astronomers don’t know whether Ross 128 b resides in its star system’s habitable zone, that sweet spot where temperatures are just right for liquid water to pool on the surface of a rocky world. But they’re still hopeful that it might be able to support life. Red dwarfs like Ross 128 are dimmer than sun-like stars, emitting infrared instead of visible light. So while Ross 128 orbits 20 times closer to its star than the Earth does to the sun, the exoplanet receives only 1.38 times more stellar radiation than Earth. Bonfils said Ross 128 is “quieter” than other red dwarfs, like Proxima Centauri, which can unleash flares that bathe orbiting planets in ultraviolet and X-ray radiation.
“Some even think that the atmosphere can erode, can evaporate, due to these activities,” Bonfils said. The quiet nature of Ross 128 may have created a comfortable cosmic environment for life on the exoplanet to take hold, despite its planet’s close proximity to the star.
Ross 128 popped into headlines in July of this year, when astronomers at the Arecibo Observatory in Puerto Rico said they had detected a mysterious radio transmission coming from the star for about 10 minutes in mid-May. Stars can emit electromagnetic radiation in the form of radio waves, but this signal was recorded at a frequency scientists hadn’t seen before in red dwarfs. Arecibo and other telescopes quickly trained their eyes on Ross 128 for more observations, but the transmission never appeared again. The astronomers entertained several possible explanations, including—much to the internet’s delight—extraterrestrials. They eventually said the signal probably came from one or more geostationary satellites. But the discovery of an Earth-sized exoplanet around the star could restart some of that conversation.
“We are considering additional follow-up in light of the new discovery at radio and optical wavelengths,” said Andrew Siemion, the director of the Berkeley SETI Research Center who runs the Breakthrough Listen Initiative, aimed at finding evidence of extraterrestrial civilizations. The Breakthrough Listen team helped Arecibo astronomers observe Ross 128 this spring. “Nearby exoplanets are particularly exciting from a SETI perspective as they permit us to search for and potentially detect much weaker signals than from more distant targets.”
While Ross 128 is, in cosmic terms, just down the street from us, it would take a spaceship traveling at the speed of light 11 years to reach it. Humans, at least the kind Earth has today, will never reach their stellar neighbor. But they can get a closer look. In the next decade, powerful telescopes like ESO’s Extremely Large Telescope will be able to sniff out the atmospheres of distant exoplanets like Ross 128 b for hints of biomarkers like oxygen. This technology will usher in a new era in the search for exoplanets, in which astronomers learn not only that more are out there—and certainly, more are—but discover what they’re truly made of. The detection of life-giving molecules in the atmosphere of a distant world will be a momentous occasion. Perhaps thousands of those discoveries, on many worlds, will follow. Perhaps someday, that will start to become routine, too.


In February 1977, the Alvin—a submersible big enough for three people, provided none of them stretched out their arms—dove to the bottom of the ocean, just north of the Galapagos Islands. The scientists who had crammed themselves into the sub were expecting to find hydrothermal vents—hypothetical sites where superheated water belched forth from the Earth’s interior. And they succeeded. But to their shock, they also discovered life, in extreme and implausible abundance.
At a depth of 2,400 meters, in pitch-black waters that were disconnected from the sun’s energy and compressed by the full pressure of the overlying ocean, the Alvin crew found what they described as a “Garden of Eden.” Clams and shellfish clung to the belching chimneys of rock. Crabs clambered over them and fish swam past. And strangest of all, giant worms encrusted the vents in stony tubes of their own making. Their scarlet gills protruded from these cylinders like tubes of lipstick—beautiful, if eerily so.
That life existed here at all, let alone in such wealth, was extraordinary. The crew, all of whom were geologists, were so unprepared for such a find that they hadn’t brought any biologists with them. And when they returned to the surface, the only substance they had for preserving the specimens they had collected was vodka.
Those vents, and others like it, have changed our understanding of the extreme conditions in which life could thrive. And the giant tube worms led to the discovery of an entirely new way of life—one that relies entirely on microbes, and that allows creatures to eat even though they’re mouthless and gutless. It’s called chemosynthesis, and its story is told in the video below—the third in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.



It’s nearly Thanksgiving, which, for most Americans, marks the one time a year their dinner table is adorned with jewellike cranberries, simmered into a delicious sauce. But hundreds of years ago, cranberry sauce was a mainstay of daily meals all around the United States. How did this acidic, tannic berry, so hard to love in its raw form, become one of the most popular fruits in America, and how did it fall so deeply out of fashion? Meanwhile, as cranberry sauce was relegated to Thanksgiving, cranberry juice became a popular drink—and mixer. But why is the juice so widely believed to combat urinary-tract infections, and does science support that claim? Join us this episode for all that, plus a tour of the cranberry bog of the future.
When the European colonists arrived in North America, they discovered that Native American tribes enjoyed a tart, bright-red berry growing wild in sandy bogs around New England. In fact, tribes across the continent’s north harvested cranberries and ate them in combination with fats, meats, corn, and other berries, in addition to using them for medicine and dye. But the colonists didn’t copy the local Native tradition of pounding cranberries with meat to create a protein-rich power bar called pemmican, says Robert Cox, the author of Massachusetts Cranberry Culture: A History from Bog To Table—in part because the cranberry fit perfectly into their own tradition of cooking tart berries into sauce to accompany meats. Used like this, as a substitute for the British gooseberry or redcurrant, cranberry sauce became so popular that, Cox told Gastropod, “People would joke that if you visited a New England home in the 18th or early 19th century, the tablecloth on the table was held down at each and every corner by big pots of cranberry sauce that were served with anything for breakfast, anything for lunch, anything for dinner.”
Today, however, cranberry sauce has almost entirely disappeared from our culinary vocabulary, aside from Thanksgiving and, in the United Kingdom, Christmas dinner. Instead, most people now consume cranberries in their dried and juiced forms. In part, cranberry juice’s popularity has thrived due to its supposed ability to help either treat a urinary-tract infection (UTI), or prevent future ones from occurring. This belief stretches back to the Native Americans. But while the cranberry is generally thought to be a healthful fruit snack overall, the science on its UTI-fighting powers has been decidedly mixed. Frustrated, Manisha Juthani-Mehta, an associate professor of medicine at Yale University, conducted her own rigorous, standardized, controlled scientific study to determine whether the juice can actually help, and she reveals the results on Gastropod.
As cranberry juice and Craisins have grown in popularity, cranberry growers’ modern bogs have kept pace with the demand; today’s bogs are far more productive than the wild cranberry bogs of the past, as we discovered when we visited A.D. Makepeace, the largest cranberry grower in the world, to check out the future of cranberry growing. New varieties, new harvesting techniques, and new farming technologies mean we’re awash in cranberries, presenting cranberry growers with a new challenge: how to break cranberries out of their monogamous relationship with turkey, and convince us to eat more of this most American berry all year round. Listen in!
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


To find the world’s most sinister examples of mind control, don’t look to science fiction. Instead, go to a tropical country like Brazil, and venture deep into the jungle. Find a leaf that’s hanging almost exactly 25 centimeters above the forest floor, no more and no less. Now look underneath it. If you’re in luck, you might find an ant clinging to the leaf’s central vein, jaws clamped tight for dear life. But this ant’s life is already over. And its body belongs to Ophiocordyceps unilateralis, the zombie-ant fungus.
When the fungus infects a carpenter ant, it grows through the insect’s body, draining it of nutrients and hijacking its mind. Over the course of a week, it compels the ant to leave the safety of its nest and ascend a nearby plant stem. It stops the ant at a height of 25 centimeters—a zone with precisely the right temperature and humidity for the fungus to grow. It forces the ant to permanently lock its mandibles around a leaf. Eventually, it sends a long stalk through the ant’s head, growing into a bulbous capsule full of spores. And because the ant typically climbs a leaf that overhangs its colony’s foraging trails, the fungal spores rain down onto its sisters below, zombifying them in turn.
The fungus’s skill at colonizing ants is surpassed only by its skill at colonizing popular culture. It’s the organism behind the monsters of the video game “The Last of Us” and the zombies of the book The Girl With All the Gifts. It’s also an obsession of one David Hughes, an entomologist at Pennsylvania State University, who has been studying it for years. He wants to know exactly how this puppet master controls its puppets—and his latest experiments suggest that it’s even more ghoulish than it first appears.
Hughes’s student Maridel Fredericksen used a special microscope to julienne infected ants into slices that were just 50 nanometers thick—a thousandth of the width of a human hair. She scanned each slice, compiled the images into a three-dimensional model, and painstakingly annotated which bits were ant and which bits were fungus. It took three months to mark up just one muscle. To speed things up, Hughes teamed up with computer scientist Danny Chen, who trained an artificial intelligence to distinguish ant from fungus.
When the fungus first enters its host, it exists as single cells that float around the ant’s bloodstream, budding off new copies of themselves. But at some point, as Fredericksen’s images show, these single cells start working together. They connect to each other by building short tubes, of a kind that have only ever been seen before in fungi that infects plants. Hooked up in this way, they can communicate and exchange nutrients.
They can also start invading the ant’s muscles, either by penetrating the muscle cells themselves or growing into the spaces between them. The result is what you can see in this video: a red muscle fiber, encircled and drained by a network of interconnected yellow fungal cells. This is something unique to Ophiocordyceps. Hughes’s team found that another parasitic fungus, which fatally infects ants but doesn’t manipulate their minds, also spreads into muscles but doesn’t form tubes between individual cells, and doesn’t wire itself into large networks.
Whenever Hughes or anyone else discusses the zombie-ant fungus, they always talk about it as a single entity, which corrupts and subverts a host. But you could also think of the fungus as a colony, much like the ants it targets. Individual microscopic cells begin life alone but eventually come to cooperate, fusing into a superorganism. Together, these brainless cells can commandeer the brain of a much larger creature.
But surprisingly, they can do that without ever physically touching the brain itself. Hughes’s team found that fungal cells infiltrate the ant’s entire body, including its head, but they leave its brain untouched. There are other parasites that manipulate their hosts without destroying their brains, says Kelly Weinersmith from Rice University. For example, one flatworm forms a carpet-like layer over the brain of the California killifish, leaving the brain intact while forcing the fish to behave erratically and draw the attention of birds—the flatworm’s next host. “But manipulation of ants by Ophiocordyceps is so exquisitely precise that it is perhaps surprising that the fungus doesn't invade the brain of its host,” Weinersmith says.
In retrospect, that makes sense. “If such parasites were merely invading and destroying neuronal tissue, I don’t think the manipulated behaviors that we observe would be as compelling as they are,” says Charissa de Bekker from the University of Central Florida. “Something much more intricate must be going on.” She notes that the fungus secretes a wide range of chemicals that could influence the brain from afar.
So what we have here is a hostile takeover of a uniquely malevolent kind. Enemy forces invading a host’s body and using that body like a walkie-talkie to communicate with each other and influence the brain from afar. Hughes thinks the fungus might also exert more direct control over the ant’s muscles, literally controlling them “as a puppeteer controls as a marionette doll.” Once an infection is underway, he says, the neurons in the ant’s body—the ones that give its brain control over its muscles—start to die. Hughes suspects that the fungus takes over. It effectively cuts the ant’s limbs off from its brain and inserts itself in place, releasing chemicals that force the muscles there to contract. If this is right, then the ant ends its life as a prisoner in its own body. Its brain is still in the driver’s seat, but the fungus has the wheel.


Think of the Earth’s climate system as a pair of dice. You never know exactly how a roll will end. But some outcomes, like rolling a seven, are much more likely than others, like snake eyes. But when we warm the globe, we essentially load the dice to favor extreme outcomes, including some of the most unpleasant weather possible in the United States.
A new study, rapidly conducted in September and published Monday in the Proceedings of the National Academy of Sciences, finds that the dice are increasingly likely to roll some very unpleasant weather indeed. Global warming may not have caused Hurricane Harvey, which ravaged Houston over the course of a week earlier this year, but it made it much more likely.
The study argues that storms that dump more than 20 inches of rain on Texas are about six times more likely now than they were at the end of the 20th century. Hurricane Harvey dropped 20 inches of rain across a swath of the state, and it deluged some parts of Houston with a record-smashing 60 inches of rain.
Harvey-like hurricanes will only get more likely as the century wears on. Between 1981 and 2000, there was about a one-in-100 chance that a hurricane would dump 20 inches of rain on Texas. By the end of this century—from 2081 to 2100, as children today enter old age—there will be a roughly one-in-5.5 chance that such a flooding hurricane would form if global warming continues apace.
“When you’re planning for the future of cities like Houston, it would be unwise to assume that the climate of the future is pretty much similar to what it’s been for the last 100 years or so,” said Kerry Emanuel, the author of the paper and a professor of atmospheric science at the Massachusetts Institute of Technology. “Even if the climate wasn’t changing, records of rainfall are too short, and the quality of them is too low, to really get a handle on flooding risks.”
Global warming will worsen hurricanes for two reasons. First, otherwise identical storms of all types—not just hurricanes—will retain more moisture in a warmer climate. “It’s one of the risks that we’re most confident of,” he told me. “The physics are so elegantly simple: Warm the air and it can hold more water.”
Second, global warming will make storms move slower. What made Hurricane Harvey so damaging was not its intensity but its longevity: The cyclone moved ashore, parked over Houston, and then rotated in place for three days. It became an atmospheric conveyor belt, picking up water from the Gulf of Mexico and raining it out over the Houston metropolitan area. These days of rain caused the city’s devastating inland floods.
In Emanuel’s study, global warming helped slow similar hurricanes down. Though tropical cyclones unleash some of the fiercest winds on Earth at their center, their movement across the surface of the planet—and thus their storm track—is determined by the slower high-altitude winds summoned by distant high- and low-pressure systems. This planetary engine is slowing down as global warming pushes land and ocean temperatures closer together.
“If [general circulation] slows down, then places near the coast will get more rain,” Emanuel said. “But the main reason our technique shows increasing rainfall is that there’s more water in the air.”
Emanuel conducted the study through the use of a computational tool he pioneered: embedding a hurricane and ocean model in a coarse, planetary-scale climate model. To determine the current climate’s propensity for a Harvey-like storm, he used three meteorological models (with the embedded hurricane tool) to simulate thousands of years of late-20th-century weather. Then, he used six different global climate models (each with the same hurricane tool) to simulate thousands more years of late-21st-century weather. By examining the models’ output of rainfall data and storm tracks, he could arrive at more accurate probability risks.
“This analysis provides strong evidence for why climate-change information should be incorporated into any plans for building Houston back stronger,” said Heidi Cullen, the chief scientist and director of the World Weather Attribution program at Climate Central, a nonprofit based in Princeton, New Jersey, in an email.
She said the study was a “very nicely constructed analysis” that “makes it clear that we must act quickly to both adapt to the changes that are already in the pipeline while also reducing our dependence on fossil fuels.”
But it did not fully capture the future risk of flooding rains in the Houston area or Texas, she added. “About one-third of extreme precipitation events on the Gulf Coast are not associated with a hurricane,” she told me, citing a forthcoming Climate Central analysis of Hurricane Harvey. “When you are looking at extremes, it’s good to look at all rain-producing events.”
While this paper was prepared for publication quickly—Emanuel said he rushed it toward completion so it could inform Houston’s rebuilding efforts—it uses some of the most cutting-edge techniques in climate science. Embedding a cyclone model inside a climate model helped a team of researchers conclude last month that New York City will soon see major, hurricane-induced flooding every five years.
And Emanuel’s scholarship helped inform the Climate Science Special Report, released this month by 13 U.S. federal agencies, which predicted that global warming will increase the strength and destructiveness of hurricanes by the end of 21st century.
The paper also examined how likely it is that a Harvey-like storm would hit the Houston metro area specifically. While major flooding cyclones will be once-in-100-year events by the end of this century, they were as rare as once-in-2,000-year events at the end of the last. “By the standards of the average climate during 1981–2000,” the paper says, “Harvey’s rainfall in Houston was ‘biblical’ in the sense that it likely occurred around once since the Old Testament was written.”


Over the last 24 hours, fans of the talk-show host Sean Hannity have taken to destroying Keurig coffee-pod machines.
In what’s being called the “Keurig Smash Challenge,” many people have posted videos online of aggressive destruction. At least one person did so with a golf club.
The challenge is metaphorical, as the coffee makers are small and fragile. And the performances are rather a protest of a protest, unfolding in the wake of Keurig pulling its advertising from Hannity’s show as a result of his tepid coverage of the child-molestation allegations surrounding Alabama judge and senate candidate Roy Moore. Hannity applied a criminal precedent to a political race, arguing that “every single person in this country deserves the presumption of innocence.”
In response to consumer complaints, Keurig wrote Saturday, “We worked with our media partner and Fox News to stop our ad from airing during The Sean Hannity Show,” in a tweet that has since been deleted. (Keurig CEO Bob Gamgort today clarified that “the decision to communicate our short-term media actions on Twitter was done outside of company protocols.”)
The ideological basis of the Keurig Smash Challenge is murky and filled with directionless energy, not unlike coffee. The protesting of Keurig is a small step removed from overtly supporting the reported actions of Moore. What’s more, destroying Keurig machines also clearly aligns these people with a global environmentalist movement. In 2015, the “Kill the K-Cup” campaign took hold among those concerned about the net waste of so many pods. A Canadian advocate encouraged people to publicly abandon the machines.
As I reported at the time, even John Sylvan, the coinventor of the Keurig machine, has regrets about inventing the system because of its wastefulness. He told me then, “I feel bad sometimes that I ever did it.”
Within days of that story’s publication, a wave took hold. Keurig announced a forthcoming machine that would allow for recyclable pods. Several other green-oriented companies entered the pod-coffee market with compostable pods or reusable devices that can be filled with coffee grounds. Still the majority remain in dumps and landfills, and vocal progressive advocates have encouraged people to get rid of their pod-based machines.
A Brewing Problem
The environmental campaign may actually go against Moore’s political agenda. Moore’s campaign site does not give a clear position on energy policy, but he is not evidently aligned with anti-Keurig-level environmentalism. He wrote in a 2009 op-ed, “Not only is there no constitutional authority for Congress to regulate carbon emissions, but the premise of ‘global warming’ and ‘climate change’ upon which such environmental theories are based does not have the support of a scientific consensus.” This was false even at the time.
Now America finds itself in the midst of hordes of angry people with clubs who will soon be going through caffeine withdrawal. They could go to Starbucks, though the chain has also been condemned and boycotted by some isolationist conservatives, since earlier this year it promised to hire 10,000 refugees in response to President Trump’s executive order barring them from the country.
Sometimes it’s difficult to avoid supporting a social cause.
Hannity himself is now well aligned with the “Kill the K-Cup” environmentalist movement, as well as unearned coffee handouts. He tweeted Sunday, reassuring his potentially lethargic followers, “Deplorable friends, I am buying 500 coffee makers tomorrow to give away!!”


In June, several dozen scientists flew to Big Sky, Montana, to discuss the latest in CRISPR research. They had a lot to talk about, given that CRISPR—a tool that allows scientists to cut DNA to disable genes or insert new ones—is currently the hottest topic in biology, mentioned in the same breath as pronouncements like “changing the world” and “curing humanity of disease.”
On the second day in Big Sky, a Japanese researcher named Osamu Nureki got up to play a short movie clip. “I was sitting in the front, and I just heard this gasp from everyone behind me,” says Sam Sternberg, who worked in the CRISPR pioneer Jennifer Doudna’s lab at the University of California, Berkeley. It was, he says, the biggest reaction to data he’s ever seen at a conference.
Nureki’s paper was published in Nature Communications Friday, and by early morning, the video that astonished the room in Big Sky was making the rounds on science Twitter, too. I watched it, still bleary-eyed from sleep, and I jolted awake immediately.
Single-molecule movie of DNA search and cleavage by CRISPR-Cas9. pic.twitter.com/3NQxmbvzJF
The video is grainy, blobular, and dark, but for a molecule-scale movie, it is remarkably clear. You can see CRISPR, in real time, cleaving a strand of DNA in two. There is nothing that surprising in the clip given what scientists already knew, but that is exactly what makes it so astonishing: Scientists had figured out so much about CRISPR without ever seeing it.
It had the satisfying snap of things falling into place—like the first time a telescope sighted Neptune years after a French astronomer had predicted its existence from perturbations in the orbit of a neighboring planet.
CRISPR obviously exists, and it obviously works. Scientists have used the technique to do everything from ridding mice of HIV to beefing up dogs. But the evidence for how it works had always been indirect. Modern biochemistry research is a series of elaborate workarounds to infer the behavior of molecules until now too small to see.
Take, for example, X-ray crystallography, which Nureki had previously used to study the structure of CRISPR-Cas9. (“CRISPR” is the popular shorthand, but Cas9 is the name of the actual enzyme that cuts DNA.) Scientists spend months perfecting the art of coaxing Cas9 to grow into crystals. Then they take these precious crystals to a particle accelerator to shoot X-rays through them, producing a pattern unintelligible to the average person but which experts can measure to infer the structure of Cas9. The end result is a computer model of Cas9, resembling a clump of curly ribbons. Repeat all of this using Cas9 frozen at different stages of editing DNA, and you can after many months or even years get a handful of static, computer-generated snapshots.
Scientists Can Use CRISPR to Store Images and Movies in Bacteria
Compare that to the fluidity and directness of the real-time video. His team used a technique called high-speed atomic-force microscopy, in which a tiny needle moves back and forth probing the shape of Cas9. The needle moves so fast that it produces a movie. “The result is fairly easy to understand,” says Hiroshi Nishimasu, one of Nureki’s collaborators on the paper. “People say, ‘Wow!’ It’s very simple.”
Nishimasu posted the video to Twitter, and that dark, grainy clip has since gotten more than 2,500 likes. The response has been far and beyond that to any other paper he’s published, including ones in the most prestigious journals like Nature and Cell.
In fact, he says, the team did submit the videos to top-tier journals, who rejected it citing a lack of novelty—which, perhaps, is true. Scientists are supposed to be dispassionate creatures of data. But they are also only human, and humans believe their eyes. “I realized that seeing is believing,” says Nishimasu.


Mark Harris says funeral directors talk about it all the time. More and more people are growing tired of traditional funeral services and opting for something a little more creative. “It’s getting more difficult to offer the cookie-cutter send-off,” explains Harris, the author of Grave Matters, which examines how people have started to think, er, outside the box about death.
And so, Harris wasn’t surprised to hear that a new British company is offering to send cremated remains to the stratosphere. High-altitude latex balloons will float to 100,000 feet above the surface of the Earth, where the curvature of the planet appears against the darkness of space, and then release the ashes into the cold, creating a glittering display. “Scatter your loved one’s ashes in space,” Ascension Flights says on its website. “We are all made of stardust.” The stratosphere is not technically space, but for their purposes, it’s close enough.
Ascension Flights, run by funeral directors and a near-space launch firm, will soon offer its high-altitude funerals, with the cheapest package starting at £795, or about $1,040. For more money, customers can choose the launch site and have the scattering photographed and filmed.
Where the Dead Become Diamonds
The near-space funeral is, at first glance, a contrast to “green” burials, which return remains to the soil in biodegradable coffins or urns. In this way, the deceased can meet “the green reaper,” as a Guardian article in 2014 colorfully put it, and contribute to the physical processes of the Earth. Blasting ashes into the stratosphere sure sounds like the opposite of that, but Ascension Flights promises some kind of return to the planet. “As the particles eventually return to Earth, precipitation will form around them, creating raindrops and snowflakes,” its website explains. “Small amounts of nutritious chemicals will stimulate plant growth wherever it lands.”
Harris, who favors going the natural route, said this promise seems considerably less certain than that of green burials, where at least “I wouldn’t have to worry about having my loved one’s ashes raining down from space on some random location like a landfill or a Superfund site or a nuclear power plant,” he said.
Both kinds of memorials are part of the same growing trend in end-of-life affairs, Harris said. People are becoming increasingly interested in how their physical remains, and the remains of their loved ones, will be handled. They want something more personal and more personalized.
These days, people can forgo metal caskets and be buried in bamboo or recycled cardboard instead, or have their remains wrapped in banana leaf, cotton, or wool. A company called Eternal Reefs will fashion an environmentally friendly artificial reef out of cremains—cremated remains—and drop it into the ocean for nearby marine life to populate. Cremains can be pressed into diamonds, incorporated into paint, and ejected as fireworks. The variety of options for the dead reflects the consumer culture of the living, says Phil Olson, a Virginia Tech professor who studies funeral practices, like the home-burial movement. “There are at least seven kinds of Coke, 500 kinds of cigarettes—options, options, options,” Olson said. Consumers want just as many choices in death as in life.
The option to send a loved one’s ashes to actual space has existed for several years already, for a steeper price than Ascension Flights charges. Since 1997, the company Celestis has flown missions into space delivering the cremains of dozens of people, including Star Trek creator Gene Roddenberry. The payload is launched inside a capsule to more than 300,000 feet, beyond the boundary of space, and eventually falls back to Earth.
While the concept of commemorating life’s final frontier in the final frontier may seem incredibly high-tech, the emotion behind it is no different than run-of-the-mill funerals on Earth. Funeral services can be, in the end, more for the benefit of those who are left behind than those who’ve passed away. They are about processing grief, and grief is personal. For some, the thought of sending their loved one’s ashes into the stratosphere is, simply, very fitting, and it’s difficult to pin down the exact reasons why.
Olson points to alkaline hydrolysis as an example of the funeral industry misunderstanding its customers. Providers of alkaline hydrolysis, which reduces bodies to skeletons in a liquid solution, believed the appeal of the process came from its eco-friendliness. They later found that the primary reason people gave for choosing hydrolysis was that they perceived it to be gentler than cremation. “For some reason, people see being dissolved in caustic alkaline as being gentler than being incinerated,” Olson said.
Perhaps having more options to memorialize the dead may ease the grieving process in some way, he said, even if it’s not clear exactly how.
“We can speculate all we want for people’s motivations for doing this, but we could be dead wrong,” Olson said of the high-altitude memorial and, when I laughed in response, quickly realized his choice of words. “Sorry, pardon the pun. I didn’t even notice that.”


The day is finally here. From the northern reaches of New England to the southernmost stretch of the Chesapeake Bay, one of autumn’s most famous performances will take place this weekend.
On Friday or Saturday night, the first hard frost will likely sweep down the coast. Ginkgo trees—known for their fractured, twisted branches and broad, fan-shaped leaves—will react like a surprised burglar and drop all their leaves to the ground at the same time. On city streets lined with ginkgo trees—like my old block in northwest Washington, D.C.— a soft yellow padding will cover everything, erasing the distinction between sidewalk and street. Only the occasional fire hydrant hints at the manmade understory.
Early on Thursday morning, the ginkgo tree in front of James Hall at the University of New Hampshire got an early start on the act, sloughing its leaves to form a small circle of gold. The event is affectionately anticipated by the school’s department of natural resources, which is housed inside the red brick hall. Since 1977, students and faculty in the department have played a guessing game, trying to anticipate when the tree’s leaves will dump. A box in the building’s foyer entreats students to place their guesses.
“I’ve been in the department for 15 years, and I knew this was going on but didn’t pay too much attention,” says Serita Frey, a soil microbiologist and a professor at the university.“It didn’t have anything to do with climate change at first. Everyone just knew that the ginkgo tree dumps its leaves in one day.”
Why the single-day drop? In the autumn, deciduous trees form a scar between their leaves and stems to protect themselves from diseases and winter’s coming chill. Most flowering trees, like oaks and maples, form the scar at different rates, in different parts of the tree, over the course of weeks. Their leaves then fall off individually. But ginkgoes form the scar across all their stems at once. The first hard frost finishes severing every leaf, and they rain to the ground in unison.
A few years ago, Frey became curious about whether there was data documenting the ginkgo-dump day over the years. According to the National Climatic Data Center, fall temperatures in New Hampshire are now more than 2 degrees Fahrenheit warmer than they were in the late 1970s. Did that affect the flagship ginkgo tree? Had someone even kept records about the contest that far back?
For the first few days of her search, she couldn’t locate the record. “And then I found a piece of paper with handwriting on it that some secretary back in 1977 started, and that someone from the department had been adding to every year,” she said. “I put all that information in a spreadsheet, and I’ve been updating that graph every year since.”
The graph revealed that the ginkgo-dump day had been sliding forward over the ensuing decades. Every decade, the ginkgo tree loses its leaves an average of three days later than it had 10 years prior. When the James Hall ginkgo dumped its leaves this Thursday, November 9, it was the second-latest that the tree had ever hit the autumn milestone.
When Did the James Hall Ginkgo Lose Its Leaves?
“It’s our poster tree—our local example of climate change,” she told me. Frey uses the slide in her classes about global warming.
The James Hall ginkgo is not, of course, the only organism subject to the creeping relocation of the seasons. In 2016, the U.S. National Park Service examined when the first leaf or first bloom of spring arrived in 276 of its parks. In three-quarters of parks, spring was arriving earlier than it had in the past; in half of the parks, spring now arrives earlier than it did in 95 percent of the years since 1901.
Nor is the story of the James Hall ginkgo data trove unique. About two decades ago, climate scientists realized that springtime bloom records were some of the longest-running observations of the climate system available. By examining and digitizing old records, they have found:
The most impressive of these seasonal records exists in Japan, where an environmental scientist has compiled the date of the first cherry-blossom bloom in the city of Kyoto, going back to 800 CE. That study found that cherry blossoms in that city now flower earlier than they have in almost 1,000 years.
But all these measurements examine the first (and often advancing) appearance of spring. Frey’s ginkgo-tree record looks at the often-regressing signature of autumn. The ginkgo-leaf rain is also a seasonal symbol that appears across the East Coast: I have seen great-leafed ginkgoes sprout on small farms, on suburban streets, and in tiny downtown Manhattan parks. Urban arborists often plant ginkgoes because they are more resistant to pollutants and pesticides than other trees. And why not? Ginkgoes have persevered in something close to their current form since dinosaurs walked the Earth.
Not that everyone loves the tree’s annual autumnal transformation. Along with their leaves, ginkgoes also dump their big pungent berries, which split and rot on the pavement. A frustrated New Yorker once complained that they reeked of “boiled egg farts.”


Nearly two months after Hurricane Maria struck Puerto Rico, many residents are still without power and struggling to get access to water, food, and basic services. This week, a failed transmission line knocked out what little of the island’s electrical grid had been restored, temporarily leaving thousands of people in the dark once again.
The recovery from the devastating Category 4 hurricane is expected to be long and costly for all of Puerto Rico, including at the Arecibo Observatory, the world’s second-largest radio telescope, which sits in a mountain range in the island’s northwest.
As the storm approached Puerto Rico in September, several researchers remained at Arecibo to keep watch over the 305-meter-wide dish, taking shelter in concrete buildings with food, water, and fuel for generators. The staff made it through unharmed, and when the worst of Hurricane Maria passed, they found the facility had survived, with some damage. The main dish, while intact, lost some of its reflective panels, and most of a 29-meter-long antenna was destroyed. But it wasn’t as bad as they expected, considering Hurricane Maria’s 155-mile-per-hour winds. “It’s a thing to be thankful for,” Joan Schmelz, the observatory’s deputy director, told Science magazine back then.
The observatory was back up and running on generators a week after the hurricane hit. But as of this week, Arecibo still hasn’t resumed normal operations. The observatory is still running on generator power, according to Suraiya Farukhi, a spokesperson for the Universities Space-Research Association (USRA), one of Arecibo’s operators.
Right now, the radio telescope is locked in a single direction, a configuration meant to save energy, Science reports. Maneuvers to point the telescope in different spots would double fuel consumption, and fuel is in high demand on the island.
Arecibo has observed the skies since the 1960s, searching for stars, exoplanets, asteroids, and potential signals from extraterrestrial civilizations. The iconic observatory has served as a research base for, among others, Jill Tarter, the cofounder of the ET-searching SETI Institute; Frank Drake, the creator of the famous equation for estimating the number of potential alien civilizations; and the late Carl Sagan, perhaps the best-known advocate for space exploration. In 1993, astronomers won a Nobel Prize in physics for discovering of a new kind of pulsar—a fast-rotating star that emits beams of light—using the Arecibo dish. The observatory employed the fictional scientist Ellie Arroway in 1997’s Contact, a character inspired by Tarter.
Some worry the expensive repairs necessary to return Arecibo to normal may influence the National Science Foundation’s thinking on the observatory’s future. NSF, which owns Arecibo, has long considered reducing its annual contributions to the facility and even passing off management to another party. The NSF spends $8.3 million on operations each year, with NASA providing $3.6 million, according to Science. The National Science Board, which governs the NSF, is expected to publicly announce plans for Arecibo’s future next week.
One of the research facilities at Arecibo, the Planetary Habitability Laboratory, which is managed by the University of Puerto Rico, said the lab sustained “extensive damage to its walls, floor, computers, materials, and furniture” and is asking for help in rebuilding. “The lab is in the process now of a complete rebuild that might take a few weeks or even months,” a post on the laboratory’s website said Thursday. “Most of the structural damage to the lab should be covered by FEMA, the insurance, and the university. However, about $14,000 of damage to our furniture, computers, and scientific materials might not be covered.”
The scientists who work at Arecibo face their own personal recoveries. USRA said this week that it had shipped from Maryland to Puerto Rico 20 portable generators and donated them to USRA employees who badly need electricity in their homes.
Several employees who hunkered down at Arecibo during the hurricane, tweeting various updates—from the state of the observatory to the kinds of snacks they’d brought with them—continue to share news on social media. Their messages highlight the incredibly slow pace of recovery. “And, lo! Our water supply was restored!” Robert Minchin, head of radio astronomy at Arecibo, tweeted Wednesday, seven weeks after the hurricane arrived. “This morning I have water, power, and broadband at home!” he wrote a day later.


The sound of death can take many forms: the retort of a gun, the screech of tires, the hack of a cough. But for many moths, death sounds like a series of high-pitched squeaks.
Moths are hunted by bats, which track them down by releasing high-frequency calls and analyzing the rebounding echoes. This skill, known as echolocation, allows them to view their world—and their prey—even in total darkness. Bats evolved the ability to echolocate tens of millions of years ago, and in the intervening time, moths have developed their own countermeasures. Some evolved ears, which allow them to eavesdrop on the calls of hunting bats and take evasive action. Others play the bats at their own game, releasing their own ultrasonic clicks to jam the radar of their predators, or to feign the echoes of distant objects.
Bats, in turn, have evolved their own tricks for circumventing the moths’ defenses. Some, for example, use stealth.
Bat calls are too high-pitched for us to hear, and we should all be grateful for that because they’re also some of the loudest sounds produced by any animal on land. If we could hear them, it would be like listening to a passing ambulance, a jackhammer, or a rock concert. But Townsend’s big-eared bat—a North American species with a foot-long wingspan—is an exception. Aaron Corcoran and William Conner from Wake Forest University have found that when it hunts, it does so at a whisper, with very quiet calls that moths can’t hear. It has evolved into a winged ninja—silent and undetectable, until it’s too late.
This discovery helps to settle a question that’s been bugging Corcoran for a while. He and other evolutionary biologists often talk about evolutionary arms races, in which predators and prey evolve ever-more sophisticated measures and countermeasures to outwit each other. But while scientists have documented myriad examples of prey adapting around their predators, there are surprisingly few strong examples of predators doing the reverse. (These examples include the shocking powers of the electric eel, the snakes that have evolved to resist the poisons of newts, and the mouse that turns scorpion venom into a painkiller.)
Partly, that’s because it’s hard to show that predators are adapting to their prey specifically. For example, for decades, scientists have suggested that bats could shift their calls to higher or lower frequencies that moths can’t hear—and indeed, there are bats that do this. But higher-pitched calls give them a sharper view of their surroundings, and lower-pitched ones travel further and give a wider view of the world. It’s possible that such calls evolved to help bats navigate, and were only incidentally useful for subverting the ears of prey.
In 2010, Hannah ter Hofstede from the University of Bristol found a more unambiguous countermeasure. She showed that the barbastelle, a small European bat, is also a whisperer, with echolocation calls that are 10 to 100 times quieter than those of other moth-hunting bats. This seemed like a clear-cut case of an anti-prey adaptation, since there’s really only one advantage to quieter sonar: catching sharp-eared moths.
Ter Hofstede demonstrated this by tethering moths in small arenas, and hooking electrodes to the neurons in their ears that detect bat calls. These neurons typically fire when bats are around 19 meters away, giving the moths plenty of time to react. But those same neurons only detect barbastelles when they are 2 meters away, giving them just half a second to dodge. By then, it’s probably too late.
Probably. In these experiments, the bats never actually got to attack the moths—and Corcoran wanted to know what would happen if they did. He chose moths that are known to jam bat echolocation and tethered them to fishing lines, hanging them in large outdoor arenas that were surveilled by cameras and microphones. Then he waited for bats to approach.
The results were clear. Compared to the long-legged myotis, a similarly sized bat and a fairly typical echolocator, the stealthy Townsend’s big-eared made calls that were 20 to 40 decibels quieter. And as a result, they catch moths on 80 percent of their attacks. “That’s pretty unheard of for a bat attacking well-defended prey,” says Corcoran. “And the moths almost never exhibited their normal defenses. They very rarely do diving maneuvers, and they never made their jamming clicks.” And when the moths did try to dodge, they did so at a third of the distance for the big-eared than the myotis.
But these whispers come at a cost: The big-eared bat can only detect moths at half the range of the louder myotis. This might explain why the former species isn’t very common. “Maybe you’re doubling your chance of capturing a moth, but you’re reducing your ability to find that moth by half,” says Corcoran. “The math may or may not work in your favor.”
There might be other costs, too. Many of the moths that make jamming clicks also do so to advertise the poisons in their bodies, says ter Hofstede. She wonders whether stealthy bats might be more likely to attack toxic prey. “Bats that catch these kinds of moths generally let them drop without eating them, so it is not dangerous to the bats, but it would represent wasted effort,” she says.
For now, it seems that the moths haven’t evolved a response to the stealthy sonar—and that’s probably because it’s not worth it. “At the sites where I work, there’s 15-plus species of bat, and this one is taking advantage of the fact that all the others echolocate very loudly,” says Corcoran. “Being a rare enemy that uses this odd trick, there’s not enough pressure for the prey to evolve a counter. If the moths focused on this one predator, their response would be all off for all the other bats.” In other words, Townsend’s big-eared bat is a hipster ninja: Its stealth only works because it’s the only bat that uses it.


You know the story, or at least some of it. Sixty-five million years ago, a rock about the size of Mount Everest careened out of space and slammed into modern-day Mexico. It opened a Hawaii-sized hole in the crust, launching soot and sulfur high into the atmosphere, blocking out the sun.
Within days, air and water temperatures plummeted around the world. Within weeks, the food chain on both the land and the ocean had collapsed. Within years, the dinosaurs—the rulers of Earth for more than 150 million years—had perished (except, of course, for the birds).
This moment, the Cretaceous-Paleogene mass extinction, is the most recent widespread die-out in our planet’s history. The dinosaurs’ departure opened the gates for a new class of creature—that is, mammals, like us—to inherit the top of food chains around the Earth. And the moment was also, according to a paper published Thursday in Nature Scientific Reports, quite improbable.
When the asteroid slammed into Mexico 65 million years ago, there was only a 13 percent chance that it would trigger a mass extinction, argue Kunio Kaiho, a geoscientist at Tohoku University, and Naga Oshima, a senior researcher at the Japan Meterological Agency.
“The probability of significant global cooling, mass extinction, and the subsequent appearance of mammals was quite low after an asteroid impact on the Earth’s surface,” they write in their paper.
Odds were 87 percent that the asteroid wouldn’t have prompted the evolution of mammals. In other words, the odds were longer that mammals would triumph than that Donald Trump would, on the eve of the 2016 president election (at least according to FiveThirtyEight).
What set these odds? The location of the asteroid’s impact with Earth—and the mechanism that actually killed the dinosaurs in the days after the impact.
Researchers once believed that the Chicxulub asteroid (so named because it struck the modern-day location of Chicxulub, Mexico) was so devastating because it prompted forest fires around the world. Those fires released soot and ash into the high atmosphere, blocking out the sun’s rays and cooling global temperatures. Indeed, all around the world, geologists find the remnants of soot and sulfur at the rock layer that signifies this moment in geological history.
But geologists and climate scientists have questioned whether forest fires alone could have set off a mass global cooling. Two years ago, a team led by Kaiho looked at the molecular structure of the soot. They argued that all that soot came from one origin point, and that its ratio of elements suggested a higher-energy burn than would normally occur in a wildfire.
The asteroid, they said, must have struck a site rich in hydrocarbons, like crude oil, natural gas, or kerogen. Only about 13 percent of the Earth’s surface contains enough subterranean fuel to set off a mass global cooling.
“Hence, if you assume that such an impact occurred at a random place on Earth, there would have been a 87 percent change that there would have been no mass extinction at all,” said Johan Vellekoop, a paleoclimatologist at the University of Leuven in Belgium who was not connected to this paper, in an email.
“It turns out the dinosaurs just happened to be very, very unlucky. This aspect of chance is often neglected in our field, and I think it is really cool that Kaiho and Oshima have tried to put a number on it,” he said.
Kaiho and Oshima are not the only researchers who have looked at how contingent the timing of the asteroid’s impact was on its consequences. Earlier this year, a group of British and American scientists showed that if the asteroid had entered Earth’s atmosphere mere seconds earlier or later, it would have collided with the sea instead of North America. An aquatic impact would have sent much less dust and soot into the sky, dampening its effects.
Vellekoop took issue with how Kaiho and Oshima arrived at their 13 percent number. They used climate models to estimate global temperature decline with the amount of soot in the atmosphere. He said this linked climate change too directly with the mass extinction:
For their calculations, they basically assume that all the extinctions that occurred at the Cretaceous-Paleogene boundary can be ascribed to climate change, and then go on to put a number on it: “Approximately 8–10 degrees Celsius temperature changes causes a mass extinction.” In doing so, they basically dismiss all other detrimental consequences of the Chicxulub impact, such as the blocking of sunlight and surface-water acidification.
The severity of climate change–driven extinctions greatly depend on the rate of change. Does it occur over days, months, years? Also, different species of plants and animals react differently to changes. Many species will already go extinct with a 5-degree change, some others might require much more. Putting actual reliable numbers on this would require a whole new study, probably involving a complex biological modeling component.
But he said their experiment was valid as a “thought experiment,” and as a first-pass estimation of how unlikely the rise of mammals may have been.
Kaiho’s work is not the only work being done on the mass extinction of the dinosaurs. Last year, the evolutionary biologist Manabu Sakamoto argued that dinosaurs began declining 40 million years before the infamous asteroid collision. For tens of millions of years, more species of dinosaurs disappeared than were replaced by new species.
The time scale, by the way, points to the longevity of dino dominance on Earth. Dinosaurs first appeared on this planet about 230 million years ago. They declined for 40 million years before they went extinct, at least in Sakamoto’s telling. But only 65 million years separates us from that asteroid collision. During that interval, a tiny shrewlike creature branched into horses and antelopes, sloths and polar bears, orangutans and blue whales—and the entirety of human history took place.
“Their long decline had made dinosaurs vulnerable to sudden environmental changes, such as those caused by the asteroid impact,” said Sakamoto in an email. “A sudden catastrophe followed by a substantial global cooling may have been too much of a stressor for dinosaurs to recover from.” He said that Kaiho’s paper did not conflict with his team’s hypothesis.
Lucky for us, the odds that an asteroid collision prompts a mass extinction haven’t changed much in the past 50 million years. In an email, Kaiho said there’s still about a 13 percent chance that any asteroid impact would toss up enough soot and ash to cool global temperatures.


The chatty green parrots had a front-row seat to a spine-chilling show. Tethered to a tree branch not far from their cage, another parrot, similar in appearance but of a different species, was armored in a small leather vest. As the green parrots looked on, a man approached the lone parrot with yet another bird leashed to his arm: a red-tailed hawk. The hawk lunged at the parrot in the vest, wrapping its talons around it. The parrot screamed, a sound only made when death is imminent. Satisfied, the man pulled the hawk off.
This simulated attack—don’t worry, the parrot was unscathed thanks to the vest—was a ruse, aimed at Puerto Rican parrots about to be released into the wild for the first time. A critically endangered species found nowhere else in the world, those chatty green birds had never known life outside of captivity. So they were being taught what to look out for when they headed into hawk territory.
“We wanted an experience that would instill in those birds a very real fear and recognition of a red-tailed hawk as a deadly predator,” says Tom White, a wildlife biologist with the U.S. Fish and Wildlife Service’s Puerto Rican Parrot Recovery Program. In 2001, White helped spearhead this fear-based training regimen for parrots before reintroducing them into the island’s forests. The training involves several other phases before the simulated attack, including “flying” a hawk-shaped cutout over the parrots’ aviary, playing recorded hawk calls, and having a live hawk attack their cage.
For endangered species, even a small boost in numbers can help shift the needle away from extinction. But isolating animals for safe breeding comes with a cost. Captive “insurance populations” of endangered species, like the protected group of Puerto Rican Parrots, can lose their survival edge, both behaviorally and genetically, in just a few generations. Once they become easy prey, they’re less likely to survive if released.
This situation has left some conservationists in an ethical bind. So far, wildlife-reintroduction programs have been hesitant to embrace an approach where live predators possibly injure or kill one animal for the sake of teaching many—even if, as in the safety-vested parrot’s case, the animal that’s attacked isn’t endangered. But White’s results seem promising. And researchers behind these programs know that pampering wildlife in captivity won’t help if the species can no longer make it on its home turf.
Earlier this year, on an island thousands of miles from Puerto Rico, another group trying to bring back a critically endangered bird called White for advice. The group, based primarily on Hawaii’s Big Island, had released five Hawaiian crows—raven-like birds with bristly feathers framing dark beaks—from a captive flock into their native forest, a place the species had not been in nearly 15 years. But within a week, three of the crows had died. Two were killed by their native predator, the Hawaiian Hawk, or ‘io.
Having some deaths in a reintroduction program, especially early on, is not unusual, says Alison Greggor, a researcher with San Diego Zoo Global, one of a few institutions collaborating to reintroduce the crows. Nevertheless, this initiative—known as the the ‘Alalā Project, after the crows’ native name—collected the surviving birds back into captivity, and vowed next time would be different. While the group had attempted antipredator training before releasing the crows, they realized it wasn’t rigorous enough. For these ‘alalā, approximately five generations removed from the wild, to have a chance at survival, the species needed to relearn to outwit its old adversary.
White advised his colleagues in Hawaii to show the ‘alalā that hawks mean business, and recommended a similar approach to the training he employs with the Puerto Rican Parrots. If the ‘alalā were going to watch a non-native bird get attacked, “I told them that you don’t need to worry about a vest,” White says. “Just go ahead and have the real experience.”
But the “real experience”—allowing a hawk to actually kill a bird in front of the ‘alalā, as opposed to merely simulating an attack—might not go over well with everyone. Animal-welfare permits and public perception are concerns, and the ‘alalā is revered as a family guardian in Native Hawaiian culture. Besides, the ‘Alalā Project wanted to take a “data-driven approach” to the training and adapt it to the learning style of crows, which are notoriously perceptive, social birds. To narrow down the best techniques, the group performed pilot trials, observing how the crows reacted to multiple scenarios, including one in which a hawk attacked live prey.
When they settled on what would go into the actual training regimen, the ‘Alalā Project tried to incorporate as many of the elements of a real attack as they thought would be effective. In line with many of White’s techniques, a taxidermied ‘io with wings spread wide was rigged to “fly” across the crows’ aviary while they heard its cries, along with ‘alalā alarm calls. A live ‘io appeared just outside their cage, flapping its wings as if swooping toward them. Finally, they saw and heard the consequences of ignoring those signals: the sound of ‘alalā shrieking in distress and the body of a dead American Crow.
Throughout these experiences, the researchers watched for what they considered appropriate reactions from the ‘alalā: calling in alarm, acting vigilant, flying, or making themselves bigger and more threatening, a behavior called “mobbing.” The approached seemed to work. Even though the crows didn’t observe the gruesomeness of an actual kill, Greggor, who oversaw the training, thinks the birds still got a frightening enough picture of a predator. In fact, during the training, one ‘alalā that had been taken back into captivity after the last release started mobbing the hawk outside their cage, which prompted a second round of training to see if other birds picked up this behavior. That time, three other crows joined in.
In the end, the group didn’t feel that a real kill would be “more effective or more reliable” than their staged attack, Greggor says. Their approach, she notes, “doesn’t mean that that’s now the perfect formula forever moving forward, but this was our attempt at taking an evidence-based approach to designing a training scenario that would be most relevant to the birds.”
Katherine Moseby, a wildlife-reintroduction biologist at Australia’s University of New South Wales who isn’t involved with the ‘Alalā Project, was encouraged to hear that Greggor and her colleagues used a live ‘io in the training. But Moseby remains somewhat skeptical of just how much of an effect simulated encounters without actual attacks can have. “Here is a question, what was the first animal that bit you?” she asks. “I bet you still remember it, even though it likely happened when you were young. Real experiences with real animals that truly scare you are likely to have stronger and longer lasting effects.”
White acknowledges that some people might think his recommendation—in which a live bird is attacked or even killed by a live predator—is cruel or drastic. “But in my opinion, it’s more cruel and more irresponsible to release captive-reared animals that have not had the best training possible,” he says. He stands by this view even if not training some of the released animals at the same time would be a more scientific way of discerning what about that training does and doesn’t work. “If we are then going to expose them to the real world, then we need to prepare them for the real world,” he says.
The ‘Alalā Project may soon have a better idea of how much its training has sunk in. Over two occasions this fall, they released 11 total ‘alalā into Pu‘u Maka‘ala Natural Area Reserve on Hawaii’s Big Island. Still, Moseby points out, “most practitioners train all of the released animals, so it is difficult to isolate the effect of training.” Not training a few of them would mean allowing some crows to encounter their first hungry hawk after flying blithely into the forest for the first time. But with less than 130 ‘alalā left on the planet, the species may not be able to afford that.


To understand the lengths Vaquita CPR has gone to rescue vaquitas from the brink of extinction, consider that the team mobilized not just fellow humans but also four U.S. Navy–trained dolphins.
Andrea, Fathom, Katrina, and Splash—chosen for their “gentle nature”—arrived in Mexico in early October. Since then, they have assisted in the delicate task of locating vaquitas, a species of porpoise of which fewer than 30 are left in the world. They are finding vaquitas because the Mexican government-led Vaquita CPR project wants to capture and breed them away from the dangers of the wild.
This is an incredibly risky plan. When they set out, no one had ever raised vaquitas in captivity or successfully mated them or even, to start, captured one alive. “Vaquitas are exceptionally shy,” says Barbara Taylor, a marine-mammal researcher with NOAA, who is also involved in Vaquita CPR. And the 30 or so surviving vaquitas might be the most elusive of all; they are the few who have managed to avoid the fishing nets that have entangled and killed the rest of their species.
In late October, the team managed to catch its first vaquita. That was the good news. The bad news was that the young vaquita quickly became distressed and had to be released. But the team tried again, and on Saturday, Vaquita CPR had caught an adult female. Things again got worse and quickly. The team decided to release the vaquita, but she died—seemingly of cardiac arrest, though it’s too early to say for sure.
The scientists always knew this was a theoretical risk, but here was an actual dead vaquita on their hands. “A devastating setback. There are no words to express how sad I feel,” Andy Read, a marine conservation biologist on the Vaquita CPR project, wrote on Twitter. If the team can’t keep vaquitas alive in captivity, then they can’t breed vaquitas. And if they can’t breed them, then the species will almost certainly die out in the wild. This might just be the end.
What makes it worse is that some of Vaquita CPR’s marine biologists had experienced this before—a fruitless field expedition, the dawning realization that it was too late—and they had vowed to never let it happen again.
* * *
When I asked Taylor to tell me the history of Vaquita CPR, she began with the story of the baiji, a rare white dolphin that lived in China’s Yangtze River. Like the vaquita, it was becoming entangled in fishing nets and dying. Through the ’80s and ’90s, the number of baiji fell precipitously, but lack of survey data obscured the extent of the problem. In 2006, scientists finally convinced a wealthy Swiss man to fund a six-week expedition in search of baiji in the Yangtze River. Taylor was on that expedition. They hoped—and this plan will sound familiar—to gather information so they could later capture baiji and breed them in a reserve before returning them to the river.
But they found nothing. They didn’t see or hear a single baiji. “This species that had been on planet Earth for 30 million years was gone in an eyeblink,” says Taylor. The team concluded the baiji was functionally extinct—the first documented megafauna extinction in half a century. Then the international media started to pay attention. The baiji “only became a story when it was gone,” another scientist on the expedition later observed to The New York Times.
It was too late for the baiji, but there was still hope for the vaquita. “We were very committed to not let that happen to the vaquita,” says Taylor. Within a year of their failed baiji expedition, many of the scientists behind what would become Vaquita CPR published a paper with a call to action, titled “Saving the Vaquita: Immediate Action, Not More Data.” In its opening line, they noted that the likely extinction of the baiji made the vaquita the most endangered cetacean remaining in the world. It was time to act.
At first, the scientists focused on getting nets out of the Gulf of California, the one place where vaquitas live. They lobbied the Mexican government to ban gill nets, which it did in 2015. But the main threat to the vaquitas now are illegal nets used to poach a fish called totoaba, whose swim bladders are a delicacy in China. The gill-net ban wasn’t enough to slow the decline of vaquita. “Once the population dropped below 100, it became evident we needed to consider radical and risky conservation actions,” says Frances Gulland, a senior scientist at the Marine Mammal Center, one of the organizations assisting in Vaquita CPR.
Captive-breeding programs have saved other species from the brink of extinction before. California condors and black-footed ferrets both rebounded from populations in the low double digits. But large marine mammals like the vaquita are much more difficult to keep in captivity. Since the death of a vaquita on this weekend, the team has made no additional attempts to capture the animals and a spokesperson says they will not for the three remaining days of scheduled field operations. It’s unclear what will happen in the future.
How We Almost Lost the Island Fox
“Everyone here recognizes it’s risky,” says Gulland about Vaquita CPR. But they went ahead because the alternative, doing nothing, would mean vaquitas dying one by one in the wild.
Gulland cited other reasons for the risky plan, too. Having Vaquita CPR’s boats in the water, she hoped, would discourage illegal nets. And if conservation is also a fight for public attention, then an audacious plan like Vaquita CPR could get more people to pay attention. “This effort to develop captive breeding has really increased awareness in the community,” says Gullan. At the very least, the vaquita will not be like the baiji: forgotten until it went extinct.


In the interest of cutting costs, airlines have taken away everything that used to be free—so the refrain goes. Gone are the meals that used to be included in the ticket price for flights that verged on mealtime hours. First checked bags started carrying an additional charge, and now sometimes you have to pay for a carry-on.
On planes, I like to joke to whoever is seated next to me that soon we’ll have to pay extra if we want oxygen masks to fall from the ceiling in case of emergency. My seatmates pretend to be listening to their headphones, because I’ve been talking a lot, but I think they get the point.
The one thing that hasn’t been taken away, I continue, is soft-drink service. And with every drink comes a three-inch-by-three-inch cocktail napkin—which, unlike pretty much anything else, the airlines really, really want to give you.
Have you ever tried to decline a cocktail napkin on an airplane?
It seems straightforward. What you do, basically, is ask for a cup of water or coffee or something, then say, “I don’t need a napkin, thank you.” I’ve probably tried a hundred times, but I’ve only succeeded a few. Surely sometimes giving me the napkin is just muscle memory on the part of the very busy flight attendant, which I totally understand. But some flight attendants look at me like I have no idea what I’m talking about, like I’ve just asked to take a drink straight from their bottle of non-dairy creamer, and hand me a napkin. Sometimes it becomes a conversation, where my sanity is called to question. “Are you sure about that? You really should have a little napkin. What if you spill? Here.”
It’s a plane, and I really might spill my drink, but what good would a cocktail napkin be? Cocktail napkins were designed to handle rings of condensation, but are wholly ineffective in spill scenarios. It’s like tossing a sponge into a pond. Yet millions are given out and thrown away every day, and in aggregate their effect on the world isn’t inconsequential.
“It’s really just how we’ve always done it,” said Ivan Noel, the president of a cabin-crew training organization called the In-Flight Institute, which staffs airlines around the world. “It’s probably a holdback from the past. I haven’t put a great deal of thought into it.”
It’s not clear to me anyone has.
“They’re light, they’re inexpensive, and they can also be used as marketing,” he continued, explaining to me that during training, flight attendants are told the logo on the cocktail napkin goes up and toward the passenger. Some low-cost carriers have begun printing actual advertisements on the napkins.
“It does have a utilitarian use, as well,” he said. “Particularly if you’re on a bumpier flight and you’re stuck in your seat, and if you don’t have a napkin on you, I would suggest that it would be uncomfortable to have fluid on your table tray.”
That’s when I would ring the call button and say there is fluid on my tray and ask for a napkin.
“That’s a very good point as well,” he said. “However, let’s say that you just have condensation.”
Commercial airplanes have plastic tray tables, which aren’t damaged by water rings. [Editor’s note: What about damage to a magazine you put on the table? JH: How much condensation do you get in airplanes? I feel like the climate control keeps it minimal. Ed.: Just looking out for magazines.] It takes concerted effort to damage a tray table, in my experience.
“Well let’s say you have a ring on your table tray, which now you’re going to put away, and it’s going to get the inside of the seat back wet,” Noel said. “That could be another way to look at it.”
The other argument in favor of the cocktail napkin is that it prevents the drink from sliding across the tray table. That’s true—assuming you don’t have one of those tray tables with a recessed well that’s made to sort of secure a cup. Like the idea of cupholders on planes is so impossible. But that’s a screed for another time.
If the idea is that the napkin prevents the cup from sliding, the scenario in which the napkin actually prevents a spill is a very rare one. The force of turbulence or a bumped tray would have to be great enough to overcome the friction of the cup-tray interface, but not great enough to overcome the cup-napkin-tray interface.
I don’t know what made me start getting anxious about wasted airplane napkins, but once I started thinking about them, they became really important. Every day, 2,587,000 people fly in and out of U.S. airports alone. I started seeing people as napkins.
Of course, the fuel that’s burned to carry all these planes and people and luggage is a vastly more significant source of environmental influence than billions of wasted napkins, so this could seem like a drop in the bucket. But since the proverbial bucket is actively overflowing at the moment, every drop counts. If every plane stopped carrying boxes of cocktail napkins, the effect on the weight of the aircraft could add up. It could mean less wasted fuel, or at least an improved ability to fly in increasingly hot climates. As The New York Times recently reported, “Hot temperatures cause air density to decrease, reducing lift and forcing airlines to either reduce weight on flights or move departures to cooler hours of the day. Experts say that will most likely pose a long-term economic challenge for airports.” A recent study in Climatic Change found that by the middle of the century, as many as 30 percent of flights will face additional weight restrictions to take off during midday heat.
So could getting rid of cocktail napkins help?
“It would be actually an interesting cost-benefit analysis,” said Noel. “You could be saving some money and space. Though it would be interesting to see how many people would then use the motion-sickness bags for their garbage or their gum—and those bags are not cheap. They’re lined.”
In economics I believe this is referred to as the vomit-bag externality. And of course one must also consider the nuanced sociocultural milieu into which a change like this is introduced.
“I would wonder how the passenger would perceive the loss of a napkin,” said Noel. “Someone who’s not thinking directly about the environmental impact might see that as just one more thing that’s being taken away.”
That’s probably true. Agency is important in a change like this. If people are asked if they need a little napkin, many would probably decline. Those same people might grumble if the expected napkin had simply disappeared.
The nice thing about life is you get to choose whether you want to see stuff like this as one more thing taken away or one fewer thing wasted.
After years of declining napkins, as I was writing this—on a plane—I was drinking coffee and the person next to me bumped my arm. I spilled just the tiniest drop onto my shirt. And I thought, you know what would be perfect right now? So I commandeered his little napkin.


The death of a big star, much more massive than our sun, usually proceeds like this: After millions and millions of years of shiny existence, the star starts to run out of hydrogen. Without this fuel, the star can’t power the nuclear fusion that produces its light. Its core shrinks and heats up, spawning heavier and heavier elements until mostly iron remains. Within a second, the core collapses and sends star material flying in a spectacular light show—a supernova—that fades after several months. The dead star leaves behind a neutron star, a very dense object, or a black hole, the light-gobbling lurkers of the universe.
What a star isn’t supposed to do, however, is stay alive.
A team of international astronomers announced Wednesday the discovery of a supernova that occurred in 2014 and continued to erupt for more than 600 days, making it the longest such explosion ever observed. The light of typical supernovae usually lasts only 100 days, and anything more than 130 days is extremely rare. When astronomers dug into archival data, they found evidence of another supernova in the same location in an image from 1954. The star responsible for both explosions, the researchers say, somehow managed to survive a blast and explode again 60 years later.
The latest supernova was detected in September 2014 by the California Institute of Technology–operated Palomar Transient Factory, which is designed to spot new objects in the sky. The explosion, named iPTF14hls, was classified as an ordinary Type II-P supernova. Astronomers at the Las Cumbres Observatory, a global network of robotic telescopes, were studying the Palomar data in their search for supernovae at their peak. They took note of it, but moved on after they saw the supernova appeared to be fading. Later, in early 2015, Iair Arcavi, an astronomer at Las Cumbres, asked an intern, Andrew Wong, to go through the whole data set again. Wong found something unusual: iPTF14hls, once fading, had flared up. He asked Arcavi if this was normal.
“I said, absolutely not,” Arcavi said. “That’s very strange.”
After all, supernovae are a one-time deal. “A supernova gets bright and then fades,” he said. “It’s not supposed to get bright again.”
Arcavi thought iPTF14hls was more likely to be a nearby twinkling star that was swept up in a survey of supernovae. He and his colleagues decided to run some forensics on the object. They used telescopes to study its spectra, which provide information about its chemical composition, the speed of the material, the age of the blast, and other important properties. The results showed that this wasn’t a healthy, living star, but a supernova. “I was really surprised,” Arcavi said. “The last thing I expected to see was a spectrum of the most typical supernova, the most boring kind of supernova you can get.”
Arcavi directed Las Cumbres telescopes to stare nonstop at the mystery explosion. Every few days, new data came back. “As time went on, it just got more and more mysterious,” he said. The supernova was challenging every known theory about the end of a star’s life that astronomers have.
Over two years, the brightness of iPTF14hls varied over time in such a way that it seemed like it was exploding over and over again. The ejected material appeared to travel at the same speed over time rather than slowing down. The temperature of the explosion remained unchanged, too. One by one, every theoretical model for star death was thrown out, unable to explain all of the explosion’s bizarre attributes. The discovery of the image from 1954 further confounded the researchers.
The astronomers eventually settled on a theoretical model that describes the potential existence of a never-before-observed event with the strange name “pulsational-pair instability supernova.” In this model, very massive stars, as much as 100 times the mass of the sun, become so hot at their cores that they convert energy into matter and antimatter. An explosion follows, ejecting the star’s outer layers but leaving its core intact. More explosions could continue like this for decades before the core finally collapses.
But there are limitations to this explanation. iPTF14hls produced far more energy than the theory predicts it should. Arcavi said every scientist to whom he’s shown the findings is stumped. “And still now, even with the paper being published, we still don’t have any theory or any model that fully explains the observations of it,” he said. It even took some convincing for Nature to publish a paper that didn’t have a concrete answer for such a significant conundrum, he said. But to truly explain the supernova, astronomers around the world need to see the data and come up with a completely new model. Perhaps someone will even discover earlier images of the supernova in their own archival data, Arcavi said.
Sarah Sadavoy, an astronomer at the Smithsonian Astrophysical Observatory at Harvard University who was not involved in the study, called the long-lasting supernova a “strange event.” The biggest mystery in the findings, she said, is the detection of hydrogen at the scene of the cosmic explosion. “Hydrogen is found in the outermost layer of these massive stars, and as such, should be lost during the first burst,” Sarah said. “This particular event is just as powerful as most other supernova of this type, so it doesn’t make sense for it to still have hydrogen.”
But cosmic mysteries, while frustrating, are a good thing. “There are many things that we still don’t understand about the explosion mechanisms of massive stars and the associated element production,” said Anna Frebel, an astrophysicist at the Massachusetts Institute of Technology who studies stars. “Finding this object appears to cause more puzzling ‘problems’ since it’s never been seen before, but it’s those kind of challenges that help astronomers understand more about the death of stars.”
Arcavi said the discovery will force astronomers to reexamine the supernovae they’ve encountered in their observations. “Anytime someone reports a supernova of this type, we have to look at it again and check if it’s not one of these weird ones,” he said. “We don’t know how many of these we might have missed.”
Some researchers are calling the star responsible for iPTF14hls a zombie star, an undead being glowing in the cosmos. Arcavi said he has some mixed feelings about the nickname. The Walking Dead and many other depictions of the undead have shown that zombies, after considerable effort, can be killed. And when they’re dead, they usually stay dead, save for, perhaps, one final gasp when the unlucky human who took it out has turned away. This star, on the hand, won’t stop dying.
The supernova has begun to fade slowly in the last year, however. Astronomers hope that as the ejected material expands, the area may become transparent enough for powerful telescopes to glimpse the heart of the explosion. Hubble, the premier star-observing space telescope, will take a look next month. The future of iPTF14hls is unclear. Arcavi and his colleagues don’t know whether the star survived the explosion.
“It is weird that the current explosion is going on for so long, but that doesn’t necessarily mean that the star is still alive,” Arcavi said. “I wouldn’t bet my car on it either way.”


At the age of 7, Hassan had already seen more than his fair share of hardship. A week after he was born in Syria, a blister appeared on his back. The doctors there diagnosed him with a genetic disorder called epidermolysis bullosa, or EB, which leaves one’s skin extremely fragile and prone to tearing. There was no cure, they said. When Hassan’s family fled Bashar al-Assad’s regime and moved to Germany as refugees, the doctors there said the same thing. Meanwhile, the blisters were getting bigger.
In June 2015, Hassan was admitted to the burn unit of a children’s hospital in Bochum, Germany. By that time, around 60 percent of his epidermis—the top layer of his skin—was gone. His back, flanks, and limbs had become a continuous landscape of open wounds, red and raw. Much of it was badly infected. The pain was excruciating. “Why do I have to live this life?” he asked his father.
Five weeks later, Hassan’s doctors had run out of options, and were planning to start end-of-life care. But after his father asked about experimental treatments, they contacted Michele de Luca, a stem-cell biologist at the University of Modena and Reggio Emilia. Over the past decades, de Luca had been working on a way of giving EB patients fresh skin. He would collect stem cells from their body, edit the faulty genes that were causing their condition, use the corrected cells to grow healthy epidermis, and graft these new layers back onto the patients.
He and his colleague Graziella Pellegrini had tried this once before in 2006, but on a smaller scale. Back then, they successfully treated a 49-year-old woman with a large EB-induced wound on her right leg. Hassan’s condition was much worse—and he was just a child. Still, there was nothing else to try.
In August, De Luca and Pelligrini got the green light to try their technique. In September, they collected a square inch of skin from Hassan’s groin—one of the few parts of his body with intact skin. They isolated stem cells, genetically modified them, and created their gene-corrected skin grafts. In October and November, they transplanted these onto Hassan, replacing around 80 percent of his old skin.
It worked. In February 2016, Hassan was discharged from the hospital. In March, he was back in school. He needs no ointments. His skin is strong. It doesn’t even itch. “He hasn’t developed a single blister,” says de Luca, who shared the details of Hassan’s story with me. “He’s gaining weight. He’s playing sports. He’s got a normal social life.”
EB has been called “the worst disease you’ve never heard of.” In the United States, it affects around one in every 20,000 people, and the many types and subtypes are caused by mutations in at least 18 possible genes. But all of these mutations have one thing in common: They impair the molecules that strengthen skin, making it extremely fragile. For some people, the symptoms are mild, while others are afflicted with massive wounds and blisters. When it comes to junctional EB—one of the most severe types, and the one Hassan had—around 40 percent of people die before adolescence.
“Whenever I speak about EB, I find myself saying: until there is a cure ...” says Kimberly Morel, who directs an EB clinic at Columbia University. “Now it seems there is more hope on the horizon for this terrible disease.”
Hassan’s treatment is “a sea change to the world of EB,” says Brett Kopelan, the executive director of the Dystrophic Epidermolysis Bullosa Research Association of America. Even though it’s only one instance of success, “it’s made such a change in that one subject that I think it’ll be very much welcome by the EB community. You’re looking generally at a desperate patient population. I have a daughter with recessive dystrophic EB. If something like this were to be commercially viable, it’s certainly something we would pursue.”
Hassan’s miraculous recovery mirrors that of two young boys who, in 1983, accidentally set themselves on fire and burned the skin off more than 97 percent of their bodies. Their lives were saved by the physician Howard Green, who had inadvertently discovered a way of regenerating skin so it could be grafted onto burn victims. Green’s technique has since saved countless lives, and fueled the entire fields of stem-cell biology and regenerative medicine.
De Luca studied with Green in his early career, and his procedure is almost identical to the one his mentor developed, except for one crucial part: He added gene therapy to the mix, modifying the stem cells behind the regenerating skin to correct the mutations behind Hassan’s condition.
“It’s a landmark in the field of stem-cell therapy,” says Elaine Fuchs, from The Rockefeller University. It also helps resolve what she calls “a brewing controversy in the field” about the exact nature of the stem cells behind the substitute skin. In some cases, burn victims who are treated with regenerated skin do really well. In others, the new skin proves to be unstable. By analyzing the DNA in Hassan’s new skin, de Luca’s team showed why: Almost all of the new cells are produced by a small but elite group of long-lived stem cells. In the future, scientists like de Luca would need to check for such cells when they grow their grafts.
A second group led by Peter Marinkovich and Jean Tang, both from Stanford University School of Medicine, have also tried to use gene-corrected skin grafts to treat four patients with a different type of EB. They replaced smaller patches of skin with variable degrees of success, but their results were promising enough to start recruiting for a larger clinical trial.
So far, both teams haven’t seen any negative side effects in their patients—a real concern, when it comes to gene therapy. Hassan’s EB was caused by a faulty version of a gene called LAMB3, and de Luca’s team used a virus to insert the normal version of the gene into the DNA of the boy’s stem cells. But if the virus inserts the gene into the wrong place, it could cause new mutations that might ultimately lead to cancer. Fortunately, when de Luca’s team sequenced DNA from Hassan’s new skin, they found that the normal LAMB3 gene had landed in safe zones. There were no signs of any potentially cancerous changes in either Hassan’s new skin cells or those from de Luca’s earlier 2006 patient.
De Luca’s team is now running two separate clinical trials to test their gene-corrected skin grafts on around two-dozen children with EB. His ultimate goal is to develop an effective and standardized procedure that could be carried out during early childhood, to prevent the painful blisters before they happen, rather than restoring lost skin after the fact. “It will take years to get there but it’s clearly doable,” he says. “Maybe this will be the last thing I’ll do in my career.”


Anna Phillips is delighted because I’ve just found her favorite parasite, which she had misplaced a year ago.
We are walking through what, at first glance, could be mistaken for an oddly macabre Italian deli. The shelves around us are full of chaotically arrayed jars, which contain what look like formless bits of meat and coiled balls of pasta. But this is actually part of the largest collection of parasites in the country, and on closer inspection, a bundle of tagliatelle is actually a tapeworm. A tangle of capellini is actually a cluster of nematodes. “You can make a lot of food references,” says Phillips. “I try to avoid that, because it ruins food for people.”
But there’s one jar that Phillips is especially keen to find, and after 20 minutes of sweeping the dimly lit shelves with the light of our phones, I spot it. It seems to contain nothing more than a lump of grisly, amorphous tissue, preserved in yellow liquid. But its label, in just a few terse and unconnected words, tells a rich story.
A century ago, the lump was once part of a zebra that was captured in Mombasa, Kenya, and imported to the United States for Barnum and Bailey's Circus. During its stay, it fell sick, and eventually died in quarantine. Upon dissecting its arteries, parasitologist H. W. Graybill found bloodworms—a common and occasionally fatal parasite of horses. Those worms—and a part of the zebra they fed upon—are now in the jar that Phillips is gushing over. She is fascinated by the creatures, but she also loves the way their stories thread through history. She picks up another jar containing a fish tapeworm that was pulled from a dog in New York in 1922. “That was someone’s pet,” Phillips says. “There are so many stories here.”
Phillips is the curator of the National Parasite Collection, a 125-year-old hoard of bloodsucking, fluid-draining, flesh-eating, mind-controlling, and host-castrating creatures that all make a living by living off other animals. These smaller members of this gruesome menagerie are housed in a room full of slides at the Smithsonian National Museum of Natural History. The bigger ones are kept in a separate building on the outskirts of Washington, D.C., which houses much of the Smithsonian’s collection.
The building’s walkways are lined with taxidermied polar bears, leopards, cougars, anacondas, and other superstars of the animal kingdom. But its real treasure is smaller and less charismatic. Put it this way: The largest museum collection in the world belongs to the Smithsonian Institution; the largest Smithsonian collection belongs to the National Museum of Natural History; the largest collection in that museum belongs to the Department of Invertebrate Zoology; and the largest collection in that department is the National Parasite Collection.
The collection was founded in 1892 by two parasitologists from the U.S. Department of Agriculture—Albert Hassall and Charles Wardell Stiles (who waged a successful eradication campaign against hookworms in the American South.) The duo began with whatever they had personally collected, but they were soon flooded with incoming specimens. Doctors would call them up with worms they had yanked out of patients. The local zoo would offer the chance to conduct necropsies on their dead animals.
At first, the collection focused on parasites that infect people and livestock, but it widened its scope after the 1940s. During World War II, U.S. troops found themselves in places with unfamiliar parasitic diseases, and scientists realized that treating those diseases meant understanding the complicated ways in which parasites travel from one host to another. They started looking at parasites in wildlife, and they began appreciating these creatures as important parts of their respective ecosystems—worthy of study in their own right.
Indeed, parasitism is the norm rather than the exception. This lifestyle has evolved independently at least 200 times in the animal kingdom, and around 40 percent of modern animal species are parasites. Most of them are small, but their numbers are astronomical and their influence enormous. In California, a painstaking census of three estuaries revealed that the local parasites weighed as much as all the fish, and outweighed the birds by up to 9 times. Meanwhile, in Japan, researchers estimate that freshwater trout get 60 percent of their energy from suicidal insects, which are compelled to drown themselves by the mind-controlling horsehair worms inside them. And all around the world, parasitic wasps and flies keep pest insects in check, to the delight of farmers and gardeners.
So the National Parasite Collection isn’t a selection of dismissible oddities—it’s really an assembly of the animal kingdom’s unsung heroes and secret overlords. And it’s perhaps one of the largest such assemblies in the world, rivaled only by those in Geneva and London. It’s hard to say exactly how big the collection is because many of its specimens are stuffed into containers by the hundreds. “When you look at a jar of hookworm larvae, it looks like jelly beans in a jar,” says Phillips, going back on her promise to avoid food comparisons. “We say there are around 20 million parasites, but we could easily say 87 million.”
Over the decades, the collection has grown haphazardly, such that samples gathered for a researcher’s dissertation might be sitting pretty next to a tray of rabbit parasites. In 2014, when the Smithsonian took charge of the collection from the USDA, its curators began reorganizing everything. That work will take another year or two. For now, the chaos is frustrating—you can eyeball a row of shelves with no idea what you’re going to find.
The chaos is also delightful for exactly the same reason. People fear parasites, avoid them, kill them. But here, among these shelves, you get to browse them. There’s a tapeworm that was extracted from a python at D.C.’s National Zoo in 1961. There’s a fish tapeworm that was wrenched from the body of an Antarctic seal in 1948. There’s a whole set of tapeworms that were pulled out of reindeer, and Phillips points to the fifth of them. “Dasher, Dancer, Prancer, Vixen, Comet ... that’s Comet’s tapeworm,” she says. “We don’t mention that on the kid tour.”
Some parasites are preserved with the tissues of their hosts, like the bloodworm that fed off the Barnum and Bailey zebra. We see part of a horse stomach with fly larvae attached to it, and flatworms clinging to a piece of cow gut. “It’s important to know how the parasite is associating with the host,” Phillips says. “But also, it would take a hell of a long time to pick each of these out.” Impatience was clearly a problem for yesterday’s parasitologists, who often shoved entire animals into jars. We see a bird, some crabs, a frog in really bad shape, a rat. “There’s something in it, I assume,” says Phillips. Her colleague Kayce Bell reads the label. “Subcutaneous nematode,” she says. “Those are fun.”
Phillips was always interested in animals, and during her college years, she applied for an internship at the American Museum of Natural History in New York. “There were projects on bees, scorpions, and catfish, and I thought, no one’s going to apply for leeches,” she says. That kicked off a career-long fascination with leeches and other parasitic worms that has taken her to six continents, and led to some memorable discoveries. In 2010, she described a new two-inch-long species of leech that had been found inside the noses of three Peruvian children. She called it Tyrannobdella rex—the tyrant leech king, or T. rex for short.
To the study of parasites, Phillips has often given blood, sweat, and tears—sometimes, very literally. For example, she collects leeches “in the usual way”—by finding the right watery habitat, donning shorts, wading in, and making a lot of movement. The leeches come to her. When one bites, she has to lift her leg very slowly because some of them will drop off if they leave the water too quickly. “This is why I do yoga,” she tells me.
Leeches are exceptional, though. In most cases, to collect a parasite, you need to collect its host. Kayce Bell studies the parasites of mammals, and she’s spent a lot of time trapping chipmunks. After euthanizing the animals, she plucks them of fleas, mites, ticks and lice, and then systematically dissects them, scanning every gastrointestinal organ for tapeworms, flukes, and more. Each rodent is more than just an individual: It’s a world, ripe for exploring. “I can do a chipmunk in about 30 minutes,” she says.
This work matters, because parasites are ecologically vital, and poorly studied. And much like the rest of nature, they’re in danger. As hosts die, so do their parasites—and sometimes, conservationists deliberately usher the latter into oblivion. In one notable case, the same people who saved the Californian condor from extinction also wiped out the harmless condor louse, by delousing the last surviving birds.
Humans can also make things harder for parasites in subtler ways. Many of these creatures have free-living stages, where they travel through the world in search of hosts. On these journeys, they’re vulnerable to the same changes in temperature and rainfall that are affecting the entire planet. Global climate change is also forcing animals to move into new areas; that could be a problem for the parasites that depend on those animals, especially if they have complex life cycles that involve several hosts.
Phillips and a team of like-minded scientists recently used the National Parasite Collection to simulate these changes, by looking at how the whereabouts of parasite groups have shifted over time, and how those ranges relate to climate. They estimated that between 5 and 10 percent of parasite species will go extinct by 2070 because of climate change, as will 30 percent of parasitic worms.
These are conservative figures, born of research that has been heavily skewed toward North America, says Carrie Cizauskas from the University of California, Berkeley, who was involved in the study. “There’s hardly any data across Africa. There’s very little parasite research in global-biodiversity hot spots, which are also likely to be parasite-biodiversity hot spots.” With a more detailed portrait of our parasitic planet, Cizauskas says that scientists can work out which groups are most in need of saving.
Save the Parasites (Seriously)
The very concept of saving parasites is so counterintuitive that Cizauskas and her colleagues found it hard to publish their studies on parasite extinction risk. Reviewers would say, “Why should we conserve these things that everyone thinks are gross?” This attitude seems especially trenchant, but as I’ve written before, history suggests that it can change. Microbes were once seen as germs; now, we’re starting to appreciate how important those in our bodies are for our health. Top predators were once seen as competitors or trophies; now, conservationists talk about protecting or even reintroducing them. Parasites could get the same reputational makeover.
“People ask me if I dreamed about being the curator of the National Parasite Collection when I was five,” Phillips says. “No, not at all. But dreams change over time.”


It’s official. When it comes to climate change, there’s now literally everyone else—and then there’s the United States.
Syria, the last remaining holdout from the Paris Agreement on climate change, announced at a United Nations meeting in Germany Tuesday that it will sign the agreement. The Syrian Arab News Agency, a state-sponsored news outlet, also reported that the country’s legislature voted to accept the agreement last month.
Its declaration means that the United States is the only country in the world that has rejected the treaty and promised to withdraw from it.
If the news isn’t exactly pleasant for the Trump administration, which announced the intent to pull the United States out of the Paris Agreement in June, it’s also something of a poor advertisement for the treaty itself. That Syria—war-torn, war crime–committing Syria—has acceded to the Paris accord does not make an obvious case for the United States doing the same.
At the same time, Syria is committing to Paris now because 195 other countries have already signed on. In Britain, France, Germany, Japan, and South Korea, the Paris Agreement is considered a relatively uncontroversial international achievement.
“With Syria on board, now the entire world is resolutely committed to advancing climate action—all save one country,” said Paula Caballero, a climate-policy specialist at the World Resources Institute. “This should make the Trump administration pause and reflect on their ill-advised announcement about withdrawing from the Paris Agreement.”
“Syria’s participation puts an exclamation point on the fact that the U.S. actions are contrary to the political actions, and the sincerely held beliefs, of every other country on the face of the Earth,” said Michael Oppenheimer, a professor of geosciences at Princeton University and a longtime observer of UN climate negotiations.
The United States is “the only powerful country” that has disavowed the treaty, he said. “And that was the case from the day it withdrew.”
“I find it ironic that the government of Syria would say that it wants to be involved [in the Paris Agreement] and that it cares so much in climate and things like CO2 gases,” said Heather Nauert, a spokesperson for the State Department, at a briefing on Tuesday. “If the government of Syria cared so much about what was put in the air, then it wouldn’t be gassing its own people.”
The People’s Council of Syria may not have made the decision to enter the Paris Agreement in the first place. Syria has been engaged in a horrific civil war since 2011, and the areas under government control are tightly ruled by President Bashar al-Assad. The United Nations has implicated Assad in war crimes, including sarin-gas attacks on Syrian children. Assad’s family has run the country since 1971.
Syria has not yet submitted a plan to reduce its greenhouse-gas emissions, as the Paris Agreement requires. In fact, preparing a plan to emit less carbon pollution is just about the only thing the Paris treaty requires.
“I don’t know what the reasons are that Syria chose to focus on this now. It’s got enough problems and doesn’t want to be seen as an outlier in any other way,” Oppenheimer told me.
Its acceptance of the accord can also be seen as a kind of geopolitical troll. Since Nicaragua signed onto Paris last month, Syria was the only remaining country left out of the Paris process. Nicaragua gets almost all of its energy from renewable sources, and it declined to join the treaty in 2015 because it said the accord did not go far enough.
Some analysts have argued that Syria’s decision to join the Paris treaty shows that the agreement is a weak or toothless document. The Paris Agreement, after all, sets few limits on its signatories. Most importantly, it allows countries to set their own emission-reduction goals, rather than imposing them as part of the treaty text.
The United States has never taken issue with the treaty’s toothlessness. In fact, the Paris accord adopts a “bottom-up” strategy in part because the United States has long insisted on it. In 2007, President George W. Bush’s administration began arguing for a “pledge-and-review” climate treaty, similar to the one adopted in Paris. Half a decade later, the Obama White House and the United Nations used a broader version of that plan as they began work on what became the Paris Agreement.
Trump’s critiques of the agreement have focused on its alleged strength. In June, the president said that his “solemn duty to protect America and its citizens” required him to withdraw from the agreement. He and other officials have insisted, to the contrary, that the document is impossibly strong and that it benefits China and India at the expense of the United States.
“We intend to withdraw from the Paris Agreement as soon as we’re eligible to do so, unless the president—and he’s been very clear about saying this—unless he’s able to identify terms of engagement that he feels are more favorable to American businesses, workers, and taxpayers,” Nauert said.
Under the original terms of the Paris Agreement, the United States can announce a new greenhouse-gas reduction plan at any time, and it can elect to remain in the treaty at any time up to 2020. Trump has so far not used this power. Roughly seven out of 10 Americans want America to remain in the Paris accords, according to a Yale University poll conducted earlier this year.
Syria’s accession to the treaty is the first major news to emerge from this year’s UN climate talks, which are being held in Bonn, Germany, this month. The United States cannot formally withdraw from the Paris Agreement until 2020, so a team from the State Department is attending the talks. But experts say its negotiating power will be undercut by America’s looming withdrawal from the process.
“The United States stepped aside from fighting at the beginning of two world wars,” Oppenheimer said. “Then it found it had to join the fight, and it led the fight, and it saved the world from terrible calamities. The United States is going to have to join this fight, too, eventually, and it will have to lead the world in the direction of solving a terrible calamity.”
“The Paris Accord isn’t strong, but it’s a way forward. The world is better with it than without it,” he continued. “I think we’re going to cut emissions, and we’re going to get away from fossil fuels. It’s just a matter of how much damage we bake in in the meantime.”


With the recent drought in the American West, ranchers may be forced to find more sustainable options. And some researchers are looking to a breed of cattle from the past to find the answers.
Jon and Susie Eickoff are used to working together and going home together. Jon says that over three decades they have only been apart a couple of times.
“Except for a business trip to Houston and a stint that I spent in the hospital for 10 days, I can say that in 30 years those are the only times we have not been together,” said Jon Eickoff.
That stint in the hospital was due to a heart attack Jon suffered while being the CEO of a fast-paced, successful engineering-services company that he managed with Susie. The couple say they decided just years afterward that it was time to leave the “pressure-cooker” career and trade it in for a slower-paced ranching life near Faywood, New Mexico.
“Being out here in a beautiful place with beautiful scenery and around some nice docile animals, it’s a nice place to be,” said Jon Eickoff.
Those animals are criollo cattle.
Susie Eickoff says while trying to decide on a cattle breed, they watched as ranching friends struggled during New Mexico’s recent drought.
“Friends of ours had a large cattle ranch and they were selling off their cattle because there was no grass for the cattle, there was no water for the cattle, it was very, very severe on them,” she said.
The Eickoffs say they came across research being done on criollo cattle at the U.S. Department of Agriculture’s Jornada Experimental Range that showed the breed could adapt to the arid desert environment in southwest New Mexico.
Susie Eickoff says they read up on a lot of the studies being done on the criollo before selecting a breed that has been in the region for hundreds of years.
“This is a sustainable animal. It requires less food, less supplementation, less water, all of those things that the British breeds require more of,” she said.
She says that once a rancher goes through the struggles of drought, it’s very difficult to go through it again.
“They can survive these droughts and you know, it will get us through the tough times,” she said.
Jon Eickoff says that they are lucky to have 141 acres of land with irrigable water.
“Water is a premium in this part of the country,” he said.
How much of a premium? Research published in the peer-reviewed journal Environmental Research Letters found one in 30 wells in the West is probably dry. The study by Stanford University and the University of California, Santa Barbara, looked at data in 17 Western states covering more than 2 million groundwater wells.
In eastern New Mexico, there is concern in Curry and Roosevelt Counties about increased use of the Ogallala Aquifer.
Alex Rinehart, a hydrogeologist with the New Mexico Bureau of Geology and Mineral Resources, says he worries about the lack of recharge to the aquifer.
“Even if the drought reverses and we end up in wet period for a while, it’s not really going to change anything,” said Rinehart.
Rinehart says officials are looking into piping in water to the area as a solution.
As for a possible solution for ranchers facing drought, researchers at the USDA’s Jornada Experimental Range north of Las Cruces continue their work on the criollo.
Alfredo Gonzalez, an animal scientist at the range, drives out to check on a group of criollo cattle on the range in a pasture that he says is around 6,000 acres.
Gonzalez is comparing how this group of criollo compares to a group of Angus and Hereford crossbred to see how the cattle respond to the desert temperatures.
“They seem better adapted to the hot temperatures,” said Gonzalez.
Gonzalez says not only does the criollo thrive in the desert temperatures, but by monitoring the cattle with GPS tracking collars, they found the breed can travel further from water.
“We’ve found that they go days more without water, again, they are responding by going out further and so they distribute a lot better than the Angus. So those are things that the researchers are continuing to collect data on the breed,” said Gonzalez.
That data also includes understanding the criollo’s diet better. He says the breed is helping restore grasslands that have disappeared due to overgrazing in the state.
“So with them using these areas a lot better, a lot more diverse plants, [then] they’re utilizing water in a better manner,” Gonzalez said. “I think this is good for the remediation of our grasslands.”
The Eickoffs say the breed of cattle has improved the grasslands on their ranch.
“Because they’re grazing constantly, they’re spreading seeds for us, they’re fertilizing seeds, even with their small hooves they’re doing very little damage to what’s here,” said Jon Eickoff.
The Eickoffs also say criollos birth more easily—they’ve never had to pull a calf—and the breed seems to be great at avoiding disease. It’s another insurance policy for this three-person ranching operation facing conditions that ranchers in the West continue to deal with.
“I’m not a pessimist, I’m just a disillusioned optimist. It’s really difficult to look into the negative side of things, but if it does go negative, I’m sure going to be glad there is going to be criollos around,” said Jon Eickoff.
This post appears courtesy of High Country News.
This story is part of the State of Change project, produced in partnership with the Solutions Journalism Network.


Every big, ambitious project has to start somewhere, and for U.K. Biobank, it was at an office building south of Manchester, where the project convinced its very first volunteer to pee into a cup and donate a tube of blood in 2006.
U.K. Biobank would go on to recruit 500,000 volunteers for a massive study on the origins of disease. In addition to collecting blood and urine, the study recorded volunteers’ height, weight, blood pressure; tested their cognitive function, bone density, hand-grip strength; scanned their brains, livers, hearts; analyzed their DNA. In breadth and depth, the study is the first of its kind.
Handling all the samples was a logistical challenge. To process thousands of tubes of blood, for example, U.K. Biobank’s lab needed a new robotics system. (This ultimately came from a company that builds machines for packing sausages, not unlike tubes of blood in shape.) Each tube of blood was split into its component parts—red blood cells, white blood cells, plasma—and run through a battery of tests. White blood cells contain DNA, which the project had analyzed, too. When all was said done, U.K. Biobank had assembled one of the largest single genetic data sets ever. It all took a while.
This spring, 11 years after the first volunteer gave up a tube of blood, U.K. Biobank announced it would release its full genetic data set to registered scientists in July. This huge amount of genetic information, combined with the thousands of other characteristics tracked by U.K. Biobank, allows scientists to look for the genetic determinants of virtually any disease. Geneticists marked their calendars. “We heard stories that people who head groups had canceled holidays,” says Jonathan Marchini, a statistical geneticist at the University of Oxford. “Everyone has been waiting for this for so long.”
U.K. Biobank had done data releases before, including an earlier subset of the genetic data set with just over 100,000 people. In the past, research groups using the data wrote up their papers, submitted to journals, waited for peer review, and eventually their papers trickled out to the public. In the last year, however, an increasingly popular website called bioRxiv—pronounced “bio archive”—has changed the game. BioRxiv allows biologists to publish preprints, or preliminary drafts of their papers that have not yet been peer-reviewed.
Preprints based on the latest U.K. Biobank data started to come out almost immediately. Within two weeks, David Howard and Andrew McIntosh, psychiatry researchers at the University of Edinburgh, had posted not one but two preprints, one on genetic variants linked to depression and the other to neuroticism. Their team subsisted on pizza and worked “constantly.”
Others soon followed, and the flood of preprints has continued ever since. Never had genetics research moved so fast.
* * *
Ask scientists what’s so revolutionary about U.K. Biobank and they’ll say it’s big. But they’ll also say this: Nobody gets preferential access.
In the past, research groups that had gone through the trouble and expense of building DNA data sets have hoarded it for themselves, so that they could be the first to mine it for publishable insights. U.K. Biobank, however, is supported by the United Kingdom’s National Health Service. Its data is open to anyone in the world, as long as they are a legitimate researcher and pay a fee commensurate with the amount of data they want to access—a couple thousand dollars for the full genetic data.
When it came to releasing the 500,000-person data set, making sure everyone got the huge file (12 terabytes uncompressed) at the same time was no trivial matter. U.K. Biobank decided to allow registered researchers to start downloading the data weeks before its official July release. The catch: It was encrypted. The decryption keys went out to all research groups simultaneously on the official release date. Nobody got a head start of a few days, or even a few hours. Even Marchini, who helped U.K. Biobank process some of the data, was not allowed to analyze it for his own research purposes until it was available to all.
“The vision for providing the data to any bona fide researcher without preferential access was really a game changer,” says Manny Rivas, a biostatistician at Stanford University. Rivas, who is an assistant professor, noted it is a real boon for junior faculty, who haven’t had years to amass their own data. The availability of a data set as rich and deep as U.K. Biobank democratizes genetics research.
On top of this shared data set, several research groups have now built freely available tools to help other scientists make use of U.K. Biobank’s data. Marchini’s group made a web browser dedicated to parsing genetic and brain data from U.K. Biobank. Albert Tenesa, from the University of Edinburgh, created GeneATLAS, which accounts for family members in the database, the presence of whom usually screw up the math used to find links between genetic variants and disease. Rivas made the Global Biobank Engine, which is essentially a search engine for genes potentially associated with any disease. The Global Biobank Engine, in turn, is partly based on calculations done by Ben Neale, a geneticist at the Broad Institute, who looked at nearly 2,500 traits and disorders and how they corresponded with genetic variants in the U.K. Biobank.
(Unlike U.K. Biobank’s full data, these tools are accessible to anyone with an internet connection, but they show only aggregate data, so study participants should not be individually identifiable.)
In the past, looking at how a single trait corresponded with a set of genetic variants could be a paper in itself. It’s called a genome-wide association study, or GWAS. Neale’s group did 2,500 GWASs in a single day—and he didn’t even bother to write a paper. It’s a blog post on his website. Neale says it didn’t quite feel like a discrete journal article. It’s more a starting point for scientists interested in specific genes or traits. He’s since heard from both pharmaceutical companies and academic researchers using his GWAS data.
Tenesa, who uploaded a preprint describing GeneATLAS on bioRxiv in August, says he has also heard from a couple dozen researchers using the tool. Some have asked him to run calculations for specific traits. This is happening as he’s still working to publish a paper about GeneATLAS in an official journal. It’s the way things are now. “When I get my email from Nature Genetics these days, and they tell you what papers have just been published, I’ve often seen the papers nine months earlier on bioRxiv,” says Marchini.
But is there such a thing as too fast? Jeffrey Barrett, a geneticist at the Sanger Institute, has cautioned against hastily posting preprints based on a quick GWAS. “I understand why,” he says. “It’s the quickest way to get out the stamp that you’ve done this analysis first.” But it’s easy to miss possible artifacts or mistakes in a data set as big and complex as this one. And now that it’s easy to identify genetic variants linked to a disorder, says Barrett, simply enumerating the variants doesn’t add much value. U.K. Biobank has made genetics research easier, but it is also raising the bar.
Huge DNA Databases Reveal the Recent Evolution of Humans
To publish, researchers increasingly will have to tease out how a genetic variant found via GWAS may be causing a disease—perhaps by tracking how it’s expressed in different parts of the body or sequencing the gene. U.K. Biobank did not fully sequence the DNA in its blood samples, which would have been far too expensive; it used a technique called genotyping that spot-checked 820,967 sites in the genome. In March, though, it signed a deal with the pharmaceutical companies Regeneron and GSK to sequence the DNA of everyone in the study. This deal gives the pharma companies exclusive access to the sequences for nine months—a change in access policy. Eventually, the data will be available to the wider research community.
U.K. Biobank is still following its 500,000 volunteers, and will continue to do so for many years as they age. The technology available to scientists will advance over time, too. When planning for the study was going on, says U.K. Biobank’s principle investigator Rory Collins, studying 820,967 genetic markers for 500,000 people seemed unlikely: “No one envisaged that being possible so soon.” A decade on, any scientist in the world can do it.


Climate change is real. It’s caused by greenhouse-gas pollution released by human industrial activity. Its consequences can already be felt across every region and coastline of the United States—and, unless we stop emitting greenhouse gases soon, those consequences will almost certainly get worse.
 Those are the headline findings of the Climate Science Special Report, a sweeping and more than 800-page examination of the evidence. The report was published Friday by four agencies of the U.S. government and academics from across the country.
Their conclusions form the first volume of the new National Climate Assessment, a report on the science and impacts of global warming that Congress requires agencies to complete every four years. A draft version of the second volume, on the human impacts of climate change, was also released Friday.
“This is the most comprehensive assessment of climate science currently available in the world, and it reaffirms what we’ve already known,” said Robert Kopp, one of the lead authors of the report and a professor of climate science at Rutgers University. “If we want to do something like stay under 2 degrees Celsius of warming, the window to do that is closing in the next couple decades.”
The two-degree limit is a rough target used by the United Nations to signal the point where dangerous climate change could begin. The report finds that the world can only continue to emit carbon for roughly another 23 years at current levels before it will have a more than two-thirds chance of going over the limit.
The Climate Science Special Report comes at an auspicious time in the history of global warming and the United States. The report’s conclusions do not deviate wildly from the last 20 years of consensus in climate science. They do not shock anyone who follows the field. And they don’t break new ground: The authors have synthesized the best available papers; they have not conducted new research for this report.  
But simply by affirming the science of climate change, the authors—and the interagency bureaucrats who shepherded the writing of the document—provide a contrast to the actions and statements of political figures in the Trump administration. Scott Pruitt, the director of the Environmental Protection Agency, has cast doubt on the idea, fundamental to climate science, that the amount of carbon dioxide in the atmosphere significantly controls Earth’s climate.
President Trump and Pruitt have dismantled the aggressive policies advanced by President Barack Obama meant to reduce U.S. carbon pollution. In October, Pruitt repealed the Clean Power Plan, which would have reduced greenhouse-gas emissions from the power sector. In June, Trump withdrew the United States from the Paris Agreement—and, with it, pulled back from Obama’s decision to apply the full force of the U.S. foreign-policy apparatus to reducing carbon pollution worldwide.
The assessment is a scientific achievement by itself. The last major synthesis of climate science, as a field, was published in 2013 by the UN Intergovernmental Panel on Climate Change. Much has changed over those four years, including significant updates to how the field understands the interaction between global warming and hurricane strength.
It is also the end result of a colossal amount of work. The National Academy of Sciences and the U.S. Global-Change Research Program both convened expert panels to comb through the report line by line and subject it to meticulous comment and approval. During the writing process, scientists from four federal agencies—including NASA, the National Oceanic and Atmospheric Administration, the Department of Energy, and the Environmental Protection Agency—contributed significant writing and expertise. They were joined by researchers from the U.S. Army Corps of Engineers and 11 academic institutions, including Columbia University, Texas Tech University, and the Naval Postgraduate School. It is not an overstatement to say that a vast swath of the profession of American climate science played a hand in this report.
Its conclusions span the Earth system. Modern climate change is primarily caused by the release of carbon dioxide and other gases into the atmosphere, which prevent the sun’s heat from escaping back into space. Over the past several decades, this heat has accumulated in the atmosphere and oceans. Now, the United States sets many more extreme heat records than extreme cold records; it sees more intense heat waves and weaker, briefer cold snaps.
That heat is shrinking the ice sheets at the planet’s north and south poles and causing the oceans to expand. The report includes forecasts of how much the sea level will rise around the world. The sea level worldwide has risen by about seven or eight inches since 1900, with three of those inches coming in the last 25 years. This will intensify: The world ocean is almost certain to rise one to four feet by the end of this century. And if some of the fastest scenarios for the melting of the Antarctic come to pass, then the sea could rise as much as eight feet.
The report sketches out what that sea-level rise will do to different regions of the United States. The East Coast will likely experience even more of that sea-level rise than the world average. The West Coast will likely experience less—unless Antarctica begins to melt in its entirety, in which case it will also see above-average rise.
It also dives into “potential surprises” that the United States may encounter in a climate-changed world. In that chapter, which previous National Climate Assessments did not include, the authors warn of “compound extremes,” the risk of multiple unusual weather conditions coming to pass at the same time. For instance, scorching heat and a lack of rain during the same summer will intensify a drought (and increase the chance of wildfires) far more than either one would alone. If excess rain falls on waterlogged ground, then the chance of a devastating flood also rises. It’s hard for scientists to predict how the various extremes of climate change will overlap with each other.
The authors caution that current climate models are more likely to underestimate future warming than overestimate it. While climate models have accurately predicted the past few decades of warming, they struggle to describe warmer climates that occurred millions of years ago. Models suggest that these climates should be colder than archaeological and climatic evidence tells us that they were, which means that they may fail to capture how warm Earth can get. There may be tipping points in the climate system—difficult-to-predict points of no return—that researchers may not fully understand.
Environmentalists and some government scientists had worried that the Trump administration would try to suppress the release of the report. But some of the authors, speaking anonymously so as not to distract from the release, said they saw little evidence of political interference during the writing process.
Some references to the Paris Agreement were removed from the final report, as compared to a draft version leaked over the summer, sources indicated. Environmental groups plan to conduct a line-by-line comparison of the final report with the leaked drafts in the coming days.


Leslie Bolen was there for every one of her son Michael’s medical tests. He’d had more than his fair share in his 14 years, but this last one was almost too much for his mother to bear.
That test was on a Saturday night in April 2016. Eight days earlier, Michael had had a seizure at the residential facility where he lived. He had autism, and he’d had seizures almost every week since he was 8 years old, but never one this bad. He stopped breathing, and his heart stopped beating. Medics managed to restart his heart with a shot of adrenaline, but his brain never recovered.
Michael spent the following week on life support. His parents rarely left his side, ducking out of the hospital only three times for clean clothes and to spend time with their then-13-year-old daughter, Rachel. Sitting in a chair beside Michael’s bed, Bolen played back events from the previous Friday in her mind: the frantic call from Michael’s caseworker, the seemingly endless 10-minute drive to the hospital, the look on her husband Chad’s face when he arrived at the hospital.
As part of the final test, doctors pinched Michael’s chest and poured ice water in his ear. Michael showed no response to these normally painful stimuli, confirming that he would never regain consciousness. For some of that time, his mother had prayed in the next room; heartbroken and exhausted, she focused on practicalities: She and her husband had already decided that they would donate his organs, because Rachel had benefited from a corneal transplant as an infant. But what about Michael’s brain?
Bolen Googled “autism brain donation” on her phone and dialed the number on the screen. About 16 hours later, she joined the ranks of more than 350 families that have donated a loved one’s brain to autism research. “Michael died for a reason. I have to believe that, otherwise I can’t go on; maybe this was [the reason],” she says.
These decisions, however painful for families, are invaluable to researchers. Much of autism research has relied on mice, whose behaviors and brains are a far cry from those of people. Studying postmortem brains has revealed the pathological hallmarks of many neurological conditions—including Alzheimer’s disease, Parkinson’s disease, and multiple sclerosis. Postmortem autism studies present unique challenges, however. Autism is much more heterogeneous than most neurological conditions. Researchers don’t yet know of any obvious features that distinguish the brains of people with autism. Whatever is present is likely to be subtle, and to vary from brain to brain. “You can be sure that every case is different,” says Patrick Hof, director of the Seaver Autism Center Tissue Program at the Icahn School of Medicine at Mount Sinai in New York. “If you want to look for neuropathology and find patterns of cell pathology in these brains, you’re going to need numbers.”
Finding brains for autism research has another, thornier complication. Whereas most people with Alzheimer’s or Parkinson’s diseases die in their 70s or 80s, autism is a developmental condition: The brains of children are most pertinent to research, but those who die usually do so unexpectedly, after drowning or having a seizure, for example. Getting enough numbers to definitively tie any feature to autism means reaching out to families in their darkest hour. A brain that sits unpreserved after death rapidly decays: After 24 hours, genetic messages inside the tissue start to fade; after 36 hours, the walls of each cell start to crumble. Depending on where the family is, and their proximity to a medical examiner and a tissue bank, there might not be a minute to spare.
Given all of these difficulties, there have been few brains available for autism research. In 1998, the advocacy group Autism Speaks launched the Autism Tissue Program—an effort to collect and distribute brain tissue for autism research. In 2013, the National Institutes of Health launched its own brain bank, the NIH NeuroBioBank, to collect brains for studying a wide range of conditions, including autism. But the two efforts had different protocols for processing and distributing samples. Neither had a wide reach across the United States, which meant some families were unable to donate even if they wanted to. “A lot of times, we would get brain authorization and not have a research home for it,” says Jason Bridge, vice president of eye and tissue services for Unyts, an organ-procurement organization based in Buffalo, New York.
New initiatives in the United States and Europe aim to boost the number of autism brains by streamlining the donation process. They try to engage families long before death occurs, and they are working with hospitals, organ-procurement organizations, and medical examiners around the country to reach families immediately after the loss of a loved one. “People have to know us and be convinced that this is a good thing to do,” Hof says.
* * *
The call for autism brains grew particularly urgent five years ago. In May 2012, a freezer at Harvard University containing about 50 specimens from the Autism Tissue Program—roughly one-third of the collection—malfunctioned. The freezer had two separate alarm systems, neither of which activated when the temperature began to climb above -80 degrees Celsius. The contents slowly thawed, destroying the cells in the tissues and degrading their scientific value. Scientists realized they needed better procedures to protect and distribute this limited resource.
“My first thought really was, ‘Oh my gosh, this is a tragedy,’” says Eric Courchesne, a professor of neuroscience at the University of California, San Diego. “You can always collect new DNA samples, you can always collect new RNA, you can always do new brain scans,” he says. But a physical brain? “It’s irreplaceable.”
The living brain is guarded by three layers of mesh-like tissue called meninges, the hard bones of the skull, and a thin cushion of fluid in between that helps absorb shocks. But these protections were no match for Michael’s self-injurious urges. He once smashed his head against the windshield of his school bus so hard that the glass fractured into a spiderweb of cracks.
Michael was diagnosed with autism at 21 months. Bolen, who has an older son from a previous relationship, knew something was off when Michael swapped babbling and pointing for spinning and “stimming”—self-stimulating behaviors. He loved to watch liquids seep into the carpet or couch, and so he would sometimes empty jugs of milk or orange juice or urinate right in the living room. Within a year of his diagnosis, he began hurting himself, at first slapping his head and then banging it against the wall or the floor.
Michael’s self-injury worsened as he grew. When he was 6, a neurologist ordered brain scans, which revealed that part of Michael’s brain extended down into his spinal column. This “Chiari malformation” may have been contributing to his dangerous behavior, the neurologist said, and recommended surgery to fix it. Before opting for surgery, the Bolens tried giving Michael risperidone, a drug that can ease aggression and self-injury in children with autism. It helped for a few weeks, but then the effects waned. Boosting the dose restored the drug’s effects temporarily, but within five months they had reached the maximum dose.
About a year after Michael was diagnosed with the Chiari malformation, he underwent a three-and-a-half-hour procedure to remove a small piece of his skull and a piece of a vertebra to ease the pressure on his brain. At the same time, the surgeon repaired a portion of his dura, the outermost layer of the meninges. Dura should be tough, but the surgeon reported that Michael’s, for some reason, had the consistency of wet newspaper.
Even so, the surgery seemed to go well. Within 24 hours, though, Michael developed meningitis, a rare infection of the meninges. He recovered, but 10 weeks after the surgery, he began to have seizures. At first, they were sporadic, but within two years Michael was having a severe seizure every week. He would suddenly stop moving and then slowly begin to circle the room in a trancelike state. His lips would sometimes turn blue, signaling a lack of oxygen. Doctors suggested giving him Diastat (diazepam)—an anticonvulsant drug—if a seizure lasted more than five minutes. Even with the medication on hand, his mother sometimes panicked and called 911.
In June 2014, when Michael was 13 years old, he had his first convulsive seizure. His father was just drying him off after a shower when his 180-pound body went limp. His parents realized they could no longer keep him safe at home. That October, Michael moved into a residential facility in Media, Pennsylvania, a 10-minute drive from their house. Michael had his final seizure 18 months later.
When Michael died, Bolen’s Google search led her to Autism BrainNet, a network of brain banks set up in 2014 to preserve the brains of people with autism for research. (Autism BrainNet is funded by the Simons Foundation, Spectrum’s parent organization.) She called the network’s main number, and the organization worked with a local agency to collect Michael’s brain and ship it to Hof’s lab at Mount Sinai.
* * *
Brain banking is a finely tuned science, aimed at maximizing the research potential of each precious sample. Each brain is first dissected into its two halves, or hemispheres. One hemisphere is then cut into slabs about 1 centimeter thick; the slabs are quickly frozen in liquid nitrogen and stored in labeled bags. These can be kept in a deep freezer for years, if not decades.
The other hemisphere is typically submerged in a pungent formaldehyde bath that hardens the tissue and fixes each cell in place. Some brain banks keep this fixed hemisphere whole. Others cut it into slabs of tissue, also about a centimeter thick. They can store these slabs in tubs of formaldehyde in a walk-in refrigerator or embed them in paraffin wax and maintain them at room temperature. They can later shave the waxed blocks into delicate slices for inspection under a microscope as needed.
Imaging studies can provide clues to when and where brain development is altered in autism, but only postmortem tissue can reveal how development goes awry. “You have to be looking at the individual cells in those brain regions that appear to be developing differently,” says Cynthia Schumann, an associate professor of psychiatry and behavioral sciences at the University of California, Davis, MIND Institute.
One of the first autism studies of postmortem brain tissue, published in 1986, analyzed tissue from four males with the condition, ranging in age from 10 to 22 years. The researchers focused on the cerebellum—a wrinkly lobe at the back of the brain that helps to coordinate movement, among other things—and found a dearth of a type of neuron called Purkinje cells. The results prompted Courchesne to wonder whether the same feature would be visible in brain scans from people with autism. Using magnetic resonance imaging, he confirmed in 2001 that a wormlike structure known as the cerebellar vermis, which is normally rich in Purkinje cells, is smaller in people with autism than in controls.
Courchesne also showed that young boys with autism have an unusually large prefrontal cortex—a brain region involved in attention, decision-making, and social behavior. The scans hinted at an early overgrowth of neurons in the prefrontal cortex. To confirm this, his team manually counted neurons in samples of brain tissue from seven boys with autism and six controls. They discovered in 2011 that the boys with autism had, on average, 67 percent more neurons in the prefrontal cortex. “We were astonished,” Courchesne recalls. Neurons in the prefrontal cortex develop during the first and second trimesters of pregnancy, so the findings provided some of the first evidence that autism originates before birth.
Brain tissue has also shed light on which genes are involved in autism. A 2011 study found, for example, that brains from people with autism show an increased expression of immune genes and diminished expression of genes involved in neuronal connections. A 2014 study with more brains, including some used in the 2011 study, confirmed these findings. The team behind the first study later discovered that people with classic autism share patterns of brain-gene expression with those who have dup15q duplication syndrome—a rare genetic condition associated with autism. In a separate study, they showed that the brains of people with classic autism and those with dup15q syndrome bear a similar pattern of chemical tags that affect gene expression.
Together, the studies hint at a common molecular signature across multiple forms of autism, regardless of the cause or clinical features. “I don’t think we would have been able to find that result without looking at gene expression in autistic brains,” says Dan Arking, an associate professor of medicine at Johns Hopkins University in Baltimore, who led the 2014 study.
* * *
The tissue for Arking’s analysis came from two brain banks: one in Maryland that is now part of the NeuroBioBank, and the one at Harvard University. Arking says he could tell the Harvard tissue was of poorer quality, thanks to quality-control steps he included in his analysis. “We could, based on our data, go back and identify which brain bank the samples came from,” Arking says. He suspects the Harvard freezer may have been warming for months, explaining the low quality of some of the specimens they had received.
Two years after the freezer malfunction, in 2014, several nonprofit organizations came together to form Autism BrainNet. The idea had been in the works before the malfunction but quickly gained momentum, says David Amaral, the director of Autism BrainNet and a distinguished professor of psychiatry at the MIND Institute. (Autism BrainNet was jointly funded by the Simons Foundation and Autism Speaks initially, but the Simons Foundation became its sole funder in 2016. Another nonprofit organization, the Autism Science Foundation, coordinates outreach.)
Autism BrainNet has 164 autism brains from the Autism Tissue Program. As of October, it has collected an additional 41 autism brains, 56 control brains, and 13 brains from people with other genetic conditions, including dup15q syndrome. It has five collection “nodes”—teams trained to process brain tissue—that partner with local hospitals, organ-procurement programs, and medical examiners, who each raise the possibility of brain donation with grieving families. To reach families earlier, the program runs a campaign called “It Takes Brains.” Families can register regardless of whether any members have an autism diagnosis because having control brains is important for research. Autism BrainNet also works closely with advocacy groups for related syndromes.
“We’re all hoping to find a common cell-system defect,” says Carol Tamminga, the chair of psychiatry at the University of Texas Southwestern in Dallas and director of the Texas node of Autism BrainNet. Finding that common alteration could point to treatments for multiple types of autism, which is most families’ primary motivation for donating brains. “They mourn their family member’s passing, for sure, but they want to think that that life will also be useful in helping other people,” she says.
Jeanine and Bill Rossbach heard about Autism BrainNet through the Dup15q Alliance. Their son, Michael, began having seizures at 7 months and was diagnosed with dup15q syndrome around his second birthday. He didn’t receive a formal autism diagnosis until he was 15, but his mother—a special-education teacher—had suspected it since he was an infant. The Rossbachs constantly worried about his safety. Michael would sometimes bolt, or “elope,” out the door, and he had excruciating gut pain that would make him bite his hands until they bled. His digestive problems made him small and frail. His body seemed to weaken with every seizure and his related health issues intensified before his death at age 35.
One morning in December 2015, Michael’s father opened the door to his bedroom and found him lying, as he usually did, on his side with his head resting on his right arm. “He must have died probably no later than midnight,” Bill Rossbach recalls.
The Rossbachs knew they wanted to donate their son’s brain, but time was of the essence. The family’s home in Acme, Michigan, was nowhere near a brain bank. Jeanine Rossbach called Autism BrainNet, kicking off the search for someone who could harvest Michael’s brain. Eventually, BrainNet found someone in Wisconsin willing to make the four-and-a-half-hour drive over the upper peninsula of Michigan in the snow to collect Michael’s brain and arrange for its delivery to Mount Sinai. “I’m forever grateful,” Jeanine Rossbach says. “I wanted to make his life as meaningful to others as possible.”
* * *
The Netherlands Brain Bank launched a campaign with the slogan “We need brains” last year, encouraging people with a range of psychiatric conditions to register as brain donors. As of October, 179 adults with autism have registered to donate their brains, according to Mignon de Goeij, who manages the Netherlands Brain Bank in Amsterdam. But the bank has yet to collect an autism brain. “It’s the only disorder we have tried to collect [for] that we haven’t gotten a brain [for] yet,” says de Goeij. More than half of the registrants with autism were born after 1970, so brain donation is still years away for many of them, she says.
As of October, more than 4,000 people are registered donors with Autism BrainNet. John, a 54-year-old man with autism who lives in Connecticut, registered last year. He learned about the initiative through a lawyer who helped him write his will. His job in the power industry is extremely dangerous. (He requested that we use only his first name because his employers do not know he is on the spectrum.) He says he wants to know that if he dies on the job, his organs and brain will go to good use. John has already donated 13 gallons of blood to the American Red Cross—enough to fill his own body about eight times. Donating his brain, he says, “goes right along with that.”
Autism BrainNet’s growing collection will be available to researchers in 2018, Amaral says. A panel of experts will review each request for tissue, considering its scientific merits and the type of tissue available. Some brain regions are large, and there is plenty of tissue to go around; others are quite small, and competition for those tissues is fierce. The fusiform face area, for instance, occupies a piece of the brain’s surface no bigger than a chickpea. Hof and his team have found a shortage of neurons in this area in people with autism. The difference in neuron number between autism and control brains is subtle, he says. “You can find it, but you have to look for it; it’s little bit of a needle-in-a-haystack search.”
Another brain region, the amygdala, is about the size of an almond and helps to process emotions, a function altered in people with autism. A 2006 study by Amaral and Schumann that looked at 9 autism brains and 10 control brains revealed a decrease in neurons in the amygdalae of people with autism. But subsequent analyses with 31 more brains have painted a more complex picture: Adults with autism have a dearth of neurons in the amygdala, but children with the condition have a surplus. The findings have yet to be published, but they reinforce the need for more postmortem brains. “To this point, all studies of the autistic brain have been underpowered,” Amaral says.
To help boost the studies’ power even with limited samples, Amaral and others are developing a new method to process the tissue. Instead of storing the fixed hemisphere whole, they plan to slice it into about 2,000 paper-thin sections that can be rolled up like scrolls and stored in fluid-filled test tubes. They can then cut these sections further to isolate certain brain regions, or slice them for various types of microscopy. That way, a single brain region the size of the amygdala could be cut into 215 sections that multiple labs can then use.
The brain banks also collect clinical data for every brain donor—including diagnostic records, brain scans, and genetic sequences. In addition, the clinical coordinator from Autism BrainNet visits each donor’s family and interviews the members to piece together a detailed portrait of the deceased person’s clinical profile. Collecting these data could help to cut through autism’s heterogeneity problem: A string of studies over the past three years have shown, for example, that some mutations linked to the condition affect only a subset of a person’s cells. The proportion of brain cells affected, and where they are in the brain, may influence the type and severity of the person’s condition.
A host of new techniques is making it possible to analyze gene expression in individual cells from postmortem tissue. “There is some really exciting research and techniques available now; we are simply limited by the number of brains available,” Schumann says. She notes that one brain donated before Autism BrainNet launched has already been used in more than 30 studies. “Every single donation makes a big difference.”
The researchers who study donated brain tissue understand the sacrifices families have made. “We do appreciate that our success is based on a family having to go through a really difficult time,” Amaral says. “Knowing that, we try to do everything in our power to make it as straightforward and easy as possible for families to make a donation.”
Hope that something good will come of her son’s brain donation is the only thing that brings Leslie Bolen comfort. Bolen wears a heart-shaped locket that is etched with Michael’s fingerprint and contains a pinch of his ashes. Advocating for Michael was her full-time job for 14 years. In a way, donating his brain lets her continue that work, helping children like her son.
Bolen had one request when she made the donation: She wanted to see Michael’s brain at the bank and keep a photograph of it. It’s not something many parents request, and even her husband, Chad, didn’t feel he could accompany her. But the clinical coordinator connected Bolen with Hof to make the arrangement. “[Hof] was just so warm and understanding and empathetic,” Bolen says. “He knew where I was coming from.”
Bolen got to visit Hof’s lab in New York twice. She held Michael’s brain in her hand. And felt at peace.
This article appears courtesy of Spectrum.


Climate change is already afflicting human health worldwide, exposing tens of millions of elderly people to excess heat while possibly reducing the ability of hundreds of millions of workers to do their jobs, according to an expansive new synthesis from The Lancet, one of the world’s oldest and most widely cited medical journals.
The report examines dozens of statistics from around the planet and finds that the long-predicted effects of climate change have already become a reality in many places. Heat waves now last longer, reaching more people and broiling more territory, than they did in the 1980s and 1990s. In the United States, this spike in warmth is lengthening the allergy season, sometimes by weeks, and helping infectious diseases to spread.
But one of its techniques is questionable. Its findings about the global economy raised eyebrows among academic economists who more formally study climate change.
“The human symptoms of climate change are unequivocal and potentially irreversible—affecting the health of populations around the world today,” warn the paper’s coauthors, who are drawn from nonprofits and 24 different academic institutions. “Whilst these effects will disproportionately impact the most vulnerable in society, every community will be affected.”
The report, dubbed “Countdown 2017,” represents the first coherent attempt to unify many of the different climate-related threads in medical and global-health research. In 2015, a commission of researchers—also convened by The Lancet—warned that climate change could “undermine the last 50 years of gains in public health.” They also called for more comprehensive tracking of important climate signals.
This report, which will now be updated annually, is the result. The coauthors have pulled together figures and estimates from across the medical literature and attempted to tabulate single, worldwide figures for many values.
Some of its techniques may have significant weaknesses. The Lancet report makes an eye-popping assertion about the global economy, arguing that climate change has already significantly harmed labor capacity around the world. Between 2015 and 2016—which are the second- and first-hottest years ever recorded—it argues that “outdoor-labor capacity” fell by 2 percent. Since the year 2000, outdoor-labor capacity has fallen by 5.3 percent overall, it claims.
Academic economists who study climate change were very doubtful of this estimate. “I would back way off the claim that [the data] show any of these things, since they don’t have or use any actual data on labor,” said Solomon Hsiang, a professor of public policy at the University of California, Berkeley, and a member of the Climate Impact Lab.
The Lancet authors, he said, used a technique drawn from a 2013 Nature Climate Change article to estimate “labor capacity.” They effectively interpolated temperature data with conclusions from U.S. military and industrial experiments, from 1996 and 2003, that researched how much work soldiers could do in different temperature conditions. But their findings do not align to a type of labor measurements that economists usually track.
“The numbers presented are (in the most generous interpretation) actual measurements (although still not sure what kind) from a few military lab experiments using U.S. soldiers as subjects and then extrapolated to people across Africa, Latin America, the Middle East, and South Asia,” Hsiang told me in an email.
He continued: “There’s lots of ways this could go wrong, since we don’t know if people adapt, we don’t know if U.S. soldiers are good proxies for normal people in Indonesia, and we don’t have any idea how the “labor-capacity” index these guys put together actually connects to “labor productivity” or earnings as any economist would recognize it.”
Climate change will almost certainly harm global labor output, but a lack of empirical observations around the world makes the kind of measurement The Lancet is attempting difficult. The controversy points to one of the most difficult aspects of finding climate change’s effects on human life: While scientists can measure weather worldwide, measuring the fingerprint of climate change on all human activity is far more complicated.
The Lancet report makes other broad claims about global public health that were more widely accepted. It finds that many more older people experience heat waves now than did two or three decades ago. The report says that roughly 175 million more people older than 65 worldwide were exposed to excess heat in 2015 as compared to several decades ago. On average, 125 million more older adults are exposed to heat than were in previous decades.
In the United States, an additional 14.5 million people older than 65 were exposed, a number larger than the population of Pennsylvania.
Howard Frumkin, one of the coauthors of the report and a professor of public health at the University of Washington, said that heat’s health effects are insidious. While a heat wave may only seem to directly kill a couple thousand people via heat stroke, researchers find that hotter months—when studied after the fact—have many more deaths than would otherwise be expected.
“Heat does a lot of things to people. It disrupts sleep and contributes to sleeplessness. It triggers violence—crime goes up during heat waves. It may trigger self-harm. And there’s very strong evidence that occupational injuries rise during heat waves,” he told me. Heat appears to function as a general stressor on people, wearing down the most vulnerable among them.
The European heat wave of 2003, one of the worst natural disasters ever experienced, is estimated to have ultimately killed more than 70,000 people.
The report also examines climate-related migration from around the world. It finds that a minimum of 4,400 people have definitively been forced to leave their homes because of climate change. “The total number for which climate change is a significant or deciding factor is much higher,” it adds (and it exempts events like the Syrian Civil War, which some experts think climate change helped aggravate).
While this number may seem small, it presages tens of thousands more relocations to come. It also shows how poorly documented most of the relocations are: Most of the 4,400 come not from inundated islands or low-lying coasts in the tropics, but from indigenous villages in northern Alaska.
The report does pull out a spot of good news: Despite a 44 percent increase in the number of extreme-weather events since 2000 (as compared to the decades before that), there’s been no equivalent rise in the number of deaths. Frumkin said that suggested that—if it plans ahead—society may be able to adapt to some of the consequences of climate change.
A separate report from The Lancet pulls out specific findings about how climate change has already altered the public health of the United States. The allergy season here is getting much longer: Nebraska’s ragweed season has extended by 17 days since the early 1990s, and Minneapolis has seen it lengthen by 21 days. The ragweed season in Kansas City, Missouri, extended by 23 days and it now nearly encompasses a quarter of the year.
And certain insect-borne infectious diseases are already showing climate-related spikes. Doctors found three times as many Lyme cases in the United States were diagnosed in 2016, compared to 1990. And mosquitoes that carry dengue fever now transmit the virus 5 percent more effectively than they did a couple decades ago.
Public-health groups in the United States used the report to make a larger point about the direction of American climate policy. “The report emphasizes the scale of the threats to human health—and makes clear that reducing dangerous climate pollution is critical to protect health and save lives,” said Harold P. Wimmer, the president of the American Lung Association, in a statement.
“The U.S. Environmental Protection Agency must listen to this latest warning and implement strong tools, including many already in place, to fight climate change and protect the health and safety of all Americans. We urge the EPA to halt its efforts to repeal the Clean Power Plan,” he added.
Frumkin said it made sense to link the report’s conclusions to recent policy changes. “In my view, and the view of most health professionals, the current administration’s policies are nothing short of tragic,” he said. “They’re misguided; they will cause more suffering and deaths in the population; and they will only delay us in our attempts to tackle climate change.”
“The impacts of climate change are not some distant future event. They’re happening now,” he added.


By the time they got to the orangutan, it was already dying.
In the Batang Toru forest, on the western flank of Sumatra, orangutans will often venture from the jungle to pick fruit from nearby gardens—a habit that puts them in conflict with villagers. In November 2013, the conservationist Matthew Nowak got word of one such conflict, and his veterinary colleagues went to investigate. They arrived to find a male orangutan, badly beaten, his face and hands riddled with cuts. Despite the team’s efforts, he died from his injuries eight days later.
With just 120,000 orangutans left in the wild, the loss of any one is a tragedy. But this particular ape has a significance that will transcend his death. Based on a close analysis of his skeleton, and a study of several orangutan genomes, Nowak and his colleagues think that the dead individual belongs to a different species of orangutan than those that we’re familiar with. If they’re right, there are actually three species of these orange-haired apes. And the newly described one would be the most endangered great ape alive.
When I was a child, an orangutan was an orangutan was an orangutan. But in 2001, after years of debate, scientists formally agreed that there actually two species—one from the Indonesian island of Borneo, and the other from neighboring Sumatra. The Sumatran species is slimmer and paler, with fur that’s closer to cinnamon than maroon. It spends more time in trees (perhaps because Sumatra, unlike Borneo, has tigers). And it’s rarer, with about 14,000 remaining individuals, compared with 105,000 in Borneo.
Most of the Sumatran orangutans live on the northern part of the island. But there’s another small group that lives in Batang Toru—100 kilometers to the south, on the other side of the sizable Lake Toba. A few obscure reports from the 1930s hinted at the existence of this splinter cell, but the group was only formally described in 1997, by a team led by the conservationist Erik Meijaard. These orangutans always seemed a little unusual. They live in more mountainous forests, and they eat different kinds of food.
Their genes are also distinct. In 2013, Michael Krützen, from the University of Zurich, analyzed the DNA of 123 Sumatran orangutans, and found that, in at least one part of their genome, the Batang Toru (or Tapanuli) orangutans were distinct. If anything, they seemed more closely related to the Bornean orangutans on a different island than the Sumatran ones just a day’s walk to the north. “We didn’t expect that,” says Krützen. “It was peculiar, but we needed more data.”
He later mentioned this peculiarity while giving a talk at a conference, where both Meijaard and Nowak happened to be in the audience. The three talked, and, suspecting that these orangutans might belong to their own distinct species, they teamed up to test that idea.
Krützen’s team analyzed the entire genomes of 37 orangutans, including two from Batang Toru. This more thorough analysis confirmed that these animals are indeed genetically distinct from both the Bornean and Sumatran species—and closer to the former than the latter.
They think that the ancestors of all modern orangutans traveled from mainland Asia into Sundaland—a continuous landmass that includes what is now Sumatra, Borneo, and other islands. Around 3.4 million years ago, these ancestral apes split into two populations, one of which gave rise to the current Batang Toru lineage. The other group spread throughout Sundaland; around 670,000 years ago, they split again into two new lineages, which we now know as the Bornean and Sumatran orangutans. The Batang Toru population occasionally crossbred with their Sumatran cousins, but those interspecies shenanigans stopped almost completely 100,000 years ago, when an erupting volcano cut them off.
It’s often said that humans differ from chimps by just 1 percent of our genome, so I wondered how close the three orangutan species are. Krützen disabused me of that question; the number depends on the history of each pair of species, and there’s little to be gained by comparing between different pairs. “There’s no magic number where, beyond that level, it’s a different species,” he says. And besides, genetic differences aren’t the only line of evidence the team has.
At the time Krützen was analyzing orangutan DNA, Nowak got word of the dying male who had been attacked in Batang Toru. And when his team compared its skeleton to those of 33 other orangutans, whose remains are housed in museums, they found clear differences in the shapes of its skull, teeth, and jaw. Even from the outside, they look distinctive, with frizzier fur and a more prominent mustache, and beards on the females. They behave differently too: They eat different plants than the northern populations, and the males have a higher-pitched call.
Not everyone is convinced, though. “It’s premature to consider this a separate species based on one cranium and two genetic samples,” says Rebecca Stumpf, from the University of Illinois at Urbana-Champaign. “I’d suggest that more evidence is needed before adopting separate species designations.”
But Nowak and Krützen argue that it’s the weight of all their evidence that matters. The physical measurements “are based on a sample size of one, which is a pity but we can’t change that,” says Krützen. But there’s also the genetic data, and the unique behaviors. Besides, other primates have been billed as distinctive species on the basis of much less. Bonobos, for example, were proposed as a distinct species from chimpanzees based on a single female specimen and five skulls—without any supporting genetic information to begin with. “There’s no one smoking gun that this is a species,” says Nowak, “but we kept looking, and we kept on finding unique things.”
“Time will tell,” Krützen says. “We’ll continue to work on this, and it might be that in 10 years’ time, we’ll say we have new data that doesn’t support a species status. That’s okay. That’s scientific progress.”
The apes might not have 10 years, though. The Sumatran orangutans were already critically endangered, and now their population might be even smaller than anyone suspected. The newly identified Batang Toru orangutan is rarer still, with an estimated 800 individuals left. Like the other two species, they are killed as agricultural pests, hunted for the pet trade, and rendered homeless as their forests are felled.
The good news is that since 2006, biologist Gabriella Fredriksson from the Sumatran Orangutan Conservation Program has been pushing the local government to spare the Batang Toru forest from logging. Thanks to her efforts, around 85 percent of the forest is now at least partially protected.
Forest Animals Are Living on the Edge
The bad news is that the unprotected 15 percent includes land that’s being set aside for a hydroelectric dam. If built, the dam would cut off two large chunks of forest where the Batang Toru orangutans live, splitting this already small population into even smaller factions. That would be devastating. “It’s probably one of the most endangered great apes we know,” says Krützen. “With just 800 individuals, there’s not much leeway for any mistakes.”
Marc Ancrenaz, co-director of the Kinabatangan Orangutan Conservation Project, is hopeful, though. “I hope that this new status will foster conservation efforts to make sure that the population doesn’t go extinct shortly after being described,” he says. “It’s definitely good news in these times where conservation is more often than not gloom and doom.”


On the Giza Plateau in Egypt rise three large pyramids—the tallest and oldest of which is the Pyramid of Khufu. It is also known as simply the Great Pyramid of Giza. You know what it looks like. It’s one of the seven great wonders of the world.
Yet, for all its fame and antiquity, so many questions remain. How was it built? Why is there nothing in the pyramid, except a broken sarcophagus missing its lid? Could there be anything else hidden inside this massive structure? In the absence of information, there is of course ferocious speculation. And now, an intriguing new piece of information: the discovery, announced today, of a large, previously unknown “void” in the Great Pyramid.
This discovery comes by way of cosmic rays. When these high-energy rays hit atoms in the Earth’s atmosphere, they send subatomic particles called muons shooting toward the ground. The muons can be slowed down by large masses—like the rocks that make up the Great Pyramid. And if muons pass through a cavity inside a large mass, that cavity will show up on muon detectors, too. Three groups of particle physicists using three different techniques patiently tracked muon patterns over several months—gathering evidence that a large cavity lurked in the middle of the pyramid.
It is an incredible—and incredibly expensive—technical feat. ScanPyramids is a project of Cairo University and the Heritage Innovation Preservation (HIP) Institute, the latter of which is funded by a number of private technology and media companies.
As for what it all means, Egyptologists are being very cautious. “The significance of it is still an open question. Even the shape of the void is not quite clear yet,” says Peter Der Manuelian, an Egyptologist at Harvard University, who was not involved with the study.
In fact, the study’s authors exhorted journalists, please, please do not call it a secret chamber. “We know it is a void, but we don’t want to use the word ‘chamber,’” says Mehdi Tayoubi, president of the HIP Institute and an author on the paper. Their caution maybe sharpened by the reaction to a press release last October extolling their preliminary results, which media reports quickly indeed turned into speculation about “secret chambers.”
The new void is above the Grand Gallery—a passage with 28-foot vaulted ceilings leading to the King’s Chamber. The ScanPyramids group first saw hints of a void when they placed nuclear-emulsion film in the Queen’s Chamber, the room below the King’s Chamber. Nuclear-emulsion film records muons, not unlike how ordinary photographic film records photons. The team could see the Grand Gallery and the King’s Chamber in their muon pattern, but they also saw an anomaly. Two other teams of physicists—using instruments that detect muons passing through plastic arrays or argon—then verified this anomaly.
Using muons to study pyramids isn’t an entirely new idea. In the 1960s, future physics Nobel Prize winner Luis Alvarez took his early muon detector to the Pyramid of Khafre. He did not find any secret chambers or even unexpected anomalies. But the idea has lived on, and scientists have used muography to study volcanoes and man-made structures.
The ScanPyramids paper published in the scientific journal Nature is heavy on particle physics and deliberately light on archaeology. Hany Helal, an engineer at Cairo University and a member of the ScanPyramids team, says he is organizing a seminar in Egypt later this year, where archaeologists can come and debate the significance of the void for the pyramid’s construction.
Mark Lehner and Zahi Hawass, two members of the Egyptian Ministry of Antiquities’ scientific committee, to whom ScanPyramids presented it results earlier this year, both told me they suspected the void to be a “construction gap.” All of the chambers and major passageways of the pyramid are aligned along one vertical plane. In order to build the chambers and fill in the rest of the pyramid simultaneously, workers may have worked along what is essentially a trench that allowed them continual access to the King’s Chamber and Grand Gallery. A construction gap could be a remnant of the trench. So it is not surprising, they say, that a void from the construction gap might appear in the space above the Grand Gallery.
In contacting Egyptologists for this story, I could sense a weariness and wariness in their responses. Weariness because claims about hidden chambers in pyramids surface all the time.
The thing to understand, says Lehner, is “the pyramid is more Swiss cheese than cheddar.” That’s only a slight exaggeration, he adds. The inside of the Great Pyramid is filled with stones of irregular sizes, so there are numerous small gaps. In this case, he agrees the void appears to be large enough as to be deliberate, like a construction gap. But many people before have found evidence of a small cavity in the pyramid and gone on to speculate wildly about secret chambers. Lehner said he found ScanPyramids’ characterization of a different anomaly on the pyramid’s north face as a “corridor” to be premature.
The wariness, on the other hand, seems to stem from the project’s origins. Tayoubi, the president and cofounder of the HIP Institute, is also a VP at Dassault Systèmes, a French 3-D-design software company. In 2005, he teamed up to visualize the Great Pyramid construction site with architect Jean-Pierre Houdin, whose idea that the pyramids were built using a series of ramps is not accepted by mainstream archaeologists. (He has since also worked with Der Manuelian now at Harvard to reconstruct the Giza Plateau in 3-D.) Funding for the HIP Institute comes from a number of companies, including: Dassault, Japan’s national broadcasting agency, a watch company, a VR company, and a hotel near Giza.
Hawass, who is also a former Egyptian minister of antiquities, and an outsized, outspoken, and sometimes controversial figure in Egyptology, was blunt—his bluntness perhaps the result of longstanding frustration. “Everybody who comes to the pyramid,” he says, “either they’re looking for fame or they want to make experiments with their equipment and the equipment belongs to a company, and the company can make money.”
The Coolest Thing About King Tut’s Space Dagger
In an interview, Tayoubi acknowledged he is no Egyptologist, and he now assiduously avoided speculation on how the pyramid was built. He did want to tout the technologies used in the study, though not by company name. “We love innovation,” he says, “This mission is about better understanding the pyramid, but above all it’s about innovation,” he says. He likened studying the pyramid to space exploration—an endeavor driven by pure wonder that may nevertheless result in practical innovations in fields like muography and robotics. In fact, the ScanPyramids project is already designing its next piece of technology, a robot to explore inside the pyramid.
New technology might one day crack some of the questions about the Great Pyramid. But so much of its appeal may just be how little we know, despite its prominence and endurance. A mystery right in front of us, daring us to solve it.


Between 1956 and 1962, the University of Cape Town psychologist Kurt Danziger asked 436 South African high-school and college students to imagine they were future historians. Write an essay predicting how the rest of the 20th century unfolds, he told them. “This is not a test of imagination—just describe what you really expect to happen,” the instructions read.
Of course, everyone wrote about apartheid. Roughly two-thirds of black Africans and 80 percent of Indian descendants predicted social and political changes amounting to the end of apartheid. Only 4 percent of white Afrikaners, on the other hand, thought the same. How did they get it so wrong?
Students’ predictions were more like fantasies. “Those who were the beneficiaries of the existing state of affairs were extremely reluctant to predict its end,” Danziger explains, “while those who felt oppressed by the same situation found it all too easy to foresee its collapse.”
Psychology research indeed suggests that the more desirable a future event is, the more likely people think it is. When the sociologists Edward Brent and Donald Granberg studied wish fulfillment in U.S. presidential elections between 1952 and 1980, they found that 80 percent of each of the major candidates’ supporters expected their preferred candidate to win by a ratio of around four to one. “People distort their perception of an election's closeness in ways that are consistent with their preferences,” a later paper concluded. Likewise, after the 2008 election, researchers analyzed survey predictions from 19,000 Americans and found that Democrats tended to think Barack Obama was more likely to win, while Republicans assumed John McCain would.
Conversely, the more someone dreads or fears a potential outcome, the less likely they think it is to happen. In November 2007, economists in the Philadelphia Federal Reserve’s Survey of Professional Forecasters predicted just a 20 percent chance of “negative growth”—read: decline—in the U.S. economy any time in 2008, despite visible signals of an impending recession. There is, the economist Sergey Smirnov wrote in a review of economists’ botched predictions on the 2008 recession, “some deep inherent unwillingness to predict undesirable things.”
But people’s thinking isn’t as simplistic as “I wish it to happen, so it will” or, “I don’t want it to happen, so it won’t.” Self-interest influences our predictions in subtler ways.
* * *
The Rutgers University psychologist Neil Weinstein discovered unrealistic optimism by accident. It was the late 1970s, and for no particular reason he had asked study subjects to rate their likelihood of experiencing certain negative future events, like getting divorced, fired, or mugged, from “below average risk” to “above average risk.” This was back when data were manually entered on yellow punch cards. So he was sitting at his punch-card machine punching people’s responses when he realized that “all the responses were on the below-average side of the scale.”
Unrealistic optimism is thinking that good things are more likely to happen to you than to other people, whereas bad things are less likely. It’s not outright denial of risk, says Weinstein. “People don’t say, ‘It can’t happen to me.’ It’s more like, ‘It could happen to me, but it’s not as likely [for me] as for other people around me.’” People predict that they’re less likely than others to experience illness, injury, divorce, death, and other adverse events—even when they’re exposed to the same risk factors. For instance, someone might think she’s less prone to diabetes than others, even if she weighs the same, eats the same, shares similar family history with, and has the same lifestyle as the people she's comparing herself to.
Take these Pew Research Center findings: In 2015, 65 percent of a representative sample of American workers predicted that automation would monopolize most of the work currently performed by humans within 50 years. But 80 percent of workers believed that their own jobs would remain intact. In essence, they admitted that automation posed a threat to workers but assumed they were less susceptible than average to its effects.
Anxiety affects people’s predictions subliminally. For example, they may unwittingly only gather and synthesize facts about their prediction that support the outcome they want. This process may even be biologically ingrained: Neuroscience research suggests that facts supporting a desired conclusion are more readily available in people’s memories than other equally relevant but less appealing information. Our predictions are often less imaginative than we think.
They’re also more self-absorbed. Unrealistic optimism occurs in part because people fail to consider others’ experiences, especially when they think a future outcome is controllable. Imagine you’re trying to find a job. You do everything you can to make yourself an appealing candidate: get relevant experience, fix up your resume and cover letter, network. You might conclude that these measures will make you more likely to get a job than other job seekers, not taking into account that others are likely doing the same things you are to boost their chances, too. People may think their odds are better than average because they don’t know what the average really is.
Weinstein tried to curb this bias in his lab with limited success. He told students to list the factors that influenced their chances of experiencing certain events, like a heart attack, and then read other students’ responses to the same prompt. When students realized that their risk factors matched everyone else’s, their unrealistic optimism was reduced but, oddly, not eliminated. They still thought they were less likely than the average student with the same risk factors to experience negative events, even without any objective justification. Other research indicates that people resist revising their estimations of personal risk even when confronted with relevant averages that explicitly contradict their initial predictions.
Despite attempts to remedy unrealistic optimism, Weinstein insists that it isn’t all bad. Thinking things are going to turn out well may actually be adaptive, a way to soothe our fears about the future. “It keeps you from falling apart.”
* * *
Before Ray Kurzweil became a high-profile inventor, author, and futurist, he was a kid who lost his father. Kurzweil has kept every physical memento of his dad—records, notes, pictures, electric bills, 50 boxes in total—in a storage facility in Massachusetts in preparation for his prediction: By the mid-2030s, “we will be able to create avatars of people who have passed away from all of the information they have left behind.”
Kurzweil himself plans on immortality. He takes 90 supplements per day, gets regular blood tests and infusions, and has been working with the famed longevity doctor Terry Grossman for the last two decades to arrest his aging. But Kurzweil thinks that someday he’ll transcend the need for these antiaging measures. He predicts that in the 2030s our brains will connect directly to the cloud to augment our existing intelligence, and that our biological bodies will be replaced with machines part by part.
Research suggests that far-off events, like death, are particularly vulnerable to overly optimistic predictions. Moreover, predictions appear to be most influenced by whether “the event in question is of vital personal importance to the predictor.”
The author, speaker, and global-trends expert Mark Stevenson says that people who predict the future are victim to their own prejudices, wish lists, and life experiences, which are often reflected in their predictions. When I asked Stevenson for an example, he told me to consider at what point in time any futurist approaching 50 predicts life extension will be normal—“quite soon!”
But Kurzweil tells me that he makes his predictions based on what he calls the “law of accelerating returns,” which says that technology follows “a predictable and exponential trajectory.” Kurzweil says that he controls for wishful thinking in his predictions by using that trend to project out a future curve. It’s math. In 2045, the year Kurzweil predicts humans will become effectively immortal after they’ve merged with machines, Kurzweil will be 97.
* * *
In 2010, Pew Research Center found that 41 percent of Americans—70 percent of whom identify as Christian—believed that Jesus would probably or definitely return to Earth by 2050. These 50 million Americans might be right this time, but the oft-prophesied Second Coming has failed to come before. Jesus predicted his own return within a single generation in Matthew 24:34; 500 years ago, Martin Luther predicted that Judgment Day would occur within the next 300 years.
University of Pennsylvania’s Philip Tetlock, who studies the art and science of forecasting, says that strongly held beliefs are a big reason people make bad predictions.
For example, the Western faith in progress—the idea that, as sociologist Robert Nisbet put it, “mankind has advanced in the past, is now advancing, and may be expected to continue advancing in the future”—biased early predictions about the internet. In 2005, only 32 percent of hundreds of internet experts surveyed by Pew agreed that by 2014 most people “will use the internet in a way that filters out information that challenges their viewpoints on political and social issues.”
Sometimes the self-interest influencing people’s predictions is simply their desire to be right. People predict outcomes that will affirm their beliefs about the world: that democracy is winning, that death is a needless tragedy, that there is or isn’t a God. “Once you commit to anything you then have a vested interest in the outcome,” Kurzweil says. Strongly held beliefs become self-interested beliefs.
People aren’t so naïve as to think that just because something is important to them, it will happen. Rather, they tend to think most other people share their beliefs, and thus the future they endorse is likely. What researchers call “projection bias” explains why individuals so often bungle election predictions. Because they assume that others have political opinions similar to their own, people think their chosen candidate is more popular than she or he actually is. Liberals’ underestimates of the true scale of Trump support during the 2016 election may have at least partially resulted from this bias.
People who are abnormally good at predicting the future—“super-forecasters”—skillfully ban their prejudices from their probability equations. As Tetlock told Steven Dubner on the Freakonomics podcast, super-forecasters “try not to have too many ideological sacred cows.”
* * *
Sometimes self-interested predictions pan out. When Cold War historians predicted the imminent Westernization of the world amid the threat of nuclear war with the Soviet Union, they were praying they wouldn’t witness the alternative. They ended up being right. And of course, the black and Indian South Africans who predicted—and hoped—that apartheid would end were right, too.
Still, on the whole people would make better predictions with more objectivity and awareness. And good predictions matter. If you think you won’t get an STI, you may not practice safe sex or get tested regularly, thereby increasing your risk. On a larger scale, people may settle in regions prone to natural disasters because they assume they won’t be affected. Or, as Weinstein observed, people may think Trump’s proposed tax reform will help them more than it actually will, or at least that it will help them more than it will help other people. These beliefs could sway public support and congressional votes and, as a result, the future of American taxes.
In short, how we predict the future is important because it affects what we do in the present. So how do you forfeit your fantasies?
Faith Popcorn, the CEO of “a future-focused strategic consultancy,” advises shaking up your perspective: “Learn how the other side thinks,” she says. Chat up interesting people; go to readings, talks, and fairs to “expand your horizons.”
Tetlock says forecasting tournaments can teach people how to outsmart their shortcomings and instead play “a pure accuracy game.” (His wife runs one such tournament, called the Foresight Project.) He says that many people “have a little voice in the back of their heads saying, ‘Watch out! You might be distorting things a bit here,’ and forecasting tournaments encourage people to get in touch with those little inner voices.”
But Weinstein has been around long enough to know that extinguishing unrealistic optimism isn’t so simple. “It’s hard because it has all these different roots,” he says. And human nature is so obstinate.
In one study, Weinstein and his collaborators asked people to estimate the probability that their Texas town would be hit by a tornado. Everyone thought that their own town was less at risk than other towns. Even when a town was actually hit, its inhabitants continued to believe that their town was less likely to get hit than average. But then, by chance, one town got hit twice during the study. Finally, these particular townspeople realized that their odds were the same as all the other towns. They woke up, says Weinstein. “So you might say it takes two tornadoes.”


At the very beginning of his book The Song of the Dodo, the author David Quammen invites us to imagine a fine Persian carpet, which we then slice into 36 equal pieces. “What does it amount to?” he writes. “Have we got 36 nice Persian throw rugs? No. All we’re left with is three dozen ragged fragments, each one worthless and commencing to come apart.” He wrote that almost two decades ago, and it’s still the perfect metaphor for the state of the world’s forests.
Humans chop down an estimated 13 million hectares of trees every year, but even that huge number doesn’t fully capture the destruction we inflict. We don’t just destroy forests—we fragment them, turning unbroken stretches of green into ragged patches fraying at the edges. There are only two places on Earth—the Amazon and the Congo—where forests have retained their old, continuous glory. Everything else has been partitioned into green islands, separated not by water but by roads and farmland.
It matters, this slicing of the world. Just as Quammen’s 36 swatches of fabric don’t add up to a full carpet, fragmented forests are less conducive to life than their total area might suggest. Each green island is limited in how many species it can sustain. And since these populations are cut off from their neighbors, they’re uniquely vulnerable to disease, disaster, or dumb, bad luck. For these reasons, fragmented habitats tend to lose half their animal and plant species within two decades.
Many of the survivors are, quite literally, living on the edge. About 70 percent of the world’s remaining forests lie within a kilometer of an edge, and about half lie within 500 meters. And in a new and unprecedentedly detailed study, a team of scientists led by Marion Pfeifer at Newcastle University has shown that 85 percent of forest animals are affected by the presence of an edge. Some benefit, others suffer. Either way, the results show that humans, by fragmenting forests, have radically restructured them—and that the edge of a forest is almost a completely different world than its core.
Many researchers have tried to assess edge effects in simple ways, such as measuring how the abundance of a species changes as it gets closer to an edge. This ignores the fact that a patch of forest might be within reach of many edges, each of which compounds the hazards of the others. It also treats land simplistically, as if it’s either forest or nonforest; in reality, the land surrounding a group of trees can vary a lot in its usefulness and attractiveness to local species.
To improve on these methods, Pfeifer’s colleague Véronique Lefebvre from Imperial College London developed two new metrics for making sense of fragmented landscapes. The first—edge influence—reflects the variations in tree cover around a given point; roughly speaking, it tells you how “edgy” a particular bit of forest is. The second metric—edge sensitivity—reflects how edgy a habitat has to get before a species avoids it. They then calculated edge sensitivities for 1,673 vertebrate animals around the world. “The study sets a new standard of how edges should be measured,” says Nick Haddad from Michigan State University, who was not involved in the work.
They found that around 39 percent of these species steer clear of edges while 46 percent seem to gravitate toward them. This doesn’t mean that edges are a net good, though. The edge-seeking animals are more likely to be generalists or invasive species, such as green iguanas and boa constrictors. By contrast, the species that stick to the forest core, such as the Sunda pangolin and Baird’s tapir, are almost four times as likely to be on the endangered list. They’re the creatures most at risk of extinction, and their shrinking habitat is even smaller than it first seemed. There’s a lot of them too. About 57 percent of mammals decline in number as you get closer to edges, as do 41 percent of birds, 30 percent of amphibians, and 11 percent of reptiles.
“Edge effects have long been studied but not with the level of detail in this study,” says Isabel Rosa from the German Centre for Integrative Biodiversity Research. “It highlights the importance of considering not only the habitat amount that is available for the species, but also its quality.”
For example, the team’s calculations show that forest-core species only thrive at least 200 to 400 meters away from an edge. To a long-billed black cockatoo, a circular clump of trees that’s 800 meters in diameter is not 200 hectares of inviting greenery—it’s nothing. To a Sunda pangolin, a strip of forest that’s extremely long but just 800 meters wide might as well be no forest at all. “We need to manage patches in a way that maximizes the forest core,” Pfeifer says. Her team has developed a freely available tool called BioFrag to help them, by calculating the edginess of their forests and the edge sensitivities of the local animals.
Conservationists do already think about edge effects, Haddad says, but typically on the scale of tens of meters—not hundreds. Pfeifer and her colleagues have shown that edges are more pernicious, and that their influence extends far deeper into a forest. Only half the world’s forests lie more than 500 meters from an edge, and conservationists should focus on the species that persist in this invisible corral. If they ignore these edge effects, Haddad says, “they may be missing the forest for the trees.”


Joshua Plotkin’s dive into the evolution of language began with clarity—and also a lack of it.
Today, if you wanted to talk about something that’s clear, you’d say that it has clarity. But if you were around in 1890, you would almost certainly have talked about its clearness.
Plotkin first noticed this linguistic change while playing with Google’s Ngram Viewer, a search engine that charts the frequencies of words across millions of books. The viewer shows that a century ago, clearness dominated clarity. Now the opposite is true, which is strange because clarity isn’t even a regular form. If you wanted to create a noun from clear, clearness would be a more obvious choice. “Why would there be this big upswing in clarity?,” Plotkin wondered. “Is there a force promoting clarity in writing?”
It wasn’t clear. But as an evolutionary biologist, Plotkin knew how to find out.
The histories of linguistics and evolutionary biology have been braided together for as long as the latter has existed. Many of the earliest defenders of Darwinism were linguists who saw similarities between the evolution of languages and of species. Darwin himself wrote about these “curious parallels” in The Descent of Man. New words and grammatical rules are continually cropping up, fighting for existence against established forms, and sometimes driving those old forms extinct. “The survival ... of certain favored words in the struggle for existence is natural selection,” Darwin wrote.
Darwin, Plotkin says, used the way language changes “to popularize his heretical theory and explain for a broad audience what natural selection means. The process wasn’t easy to observe in organisms, but it was easier to see in words.”
But natural selection is just one force of evolutionary change. Under its influence, genes become more (or less) common because their owners are more (or less) likely to survive and reproduce. But genes can also change in frequency for completely random reasons that have nothing to do with their owner’s health or strength—and everything to do with pure, dumb luck. That process is known as drift, and it took decades for evolutionary biologists to recognize that it’s just as important for evolution as natural selection.
Linguists are still behind. It’s easy to see how languages can change through drift, as people randomly pick up the words and constructions they overhear. But when Darwin wrote about evolving tongues, he said, “The better, the shorter, the easier forms are constantly gaining the upper hand, and they owe their success to their own inherent virtue.” That’s a view based purely on natural selection, and it persists. “For the most part, linguists today have a strict Darwinian outlook,” Plotkin says. “When they see a change, they think there must be a directional force behind it. But I propose that language change, maybe lots of it, is driven by random chance—by drift.”
To see whether that was true, he and his colleagues developed statistical tests that could distinguish between the influence of drift and of natural selection. They then applied these tests to several online repositories, such the Corpus of Historical American English—a digital collection of 400 million words, pulled out of 100,000 texts published over the past 200 years.
The team focused first on the past-tense forms of verbs, and found at least six cases where natural selection is clearly in effect. In some cases, the verbs were regularized, losing weird past forms in favor of more-predictable ones that end in –ed. Wove, for example, gave way to weaved, while smelt lost ground to smelled. That’s not surprising: Many linguists have suggested that verbs tend to become more regular over time, perhaps because, like Darwin theorized, these forms are just easier to learn.
But Plotkin found just as many instances where selection drove verbs toward irregularity: Dived gave way to dove, lighted to lit, waked to woke, and sneaked to snuck. Why? Perhaps because we like it when words sound alike, and we change our language to accommodate such rhymes. For example, dove began to replace dived at the same time that cars became popular, and drive/drove became common parts of English. Similarly, the move from quitted to quit coincided with the rise of split, which became much more widely used when it acquired a new meaning—to leave or depart. In both cases, changes in one irregular verb—drive or split—may have irregularized others. “We can’t definitively say that’s the reason, but it’s coincident,” Plotkin says.
“It gets you to think harder about the motivation for change,” says Salikoko Mufwene, from the University of Chicago. “The general claim is that there has been an evolution toward regularization, and they’re showing that this hasn’t always been the case. Now we need to think harder about when irregular forms are favored over regular variants.”
That is, if anything is favored at all. The team found that the changes that have befallen the vast majority of our verbs are entirely consistent with drift. You don’t need to invoke natural selection to explain why we say spilled instead of spilt, burned instead of burnt, and knit instead of knitted.
In other cases, drift and natural selection work together to shape languages. For example, Plotkin’s team also looked at the rapid rise of do in the 16th century, when phrases like “You say not” quickly changed into “You do not say.” They concluded that at first, the word randomly drifted its way into questions, so that “Say you?” gradually became “Do you say?” Once it became common, natural selection started pushing it into new contexts like declarative sentences, perhaps because it was easier for people to use it consistently.
The team also analyzed a third and more obscure grammatical change called Jespersen’s Cycle. In Old English, spoken before the Norman Conquest, speakers would negate a verb by putting a not in front of it. In Middle English, spoken between the 11th and 15th centuries, the negatives would surround the verb as they do in modern French (“Je ne dis pas”). And in Early Modern English, spoken between the 15th and 17th centuries, the negative followed the verb—the Shakespearean “I say not.” Now, we’ve come full circle, back to “I don’t say.”
Jespersen’s Cycle exists in many unrelated languages. In French, for example, the formal “Je ne dis pas” is giving way to the colloquial “Je dis pas.”
Natural selection still explains Jespersen’s Cycle far better than drift does, according to Plotkin's analysis. Perhaps it’s due to emphasis, he says. If one form is common, speakers could emphasize their disagreement by adding or subtracting words (“I don’t say that at all,” versus “I don’t say that”). As the emphatic forms become more common, they lose their sting, and are themselves replaced.
These results are part of a wider trend where linguists are starting to use these massive online corpora to address long-standing puzzles in language change. “This is an excellent trend,” says Jennifer Culbertson, from the University of Edinburgh. “Linguists have uncovered many really fascinating cases of language change, but the explanations on offer sometimes read like just-so stories. Random processes are simply underappreciated, because we want to come up with interesting explanations.” But by considering drift, too, linguists could “focus our energies on providing interesting explanations where they are really warranted.”
What about the change from clearness to clarity, which set Plotkin onto this quest in the first place? He says that he’s found signs of natural selection’s hand, but that will have to wait for another publication. “There’s lots to be done,” he says. “This is just the beginning of an investigation, which need not stop at written texts. Spoken records are just as ready and ripe for scrutiny.”


The Plague of Cyprian, named after the man who by AD 248 found himself Bishop of Carthage, struck in a period of history when basic facts are sometimes known barely or not at all. Yet the one fact that virtually all of our sources do agree upon is that a great pestilence defined the age between AD 249 and AD 262.
Inscriptions, papyri, archaeological remains, and textual sources collectively insist on the high stakes of the pandemic. In a recent study, I was able to count at least seven eyewitnesses, and a further six independent lines of transmission, whose testimony we can trace back to the experience of the pestilence.
This article is adapted from Harper’s recent book.
What is starkly lacking, however, is a Galen. The previous century’s dumb luck of having a great and prolific doctor to guide us has run out. But, now, for the first time, we have Christian testimony. The church experienced a growth spurt during the generation of the plague, and the mortality left a deep impression in Christian memory. The pagan and Christian sources not only confirm one another. Their different tone and timbre give us a richer sense of the plague than we would otherwise possess.
The lack of a medical witness like Galen is partly compensated by the vivid account of the disease in Cyprian’s sermon on the mortality. The preacher sought to console an audience encircled by unfathomable suffering. It took no mercy on his Christians.
“The pain in the eyes, the attack of the fevers, and the ailment of all the limbs are the same among us and among the others, so long as we share the common flesh of this age.” Cyprian tried to ennoble the victims of the disease, likening their strength in pain and death to the heroic intransigence of the martyrs. Cyprian conjured the symptoms for his hearers.
These are adduced as proof of faith: that, as the strength of the body is dissolved, the bowels dissipate in a flow; that a fire that begins in the inmost depths burns up into wounds in the throat; that the intestines are shaken with continuous vomiting; that the eyes are set on fire from the force of the blood; that the infection of the deadly putrefaction cuts off the feet or other extremities of some; and that as weakness prevails through the failures and losses of the bodies, the gait is crippled or the hearing is blocked or the vision is blinded.
Cyprian’s account is central to our understanding of the disease. The pathology included fatigue, bloody stool, fever, esophageal lesions, vomiting, conjunctival hemorrhaging, and severe infection in the extremities; debilitation, loss of hearing, and blindness followed in the aftermath. We can complement this record with more isolated and frankly uncertain hints from other witnesses. According to Cyprian’s biographer, the disease was characterized by acute onset: “carrying off day by day with abrupt attack numberless people, every one from his own house.”
The course of the infection and illness was terrifying. This impression is confirmed by another North African eyewitness, a Christian not far removed from the circle of Cyprian, who insisted on the sheer unfamiliarity of the disease. “Do we not see the rites of death every day? Are we not witnessing strange forms of dying? Do we not behold disasters from some previously unknown kind of plague brought on by furious and prolonged diseases? And the massacre of wasted cities?” The pestilence, he argued, was a manifest encouragement to martyrdom, since those who died the glorious death were spared the “common fate of others amidst the bloody destruction of ravaging diseases.”
The Plague of Cyprian was not just another turn through the periodic cycle of epidemic mortality. It was something qualitatively new—and the evocation of its “bloody” destruction may not be empty rhetoric, if hemorrhagic symptoms are implied.
The disease was of exotic origin and moved from southeast to northwest. It spread, over the course of two or three years, from Alexandria to other major coastal centers. The pandemic struck far and wide, in settlements large and small, deep into the interior of empire. It seemed “unusually relentless.” It reversed the ordinary seasonality of death in the Roman Empire, starting in the autumn and abating in the following summer. The pestilence was indiscriminate; it struck regardless of age, sex, or station. The disease invaded “every house.”
One account predictably blamed the “corrupted air” that spread over the empire. But another chronicle tradition, going back to a good contemporary historian in Athens, recorded that the “disease was transmitted through the clothes or simply by sight.” The observation is notable; in a culture without even a rudimentary sense of germs, the comment betrays a pretheoretical sense of contagion. The concern that the disease could be transmitted by clothing or eyesight suggests at least a dim awareness of an infectious origin. And it just might provide a further hint that the disease affected the eyes.
The ancients harbored plenty of eccentric notions about the powers of eyesight, among them that it was tactile, ejecting a flow of particulates from the eye of the looker. The bloody eyes of Cyprian’s victims may have presented a terrifying visage, in a culture where the eyes had the power to reach out and touch.
The death toll was grim. We have an intriguingly specific report from the bishop of Alexandria, who claimed that:
This immense city no longer contains as big a number of inhabitants, from infant children to those of extreme age, as it used to support of those described as hale old men. As for those from 40 to 70, they were then so much more numerous that their total is not reached now, though we have counted and registered as entitled to the public food ration all from 14 to 80; and those who look the youngest are now reckoned as equal in age to the oldest men of our earlier generation.
The reckoning implies that the city’s population had declined by about 62 percent (from something like 500,000 to 190,000). Not all of these need be dead of plague. Some may have fled in the chaos. And we can always suspect overheated rhetoric. But the number of citizens on the public grain dole is a tantalizingly credible detail, and all other witnesses agreed on the scale of the mortality. An Athenian historian claimed that 5,000 died each day. Witness after witness—dramatically if imprecisely—testified that depopulation was invariably the sequel of the pestilence. “The human race is wasted by the desolation of pestilence.”
These haphazard clues do not equip us well to identify the pathogenic agent of the Plague of Cyprian. But the range of suspects capable of causing a disease event of this scope is not large, and some possible agents can be almost certainly exculpated.
Bubonic plague does not fit the pathology, seasonality, or population-level dynamics. Cholera, typhus, and measles are remote possibilities, but each poses insuperable problems. Smallpox must be a serious candidate. The two-generation lapse between the episode under Commodus and the Plague of Cyprian means that effectively the entire population would have been susceptible again. The hemorrhagic form of the disease might also account for some of the features described by Cyprian.
But in all the case for smallpox is weak. A North African author claimed it was an unprecedented disease (though whether he would have had any memory of previous smallpox epidemics is of course questionable). None of our sources describe the full-body rash that is the distinctive feature of smallpox. In the church history of Eusebius, written in the early fourth century, an outbreak more like smallpox was recounted in AD 312–13. Eusebius both called this a “different illness” than the Plague of Cyprian and also distinctly described the pustular rash. The exotic origins of the third-century event, again from beyond the Roman Empire, do not suggest the eruption of a now-endemic pathogen. Finally, the putrescent limbs and permanent debilitation of the Plague of Cyprian are not a fit for smallpox. None of these clues are conclusive, but collectively they militate against the identification of smallpox.
Any identification must be highly speculative. We would offer two candidates for consideration. The first is pandemic influenza. The influenza virus has been responsible for some of the worst pandemics in human history, including the “Spanish flu” epidemic that carried off some 50 million souls at the end of World War I. The lack of clear evidence for influenza from the ancient world is puzzling, because the flu is old and it was undoubtedly not a stranger in the ancient world. Influenza is a highly contagious acute respiratory disease that comes in many forms. Most types are relatively mild, causing familiar cold-like symptoms. Other rare types of influenza are more menacing.
Zoonotic forms of the disease, especially those native in wild aquatic birds, can be pathogenic to other animals, including pigs, domestic fowl, and humans; when these strains evolve the capacity to spread directly between humans, the results are catastrophic. There have been four global outbreaks in the last century, and avian influenza (which includes some dreaded strains such as H5N1) remains a terrifying threat today.
Pathogenic zoonotic influenzas are viciously lethal. They induce an overheated immune response which is as dangerous as the viral pneumonia itself; hence, the young and healthy are paradoxically put at risk by the vigor of their immune response. The lack of any respiratory symptoms in the account of the Plague of Cyprian is a strike against the identification. But it is worth reading some observations of the 1918 pandemic.
Blood poured from noses, ears, eye sockets; some victims lay in agony; delirium took away others while living ... The mucosal membranes in the nose, pharynx, and throat became inflamed. The conjunctiva, the delicate membrane that lines the eyelids, becomes inflamed. Victims suffer headache, body aches, fever, often complete exhaustion, cough ... Often pain, terrific pain ... Cyanosis ... Then there was blood, blood pouring from the body. To see blood trickle, and in some cases spurt, from someone’s nose, mouth, even from the ears or around the eyes, had to terrify ... From 5 to 15 percent of all men hospitalized suffered from epistaxis—bleeding from the nose.
Pandemic influenza might indeed account for the horrifying experience of the Plague of Cyprian.
The winter seasonality of the Plague of Cyprian points to a germ that thrived on close interpersonal contact and direct transmission. The position of the Roman Empire astride some of the major flyways of migratory birds, and the intense cultivation of pigs and domestic fowl such as chickens and ducks, put the Romans at risk. Climate perturbations can subtly redirect the migratory routes of wild waterfowl, and the strong oscillations of the AD 240s could well have provided the environmental nudge for an unfamiliar zoonotic pathogen to find its way into new territory. The flu is a possible agent of the pestilence.
A second and more probable identification of the Plague of Cyprian is a viral hemorrhagic fever. The pestilence manifested itself as an acute-onset disease with burning fever and severe gastrointestinal disorder, and its symptoms included conjunctival bleeding, bloody stool, esophageal lesions, and tissue death in the extremities. These signs fit the course of an infection caused by a virus that induces a fulminant hemorrhagic fever.
Viral hemorrhagic fevers are zoonotic diseases caused by various families of RNA viruses. Flaviviruses cause diseases like yellow fever and dengue fever, which have some resemblance to the symptoms described by Cyprian. But flaviviruses are spread by mosquitoes, and the geographic reach, speed of diffusion, and winter seasonality of the Plague of Cyprian rule out a mosquito-borne virus.
The speed of diffusion points to direct human-to-human transmission. The belief that caring for the sick and handling the dead were fraught with danger underscores the possibility of a contagion spread between humans. Only one family of hemorrhagic viruses seems to provide a best match for both the pathology and epidemiology of the Plague of Cyprian: filoviruses, whose most notorious representative is the Ebola virus.
Filoviruses are millions of years old. Fragments of their genetic material are anciently embedded in mammalian genomes, and for millions of years they have infected bats, insectivores, and rodents. Yet filoviruses, like Ebola virus and Marburg virus, were only recognized in the second half of the 20th century during a series of small-scale outbreaks. The Ebola epidemic of 2014 brought further attention to the family. The natural host of the Ebola virus remains unconfirmed, although bats are suspected. Ebola virus grabs public attention because of its ghastly clinical course and extreme case-fatality rates.
To cause an epidemic, the Ebola virus must first leap from its host species to a human; this probably occurs when humans come into contact with infected bats or apes. Once infected, after a brief incubation period (on average four to 10 days, sometimes longer), victims suffer intense fever and a disease that breaks down multiple systems simultaneously, including gastrointestinal and vascular involvement. Conjunctival injection and severe hemorrhagic symptoms could well account for the disturbing reports of Cyprian. Tissue necrosis and permanent disfigurement of the limbs might reflect Cyprian’s description of extremities turning putrid and becoming irreversibly disabled.
Case-fatality rates, even with modern treatment, are grotesquely high: 50–70 percent. Death usually comes between days six and 16; survivors are thought to possess immunity. The Ebola virus is transmitted by bodily fluids, but not aerial droplets; it spreads easily within households. Caregivers are at special risk, and cadavers remain a potent source of infection. The observance of traditional burial rites has been a problematic risk factor even in recent outbreaks.
Retrospective diagnosis from anguished reports of nonmedical personnel across nearly 2,000 years is never going to offer great confidence. But the hemorrhagic symptoms, the shocked sensibilities, and the insistence on the novelty of the disease all fit a filovirus. An agent like Ebola virus could diffuse as quickly as the Plague of Cyprian, but because of its reliance on body fluids for transmission, it could exhibit the slow-burning, “unusually relentless” dynamics that so struck contemporary observers. The obsession with deadly corpses in the third-century pandemic strikes a profound chord, given the recent experience of the Ebola virus. The uncertainty lies in our profound ignorance about the deep history of pathogens like Ebola that never became endemic in human populations.
As historians, we understandably default to the familiar suspects. But our broadening awareness of the incessant force of emerging disease, at the frontier between human society and wild nature, suggests a place for significant disease events in the past, like the Plague of Cyprian, caused by zoonotic diseases that wreaked havoc and then retreated back to their animal hosts.
By the time of the Plague of Cyprian’s appearance in AD 249 there was much that was different. The empire’s stores of reserve energy were depleted. Perhaps this microbial enemy was just more sinister. In this event, the center could not hold. There is much that must remain uncertain about the Plague of Cyprian, but not this: In its immediate wake, anarchy was loosed on the world.
This post is adapted from Harper’s recent book, The Fate of Rome: Climate, Disease, and the End of an Empire.



For most of us, it’s unthinkable: Human is never what’s for dinner. Sorry to burst any bubbles, but in this episode, we discover that not only is cannibalism widespread throughout the natural world, it’s also much more common among our own kind than we like to think. Spiders and sharks do it; so do both ancient and modern humans. So why it sometimes make sense to snack on your own species—and what are the downsides? From Hannibal Lecter to the Donner party, cannibals are now the subject of morbid fascination and disgust—but how did eating one another become such a taboo? Join us this episode for our Halloween special: the science and history of cannibalism!
According to the zoologist Bill Schutt, the author of Cannibalism: A Perfectly Natural History, until recently, “the party line was basically that if you saw cannibalism in nature it was because of a lack of nutrition or cramped captive conditions.” In the past three decades, however, scientists have come to realize that cannibalism is surprisingly common, and that it occurs for a variety of different reasons: Male spiders who become their consort’s dinner gain a reproductive advantage, Schutt explains, while sand tiger sharks take advantage of their spare siblings in utero to hone their hunting skills before they’re even born.
Fewer species eat members of their own kind as you move through the animal kingdom toward primates—but, according to the archaeologist James Cole, cannibalism seems to have been a reasonably regular part of early human behavior too. His question was: Why? Were ancient humans eating one another out of hunger, or for more complicated reasons that have to do with spiritual beliefs about the soul and the body? To find out, Cole determined how many calories a raw male would provide, and then compared that with the number of calories in early humans’ other dinner options, such as mammoth, boar, and deer. He reveals his findings on Gastropod, which include a macabre organ-by-organ guide to the human body—useful for anyone who’d like to try cannibalism but is worried about their weight.
Cannibalism became increasingly taboo in modern history, as mainstream religions have typically frowned on the practice, labeling it as barbarous and driving it almost to extinction—while simultaneously using the accusation of man-eating as justification for colonial exploitation. Questionable morality aside, there are good reasons to avoid eating members of our own species. One of the bizarre medical mysteries of modern times is kuru, a fatal neurodegenerative “laughing” disease that began killing large numbers of the Fore people in the 1960s. This episode, we talk with Shirley Lindenbaum, the anthropologist whose fieldwork, carried out in remote Papua New Guinea in her 20s, uncovered the cause of the disease in the cannibalistic Fore funerary rituals. Today, however, despite the risks and the taboo, one kind of cannibalism is having a resurgence among celebrities and natural-birth advocates alike: placentophagy. It may be endorsed by Kim Kardashian West, but is there any scientific evidence behind the trend? Answers to all this and much more in this week’s episode!
This post appears courtesy of Gastropod, a podcast co-hosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


Between 7,000 and 9,000 years ago—during the middle Holocene—the Four Corners area went through a slow but dramatic climatic shift. As the region became hotter and drier, stream and lake levels dropped, and larger game animals and firewood became harder to find. Indigenous communities had to rely on foods that were less nutritious and took more time to prepare, such as grass seeds and chenopodium seeds, a tiny grain similar to quinoa.
But recently, archaeologists working with local tribes have recognized a surprising addition to these early food sources: Eleven millennia ago, communities at the North Creek Shelter—a rock overhang in southern Utah’s Escalante Valley—began harvesting a unique species of potato. That’s the earliest known use of a potato in North America, and the evidence suggests that the nutritious tuber helped communities adapt to climate change during the middle Holocene, even as other food sources disappeared.
Now, this potato—and the Westerners who still harvest it—may hold answers to present-day challenges that face the region, including climate change, food security, and reestablishing tribal connections to the land.
* * *
Archaeologist Lisbeth Louderback researches the effects of the middle Holocene climate shift on humans’ diets by sifting through plant remains from ancient hearths. By microscope, she identifies the charred seeds, grains, and other leftovers from meals cooked thousands of years ago to understand how people’s diets reflected their shifting worlds. But one set of samples from fire pits built at North Creek Shelter defied her expertise, resembling nothing she could identify. So Louderback tried something unusual: She analyzed the residues left over millennia on manos and metates, grinding stones and surfaces used to prepare corn and other foods, found at the site. She and her colleagues came to a surprising realization: Almost 11,000 years ago, people in the region were eating a species of potato, known as Solanum jamesii, that’s different from the potatoes widely eaten today, Solanum tuberosum.
Every type of potato purchased in the store today, whether white, red, or purple, is Solanum tuberosum. The Four Corners species, though, is unique, producing small, hardy tubers from long underground shoots, explains Bruce Pavlik, director of conservation at the University of Utah’s Red Butte Garden. The plant, which has white flowers shaped like stars, grows in the mid-elevation pinyon-juniper and oak forests of the Mogollon Plateau in central Arizona and New Mexico. Though starch grains turned up at North Creek Shelter, it’s not a common plant in Utah.
After identifying the starch grains on manos and metates as Solanum jamesii, Louderback teamed up with Pavlik and living descendants of the region’s tribal communities to understand how it got there, and where the potato lives today. As Louderback surveyed archaeological sites and materials for traces of plants that were on the menu thousands of years ago, Pavlik inventoried the vegetation growing on the landscape surrounding past and current sites of human habitation. The research team—with help from citizen scientists—has found more potato plants growing near places of human habitation in Utah, with further evidence of ancient use tucked into university herbarium collections. As expected, the potato has been widely collected from the Mogollon Plateau, Pavlik says. But it also has been collected in Mesa Verde, Chaco Canyon, the Escalante Valley, and other regions inhabited by ancient peoples.
By talking with descendants of pioneer families and researching the diaries of cavalry soldiers, Louderback and Pavlik learned that pioneers called Escalante Valley “Potato Valley” during the 1800s. Delane Griffin, an Escalante man in his 90s, recollected for the researchers how he and other kids would collect the potato during the Great Depression. Griffin directed the researchers to a place where he’d harvested the potato as a child. “Sure enough, it was still there,” Pavlik says.
The evidence suggests that ancient peoples introduced the potato around the Four Corners area. “If you were going on a long journey, these tubers were a perfect food source,” Pavlik says. “And then if you arrived at your destination, the thought might occur to you, ‘I could plant these and come back here sometime in the future and have food.’” Pavlik doubts that the story of how the potato originally came to the Four Corners region is a simple one, though. Over the thousands of years that people have lived in the region, communities have shifted. The very shelter where Louderback first collected manos and metates to analyze has been used intensely and abandoned multiple times over the millennia. “How many events were required to establish a (potato) population, if that’s what happened?” Pavlik wonders. It’s a question he hopes to answer through genetic analysis.
Pavlik cautions that the researchers need to do more work before they can be confident that ancient peoples intentionally brought the potato to these sites. “We want to look at sites that (the potato) isn’t as well as sites that it is,” he explains. “We can’t just be biased and only look in certain places, because it is still a possibility that it’s a remnant of a natural distribution.” Still, the genetics of the plant also suggest a human connection. After monsoon flowering, wild potatoes in the central part of their range produce fruit resembling tiny tomatoes or eggplants. But the potatoes near archaeological sites only reproduce clonally, by growing more tubers. They seem to have lost the ability to use their flowers for sexual reproduction. “It suggests that those Utah populations—the archaeological ones—don’t have all the genes with them they need in order to complete the sexual process,” Pavlik says. “That would be indirect evidence of having been transported because it means they’ve gone through a genetic bottleneck.” He’s testing this idea in greenhouse experiments by crossing potato plants from archaeological sites with potato plants from the Mogollon Plateau and seeing whether they produce fruit.
* * *
Either way, this research has revealed this potato to be an enduring food source, capable of persisting through major environmental changes such as drought, Louderback says. These characteristics make it a valuable food crop for all Westerners in a warming, drying climate. The potato’s genetic diversity also could protect the more common Solanum tuberosum from future blights, so something like the Irish potato famine doesn’t happen again.
Tribal collaboration has been essential to this research. Louderback and Pavlik work with Utah Diné Bikéyah traditional-foods program director Cynthia Wilson to document the occurrence and uses of the potato. Both Louderback and Pavlik support Wilson’s more general goal of reconnecting tribal communities to traditional food resources. “Native Americans have managed the potato for thousands of years,” Louderback says. “It still exists because of them. This is their resource.” Utah Diné Bikéyah is a nonprofit organization that supports the Bears Ears Intertribal Coalition, a group of five tribes working to protect Bears Ears National Monument. Connecting the potato’s current distribution to ancestral history has profound meaning for many of the region’s contemporary tribal inhabitants, some of whom still grow Solanum jamesii today, many others of whom were cut off from traditional food practices through forced relocations. For Wilson, the potato is yet another example of traditional wisdom that’s protected by Bears Ears National Monument, along with petroglyphs, potsherds, and ancestral structures. Wilson learned about the potato from her grandfather. He called it leeyi’ naa’ mááz, or “rolling around under the soil” in the Diné language.
Tribal communities use the potato less now than they historically did, which Wilson attributes to forced removal from their homelands and the arrival of grocery stores. Yet the potato persists in home gardens of Zunis, Hopis, Jemez people, and Navajos, among others. In addition to further documenting how tribes moved through the Four Corners region over millennia, Wilson says, the potato demonstrates how the designation and design of Bears Ears National Monument assists the region’s tribes in once again accessing traditional—and nutritious—food sources. Louderback has found that S. jamesii has twice the amount of protein and calcium and three times the amount of zinc, iron, and manganese as the potatoes most people eat today. And, according to very unscientific trials by Louderback, the hardy spud is quite delicious if roasted in butter and served with salt and pepper. “It was kind of nutty and earthy and it was really fluffy on the inside and crispy on the outside,” she recalls. It can be bitter, especially if eaten raw; the Hopi prepare it with white clay to draw out the bitterness.
The researchers want to make potato plants available to the tribal communities that originally stewarded them, Pavlik says, as well as the communities in which they still exist, such as Escalante. “That’s a place where if you serve the potato there, it would have been a food that’s been served there for nearly 11,000 years,” he says. “There’s nowhere else I can think of where a claim like that could be made. So it really could be pretty cool.”
This post appears courtesy of High Country News.


“The entire paper is an enormous bummer,” Chelsea Wood told me during the annual meeting of the Ecological Society of America in Portland, Oregon, in August. She was referring to research she had published in June as part of a special theme issue, which she had helped to compile and edit, of the Philosophical Transactions of the Royal Society B. The issue explored the link between biodiversity and infectious disease, and Wood had just presented her findings before a packed room. It was a tough audience, she said.
“The news that ecologists want to hear is that biodiversity is something that we can market as an intervention that has co-benefits—an intervention that pays for itself,” she said. “And we don’t see any evidence of that in our data set. Not even a hint of it.”
Wood is an assistant professor at the University of Washington’s School of Aquatic and Fishery Sciences, and she is a prominent voice on one side of what has become a contentious and heated ecological question: Is biodiversity beneficial to human health or not? The argument centers around what’s known as the “dilution effect”—the idea that preserving and protecting an abundance of other species can help to dilute the risk of diseases spreading among humans.
The notion was first proposed in 2001 in relation to the spread of Lyme disease, and in some circles, it has become settled science. “The evidence is already in,” Newsweek declared in 2009. “The loss of biodiversity is itself a threat to public health.” In 2015, the Earth Island Journal asserted unequivocally that “biodiversity limits disease outbreaks among humans and wildlife.”
Proponents of the theory believe the dilution effect is widespread, and they champion human-health policies that include conservation initiatives. But critics say the evidence for this remains thin, and they call such ideas both panglossian and irresponsible. Both sides have sizable support among researchers, and the back and forth in scientific journals has become acrimonious at times.
That’s how science is supposed to work, of course. The problem is that very little coverage of this debate has migrated into the popular-science press—and in that sense, science journalists are succumbing to the same sort of publication biases that we bemoan among scientists themselves: Studies that support appealing ideas get widespread coverage, while ambiguous findings or ones that counter “feel-good” stories are ignored.
Presenting a false balance, as is often seen with climate-change reporting, is problematic. But so, too, is presenting a false consensus. It undermines the public’s understanding of science while providing ammunition to those claiming the media is biased and unreliable.
Intuitively, the dilution effect makes sense.
Take West Nile virus, for example. It’s a bird disease, but people can catch it when bitten by Culex mosquitoes. Where there are more birds overall, it would be statistically less likely for a female mosquito to bite an infected animal, reducing the number of infected mosquitoes buzzing around. Similarly, mosquitoes that are infected would be more likely to come across and seek out a blood meal from another bird than from a human. And some bird species will be more resistant to the infection, helping to check its overall spread.
All of these mechanisms would lead to a “dilution” of the disease and, ultimately, a reduction in the number of human disease cases—or at least that’s the theory. It’s also exactly what researchers in a 2008 study published in PLOS One found. When they compared bird-species richness to West Nile transmission during the 2002 outbreak in the eastern United States, they found a strong, negative correlation between the diversity of birds and cases of the virus. And they aren’t the only ones to find evidence for a dilution effect. A 2015 meta-analysis boldly states so in its title: “Biodiversity Inhibits Parasites: Broad Evidence for the Dilution Effect.”
Jason Rohr is an associate professor at the University of South Florida and senior author of that paper. “There was general support for the hypothesis that when you increase biodiversity you get decline in parasites per host or in parasite prevalence,” he told me in an interview. The results are convincing enough that Rohr says conservation should be seen as a prophylactic. Much like exercise and a balanced diet are considered effectively safe methods of supporting health, “general biodiversity conservation,” he said, “should be approached as a general proactive strategy to disease management.”
But that’s where critics of the dilution effect strongly disagree. They see the literature very differently.
“A key element of their argument is that our previous meta-analysis, which failed to find unequivocal support for dilution effects in zoonotic disease systems, should be discounted,” write Daniel Salkeld and his colleagues in a critique of the paper by Rohr and his colleagues. Their work focused in on just human pathogens, and found “weak support at best for the dilution effect” and “evidence of publication bias toward publishing reports of a negative relationship between biodiversity and disease,” Salkeld argued.
The bitter debate isn’t just about interpretation of results. Critics also say the research to date is flawed because it has only examined a small subset of human pathogens. Studies focus on accessible disease systems like Lyme and West Nile virus—ones that Wood says almost exclusively affect affluent Westerners. Rohr and his coauthors included just 14 human diseases, for example (though the analysis included schistosomiasis, which predominantly affects the developing world).
By broadly claiming the majority of infectious diseases will follow the patterns of West Nile and Lyme, Wood says Rohr and other proponents are trying to apply a “pretty thinly supported idea that’s been demonstrated only in diseases of rich people” to diseases of poverty in developing countries.
That’s why she and her colleagues decided to focus on the infectious diseases tracked by the World Health Organization’s global burden of disease database instead. “We felt like it was really important to figure out whether dilution worked for these more-important diseases because people were starting to call for conservation as this intervention,” Wood told me, “and we felt that was really irresponsible because the evidence is so thin.”
Wood and her colleagues examined the change in disease burden for 24 infectious diseases over two decades, including notorious killers like HIV/AIDS, tuberculosis, and malaria. Using per-person disability-adjusted life years as their measure of lethal and nonlethal health impacts, they examined a variety of potential drivers of these diseases in 60 intermediate-sized countries, including forestation and urbanization—which Wood considers proxies for biodiversity tied to conservation action—as well as a calculated measure of biodiversity per unit area.
“Contrary to the dilution-effect hypothesis, increases in biodiversity over time were not correlated with improvements in human health, and increases in forestation over time were actually associated with increased disease burden,” the authors concluded. Or, as Wood puts it, “If anything, conservation is potentially a problem for infectious-disease transmission.”
Of course, Rohr and other proponents were quick to find flaws with this analysis.
“I could go on and on and on and talk about all of the factors that are conflated with biodiversity by using urbanization or deforestation as a proxy,” he told me. To Rohr, the study doesn’t “provide a real solid test of the hypothesis relative to studies that do a better job of isolating the effects of biodiversity.”
And so continues the disagreement among scientists—a debate that remains invisible for many general readers.
Indeed, based on media coverage over the last 10 years, it would seem the dilution effect is universally supported.
“A new study shows that biodiversity serves as a bulwark against the transmission of diseases and parasites,” Conservation Magazine’s Sarah DeWeerdt writes in an article covering the 2015 meta-analysis by Rohr and his coauthors. DeWeerdt’s piece was titled “The Bloodsucking Consequences of Biodiversity Decline.”
Such stories sometimes include conditional words like “may” and “might,” but they often fail to provide voices of critics like Wood. And while there have been instances of balanced coverage—by NPR, for example, or TakePart, or a nuanced piece, which included Wood’s perspective, published by Smithsonian magazine in May—such articles tend to take their inspiration from new research and papers that support the dilution hypothesis, relegating Wood and her colleagues to counterpoints instead of leads.
Sometimes, counterpoints are lacking altogether. “Biodiversity protects ecosystems against infectious diseases, researchers have concluded,” a 2010 Nature News story boldly stated. The only outside comment comes from Conservation International senior scientist Will Turner, who did not doubt the universality of the findings. “The clear message is that we degrade ecosystems at our own peril,” Turner told Nature. Similar coverage came from NPR.
But the most glaring issue seems to be what studies are chosen for coverage. Those that find evidence for the dilution effect are readily picked up by a diversity of outlets. A search for “dilution effect” brings up article after article on such studies, including coverage in prestigious outlets like The New York Times. Studies that find amplification of disease by biodiversity or no relationship at all are generally overlooked. I could only find these two old pieces from Futurity and Bioscience Technology reporting the meta-analysis that didn’t find support for the dilution effect, and a few articles in outlets like The Scotsman and Farmers Weekly covering a recent paper suggesting biodiversity may actually increase risk of Lyme disease.
“When papers that support the dilution effect come out, they tend to get tons and tons of press coverage,” said Wood. “In contrast, when you’re telling the story that nature is actually dangerous for people, that has less legs.”
The bias in reporting gives the impression that the dilution effect is fairly uncontroversial. But ecologists as a group are far less certain than such coverage would suggest. “We have a lot of people now who are of the opinion that sometimes you see dilution, and sometimes you see amplification, and sometimes neither,” says Skylar Hopkins, a postdoctoral associate with the National Center for Ecological Analysis and Synthesis at the University of California, Santa Barbara. Andrew MacDonald, a National Science Foundation postdoctoral research fellow at Stanford University, echoed this idea. “At least at this stage, there’s a lot more that we need to learn about the ecology of disease,” he said, “and how it relates to diversity.”
Both researchers say they are somewhat frustrated that the press coverage they’ve seen so far is lopsided. “It’s not necessarily exciting to share work that has a whole bunch of caveats,” MacDonald said. “The challenge for science journalists in general is how to talk about scientific uncertainty and how to get that across to the general public, who might be more interested in a yes/no answer,” MacDonald says.
Hopkins adds that hypotheses like the dilution effect need to be handled with extra care. “The argument here is that we can reduce human risk of disease and that’s a really big deal,” she said, “so we don’t want to promise that if that’s not something that can actually be delivered as a solution.”
Ultimately, that’s Wood’s biggest concern, too. “We need to not close our eyes to the fact that conservation can sometimes increase human disease risk,” she said.
* * *
Of course, Wood would be upset if her research was used as an excuse to halt conservation efforts. “I didn’t get into this business because I want people to cut down forests,” she emphasizes. But she also fears that claiming conservation will broadly reduce disease could lead to unintended consequences.
“My position is not that the dilution effect never happens—that is absolutely the opposite of what I think,” said Wood, pointing out several studies that have found evidence. “But there’s also a whole bunch of parasites that show the opposite response.” Wood says she “would be 100 percent behind efforts to use conservation to control infectious diseases,” if those efforts put the time and resources into examining how their actions affect the diversity of human pathogens in the area.
“Our argument is that you really need to understand the whole spectrum of responses of disease to environmental changes, because otherwise, conservation is a roll of the dice in terms of public health,” she explained. “We want people to go into conservation with their eyes wide open so that they can plan for potential collateral impacts.”
Any associated costs with implementing disease monitoring or prophylactic measures given that risk, Wood says, are “very minor when compared to the potential damage you could do through a conservation project that initiates an epidemic.”
It’s a sentiment that journalists would be wise to heed as well. After all, when covering hypotheses like the dilution effect that are hotly debated amongst scientists, the costs of diligent coverage are minor compared to the potential harms of shoddy reporting.
One-sided coverage not only undermines the media’s credibility and the public’s trust in the process of science. Such weighted stories imply advocacy, even if carelessly. And if, as Wood supposes, an outbreak occurs after our tacit approval, then we, too, would share in the blame for the lives destroyed by it.
This post appears courtesy of Undark Magazine.


In 1986, the social psychologist David Sears warned his colleagues that their habit of almost exclusively studying college students was producing a strange and skewed portrait of human nature. He was neither the first to make that critique, nor the last: Decades later, other psychologists noted that social sciences tended to focus on people from WEIRD societies—that is, Western, educated, industrialized, rich, and democratic. The results of such studies are often taken to represent humanity at large, even though their participants are drawn from a “particularly thin and rather unusual slice” of it.
The same concerns have been raised in virtually every area of science that involves people. Geneticists have learned more about the DNA of people in Europe and North America than those in the rest of the world, where the greatest genetic diversity exists. The so-called Human Microbiome Project was really the Urban-American Microbiome Project, given that its participants were almost entirely from St. Louis and Houston.
Neuroscience faces the same problems. When scientists use medical scanners to repeatedly peer at the shapes and activities of the human brain, those brains tend to belong to wealthy and well-educated people. And unless researchers take steps to correct for that bias, what we get is an understanding of the brain that’s incomplete, skewed, and,  well, a little weird.
Kaja LeWinn, from the University of California, San Francisco, demonstrated this by reanalyzing data from a large study that scanned 1,162 children ages 3 to 18 to see how their brain changed as they grew up. The kids came from disproportionately wealthy and well-educated families, so LeWinn adjusted the data to see what it would look like if they had been more representative of the U.S. population. That's called “weighting,” and it’s a common strategy that epidemiologists use to deal with skews in their samples. As an easy example, if you ended up recruiting twice as many boys as girls, you’d assign the girls twice as much “weight” as the boys.
When LeWinn weighted her data for factors such as sex, ethnicity, and wealth, the results looked very different from the original set. The brain as a whole developed faster than previously thought, and some parts matured earlier relative to others.
Natalie Brito, from New York University, says that this study “clearly shows how our interpretation of brain development changes based off who is being represented within the sample.” She adds that most neuroscientists would acknowledge or agree that representative samples are a good thing, but that there are practical reasons why such samples are hard to get. Most obviously, brain-scanning studies are very expensive, so most of them are small and rely on “samples of convenience”—that is, whoever’s easiest to recruit.
“Neuroimaging research is also complex and difficult to conduct, and because of this, I think there is a tendency to focus on its technological aspects,” says Duke Han, from the University of Southern California. That’s symptomatic of a problem in neuroscience that I’ve written about before: an inclination to focus on the technological innovations that allow us to study the brain, and to forget about the people whose brains are being studied.
Perhaps the brain itself invites this lapse. We intuitively understand that our thoughts and behavior vary considerably from person to person. But when it comes to the lump of gray tissue behind those behaviors, it’s easy to forget that variation. “To a degree, I think there’s a sense that a brain is a brain is a brain,” says LeWinn. “That’s problematic. Everyone’s brain is shaped by their experiences, and we want to capture the diversity of people’s experiences rather than just a few kinds.”
For example, in the study she reanalyzed, around 35 percent of the children had parents with college backgrounds, and around 38 percent had parents who earned more than $100,000 a year. If the sample had been truly representative of the U.S. population, those proportions would have been 11 percent and 26 percent, respectively. And weighting the data to account for these biases produced a different picture of brain development.
Brains get bigger as we get older, before shrinking again during later childhood. In the unweighted data, the brains hit their peak volume at 6 years on average, and their peak surface area at around 12 years. But in the weighted data, the brains hit those milestones 10 months and 29 months earlier, respectively. The pattern of development across the brain also changed. In the unweighted data, three of the brain’s four lobes hit their maximum area from the ages of 12 to 13, with only the parietal lobe peaking earlier at around 10 years. But the weighted data showed more of a wave of maturation, from the back of the brain to its front, and going from 9 years to 11.
“These results shine a much-needed spotlight on an issue that does not receive enough attention in neuroimaging research—the lack of diversity among study participants,” says Han. “Unless these issues are adequately addressed, it would be wise to show temperance in discussing the implications of a study.”
Jim Coan, from the University of Virginia, learned the same lesson in his own work. A decade ago, he put 16 women in a brain scanner, promised to give them an electric shock, and looked at parts of their brains that respond to threats. He found that these areas are less active if the women held the hand of a stranger, even less active if they held their spouse’s hand, and less active still if they were in an especially happy relationship. “I had to raise $30,000 to do that experiment and everyone was white, wealthy, well educated. And yet, we thought: Here’s the story,” he says. “By yourself, you’re maximally responsible for meeting the demand of the threatening situation so you have more of a threat response. If you’re with your trusted romantic partner, you’re minimally responsive because you outsource.”
Years later, he got more money to do a bigger and more representative study of racially and socioeconomically diverse people drawn from the local community. “And the findings changed,” he says. The romantic partners still reduced the threat response, but a stranger’s hand had no effect at all. Why? Perhaps it’s because, as he showed in another study, the wealth of the neighborhood you grow up in affects the way your brain weighs up rewards and threats. “This shouldn’t surprise anybody,” he says. “The context in which you develop shapes the way your brain functions and probably the way it’s structured.”
Neuroscientists are increasingly grappling with this self-evident truth. Brain-scanning studies are getting bigger, and researchers are making more of an effort to recruit samples that are at least representative of the local community—if not America as a whole.
The Adolescent-Brain Cognitive Development study—the largest study of childhood brain development in the United States—is perhaps doing the best job. It’s looking to recruit around 11,500 children, ages 9 and 10, and follow their brain development over the next 10 years. The plan is to get a truly representative sample, and to deal with any small skews with the same weighting approach that LeWinn uses. That has many advantages, says Wes Thompson, from the University of California, San Diego, who is involved in the study. You not only can see what the average American brain looks like as it grows up, but you can see how different subgroups differ from this population-wide norm, and how individuals differ from their particular subgroup.
“Since bigger studies are now coming online, this is the time to think about the sampling issue,” says LeWinn. “We’re finally doing studies that are large enough to get representative samples.”


Americans have a complicated relationship with fear.
On the one hand, we enjoy fear enough to dedicate a holiday to it. This year, we will spend an estimated $9.1 billion celebrating Halloween. Horror films gross nearly half a billion dollars per year, and are known in Hollywood to have the best return on investment in the movie business. Quasi-dangerous activities like roller coasters are a big industry as well, following Hunter S. Thompson’s famous exhortation, “Faster, faster, until the thrill of speed overcomes the fear of death.”
These pursuits are occasions of “fake” fear. They simulate frightening circumstances that lie outside the realm of ordinary life, providing a fun shot of adrenaline without putting anyone in actual danger.
Real threats, however, are far less enjoyable. Not even roller-coaster fans look forward to losing their car’s brakes on a steep hill. To enjoy genuine mortal danger is considered abnormal: Indeed, in psychology, the “fear-enjoyment hypothesis” holds that pleasure from authentic fear increases along with sociopathic traits.
Given that real fear can be scarring and unpleasant, there’s a temptation to believe that the best way to deal with it is to avoid it at all costs. But science and philosophy often suggest otherwise. Fear can be one of the great sources of personal improvement. In particular, fear can help people cultivate several classic virtues that religious figures, sages, and secular moral traditions have all seen as essential for living a well-ordered life.
One such virtue is courage. The University of British Columbia psychologist Stanley Rachman, a leading expert on fear, has studied people in the world’s most dangerous professions, from bomb defusers to paratroopers. He has concluded that courage is misunderstood when it is defined as complete fearlessness. In his book Fear and Courage, Rachman makes the case that courage is not the absence of fear, but the decision to go forward in spite of it. Brave people are not merely numb to danger or discomfort; they feel and acknowledge fear, and just refuse to allow it to dominate their behavior.
By this definition, fear is not the antithesis of courage. It is courage’s necessary precondition. The German philosopher Josef Pieper writes in The Four Cardinal Virtues that a man can only show courage when he “walks straight up to the cause of his fear and is not deterred from doing that which is good.” For Pieper, this final qualifier is important: The confrontation of fear must be oriented toward the common good. In practice, this could mean confronting your fear on behalf of people weaker than you—for instance, risking physical harm to bring someone else to safety in an emergency, or speaking up to stop bullying. (This rules out extreme thrill seeking and other fear-provoking situations that are ultimately just entertainment.)
Fear also can signal where people need to do moral work on themselves. Buddhists, for example, believe that fear is a sign of attachment. According to the Buddha, the key to freedom from fear is to abandon “passion, desire, fondness, thirst, fever, and craving for sensuality.” There is a famous Zen Buddhist story about a band of samurai who ride through the countryside causing destruction and terror. As they approach a monastery, all the monks scatter in fear, except for the abbot. The samurai enter to find him sitting in the lotus position in perfect equanimity. Drawing his sword, the leader snarls, “Don’t you see that I am the sort of man who could run you through without batting an eye?” The master responds, “Don’t you see that I am a man who could be run through without batting an eye?”
For non-Buddhists, indifference to death might entail a bit more nonattachment than is optimal. But virtually every major faith and moral tradition preaches the same core principle. Christians and Jews see a similar connection between fear and the deadly sin of pride—“an excessive desire for one’s own excellence,” in the formulation of Thomas Aquinas. Modern research might back a connection like this up: Rankings of Americans’ top fears consistently reveal that one of their most prevalent social fears is public speaking. Presumably, the explanation is that people are abjectly terrified of humiliation in front of others. As Rousseau phrases it in his Confessions: “I was not afraid of punishment, I was only afraid of disgrace; and that I feared more than death, more than crime, more than anything else in the world.”
A few jitters before a big presentation is one thing. But a paralyzing horror of being judged by others? That seems to be a bright-red warning that too much of your identity is tied up with others’ esteem for you.
If fear is often a weed growing over the roots of attachment and pride, what can we do? Try taking inventory of your daily anxieties and worries. See which ones boil down quickly to your wealth, your looks, your reputation, your social status, or your influence. Then attack these inordinate attachments—and be grateful for the fear that led you to them.
John Bunyan wrote in The Pilgrim’s Progress in 1678 that fear “keeps the soul tender.” Properly understood, fear is good. So this Halloween, don’t settle for fake fear. Enjoy the haunted house, but then embrace the real thing.


The first thing to know about crows is that a group of them is called a murder.
In America, crows count as a Halloween decoration, like skeletons and mini-gravestones. Homeowners perch plastic ones in their trees to instill fear in passersby. People in many cultures consider the crow to be an omen, a harbinger of war and death. In Islamic hadith, reports about Muhammad’s sayings and practices, crows are one of the five animals “for which there is no blame on the one who kills them.” On the Faroe Islands, virginal women once had to throw a stone, a bone, and a clump of dirt at a crow for some reason. Farmers literally erect mannequins in their fields to scare them.
Can we give crows a break?
The world’s treatment of crows is grossly unfair. When I was a kid, my parents told me about their friend Linda, an elementary-school teacher in Illinois. One day, Linda and her class decided to adopt a zoo animal as a service project. When she was reading to them from a list of available animal adoptees, “crow” came up as an option. “Who would want a stupid old crow?” one little boy asked. The class adopted a polar bear instead.
When I heard this story as a child, it devastated me. I knew what it was like to get picked last. I would feel a pang of pity whenever I saw a crow perched on the power lines behind my house or pecking through the yard. Stupid old crow, I would say to myself sadly. But in fact, crows are not stupid. That little boy in Illinois can eat crow—because these birds are really, really smart.
Crows, along with magpies, jays, and ravens, are members of the family Corvidae, a group known for two things: exhibiting complex behaviors, and having massive brains. A New Caledonian crow named Betty once made a TED Talk audience go bananas by bending a wire to make a hook. And researchers believe the Hawaiian crow is likely to have the same talent.
Scientists Have Found Another Crow That Uses Tools
“One of the reasons I got so excited by them was because hooked-tool use is something that only New Caledonian crows and ourselves have invented,” said Alex Taylor, a senior lecturer in psychology at the University of Auckland. “The great apes haven’t invented using hooks, and [humans] only invented using hooks probably around 100,000 years ago. It’s possible that crows beat us to that bit of technology.” Taylor says the birds have a high encephalization quotient, which means their brains are really big for their bodies. The crows he works with aren’t spooky; they’re observant: “Basically it feels like you’re constantly being observed and assessed,” Taylor says. “You’re dealing with a very smart and watchful animal.”
Kevin McGowan, an ornithologist at the Cornell Lab in New York, told me that the crow’s spooky reputation is pretty unwarranted. “It’s because they’re black and they’re scavengers,” McGowan said. Their bad rap started in northern Europe, where there are no vultures, so ravens and crows were always the first to show up to snack on animal carcasses. When Europeans came to North America, they brought their crow prejudice with them. “That whole combination of being near death, not having a very pretty song, that was all a big negative stigma for these guys to overcome.” He also wishes The Birds had never been made.
“Crows are beautiful,” McGowan says. “A big adult in the sunshine is beautiful, the way the light shines off their wings ... They’re just a really nice-looking bird.”
But sometimes crows are victims of mistaken identity. Grackles look like crows, except they are smaller and shinier, more of an iridescent purple. They bully other birds at feeders, and they can damage crops. They also make a horrible sound, like a screen door closing, or a rusty gate swinging open. Grackles are the birds that you hear and you think ugh. Crows, though. Crows are matte black, and they make a sharp, clear caw caw. Ravens, like the one who barges into the chamber in the Edgar Allan Poe poem, are also not crows. They are much larger, roughly the size of a red-tailed hawk.
While humans might sometimes conflate crows with their avian relatives, research shows that crows rarely forget a face. John Marzluff, a wildlife biologist at the University of Washington, once conducted an experiment with two masks—a “dangerous” mask and a “neutral” mask. His assistants wore the dangerous mask to capture and band several crows, and for months afterward, he and his students wore both disguises around campus: The ones in the dangerous masks were repeatedly scolded by crows, while the ones in neutral masks were left alone. The crows also appeared to warn their friends of the danger at hand.
They have “great family values,” McGowan told WBUR in 2015. “They do neighborhood watch. They help each other out.” McGowan said. “They are very interested in working together to make the world a safer place for other crows.”
Kaeli Swift, a Ph.D. student at the University of Washington, researches crow “funerals.” On her blog Corvid Research, Swift writes that crows “appear to respond strongly once they discover a dead member of their own species.” They use death as a teachable moment, gathering around their fallen comrades in an attempt to identify potential threats. It’s something only a handful of nonhuman animals do.
And they can be generous gift givers, too: In 2015, an 8-year-old girl in Seattle named Gabi Mann started feeding her neighborhood crows scraps of food. In return, they left her dozens of little tokens—buttons and paper clips and pieces of colored glass. Gabi told the BBC that her favorite gift from the crows was a tiny heart charm. “It's showing me how much they love me,” she said.
It’s high time we give these resourceful, community-minded creatures some love in return. McGowan, who has spent more than 30 years studying and teaching people about crows, says he’s working to do just that. “I do try to tell people they’re not evil birds,” he said. “They’re just trying to make their way and do the best they can.” He tells me that this year at the Cornell Lab, the tagging color is orange, which means every crow identified by his team will get a little pumpkin-colored tag pinned to its wing. “It looks real nice, the orange with the black feathers,” McGowan says. “It looks really nice.”


In 1928, after returning from a countryside holiday and examining a stack of petri dishes that he had left in the sink, British chemist Alexander Fleming discovered a new type of bacteria-killing mold. From that mold, he isolated a chemical called penicillin, and ushered in the modern antibiotic era—an age when humans could finally keep infectious diseases at bay. But in 1945, two years after penicillin became widely used and shortly after Fleming won a Nobel Prize for its discovery, he issued a stark warning against overusing these wondrous chemicals. “The thoughtless person playing with penicillin treatment is morally responsible for the death of the man who succumbs to infection with the penicillin-resistant organism,” he told The New York Times.
His words were prophetic. The year before, scientists had identified penicillin-resistant strains of Staphylococcus aureus—a bacterium that commonly lives on our skin and in our noses, but sometimes causes life-threatening infections. As penicillin became more widely used, these resistant strains also became more common. To deal with these incipient superbugs, scientists turned to methicillin—a chemical relative of penicillin. But its usefulness was also short-lived. A year after it made its way into British clinics in 1959, Margaret Patricia Jevons isolated three strains of methicillin-resistant Staphylococcus aureus, or MRSA.
MRSA is now a global problem, and has become something of a poster child for the superbug threat. It supposedly showed how bacteria can quickly evolve to resist a drug that comes into wide use—a process that’s illustrated in the video below, the second in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.

But the MRSA origin story always had a few glaring plot holes. For a start, the three initial strains of resistant staph all came from patients who had never been exposed to methicillin, and who were treated in a hospital that had only ever used the drug once. On top of that, MRSA appeared in India and some Eastern European countries before those nations started using methicillin. How exactly did the bacteria evolve to resist a drug that they had never actually seen?
Catriona Harkins and Matthew Holden at the University of St. Andrews have the answer, in a study that turns the history of MRSA on its head, and makes it even scarier than before.
They sequenced the DNA of 209 MRSA samples that were collected between 1960 and 1989, including the earliest resistant strains ever identified. By comparing these strains and reconstructing their evolutionary history, the team calculated that they all descended from a common ancestor that first acquired the ability to resist methicillin in 1946—13 years before people started using the drug to treat infections. “Methicillin use was not the original driving factor in the evolution of MRSA as previously thought,” they write.
That original driving factor was, ironically, penicillin. It turns out that mecA, the same gene that allows staph to shrug off methicillin, also confers some measure of resistance to penicillin. When penicillin became widely used in the 1940s, it likely fueled the rise of staph strains that carried mecA, and were already resistant to methicillin. Initially, those strains were rare—despite its benefits, mecA doesn’t spread easily between staph strains, and slows the growth of the microbes that carry it. But as soon as methicillin came into use, the advantages of carrying mecA outweighed the disadvantages, and the gene became more common.
So bacteria can begin evolving resistance to antibiotics that they haven’t even encountered yet. And new drugs can be neutralized by adaptive genes that are lurking in the environment, waiting for the chance to rise to the occasion. As Hsu Li Yang from the National University of Singapore writes, “Antibiotic resistance is a web of unintended consequences, rather than a simplistic cause-effect model that we often find (too much) comfort in.”
We might be able to predict some of these unintended consequences by routinely sequencing the DNA of wild bacteria, identifying the resistance genes that are already out there. “However, this will be difficult because it is not currently possible to confidently predict resistance from genome-sequence data alone,” says Jessica Blair from the University of Birmingham. “We can only detect resistance mechanisms we already know about.”


Twenty-five years ago, James Anderson discovered a fungus that expanded the possibilities of life on Earth.
It was a single fungus of the genus Armillaria, weighing an estimated 22,000 pounds and spread over a remarkable 15 hectares. The organism had been growing for around 1,500 years, more than a millennium before the land under which it grew even became the state of Michigan. When Anderson and his collaborators wrote it up in Nature, they suggested it was “among the largest and oldest living organisms” in the world.
This suggestion, in its use of superlative, set off a competition—naturally. Scientists all over the world were soon hunting for Armillaria, or honey mushrooms, in their own forests. The title of the largest fungus in the world eventually went to one in the Malheur National Forest in Oregon: nearly 1,000 hectares big, as many as 8,650 years old. This “humongous fungus,” as it’s sometimes called, is by some counts still the largest living organism ever found.
Fungi normally grow as mycelia—soft, white, cottony tufts that you may have seen on food kept in the fridge too long. Some of them also form mushrooms. But Armillaria, somewhat uniquely, can also grow thick, black, rootlike rhizomorphs whose networks can extend miles through the soil in search of wood to eat. The rhizomorphs, scientists think, are what allow a single Armillaria organism to get so big. A comprehensive new genetic study takes up the question of how Armillaria got its rhizomorphs.
“Ever since I since I was a graduate student, I wanted to do exactly the study that was just published,” says Anderson, who is now a professor of biology at the University of Toronto. Anderson contributed a couple of genomes for the study, but the bulk of the research and analysis was done by György Sipos and László Nagy, of the University of Sopron and the Hungarian Academy of Sciences, respectively.
Sipos and Nagy not only sequenced four species of Armillaria, but they also pinpointed genes active in the fungus’s rhizomorphs and mushrooms. To identify those genes, they had to figure out how to grow at least one species of Armillaria in a lab.
The rhizomorphs were the easy part. Once Armillaria took to their growing medium of rice, sawdust, tomato, and orange—“this fungus has really weird tastes,” notes Nagy—they spontaneously formed rhizomorphs. The mushrooms were much trickier. They had to trick the fungus into thinking it was autumn, which they did by moving their fungi to colder temperatures with progressively less light in Nagy’s lab. Sipos says his colleagues “did a really excellent job. It used to be be difficult to make this fungus produce the mushroom.” They succeeded in getting one species—Armillaria ostoyae, also the species of the giant Oregon fungus—to produce mushrooms.
All that trouble paid off though. When the team got the sequencing data back, they noticed that the same networks of genes appear to be active in both the fungus’s rhizomorphs and its mushrooms. It suggests one potential evolutionary origin for rhizomorphs in this genus: Armillaria could have gained its rhizomorphs—and consequently its ability to spread so wide—by co-opting genes originally used to grow mushrooms. Nagy speculates the rhizomorphs may be akin to mushroom stems that failed to sprout and grow a cap, instead growing long and thin underground.
The rise of Armillaria has come at the expense of trees. The fungus actually grows into trees and spread under the bark. At first they digest living wood and when they’ve done enough damage, they continue to feast on the dead wood. “You can basically see entire hills wiped out, entire forests wiped out,” says Nagy. You can’t see much of the humongous fungus in the Malheur Forest in Oregon since the Armillaria is mostly underground, but you can see all the trees it has killed.
Armillaria as a genus not a particularly picky eater either, and it attacks all sorts of plants. Understanding how the fungus spreads could impact many agricultural industries. For example, Kendra Baumgartner, a plant pathologist at the U.S. Department of Agriculture, studies Armillaria that specifically attack California vineyards. She was ecstatic to see the new study, which also catalogues the genomes, proteins, and active genes in Armillaria. “They generated an incredible amount of data,” she says. When we spoke last week, she told me she had the article’s official publication date marked on her calendar, so she could start digging through the data as soon as it’s out.
How a Guy From a Montana Trailer Park Overturned 150 Years of Biology
This fall, Anderson went back to the Michigan fungus whose discovery he first reported in the 1990s. He is also sequencing this fungus now—specifically, different parts of the same fungus—in hopes of understanding how it has mutated over its 1,500 years of life. DNA-sequencing technology has come a long way in 25 years.
But there is another thing Anderson told me he wishes he could do that he knows he never will. For all the estimates of how big Armillaria can grow, no one has really seen it in full. “I wish all of the substrate”—the soil and matter the fungus grows in—“would be transparent for five minutes, so I could see where it is and what it’s doing. We would learn so much from a five-minute glimpse.”


To many cosmologists, the best thing about neutron-star mergers is that these events scream into space an otherwise close-kept secret of the universe. Scientists combined the gravitational and electromagnetic signals from the recently detected collision of two of these stars to determine, in a cleaner way than with other approaches, how fast the fabric of the universe is expanding—a much-contested number called the Hubble constant.
In the days since the neutron-star collision was announced, Hubble experts have been surprised to find themselves discussing not whether events like it could settle the controversy, but how soon they might do so.
Scientists have hotly debated the cosmic-expansion rate ever since 1929, when the American astronomer Edwin Hubble first established that the universe is expanding—and that it therefore had a beginning. How fast it expands reflects what’s in it (since matter, dark energy, and radiation push and pull in different ways) and how old it is, making the value of the Hubble constant crucial for understanding the rest of cosmology.
And yet the two most precise ways of measuring it result in different answers, with a curious 8 percent discrepancy that “is currently the biggest tension in cosmology,” said Dan Scolnic of the University of Chicago’s Kavli Institute for Cosmological Physics. The mismatch could be a clue that cosmologists aren’t taking into account important details that have affected the universe’s evolution. But to see if that’s the case, they need an independent check on the measurements.
Neutron-star collisions—newly detectable by the Laser Interferometer Gravitational-Wave Observatory (LIGO) and Virgo detectors—seem to be just the thing.
“This first [collision] gives us a seat at the cosmology table,” Daniel Holz, an astrophysicist with the University of Chicago and LIGO who was centrally involved in the new Hubble measurement, said in an email. “And as we get more, we can expect to play a major role in the field.”
In an expanding universe, the farther away an astronomical object is, the faster it recedes. The Hubble constant says how much faster. Edwin Hubble himself estimated that galaxies move away from us 500 kilometers per second faster for each additional megaparsec of distance between us and them (a megaparsec is about 3.3 million light-years). This was a gross overestimate; by the 1970s, astrophysicists favored values for the Hubble constant around either 50 or 100 kilometers per second per megaparsec, depending on their methods. As errors were eliminated, these camps met near the middle. However, in the past year and a half, the Hubble trouble has reheated. This time, 67 stands off against 73.
The higher estimate of 73 comes from observing lots of astronomical objects and estimating both distance and velocity for each one. It’s relatively easy to see how fast a star or galaxy is receding by looking at its “redshift”—a reddening in color that happens for the same reason the sound of a receding ambulance’s siren drops in pitch. Correct for an object’s “peculiar velocity,” caused by the gravitational pull of other objects in its neighborhood, and you’re left with its recessional velocity due to cosmic expansion.
Historically, however, it has proven much, much harder to measure the distance to an object—the other data point needed to calculate the Hubble constant.
To gauge how far away things are, astronomers build up rungs on a “cosmic-distance ladder” in which each rung calibrates more-distant rungs. They start by deducing the distances to stars in the Milky Way using parallax—the stars’ apparent motion across the sky over the course of the year. With this information, astronomers can deduce the brightness of so-called Cepheid stars, which can be used as so-called standard candles because they all shine with a known intrinsic brightness. They then spot these Cepheid stars in nearby galaxies and use them to calculate how far away the galaxies must be. Next, the Cepheids are used to calibrate the distances to Type Ia supernovas—even brighter (though rarer) standard candles that can be seen in faraway galaxies.
Each jump from one rung to the next risks miscalculation. And yet, in 2016, a team known as SH0ES used the cosmic-distance ladder approach to peg the Hubble constant at 73.2 with an accuracy of 2.4 percent.
However, in a paper published the same year, a team used the Planck telescope’s observations of the early universe to obtain a value of 67.8 for the current expansion rate—supposedly with 1 percent accuracy.
The Planck team started from the faint drizzle of ancient light called the cosmic microwave background (CMB), which reveals the universe as it looked at a critical moment 380,000 years after the Big Bang. The CMB snapshot depicts a simple, nearly smooth, plasma-filled young universe. Pressure waves of all different wavelengths rippled through the plasma, squeezing and stretching it and creating subtle density variations on different length scales.
At the moment recorded in the CMB, pressure waves with particular wavelengths would have undergone just the right fraction of an undulation since the Big Bang to all reach zero amplitude, momentarily disappearing and creating smooth plasma densities at their associated length scale. Meanwhile, pressure waves with other wavelengths undulated just the right amount to exactly peak in amplitude at the critical moment, stretching and squeezing the plasma to the full extent possible and creating maximum density variations at their associated scales.
These peaks and troughs in density variations at different scales, which can be picked up by telescopes like Planck and plotted as the “CMB power spectrum,” encode virtually everything about the young universe. The Hubble constant, in particular, can be reconstructed by measuring the distances between the peaks. “It’s a geometric effect,” explained Leo Stein, a theoretical physicist at the California Institute of Technology: The more the universe has expanded, the more the light from the CMB has curved through expanding space-time, and the closer together the peaks ought to appear to us.
Other properties of nature also affect how the peaks end up looking, such as the behavior of the invisible “dark energy” that infuses the fabric of the cosmos. The Planck scientists therefore had to make assumptions about all the other cosmological parameters in order to arrive at their estimate of 67 for the Hubble constant.
The similarity of the two Hubble measurements “is amazing” considering the vastly different approaches used to determine them, said Wendy Freedman, an astrophysicist at the University of Chicago and a pioneer of the cosmic-distance ladder approach. And yet their margins of error don’t overlap. “The universe looks like it’s expanding about 8 percent faster than you would have expected based on how it looked in its youth and how we expect it to evolve,” Adam Riess of Johns Hopkins University, who led the SH0ES team, told Scientific American last year. “We have to take this pretty darn seriously.”
The 67-versus-73 discrepancy could come down to an unknown error on one side or both. Or it might be real and significant—an indication that the Planck team’s extrapolation from the early universe to the present is missing a cosmic ingredient, one that changed the course of history and led to a faster expansion rate than otherwise expected. If a hypothesized fourth type of neutrino populated the infant universe, for instance, this would have increased the radiation pressure and affected the CMB peak widths. Or dark energy, whose repulsive pressure accelerates the universe’s expansion, might be getting denser over time.
Suddenly, neutron-star collisions have materialized to cast the deciding vote.
The crashing stars serve as “standard sirens,” as Holz and Scott Hughes of the Massachusetts Institute of Technology dubbed them in a 2005 paper, building on the work of Bernard Schutz 20 years earlier. They send rushes of ripples outward through space-time that are not dimmed by gas or dust. Because of this, the gravitational waves transmit a clean record of the strength of the collision, which allows scientists to “directly infer the distance to the source,” Holz explained. “There is no distance ladder, and no poorly understood astronomical calibrations. You listen to how loud the [collision] is, and how the sound changes with time, and you directly infer how far away it is.” Because astronomers can also detect electromagnetic light from neutron-star collisions, they can use redshift to determine how fast the merged stars are receding. Recessional velocity divided by distance gives the Hubble constant.
From the first neutron-star collision alone, Holz and hundreds of coauthors calculated the Hubble constant to be 70 kilometers per second per megaparsec, give or take 10. (The major source of uncertainty is the unknown angular orientation of the merging neutron stars relative to the LIGO detectors, which affects the measured amplitude of the signal.) Holz said, “I think it’s just pure luck that we’re smack in the middle,” between the cosmic-distance-ladder and cosmic-microwave-background Hubble estimates. “We could easily shift to one side or the other.”
The measurement’s accuracy will steadily improve as more standard sirens are heard over the next few years, especially as LIGO continues to ramp up in sensitivity. According to Holz, “With roughly 10 more events like this one, we’ll get to 1 percent [of error],” though he stresses that this is a preliminary and debatable estimate. Riess thinks it will take more like 30 standard sirens to reach that level. It all depends on how lucky LIGO and Virgo got with their first detection. “I do think the method has the potential to be a game changer,” said Freedman. “How fast this will occur [or] what the rate of these objects will be ... we don’t yet know.”
Scolnic, who was part of SH0ES, said his team’s tension with Planck’s measurement is so large that “the standard siren approach doesn’t need to get to 1 percent to be interesting.”
As more standard sirens resound, they’ll gradually home in on the Hubble constant once and for all and determine whether or not the expansion rate agrees with expectations based on the young universe. Holz, for one, is exhilarated. “I’ve dedicated the last decade of my life in the hopes of making one plot: a standard-siren measurement of the Hubble. I got to make my Hubble plot, and it is beautiful.”
This article appears courtesy of Quanta Magazine.


There’s a decent chance, if I may blow your mind for a moment, that your favorite pumpkin-pie recipe does not contain any actual pumpkin—at least, not the way you think. Scoop that autumnal goop out of a can, even one labeled “100 percent pure pumpkin,” and you just may be living a delicious lie.
The canned pumpkin you buy in the grocery store often contains little to no amount of the bright-orange, jack-o’-lantern kind of pumpkin. So what, exactly, is in there? And does it still count as a pumpkin?
Debating canned-pumpkin labels is becoming something of a perennial exercise. Snopes investigated the debacle last year, building its case on a Food and Wine article that got the whole internet’s harvest knickers in a collective twist. But this trivia tidbit has been bouncing around the web since at least 2012, when Heirloom Gardener revealed in a shocking exposé that the Libby’s brand of canned “pumpkin” contains a squash variety called the Dickinson pumpkin. In fact, Libby’s even acknowledges that they use the “Dickinson variety of pumpkin” on their website.
If you Google the Dickinson pumpkin, it will quickly become clear that no one would ever make it into a jack-o’-lantern. It looks like a pale, slightly misshapen butternut squash. Heirloom Gardener describes the Dickinson pumpkin as “uniform, smooth, tan”—no ridges, no familiar orange hue.
But wait! you cry, desperate to know if, like the Food and Wine author, your “whole life is basically a lie.” “Dickinson pumpkin” has pumpkin in the name. So how is it not a real pumpkin?
It depends who you ask.
In this case, the most relevant definition might come from the FDA, which authorizes food sellers to label as pumpkin any article “prepared from golden-fleshed, sweet squash or mixtures of such squash and field pumpkin.”
“Field pumpkin,” as you might have guessed, is the canonical, jack-o’-lantern type, the embodiment of fall and Halloween and resident of hay-strewn patches across America. It’s actually a subspecies of Cucurbita pepo, along with acorn squash, zucchini, and decorative gourds.
What’s not in the same species as field pumpkin? You guessed it: the dastardly Dickinson.
Okay, but maybe the species the Dickinson does belong to—Cucurbita moschata, to be precise—could also be included under the umbrella term of “pumpkin,” right? This is where the line between pumpkins and squash starts to break down.
Heirloom Gardener claims “there is only one species of ‘true’ pumpkin, the Cucurbita pepo.” Encyclopedia Britannica disagrees, saying that pumpkins are the “fruit of certain varieties of squash (namely, Cucurbita pepo and Cucurbita moschata)”—the field-pumpkin species and the Dickinson species. And if you think the Brits just don’t know what they’re talking about, know that Merriam-Webster is on their side.
The Missouri Botanical Garden takes an entirely different tack, saying that “pumpkin” as a category “really has no botanical meaning, as they are actually all squash.” Where, then, do we draw the line? The first jack-o’-lanterns were carved out of potatoes; does that mean we have to start calling potatoes pumpkins too?
Cindy Ott, who wrote Pumpkin: The Curious History of an American Icon, told me that, up until the 19th century, the words “pumpkin” and “squash” were used interchangeably. “It’s the same damn vegetable!” she said. The difference is all about symbolism. (Or, in some cases, about color: Pumpkin-growing competitions across the country will only consider specimens that are at least 80 percent orange.)
The whole reason canned pumpkin isn’t field pumpkin is that field pumpkin isn’t good to eat. Colonial Americans only ate them when they couldn’t afford anything else. It’s icky. So when the Industrial Revolution came along, according to Ott, “the pumpkin stayed behind on the farm.” Unlike squash, which were sold in markets and remained “part of people’s everyday life, just like how we see squash now,” no one tried to sell pumpkins in the city. They were just used as a cheap alternative to animal feed on small family farms. The pumpkin thus became a symbol of agrarian values, a bright-orange beacon to Americana; what we call a pumpkin comes down to what we decide fulfills that ideal.
As for the stuff that comes out of a can, a spokesperson from Libby’s confirmed to The Atlantic that “Libby’s canned pumpkin contains 100 percent Dickinson pumpkins”—you know, the kind that isn’t even the same species as the big round orange ones. You can even watch a video of Libby’s harvesting their pale, oblong fruit. (Yes, pumpkins are fruit; don’t get me started.)
And to settle any worries that your cans are full of butternut squash, Snopes confirmed that most canned pumpkin is not majority-butternut, despite what much of the internet suggested in its autumnal freakout. So we don’t have to start calling that a pumpkin.
But beyond that, we’ve ended up right back where we started. We may lack any botanical, scientific, or regulatory unanimity, but the popular consensus remains that the field pumpkin is the One True Pumpkin. I’m not about to disabuse myself of this notion just because a bunch of botanists can’t make up their minds.
Adrienne LaFrance contributed reporting to this article.


What new responsibilities should a rich and liberal democracy take on in the 21st century? If you ask almost any American progressive, they will talk about the need to regulate greenhouse-gas emissions and lessen the blow of climate change. Lately, a growing number of conservatives have glommed onto another policy. They suggest that such a democracy should also be pronatalist: It should take children as a virtue and a public good (even a blessing), and it should direct its welfare state toward encouraging human reproduction.
Given that some environmentalists seem to treat low population growth as just one more variable in the climate equation, the two policies may seem incompatible. In fact, they support each other; they may even need each other. Aggressive climate policy is pronatalist.
I’ve been thinking about this because two writers have gone back and forth lately on what they call modern liberalism’s “doom loop.” Derek Thompson identified the idea first, here at The Atlantic, describing a three-step process. Step one: Liberal and richer democracies tend to have lower birth rates and aging populations, which strains their respective welfare states. Step two: Those same democracies admit more immigrants, in order to reinforce their welfare state and beef up their working-age population. But—step three—as an increasing amount of a state’s population becomes foreign-born, its aging population becomes more xenophobic and less supportive of the same welfare state.
And that “leads to stratification, further discontent and an authoritarian turn, which presumably slows growth further, etc., etc., until liberalism goes kaput,” writes Ross Douthat, picking up the idea at The New York Times. Part of what makes the loop so insidious is that—much like a plane’s real-life death spiral—a polity’s knee-jerk instinct to escape the loop only reinforces it.
Thompson names the loop but doesn’t quite describe a way out of it, though he suggests that guiding people out of bigotry is a possible exit in the long term. “The liberal cause,” he says, “requires Americans learning to break the catch-22 of diversity and equality.”
Douthat, seizing on that sentence, says that maybe nations should consider trying to tinker with another variable:
It could also be that there’s a policy mix that would make that combination a little less of a catch-22, and require less in the way of eliminating the inherent human bias in favor of one’s own posterity in order to succeed.
Maybe, for instance, if you cut immigration rates modestly while also spending heavily to encourage old-fashioned procreation, you would have a better chance of getting faster growth while also creating a socioeconomic environment—youthful, future-oriented, optimistic—that makes assimilation easier.
“Youthful, future-oriented, optimistic”—who wouldn’t want a country like that? It even sounds like the United States, once upon a time.
How do we get back there? By addressing the full scope of the problem. Americans have to talk about climate change—because climate change will simply be happening in the background of all of this. If liberalism’s doom loop is one condition of the 21st century, then the accumulating degradation of Earth’s only climate is another. Its political economy is as specific to this century as the unusually high number of old people kicking around is. And it specifically informs the issues here in two ways.
First, the warming of the world will set off more waves of mass migration. No matter how the doom loop churns, global warming will very likely increase the foreign-born population of the northern and richer democracies all by itself. The immigration reporter Dara Lind has argued that there’s a tendency to take diversity’s threat to nationalism for granted, even though racism corrodes democratic solidarity at least as much as a foreign-born population does:
I would love to read a defense of nationalism that takes the threat racism poses to it at least as seriously as the threat diversity does.
The point holds for climate migration as well. As I argued last year, reducing racism and neutralizing the threat it poses to democratic solidarity constitutes climate adaptation as much as raising a seawall or protecting a coastal wetland does.
But here Douthat would insert that racism isn’t the only variable at play in the doom loop; there’s also “the inherent human bias in favor of one’s own posterity.” He proposes pronatalist policy—that is, incentives for procreation and family formation—as a solution.
I think it’s good to have kids; I think taking care of kids gives rise to responsibility, generosity, and selflessness among adults and communities; and I agree that the longer horizons of child-rearing promote a “youthful, future-oriented, optimistic” politics. I also suspect that pronatalist policy will soon make for good U.S. electoral politics: By the time of the next presidential election, the biggest single-age cohort in America—that is, my own, the 1991-er—will be narrowing in on prime parental age. Perhaps our progeny-concerned parents could even be converted into a new and coveted bloc.
But if you want to make American politics more future-oriented, you first need to fill the climate-shaped hole in our future-oriented-ness. As a polity, we have little collective vision for the future, and little public determination to make sure it’s a better place to live.
When you talk to people about why they don’t want kids, they don’t always talk about their aversion to children or child-rearing; they often talk about how bleak they think the future will be. Between the breakdown of the global liberal order and the ongoing degradation of the planet’s climate, the next few decades don’t seem like a particularly sociable place, for them or their hypothetical children. To many potential parents, it feels like older folks are pressing down the accelerator, trying to burn all the gas in the engine before the car goes careening straight off the side of the canyon.
But wait: Haven’t people birthed kids into hopeless environments since, well, forever? How is climate-change doom now any different from doom over nuclear weapons at the height of the Cold War? After all, in 1962, the same year as the Cuban Missile Crisis, the U.S. fertility rate was nearly double its current number. (In fact, there’s some evidence that the crisis itself slightly increased births in states near Cuba or with large military installations.) And the most radical of anti-procreative climate-themed projects, like the Conceivable Future Project, record only dozens of examples of people swearing off child-rearing.
Well, sure. But much of the conservative argument for pronatalist politics respects the fact that cultural changes—and important medical and political advances—have altered childbearing decisions. Isn’t the general anxiety about climate change as a cultural phenomenon—and the lack of political amelioration of it—one of those changes?  Potential parents undertake a complex and often spiritual calculus when they plan their families, when they decide to have zero, or one, or five kids. It seems reasonable to me that if you want to coax people back into having larger families, or families at all, you may have to soften that calculus by assuring them the future will be a good place to live.
Because right now, the future does not seem like a very pleasant place to live at all. Economists who study climate change say that, at best, the phenomenon will exact persistent and troubling costs on the poorest parts of the United States; at worst, it will initiate one of several globally destabilizing crises. Global warming will also degrade Earth in plenty of hard-to-calculate ways, wrecking the gentle rhythm of the seasons and strangling the natural biodiversity of every stream and mountain. This last seems like an impoverishment to which pronatalists should be especially sensitive.
This line of argument is somewhat awkward, because Douthat himself (and many of his fellow pronatalists) does not believe in the bleakness of a climate-changed future. He has written that climate change is not “a crisis unique among the varied challenges we face,” and he has doubted the apocalyptic future that some climate advocates describe. Many conservatives seem to reject climate change in part because of its anti-procreative supporters.
And perhaps many conservatives will never buy into a future that bleak. But could they take seriously that many of their younger peers do? A recent poll taken by, alas, an energy-reform interest group found that a majority of young Republicans, ages 18 to 30, support a carbon tax. Young Americans overall, ages 18 to 34, are more concerned about climate change than other age cohorts.
I go into these details not to make a generational plea but to assert a public reality. For voters, climate change is unmistakably a low-intensity issue: Even though millions of Americans say they care about it, that concern doesn’t seem to change which candidate they support. But global warming can have political and cultural consequences that are not electoral. As our climate problem goes unfixed, as it seems to be intentionally aggravated by federal policy, it corrodes confidence in the future.
Planning for the future has many benefits that do not help the future alone. If you want a society that encourages people to have kids, you must first tell them that you are working to make the future modestly more hospitable. Not a perfect place, not a problem-free place, not a place where everything will be okay. Just a modestly better one. Pronatalist policy and aggressive climate action could reinforce a vision of a healthy, thriving world, full of human beings.
Optimism can break the doom loop—and even if it fails in doing so, at least our grandchildren under the Third People’s Junta will pay less for their flood insurance.


It’s been two years since an epidemic of Zika began in Brazil, three since the largest Ebola outbreak in history erupted in West Africa, eight since a pandemic of H1N1 flu swept the world, and almost a hundred since a different H1N1 flu pandemic killed 50 million people worldwide. Those viruses were all known, but no one knew when or where they’d trigger epidemics. Other diseases, like SARS, MERS, and HIV, emerged out of the blue.
Sick of being perpetually caught off guard, some scientists want to fully catalogue all viral threats, and predict which are likely to cause tomorrow’s outbreaks. The PREDICT project has been doing that for 8 years; with $100 million in funding from the U.S. Agency for International Development, they’ve discovered nearly 1,000 new viruses. The Global Virome Project is even more ambitious. Proposed in 2016, and still existing in concept only, it aims to find and sequence almost all the viruses in birds and mammals that could potentially spill over into humans.
The GVP estimates that around half a million such viruses exist, and finding them would cost $3.4 billion. With that hefty price tag would come security. In lofty language, the project promises to switch the world “from responding to outbreaks to proactively preparing for them” and to “mark the beginning of the end of the Pandemic Era.”
There’s just one problem, say Jemma Geoghegan and Edward Holmes, two virologists based on Sydney. It won’t work.
In a new paper, Geoghegan and Holmes argue that these projects aren’t going to help preempt pandemics, for the simple reason that there are just too many viruses. About 4,400 have been identified; millions more have not, and only a tiny fraction of these could conceivably jump into humans. “The GVP will be great for understanding more about viruses and their evolution, but I don’t see how it’ll help us work out what’s going to infect us,” says Geoghegan. “We’re only just coming to terms with the vastness of the virosphere.”
There are ways of narrowing down the culprit list. Many teams have tried to map geographical hotspots from which diseases are most likely to emerge, pinpointing areas with tropical forests and lots of mammal species. Others have tried to find features in viruses that make it easier for them to spread between people. But having tried this approach themselves, Geoghegan and Holmes argue it’s not very useful.
Partly, that’s because the results of such studies are too broad to narrow down the list of suspicious viruses in a helpful way. Partly, it’s because such work is based on past epidemics—events that are relatively rare, and so difficult to draw reliable patterns from. For example, Saudi Arabia comes out as mostly cold in maps of disease hotspots, and yet it’s where MERS virus recently jumped into humans from an unlikely host: camels. “We’re trying to predict really, really rare events from not much information, which I think is going to fail,” Geoghegan says.
Ultimately, the odds that a given virus will cause an outbreak depend on the virus itself, the animals that host it, the people who stand to contract it, and the environment that all of them live in. “Within each of these categories, there are so many variables that could influence disease emergence,” says Jennifer Gardy, from the University of British Columbia. “It’s hard enough to model the effect of any one, and these factors likely interact in ways that we can’t possibly understand just by looking at each of them discretely.”
It’s even difficult to work out whether the viruses we already know about are going to cause outbreaks. Ebola and Zika, for example, were discovered in 1976 and 1947 respectively, but both managed to catch the world unawares this decade. “This is the easiest kind of prediction to make,” says Kristian Andersen, from the Scripps Research Institute, and we’re still about 10 to 20 years from doing it well. Next up in difficulty: predicting whether a virus like H7N9 bird flu, which can infect humans but isn’t known to cause major outbreaks, will eventually do so. Again, Andersen says that this isn’t feasible now, but should be with more research.
But predicting whether a newly discovered animal virus could jump into humans and cause a pandemic “is simply impossible,” he says. “What you’re trying to predict is likely something that happens maybe once out of tens of billions of encounters, with one virus out of millions of potential viruses. You will lose your fight against the numbers.” Even machine learning—using computers to divine patterns in data that humans might miss—won’t solve the problem because there isn’t enough good data for the computers to sift through.
Proponents of predictive initiatives say it’s too early to discount such approaches. If the same complaints had been raised in meteorology a century ago, “we wouldn’t have created the data that lets us forecast the weather, which we can do pretty well now,” says Jonna Mazet, the global director for PREDICT who also sits on the Global Virome Project steering committee.
“Can we predict pandemics? The answer right now is no. But just because something is hard to predict does not mean we cannot quantify its risk in a useful, actionable way—a logic that the insurance industry profits from,” adds Barbara Han, from the Cary Institute of Ecosystem Studies. No predictions are perfect, but at the very least, we can put boundaries on what is likely.
Resources aren’t infinite, though, and public health is an area that’s historically underfunded. Geoghagen argues that it would be best to channel efforts into approaches that would do the most good. For her, that involves looking at the “fault lines” where humans and animals meet—regions where people are more likely to be exposed to animal viruses because they are chopping down forests, or setting up dense animal markets, or hunting wild creatures for meat, or moving around a lot because of political instability.
Mazet agrees, and says that the Global Virome Project plans to look for viruses precisely at such fault lines. They want to, for example, search blood and meat samples of bushmeat, or the urine or saliva of rodents that share human homes. “It’s not aimed at detecting every virus out there,” she says, but she admits that the team hasn’t done the best job in explaining that to their fellow virologists.
But Geoghagen and Holmes argue that searching for these viruses in animals is still “a Sisyphean exercise.” You’d find too many, with no way of accurately assessing their risk of jumping into us. The project, they say, would be better off focusing on people—the workers in the bushmeat trade rather than the meat itself, for example. “Humans are the best sentinels: A virus discovered in humans very obviously can replicate in that host, which will not be the case for myriad viruses identified through biodiversity surveys of other [animals],” they say.
Andersen agrees. For the moment, preempting pandemics isn’t possible; what matters is catching them as early as possible. “Forget about detecting the virus before it jumps. Forget even about detecting the first patient,” he says. “Detect the first cluster of cases.” That’s possible if health workers routinely search for viruses in people who live at disease fault lines, and the advent of portable, pocket-size DNA sequencers could make such searches a reality.
These goals shouldn’t be seen in opposition, though. Kevin Olival from EcoHealth Alliance, who works with PREDICT, says that it would be impractical to study all fault-lines. “We need tools to help us narrow down and target our resources to the locations, host species, and viruses of greatest concern,” he says. Projects like PREDICT and the Global Virome Project may not act as crystal balls for future outbreaks, but they “help us prioritize on-the-ground disease surveillance.”
And PREDICT, through its work on detecting new animal viruses, has also helped to develop analytics tools and strengthen labs in developing countries, which will make it possible to do the kind of surveillance that Geoghagen, Andersen, and others are calling for. Everyone agrees that’s vital. “If we can’t even get routine surveillance working in hot-spot settings, we have no chance of getting something even more complex, like prediction, in place,” says Gardy.


Imagine you’re Edwin Hubble in 1923, about to prove that the Milky Way is just one galaxy in a universe filled with them. You have just spotted a faraway variable star. You write down a note about that star on a photographic plate: “VAR!”
Or imagine it’s 1977, and you’re reading a printout from a radio telescope that listens for aliens, red pen in hand, when you find a long, strange, still-unexplained signal. “Wow!” you write.
Or imagine it’s August 2017, you’re signed on to Slack, and you’ve just seen the smoldering wreckage of a collision of two neutron stars. “!” you type to your colleagues, unable to muster anything else.
Each of these astronomical classics highlights one particular aspect of discovery: the thrill of knowing something about nature that no one else does. But these moments from the highlight reel of astronomy’s history minimize the more prosaic aspects of research, the tedium of peering at a screen for hours on end, blinking, clicking, or executing a computer script, again and again, forever, and maybe not finding anything noteworthy at all.
But now AI is here to do the boring part.
In a new paper published by the journal Monthly Notices of the Royal Astronomical Society, a neural network has successfully flipped through images of more than 20,000 galaxies and pulled out a few hundred of the most intriguing. “I think it will become the norm since future astronomical surveys will produce an enormous quantity of data,” said Carlo Petrillo of the University of Groningen in the Netherlands, in a statement. “We don’t have enough astronomers to cope with this.”
Petrillo’s specific quarry was gravitational lenses: rare patterns in the night sky that let astronomers fathom the depths of many of modern physics’ most pressing mysteries. Picture a galaxy so massive that space is distorted by its gravity, as Einstein’s theory of general relativity predicts. As rays of light from galaxies far beyond it pass through that warped space around the foreground galaxy, they curve, bending toward the Earth. This makes the foreground galaxy like a lens made of space, not glass.
Astronomers often call gravitational lenses cosmic telescopes, whose lenses amplify the light from very distant galaxies, opening a window to older, more primitive parts of the universe than we could otherwise see. That is, if they can find them. Since galaxies needs to fall directly behind each other by random chance, lenses are rare, and graduate students have only finite time to comb through endless images hunting them down. They all look a certain way, like broken arcs of light ringing a galaxy or a galaxy cluster. But they’re all different, and arms of spiral galaxies look like arcs too, confounding the search. It’s the kind of thing you’d think you need human intuition to do, that is until an AI pulls it off.
Petrillo’s group turned to convolutional neural networks, which are often used for other image-analysis tasks like facial recognition. They showed their network 100,000 images of gravitational lenses. Since fewer than a thousand real lenses are known, they used fake ones simulated in a computer. They let their network build up a feeling for how lenses look. Then they turned it loose on real galaxies, where it found 761 candidates, which the team winnowed down to 56 after checking by human eye.
According to astrophysicist Brian Nord at Fermilab, Petrillo’s work is part of a boom in applying artificial intelligence to this exact problem. From January to March, he estimates, about 8 different papers came out on the subject from different groups—including a draft of this one. While most of these efforts limited themselves to looking only at simulated pictures, as a proof of concept, Petrillo’s network and another made by graduate student Colin Jacobs seem to have identified real lenses that no human had ever seen.
And AI can do more than just find lenses. It can use them. In April, Nord says he visited Stanford and talked about how making changes at just “two little places” in a neural network’s code could let a network switch from recognizing lenses to measuring them—specifically, it could allow them to add up the mass that was creating the lens effect. By August, Stanford researchers did it. Their network analyzes lenses some 10 million times faster than older, simulation-based methods. Like any good scientist, the network had graduated from simply classifying—lens or not a lens?—to measuring.
How far this goes is an open question. In the past few years, cosmologists have started touting gravitational lenses as the solution to many of the field’s woes. Besides just magnifying distant galaxies, lenses work like cosmic bathroom scales: Their exact curvature traces the mysterious, unidentified dark matter that surrounds all galaxies. They are all also hyper-accurate yardsticks for astronomers who hope to clock the speed at which the universe is expanding. Better still, it seems like new AI methods and faster-than-ever computers can understand them.
Sending AI after lenses might only be a fad, a technique with pros and cons that is folded into other kinds of computer classification, Nord thinks. Just another part of the astronomer’s tool kit. Or it could transform discovery. It might soon be possible to give a neural network images of galaxies and simply set it free—allowing it to find the lenses, assess them, and to report back with some overarching measurement like the expansion rate of the universe. Provided astronomers could trust it, of course.
“Now you’re getting into the era that is scary for scientists, or may someday be scary,” Nord says. There would be no written notes for the history books, no exclamation points. The excitement of finding something no one else had would go unfelt, or would at least be unreadable.
It’s striking to consider what gravitational lenses and neural networks have in common. Science uses them both as tools—whether made by random chance or by design, they give us a way to pry open nature’s secrets. But here it might get recursive. Someday, not too far from now, we could be empowering a scientific instrument to find telescopes out in the void, to look through them on its own, and to tell us what it learned.


On April 22, more than a million people took to the streets, in Washington, D.C., and over 600 satellite locations around the world, to march for science. But six months later, the eponymous organization behind those gatherings—March for Science (MFS)—is still struggling with many of the same issues that have troubled it since its conception.
On Monday, Aaron Huertas, the former communications lead for MFS, posted an open letter that called out the group’s leaders for creating a culture beset by miscommunication, opacity, and disorganization. “Though the organization calls itself an open, grassroots movement, it is run like a closed, hierarchical organization,” the letter says. Seven other people told The Atlantic that their experience of working with March for Science was consistent with the open letter. “I really do think everyone has the best intentions, but not everyone has the skill sets they need to run a grassroots organization,” Huertas says.
As an example, Angela Carpio, a volunteer who helped to organize a satellite march in the Twin Cities, says that the leadership team ignored her repeated requests for help in moderating the MFS Facebook group—a community of almost 800,000 members that Carpio single-handedly oversaw for months. Eventually, she just picked three people from the community to help. “We just do our own thing. We have no direction,” she says. “There’s no transparency and no one knows what’s going on.” (Koren Temple-Perry, the newly hired communications director for MFS, says that the organization has had internal discussions about how to improve the moderation process, and that a newly hired director of social-media engagement is working with the team to bring on more moderators.)
“This is what happens when you have a group of very passionate, well-meaning people without the organizational experience who take on a tremendous amount of work, with this sort of Herculean mission of saving science,” says Jacquelyn Gill, who volunteered for the March for Science in its early stages, left the organization in April, and had signed the new open letter. “It set the stage for a culture that was big on enthusiasm and energy but weak on logistics.”
In a statement responding to the letter, Temple-Perry notes that the organization has taken several steps to address these problems, including soliciting feedback from partners and volunteers, running a retreat in May, issuing an open invitation to join an internal communications network, and staging biweekly calls with satellite organizers and partners. “Unfortunately, individuals on the letter have not yet called in to participate,” she notes. “That being said, the concerns brought up in this letter are being discussed by the board. We will continue to work toward greater transparency in all stages of our development as an organization and movement.”
But several of these measures were only initiated after the damage was done and volunteers had walked away, says Huertas. “People had stopped interacting with the national organization because they weren’t being listened to,” he says. “Volunteers don't see it as an effective use of their time.”
Rufus Cochran, who led the march’s satellite group in Indianapolis, has seen evidence of this through his attempts to unite leaders from the Midwest groups into a unified network. He says that around 8 groups have unplugged themselves from the national team because “their voices weren’t being heard” and several have become largely inactive. The central team, Cochran says, “are not 100 percent responsible for that but they’re a major factor in that loss of momentum.”
Despite the changes, confusion still abounds around the organization’s structure and finances. Huertas has pushed for MFS to publicly publish details of its expenditures and revenue, which he says is good practice for grassroots organizations—and a standard that several satellite marches have adhered to. Stephani Page, who was formerly on the national leadership team but left in April, says that even before the march, “I and others asked questions specifically about what was happening with the money. The response was always: All the money is going toward making the event itself happen. I never saw a budget.” When I asked about details of the organization's funding, I was directed to this page which breaks down revenue according to donations from organizations, individual donations, and merchandise sales, with no further information on financial structure and outputs.
The open letter also alleges that Caroline Weinberg—one of the three MFS cochairs—had been promoted to a paid director position, contrary to promises made that such positions would be open and competitive, and that the cochairs would only serve on the board or as advisors. After five rounds of circuitous emails, some of which wrongly stated that Weinberg is still simply a cochair, the MFS finally confirmed with The Atlantic that Weinberg is currently acting as executive director in an interim capacity; this was agreed to by the board of directors, and an openly posted search for a permanent ED will be conducted in the future. Weinberg is leading a team of eight other part-time staff working for MFS, including Terry Kush, the new chief operating officer. None of this information is currently available on the MFS website. Temple-Perry, the comms director, notes that the site will be updated soon.
Huertas and others claim that the March’s leaders have consistently ignored the views and contributions of people of color. As previously reported, diversity statements were repeatedly tweaked and pulled back after online pushback. Several individuals within the organization told The Atlantic that diversity concerns from people of color were met with hostility on internal forums, culminating in the resignations of roughly half of the MFS diversity committee in the week before the march. They requested anonymity because they feared harassment. Some of their colleagues who spoke out publicly said they faced online vitriol and threats.
Concerns around diversity strained relationships with partner organizations like 500 Women Scientists, a grassroots effort to foster more diversity, inclusion, and accessibility in science. “We were early supporters and partners of the MFS and it seemed, at first, that our values were aligned,” says Kelly Ramirez, a cofounder of 500 Women Scientists. So in February, when MFS drew heavy criticism for repeatedly posting messages that some thought were tone-deaf on Twitter, Ramirez and her team offered to help. “We had someone who would volunteer their time, but they weren’t added to any meetings or discussions. They used us as an example of how they were highlighting diversity, but we weren’t given a voice at the table.” By the time the volunteer was finally invited into MFS discussions, the group had had enough; they officially severed their ties with MFS in April. (“We are sorry to lose that connection, though we were excited to see that they continued to participate in the actual march as an organization,” said Temple-Perry in a statement. “We deeply respect their work and hope that there are additional opportunities to collaborate in the future.”)
On the day of the march itself, many speakers at the D.C. event emphasized the importance of all kinds of diversity, and several of the new recruits to the national team are people of color. But the signatories on the open letter feel that these changes are superficial. “From the outside, it looks like we’ve hired a racially diverse group of people with different social backgrounds,” says Amber Ying, who helps to moderate the Facebook group and volunteered for a largely defunct Boston chapter. “It feels like they’re only paying lip service to this idea of inclusivity. The actual practice of making sure people are heard is a much harder process and they’re not very good at it.”
Lucky Tran, who serves on the board, counters that “many on our board have served in and built grassroots organizations and social-justice movements, and we’re committed to creating a world in which science serves, and is inclusive of, everyone.” He says that MFS has worked with local leaders to organize aid for communities affected by Hurricanes Harvey and Maria, and has spoken up about social-justice issues like the white-supremacist rally in Charlottesville and Nature’s since-retracted editorial on the removal of a statue of J. Marion Sims. (Carpio notes that the Charlottesville post only happened after she pushed MFS leadership to act—and after raising the matter on an internal forum, she was billed as “hysterical and divisive” by a satellite organizer.)
“The concerns voiced in the letter and elsewhere are highly valued, and dialogue like this is needed to make movements better and more accountable,” he adds. “We are all committed to working hard to address them and building a better and more just world.”
That’s what the signatories of the open letter want, too. They all feel that MFS has tremendous promise, and a large and motivated base. But they also felt that these opportunities were being wasted. “There are lofty goals and ideas, but not enough organization to distill that into action on the ground,” says Ying.


One of my first jobs was to keep a lookout for lions. There are some occupations that are not suitable for someone with untreated narcolepsy and this is probably one of them. I was 22, a recent zoology graduate studying meerkats in the Kalahari desert in South Africa. We worked in pairs, one of us on foot, walking with meerkats, the other in the jeep scanning the horizon for signs of leonine danger. On many occasions, I awoke with the imprint of the steering wheel on my forehead, realizing that meerkats and colleague had wandered out of sight. I would look for signs of life and, as the panic grew, signs of death. I can tell this story now only because nobody got eaten.
I have not always been like this. For the first 20 years of my life, I had a healthy relationship with sleep. Shortly after my 21st birthday, though, I began to experience symptoms of narcolepsy, a rare but not-so-rare disorder thought to affect around one in 2,500 people. If people know one thing about narcolepsy, it’s that it involves frequent bouts of uncontrollable sleepiness. This is true, but the condition is so much more disabling, often accompanied by cataplexy (where a strong emotion causes loss of muscle tone and a rag doll–like collapse), trippy dreams, sleep paralysis, frightening hallucinations, and, paradoxically, fractured nighttime sleep. There is no cure. Yet.
In the Kalahari, back in 1995, I was new to these symptoms. I had little sense of the incalculable toll that fighting a never-ending battle against sleep (with defeat the inevitable outcome) would take on mind, body, and soul. I was not alone. Few family doctors had heard of the disorder, let alone encountered a patient. Some neurologists knew what to look for, but many did not. Not even sleep specialists could explain why this disorder would suddenly strike, with peak onset at around 15 years of age.
A lot has changed in 20 years. There is now overwhelming evidence that by far the most common cause of narcolepsy is an autoimmune attack, where the body’s immune system mishandles an upper respiratory infection and mistakenly wipes out the estimated 30,000 neurons in the center of the brain.
In an organ of up to 100 billion cells, this might not sound like too much to worry about. But these are no ordinary cells. They are found in the hypothalamus, a small, evolutionarily ancient, and unbelievably important structure that helps regulate many of the body’s basic operations, including the daily seesaw between wakefulness and sleep. The cells in question are also the only ones in the brain that express the orexins (also known as hypocretins). This pair of related peptides—short chains of amino acids—were completely unknown at the time of my diagnosis in 1995.
The story of their discovery, beginning in the 1970s, is a brilliant tale of chance and luck, imagination and foresight, risk and rivalry, and involves a colony of narcoleptic Doberman pinschers to boot. It might even be the perfect illustration of how science works.
Yet while there are drugs that can help manage the worst of the symptoms of narcolepsy, none of these comes close to repairing the underlying brain damage. It is remarkable that a lack of two chemicals results in such a bewildering constellation of symptoms. The answer to my problems appears to be simple—I just need to get the orexins (or something similar) back inside my brain. So why am I still waiting?
* * *
In April 1972, a toy poodle in Canada produced a litter of four. Eager families were quick to snap up the cute puppies, but one of them, a silver-gray female called Monique, soon developed what her owners described as “drop attacks” when she tried to play. These did not look like sleep; they were mostly partial paralyses: Her hind legs would go weak, her bottom would slump to the floor, and her eyes would become still and glass-like. At other times, particularly when fed, Monique would be struck by a full-blown attack.
When vets at the University of Saskatchewan observed Monique, they suspected these were bouts of cataplexy, and hence figured this might be a case of narcolepsy with accompanying cataplexy. As luck would have it, Monique’s diagnosis coincided with the arrival of a peculiar circular from William Dement, a sleep specialist at Stanford University. He was on the lookout for narcoleptic dogs. The Saskatchewan vets wrote back to him immediately. With Monique’s owners persuaded to relinquish their pet, all that was needed was to figure out a way to get her to California.
I met Dement, now 89, to find out what he remembers about those early years. He retired several years ago, but still lives in a leafy neighborhood on the edge of the Stanford campus. His office is a large, shedlike structure attached to the main house and not unlike a Scout hut.
The walls are wood-clad and covered with framed posters, photographs, and miscellaneous memorabilia from an illustrious career in sleep medicine. Dement’s desk is a picture of organized chaos. Among all this is a water pistol. I ask him why. “It’s for when students fall asleep in class,” he explains, referring to an incredibly popular lecture series on sleep and dreams he instigated in the early 1970s.
In 1973, Dement approached Western Airlines to see if they could fly Monique down from Saskatchewan to San Francisco. They had a strict “no sick dogs” policy. “It’s not a sick dog. It’s a dog with a brain abnormality,” he told them. “It’s an animal model of an important illness.” Eventually, with some political lobbying, Dement succeeded in persuading the airline to help. Once in San Francisco, Monique quickly became something of a celebrity.
“Monique is very likely to collapse when she’s eating something she especially likes, or when she smells a new flower outside, or romps around,” Dement’s colleague Merrill Mitler told the Associated Press for a story that ran in dozens of newspapers across the United States. “We hope to discover exactly where in the brain the dysfunction occurs that causes narcolepsy,” Mitler had told the newspapers soon after Monique’s arrival at Stanford. “This could be the first step toward developing a cure.”
Mitler is now a forensic examiner based in Washington, D.C., specializing in litigation arising from fatigue-related accidents. I ask him if the story of the discovery of narcolepsy is really as good as it appears. “In a word, yes,” he says. “In the ’70s, we didn’t know what we didn’t know about narcolepsy.” There is simply no way anyone could have anticipated how profitable the research into Monique and other dogs would turn out to be. The plan at that stage, he admits, was simply to use the animals to test new drugs that might improve treatment of the symptoms and to carry out autopsies in case there were some obvious physical changes to the brain.
Word began to spread, and soon Dement and Mitler were looking after Monique alongside several other narcoleptic dogs, including a Chihuahua-terrier cross, a wirehaired griffon, a malamute, Labrador retrievers, and Doberman pinschers. The fact that narcolepsy appeared to be more common in some breeds than others suggested there could be some kind of genetic basis to the disorder. Then came the breakthrough: a litter of around seven Doberman puppies, all of them with narcolepsy and cataplexy. “Within 24 hours or less we saw the first of the litter and then the last of the litter all collapse,” says Mitler. “There was a large group of us at Stanford and we collectively had our chins on the floor.”
It turned out that in Labradors and Dobermans, the disorder was inherited. Dement made the decision to focus on Dobermans and, by the end of the 1970s, he was the proud custodian of a large colony and had established that narcolepsy in this breed was caused by the transmission of a single recessive gene. By the 1980s, methods of genetic analysis had advanced just enough to contemplate an effort to hunt down the defective Doberman gene.
* * *
I can never reconstruct the combination of factors that led to the onset of my own narcolepsy, but the stage was set at the moment of my conception in 1972, at around the time of Monique’s birth in Saskatchewan. My one-cell self inherited a particular version of a gene (known as HLA-DQB1*0602) that forms part of a set that helps the immune system distinguish friend from foe. HLA-DQB1*0602 is pretty common—around one in four people in Europe boasts a copy—but it plays a key role in many cases of narcolepsy, and is present in 98 percent of those with narcolepsy and cataplexy.
On top of this genetic background, there may have been some bad timing too. People with narcolepsy are slightly but significantly more likely to be born in March (as, indeed, I was). This so-called “birth effect” is seen in other autoimmune disorders and is probably explained by a seasonally variable infection at a particular moment in development. In the case of narcolepsy, it seems that those of us born in March are just a little bit more vulnerable than others.
While other infections during my childhood, hormonal fluctuations, and emotional stress may also have played a part, it was in late 1993 that I probably encountered a key pathogen—an influenza virus or Streptococcus perhaps. It was this that took me to an autoimmune tipping point and resulted in the rapid dismantling of my orexin system. In short, most cases of narcolepsy are probably the result of an unfortunate combination of events that create the perfect immunological storm.
Around this time, the Doberman project in Stanford was on the verge of unraveling the genetic basis of narcolepsy in this breed. The man tasked with hunting down the mutation responsible was Emmanuel Mignot, who subsequently succeeded Dement as director of the Stanford Center for Sleep Sciences and Medicine. We meet in his office there, joined by Watson, a narcoleptic Chihuahua he adopted a few years ago. “It’s such a silly breed,” he says, holding down Watson’s ears to prevent them from burning, then setting him on the floor. “Not one I would ever have chosen myself.”
At first, Watson is wary of me, keeping his distance and growling. When I get down to his eye level, he yaps and jumps in at me, then out, pretending he is fiercer than he is. I can empathize, even across the gulf that separates his species from mine. I know about the excessive daytime sleepiness. I know about the cataplexy, how it feels to have emotions short a neurological circuit in the brainstem and cause a muscular collapse (just as occurs in the rapid eye movement, or REM, stage of sleep, when most dreaming takes place). I wonder if Watson suffers the total terror of sleep paralysis and the supernatural hallucinations that often accompany it.
As he looks back at me, his eyelids close and open with a dullness I recognize. He turns, daintily steps into his basket, and curls up for the rest of the interview.
Back in the 1980s, the idea of locating the gene for canine narcolepsy was off-the-scale ambitious. Breeding narcoleptic Dobermans is harder than it sounds, as the afflicted tend to topple over mid-coitus, temporarily paralyzed by a cataplectic thrill (a so-called orgasmolepsy that can occur in humans too). This impracticality aside, there was also the task of locating a gene whose sequence was not known, in a genome that was, at the time, a no-man’s-land. “Most people said I was crazy,” says Mignot. In a sense, they were right, because it took him more than a decade, hundreds of dogs, and over $1 million. And he was nearly beaten to it.
In January 1998, after more than a decade of painstaking mapping, and just as Mignot’s team was closing in on the gene, a young neuroscientist called Luis de Lecea at the Scripps Research Institute and colleagues published a paper describing two novel brain peptides. They gave them the name “hypocretins”—an elision of hypothalamus (where they were found) and secretin (a gut hormone with a similar structure). They appeared to be chemical messengers acting exclusively inside the brain.
Just weeks later, a team led by Masashi Yanagisawa at the University of Texas independently described the exact same peptides, but called them “orexins” and added the structure of their receptors into the bargain. They speculated that the interaction of these proteins with their receptors might have something to do with regulating feeding behavior. “We didn’t even think about sleep at all,” admits Yanagisawa, now director of the International Institute for Integrative Sleep Medicine at the University of Tsukuba in Japan.
Back at Stanford, Mignot heard about the two papers, but there was no reason to imagine this new pathway had anything to do with narcolepsy or sleep. By the spring of 1999, however, he and his team had worked out that the recessive mutation had to lie in one of two genes. One was expressed in the foreskin. “It didn’t look like a candidate for narcolepsy,” says Mignot. The smart money was on the other gene, which encoded one of the two orexin receptors. When he got wind that Yanagisawa had engineered a mouse lacking orexins that slept in a manner characteristic of narcolepsy, the race was on.
Within weeks, Mignot and his team had submitted a paper to the journal Cell, revealing a defect in the gene encoding one of the orexin receptors. “This result identifies hypocretins [orexins] as major sleep-modulating neurotransmitters and opens novel potential therapeutic approaches for narcoleptic patients,” they wrote. Kahlua—one of a litter of Dobermans all named after alcoholic beverages—lay sprawled across the cover of the issue. Yanagisawa and colleagues added their experimental evidence to the mix just two weeks later, also in Cell.
* * *
Under normal circumstances, a chemical messenger and its receptor work a lot like a key and lock. A key (the messenger) fits into a lock (its receptor) to open a door (cause a change within the target cell). In the case of Mignot’s Dobermans, a massive mutation had effectively jammed the lock of the orexin receptor, rendering the orexin useless.
Whether it’s the lock that doesn’t work, as in this case, or that the keys are missing, as they were in Yanagisawa’s mice, the upshot is the same. The door won’t open. The orexin system is broken. In human narcolepsy, there are many ways to break the orexin system. Occasionally, a brain tumor or head trauma is sufficient to do the damage. In most cases, however, narcolepsy is caused by the series of unfortunate events outlined above.
The orexin neurons are a very big deal, and not just for those like me who’ve lost them. Present in every major class of vertebrate, they have to be doing something seriously important. When de Lecea first described the orexins in 1998, he was in his mid-20s and had only recently moved from Barcelona to San Diego. In 2006, he made the move from there to Stanford to be closer to the sleep action. “To be honest, I thought we’d understand the system much better at this point than we actually do,” he says.
But we have found out a lot, particularly thanks to optogenetics, a technique de Lecea helped pioneer. By deploying a virus, a promoter, and a gene found in blue-green algae, it is possible to render a particular population of neurons sensitive to light.
To illustrate this wizardry, de Lecea brings up a video on his laptop. There is a mouse in a cage that has been engineered so its orexin neurons will fire in response to light. There is a thin fiber-optic cable running into its brain. “The mouse is asleep,” he says, waves of electrical activity characteristic of deep sleep spooling across an inset video at the top of the screen. The optic cable comes alive, a pulse of bluish light flashing for precisely ten seconds. The light-sensitive orexin neurons release their neuropeptides and, all of a sudden, the mouse wakes up. When the light goes off, it falls asleep as rapidly as it awoke.
There can be few more striking illustrations of the power of the orexins than this. Completely unexpectedly, I feel my tear ducts tingling and for a split second I almost envy the mouse.
Using optogenetics and other methods, de Lecea has been able to show that the orexins have a powerful effect on many important neurological networks. In some settings, they act like neurotransmitters, crossing gaps in neurons to activate target neurons that release a chemical called norepinephrine throughout the brain’s cortex.
In other settings, the orexins act more like hormones, working further afield in the brain. This is how orexins influence other brain chemicals, including dopamine (essential for the processing of reward, in planning, and for motivation), serotonin (strongly associated with mood and implicated in depression), and histamine (an important alerting signal).
“In most other neural networks, there are parallel and multiple layers of security,” says de Lecea, so if something isn’t working properly, there are systems that can step in and pick up the slack. In the case of the orexins, however, there appears to be little or no backup at all. So, manipulating this system produces the kind of clear-cut response that scientists can work with. “It is a brilliant model for understanding neural networks more generally,” says de Lecea.
What we now know about orexins also helps explain why losing just a few tens of thousands of cells should result in a disabling, multi-symptomatic disorder like narcolepsy—something that messes with wakefulness and sleep, body temperature, metabolism, feeding, motivation, and mood. These proteins are giving us a privileged insight into how the human brain does what it does.
All this makes the orexin story sound like the archetypal double helix–like tale of scientific discovery, the perfect illustration of how science works. There’s an underlying puzzle (narcolepsy), an origin story (Monique), foresight (Dement), ambition (Mignot), technological developments (genetics), a photogenic animal (Dobermans), a race (with Yanagisawa), it looks like science (optogenetics), and there’s a still higher purpose (sleep and the brain).
It is elements like these that can transform everyday scientific events into a compelling cultural narrative, says Stephen Casper, a historian of neurology at Clarkson University in New York. “It has all the ingredients of something that I think physiologists and neurologists in the early part of the 20th century were looking for and hoping they would find, something that would bring together heredity, biochemistry, biophysics, neurology, and psychology.”
But there is a pattern in biomedical research of niche disorders opening up promising avenues of research that never end up helping the patients themselves, Casper adds. The narrative around narcolepsy has something missing, he says: “A good story should have a clear happy ending.”
* * *
We are still waiting for that happy ending. Even if I could get my hands on a vial of orexin-A or orexin-B, how would it get into my brain? Swallowed in solution, the enzymes in my gut would make short shrift of it, plucking off the amino acids like beads off a necklace. Injected into muscle or the bloodstream, not enough would make it through the blood-brain barrier. There have been some experiments on a nasal delivery, suggesting that sniffing orexins may be a way to smuggle some of them into the hypothalamus via the olfactory nerve, but there has been relatively little investment in this approach.
This does not mean that the pharmaceutical industry has ignored the discovery of the orexin pathway. Far from it. Within just 15 years of the Cell publication by Mignot and colleagues that linked a loss of orexin to narcolepsy, Merck had received U.S. Food and Drug Administration approval for suvorexant (or Belsomra as it’s known in the trade), a small molecule capable of getting through the blood-brain barrier and blocking orexin receptors.
A drug that promoted sleepiness was not the application that most people with narcolepsy were looking for. By preventing the orexins from binding to their receptors, Belsomra effectively creates an acute case of narcolepsy, but where the fog, ideally, will have started to lift by the morning.
Sleeping pills commonly used to treat insomnia tend to work by depressing the central nervous system as a whole, says Paul Coleman, a medicinal chemist who works at Merck’s laboratories at West Point, Pennsylvania, and who was instrumental in the development of Belsomra. “What’s so exciting about Belsomra is that it is very selective for blocking wakefulness, so it doesn’t affect the systems that control balance, memory, and cognition,” he says.
In his career, Coleman has developed drugs to treat a range of different infections, illnesses, and disorders, but the orexin system stands out. “Narcolepsy has given us a thread we can pull on to unravel a lot about what underlies the systems that govern wakefulness and sleep,” he says.
“Wakefulness is a pretty central process for everybody, whether you are a healthy person or have narcolepsy or insomnia. It’s the most exciting thing I’ve had a chance to work on.” The applications of Belsomra may be wider still, with clinical trials proposed to investigate its potential to help shift workers’ sleep during the hours of daylight, improve the sleep of Alzheimer’s patients, help those suffering from post-traumatic stress disorder, combat drug addiction, and ease human panic disorder.
I am delighted to see these developments, but the millions of us with narcolepsy are still hoping for a drug that could work in the brain to rouse rather than silence the orexin system.
This has been a long-term project for Masashi Yanagisawa, who was in the race with Mignot to link the orexins with narcolepsy 20 years ago. But designing and synthesizing a compound that will make it through the gut intact, that has what it takes to find its way from blood to brain, and that boasts the perfect configuration to activate one or both of the orexin receptors is “a very, very high challenge” he says, one that is “significantly” greater than finding a compound to interfere with the receptor as Belsomra does.
Earlier this year, Yanagisawa and his colleagues published data on the most potent such compound to date, a small molecule called YNT-185. Injections of this molecule into narcoleptic mice significantly improves their wakefulness and cataplexy and reduces the abundance of the REM stage of sleep in which most dreaming occurs (one of the characteristics of narcolepsy). This, says Yanagisawa, is a “proof of concept.” Although the affinity of YNT-185 (how strongly it binds to the orexin receptor) is not great enough to warrant a clinical trial, Yanagisawa’s team has already hit upon several other potential candidates. “The best one is almost 1,000 times stronger than YNT-185,” he says.
While the symptoms of narcolepsy can vary wildly from one person to the next, the underlying pathology—the absence of orexins—is still the same. “If this compound works, it’ll work for all those patients,” he says. “In that sense, it’s a relatively simple clinical trial compared to many other disorders.”
A still more futuristic avenue involves stem cells. Sergiu Paşca has the office next to Emmanuel Mignot at Stanford and in 2015, he and his colleagues developed a way to take induced pluripotent stem cells (fashioned from skin cells) and direct them toward a new life as brain cells. “You can use this system to derive various brain regions and like a Lego game, assemble them to form circuits in a dish,” he says.
Recently, his lab has developed methods to do something similar for people with narcolepsy, starting with a skin cell and ending up with a fully functional orexin neuron. In theory, it should be possible to transplant this into the brains of people with narcolepsy and restore some of the function. This is, however, not something to be taken lightly. For a start, the cells themselves are unlikely to be exactly the same as orexin cells, inserting a needle into the brain is not a risk-free exercise, and there’s always the possibility that the immune system might make another assault on the transplanted cells.
So, will the tale of the orexins really have a happy ending? The translation of basic research into the clinic is notoriously difficult and expensive, says Casper. (The cost of the current best available treatment for narcolepsy—sodium oxybate, or Xyrem—is such that it is not routinely available for adults in England, even though it could transform the lives of many.)
There is a widespread perception that narcolepsy is a rare disorder with a small
market, so any pharmaceutical research and development in this area would be unlikely to reap a significant return. This ignores the fact that narcolepsy is probably undiagnosed in many people, and that someone who develops narcolepsy in their teens and lives into their 80s would need some 25,000 doses over their lifetime.
Even more compellingly, perhaps, the orchestrating role that the orexins play in the brain suggests the market for such a drug would go far beyond narcolepsy. Something that tickled up the orexins would be useful for any condition where excessive daytime sleepiness is an issue, not to mention the myriad other situations where low levels of these messengers may play a role, including obesity, depression, post-traumatic stress disorder, and dementia.
There is, I believe, one other reason why this story has not yet reached its conclusion. For too long, sleep has been undervalued, seen as an inconvenient distraction from wakefulness. With this mindset, research into the neuroscience of sleep does not seem like it should be a priority. Nothing could be further from the truth. There is now abundant evidence that poor sleep can have devastating consequences for physical, mental, and psychological health. Sleep is not incidental. It is fundamental, a matter of serious public health. Investing in sleep research is not just about the few with demonstrable sleep disorders. It is about everyone.
This post appears courtesy of Wellcome and Mosaic Science.



It was the nightmare that wasn’t.
On July 20, 2014, as West Africa struggled to quash a historically large outbreak of Ebola, an infected man carried the virus to Lagos, Nigeria—Africa’s largest city. In that dense throng of 21 million people, many of whom travel extensively, it seemed that Ebola would be impossible to track and contain.
The Threat of Polio in the Badlands of Boko Haram
But Nigeria was ready. In the previous years, it has been using investments and support from the United States and other countries to boost its efforts to eradicate polio. When Ebola came, it swiftly redirected all of that infrastructure at the problem, including an emergency operations center, a crack team of epidemiologists trained by the U.S. Centers for Disease Control and Prevention (CDC), and GPS systems that could be used to track potential cases. In the end, Nigeria brought Ebola to heel in just three months, with only 19 cases and eight deaths.
The country’s spectacular success is a testament to the decisive actions of its government and health workers. But it also shows how important it is for rich countries to bolster the capacities of poorer ones, where outbreaks are most likely to begin due to weaker health systems and dense populations. No nation can tackle the problem of epidemics alone. In a world in which someone with a deadly virus can fly to any other continent in less than a day, the United States is connected to the entire planet’s diseases. And so, to protect itself, it must protect everyone else. As Rebecca Katz from Georgetown University once said to me, “If your desire is to keep disease out of your country, the best way to do that is to contain it at the source.”
In the years since the Ebola outbreak, the United States has channeled billions toward this goal, and has led other countries in doing the same. But some of that funding is set to disappear. This is a common pattern with epidemics. Diseases flare up and we throw money and resources and troops at them. The crisis ends and peacetime brings complacency. “We’ve got to get out of this cycle of panic and neglect,” says Carolyn Reynolds, vice president for policy and advocacy at PATH, a nonprofit working in global health.
In a new report called Healthier World, Safer America, launched today, PATH calls for the United States to redouble its commitment to global health. The timing is no coincidence: Tomorrow, delegates from 50 countries will gather in Kampala, Uganda, for a ministerial meeting of the Global Health-Security Agenda—a five-year international partnership that aims to improve the health security of developing nations. PATH wants America’s support for the GHSA to continue, and to be backed up by strong leadership and a firm plan.
Barack Obama convened the GHSA in 2014 with strong bipartisan support, and since then, the United States has committed more than $1 billion to the program. These investments have already made tangible differences. As just one example, Cameroon’s response time to recent outbreaks of cholera and bird flu shortened from 8 weeks to just 24 hours. “That’s a really significant change,” says Reynolds, when thinking thinking about whether a disease remains a localized outbreak or flares into a globe-spanning pandemic. “We feel that the Ebola epidemic was a tipping point—a global wake-up call that the world is not prepared.”
This is as uncontroversial a position as exists in public health, and one also shared by many other major organizations. Since the Ebola epidemic, the National Academies of Sciences released a report urging that the “U.S. government should maintain its leadership position in global health as matter of urgent national interest.” The World Bank released its own report saying that “investing in preparedness is not a one-off, but an ongoing requirement” and that “every expert commentary and every analysis in recent years tells us that the costs of inaction are immense.” Meanwhile, an international coalition called CEPI—the Coalition for Epidemic-Preparedness Innovations—has raised $460 million to develop vaccines against potential pandemic diseases. “We have taken some steps, but there’s always another crisis that shifts political attention,” says Reynolds. “We can’t afford to take our eye off the ball.”
America has a long and bipartisan history of supporting global health. In 2003, George W. Bush created the President’s Emergency Plan for AIDS Relief (PEPFAR), which has since used over $72 billion in funding to distribute antiretroviral drugs to almost 11.5 million people in sub-Saharan Africa. In 2014, Barack Obama deployed thousands of troops and health workers to fight Ebola in West Africa, and secured an emergency budget of $5.4 billion to deal with the epidemic.
The Trump administration’s attitudes toward global health have been harder to gauge. The president’s penchant for isolationism, from his America First rhetoric to his thrice-attempted travel ban to his NATO skepticism, sit uneasily with an ethic of international cooperation, and the administration’s proposed budget threatened to slash $2.2 billion from global health programs. But recent signs from administration officials have been more reassuring.
For example, the U.S. delegation at the GHSA meeting in Kampala will be led by Tim Ziemer, a retired rear admiral who led George W. Bush’s President’s Malaria Initiative, and has been described as “one of the most quietly effective leaders in public health.” In July, at the Aspen Security Forum 2017, Thomas Bossert, assistant to the president for homeland security and counterterrorism, said the United States would “continue our full-throated support” of the GHSA. “The weakest country among us with the ... least preventative-care capabilities [is] going to be the patient-zero outbreak source,” he said. “And they're going to end up killing and infecting the world, and so we need to put money into places that don't have the money to do it themselves to prevent loss of life here. So that's it.” Secretary of State Rex Tillerson backed up this rhetoric  in early October, saying that “the United States advocates extending the GHSA until the year 2024.”
Reynolds argues that these strong verbal commitments must be backed up by equally strong financial ones. Currently, the United States spends around $450 million on global health security programs—less than 0.1 percent of what it spends on military defense. Congress has averted the worst of Trump’s proposed cuts, but its budget still reduces the total spent by 10 percent in fiscal year 2018.
An even steeper fiscal cliff looms in 2019, when $1 billion, which was diverted from the money allocated to fighting Ebola, runs out. That money has been used well, to train epidemiologists, buy equipment, upgrade labs, and stockpile drugs. If it disappears, progress will halt, and potentially reverse. The CDC, for example, would have to pull back 80 percent of its staff in 35 countries, breaking ties with local ministries of health. “We’re in the middle of substantial capacity-building efforts and it’s the wrong time to pull out,” says Reynolds.
Research and development into drugs, vaccines, and diagnostic tests for neglected diseases has also been underfunded. Until the Ebola money was appropriated, such funding was at its lowest since records began in 2007. It will fall back to that historical nadir unless new commitments are made.
Of course, budgets aren’t infinite, and the United States has any number of domestic health crises too, from environmental catastrophe in Puerto Rico to a nationwide opioid epidemic. But “it can’t be an either/or,” says John Monahan at Georgetown University, a former adviser to the State Department. “We have to do multiple things to protect the health of the American people, and one of them is investing in health security abroad.”
The word “investing” is key. A severe outbreak, equivalent to the flu pandemic of 1918, could cost the world up to $6 trillion and kill between 50 and 80 million people. It is far more cost-effective to prepare for one than to react to it.
Nor does preparedness involve huge amounts of money, doled out in perpetuity. PATH calculates that USAID and CDC could continue the work that was supported by the Ebola money with an extra $250 million over the next three years. That would go a long way toward helping the countries that are assisted by the GHSA to meet development benchmarks laid out by the World Health Organization, and reach a point when they can sustain their own progress.
“The American government, the American people, have done so much in global health,” said Awa Marie Coll-Seck, Senegal’s minister of health, at a recent event hosted by PATH. “It’s important for the United States to understand that supporting our countries to have strong preparedness and response to epidemics will protect the United States. It’s a win-win situation. If we are weak, everything will come. There are no borders, no passports, no visa if it’s a disease.”


New York is a city on the water. For hundreds of years, its rivers and harbor have worked to its advantage, bringing it speedy transportation and pleasant temperatures.
The next couple hundred years may not be as smooth sailing. Global warming, caused by the release of carbon-dioxide pollution into the atmosphere, will cause the seas to rise and the storms to intensify around the city. A new study from an all-star list of climate scientists attempts to estimate how a few of climate change’s symptoms—higher seas, large storm surge, and more intense hurricanes—will intersect in New York over the next 300 years.
It isn’t pretty. Sea-level rise will make every tropical cyclone that hits New York more likely to release damaging floods. For instance, storm floods of nearly seven-and-a-half feet once occurred only a couple times per millennium. In today’s somewhat warmed climate, 7.5-foot floods are projected to happen every 25 years. By 2030, these floods will occur every five years.
New York City has experienced 7.5-foot floods several times in the past decade. Superstorm Sandy loosed 10- or 11-foot floods on much of Manhattan, Brooklyn, and Staten Island, killing 43 people and inundating more than 88,000 buildings.
Not all of the news from the study’s estimates was bad. In a climate-changed world, the effects of storm surge on New York may remain about the same. Even though future hurricanes are likely to be more intense—and thus more likely to “push” more water in front of them, as storm surge—the hurricane models also showed the same storms avoiding New York Harbor. In other words, global warming seems to redirect some of the largest hurricanes eastward. It’s unclear why this may be the case.
Andra Garner, a climate scientist at Rutgers University and the first author of the paper, described the shift as “the biggest surprise from this work.”
“The eastward shift in storm tracks at the latitude of NYC in future simulations [was] a result that we did not expect to find, and something that we hope to investigate further with future research,” she told me in an email.
The study is the first to compare data from three sources: models of storm surge in New York City; probabilistic projections of sea-level rise; and advanced climate models that include high-resolution hurricane simulations. Its coauthors include Kerry Emanuel, a hurricane researcher at the Massachusetts Institute of Technology; Bob Kopp, a climate economist at Rutgers University; and Michael Mann and Richard Alley, two geoscientists at Pennsylvania State University.
The study also investigates how hurricane flooding will be affected by the possible collapse of the West Antarctic Ice Sheet. The timing of the ice sheet’s collapse remains debated: While some models suggest that a warmed world will erode the ice sheet between the years 2200 and 3000, newer and more aggressive studies argue it could collapse within decades. Its collapse will have profound consequences for the U.S. Eastern Seaboard, as its powerful gravitational pull currently keeps sea levels on that coast unusually low.
If it does begin to fall into the sea, ocean levels worldwide could rise five to six feet by 2100. “To our knowledge, this is the first study of its kind, looking at flood risk associated with tropical cyclones, to incorporate such ‘worst-case scenario’ sea-level rise projections,” said Garner.
The study doesn’t capture every kind of flooding event that New York will experience—only spinning tropical storms. It doesn’t account for systems like Superstorm Sandy, for instance, which became a more disorganized “extratropical cyclone” before it made landfall.



In just over a month, the world’s first theme park devoted entirely to Italian food will open its doors—and Gastropod has the scoop! Among Eataly World’s delights will be hunt-your-own truffles, baby lambs, beach volleyball, and custom Bianchi shopping bike-carts. But there’s a bigger story, and it’s that Oscar Farinetti, the founder of the Eataly empire, has somehow managed to make money by merging two businesses—grocery stores and restaurants—that are both incredibly challenging when it comes to turning a profit. In the process, he’s transforming the way we shop for food. Join us this episode as we tell the story behind the life and death of the great American supermarket—and take a trip to Italy for a sneak peek at its future.
The supermarket is such a mainstay of daily life throughout most of the developed world that it’s hard to imagine that someone invented it. But that is exactly what Michael Cullen did when he opened King Kullen in Queens, New York, on August 4, 1930. Before that, as Michael Ruhlman, the author of Grocery: The Buying and Selling of Food in America, explained, a grocery store was simply a dry-goods store, selling boxed and canned goods. To get their hands on perishable goods like meat, produce, and dairy, Americans had to either grow their own, await delivery (remember the milkman?), or visit one of the specialist shops that started appearing in cities in the late 1800s and early 1900s—butchers, greengrocers, and creameries.
The supermarket of today is many, many times bigger than the first King Kullen: It stocks 200 times as many products, and is often surrounded by a vast car park. But as impressive as it seems, the supermarket is in trouble. As Ruhlman explained, the typical grocery-store chain operates on a tiny 1 percent margin. “It’s a crazy business model,” he told us. “Nobody in their right mind would get into it.” On every front, the supermarket is under attack: It is losing the price wars against Walmart, and the battle for convenience against Amazon, Instacart, and meal-delivery services. And yet Farinetti, the founder of the Eataly chain of grocery mega-stores, has somehow managed to make selling food profitable again. How?
To find out, this episode we speak with Sunil Gupta, a Harvard Business School professor and the author of a recent case study on the Eataly chain, as well as CEO Farinetti himself. The secret? It lies in a blend of good, old-fashioned synergy—combining the best of the restaurant and retail business models to overcome the weaknesses of both—and Farinetti’s own creativity and passion for experiential storytelling. “I’d call him a maverick or a visionary,” said Gupta, who sees the lesson of Eataly as “how you can take an old, traditional industry and completely reimagine that.” Listen in as we tease apart Eataly-nomics, experience its ultimate expression at Eataly World, and discover what Farinetti’s example can tell us about the future of shopping for food.
This post appears courtesy of Gastropod, a podcast cohosted by Cynthia Graber and Nicola Twilley that looks at food through the lens of science and history.


When Chris Lowe first saw the buck stoop to lick the small, silver-speckled fox, he thought his eyes might be playing tricks on him. He’d just gotten back from a run on Santa Catalina, a remote Southern Californian island where he studies sharks, and came upon the two animals in the scrub. Mule deer and island foxes, the rascally miniature descendants of gray foxes, are everyday sights on Catalina’s grassy hills. But to see them nuzzling was downright weird.
Was the buck simply nibbling on a plant behind the fox? Had the fox happened to hop in front of the buck’s face? Lowe dashed into his apartment to grab his camera, and made it to the window to catch the deer taking another lick. The fox, docile in the shade of its antlered friend, wasn’t just tolerating the apparent cleaning, Lowe realized. “It looked like it was actually enjoying this,” he says.
How We Almost Lost the Island Fox
Lowe tweeted a picture of the curious scene a few hours later, and it quickly racked up several thousand likes and retweets. In the image, the buck has its pursed lips planted on the fox’s forehead. The fox, its eyes closed, resembles a dog getting a good behind-the-ears scratch. People responding to Lowe’s tweet were captivated by the strange pairing. It was adorable—in one person’s words, “a Disney moment!” And no one had ever seen anything quite like it.
Well, no one except Michael Cove. To match Lowe’s tweet, Cove offered picture of his own: a lean doe in a forest rubbing noses with a cat. “We get this all the time in the Keys ... interesting that it is happening on islands,” he wrote. Then he brought the party down: “Certainly a pathway for disease transmission.”
Cove, a mammologist at North Carolina State University who spends several months each year on the Florida Keys, has in fact spotted several peculiar meetings between the islands’ diminutive Key deer and other creatures. Motion-triggered cameras he’s set up around a wildlife refuge on one of the islands have photographed a deer dancing around a peacock, and a deer getting its face groomed by raccoon. There are a few more cases with cats, including a time off-camera that Cove passed a dumpster and saw two deer licking the same cat at once. (“The Florida Keys are an interesting place,” he says.)
Cove speculates that cut-off places like the Keys and Catalina, which is one of California’s eight Channel Islands, have two features that could encourage such interspecies intermingling. The most prominent is a lack of large predators. The islands’ deer have lived for generations on verdant floating worlds devoid of wolves, mountain lions, and other sharp-toothed threats. It’s possible their isolation has granted them a peace of mind that mainland deer can’t afford. Perhaps by now they don’t even know they could be afraid of other curious creatures.
The second is geography. Since an island’s inhabitants have limited land to roam, it’s easy for them to bump into each other. And as Cove points out in a new research paper in the journal Mammalian Biology, the scattered centers of human activity in the Keys attract animals that can find easy meals, pulling them into an even tighter orbit. The paper focuses on the Key deer and raccoons, but the same could likely be said of cats and island foxes, the latter of which are known to beg tourists for food and sneak off with your peanut butter even though you left it safely on your campsite’s picnic table, you swear.
These two factors account for a greater probability of animal-to-animal encounters on islands, but they don’t explain what would convince a deer to run its tongue over a cat or fox in the first place. Moreover, there’s evidence that this licking isn’t an island thing exclusively: Deer are occasionally spotted giving tongue baths to cats, at least, in mainland backyards. The exact motivation behind this behavior is much harder to pin down.
There’s a temptation to describe their interactions as mutually beneficial, in line with the natural world’s other astounding instances of species-to-species symbiosis. When Lowe first saw the buck and fox together, for example, he was reminded of underwater “safe zones,” where “predators and prey all line up to get cleaned” by small fish that munch on parasites.
Yet as Gary Roemer, an ecologist at New Mexico State University, points out, scientists reserve the concept of mutualism specifically for relationships in which both sides benefit in ways that help them survive. The dynamic is conceivable for Key deer and raccoons; in Cove’s camera-trap photos, a slinking raccoon takes a doe’s snout into its paws and nibbles around the patient animal’s eyes and ears, probably hungry for a snack of ticks. Neither Cove nor Roemer, who spent years studying island foxes earlier in his career, however, are convinced licking does much for the ecological fitness of deer, foxes, or cats.
Both researchers suggested what might be a more obvious benefit for the foxes and cats: Getting licked feels good. “Maybe deer are getting those hard to reach places,” Cove says. As for the deer, ocean breezes cover islands—foxes and cats included—in salt. Cove has a theory that deer on islands particularly might be lured into the cleanings by a little extra seasoning.
But Cove’s tweet about disease transmission also underscores the much more ominous way these pictures can be read. Even if it feels and tastes nice, contact between animals isn’t necessarily positive, because it can cripple populations by passing along rabies, roundworms, and plenty of other viruses and parasites. These dangers are especially threatening in locations where the entirety of a species resides. Key deer and island foxes, both endemic to their respective coastal islands, have each been pushed to the edge of extinction in the past. If their newly observed canoodling sessions hint at any larger changes in island ecosystems, they conceivably are causes of concern.
But Roemer cautions against the impulse to read anything more into a few documented instances of deer licking smaller and probably salty animals than what they perhaps most clearly seem to be: two wild creatures inquisitive enough to give each other a closer look. “This is probably a novel, random, curious interaction,” he says of the buck and the fox. “It probably doesn’t have much significance either from an evolutionary or an ecological standpoint.”
(Roemer doesn’t buy the salt theory, either: If plants and rocks are also coated by the breeze, he reasons, it wouldn’t make sense for a deer to go through the trouble of tracking down a moving, claw-possessing island resident for tastiness alone.)
Still, randomness leaves open two opposite conclusions about interspecies encounters like these. It’s possible—if not certain—that animals bump into each other in all kinds of undiscovered ways. “[Camera traps] are opening our eyes to just how fascinating the natural world is,” Cove says. “There are tons of species interactions that we might have never noticed just casually walking around the woods and stuff.” Key deer might not lick cats for a reason, but that doesn’t mean they don’t do it often.
Then again, it might be misguided to say what all deer on an island do in the first place. “More and more, we’re recognizing that, just like us, animals have different personalities,” Roemer says. “Sometimes they do bizarre things.” While working on one of the Channel Islands, he befriended an exuberant island fox named Josie, who made a game of goading a nature conservancy’s surly hunting dog into chasing her up trees.
So maybe it was an especially bold buck and a uniquely lonely fox that met under that fading afternoon sun on Santa Catalina Island. They neared each other in the brush of the only land they’ve ever known. And when they were close enough to touch, they were both filled with enough wonder to decide: why not?


The rain began on August 25, and it would fall, remarkably, for four more days. We know now that Hurricane Harvey dumped as much as 60 inches of rain over parts of Texas. Twenty trillion gallons in all. The equivalent of the entire Chesapeake Bay. Enough to push the Earth’s crust down two centimeters.
All of that water eventually had to go somewhere. It made its way to the Gulf of Mexico, and its volume was so massive that it did not immediately mix with the ocean. Nearly two months after the hurricane, a distinct blob of freshwater from Harvey is still moving through the Gulf. “We’ve literally never see that much freshwater added to the Gulf of Mexico at once,” says Kathryn Shamberger, an oceanographer at Texas A&M University.
Because it’s so unprecedented, scientists do not know exactly what effects the plume is having. Will it bring a path of destruction through marine ecosystems unused to freshwater? Will contaminants from land sweep through? Or will marine life quickly rebound? In the weeks after Harvey, scientists like Shamberger have mobilized to study the effects of this giant blob of freshwater. Shamberger and her colleagues transected the Gulf sampling water in late September, and they are embarking on a second research cruise this Friday to study the potential impacts on coral reefs.
What oceanographers do know about the interface of freshwater and ocean comes from studying rivers that naturally empty into the sea. The key is density. Because freshwater lacks dissolved salt, it is less dense and floats atop seawater. It becomes a barrier between the air and the ocean water, which can have nasty consequences. “The freshwater sitting on the salty water cuts off the oxygen from the atmosphere getting into the ocean, and then you get the dead zone,” says Steve DiMarco, one of Shamberger’s colleagues at Texas A&M.
Freshwater doesn’t move in one uniform blob, either. It can squirt and jet far away from the main body. Think of what happens when you add milk to your coffee, says DiMarco. In the first few seconds, the milk squirts through the coffee before fully mixing. Eventually, Harvey’s freshwater blob will mix into the ocean, too. DiMarco says he expects for it to take a few more weeks; the winter winds will stir it all up when they come in.
Meanwhile, a number of scientists have received or are expecting Rapid Response Research grants from the National Science Foundation to study Harvey’s freshwater pulse. Lisa Campbell, another oceanographer at Texas A&M, has received one such grant to look at the effects of Harvey’s freshwater on phytoplankton. “Previously when we seen slugs of freshwater, it changes the type of phytoplankton that grow,” says Campbell. And since phytoplankton are at the bottom of the food chain, that can have knock-on effects up the food chain from the copepods to the coral to the fish.
Shamberger and DiMarco are going out to the Flower Garden Banks National Marine Sanctuary, a coral reef about 100 miles off the coast of Texas. Because winds have blown Harvey’s freshwater blob south and west, Shamberger says the corals have likely escaped the worst of it. But the reef may still be susceptible to the aforementioned squirts and jets of freshwater or more subtle changes in phytoplankton. In July 2016, recreational divers had noticed unusually hazy waters and dying corals in the Flower Garden Banks, which scientists suspect were the result of floodwaters that went through Houston during April. In the early days after Harvey, satellite pictures captured brown sediment-rich runoff pouring out of Galveston Bay. The team will also collect samples to test for contaminants.
The Houston Flooding Pushed the Earth's Crust Down 2 Centimeters
“Often when we think about hurricanes and their impact on a coral reef, we are thinking about wave energy and storm surge and physical breakage of the 3-D reef structure,” says Adrienne Correa, a marine biologist at Rice University. But because Harvey dumped so much freshwater into the ocean, it might have this whole other set of effects, and this is a unique opportunity to study them. Correa is leaving on the cruise Friday as well, despite the fact that her home was flooded during Harvey and it’s still “pretty much a mess.”
DiMarco was actually on a research boat in the Gulf when Hurricane Harvey started threatening Texas. When the storm hit, they pulled into port west of Louisiana and started driving back to Texas through the rain. “It was just incredible how much water there was,” say DiMarco. “Every time we went over an overpass, the people in my car—and I was driving—were like, ‘I wonder if we’re going to come down in the middle of lake.’” That was when the planning of his post-Harvey research began. And now, nearly two months later, they’re still tracking the effects of that water through the Gulf.


It took 130 million years for astronomers to see the light. On August 17, scientists observed through telescopes a small, glowing orb, the remnants of a collision between two neutron stars in another galaxy that triggered universe-bending gravitational waves. They watched as the sphere changed from royal blue to crimson red, as lighter chemical elements in the cloud of radioactive debris gave way to heavier ones, like gold, platinum, and silver. About a week later, it faded.
The light show may be over in the night sky, but it can be found on the internet and replayed, over and over, as a dreamy short video:

Luís Calçada created the video for the European Southern Observatory, whose fleet of telescopes in Chile tracked the aftermath of the collision. Calçada is a member of ESO’s education and public-outreach department, a team of astronomers and science-communication specialists in Munich.
“We wanted to have something striking, but we wanted it to be correct,” Calçada said.
The clip is another addition to a rapidly growing volume of illustrations and animations of wondrous astronomical objects and phenomena. As the rate of discovery of exoplanets has picked up in the last several years, so has the production of visualizations of these worlds. Often, scientists and illustrators have only a few pieces of the puzzle, like the mass, temperature, and orbit. They look carefully at how these factors have shaped the celestial bodies that we can see, and use it as inspiration to create a full picture of those we can’t. When astronomers discovered the presence of seven Earth-sized planets in a star system 39 light-years away, illustrators turned tiny blips in data into colorful alien worlds.
Calçada has worked at ESO for about 11 years. He was studying astronomy in school when he started experimenting with computer graphics. He eventually decided that he wasn’t going to be a scientist, and he went into science communication instead. The ESO gig, he said, is the perfect mix.
When exciting research papers come in at ESO, it’s up to Calçada and the rest of the team to create compelling visuals out of the data. They come up with designs for illustrations and videos, send it to the paper authors for feedback, and exchange notes until everyone’s happy with the product. When scientists have few specifics, the animators have some creative freedom. “But of course, some other times, they say no, that wouldn’t happen because the star is too big or the temperature is too high,” Calçada said.
On some rare occasions, illustrators get to check their designs against the real thing—they just have to wait for technology to catch up with them. In 2009, astronomers using ESO’s Very Large Telescope published some research about Pluto’s thin atmosphere, which is mostly nitrogen with some methane. Calçada whipped up a short video to accompany the research. The clip, “filmed” from the perspective of a camera panning over the surface of the dwarf planet, showed a hazy, bluish atmosphere over gray, Arctic-esque terrain. When the New Horizons spacecraft arrived at Pluto in 2015, it returned images of the dwarf plant that looked eerily similar.
Illustrations and animations of scientific research make the cosmos look like a radiant place bursting with color. But it’s worth remembering that the universe doesn’t always look that way to the scientists doing the work, Calçada notes. “They’re usually looking at boring code on the computer,” he said.
Many astronomers rely on spectroscopy, a technique that involves measuring the light across the entire spectrum of electromagnetic radiation. Astronomers use spectroscopic instruments in telescopes to observe different wavelengths coming from an object, from radio waves to visible light to gamma rays. The data, known as spectra, appears to astronomers as a simple squiggly line on a plot, like a cosmic electrocardiogram. But the spikes and dips can reveal a lot about the object in question, whether it’s a single star or an entire galaxy, like its mass, temperature, and chemical composition. In other fields of scientific study, Calçada said, “you can go outside, you can study the rocks, you can study the plants, you can look at the fossils. But astronomers, they only have light.”
Light was exactly what astronomers wanted to find in this kind of discovery. Before this week, scientists had detected gravitational waves four times, but the ripples came from collisions of black holes, which don’t emit any light. Astronomers had no way of knowing where exactly the waves originated. The new finding marks the first time we have seen the source.
Space illustrations sounds like they should evoke wonder at all times, but for the people making them, they can seem pretty routine. Calçada said his first few exoplanet projects were exciting, but as more and more were discovered, the work of animating them began to feel a little mundane. Oh, look! Another rocky planet. Eventually, perhaps soon, the work of visualizing cosmic collisions might lose some of its thrill, too; the scientists running the gravitational-wave detectors predict the extremely sensitive instruments will detect one or more mergers a week.
“But don’t worry, there’s going to be more exciting astrophysics and astronomy,” Calçada said. “There’s always going to be something more exciting and new coming out. I’m not worried of getting bored.”


The Centers for Disease Control and Prevention (CDC) keeps a Most Wanted list for flu viruses. The agency evaluates every potentially dangerous strain, and gives them two scores out of 10—one reflecting how likely they are to trigger a pandemic, and another that measures how bad that pandemic would be. At the top of the list, with scores of 6.5 for emergence and 7.5 for impact, is H7N9.
Influenza viruses come in many flavors—H5N1, H1N1, H3N2, and so on. The H and N refer to two proteins on their surface, and the numbers refer to the versions of those proteins that a particular virus carries. H1N1 was responsible for both the catastrophic pandemic of 1918 that killed millions of people, and the most recent (and much milder) one from 2009. H5N1 is the bird-flu subtype that has been worrying scientists for almost two decades. But H7N9? Until recently, it had flown under the radar.
H7 viruses infect birds, and only very rarely jump into humans. H7N9 in particular had never been known to infect humans at all before 2013, when it caused an unexpected epidemic in China. It was billed as low-pathogenic (or “low-path”) because it only caused mild disease in chickens. But in humans, the story was different: Of the 135 people infected, around a quarter died.
Every year since, there’s been a new epidemic, and the current one is the worst. H7N9 has evolved, acquiring mutations that allow other flu strains to reproduce more effectively in both birds and mammals. It has started killing birds. In one year, H7N9’s highly pathogenic (“high-path”) strains have caused as many human infections as the previous four epidemics put together. As of September 20, there have been 1,589 laboratory-confirmed cases, and 39 percent of those people have died. “It was a matter of time,” says the flu expert Yoshihiro Kawaoka, from the University of Wisconsin-Madison. “It wasn’t surprising to see this change.”
Kawaoka and his colleagues have now studied the new high-path strains collected from one of the people who died this year. They’ve shown that these strains reproduce efficiently in mice, ferrets, and monkeys, and cause more severe disease than their low-path ancestors. They can spread through the air between captive ferrets, and in some cases, kill the animals they land in. Perhaps most worrying, some strains have already evolved the ability to resist Tamiflu, a frontline drug that’s used to treat flu infections.
These are, of course, just animal studies, and they’re an imperfect reflection of how the high-path viruses behave in humans. “The little data available to date does not reveal an obvious increase in virulence for humans,” says Malik Peiris, from the University of Hong Kong, “but this is very difficult to assess because we only see the more severe infections who present to hospitals. This is an issue that needs to be closely monitored in the upcoming winter season.”
“When you compare H5 and H7 viruses, I think H7 are more worrisome,” says Kawaoka. That’s because the H5 viruses need several further mutations to spread between mammals, as Kawaoka showed in controversial lab experiments where he engineered strains with those mutations. But H7 strains apparently don’t need such tweaks. The strains that are out there right now are already capable of spreading between ferrets.
And yet, there’s no strong evidence that they’re hopping from person to person. Some of the cases this year have occurred in family groups, but it’s hard to say if they passed H7N9 between them or simply acquired it from the same birds. For now, the CDC still notes that “the risk to the general public is very low,” since most people who were infected had been in direct contact with birds, whether in poultry markets, vehicles, or their own homes.
“Clearly this is a virus that we don’t want to become any more transmissible between humans,” says Wendy Barclay, from Imperial College London. “But it’s not already transmissible enough to cause a pandemic—otherwise, we would have seen one.” She also notes that, in Kawaoka’s study, the high-path strains didn’t spread any more easily between ferrets than their low-path cousins. Even though this year’s epidemic is unprecedentedly big, the viruses don’t seem to be any more transmissible than when they first emerged in 2013.
There’s also a silver lining to the Tamiflu-resistant strains that Kawaoka identified. The mutation behind this resistance works by changing the shape of a protein on the virus’s surface—a protein that Tamiflu normally attacks. But the same protein is also part of the infection process; by changing its shape, the strains weaken themselves. They cause milder disease in both mice and ferrets (although they still spread with the same ease as the drug-sensitive strains).
That’s good news, but it’s no reason to rest on our laurels. In 1999, scientists discovered a mutation called H274Y that made H1N1 strains resistant to Tamiflu, but that also reduced their ability to infect mouse and ferrets. The scientists thought that this mutation was “unlikely to be of clinical consequence.” They were wrong. H1N1 picked up other mutations that compensated for H274Y, creating flu strains that were infective and resistant. By 2008, almost all the seasonal strains of H1N1 had become resistant to Tamiflu. With H7N9, history could well repeat itself.
But Tamiflu isn’t our only weapon against influenza. There’s an experimental new drug called Avigan (or favipiravir) that, rather than going after a surface protein, attacks an enzyme that the virus uses to copy its genetic material. Even Tamiflu-resistant strains of H7N9 fall to this drug, as do other kinds of flu that Kawaoka has looked at—at least in animals. “Whether that’s also the case in humans, we don’t know,” he says.
The viruses could eventually evolve to resist this new drug, too. But, Kawaoka says, “many people, including us, have looked for viruses that are resistant to favipiravir, and I don’t think anyone has found one yet.” And Barclay suggests that scientists should start running clinical trials that test both drugs together. “It still astonishes me that we continue to treat flu patients with a single drug when we know that the virus is highly mutable,” she says. “It’s almost inevitable that drug-resistant viruses can evolve.”
In the meantime, vaccines are being developed to match the viruses seen in the fifth and current epidemic. Other control measures have waxed and waned. When the first of the epidemics struck, Chinese health ministries closed markets and slaughtered birds. But as Helen Branswell reports in STAT, some of those containment efforts became more lax in 2015 and 2016.
Again, there is some good news: H7N9 infects chickens very well, but unlike H5N1, it seems to avoid ducks. That matters because Chinese ducks are often housed outside, and domestic birds can mingle with wild ones. Aboard ducks, bird flu can easily spread from one infected farm to other parts of the world. “That may be a major difference that may make it easier to control H7N9 compared to H5N1.”
It might also be a blessing in disguise that the high-path strains have emerged. The low-path strains were very hard to detect because they didn’t cause symptoms. But the high-path viruses kill infected birds, which means “they might be easier to eradicate from chickens since they can be more easily detected,” says Adolfo García-Sastre, from the Icahn School of Medicine at Mount Sinai in New York. “However, one would need a very well-organized eradication campaign to eliminate them from poultry before they spread to other areas beyond China. I’m afraid that this will not happen, since it did not happen with the H5N1 viruses, which were first detected in 1997, and finally disseminated to most of the rest of the world starting in 2003.”


The bottles were getting emptier: That was the first sign that something awful was happening.
Since 1989, scientists from the Entomological Society Krefeld had been collecting insects in the nature reserves and protected areas of western Germany. They set up malaise traps—large tents that funnel any incoming insect upward through a cone of fabric and into a bottle of alcohol. These traps are used by entomologists to collect specimens of local insects, for research or education. “But over the years, [the Krefeld team] realized that the bottles were getting emptier and emptier,” says Caspar Hallmann, from Radboud University.
By analyzing the Krefeld data—1,503 traps, and 27 years of work—Hallmann and his colleagues have shown that most of the flying insects in this part of Germany are flying no more. Between 1989 and 2016, the average weight of insects that were caught between May and October fell by an astonishing 77 percent. Over the same period, the weight of insects caught in the height of summer, when these creatures should be at their buzziest, fell by 82 percent.
“We were expecting declines, but the extent of them was tremendous,” says Hans de Kroon, who was involved in analyzing the Krefeld data. “If this was in agricultural settings, we wouldn’t be quite so surprised. But it’s especially alarming that it happened in nature reserves.”
There have long been signs of such a decline. Studies have also shown that populations of European butterflies have halved since 1990, honeybee colonies have fallen by 59 percent in North American since World War II, and populations of British moths have dropped by 30 percent per decade. But most of these surveys focused on particular groups, whereas Hallmann’s group looked at the entire spectrum of flying insects. “It confirms the widespread, windscreen phenomenon,” he says. “Any truck driver in the developed world will tell you that they used to squash a lot of insects on the windscreen. Now the windscreens stay clean.”
“The study makes visible what otherwise has been an invisible decline in insect abundance,” says Michelle Trautwein, from the California Academy of Sciences. “Our mistreatment of the planet has been recognizably bad for elephants and coral reefs, but it seems likely that it has also been just as bad for flies, moths, beetles.”
This is, to put it mildly, a huge problem.
Insects are the lynchpins of many ecosystems. Around 60 percent of birds rely on them for food. Around 80 percent of wild plants depend on them for pollination. If they disappear, ecosystems everywhere will collapse. But also, insects are the most diverse and numerous group of animals on the planet. If they’re in trouble, we’re all in trouble.
There’s a debate about whether the Earth is in the middle of a sixth extinction—an exceptionally severe period of biological annihilation of the kind that has only happened five times before. One of the talking points in this debate is that, as Peter Brannen recently wrote, “when mass extinctions hit, they don’t just take out big charismatic megafauna, like elephants ... They take out hardy and ubiquitous organisms as well—things like clams and plants and insects.”
And. Insects.
But remember that the German study only looked at one particular region. And it raises a question: If insects have disappeared by such a large degree, wouldn’t other species that depend on them be in much worse shape? Wouldn’t Germany’s flowers, birds, spiders, and reptiles also be plummeting? “We see great declines of insectivorous species—but not to this extent in most cases,” de Kroon acknowledges. “Some species could switch food sources, but we don’t really know what’s going on. We do know that we see declines in even common species, like blackbirds, starlings, and sparrows.”
Another unanswered question: Are all groups declining equally? “It would be interesting to see the list of species they collected, as Malaise traps are very good at collecting certain species and poor at collecting others, like dragonflies,” says Jessica Ware, from Rutgers University. “If insect [groups] vary in their response to climate change, temperature, habitat change, or other factors,” that could change the implications of the study’s stark percentages. (Hallmann notes that identifying the thousands of individuals in a single trap, let alone all 1,503, would mean months of work for a team of specialists. That’s why they focused on total weight.)
Also, what’s behind the insect downfall? Pollutants and pesticides are likely to be a problem. Neonicotinoids—the world’s most popular insecticides—can mess with bees in myriad ways, impairing their memory, befuddling their spatial skills, and preventing them from finding food.
More surprisingly, the German team couldn’t find any evidence that the two usual suspects—habitat loss and climate change—were important culprits. The declines were similar in every kind of habitat, whether healthy grasslands or nutrient-poor wastelands. And although weather patterns in the region could explain the numbers of insects across a season, they couldn’t account for the year-on-year decline.
But neither line of evidence is clear-cut. The team didn’t look at larger-scale climate events, like prolonged droughts, and they couldn’t measure the effect of habitat fragmentation—cutting up the land available to insects rather than merely reducing it. Indeed, the nature reserves in the German study are small, too distant from each other for insects to travel between, and locked in by agricultural land. Those are “hostile environments” for insects, de Kroon says, so species that thrive in the reserves could drain into the surrounding no-man’s-land—and be lost.
Ware wonders if some of the vanished insects are simply migrating into other areas. “We know that certain dragonflies, for example, are changing their ranges in response to climate change,” she says. “So are neighboring countries experiencing a similar loss, or are specific species moving northward?”
That’s why researchers need to do similar surveys in other countries, says Crystal Maier, from Chicago’s Field Museum. “We could actually do that. We have similar samples here at the museum, for similar ranges and time periods. You could spend a lot of time identifying species but they just weighed the samples. That’s something we don’t usually do but it’s so simple, and it would be interesting.”
In the meantime, “we should use anything we have to enhance insect populations, like adding flower-rich areas around the margins [of agricultural land],” says Hallmann.
“We don’t want people to get depressed,” says de Kroon. “Ecosystems are very resilient. They’re still functioning quite well despite this loss. Let’s make use of that resilience. We can’t wait till we know exactly what’s leading to these losses. We have to act.”


Last year, a 77-year-old woman traveled to a clinic in Georgia to have stem cells injected in her eyes. She came in hope of a cure—or at least something that could help her macular degeneration, which causes a dark spot to appear in the center of vision.
The procedure was supposed to work like this: The clinic would take fat from her belly, separate out stem cells that naturally occur in fat, and inject them into her eyes to regenerate damaged tissue. The procedure cost $8,900. It had not been approved by the Food and Drug Administration and was not covered by insurance. To pay out of pocket, she had to raise money on a crowdfunding site.
Her vision did not get better. It got much worse. Within three months, her retinas—the eye’s layer of light-sensitive cells—had peeled away from the rest of her eyes. As a result, she can only make out hand motions in her right eye and light in the left, according to a recent case report. She could no longer walk on her own.
In March, eye doctors based primarily at the Bascom Palmer Eye Institute in Miami had published a widely covered report describing three eerily similar cases: Three elderly women with macular degeneration got stem cells derived from their own fat injected into their eyes at a different stem-cell clinic in Florida. The same thing happened: Their retinas became detached, and they went blind. The doctors ended up examining the 77-year-old woman too, which led to the recent case report describing her condition.
And there are likely even more cases. Since writing the first report, says Ajay Kuriyan, an author on the report and now a retinal specialist at the University of Rochester, eye doctors around the country have come forward with similar stories of stem-cell injections gone awry. They are now preparing an article describing the additional cases.
* * *
Stem-cell clinics that offer seeming miracle cures for everything from back pain to erectile dysfunction have proliferated in the United States in the past decade. These cases of blindness now cropping up in the medical literature point to the potential dangers of letting hundreds of such clinics operate without oversight.
In August, the FDA moved toward a crackdown. It posted a warning letter to the Florida clinic that had treated the first three women and called the fat-derived stem cells an unapproved treatment. On the same day, the agency announced that federal marshals had seized live-virus vaccines from a California company that was injecting the viruses along with stem cells into cancer patients. After the news broke, says Mark Berman, a plastic surgeon and the California company’s director of stem-cell implantation,“I’ve actually had patients call me up, cancel their surgery, demand their money back, and tell me what a disgusting human being I am and I should be removed from this planet.” He criticized the initial news reports as “classic leftist kind of propaganda, fake news.”
Berman also cofounded the Cell Surgical Network, of which the Georgia clinic that treated the 77-year-old woman is an affiliate. The network trains affiliated doctors to use their equipment and follow their stem-cell therapy protocols.
The case report does not name the Georgia clinic, but The Atlantic has independently confirmed it is the Stem Cell Center of Georgia, which operates within the Ageless Wellness Center in Peachtree City. The clinic declined to comment for this story. A local news report from June 2016 quotes the center’s doctor as saying, “We have an ophthalmologist who is going to treat three people with macular degeneration with intraocular injections.”
Berman says that his network’s affiliates have performed about 15 eye procedures total. They stopped offering it after the woman went blind. (Cell Surgical Network and the Stem Cell Center of Georgia both still list macular degeneration on their websites.) The injections, he says, were part of a study approved by an institutional review board. At the clinic, the 77-year-old woman received injections into her two eyes one day apart. Berman concedes that they should have waited longer to make sure there were no serious side effects after the first eye. “That’s a pretty good lesson learned. Unfortunately it was learned by doing them,” he says.
Others say the clinic should have known better. “It’s just not a professional thing to take an unproven intervention and inject it in both eyes,” says Leigh Turner, a bioethicist at the University of Minnesota, who tracks stem-cell clinics. Kuriyan says that injecting both eyes and asking patients to pay out of pocket for their treatment are both highly unusual for clinical trials. “Those are all big red flags,” he says. A better approach, he says, would have been to test the injections in animals for safety first.
The Dangers of Snake-Oil Treatments for Autism
It’s unclear exactly why the stem-cell injections caused such a bad reaction in these women. Perhaps the stem cells had differentiated into cells that formed a membrane and then contracted, peeling the retina away from the rest of the eye. Or perhaps there was scarring caused by immune cells, which are part of the mix of cells in fat that can be injected along with stem cells into the eye.
Given the growing number of retinal detachments coming to light, Turner wonders if there are other consequences of stem-cell injections that have not been reported. Berman says that the Cell Surgical Network has performed 7,000 stem-cell injections into various parts of the body, and the 77-year-old woman’s case is “the only real significant problem we’ve had.” A FDA inspection report from July chastised his network for failing to investigate and report four “serious adverse events” to the FDA, including the 77-year-old woman’s case as well as a hip infection, another “severe infection,” and a case where a patient was hospitalized for confusion and headache.
The FDA has drafted guidelines on how to oversee stem-cell clinics. The agency says stem cells do not have to be regulated as drugs as long as clinics follow certain standards, like if they only minimally manipulate the cells and don’t change their purpose in the body. For example, transferring fat from the belly to breasts would not fall under FDA purview because the fat is still acting as fat. But if stem cells are being separated from fat and then injected to treat a disease, then the FDA may have reason to step in.
“The question to me is, ‘Will the FDA really dedicate the resources that are needed?’” says Turner. The FDA very publicly criticized a couple stem-cell clinics in August as a warning shot, but there are hundreds of such clinics in the Untied States, and overseeing them all would require a significant investment. In just the five years Turner has been closely tracking the stem-cell industry, he says, business seems to have been booming.


Roses are red, violets are blue. Except they’re not. They’re, er, violet.
True blue flowers are exceedingly rare, and not for lack of effort. Plant breeders have repeatedly tried to nudge roses and chrysanthemums into blueness, but doing so is really hard (at least, without the use of dyes). These flowers get their colors from pigments called anthocyanins, which typically look pink or red. A flower must chemically tweak these pigments to make them bluer, and even if they did, the results are essentially purple.
Only a few flowers, like cornflowers and Himalayan blue poppies, have achieved true blue, and all by using special chemical tricks like adding metals to their pigments, or making their petals more alkaline. “All of this is chemically quite difficult and not many species have evolved the enzymes to do it,” says Beverley Glover from the University of Cambridge. “Even with genetic modification, people have managed to make purple, bluish roses, but true blue isn’t happening.”
So imagine her surprise when she found that many flowers have secret blue halos in their flowers.
The halos are rings at the bases of the flowers’ petals. Sometimes, they’re visible to us, especially if the petals are dark. But in most cases, they’re so faint that we can’t see them. Glover only detected them with the aid of laboratory equipment. And yet, they’re there—and they’re visible to bees, whose eyes are more sensitive to blue wavelengths of light than ours are.
Glover discovered the first of these halos back on a sunny day in 2009. While walking through the Cambridge University Botanic Garden, she came across Hibiscus trionum—a white flower whose petals have dark-red bases. And those bases, Glover noticed, were shiny. If you moved them around, they would take on blue, yellow, or green metallic sheens on top of the red undercoats.
The cells on the surface of these petals are mostly flat and smooth, but those at the base have many microscopic ridges, like the grooves on a vinyl record. When light hits each ridge, part of it reflects away, and the rest passes through to the other ridges. And because the ridges are regularly spaced, the reflected beams amplify each other to produce colors that are unusually vivid from certain angles. That’s iridescence. It’s color produced not by chemical pigments, but by microscopic structures. “We wanted to know how common this was,” Glover says. “Was this just one or two weird plants—or more?”
More, it turned out. Her team, including Edwige Moyroud and Tobias Wenzel, found at least 11 families of flowers in which at least one species has the same microscopic ridges, and the same iridescent halos. At first, they were puzzled because the iridescence is inconsistent—strong in some species, but weak in others. But weak or strong, it always has a blue component. “If we had just looked at one plant, we wouldn’t have thought anything of it,” Glover says. “It’s only when we looked at a dozen species, and they were all making the same blue, that we thought: Maybe it’s the blue we should be looking at.”
And when they took a closer look at the petals producing the blue halo, they found something stranger.
To create a vivid sheen, those microscopic ridges should all have the same dimensions, and be equally spaced apart. But biology is messy, and plants can’t manufacture structures to such exacting specifications. So the actual ridges come in a variety of heights, widths, and spacing. Glover’s team found that the degree of this variability, which they term “disorder,” is the same across flowers. And for reasons that are still unclear, this particular amount of disorder scatters blue light at specific angles away from the petals. Hence the halo.
The team tested this by creating artificial ridges of their own, with varying degrees of disorder built in. If the ridges are massively disordered and very different from each other, they don’t produce any interesting optical effects. If they’re all the same, they’re iridescent, but there’s no blue scattering. But in the Goldilocks zone, where the ridges are disordered but not too disordered, they create a blue halo. “It’s a different way of making blue,” Glover says.
If the halo lies over a black, purple, or red base, it’s possible for humans to see it. If it lies over white or yellow, we can’t. But Glover’s team showed that bumblebees can see the pattern no matter the background, and in lab conditions, they find flowers more quickly if they have a blue halo.
“When the same trait evolves over and over again, it is strong evidence that it’s adaptive,” says Lena Hileman from the University of Kansas. And since the blue halo is “very widespread in flowering plants, that suggests it is a reliable signal to pollinators about the nectar reward of the flower.”
But “being able to perceive the pattern doesn’t necessarily mean the pollinators actually care about it, or prefer it to simple, pigment-based colors,” says Yaowu Yuan from the University of Connecticut. “The importance of this blue halo to pollinators is still an open question.”


In addition to the cars flipped, the roofs collapsed, and the streets flooded, this season’s unprecedented string of devastating hurricanes also hit a much grander target: a volcano. The Caribbean island of Montserrat, home to the active Soufrière Hills volcano, twice suffered blows: first an almost-direct hit by Category 5 Hurricane Irma, then a direct hit by Category 5 Hurricane Maria.
The volcano’s peak this year was, thankfully, more noisy than explosive. No volcano-related calamities were reported from the storm. But the collision of the two disaster-movie terrors reveals the possibility of an unnerving scenario: What happens when a superstorm lands on a volcano that’s erupting?
While there’s no record of a top-of-the-scale Category 5 hurricane or super typhoon ever running over such a volcano, there are hundreds of active volcanoes in the tropics—including 20 in the Caribbean—where some of the worst storms start tearing things apart. And evidence suggests a warming climate only intensifies these storms. No one is certain about what would happen in a matchup, but most likely neither of the two primeval forces could take the stuffing out of the other. Even so, the battle could prove especially deadly for people caught nearby.
Some meteorologists and geologists speculate that the collision would be like a pair of agitated gorillas: two giants making a lot of noise, shaking the ground, and then parting ways. It’s unlikely the volcano would make much of a dent in this battle. “As massive as the impact of a volcanic eruption can be, it’s hard to grasp the scale of a hurricane,” says David Nolan, an atmospheric scientist at the University of Miami. “The eye wall of a hurricane typically has a radius of 30 miles. Off the top of my head, I'd say the primary updraft from a volcano is just one mile across.”
As a result, he explains, almost anything a volcano can do to disrupt a storm would be localized and likely erased as it moved along. Tracy Gregg, a geologist at the University at Buffalo, also emphasizes how little impact typical volcanic eruptions would likely have on such a powerful storm. “A volcano punching up through [a hurricane] would be a smudge on the windshield,” she says.
Nevertheless, there are certain circumstances that could be much more extreme. It’s possible that a volcano’s intense heat—lava can register 2,200 degrees Fahrenheit—could intensify a tropical cyclone. Heat evaporates seawater, which rises to create thunderstorms. No heat, no storm. And an explosive eruption might still temporarily disrupt areas within a hurricane. For instance, dust shot into the atmosphere could delay or accelerate rainfall, Nolan says. One study suggests the dust would contribute to more lightning. In fact, massive eruptions often generate cinematic lightning within their boiling black clouds.
More intriguing—and terrifying—is a theory that a submerged, massive volcano could heat enough ocean water to supercharge a Category 5 hurricane or super typhoon. Heating the already abundant heat of tropical waters would make storms bigger and stronger, and do it in less time. Jeff Masters, the meteorology director at Weather Underground, says such an underwater eruption might even spin up something called a hypercane, with winds reaching the speed of sound.
It would take “an awful big coincidence” for this to happen, Masters reassures: The volcano would have to be big enough and hot enough to heat hundreds of miles of the ocean’s surface to 122 degrees. Standard weather dynamics would take it from there. “It’s theoretically possible,” Masters says. So is the possibility that a hypercane could inject enough water into the stratosphere to block the sun, creating a years-long global winter.
Team Hurricane’s fever dream is more mundane. Some think the low air pressure in a hurricane’s eye might weakly draw up the contents of a volcano, like a TV cowboy sucking venom from his cowpoke friend's snakebite wound. But “that’s really unlikely,” Gregg says. After all, volcanoes begin 1,800 miles down, where temperatures exceed 6,700 degrees, and liquid rock (or magma) is squeezed by pressures that are 1.4 million times greater than air at sea level. A change in air pressure over a volcano is like a fly buzzing off an elephant’s back.
The feet of water that a storm can drop also isn’t likely to have any effect. The entire state of Hawaii, for example, is a collection of gurgling volcanoes that rose through 20,000 feet of water from the floor of the Pacific. Volcanoes experiencing a Category 5 hurricane might as well be enjoying a music-festival mist station.
Major hurricanes slamming into volcanoes could still have a devastating impact, but the victims would be the environment below the crater—and anyone who lives there. As often happens even with lesser hurricanes, rain soaks into the enormous deposits of fine ash resting precariously on a steep volcano’s flank until the resulting warm mud suddenly breaks free. The rush of debris, known as a lahar, roars down the slope at 120 miles per hour like a runaway freight train. Anything—and anyone—not washed away is encased in the mud.
In 1991, Typhoon Yunya hit Mount Pinatubo on the Philippines island of Luzon and triggered lahars that, according to Masters, killed as many as 300 people. Yunya, barely a typhoon at the time, delivered a glancing blow to Mt. Pinatubo as the mountain—coincidentally—ended almost a year of fitful activity with the 20th century’s second-most-monstrous volcanic explosion. The lahars easily inflicted more damage than the eruption.
At lease one positive could result from a geological-meteorological grudge match, Gregg points out. The hurricane likely would act like an air filter, trapping motes of dust in raindrops. Dust then would fall to earth almost immediately, instead of into the atmosphere, as typically happens with all large, explosive volcanoes. Up there, the dust would circle the globe, reflecting sunlight back into space and measurably disrupting air temperatures for years.
Asked if he would ever buy a ticket to Montserrat, the island struck by Irma and Maria, to witness such a matchup, Masters paused a surprisingly long time before laughing. “Maybe when I was younger, I would. I don't know.” It sounded like a “no,” but as with hypercanes, he couldn’t rule it out completely.


A researcher at a Swedish university says that Viking burial clothes bear the word “Allah”—and some people really want to believe her.
Annika Larsson, a textile researcher at Uppsala University who was putting together an exhibit on Viking couture, decided to examine the contents of a Viking woman’s boat grave that had been excavated decades ago in Gamla Uppsala, Sweden. Inspecting the woman’s silk burial clothes, Larsson noticed small geometric designs. She compared them to similar designs on a silk band found in a 10th-century Viking grave, this one in Birka, Sweden. It was then that she came to the conclusion that the designs were actually Arabic characters—and that they spelled out the name of God in mirror-image. In a press release, she described the find as “staggering,” and major media outlets (including The New York Times, The Guardian, and the BBC) reported the story last week.
But other experts are not sure the silk bears Arabic script at all, never mind the word “Allah.” They warn that people being credulous of Larsson’s claim may be guided less by solid evidence than by a political motivation: the desire to stick it to white supremacists.
“Everybody wants a counter-narrative for the narrative that’s been put forward by white supremacists,” said Stephennie Mulder, an associate professor of Islamic art and architecture at the University of Texas at Austin. She was referring to the tendency of white supremacists to appropriate the symbols of Vikings, whom they claim constituted a pure-bred white race; in Charlottesville, for example, neo-Nazis were seen toting banners with Viking runes. The idea that Vikings were influenced by Muslims would likely be anathema to them. “The Vikings are every white supremacist’s favorite white guy.”
Mulder took to Twitter on Monday to debunk Larsson’s claim. In a 60-tweet thread, she described her three main issues with it.
First, the style of Arabic that Larsson says she has identified—square Kufic—is not known to have been used in the 10th century; it only became common about 500 years later.

It’s a style called square Kufic, and it’s common in Iran, C. Asia on architecture after 15th c., ex: Safavid Isfahan w/Allah and Ali 9/60 pic.twitter.com/pbGJNFITGk

Second, even if you read the script as Arabic, it does not say “Allah” but “lllah,” a meaningless non-word. In place of an alif or “a,” it has a lam or “l.”

The word “Allah” in Arabic looks like this: الله. It has an upright alif, two more uprights (lam), and a final ـه 'ha' 29/60

Third, the end of the word “Allah” does not actually appear in the artifact; instead, it’s part of what Larsson imagines might have existed beyond the edges of the frayed fragment we have today. Larsson is looking at the pattern that’s visible and extrapolating what may have been beyond it, as part of her attempt to reconstruct what the artifact might have looked like in full.

There is a small triangular shape, but no final ha ـه. Frag. was published in 1938 by Agnes Geijer, original drawing looked like this: 31/60 pic.twitter.com/DxDossuWzs

But reconstruction drawing by @UU_University textile archaeologist Annika Larsson shows extensions on either side that include a ha. 32/60 pic.twitter.com/1NyQzcqDV2

These extensions practically double width of band. Not mentioned in press accounts: Larsson’s extensions are entirely conjectural. 33/60

This reconstruction is unfounded, according to textile expert Carolyn Priest-Dorman, who told me the artifact could not have extended farther out (to include the end of the word “Allah”) given how narrow its borders are: “Larsson’s saying the artifact was wider than it is.”
“She might be indulging in some fanciful readings that aren’t justified by the evidence,” agreed Paul Cobb, a professor of Islamic history at the University of Pennsylvania. He clarified that it’s already an established fact that the Viking world and Muslim world were closely integrated through trade and travel; he and other experts like Mulder and Priest-Dorman aren’t disputing that. They’re only disputing whether these specific burial clothes truly bear Arabic script.
“People want to see Arabic there, because it resonates today with a dream of a more inclusive Europe. There’s a real desire to document that Vikings had interactions, not to mention intermarriages, with many non-Vikings,” Cobb said. “That flies in the face of the white supremacists, who see Vikings as Nordic warriors defending Europe from foreign pollution, when nothing could be further from the truth. They were one of the great international societies of the Middle Ages.”
In fact, for Vikings, Arabic may have come with cultural cachet. They circulated coins bearing Arabic inscriptions, as well as weights for measuring silver bearing pseudo-Arabic inscriptions (writing that imitates the look of Arabic but doesn’t get it quite right). In a journal article for Current Swedish Archeology, scholar Lotta Fernstal writes that Vikings may have used the language to “‘spike’ certain objects with additional meaning” as part of constructing their self-image. “It seems likely that the Eastern, Oriental, Arabic and/or Islamic was alluring and desirable, perhaps as an ideal image of the ‘Other’ as part of a Viking Age Orientalism,” she adds.
Given this exoticizing attitude, Mulder said, it wouldn’t be surprising if Vikings were to have bought funeral clothes with Arabic inscriptions. “It would be like, for us, buying a perfume that says ‘Paris’ on it,” she told me. “Baghdad was the Paris of the 10th century. It was glamorous and exciting. For a Viking, this is what Arabic must have signaled: cosmopolitanism.”
Priest-Dorman added that it wouldn’t be unusual to find an eclectic mix of styles in the burial finery of a single Viking woman. “Everything beautiful goes with everything else beautiful—that is the Viking aesthetic.”
Still, the critics believe there isn’t yet enough evidence to support Larsson’s claim, and are concerned by how quickly her non-peer-reviewed findings went viral. “If stories like this are not fully fact-checked, white supremacists can then say, ‘Look, that wasn’t Arabic at all, the journalists are just pushing their PC agenda,’” Mulder said. Or as Cobb put it: “The story might well support my political views about Europe, but if it’s poorly documented, that makes it an easy target.”
In an email, Larsson indicated to me that additional details of her research are forthcoming. Another word that she says she discovered in the burial clothes—“Ali,” the name of the fourth caliph of Islam, revered especially by Shia Muslims—does not appear in the same artifact that purportedly bears the name of God. “Ali is not depicted in this ribbon. It is to be found on other ribbons that I’m working with and that is to be published in a coming work.”
Responding to critics who say that the burial clothes say “lllah” and not “Allah,” Larsson wrote, “If it is another word, it is still Kufic … that’s interesting.” She does not agree with the experts who say that there’s a dating issue with that claim and that “lllah” is a senseless jumble of letters.
“The meaning of research is to open questions,” Larsson added. “This discovery opens new questions.”
That, at least, is certainly true. Perhaps unusually for questions in medieval archeology, these questions feed directly into a contemporary heated political debate. The answers and debunkings of those answers are sure to be used as fodder by the left and the right alike.


In between its silly chatrooms and custom emojis, Slack is a place where real work gets done. But in some offices—no offense—the projects managed on the messaging platform are way cooler than others. Some even have cosmic significance.
On August 17, observatories in the United States and Italy detected gravitational waves, forces that bend the fabric of the universe, as they washed over Earth. Space telescopes observed a short gamma-ray burst, a powerful beam of radiation, coming from about the same part of the sky about two seconds later. Astronomers around the world quickly jumped into action, mobilizing dozens of ground-based and space telescopes to search for the source of these mysterious events.
There were calls and emails and, in the case of a few scientists who work for the University of California, Santa Cruz, and their associates, there were Slack messages. Ryan Foley, an astronomer at the university, was in Copenhagen when the alert went out. He started giving orders at 8:51 a.m. Pacific Time, then jumped on a bike and headed to his office at the city’s Dark Cosmology Center, where Dave Coulter and his fellow astronomers were already working.
A few hours later, they would find the source of the mysterious observations: a massive collision of neutron stars in a galaxy 130 million light-years from Earth. The discovery, announced Monday, would produce the first-ever image of a cosmic merger powerful enough to generate gravitational waves, spawn scientific reports from some 3,500 scientists, and open a new chapter in astronomy.
But Foley, Coulter, and their colleagues didn’t know that yet.
The team drew up a list of galaxies in the region of sky where the near-simultaneous events were detected and started looking for the source. Screenshots of their messages, which they published along with their paper in Science, show a cascade of real-time coordination and information. Several people were typing. (The full exchange starts on page 26 here.) The astronomers searched images from the Carnegie Institution’s Swope telescope in Chile, looking for a “transient,” an object that hadn’t been observed in the data before.
The first few images didn’t turn up anything, said Charles Kilpatrick, a postdoctoral researcher at UC Santa Cruz, who was in California at the time.
Then, in their ninth photo, they found something.
There it was, the spectacular afterglow of a cosmic explosion.
The Swope telescope was the first to capture the light from the merger. In the coming days, dozens of observatories would study the event at every wavelength of light, from gamma rays to radio waves.
Chat services like Slack have become ubiquitous in many offices in the last several years, tethering modern workers to their jobs in new ways that, by now, seem pretty standard. There’s nothing special these days about real-time, and often nonstop, communication among coworkers, whether they’re down the hall or across the pond. But there’s something particularly interesting, even mesmerizing, about watching extraordinary moments unfold in an ordinary space. When all is said and done, the participants can return to this strange day in their chat history and look at the exact moment when things started to change.
Michele Bannister, an astronomer at Queen’s University Belfast, has a similar record, from January 2016. Back then, Bannister and a small group of researchers, spread out in locations in Canada, the United States, France, and Taiwan, were working on the Outer Solar System Origins Survey, a search for objects beyond the orbit of Neptune. Bannister’s colleague was processing images from the Canada-France-Hawaii Telescope in Hawaii when he noticed something unusual and pinged her on Flowdock, a group-chat client.
Bannister headed to J.J. Kavelaars’s office, which was two floors down from hers, and examined the picture. There was a tiny spot of bright light. “We found ourselves our very own dwarf planet,” Bannister said. The dwarf planet, they eventually determined, is among the 20 largest worlds that have been discovered so far beyond Neptune.
Bannister said she enjoyed seeing the messages that preceded UC Santa Cruz’s big find. Before they knew what they had, the chat seemed to resemble just another day at the office.
“It’s so much a typical workday for all of us,” she said. “And most of the time, it’s something that you expect to get in your data. And every so often, it’ll be something like, oh, okay. That’s fun.”


“Imagining the future is a kind of nostalgia.” This is a line from John Green’s young-adult book Looking for Alaska. It’s pretty, and melancholy, and very popular on Tumblr. It’s also scientifically accurate.
Imagining the future is a kind of nostalgia, because humans predict what the future will be like by using their memories. This is how things you do over and over again become routine. For example, you know generally what your day will be like at the office tomorrow based on what your day at the office was like today, and all the other days you’ve spent there. But memory also helps people predict what it will be like to do things they haven’t done before.
Say that you are imagining your future wedding (if you’ve never gotten married before). You probably see it as a scene—at a church, or on the beach, or under a wooded canopy in a forest with the bridal party all wearing elf ears. There are flowers, or twinkling lights, or mason jars everywhere. You can envision the guests, how they might look, what your soon-to-be spouse is wearing, what look they have on their face. All of these details come from your memory—of weddings you’ve been to before, as well as weddings you’ve seen depicted in pop culture, or in photo albums. The scene also relies on your memory of your friends and family.
“When somebody’s preparing for a date with someone they’ve never been on a date with before, or a job interview—these situations where we don’t have past experience, that’s where we think this ability to imagine the future really matters,” says Karl Szpunar, a professor of psychology at the University of Illinois at Chicago. People “can take bits and pieces, like who’s going to be there, where it’s going to be, and try to put all that together into a novel simulation of events.”
The first clue that memory and imagining the future might go hand in hand came from amnesia patients. When they lost their pasts, it seemed, they lost their futures as well. This was the case with the famous patient known by his initials, “H.M.” H.M. had epilepsy, and to treat it, he received an experimental surgery in 1953 that removed several portions of his brain, including almost his entire hippocampus, which is a vital brain structure for memory. After the surgery, H.M. had severe amnesia, and also appeared to struggle with the future. A researcher once asked H.M., “What do you think you’ll do tomorrow?” He replied, “Whatever is beneficial.”

Since then, functional MRI scans have allowed researchers to determine that many of the same brain structures are indeed involved in both remembering and forecasting. In a study Szpunar did, he and his colleagues looked at activity in the brain’s default network, which includes the hippocampus as well as regions that involve processing personal information, spatial navigation, and sensory information. They found that activity in many of these regions was “almost completely overlapping” when people remembered and imagined future events, Szpunar says.
Researchers are still trying to pin down exactly how different brain regions are involved in these processes, but much of it has to do with the construction of scenes. You can remember facts, sure, and you can make purely informational predictions—“We will have jet packs by 2050”—but often, when you remember, you are reliving a scene from your memory. You have a mental map of the space; you can “hear” what’s being said and “smell” smells and “taste” flavors; you can feel your emotions from that moment anew. Similarly, when you imagine something you might experience in the future, you are essentially “pre-living” that scene. And just as memories are more detailed the more recent they are, imagined future scenes are more detailed the nearer in the future they are.
“It takes so much cognitive effort to come up with detailed simulations,” Szpunar says. “So it’s like, why would you spend all that time when it’s not going to happen for 30 or 40 years? Whereas if it’s something happening this weekend, and you’re like ‘How’s this date going to go?’—those things, people just anguish over them and really come up with these detailed simulations.”
When people try to imagine the more distant future—that classic interview question, “Where do you see yourself in 10 years?”—they tend to rely heavily on something called a cultural life script. This is the progression of events that a life in a certain culture is expected to contain. In much of the West, the cultural life script is something like: go to school, move out of your parents’ house, get one or more college degrees, find a job, fall in love, get married, buy a house, have kids, retire, have grandchildren, die. Not everyone expects their life to contain all of those events, but they’re aware of those milestones and will generally tell their life story using them as a framework. The further into the future you try to imagine, the more unknowns there are, so people reach for these events.
“We can’t really imagine or think that far into the future, and we can’t remember that far back, if we don’t have this cultural life script as a kind of skeleton for our life story,” says Annette Bohn, a professor of psychology at Aarhus University in Denmark. In studies Bohn has done with adolescents, their conception of a script seemed to develop in parallel with their ability to remember the past and imagine the future. (At the other end of the life course, older people’s ability to imagine the future declines in tandem with their memory.)
It’s not hard to see how this ability to imagine the future gives humans an evolutionary advantage. If you can plan for the future, you’re more likely to survive it. But there’s are limitations as well. Your accumulated experiences—and your cultural life script—are the only building blocks you have to construct a vision of the future. This can make it hard to expect the unexpected, and it means people often expect the future to be more like the past, or the present, than it will be.
In a similar vein, people tend to underestimate how much their feelings and desires will change over time. Even though they know that their personalities have changed a lot in the past, they have a tendency to think that the person they are now is the person they will be forever. This applies more broadly, too. You can see it in the technological advances imagined in science fiction. As my colleague Adrienne LaFrance wrote, while Back to the Future II (made in 1989 and set in 2015) made a lot of canny predictions—it got videoconferencing and drones right—it also thought people would still be using pay phones and fax machines. Which makes sense, given how ubiquitous those technologies were at the time the film was made.
There’s also an “optimistic, extreme positivity bias toward the future,” Bohn says. To the point that people “always say future events are more important to their identity and life story than the past events.” Talk about being nostalgic for the future.
But it might help people temper their expectations if they keep in mind that even though they can dream up detailed, novel scenes of things yet to come, their imagined futures are really just projections of their pasts. The future holds more surprises—and, potentially, more disappointments—than we might predict.


The deadliest and most destructive spate of fires in California’s modern history continues to burn. The string of massive fires in the Napa and Sonoma Valleys have little precedent: They have killed 40 people, destroyed more than 5,700 structures, and incinerated more than 200,000 acres of land. The Washington Post reports that hundreds of people remain missing.
“This is truly one of the greatest tragedies California has ever faced,” said Jerry Brown, the state’s governor.
The fires come amid a rash of unusual weather disasters across the world: Hurricanes Harvey, Irma, and Maria; a landfalling superstorm in Ireland; and deadly wildfires from British Columbia to the Iberian peninsula. 2017 is almost certain to be the second-warmest year ever measured, second only to an El Niño–intensified 2016.
Northern California suffered through a record-breaking heat wave earlier this year. In late August, Napa Valley saw day after day of temperatures above 100 degrees Fahrenheit. Those hot days are blamed in part for desiccating the region’s foliage, essentially preparing the area for this month’s wildfires.
But the Napa-Sonoma wildfires have surprised experts for reasons other than their size or intensity. Driven by hot, dry winds from the state’s interior, the North Bay blazes resemble Southern California’s wildfires, which often burst into life in the Santa Ana winds. Those winds, like the gusts whipping the north, are also hot, dry, and east-to-west. (And the subject of a dynamite Joan Didion essay, but that’s another story.)
In their entirety, the fires—from their speed to their wind-driven nature—suggest a much more southwest-like climate than the region today. Is this the future? Will climate change turn Northern California into Southern California?
“The comparison is not a bad one,” said Alex Hall, a professor of atmospheric science and the director of the Center for Climate Science at the University of California, Los Angeles.
It might be literally true for temperature. If greenhouse-gas emissions continue on roughly their current trajectory, then Northern California’s temperatures will warm by between 6 and 11 degrees Fahrenheit by 2100.
“That’s approximately the current temperature difference between Southern and Northern California on the coast,” says Hall. “The warming you would expect by the end of the century would be equivalent to a shift in latitude.”
He added that it’s not clear whether wind-driven fires will increase in Northern California in a warmed world. In Southern California, researchers predict that the amount of area burned by wildfires will double through the end of the century, as the Santa Ana winds become even drier.
Michael Wehner, a senior staff scientist at the Lawrence Berkeley National Laboratory, said that while the north’s future temperatures might resemble Los Angeles, its future precipitation would not. “Precipitation in Northern California will not likely decrease to Southern California levels. Future precipitation in NorCal will undergo a phase change, i.e., a larger fraction in the Sierra will fall as rain instead of snow due to warmer temperatures,” he told me in an email.
Some climate models suggest that storm systems will also make landfall further north in California as the century goes on, bringing more precipitation to the state’s north half and Oregon. But Wehner cautioned: “This is yet to be documented in the observational record however, whereas increases in temperature, decreases in snowpack, and drier conditions all have to varying degrees.”
“It is likely an oversimplification to state that future NorCal landscapes become SoCal current landscapes. It would likely be something completely different than either,” he added.
There’s some precedent for swapping the two regions’ temperature profiles—though it last happened in the other direction. When glaciers covered the top third of North America 26,000 years ago, the climate of modern-day Los Angeles was 6 to 11 degrees Fahrenheit cooler—comparable, in other words, to San Francisco.
California as a whole is projected to be drier and hotter in the decades to come. The U.S. government projects the Sonoran, Mojave, and Great Basin deserts to expand as climate change continues to take hold. Since water evaporates faster into hotter air, it also estimates that the Sierra Nevadan snowpack will shrink and as a whole California’s water problems will get worse.
Outside of California, there’s widespread evidence that global warming has already made the West’s wildfires more intense and more expansive. As I wrote last month, a study in Proceedings of the National Academy of Sciences last year found that the size of the area burned by wildfires has doubled over the past 33 years. And the area of forest burned annually in the Pacific Northwest has increased by 5,000 percent since the early 1970s.


The origin of humanity’s best friend is still murky, but here’s a likely scenario. In the distant past, wolves started skulking around human campsites, scavenging the carcasses left behind by hunters. Perhaps, to make the most of these opportunities, they evolved to be more docile. They transformed in both physique and temperament, and eventually became domestic dogs.
As they changed, they gained the ability to interact with humans, becoming increasingly attuned to our gestures, expressions, and emotions. But they lost something, too.
Sarah Marshall-Pescini, from the University of Vienna, has now found that dogs are terrible at a simple task that requires them to cooperate. Working together, they have to pull on two pieces of rope to bring a piece of distant food within reach—and they mostly fail. Wolves, however, are far more successful, dramatically outperforming their domestic peers.
This simple experiment contradicts the common belief that dogs are just friendlier wolves. “The idea is that we’ve changed their psychology to make them into super-cooperative beings,” says Marshall-Pescini. But that’s only true for their relationships with us. By domesticating dogs (or rather, providing the conditions for them to domesticate themselves), humans ruined the pack instinct that makes wolves some of the most gregarious and cooperative hunters on four legs. “They adapted to the niche we provided for them and it changed their sociality,” Marshall-Pescini says.
“The assumption that dogs are a tamer version of wolves is an oversimplification,” says Anindita Bhadra, from the Indian Institute of Science Education and Research in Kolkata. “This is all the more reason why we need to carry out similar studies on free-ranging dogs.”
Around 80 percent of dogs, in fact, are free-ranging, and their behavior shows just how different they are to wolves. They’re mostly solitary, scavenging alone on human garbage. When they do form packs, these groups are usually small and loose-knit. They might hunt together, but they mostly congregate to defend their territory. By contrast, wolves live in extremely tight-knit family groups. They rely on their pack-mates to bring down large prey, and they work together to rear each other’s pups. The strength of the pack is the wolf, and the strength of the wolf is the pack, as Rudyard Kipling’s poem goes.
Fifteen wolves currently live in Vienna’s Wolf-Science Center, along with 15 dogs. “The center was established to look at the differences between wolves and dogs in as fair a way as possible,” says Marshall-Pescini. “They’re raised in exactly the same way, with a lot of human contact. This allows us to test a lot of different things without the confounding variables of wolves not being used to humans and pet dogs being super-used to humans.”
She and her colleagues challenged their canines to a simple task, which other scientists have used on all kinds of brainy animals—chimps, monkeys, parrots, ravens, and even elephants. There’s a food-bearing tray that lies on the other side of their cage, tempting and inaccessible. A string is threaded through rings on the tray, and both of its ends lie within reach of the animals. If an individual grabs an end and pulls, it would just yank the string out and end up with a mouthful of fibers—not food. But if two animals pull on the ends together, the tray slides close, and they get to eat.
All in all, the dogs did terribly. Just one out of eight pairs managed to pull the tray across, and only once out of dozens of trials. By contrast, five out of seven wolf pairs succeeded, on anywhere between 3 and 56 percent of their attempts. Even after the team trained the animals, the dogs still failed, and the wolves still outshone them. “We imagined that we would find some differences but we didn’t expect them to be quite so strong,” Marshall-Pescini says.
It’s not that the dogs were uninterested: They explored the strings as frequently as the wolves did. But the wolves would explore the apparatus together—biting, pawing, scratching, and eventually pulling on it. The dogs did not. They tolerated each other’s presence, but they were much less likely to engage with the task at the same time, which is why they almost never succeeded.
“The dogs are really trying to avoid conflict over what they see as a resource,” says Marshall-Pescini. “This is what we found in food-sharing studies, where the dominant animal would take the food and the subordinate wouldn’t even try to approach. With wolves, there’s a lot of arguing and it sounds aggressive, but they end up sharing. They have really different strategies in situations of potential conflict. [With the dogs], you see that if you avoid the other individual, you avoid conflict, but you can’t cooperate either.”
“Amazingly, no one had ever studied whether carnivores could solve this type of cooperative task, and it’s fun to see that the wolves coordinated,” says Brian Hare from Duke University, who studies dog behavior and the influence of domestication. He has argued that during the domestication process, dogs began using their traditional inherited mental skills with a new social partner: humans.
Simultaneously, dogs perhaps became less attentive to each other, adds Marshall-Pescini. After all, wolves need to work together to kill large prey, and sharing food helps to keep their social bonds intact. But when they started scavenging on human refuse, they could feed themselves on smaller portions by working alone. If they encountered another forager, “maybe the best strategy was to continue searching rather than to get into conflict with another dog,” she says.
But dogs can be trained. When owners raise dogs in the same household, and train them not to fight over resources, the animals start to tolerate each other, and unlock their ancient wolflike skills. This might be why, in 2014, Ljerka Ostojić, from the University of Cambridge, found that pet dogs, which had been trained in search and rescue, had no trouble with the string-pulling task that flummoxed Marshall-Pescini’s dogs.
“It speaks to the fact that living among other dogs, without interaction with humans, is arguably less natural for dogs—as if domestication both refined attention, coordination, and even pro-sociality between species, and weakened social skills within the species,” says Alexandra Horowitz, who studies dog cognition at Barnard College. “A pack of dogs living together, without human intervention, is impaired compared to dogs living with humans.”
But Laurie Santos, from Yale University, says that there’s growing evidence that dogs also differ from wolves in their physical understanding of the world. “Maybe dogs are just as good at cooperating as wolves, but they don’t get that the only way to solve the task is to pull at the same time,” she says. “They just don’t get it in terms of the physics. The cool things is that both of these interpretations have exciting implications for the evolution of dog intelligence.”
“If I ask people to close their eyes and think of a dog, everyone thinks of a pet dog,” Marshall-Pescini says. “But pet dogs are a really recent invention and free-ranging dogs are more representative of the earlier stages of domestication. We need to base our theories on a different understanding of what a dog is.”


The 19th-century American scientist Joseph Leidy has been described as the “last man who knew everything.” An extraordinary polymath, Leidy was a scholar of parasites, a discoverer of dinosaurs, a collector of gemstones, a curator of museums, an exceptional illustrator, and the first person to use a microscope to solve a murder mystery. But learned though he was, he was still shocked by what we saw when he cut open some termites in an attempt to find out what they ate.
Gazing at the dissected insects through his microscope, Leidy saw hordes of small specks evacuating their corpses, like “a multitude of persons from the door of a crowded meetinghouse,” he wrote. He billed them as parasites, and for good reason. It was 1889, 30 years after the publication of Charles Darwin’s provocative opus, On the Origin of Species by Means of Natural Selection, and biologists had begun to see nature as a red-blooded gladiatorial arena, where only the fittest survive. And in previous decades, scientists had shown that many infamous diseases, like gonorrhea and tuberculosis, were the work of microscopic bacteria. With nature framed in terms of conflict, and bacteria framed as germs, any microbes living in the bodies of animals were instantly vilified as parasites.
We now know that this view is deeply incorrect. Every individual animal is a thriving community of microbes, most of which are harmless, and many of which are beneficial. These microbiomes bestow their owners with amazing abilities.
Those of termites, for example, help them to digest their food. The ones that Leidy saw are protists—microbes that have more in common with us than with bacteria, but that still consist of a single, tiny cell. They produce enzymes that help termites to digest the otherwise indigestible chemicals in the wood that they eat, to survive on a rich and abundant food source that would otherwise be inaccessible to them.
You can learn more about that microbiome in the video below—the first in a series of online films produced by HHMI Tangled Bank Studios, which adapt the stories in my book, I Contain Multitudes.

There are different kinds of termites, too. The misleadingly named lower termites rely on protists as partners in digestion. The higher termites, which evolved later, rely more on bacteria, which they house in a series of cowlike stomachs. And the macrotermites—the most recent of these groups—rely on both architecture and agriculture. They feed bits of wood to a fungus, which breaks them down and creates a compost that the termites can swallow. In their guts, bacteria digest the wood pulp even further.
This is true for every member of a macrotermite colony except the huge and grossly swollen queen. Her distended body lacks gut microbes, but she can eat because her daughters regurgitate predigested fluids into her mouth. She has effectively turned her entire nest—towering, fungus-laced walls; thousands of diligent laboring daughters, and their billions of attendant microbes—into her gut.
Even the microbes of termites can have their own microbiomes. One species of Australian termite carries a protist in its gut called Mixotricha paradoxa—a pear-shaped microbe whose name means “paradoxical being with mixed-up hairs.” Those “hairs,” which propel the microbe through the gut, are actually bacteria—and close relatives of those that cause syphilis. Lynn Margulis, one of the most vocal proponents of the interconnectedness of life, showed that Mixotricha carries around no fewer than four types of bacteria—one inside itself, and three on its surface. As she and her son once wrote:
“Scrutinizing any organism at the microscopic level is like moving ever closer to a pointillist painting by Georges Seurat: The seemingly solid figures of humans, dogs, and trees, on close inspection, turn out to be made up of innumerable tiny dots and dashes, each with its own attributes of color, density, and form.”


Some 130 million years ago, in another galaxy, two neutron stars spiraled closer and closer together until they smashed into each other in spectacular fashion. The violent collision produced gravitational waves, cosmic ripples powerful enough to stretch and squeeze the fabric of the universe. There was a brief flash of light a million trillion times as bright as the sun, and then a hot cloud of radioactive debris. The afterglow hung for several days, shifting from bright blue to dull red as the ejected material cooled in the emptiness of space.
Astronomers detected the aftermath of the merger on Earth on August 17. For the first time, they could see the source of universe-warping forces Albert Einstein predicted a century ago. Unlike with black-hole collisions, they had visible proof, and it looked like a bright jewel in the night sky.
But the merger of two neutron stars is more than fireworks. It’s a factory.
Using infrared telescopes, astronomers studied the spectra—the chemical composition of cosmic objects—of the collision and found that the plume ejected by the merger contained a host of newly formed heavy chemical elements, including gold, silver, platinum, and others. Scientists estimate the amount of cosmic bling totals about 10,000 Earth-masses of heavy elements.
Neutron stars are the collapsed cores of dead stars, the sole survivors of supernovae. They are the densest known objects in the universe; one neutron star measures about the size of a bustling city, but has about the same mass as our sun. A teaspoon of its contents would weigh about 10 million tons.
When neutron stars merge, they release a fire hose of neutrons. Heated to extreme temperatures, the neutrons bombard surrounding atoms, and form heavy elements. The baby elements go on to become part of other objects of the universe, like stars and planets, including our own.
The discovery confirms a long-standing astronomical theory. Astronomers have suspected for decades that neutron-star mergers were responsible for the production of most of the heavy elements found in the universe. The lightest of the elements, like hydrogen, helium, and lithium, came from the Big Bang. Heavier elements, like carbon and oxygen, came later, fused in the hearts of stars. Some even heavier elements erupted from supernovae. But computer simulations showed these explosions weren’t powerful enough to forge some of the elements that are heavier than iron, like the precious metals. The universe needed another kind of explosion called a kilonova, which shines 1,000 times brighter than a typical supernova.
Until this summer, astronomers only had theoretical models for such an event. Now, they say the new data suggests neutron-star mergers could account for about half of all elements heavier than iron in the universe.
“I think this can prove our idea that most of these elements are made in neutron-star mergers,” said Enrico Ramirez-Ruiz, a theoretical astrophysicist at the University of California, Santa Cruz, who worked on the discovery, in a press release Monday. “We are seeing the heavy elements like gold and platinum being made in real time.”
Scientists will spend years poring over the data from this discovery. We’re just getting to know these cosmic collisions and the science behind them. But we have been long acquainted with their effects. The aftermath of neutron-star mergers is all around us, in our mines, computers, toasters, wedding bands—the list goes on.


In September of 2015, astronomers detected, for the first time, gravitational waves, cosmic ripples that distort the very fabric of space and time. They came from a violent merger of two black holes somewhere in the universe, more than a billion light-years away from Earth. Astronomers observed the phenomenon again in December, and then again in November 2016, and then again in August of this year. The discoveries confirmed a century-old prediction by Albert Einstein, earned a Nobel prize, and ushered in a new field of astronomy.
But while astronomers could observe the effects of the waves in the sensitive instruments built to detect them, they couldn’t see the source. Black holes, as their name suggests, don’t emit any light. To directly observe the origin of gravitational waves, astronomers needed a different kind of collision to send the ripples Earth’s way. This summer, they finally got it.
Scientists announced Monday they have observed gravitational waves for the fifth time—and they’ve seen the light from the cosmic crash that produced them. The waves came from the collision of two neutron stars in a galaxy called NGC 4993, located about 130 million light-years from Earth.
Neutron stars are strange, mysterious objects, the collapsed cores of stars that exploded in spectacular fashion—supernovae—and died. These stars measure about the size of a metropolitan city, but have about the same mass as our sun. Astronomers had long predicted that when two neutron stars collide, the resulting explosion would produce electromagnetic radiation, in the form of optical light. The afterglow would shine bright enough to be seen through powerful telescopes, the first visible proof of a source of gravitational waves, provided the latter could also be detected.
Here it is, captured by the European Southern Observatory’s Very Large Telescope in Chile, in the center of the image:
Astronomers made the observation August 17. Three gravitational-wave detectors, two at the Nobel prize–winning Laser Interferometer Gravitational-Wave Observatory (LIGO) in the United States, and one at the Virgo Interferometer in Italy, detected the cosmic ripples as they washed over Earth. About two seconds later, two space telescopes—NASA’s Fermi Gamma-ray Space Telescope and ESA’s International Gamma-Ray Astrophysics Laboratory—observed a short burst of gamma rays, the most energetic wave in the electromagnetic spectrum, coming from the same part of the sky.
The almost simultaneous detections caught astronomers’ attention, and they threw everything they had at it. Dozens of ground-based telescopes around the world quickly turned their gaze toward the same slice of sky. ESO’s army of telescopes, sprinkled across the Chilean desert, scanned through the night. When the sun set in Hawaii, the Pan-STARRS and Subaru telescopes joined in. So did space observatories like Hubble. Within hours, astronomers pinpointed the location of the collision using an ESO telescope that sees in infrared wavelengths. They aimed the Swope Telescope, also in Chile, at the region and started snapping pictures. They found the afterglow in their ninth shot.
Astronomers observed the afterglow of the merger for days. They watched as the glowing orb faded and changed colors from blue to red, a tell-tale sign that the remnants of the crash were pushing radioactive material out and cooling down.
Here’s an animation from ESO that shows two neutron stars spiraling closer together until they crash:

Astronomers examined the gravitational waves to estimate the size of the colliding objects and found they had masses far smaller than black holes. “The biggest neutron star is a lot smaller than the smallest black hole,” said Richard O’Shaughnessy, a theoretical gravitational-wave astrophysicist at Rochester Institute of Technology who works in the LIGO group. The mass measurement, coupled with the near-simultaneous observations of the gravitational waves and a light source, told scientists they were dealing with neutron stars. The event was also much closer to Earth than previous mergers recorded by LIGO, which originated between 1 billion and 3 billion light-years away.
All told, about 70 observatories captured the event, named GW170817 for the day it made itself known to Earth. The collision’s aftermath was recorded at nearly every wavelength. O’Shaughnessy described the discovery as a Rosetta stone for astronomy; the observation produced reams of data with richness seemingly unprecedented for a single astronomical event. The findings, which are spread across many papers in several journals, provide evidence for several theories in astronomy.
The discovery supports the theory that neutron-star collisions produce short gamma-ray bursts, brief streams of light that shine brighter than a million trillion times the sun. Gamma-ray bursts have been detected and imaged before, but without gravitational-wave detectors like LIGO and Virgo, astronomers couldn’t know whether they came from cosmic collisions.
The presence of the short gamma ray-burst suggests the merger led to a kilonova, a powerful explosion 1,000 times brighter than a supernova. Astronomers have long suspected kilonovae follow neutron-star collisions, spewing material out into space. In the case of GW170817, scientists estimate the kilonova ejected material at one-fifth the speed of light, faster than a typical supernova.
The findings support another prediction that neutron-star collisions produce chemical elements heavier than iron, like gold and platinum. Astronomers believe neutrons released during the merger combine with surrounding atoms in a phenomenon known as r-process nucleosynthesis. Telescope observations of GW170817’s spectra—the chemical composition of the star material—revealed it contained heavy elements, including 10 times the mass of the Earth in gold, according to O’Shaughnessy. These kinds of collisions, astronomers believe, may be responsible for populating the universe with heavy elements.
The discovery gave scientists a chance to measure the expansion of the universe, too. Since astronomers knew which galaxy the latest gravitational waves came from, they could calculate the distance between that galaxy and Earth and then plug it into equations for the rate of expansion, known as the Hubble constant. Good news: The answer matched up with previous estimates from other methods.
When scientists announced their fourth detection of gravitational waves in August, they promised that these kinds of announcements would become routine. LIGO and Virgo’s instruments, they predicted, will detect the rippling of space-time once or multiple times a week. It’s a certainty that we will experience the effects of mergers between black holes and neutron stars—and maybe between one of each—again. LIGO and Virgo scientists may even have a few confirmed detections they haven’t told us about yet. And the more, the better.
“This rain of events will continue at such a high rate that we’ll have a census of comic explosions,” O’Shaughnessy said. “And by data mining the census, we can learn something about how they form, about the origins of these mysterious events.”


The Madras Observatory offers little to the visitor’s eye. Stone slabs and broken pillars lie ignored in a fenced-off section of a local weather center in the southern Indian city of Chennai. Few tourists venture out to see the ruins of the 18th-century complex. On the other side of the subcontinent, in northern Indian cities such as New Delhi, Varanasi, and Jaipur, remains of the Jantar Mantars, vast astronomical stations, are far more popular attractions. Built in the same century as the Madras Observatory, their stark geometric structures, with looming proportions and vibrant colors, make for mandatory stops on travelers’ itineraries. Yet it is the Madras Observatory, and not the spectacular Jantar Mantars, that marks the triumphal fusion of scientific knowledge and imperial power.
South Asians had been studying the heavens long before the 18th century. The subcontinent’s first texts on astronomical phenomena date back more than 3,000 years. As was common throughout the ancient world, observations about the movements of stars and planets often served the needs of astrologers and priests. Nevertheless, they formed an impressive body of scientific knowledge, one that was further enriched by contact with other cultures. The Islamic conquest of South Asia in the medieval era brought Persian and Arab discoveries along with it, and the Mughal Empire promoted a blend of South Asian and Islamic astronomical knowledge in the 16th and 17th centuries. The city of Lahore, in modern-day Pakistan, became a center in the production of sophisticated astronomical instruments such as celestial spheres. By the early 18th century, as Mughal rulers lost control of most of the subcontinent, local rulers used astronomy to promote their own authority. They built the flamboyant Jantar Mantars across northern India to show that, just like the great dynasties before them, they too were patrons of knowledge.
The era’s greatest promoter of astronomy was Jai Singh II, the 18th-century raja of Jaipur. He oversaw the construction of monumental observatories across his domains, using them not only to overawe subjects but also to gather useful knowledge about the lands he ruled. His Jantar Mantars, like others in South Asia, featured massive sundials, sextants, and other instruments of observation, but lacked telescopes, which had been invented in Europe a century before. Eager to capitalize on European knowledge, and to show the global reach of his influence, Jai Singh II came into contact with French missionary scientists.
A team of Jesuit astronomers arrived at Jaipur in 1734, and demonstrated the practical value of their scientific advances. By establishing the exact time that the sun was at its highest over a given spot, the missionaries could determine its longitude, or distance east or west of other points on the Earth’s surface. They established the longitude of several of Jai Singh II’s cities, just as other Jesuit teams were doing for the Qing emperors in China. Offering their astronomical knowledge to Asian rulers, these Catholic missionaries hoped to win approval for their Christian faith, while the rulers they served used foreign expertise to increase their own power. The Jesuits also learned from South Asian science, studying Sanskrit, the classical language of science in South Asia, in order to translate the greatest works of South Asian astronomy.
This peaceful exchange of scientific patronage, technology, and texts between Europe and Asia was short-lived. After the raja’s death in 1743, scientific activity in his network of observatories faded, and Jaipur’s collaboration with the Jesuits came to an end. New forces entered the fray, as both the subcontinent and astronomy became arenas for the rising empires of Britain and France. Throughout the second half of the 18th century, as the two rival powers fought for control of North America, they also competed against each other in South Asia, staging proxy wars through networks of local allies. They also competed to gather scientific data, sending rival astronomical expeditions across their far-flung empires and using the knowledge gained to control their colonies. While only a few generations before, it might have seemed that the global circulation of astronomical knowledge would bring a new era of understanding between Europe and Asia, this was not to be.
In 1792, the British East India Company delivered a stinging defeat to Tipu Sultan of Mysore, France’s only remaining ally in South Asia. In the same year, it completed the construction of the Madras Observatory, one of the first modern observatories in Asia. It was armed with impressive telescopes, still rare in the Indian subcontinent. The observatory was the brainchild of Michael Topping, a British surveyor tasked with mapping the shoreline of southern India. He argued that an observatory was crucial to his task, since astronomy was the “parent and nurse of navigation.” But the site was also a tool of colonial rule, a means of showing that Britain was now the dominant power in South Asia. As Topping insisted, astronomy held the key to “the sovereignty of a rich and extensive empire.”
The East India Company destroyed what remained of Tipu Sultan’s power in 1799, when Tipu himself died in a desperate last battle at his capital of Srirangapatna. Most of his sultanate was annexed by the Company, which soon began an extensive survey of his former dominion. Fanning out from the Madras Observatory, British surveyors used it as a fixed location from which they could calculate the exact location of sites in Mysore. This was a first step to assessing the value of the lands for tax purposes, and bringing the region under direct British control, where it would remain for the next century and a half. The observatories of Jai Singh II, symbols of his kingdom’s independence and cosmopolitan collaboration with European science, were a thing of the past. Alongside other massive British projects of scientific knowledge collection, such as James Cook’s expeditions to the Pacific (1768–1778), the Madras Observatory heralded the rise of a new kind of science, serving the needs of a global empire and imposing its sway on subject peoples.
This post appears courtesy of Aeon.



Frances Glessner Lee grew up in the Gilded Age as the heiress to a fortune made in industrial farm equipment. Her childhood was “sheltered and indulged.” She would marry at age 20, have three children, and then, in a turn against convention, divorce her husband.
It was in her 40s—free of a husband and then free of a brother and father who both died, leaving her a vast fortune—that Lee embarked on the project that would consume the rest of her life. She had become enthralled by the grisly crime stories of George Burgess Magrath, her brother’s friend and a medical examiner in Boston. And so Lee began pouring her family fortune into a project that combined the very unladylike world of crime with the domestic arts: the Nutshell Studies of Unexplained Death.
The nutshells are dioramas, based on actual death scenes that Lee painstakingly researched. (“Convict the guilty, clear the innocent, and find the truth in a nutshell” went a police saying at the time.) Lee finished 20 nutshells before her death in 1962. Eighteen are still used today by Maryland’s Office of the Chief Medical Examiner to train detectives to look for clues, one was destroyed in transit, and the last “lost” nutshell has been recovered from an attic and restored for an upcoming exhibit of Lee’s work at the Renwick Gallery of the Smithsonian American Art Museum in Washington, D.C.
Lee was fastidious—to the point of obsession. In one nutshell titled “Saloon and Jail,” a man lies facedown in the street. Debris is strewn on the pavement: miniature cigarettes (hand-rolled and filled with paper), a banana peel (painted leather), scraps of paper with visible faces. A storefront in the background displays newspapers and magazines with real covers from the date of the man’s death. A bucket of tiny, colorful lollipops sits under the magazines, each piece of candy individually wrapped in cellophane.
The story goes that at one point Lee requested the carpenter who built the wooden structures and furniture in the nutshells remake a certain rocking chair: She wanted it to rock the exact same number of times as the rocking chair in the real-life scene. Lee also made use of factory-made dollhouse pieces, like the boxes of Ivory soap that show up on the pantries of various nutshells. But the textiles—from clothing on dolls to upholstery on couches—she hand-sewed all by herself. After her death, people found half-finished doll clothing that Lee was knitting with pink yarn and straight pins.
For Lee, the nutshells were more than just objects for display. They were part of a campaign to bring rigor to forensic science—a campaign, ultimately, for justice. She funded the Department of Legal Medicine at Harvard Medical School, with her friend George Burgess Magrath as its first professor. And she hosted seminars using the nutshells to teach police how to examine crime scenes.
As a high-society woman, Lee knew how to throw memorable parties to woo men who might not be so inclined to listen to a middle-aged woman. At the end of her seminars, she wined and dined them at the Ritz Carlton. The parties featured china specially made for the occasion. And she left them with a parting gift—a miniature nutshell that opened to reveal a pair of miniature, working cufflinks.
“People didn’t know where to put her,” says Nora Atkinson, the Smithsonian curator who organized the Lee exhibit. Lee was a woman in a man’s world, who had succeeded by marshaling her talents in the traditionally feminine pursuits. She won respect in professional circles. At one point, New Hampshire made her an honorary police captain. But she was still seen first as a “grandmother” with murder as a “hobby.”
Lee knew that to be taken seriously, her nutshells had to be more than just meticulously crafted. They had to be scientifically accurate. She bought porcelain doll heads and other parts, but she made sure to fashion their bodies according to real biology. “You can’t buy a doll in rigor mortis,” says Ariel O’Connor, a Smithsonian conservator who has been working on the nutshells. In one nutshell, a woman has fallen into a position with an unusually stiff neck—a sign she may not have died in this position. Some dolls show lividity, which is when blood in the body sinks to whatever part of the body is facing down, turning the skin purplish red. It can also hint at whether a body has been moved. The dolls dead by hanging were filled with lead shot, to give them the heft of real human bodies.
For all the care in creating them, the 80-year-old nutshells are showing their age now. Tablecloths have faded. The blood on the bodies, which O’Connor suspects to be red nail polish, has darkened to a less realistic deep purple.
To prepare the nutshells for exhibition at the Smithsonian, O’Connor has led a months-long effort to conserve the dioramas. The work, she says, has been a bit like detective work itself. She looks for subtle hints like glue residue to match up, say, an object that has fallen off of the shelf where it originally stood. Because any object out of place can in fact be an important clue to the murder, every move requires consultation with Maryland’s medical examiners, who have the solutions to the nutshells.
“It is one of the most challenging things I have ever tried to work on,” says O’Connor. The sheer number of materials in the nutshells add to the challenge. Materials have “inherent vice,” or a natural tendency to degrade over time. The plastic panes of the nutshells’ windows, for example, are especially prone to warping. O’Connor also had to figure out how to preserve a nutshell that features a burnt house. (Lee literally had a blowtorch taken to it.) She ended up borrowing techniques from archaeology used to preserve burnt wood.
A big part of the conservation project has focused on lighting. Lee used the incandescent bulbs available in her time, which cast a beautiful warm glow but have the disadvantage of heat. Heat causes damage. The painted linoleum in one nutshell had cracked and curled, in part, O’Connor thinks, because of the heat of the lightbulb above it.
To be in the Smithsonian exhibit, the lights would have to be on hours each day, which would have caused too much damage. So the team replaced 70 incandescent bulbs with custom-made LEDs, matching the quality of light without the heat. It’s possible now because of recent advances in LED technology. “Just a few years ago, we couldn’t have achieved this quality with LEDs,” says Scott Rosenfeld, a lighting designer at the Smithsonian. The 80-year-old dioramas now have a state-of-the-art lighting system.
The trickiest nutshell to restore was the lost nutshell, which features a dead man on his couch. The solution is completely lost. To make matters worse, when it was taken out of the attic where it was found, it was tipped over, displacing many of the objects Lee had carefully placed inside as clues. There was dirt all over the living-room floor. The bannister for the stairs had fallen down, which led some to suspect it was the murder weapon. But then O’Connor discovered that the bannister is supposed to be intact. They glued it back on.
When I stopped by the unfinished exhibit, O’Connor and Atkinson were still discussing where to place an object that had come loose and they suspected was a key clue. (I’m being purposely vague to avoid spoilers. I will say, you would have to look very, very carefully to even notice.) I asked O’Connor if she had solved the death after spending so many hours with the lost nutshell. She laughed and demurred. Visitors to the Smithsonian’s Renwick Gallery will have the opportunity to test their own detective skills.
The Renwick Gallery specifically showcases craft and decorative arts. Atkinson told me she was particularly pleased to exhibit Lee’s nutshells because museums have traditionally focused on studio craft, which has been so dominated by the work of white men. Half a century after her death, Lee is still transgressing in the world of men.


WASHINGTON, D.C.—If the United States is going to have even a chance of meeting its climate-change goals under the Paris Agreement, there are few actions more pressing than taking coal plants offline. Coal-burning power plants emit more heat-trapping carbon dioxide into the atmosphere than any other part of the electricity sector.
It’s been widely reported that coal is inexorably on its way out—that cheap natural gas, made available by fracking, is eroding coal’s dominance in the U.S. grid. But the speed of that transition matters immensely to the planet, as the United States is currently not on track to slow its emissions fast enough to avert catastrophic climate change.
This week, two powerful men set up on Capitol Hill to fight over the speed of that transition. One of them vied to keep up the pressure on coal retirements in a press conference on Wednesday; the other threw his support behind still-surviving plants in a congressional hearing on Thursday. The two were inhabiting unusual roles: The environmentalist billionaire said he was fighting for the free market, while the long-time Republican and former Texas governor asked for emergency government subsidies.
The first of the men is Michael Bloomberg, the former mayor of New York and the UN special envoy on climate change and cities. In this latter role, Bloomberg has put together an “America’s Pledge” from cities, states, companies, and universities who will continue to track their greenhouse-gas emissions under the Paris Agreement, even though Donald Trump has removed the federal government from that accord.
Bloomberg, one of the world’s richest men, is also a philanthropist—and Wednesday, he gave an additional $30 million to the Sierra Club’s Beyond Coal campaign. The Beyond Coal campaign is a targeted activism organization that uses public pressure and environmental litigation to force coal plants to close across the country.
The Beyond Coal campaign is widely considered one of the most successful environmental activist pushes of the post–Cold War period. In the past decade, it has closed or secured the retirement of nearly half of the country’s 523 coal plants. Eleven of those plants have announced plans to shutter since President Donald Trump’s inauguration.
“Coal’s share of the energy mix is at a record low,” said Michael Brune, the executive director of the Sierra Club, at the press conference. “In fact, coal plants are retiring at the exact same rate under President Trump as they were under President Obama.”
Bloomberg’s funding will keep the Beyond Coal campaign running until 2020. He also announced $34 million to support the League of Conservation Voters and job-retraining and economic-development programs in Appalachia.
“The war on coal has never been led by Washington,” said Bloomberg. “It has been led by market forces that are providing cleaner and cheaper sources of energy, and by communities, cities, and states that want to protect public health.”
The second of the men is Secretary of Energy Rick Perry. In a fractious age, in a polarized country, Perry has pulled off a rare feat: He has proposed an emergency policy that just about everyone hates—except, that is, for the coal and nuclear industry.
On Thursday, Perry sat before the House Energy and Commerce Committee and defended his “grid reliability” plan, which his department is trying to hustle through the Federal Energy Regulatory Commission, or FERC, in the next 60 days. FERC creates the rules that govern some of the nation’s power grid.
Perry’s proposed rule would pay coal and nuclear plants to remain on the electrical grid in the name of resilience, financially rewarding them for keeping three-month supplies of fuel on premises.
Experts say it would virtually assure that coal and nuclear plants—nearly all of which have been squeezed by the availability of cheaper natural gas and solar and wind power—remain profitable. It could also create a brief boom in the coal market, as plants snap up three months of fuel.
To build support for the rule, Perry has cited the polar vortex of 2014, when frigid weather descended across much of the continental United States. Though the power grid stayed intact through that episode, the largest grid operators saw roughly 20 percent of their power plants shut down. Natural gas plants, in particular, suffered in the weather, but some coal stacks froze as well.
Some experts also worry that as solar and wind power play a larger role in the electricity mix, power generation will become more unreliable, as the sun doesn’t shine and the wind doesn’t blow all the time. (Many technology companies are working on large batteries that could solve this problem.) Combined with the historical anecdote that natural-gas plants sometimes shut down in the cold, Perry says the grid is suffering a “reliability crisis.”
Some economists have worried that the proposed rule could spike rates for electricity customers. The energy-consulting firm ICF has estimated it will cost power customers $800 million to $3.8 billion every year through 2030. Asked about those concerns Wednesday, Perry replied: “I think you take costs into account, but what’s the cost of freedom? What’s the cost to keep America free? I’m not sure I want to leave that up to the free market.”
Perry has ordered FERC to make a decision about his proposed rule in the next 60 days. But when pressed by lawmakers, he said he was just “starting a conversation” about grid resiliency.
His plan has found barely any support from either side of the political spectrum. Multiple Republicans declined to endorse the plan Thursday. Congressman Greg Walden, a Republican of Oregon and the chairman of the House subcommittee on energy, didn’t even mention the plan in his statement introducing Perry.
A large and bizarre alliance of environmental groups, renewable-energy advocates, and natural-gas suppliers oppose the plan and will try to block it in court, according to the energy trade publication E&E News.
Economic experts from both the right and the left have also opposed the plan. Josiah Neeley, the director of energy policy at the conservative R Street Institute, said he found Perry’s remarks at the hearing “a little strange.”
“Secretary Perry didn’t sound very much like Governor Perry that I remember back here in Texas, because Governor Perry, of course, was a big fan of free markets in electricity,” he told me.
The R Street Institute has been “very critical” of the proposal, he said, which he said was also “pretty vague.”
“There’s certainly not a reliability crisis. And even if there was, the proposed rule doesn’t address any of the issues with reliability that are out there,” he said. “To the extent that there is an issue here with reliability, the principled way to deal with it is to create some sort of market product ... and not have the Department of Energy pick winners and losers.”
Those thoughts were echoed by Michael Greenstone, a professor of economics at the University of Chicago and a former chief economist to the White House Council of Advisers under President Barack Obama.
“The first question you always want to ask yourself is, is there some imperfection in the market that needs solving?” he said.
The biggest market failure in energy markets is climate change, he told me: Power plants can emit carbon dioxide for free, even though it’s a greenhouse gas that worsens climate change and imposes high indirect costs on people today. Therefore, it should be more expensive to release greenhouse gases into the atmosphere, he said, not cheaper. “Doing nothing is already not merited by economics,” he said. “This is like doubling down.”
Greenstone agreed with Neeley that insofar as there is a potential failure to plan for grid reliability, it should be solved with market mechanisms. He didn’t think it constituted a crisis either, though.
“In many respects it reminds me of what we just saw with the Clean Power Plan,” he said, referring to the landmark Obama climate policy that the Environmental Protection Agency repealed this week. Environmental advocates and public-health groups have alleged that the math used to justify repealing the Clean Power Plan distorts the well-documented benefits of clean-air regulation.
Greenstone continued: “The high-level political goal is set—and then poor Secretary Perry and poor [EPA] Administrator Pruitt, having received high-level political direction, have to do the plumbing to make it work. And I think this is kind of ugly plumbing precisely because it does not efficiently address any of the two potential market failures floating around—and in fact makes one of them worse.”
“Coal’s real problem, nuclear’s real problem, and—by the way—renewable’s real problem is natural gas. We’re swimming in it,” he said.
As for Bloomberg’s opinion of the plan, he did not make his views a secret. “The Trump administration wants taxpayers to pay more for an earlier death,” he told journalists Wednesday. “This is one of the worst ideas Washington has ever come up with, which is saying a lot.”


As always, a horror film managed to express the idea before the scientists ever could, and in better, more visceral terms. “The television screen,” the haunting image of Brian O’Blivion tells us in David Cronenberg’s 1983 classic Videodrome, “is the retina of the mind's eye. Therefore, the television screen is part of the physical structure of the brain.” So far, so much media theory: secondhand McLuhan, thirdhand Baudrillard. It’s what happens next that’s interesting.
Our hero, wilting under the caustic nihilism of the video age, finds that strange things start happening not to his mind, but his body. A howling cavern opens up in his stomach, rimmed by grisly pulsing labial folds. It eats weapons. His hand sprouts metallic screws, driving into his wrist, locking his gun into a hand that swells into a grotesque of formless and seeping flesh. He is told to kill, and he kills. It’s not that his mind has been invaded. It just exists beyond itself; it now contains endless shelves of video tapes. This is, somehow, obscurely, us; this monstrous body is our own.
Among philosophers, biologists, and cognitive scientists, this nightmare is an exciting new field of study, known as embodied or extended cognition: broadly, the theory that what we think of as brain processes can take place outside of the brain. In some cases, this isn’t a particularly radical idea. The octopus, for instance, has a bizarre and miraculous mind, sometimes inside its brain, sometimes extending beyond it in sucker-tipped trails. Neurons are spread throughout its body; the creature has more of them in its arms than in its brain itself. It’s possible that each arm might be, to some extent, an independently thinking creature, all of which are collapsed into an octopean superconsciousness in times of danger. Embodied cognition, though, tells us that we’re all more octopus-like than we realize. Our minds are not like the floating conceptual “I” imagined by Descartes. We’re always thinking with, and inseparable from, our bodies.
The body codes how the brain works, more than the brain controls the body. When we walk—whether taking a pleasant afternoon stroll, or storming off in tears, or trying to sneak into a stranger’s house late at night, with intentions that seem to have exploded into our minds from some distant elsewhere—the brain might be choosing where each foot lands, but the way in which it does so is always constrained by the shape of our legs. We can’t ever stalk like a creature with triple-jointed legs, or sulk in the dejected crawl of a millipede, or stride with a giraffe’s airy gangly indifference. The way in which the brain approaches the task of walking is already coded by the physical layout of the body—and as such, wouldn’t it make sense to think of the body as being part of our decision-making apparatus? The mind is not simply the brain, as a generation of biological reductionists, clearing out the old wreckage of what had once been the soul, once insisted. It’s not a kind of software being run on the logical-processing unit of the brain. It’s bigger, and richer, and grosser, in every sense. It has joints and sinews. The rarefied rational mind sweats and shits; this body, this mound of eventually rotting flesh, is really you.
That’s embodied cognition. Extended cognition is stranger.
Many years ago, when I found myself standing on the roof of a tall building, or on a platform of the London Underground, or by the banks of the river, I would feel a strange urge to throw myself off. Not because I was miserable or because I particularly wanted at that moment to die; it was like an itch, an obsessive-compulsive tic, the deep gravitational hunger of the death drive. I would visualize myself falling, stupidly and fatally, for no reason other than to indulge in the most pointless destruction. I would almost savor the feeling of being hemmed in on all sides by ordinary life—commuters on the platform, tourists gawping at the Thames—and at the same time, right on the edge of the void, the domain of gods or nothingness or both. Maybe you’ve felt the same urge. But I don’t get it any more. These days, I still sometimes feel that cold vertiginous breath down my neck, but it’s not my body I want to throw down into the void. It’s my phone. Isn’t the phone, now, part of the physical structure of the brain?
In 1998, 15 years after Videodrome, the philosophers and cognitive scientists Andy Clark and David J. Chalmers finally made sense of what was happening in their landmark paper “The Extended Mind.” The mind, they argue, has no reason to stop at the edges of the body, hemmed in by skin, flapping open and closed with mouths and anuses.
Recent studies have added weight to their provocation: It’s been shown that spiders can use their webs to process and store information, essentially “outsourcing” mental processes to physical structures. Why is it, Clark and Chalmers ask, that mentally rearranging Scrabble tiles is considered a “part of action” rather than a “part of thought”?
When we jot something down—a shopping list, maybe—on a piece of paper, aren’t we in effect remembering it outside our heads? Most of all, isn’t language itself something that’s always external to the individual mind? We can’t invent our own private languages; as Wittgenstein showed in his Philosophical Investigations, we can invent our own words for things, but only as substitutes for words that already exist; it’s impossible to make the incommunicable meaningful. Language sits hazy in the world, a symbolic and intersubjective ether, but at the same time it forms the substance of our thought and the structure of our understanding. Isn’t language thinking for us?
This is not, entirely, a new idea. Plato, in his Phaedrus, is hesitant or even afraid of writing, precisely because it’s a kind of artificial memory, a hypomnesis. (Incidentally Freud inverts the metaphor 2,000 years later: The unconscious mind is like a child’s toy, the Mystic Writing Pad.) Writing, for Plato, is a pharmakon, a “remedy” for forgetfulness, but if taken in too strong a dose it becomes a poison: A person no longer remembers things for themselves; it’s the text that remembers, with an unholy autonomy. The same criticisms are now commonly made of smartphones. Not much changes.
Most of all, though, a theory similar to extended cognition is present in the work of Hegel and his descendants—and, in particular, Marx. In the dialectical tradition, the hermetic and self-contained Cartesian consciousness is impossible: We only become conscious in and through the world and its history. Marx, in the Economic and Philosophic Manuscripts of 1844, describes the process of unalienated labor in familiar terms. “The object of labor is, therefore, the objectification of man’s species life: for he ... contemplates himself in a world that he has created.” Work, without ownership or scarcity, is a kind of play: You’re always turning the exterior world into something else, something more responsive to your needs and your being. In a liberated future, the world of objects can be an externalization of our own consciousness; it can be a true home for humanity, because it is already ourselves. But not yet; first we have to overthrow capitalism. In the 20th century, Theodor Adorno picks up this theme: The “separation between subject and object” exists—I am not the world around me, in fact for the most part I’m terrified by it while it’s monstrously indifferent to me—but this is “the result of a coercive historical process.” It wasn’t always this way, it doesn’t have to be forever. The difference is that, according to theories of extended cognition, this separation is already over and always was, that subject and object are united right now.
But not entirely. Extended cognition promises to rip up the idea of a mind that lives only in the furrows of the brain, but it doesn’t always follow through. Cognition is extended, outsourced, leaking from cranial slime into the material world—but like an octopus’s tentacle, it can always dart back in. There are stranger and more dangerous possibilities. Take the grocery list. For Clark and Chalmers, it’s a brain process—information storage and retrieval—offloaded onto a piece of paper. But by whom? In Limited Inc, Jacques Derrida uses the same object to construct a very different interpretation. “At the very moment ‘I’ make a shopping list,” he writes, “I know that it will only be a list if it implies my absence, if it already detaches itself from me in order to function beyond my ‘present’ act and if it is utilizable at another time, in the absence of my being-present-now.” The list will still do its cognitive work if you are not currently reading it. The list will still do its work if you are dead. If we can accept that a grocery list is in some way thinking, is the part of the mental apparatus that remains lodged in the human brain really so central? The thought-capacity of objects is indifferent to whichever bit of brain is plugged into it. A war memorial remembers its list of the dead for us, in the same way that a scrap of paper remembers milk, and it keeps remembering, long after the weeds have grown and the rest of the world has tumbled past caring.
In Molloy, Samuel Beckett’s strange and gorgeous thicket of a novel, a long section sees the titular character sheltering by a beach, trying to work out a system for passing sucking-stones between his various pockets and his mouth, so that he never sucks on the same stone twice. Fantastic methods are devised, new ways of ordering the world: stones moving around by fours across the ordered and Ptolemaic universe of his coat; stones moving singly in postmodern disorder. Is it Molloy who is thinking here, or the system, the dynamic complex of pockets and stones? This passage has attracted a fair amount of attention from philosophers, who tend to see it as either a parody of logical systems in general or a form of thought beyond reason. Deleuze and Guattari, in Anti-Oedipus, see in it the model of a new kind of reasoning: schizoid, unbodied, and diffuse. Molloy’s circulation of pebbles is a “complete machine,” one in which “the mouth, too, plays a role as a stone-sucking machine.”
We’re used to thinking of active machines as digital machines; when we talk about the possibility that unliving things might think, we mean computers. We might be very shortsighted. All the processes we attribute to brains and computers alone might fill the world. In the same way that the legs code the program of walking, unknown information is inscribed in the patterns of grains of sand as the wind tosses them on an empty beach; the frenetic interconnections of the internet and the spoken world are thrumming in a field of grass. The thinking machine thinks; it has its processes and its functions. And the world of inert objects might think too, in slow and strange ways which we can only borrow for a moment, and which disappear again into what sounds like silence.


Imagine if a word in a book—say, bubble—had the ability to magically copy itself, and paste those copies elsewhere in the text. Eventually, you might bubble end up bubble bubble with bubble bubble bubble sentences bubble bubble bubble bubble like these.
This is exactly what happens in our genomes. There are genes known as retrotransposons that can copy themselves and paste the duplicates in other parts of our DNA, creating large tracts of repetitive gobbledygook. Around half of the human genome consists of these jumping genes. And a quarter of a cow’s DNA consists of one particular jumping gene, known as BovB. It, and its descendants, have bloated out the cow genome with thousands of copies of themselves.
This jumping gene seems to have entered the cow genome from the unlikeliest of sources: snakes and lizards.
Retrotransposons typically jump around within a single genome, but sometimes they can travel further afield. Through means that scientists still don’t fully understand, they can leave the DNA of one species and enter that of another. And so it is with BovB. No one knows the animal in which it originated. But from that mystery source, it has jumped into the DNA of snakes and cows, elephants and butterflies, ants and rhinos.
David Adelson, from the University of Adelaide, charted the gene’s travels in 2013 by comparing the subtly different versions of BovB in dozens of animals. That was when his team showed that BovB in cows and other cud-chewing mammals is most similar to the versions in pythons and vipers—and likely descended from them. Now, Adelson’s colleague Atma Ivancevic has extended the search for BovB to more than 500 animal species. And her results show that the gene’s travels are even more erratic than anyone thought.
Genes change over time, and closely related species have more similar versions than distantly related ones. So, if you compare different versions of the same gene across a range of animals, you can usually create a family tree that shows how they’re all related. But that only works for genes that are inherited in the usual vertical way, from parent to child.
When Ivancevic did the same exercise for BovB, which jumps between species horizontally, she got one of the weirdest family trees I’ve ever seen. It’s like a window into a bizarre parallel universe where sheep are more closely related to cobras than they are to elephants, where kangaroos have more in common with bedbugs than with horses, and where pythons, zebrafish, leeches, scorpions, and sea urchins all belong to the same tight-knit family. “It was just bizarre,” Ivancevic says.
She estimated that BovB has repeatedly jumped between the genomes of distantly related animals on at least 11 occasions, and likely many more. In some cases, when it arrived in a new lineage, it ran amok: Cows, sheep, and elephants all have thousands of copies in their DNA. In other groups, like bats and horses, it kept its head down, and produced just a few dozen copies.
No one knows how BovB travels between species, but Ivancevic and Adelson suspect that it might spread via blood-sucking parasites. They have found strong similarities between the BovB versions in leeches and zebrafish, bedbugs and snakes, ticks and lizards. By biting different hosts, parasites might help jumping genes to vault over the species barrier.
This idea makes sense in principle, but it’s hard to imagine in practice. What parasite, for example, could possibly have transferred BovB between, say, a sea urchin and a rattlesnake?
We should be careful before making too much of similarities between BovB in different species, argues Sarah Schaack, from Reed College. “Let’s say some worm DNA was picked up by a virus, that virus began infecting a bird, and the DNA was transferred,” she says. “Subsequently, a tick that specialized on the bird picked up some of the viral DNA. If you only sequenced the worm and the tick, you would infer the worm got the DNA from the tick or vice versa, which might be appealing because ticks are parasites, but would be strange because ticks don’t bite worms.” When genes go on the move, it can be hard to track their footprints accurately.
Ivancevic agrees, and wants to sequence BovB in many more species, parasites included. “I feel like the [patterns we have] look really sporadic because we don’t have all the intermediate species,” she says. By looking at more animals, it may become easier to trace the gene’s history.
That was certainly the case when Ivancevic turned her attention to a different retrotransposon called L1. This jumping gene is more directly relevant to humans because it fills up 17 percent of our genome. Most copies are now broken and stationary, but some have kept their ability to move around—and their presence has been linked to diseases like schizophrenia and cancer.
L1 exists in the genomes of almost every mammal, and was presumably around in the DNA of our common ancestor. And while it can jump around any given genome, scientists believed that, unlike BovB, it doesn’t move between species.
Not so, says Ivancevic. She identified at least three possible cases in which L1 seems to have jumped between major animal groups, typically those that live in water. And she showed that it’s completely absent from the DNA of platypuses and echidnas, which means that it must have jumped into mammalian genomes after these oddities had split off from the main dynasty, between 160 and 191 million years ago.
“There’s not yet a smoking gun for this transfer, like the identification of the donor species,” says Edward Chuong, soon to be at the University of Colorado Boulder. Still, “their failure to find any trace of L1 in [platypuses and echidnas] is fairly strong evidence supporting their claim.”
“These [jumping genes] really seem to be good at moving between genomes,” says Alexander Suh, from Uppsala University. But why, he asks, are some genes like BovB more likely to do so than L1, or at least more likely to establish themselves in a new setting? Possibly, Ivancevic says, it’s because BovB is about half the size of L1 and so is easier to move. Maybe it’s because BovB hitches a ride on parasites, and L1 doesn’t.
Whatever the reason, both genes—and others like them—must surely have influenced the evolution of their hosts. Jumping genes were first discovered by the pioneering scientist Barbara McClintock in the 1940s, but it took decades for people to realize how common they are—and how influential. They could potentially cause diseases by jumping into the wrong place and disrupting vital genes. But they could also provide raw material for evolution, by reshaping or rewiring existing genes. In this way, they spurred the evolution of the placenta, and supercharged our immune systems.
“It’s mind-boggling to think about how just a few [jumps] have fundamentally altered the course of our evolution,” says Chuong. Evolutionary biologists like to wonder what would happen if we replayed the tape of life—if we went back to some earlier point in history and let evolution run its course again. Would history repeat itself? Chuong thinks not. These jumps are so unpredictable, but so potentially important when they happen, that it’s hard to imagine the events unfolding twice.
And what would happen if we ran the tape forward? Where will genes like BovB end up next, and how will they shape the destinies of their hosts? “It would be interesting to see what BovB looks like in a few million years, but I’ll probably not get a chance to do that,” says Ivancevic.


It’s a story about mystery, grime, and a phoenix rising from the ashes—so of course it started in Chicago.
The Field Museum, a Greco-Roman citadel of natural history on the shores of Lake Michigan, is famous for its Egyptology exhibit and for displaying Sue, the largest T. rex ever discovered. But among scientists and biologists, it carries a different distinction. In its basement and archives, the Field Museum holds more than 3 million specimens: drawers and drawers full of the stuffed corpses of long-dead creatures.
Among these dead are hundreds of Illinoisan birds, many of them captured and killed in the decades after the city’s founding. For years, curators and collection managers had noticed that something was off about some of them. Birds from the late 19th century were noticeably darker than other specimens of their own species from other time periods or other locations. They suspected the birds had been stained by the soot that once choked Chicago’s skies—but without looking at the birds more closely, they couldn’t be sure.
Their drabber colors remained a mystery until a few years ago, when two University of Chicago graduate students took an interest in the birds. In 2014, Carl Fuldner, a historian of photography, and Shane DuBay, an evolutionary biologist who studies alpine birds, secured a grant to photograph some specimens in the Field Museum’s collection.
Using a scanning electron microscope, they confirmed that the wings were covered in black carbon. Black carbon is a tiny waste product of coal combustion. It’s of great note to scientists: Black carbon contributes to global warming, but it’s also a potent form of toxic air pollution. A particle of black carbon is so small that it can penetrate deep into the lungs’ alveoli.
Now Fuldner and DuBay have brought their work further. They have photographed more than 1,300 birds from museum collections across the Midwest. Then they used the birds’ relative drabness to reconstruct a 130-year history of black-carbon pollution in the region surrounding the Great Lakes. Their results were published this week in the Proceedings of the National Academy of Sciences.
How Atmospheric Black Carbon Has Changed Over Time
“The birds are an incidental record—we’re using the birds as the tool, the resource, that captures the direct environmental sample from the past,” said DuBay.
Getting them to give up that data required some creativity. The pair photographed the birds in the Field Museum before realizing that their setup was imperfect, and that it allowed different amounts of light to leak onto the birds with every exposure. They had to go image all the birds again using a light box designed for commercial photography.
The pair also photographed birds at the Carnegie Museum of Natural History in Pittsburgh and the University of Michigan Museum of Zoology in Ann Arbor, Michigan.
They also used a piece of software called RawDigger to access the unedited brightness data from the photographs. This let them treat the digital camera’s output as an objective data set, rather than a subjective image. “We’re using the entire photographic apparatus as a kind of reflectance sensor,” said Fuldner.
But the technique worked. It largely matches earlier attempts to estimate the amount of coal burned in the region. “Seeing how coal consumption dropped right after the Great Depression began—and seeing that signal pop up really strongly in our dataset—really validated [this] in our minds,” said DuBay.
Their graph provides evidence that emissions rose during the late 19th century, previously the least understood period for North American emissions. The research also contained some geography-specific treats: Six of the 10 sootiest birds came from Joliet, Illinois, which held the country’s second-largest steel refinery for decades. Fuldner was even able to dig up old photographs of Joliet in the 1890s, its sky nearly black with smoke from the refinery.
“Who would’ve thought that people could mine the birds we collected over the last 150 years and get a climate record from it?” said Joe McConnell, a professor at the Desert Research Institute who was not connected to the study. His 2007 study of ice cores from Greenland also estimates the amount of black carbon released from North America over the last 200 years and lines up well with this new finding.
He said that the study, especially in its spatial precision, was important confirmation of what was already known about black-carbon concentrations during the late 19th century. He lamented that DuBay and Fuldner had not extended their record before 1880. “It would have been interesting to see if bird-feather records declined to preindustrial levels if you go back another 30 years,” he told me.
Fuldner and DuBay’s study is not the first to plumb the depths of the county’s natural-history collections. As my colleague Ed Yong has written, biologists have just started to use their contents to reveal entirely new species. And recently, a team of researchers at Princeton University used similar collections of dead plants to learn how flora had moved north and uphill in response to climate change.
The pair say they hope to continue their work in other parts of the world with deep collections and a long industrial history. (The United Kingdom comes to mind.)
“One of the things that has really excited me about this project is it’s more about the prospect of these collections being used by atmospheric scientists,” said DuBay. “This is really just the tip of the iceberg for other studies like this.”


Few human traits are more variable, more obvious, and more historically divisive than the color of our skin. And yet, for all its social and scientific importance, we know very little about how our genes influence its pigment. What we do know comes almost entirely from studying people of European descent.
To Sarah Tishkoff, a geneticist at the University of Pennsylvania, that’s a ridiculous state of affairs. “It gives you a very incomplete perspective,” she says.
To redress that imbalance, Tishkoff and her team looked to Africa—the continent where humanity is at its most physically and genetically diverse. They recruited 1,570 volunteers from 10 ethnic groups in Ethiopia, Tanzania, and Botswana, and measured the amount of the dark pigment melanin in the skin of their inner arms. Then the team looked at more than 4 million spots in the volunteers’ genomes where DNA can vary by a single letter, to identify which variations are associated with their skin color.
They found several, clustered around six specific genes: SLC24A5, MFSD12, DDB1, TMEM138, OCA2 and HERC2. And they showed that these variants collectively account for 29 percent of the variation in skin color in the three countries studied. That’s a big proportion! For comparison, a similar and much bigger study identified hundreds of genes that affect one’s height, but that collectively account for just 16 percent of the variation that you see in large populations.
Tishkoff says that her results complicate the traditional evolutionary story of human skin. In this view, humanity began with dark skin in Africa to protect against the harmful effects of the sun’s ultraviolet radiation. As people migrated to other continents, some groups evolved lighter skin, to more effectively produce vitamin D in areas where sunlight is scarce.
But most of the variants that Tishkoff’s team identified, for both light and dark skin, have an ancient African origin. They likely arose in hominids like Homo erectus long before the dawn of our own species, and have coexisted in balance for hundreds and thousands of years. In many cases, the older variant is responsible for lighter skin, not darker. That’s consistent with an idea from Nina Jablonski, an anthropologist from Pennsylvania State University, who thinks that the ancient ancestors of humans—much like other primates—had pale skin. “As our ancestors moved out of the forest and into the savannah, they lost their hair and evolved darker skin,” says Nick Crawford, a researcher in Tishkoff’s lab.
But that wasn’t an all-encompassing change. Different groups of people adapted to their own particular environments, not just around the world, but within Africa, too. “Africa is not some homogenous place where everyone has dark skin,” Tishkoff says. “There’s huge variation.” For example, her team’s measurements showed that the Nilotic peoples in eastern Africa have some of the darkest skin around, while the San of southern Africa have light skin, comparable to some East Asians.
This physical diversity is mirrored in these groups’ genes. The first gene identified as affecting human skin color—MC1R—is very diverse in European populations but remarkably similar across African ones. Based on that pattern, says Tishkoff, some geneticists have concluded that the evolutionary pressure for dark skin in Africa is so strong that any genetic variants that altered skin color were ruthlessly weeded out by natural selection. “That’s not true,” says Tishkoff—but it’s what happens when you only study skin color in Western countries. “When you look at this African-centered perspective, there’s a lot of variation.”
For example, a gene called MFSD12 has variants that are linked to darker skin; these are common in dark-skinned people from East Africa, but rare among the lighter-skinned San. MFSD12 also shows how the search for pigmentation genes can reveal new insights about the basic biology of our skin. Two years ago, the gene didn’t even have a name, but it was linked to vitiligo—a condition where people develop white patches on dark skin. By deleting the gene in fish and mice, Tishkoff’s colleagues confirmed that it controls the balance between light and dark pigments.
Another gene called SLC24A5 has a variant that has traditionally been seen as “European,” because it is so starkly associated with lighter skin in Western European populations. But Tishkoff’s team showed that the variant entered the East African gene pool from the Middle East several millennia ago and well before the era of colonization. Today, it is common in Ethiopian and Tanzanian groups, but rare in other areas.
Critically, in East African groups, the variant doesn’t lighten skin color to the same degree that it does in Europeans. It’s a stark reminder that “a person can carry a gene that confers a particular trait in one population and yet not obviously show evidence of that trait themselves,” says Jablonski. “It reminds us that we can’t be cavalier about stating that a particular crime suspect has a particular skin color based on the presence of a single genetic variant in their DNA.”
Sandra Beleza, from the University of Leicester, has done one of the only other genetic studies of skin color to include people of mixed African ancestry. She says that neither her work nor Tishkoff’s have come close to identifying all the genes behind this trait. Further studies, involving other African populations that haven’t been included in genetic studies yet, may help to plug that gap.
While many have used skin color as a means of dividing people, Tishkoff sees the potential for unity and connectedness. “One of the traits that most people would associate with race—skin color—is a terrible classifier,” she says. Even without supposedly “dark” skin, there is a lot of hidden variation. “The study really discredits the idea of a biological construct of race,” she adds. “There are no discrete boundaries between groups that are consistent with biological markers.”
Jedidiah Carlson from the University of Michigan, who has been keeping tabs on how white-supremacist groups misappropriate genetic studies, agrees. “Because visually distinguishable traits common in present-day Europeans, such as light skin color, are also assumed to have arisen within European populations, white supremacists treat these traits as a proxy for superior intelligence,” he says. The history of SLC24A5 reminds us that “light skin pigmentation, and likely other ‘European’ traits, are not unique to Europeans. Human populations have been interbreeding for as long as we have existed as a species.”
White-supremacist communities “often rally around the demonstrably false claim that Africans are more genetically similar to ancestral hominids than Europeans—and these results turn the tables,” Carlson adds. At several genes that influence skin pigments, “Europeans are actually more likely to be genetically similar to great apes.”


There’s a famous viral video in which a diver slowly swims up to a clump of rock and seaweed, only for part of that clump to turn white, open its eye, and jet away, squirting ink behind it. Few videos so dramatically illustrate an octopus’s mastery of camouflage. But ignore, if you can, the creature’s color, and focus on its texture. As its skin shifts from mottled brown to spectral white, it also goes from lumpy to smooth. In literally the blink of an eye, all those little bumps, spikes, and protuberances disappear.
The project was entirely funded by the U.S. Army Research Office—and it’s not hard to imagine why. There are obvious benefits to having materials that can adaptively hide the outlines of vehicles and robots by breaking up their outlines. But there are other applications beyond military ones, Shepherd says. It might cut down on shipping costs if you could deliver materials as flat sheets, and then readily transform them into three-dimensional shapes—like flat-pack furniture, but without the frustrating assembly. Or, as the roboticist Cecilia Laschi notes in a related commentary, biologists could use camouflaged robots to better spy on animals in their natural habitats.
“I don’t see this being implemented in any real application for quite some time,” says Shepherd. Instead, he mainly wants to learn more about how octopuses themselves work, by attempting to duplicate their biological feats with synthetic materials. “I’m just a big nerd who likes biology,” he says.
Octopuses change their texture using small regions in their skin known as papillae. In these structures, muscle fibers run in a spiderweb pattern, with both radial spokes and concentric circles. When these fibers contract, they draw the soft tissue in the papillae towards the center. And since that tissue doesn’t compress very well, the only direction it can go is up. By arranging the muscle fibers in different patterns, the octopus can turn flat, two-dimensional skin into all manner of three-dimensional shapes, including round bumps, sharp spikes, and even branching structures.
Shepherd’s team—which includes the postdoc James Pikul and the octopus expert Roger Hanlon, who took the famous video at the start of this piece—designed their material to work in a similar way. In place of the octopus’s soft flesh, they used a stretchy silicone sheet. And in place of the muscles, they used a mesh of synthetic fibers that were laid down in concentric rings. Normally, the silicone membrane would balloon outward into a sphere when inflated. But the rings of fibers constrain it, limiting its ability to expand and forcing it to shoot upward instead.
By changing the layout of the fibers, the team could create structures that would inflate into various shapes, like round bumps and pointy cones. Pikul grabbed a stone from a local riverbed and programmed the material to mimic its contours. He set the material to create hierarchical shapes—lumps on lumps. He even programmed it to duplicate the more complicated contours of a field of stones, and a plant with spiraling leaves.
For the moment, the material can only be programmed to mimic one predetermined shape at a time. Still, “the results are impressive,” writes Laschi, and “represent a first step toward more general camouflage abilities.” Indeed, Shepherd is now adapting the material so it can transform more flexibly—just like a real octopus. For example, the team could replace the fixed mesh of fibers with rubber tubes, parts of which could be inflated or deflated at whim. That way, they could change which bits of the surface are flexible, to determine how it will eventually inflate.
Shepherd’s team is just one of many groups who are attempting to build soft robots, which eschew the traditional hard surfaces of most machines in favor of materials that are soft, bouncy, and floppy. Such bots would theoretically be better at navigating tough terrain, resisting shocks and injuries, and even caring for people. Often, these researchers use the octopus as an inspiration. Last year, Harvard researchers 3-D printed a soft, autonomous “octobot” that moved by burning small amounts of onboard fuel, and channeling the resulting gas into its arms. Laschi, meanwhile, has built a robot with soft floppy arms that can wiggle through the water.
The robots are certainly cool, but they’re nowhere near as versatile as the real deal. Shepherd’s material, for example, can change texture about as fast as an actual octopus, but it can only make one rough shape at a time. The animal, meanwhile, can produce far finer undulations in its skin, which are tuned to whatever it sees in its environment. For now, nothing we produce comes anywhere close.


In the wild, a predator that eats too much of its prey can drive that species toward extinction. But there are other, less understood influences that predators can have on their prey’s survival. Take, for instance, odor: New research shows that the very smell of predators may be enough to increase the chances of a whole population of animals going extinct. Fear alone, it suggests, can shape the fate of a species.
Traditional ecological theory holds that smaller populations of any creature will usually breed more productively than larger, denser populations, since individuals have less competition and more luck in mating, says Ryan Norris, a biologist at the University of Guelph. But when hungry predators hover around, small populations of prey may not be as insulated to stress as larger ones.
To test this, Norris and his colleagues recently conducted an experiment using fruit flies and one of the flies’ natural predators: praying mantises. They put a group of flies into a cage with a mantis underneath, in a separate partition. The fruit flies couldn’t see the mantis and it couldn’t attack them, but they could smell it. Days later, during the following breeding period, female fruit flies that had been in the cage laid fewer eggs than ones that had been placed in a separate cage with no mantis below. Those eggs subsequently hatched into insects that weighed less on average, too.
The researchers speculate that part of the reason females laid fewer eggs and had lighter offspring is because worrying about a lurking mantis takes away from the time fruit flies would normally spend foraging for food and having sex. Less food for fruit mothers could mean smaller eggs, though the study, which was published in Proceedings of the Royal Society B, didn’t specifically test to see whether a lack of food caused lighter offspring and less eggs. “I suspect they are eating less because they are being more vigilant of predators,” Norris says.
This only occurred with low-density populations, he notes; at high densities, other factors like competition probably overwhelms the effect of fear. But the researchers then found through modeling that when a mantis was around to strike fear into the hearts of low-density fruit-fly populations for several generations, the risk of extinction increased sevenfold over 10 generations. In the wild, the pressure predators put on a small population of prey could be even stronger, Norris says, since the models they ran didn’t include losses from direct attacks.
Justin Suraci, an ecologist at the University of California, Santa Cruz, says that Norris’s study is one of the few that approaches the effects of fear on the population level. As he sees it, it provides a break from the traditional view of predator-prey interactions, which only involves direct killing. “There’s some evidence that the fear effects outweigh the effects of actual killing and eating,” he says, adding that killing often only affects a single individual, while the fear of predators can affect entire groups.
These results aren’t limited to fruit flies. While laboratory tests using the insects may be particularly good at honing in on the specific factors that cause population declines, some researchers have identified similar effects on wild birds. A 2011 study exposed song sparrows to the sounds of predators like ravens, hawks, and raccoons, and found that just like the worried fruit flies, the birds laid fewer eggs, and fewer of the eggs produced birds that made it to adulthood.
Is It Possible to Be Scared to Death?
Suraci, too, conducted a study earlier this year to investigate how fear might affect predators themselves. He and his coauthors tracked pumas in the Santa Cruz Mountains via radio collars and determined when the big cats killed deer. Pumas often return to feed on their kills for several consecutive nights, so the scientists came in during the day and set up video-camera traps with speakers that played the sounds of humans—a large source of mortality for pumas—when triggered by motion around the deer kill. For comparison, other motion-triggered speakers were set up to play more natural sounds, like tree frogs.
The human sounds appeared to make the fierce mountain lions nervous. “They are much more likely to flee immediately and abandon their kill completely,” Suraci says.
The study only tracked the effect on individual pumas, so Suraci isn’t certain what disruptions like this would do at a population level. But the human fear factor clearly seems to drain cats’ energy levels, because they will waste more time on killing more deer and spend less time eating their fill.
Norris says that more research will have to be done to determine the extent of the effect that fear can have on small populations of prey, but it may be an important component in wildlife conservation. Government agencies and conservation groups spend an enormous amount of time and effort on wildlife-management plans, many of which are based on ecosystem models to predict the effect predators will have on a given prey population. If the mere sounds and smells of a predator are enough to offset the success of subsequent generations of animals, models that ignore the power of fear might be more likely to fail.
“[If] you make the wrong decision, you not only waste a lot of money, you risk the species going extinct,” Norris says.


Last week, a day after The New York Times reported many years’ worth of sexual-harassment allegations against film producer Harvey Weinstein, another alarming report appeared, this one in Science magazine. The setting was different—Antarctic research expeditions, not Hollywood—but the narrative was the same. A man, well aware of his position of power, had preyed on women in his field, and his behavior had gone unchecked for years.
According to Science, Boston University is investigating sexual-harassment allegations against David Marchant, an Antarctic geologist and now department chair at the school, brought by two women, his former graduate students. The women say Marchant verbally and physically harassed them during research expeditions in Antarctica two decades ago. Science writer Meredith Wadman reported that documents related to the case suggest Marchant denies the allegations.
For young scientists, research expeditions are important experiences and career builders. They are, in some ways, workplaces like any other in a male-dominated industry, which means they are not immune to sexual harassment. In a 2014 survey of field scientists, 64 percent of respondents said they had personally experienced sexual harassment during their work, and 20 percent reported they were sexually assaulted. But the nature of field work can amplify the damaging effects of sexual harassment, particularly at very remote sites, where there’s little to no communication to the outside world. The distance from reality can become both physical and emotional. The feeling of helplessness that comes with abuse is magnified. In the moment, victims may, quite literally, have nowhere to turn.
One set of allegations against Marchant came from a small expedition in Antarctica’s Beacon Valley, where people slept in unheated tents, traversed rugged terrain, and received supplies by helicopter, Science reported. For weeks, their only contact with others was a radio connection to a base station. Jane Willenbring, now an associate professor at the University of California, San Diego, alleges that during this trip Marchant, then her thesis adviser, called her a “slut” and a “whore” and urged her to have sex with his brother, who was with them. Willenbring said Marchant told her each day, “Today I’m going to make you cry.”
Another woman, whom Science does not name, alleges that during a different expedition in Antarctica, Marchant belittled her and called her a “bitch” repeatedly. “I began to believe the things he told me,” she wrote in a formal complaint.
In some cases, Marchant’s harassment was violent, Willenbring said. She alleges Marchant shoved her, threw rocks at her when she urinated in the field, and:
In another instance, Willenbring alleges in the complaint, Marchant declared it was “training time.” Excited that he might be about to teach her something, Willenbring allowed him to pour volcanic ash, which includes tiny shards of glass, into her hand. She had been troubled by ice blindness, caused by excessive ultraviolet light exposure, which sensitizes the eyes. She says she leaned in to observe, and Marchant blew the ash into her eyes. “He knew that glass shards hitting my already sensitive eyes would be really painful—and it was,” she writes.
The details of these allegations are shocking in their vulgarity. But the fact that they exist isn’t surprising at all, said Julienne Rutherford, a biological anthropologist at the University of Illinois at Chicago and one of the authors of the 2014 survey. Rutherford said the accounts of sexual harassment, in both the Weinstein and Marchant reports, read like a script, carefully constructed from similar stories over the years. She described it from the perspective of the alleged perpetrator.
“You identify the target as wanting something you control. You break them down to the point that they don’t trust themselves. You break them down to the point where their work suffers. And you isolate them to the point that they either don’t report, or when they do report, they’re told, wouldn’t it be better if you kept this to yourself?” Rutherford said. “It’s the same story over and over again, and it’s devastating every time.”
On a remote research expedition, there may be no option to report the harassment immediately. The person in charge might be the abuser. Witnesses to the abuse may feel powerless in the moment, perhaps fearful of making themselves targets.
The trauma of the harassment follows victims from the field to their homes and institutions. Avoiding their harassers may be difficult. According to a followup report from Rutherford and her 2014 coauthors on victims, published online Wednesday, “these interactions occurred on their university campuses, at conferences, or online, and a few targets of harassment received love letters even after repeatedly rebuffing the advances of their colleagues.”
Some victims fear career-ending retribution if they report the abuse. The anonymous woman in Science alleges Marchant threatened to keep her from getting research funding. Willenbring said she waited to file her complaint against Marchant until after she received tenure last year, fearing reprisal for describing the events of the field work. “It could be traumatizing to even look at the data” from a trip, said Robin Nelson, a biological anthropologist at Santa Clara University and Rutherford’s coauthor. Entire careers can be abandoned.
Then there are the fears of not being believed. “There still seems to be a fair amount of questioning of these victims, unless the cases are really egregious,” said Meredith Hastings, an associate professor at Brown University who is part of a national project to prevent sexual harassment in earth-sciences fields.
Reports of sexual harassment by male scientists, well-known and respected in their fields, have emerged in recent years in several areas of research, from astronomy to infectious disease, with many instances going unchecked for decades. The bad behavior is discussed quietly and discreetly in a whisper network, where women trade anecdotes and warnings about male advisers or professors or researchers. They exchange stories, advising their colleagues about who’s inappropriate, who’s handsy, and who should be avoided at all costs.
The recent spate of sexual-harassment stories will not be the last, in science and in Hollywood. Similar abuses have happened before, whether in a hotel room or a research camp, and they will happen again. The ones yet to come will prompt outrage, as these have, but they shouldn’t come as a surprise.
“This is common. This is not isolated behavior,” Rutherford said. “These are not a few bad apples. These are examples of systematic campaigns of abuse against junior people.”


In 2004, a New York Times reporter asked Stephen Hawking what his IQ was. “I have no idea,” the theoretical physicist replied. “People who boast about their IQ are losers.”
President Donald Trump seems to think otherwise. After recent reports that Rex Tillerson, his secretary of state, called him a moron, Trump told Forbes: “I think it’s fake news, but if he did that, I guess we’ll have to compare IQ tests. And I can tell you who is going to win.”
As Philip Bump at The Washington Post reported, Trump has a history of boasting about his IQ, and challenging others to IQ tests. His supporters have also taken up this cause for him in the past. In December 2016, a chart made the rounds saying that Trump’s IQ was 156, putting him above most past presidents. (The median score is 100.) The fact-checking website Snopes rated this claim as false: While the chart was based on a real study, the study didn’t have real IQ scores for most presidents (it estimated their IQs based on other factors), Trump wasn’t included in the study, and most importantly,“Donald Trump’s true intelligence quotient is unknown,” the article reads.
Scientists disagree on how useful IQ tests are as a measurement of intelligence. There’s research to show that IQ can change over the lifespan, for example. And some say that it doesn’t account for things like emotional intelligence or creativity. But “IQ” remains an easy shorthand for referring to intelligence, with the added bonus of sounding scientific, quantifiable, and official.
It isn’t totally surprising that Trump seems to think his IQ is exceptional. If asked, most people would say they are smarter than the average person. (They would also likely say they’re more competent, kinder, more honest, and more responsible.) This is a well-studied phenomenon in psychology known as the “better-than-average effect” or “self-enhancement.”
Even so, people might be reluctant to publicize their sense of superiority, because such boasts tend to be poorly received. One study found that while bragging about your own good qualities didn’t necessarily make people dislike you, bragging about yourself in relation to others did. For example, saying “I am a good student” probably wouldn’t bother anybody, but saying “I’m a better student than others” would. Bragging about your better-than-average intelligence, then, would likely make other people think you’re insulting them—and make them feel aggressive toward you as a result.
“We see this consistently, that the claim, at least in this culture, of being above average is just frowned upon,” says Joachim Krueger, a professor of psychology at Brown University who has studied bragging. “There are also conversational norms.” There’s an expectation that your good qualities will reveal themselves over time, without you announcing them.
So if bragging about your intelligence can alienate people, and make you look like a loser (at least to Stephen Hawking), why risk it?
It feels great, for one thing. People love to talk about themselves—to the point that in one study, they turned down money for the opportunity to answer questions about themselves. And the same study found that self-disclosure appears to activate brain regions associated with reward.
How to Boast on the Sly
They may also miscalculate how others will react to their boasts. One study found that people often self-promote because they’re trying to make a good impression on others. But they tend to overestimate how much their self-promotion will make other people feel happy for them and proud of them, and underestimate how annoying people will find it.
Krueger coauthored a study in 2016 that found bragging about your superiority comes with a tradeoff. It will make people see you as more competent, but less moral—unless the thing you are bragging about is provably false. Then it makes people see you as incompetent and immoral.
In a case like Trump’s brag about being smarter than Tillerson, where the validity of his claim is unknown, the research suggests people would err on the side of believing him.
“My personal opinion of the president’s claims about his own IQ is that he is making a gamble: brag and be thought of as competent (intelligent), and hope that nothing will ever surface that proves his claims wrong,” Patrick Heck, Krueger’s coauthor on the study, told me in an email.
“Of course there won’t be an IQ test given and then the nation will be informed what score he had and what score Tillerson had, so it’s pure rhetoric,” Krueger says. “In the context of our study if there truly were no other information about him, that would be a slight advantage.”
But nobody is evaluating Donald Trump based on this claim and this claim alone. Krueger points out that because opinions of the president are already so polarized, people will likely incorporate this latest brag into their existing views of him. If they think he’s a clever politician and a savvy businessman, they’ll likely take his brag at face value. If they already think he’s incompetent and immoral, an unverified claim about his IQ probably won’t change that.
But what if Trump and Tillerson were actually to throw down—intellectually speaking? Mensa International—a society whose members must prove their IQs are in the top 2 percent of the population—has offered to host an IQ-test showdown between Trump and Tillerson, heaven help us all. What if it really happened? And what if Trump were to lose?
“That’s a crazy hypothetical—you and I know perfectly well that’s not going to happen and Mensa never thought it would happen,” Krueger says. “That's another rhetorical play to throw into the game. But in your hypothetical universe, [if they] take the test and we find that he is 20 points below Tillerson, then according to our study, it would be a false positive and it would be particularly embarrassing.”


The morning of January 27, 1967, Gus Grissom and his Apollo 1 crew put on their flight suits. Foil-clad, with breathing masks, they looked like a mid-century vision of the future brought to you by Reynolds Wrap. The crew of three were to perform a launch test that day, to see what it would be like when they rode a metal cone to space.*
Grissom had been to space before during the Gemini program. That day’s practice wasn’t going great, not like one would hope an actual launch would go. First, the astronauts smelled something rotten in their oxygen. That delayed them by more than an hour. Then, their communications system began to fritz out. Of this, Grissom famously groused, “How are we going to get to the moon if we can't talk between two or three buildings?”
Later, though—into that same microphone and over those same lines—came a single word: “fire.”
It was true: Damaged wires had likely ignited a spark, which fed on the all-oxygen air, growing with its consumption of space-age material—nylon, foam.
The crew tried to escape the capsule. But the hatch wouldn’t open. All three astronauts suffocated inside the vessel that was supposed to carry them—and with them, us—into the future.**
The agency’s two other fatal accidents occurred during the same January week as Apollo’s: Challenger 19 years later, Columbia 17 years after that.*** And just three years ago, the private-spaceflight industry endured its first human loss, when Virgin Galactic’s SpaceShipTwo lost its copilot.****
After each fatal incident, the nation has responded with shock and grief. These explorers—our explorers, Earth’s explorers—paid for that exploration with their lives. Questions arose. Some—How did this happen?—are left to inspectors and investigators. But others—How big a cost are humans willing to bear to leave the planet?—lie in the public domain. The answers seem to have changed throughout the decades, as space travel seemed to evolve from something novel to something routine.
Today, industry and government are both upshifting gears, back into novelty, which means the public’s attitudes toward space travel and its inevitable accidents may return to what they were in NASA’s early, more adventurous days. After decades in a stable and predictable orbit, American spaceflight will return to new vehicles and, maybe, new destinations. The country is deciding which far-off world to point ships toward next, with the moon and Mars the most likely candidates. Private companies are doing the same, and preparing to take high rollers on suborbital romps. And with that leap into the unknown, Americans may become more tolerant of the loss of astronaut life. If they don’t, the government and private industry might not be able to make the leap at all.
We all know people probably will die on these new missions, especially if they become commonplace, as many hope. What no one knows is how we will all respond to those losses.
* * *
When Grissom and his compatriots signed on to the astronaut corps, times were different. They were different: cowboy test pilots—military men, mostly, with that rightest of stuff. Space, and the flight paths to and through it, was basically uncharted. Rockets blew up—a lot—listing sideways, spinning tail over teakettle, exploding heads in the ground like ostriches.
And the astronauts themselves were, for the most part, inured to their mortality. In The Right Stuff, Tom Wolfe repeatedly references the euphemism the early astronauts used to describe fatal crashes: The fliers had “screwed the pooch.” It’s gallows humor: The pilots and astronauts couldn’t completely control their survival—but they could at once face death, distance themselves from it, and use tone to strip it of power.
The public perceived these guys (and they were all guys) as all-American swaggerers, laying their lives on the line for the primacy of the country.
“It was a battle in the Cold War,” says Rand Simberg, author of Safe Is Not An Option: Overcoming the Futile Obsession With Getting Everyone Back Alive That Is Killing Our Expansion Into Space.
The nation, of course, mourned the Apollo 1 crew’s loss—especially given its gruesome nature. But the public and the government were perhaps not surprised, or philosophically disturbed, that people had to die if Americans were to get to the moon in a decade. In an article called “Space Travel: Risk, Ethics, and Governance in Commercial Human Spaceflight,” space strategist Sara Langston looks to other fields to understand attitudes and regulations about space exploration. “In the Encyclopedia of Public Health, [Daniel] Krewski defines acceptable risk as the likelihood of an event whose probability of occurrence is small, whose consequences are so slight, or whose benefits (perceived or real) are so great that individuals or groups in society are willing to take or be subjected to the risk that the event might occur,” she writes. The risk of space accidents, by inference, are subject to the same kind of cost-benefit analysis.
After Apollo, though, came the staid shuttle program. And with it, the tenor of spaceflight changed. The Cold War ended in the ’90s. The spacecraft was called a shuttle. You know, like the bus that takes you to the airport. The Americans had already conquered spaceflight—we got to the moon, which was very hard and very far away and involved orbiting other bodies and sometimes landing. Spinning ellipses around our own planet in a sturdy vehicle? Easy.
The shuttle program left Americans—and perhaps the world—with the false sense that the space-pioneer days were over.
* * *
In technical terms, as the shuttle program developed, people began to think of its flights as operational rather than experimental. In experimental mode, engineers are still figuring the details out, fingering the edges of a craft’s envelope and seeing how hard and fast they can press before they get a cut. In operational mode, though, engineers are supposed to know most everything—the ups, downs, ins, and outs of performance given sundry contingencies.
While the shuttle mostly functioned well, that performance was never actually a given. The vehicle remained, to its last days, experimental, a status reflected in its success/failure rate. “I think people that know our industry kind of understand the edge we're on here, because these systems are tremendously complex,” says David Bearden, general manager of the NASA and civil-space division at the Aerospace Corporation. “If you look back, empirically or historically, at any launch system, about the best you're ever going to get is 1 in 200. On an airline it is a one-in-a-million chance. People who know the industry and know the way those systems operate understand that, I think.”
* * *
I was only six months old when space-shuttle mission STS-51-L sat on the launchpad on January 28, 1986. Aboard were six astronauts and Christa McAuliffe, a teacher from Concord, New Hampshire. The shuttle lifted off on the cold Florida morning. But then, nine miles above Earth’s surface, that seemingly reliable spacecraft broke apart, undone by the uncharacteristic chill at Cape Canaveral that day.
As a Miami Herald Tropic investigation later detailed, the astronauts didn’t die right away: The crew vehicle stayed intact, and continued to go up, before tipping back toward Earth, traveling 12 miles of sky before crashing into the cold ocean water—hard as the cement on the launchpad. The astronauts, the article said, were very likely alive until the very end and might have even been conscious.
Coverage from the days after the tragedy expresses, of course, sadness. “The Shuttle Explosion; Suddenly, Flash of Fear Dashes Watchers’ Hopes,” read a New York Times headline.
“What Went Wrong? Shuttle Loss in Ball of Fire Stuns Nation,” went one from the local Orlando Sentinel.
Both papers, though, declared that the show must go on: “Reflecting on Loss: Welling of Tears, a Desire to Press On,” said the Times.
“Three shuttle veterans lose peers but not faith in program,” said the Sentinel.
The losses, while tragic and (as the Rogers Commission Report would later reveal) avoidable, shouldn’t squash the program. Sacrifices, after all, must be made, for a new program whose utility the nation was still proving.
* * *
I was 17 when NASA lost the space shuttle Columbia in 2003. I’d grown up in Central Florida, not far from Kennedy Space Center. I’d seen almost every shuttle launch in person—with my classmates outside on the sidewalks of my schools, with my sisters in the backyard, and very occasionally from the far side of Cape Canaveral, with my parents. The sonic booms from landings sometimes set off the burglar alarms that hung from our door handles.
But in 2003, I had things to do that didn’t include watching out for spacecraft. I was on my way to a band (marching band, not the cool kind of band) rehearsal session when I heard about Columbia. News of the accident came slow and halting over whatever alt-rock station I was blasting from my Grand Am.
Later, investigators would reveal that a piece of foam had fallen from the shuttle’s wing during launch, leaving a hole that let gas come through when the shuttle re-entered the atmosphere. The shuttle was going Mach 18, 37 miles above the ground, when it broke apart, shaking debris across thousands of square miles.
I remember sitting in my car in a church parking lot, thinking how it couldn’t be real. I remember thinking the radio host didn’t sound like he thought it was real. We’d probably both watched shuttles launch and land safely for much of our lives. To us, the whole program seemed routine—operational. It had moved into that realm of seeming safety, and risks seemed not just less likely but also less justified. And while we always knew this could happen, we never thought it would.
The Columbia disaster represented, unlike the Challenger explosion before it, the start of the finish for the shuttle program. NASA announced its end the very next year. Two strikes, shuttle’s out.
* * *
Sometimes, you hear the phrase “Failure is not an option” associated with NASA. But it was never a slogan at the agency; no one in mission control, that we know of, ever said it, and no manager passed it down. It was just a line in the movie Apollo 13. Failure is always an option: It has to be.
Of course, no one wants a rocket to blow up or a crew capsule to fall to Earth. But to undertake space travel, the undertakers have to acknowledge those possibilities and mitigate the risks. As NASA administrator William Gerstenmaier said in his paper “Staying Hungry: the Interminable Management of Risk in Human Spaceflight,” “We never simply accept it, but NASA, our stakeholders, and the public must acknowledge the risk as we move forward.”
The public, to some extent, also knows that’s the equation. But a 1/200 mission-failure rate means that one doesn’t happen very often, which means that every one comes as a shock.
Still, astronauts’ deaths don’t always cause communal moral outrage. “A particularly risky venture can become socially acceptable in correlation with the value placed on it,” Langston wrote in her risk paper. If people value a space-exploration program, in other words, they’re okay with others risking their lives to push it forward.
Simberg contends that wasn’t fully true with the shuttle, as compared to Apollo—an inspiring and aspiring mission with political importance. “The reason we were so upset about losing these seven astronauts was that what they were doing was kind of trivial,” he says of Columbia.
We don’t always demand, though, that people be doing something Valuable that Benefits Humanity to let them risk their lives (and there were lots of ways the shuttle and, in particular, its trips to the always-peaceful International Space Station did benefit humanity). About 1 percent of people who try to climb Mount Everest historically die in the attempt, for example. And this despite the fact that topping Everest is not exactly exploration, with its $40,000 price tag and Sherpa guides and commercial expeditions. And it’s been done before.
Shuttle astronauts, meanwhile, have a 1.5 percent chance of dying on a given trip to space. And trying to keep them at least as safe as that—or safer—means the agency can’t go as boldly as private industry can.
* * *
The major players in the crewed-commercial space are SpaceX, which wants to eventually build a martian colony; Blue Origin, whose Jeff Bezos envisions an outer space full of industry and long-term habitation; and Virgin Galactic, which wants to democratize access to space closer to home, with a carrier plane that rides up to 50,000 feet, then travels up on its own and glides back down at the behest and guidance of its pilots.
On October 31, 2014, Virgin Galactic paid a human price for that system. During that October test flight, copilot Michael Alsbury unlocked SpaceShip’s feathering system, which changes the shape of the plane to aid reentry, early. The wind then pushed the system open, and the vehicle destabilized. While pilot Peter Siebold parachuted to safety, Alsbury remained with the ship, and died on impact.
After the accident, Virgin allowed its already-booked customers to back out, but just 3 percent did.
SpaceX, meanwhile, has had its own explosive setbacks, and yet the company and leader Elon Musk still remain the industry’s darlings. SpaceX blew up an uncrewed Falcon 9 rocket in September 2016. In June of the year before, the company lost another Falcon that was supposed to resupply the Space Station. In test launches and landings of its reusable rockets, SpaceX has also had a vessel tip over into the ocean and explode (January 2016); crash into a ship (January 2015); and land “too hard for survival” (April 2015).
Based on this, the NewSpace industry seems to exist firmly in the experimental phase. But, more than that, the public seems to know—and accept—that status. “You understand that you're in a test-pilot phase,” says Bearden. “The public can process that and say, ‘That's not me. By the time I fly, they're going to have worked it out.’”
The public permits mistakes for the private space companies—because they produce rockets and results on non-geologic timescales, and lay out visions like “you can go to space” and “you can have a house on Mars.”
The FAA, which regulates commercial space activity via the Human Spaceflight Requirements for Crew and Spaceflight Participants, is also relatively forgiving. “Congress mandated these regulations in the Commercial Space-Launch Amendments Act of 2004,” says the FAA’s description of this law. “Recognizing that this is a fledgling industry, the law required a phased approach in regulating commercial human spaceflight, with regulatory standards evolving as the industry matures,” attempting not to crush innovation with regulation. Flight providers do, though, have to get extremely informed consent from would-be astronauts.
NASA recognizes the value in this model, and in its different posture toward risk. The agency has teamed up with such space companies—letting them, among other things, shuttle cargo and crew to low Earth orbit. NASA no longer has to be all things to all people and missions, and can let those experimental upstarts do a little legwork.
The agency may also see, though, that the public perceives New Space cadets as pioneers—a lens through which they don’t see NASA like they used to—and so forgive their mistakes, tallying them as the cost of innovation, rather than a cost not worth bearing. And perhaps the agency hopes the same thing for itself, as it turned those duties over to private companies so that it can focus on its own bold goals, its own new risky, experimental phase of operations with both the costs and the benefits that come with that.
* This article originally stated that there were four crew members aboard Apollo 1.
** This article originally misstated the cause of death for the Apollo 1 crew.
*** This article originally implied that the Columbia disaster occurred 36 years after the Challenger explosion.
**** This article originally stated that the Virgin Galactic crash resulted in the death of the craft’s pilot. We regret the errors.


Ten months in, and the presidency of Donald Trump has acquired a reputation for ineffectiveness. Trump’s attempt to repeal Obamacare has failed three times; he has taunted and alienated some congressional Republicans; he has hemorrhaged senior administration officials while struggling to contain an FBI investigation.
But Trump has found near-total success in a few areas of lawmaking—and few are as expansive, with consequences as long-lasting, as environmental policy. Since January, the Trump administration has tried to dismantle former President Barack Obama’s broad legacy of EPA rules meant to lessen the blow of climate change. It has largely succeeded. This week, Trump’s team opens a new front in that war.
Scott Pruitt, the administrator of the U.S. Environmental Protection Agency, announced Monday that he will fully repeal the Clean Power Plan, the Obama administration’s signature plan to reduce greenhouse-gas emissions from the U.S. electricity sector.
It’s the new administration’s most aggressive attempt to materially transform U.S. climate policy since its departure from the Paris Agreement on climate change in June.
The Clean Power Plan was designed to lower greenhouse-gas emissions from American power plants by 32 percent by 2030, as compared to their historical peak in 2005. It constituted the central legal mechanism to meet the American commitments made under the Paris Agreement.
Speaking in the coal-mining town of Hazard, Kentucky, Pruitt alleged that the Clean Power Plan represented illegal executive overreach. He also linked it to a “war on coal” that he says was waged by the Obama administration.
“The past administration was unapologetic. They were using every bit of power [and] authority to use the EPA to pick winners and losers on how we generate electricity in this country. That is wrong,” he said. Mitch McConnell, the Senate majority leader and a longtime Republican of Kentucky, appeared by his side.
Environmentalists condemned the repeal and promised to fight it in court. Ditching the Clean Power Plan, they said, would further degrade the planet’s climate by leading to the release of more heat-trapping gas into the atmosphere.
Public-health groups, including the American Lung Association, also condemned the planned repeal. Nearly half of Americans already live in counties with unhealthy levels of air pollution, the organization has said, adding that climate change appears to makes conventional air pollutants worse.
“Science shows that warmer temperatures can reduce air quality, due to increases in ozone and particulate matter,” said Laura Anderko, a professor at Georgetown University. Air pollution poses a particular health threat to children, she noted.
The Clean Power Plan was sold in part on its public-health benefits. A 2015 study in Nature Climate Change found that a set of rules similar to the Clean Power Plan would prevent 220 heart attacks and 3,500 premature deaths per year. Pennsylvania, Ohio, and Texas would all see the greatest gains under the simulated plan, each avoiding hundreds of early deaths annually.
But the Clean Power Plan has also long contained a tension inherent to American environmental law. Since 2007, the Supreme Court has held that the EPA has the legal authority to regulate carbon dioxide and other greenhouse gases under the Clean Air Act.
The Obama administration didn’t pursue regulation at first, attempting to pass the Waxman-Markey climate-change bill through Congress. That bill would have allowed companies to bid on the right to emit carbon dioxide into the atmosphere, mirroring previous federal laws to reduce acid rain. But after it failed in the Senate—and Democrats lost control of Congress in 2010—Obama’s EPA turned to the Clean Air Act. Over the course of several years, the agency studied, issued, and revised state-by-state rules guiding how local governments should reduce greenhouse-gas emissions from their power plants. The final version of the Clean Power Plan was released in August 2015, months before the Paris conference on climate change.
Almost immediately after the Clean Power Plan was published, dozens of state attorneys general sued the Obama administration, alleging the rules were illegal. They were led by Pruitt, then the Oklahoma attorney general and the chairman of the Republican Attorneys General Association. Despite the Supreme Court’s finding that the Clean Air Act allowed the EPA to regulate carbon dioxide, they argued that this plan, the Clean Power Plan, was illegal.
Did the Obama rules break the law? We may never know. The D.C. Circuit Court of Appeals was still considering the case on Election Day last year. Trump ordered the EPA to revisit the plan in an omnibus executive order in March.
Due to the Supreme Court’s 2007 ruling, the Trump administration will have to issue a new regulation to replace the Clean Power Plan. But it has great leeway in when it may issue that new rule, and it appears to be taking its time.
Of course, the Clean Power Plan is not the only change convulsing the U.S. power sector. While solar and wind energy are on the rise worldwide, they have mostly displaced nuclear power in the United States. Coal use has indeed been plummeting in recent years, but this has mostly been due to cheap natural gas made available by fracking.
“You want to know how you bring the coal jobs back? Ban fracking,” said Ted Thomas, the chairman of the Arkansas Public Service Commission and a former Republican legislator in the state, in March. “When you talk to utility professionals, there are more of them who think Elvis is still alive than believe that” the Obama administration’s rules are to blame for the decline in coal, he told me then.
Indeed, energy experts say that the United States may meet the ostensible emissions-reduction goals of the Clean Power Plan. The Rhodium Group, an energy consulting firm at the forefront of climate-economic forecasting, projects that the U.S. power sector will fall by between 27 and 35 percent by 2030 without the Clean Power Plan.
But its experts also caution the 32-percent figure was just an estimate, and the real emissions gains of the Clean Power Plan may have been higher than anticipated. Historically, energy forecasts have undershot the rise of solar and renewable energy.
And there are graver implications to the rollback. The United States is not on track to reduce its carbon emissions fast enough to avert catastrophic global warming—even under the most optimistic economic forecasts. A recent study on the world’s carbon budget found that the world would need to reduce its emissions in ways that parallel “the Great Depression, the years following World War II, and during the collapse of the Soviet Union” to stave off the worst of global warming.


Rumors that the Trump administration was more interested in the moon than Mars began circulating days after the inauguration. Leaked memos published in February revealed the president’s advisers wanted NASA to send astronauts there by 2020, one part in a bigger plan to focus on activities near Earth rather than missions deeper in the solar system. Vice President Mike Pence spoke vaguely of a return to the moon in a speech in July. In September, the administration nominated a NASA chief who extolled the construction of lunar outposts. All signs pointed to a significant shift in the country’s Mars-focused space agenda of the last seven years.
This week, the Trump administration made it official.
“We will return NASA astronauts to the moon—not only to leave behind footprints and flags, but to build the foundation we need to send Americans to Mars and beyond,” Pence said Thursday at the inaugural meeting of the National Space Council, an advisory body his administration recently revived.
The vice president’s comments marked a pivot from Barack Obama’s directive for a “Journey to Mars,” established in 2010, and harkens to the aspirations set forth by the George W. Bush administration. The Obama administration had maintained that some kind of human activity in cislunar space—the region between the Earth and the moon—was necessary to test technology for a mission to Mars, but the efforts would amount to a pit stop, not a destination. While Pence did not provide details on what kind of “foundation” Americans would build on the moon, the new direction was clear: Americans should be spending more time in their cosmic backyard before flying off into the solar system.
“It’s a 180-degree shift from no moon to moon first,” said John Logsdon, a space-policy expert and former director of the Space-Policy Institute at George Washington University.
The announcement is obviously good news for space-transportation companies and lunar researchers lamenting the country’s 45-year absence from the moon. For those in the Mars camp, many of whom aim for a human mission to the planet by 2033, the news puts their ambitions on shakier ground.
The administration’s push for a return to the moon may be unambiguous now, but plenty of questions remain, ranging from the basic, like when and how, to the intriguing, like the role commercial spaceflight companies might play. NASA also wouldn’t be starting from scratch. The space agency has spent the last decade building the Space-Launch System and Orion, a rocket and spacecraft intended to carry people into deep space but also to build a cislunar way station called the Deep-Space Gateway. NASA planned to use the Deep-Space Gateway as a place for astronauts to prep for deep-space journeys, but the new shift could see the station being used for lunar landings.
Both time-tested contractors and growing commercial companies will be eager to work on potential lunar activities. Boeing is currently developing a capsule that would ferry people into low-Earth orbit, and SpaceX said its proposed mega-rocket, which is mostly intended to fulfill Elon Musk’s Sim City-esque ambitions for Mars, could contribute to travel to the moon. Musk, well aware of the political benefits of it, leaned heavily into lunar ambitions. “It’s 2017. We should have a lunar base by now,” he said recently. “What the hell’s going on?”
Federal support for moon research is also good news for lunar scientists like Paul Spudis, a scientist at the Lunar and Planetary Institute in Houston who writes frequently in support of a return. He dismissed NASA’s Mars ambitions as unrealistic and a public-relations stunt. “I see the vice president’s remarks not so much as a pivot in policy as a belated recognition of simple reality,” Spudis wrote in an email. “They don't have have an architecture, they don't have the spacecraft, they don't have the technology, and most assuredly, they don't have the money to bludgeon any difficulties into submission.”
Mars proponents, naturally, disagree. “We weren’t anywhere close to being ready” to going to Mars, Logsdon said, but to dismiss the objective as simply publicity is wrong. NASA has invested over the years in some research for the requirements of a mission, including life-support systems and landing technology, he said.
“Most of the people who are Mars-centered worry that we’ll get stuck on the moon, all the resources available will be focused on lunar exploration, and the idea of getting to Mars will slip indefinitely into the future,” Logsdon said.
Chris Carberry, the CEO of the nonprofit group Explore Mars, fears the same. Carberry said he’s pleased with the administration’s focus on human spaceflight and doesn’t oppose a pit stop on the moon. But he wonders whether the construction of a full-fledged lunar base could consume enough resources that would delay a Mars mission for decades. “If we got to the surface of the moon, we need to do it in a way that really is a stepping-stone to getting to Mars, not just an excuse to build a base there,” Carberry said.
In Washington, NASA’s Mars goals have received bipartisan support, which is typical for space programs in general. In March, Trump signed a NASA funding bill that included some of the strongest language about a human mission to ever appear in U.S. legislation, listing “achieving human exploration of Mars” as a key objective. The space agency has also been buoyed by interest from a general public bombarded with Hollywood movies about Mars and deep-space travel.
A renewed focus on the moon brings the United States into some alignment with space agencies in Europe, Russia, India, and China. The European Space Agency envisions building a “moon village.” Roscosmos, the Russian agency, is recruiting cosmonauts to be the first Russians to land on the moon in the 2030s. ISRO, the Indian agency, launched its first lunar orbiter in 2008 and plans to send a second mission, this time to land on the surface, next year. China has spent the last decade experimenting in cislunar space with both robotic and crewed missions, and officials have said they would put astronauts on the moon by the mid-2030s. A mission to send a rover to the far side of the moon, a first for humankind, is planned for next year. The United States is unparalleled when it comes to exploration of the solar system beyond Earth, but some in the country, particularly in the security community, worry that the country’s cislunar capabilities are rapidly atrophying.
Pence said this week he believed other spacefaring nations have outpaced the United States. “In the absence of American leadership, other nations have seized the opportunity to stake their claim in the infinite frontier,” he said. “Rather than lead in space, too often, we have chosen to drift.”
Space-policy experts say it’s too early to say who the winners and losers might be in this new chapter in the country’s space agenda. The administration has promised a robust human spaceflight program, an effort that both moon and Mars proponents can get behind. And the administration didn’t rule out a Mars mission entirely. It just chooses to go to the moon in this decade and do the other things—later.


Squirrels are among the peculiar menagerie of creatures who’ve made a home for themselves where humans live. Like pigeons, they’ve figured out how to continue their ways in our parks, cities, and towns, tucking away nuts in the walls of houses, in basements—and in the lawns of institutions of higher learning.
At the University of California, Berkeley, that behavior is the focus of a recent paper by comparative psychologists. Having performed a number of experiments with their small furry neighbors, Mikel Delgado and her colleagues discovered that squirrels are not putting nuts away willy-nilly. When they have the chance, they'll sort their bounty by type, like nuts with like nuts. That suggests they’re remembering not only the locations of their caches but, intriguingly, their contents.
The work began some years ago, with this paper just the latest in a long string.“The question always is, given that squirrels bury so many nuts, how do they decide where to bury them and how are they able to find them again?” says Delgado, who performed the research as a graduate student and now is a postdoctoral researcher at University of California, Davis.
If you’ve ever watched a squirrel with a nut, you might have noticed that they turn them over and over in their paws, bobbing their heads. That seems to be a way for them to judge the weight and other qualities of the nut they are holding, which suggests that information is useful for them in their subsequent stashing. Perhaps, the scientists thought, the squirrels were sorting the nuts somehow.
Delgado and her colleagues designed a series of experiments in which they gave nuts to 45 squirrels and watched to see where they put them. Each squirrel was given an almond, a walnut, a pecan, or a hazelnut, then the researchers followed them from a distance to see where on the Berkeley campus they cached it. After it was hidden, the researchers took a GPS reading at the site. Half the squirrels got their next nut right then and there, while the others were lured back to the starting point. Not all squirrels are very patient with this, it turns out. “[Sometimes it] takes a while, because you have to convince them to come back,” says Delgado. The intent was to mimic a situation where a squirrel has found a tree flush with nuts that they’ll return to.
This process repeated 16 times, so each participant got 16 nuts. Some squirrels got theirs in runs of the same kind—four walnuts, then four almonds, and so on—and some got them in no particular order. This was to test whether they would cache the walnuts together, for instance, even if they got lots of other kinds of nuts in between.
When the researchers tallied up the contents of the caches, they discovered that the squirrels that got their nuts in the same place every time would group the same kinds of nuts together, a process called “chunking” in cognitive science. “That is pretty cool because it suggests they can remember” where they put certain types of foods for at least a few hours, says Delgado.
When the foraging was spread out over the campus, the squirrels wouldn’t go all the way to the last almond when they got a new one, however. Instead, they made caches of mixed nuts along their route. Chunking wasn’t a universal strategy, which suggests it takes some mental resources to keep track of where that last nut of that particular kind went. Sometimes it’s helpful; sometimes—perhaps when a squirrel is already pretty far away from its first caches—it’s not worth the strain.
It’s fascinating to think that right under our noses squirrels are engaging in some complex sorting processes. Then again, this is a species that lives for months on food hidden all across the landscape, in patterns that researchers are still working to understand. That may shame those of us who can’t find that butter we know we bought, or that stray can of beans that turns up years later. Perhaps if we paid a little more attention to creatures living around us, we could pick up a few tips.


At first, they look like stars. I see them as I gaze upward at the ceiling of a flooded, pitch-black cave—hundreds of blue pinpricks. As my eyes habituate to the darkness, more and more of them resolve, and I see that they are brighter and more densely packed than any starry field. And unlike the night sky, these lights don’t appear as a flat canvas, but as a textured one. Some are clearly closer to us than others and they move relative to each other, so the whole tableau seems to undulate gently as our boat sails beneath it. These lights are not astrological, but entomological. They are produced by insects called glowworms.
The word “glowworm” is sometimes used colloquially to describe fireflies and click beetles. The insects in the caves aren’t part of either group. They’re maggots—the larvae of small flies called fungus gnats. Hatching out of eggs that are laid on the ceilings of caves, the larvae spin hammocks of silk. From these hammocks, they lower up to 70 threads of silk—extremely strong, and coated with blobs of sticky mucus. These threads, which dangle downward like bead curtains, are traps, and the glowworms bait them by triggering a chemical reaction in their rear ends that emits blue light. The light lures in other insects that get entangled in the silk, and are eventually reeled in and devoured by the glowworms.
After 6 to 12 months of eating whatever they can ensnare, the larvae transform into adults, which lack mouths and never eat. Their only job, in the final few days of their lives, is to mate and create the next generation of glowing-bottomed, trap making juveniles.
These luminous insects are found in the dark and damp corners of New Zealand and Australia, and the Māori know them as “titiwai”—a word that refers to light reflected in water. That etymology reflects the glowworms’ habit of inhabiting damp and dark places, including riverbanks, tropical rain forests, and—most famously—caves like those at Waitomo. For thousands of tourists, these places are major attractions, where one can gaze at the ethereal beauty of a living, indoor star field. But for the thousands of moths, midges, and mayflies flitting around in the darkness, these are places of death.
The origin of the glowworms’ fatal beauty is unclear, but you can piece together a plausible story by looking at their relatives. Most fungus gnats live and feed on mushrooms, and some of them build sticky webs to trap edible spores. Such webs would inevitably and accidentally have snagged passing insects. Tempted by this source of protein-rich food, some fungus gnats went all-in on carnivory, and transformed their spore-catching webs into dangling fishing lines for capturing flying prey. Such prey are plentiful in the tropical caves of South and Central America, and Southeast Asia, where many fungus-gnat species still dangle their sticky threads. But in temperate regions like Australia and New Zealand, where prey are scarcer, fungus gnats would have needed a way of attracting their victims. That, perhaps, is why they started to glow.
They did so, bizarrely enough, by refashioning their kidneys. Insects have a series of tubes called Malpighian tubules that branch off their guts and produce urine, much like our kidneys do. In glowworms, the cells at the very end of these tubes have become swollen and transparent—they’re the ones that glow. “It’s absolutely unique,” says David Merritt from the University of Queensland. “Insects are such a broad and diverse group that you’ll always find some insect that has done something weird with a preexisting structure.”
Merritt has been studying the glowworms for years. Over that time, he has shown that they control their light with great finesse, using nerve cells that run into the light-producing kidney-esque organs. If you shine a light onto a glowworm, it will switch off its own light after a few minutes. If you anesthetize one, it will glow very brightly before dimming down again. And if you give them good vibrations, they’ll, er, get the excitations. There are some tours in New Zealand, Merritt tells me, where guides will deliberately hit the water or cave walls with an inflated inner tube; in response, the field of living stars will double in brightness. Merritt can achieve the same effect in his lab by pressing a vibrating cellphone against the aquarium where his captive glowworms live. “They really brighten up intensively if they detect vibration,” he says. “I’m not sure of the function.”
He also found that the glowworms glow on a cycle. Even in the constant darkness of a cave, cut off from the daily rhythms of the outside world, they’ll glow particularly brightly at around 6 or 7 p.m. They also synchronize with each other. “We’ve done experiments where we get them out of sync and put them together—and they sync up over five to six days,” says Merritt. “I think they’re maximizing the population’s light output for the time when there are most insects flying around inside the cave.”
All of which makes me wonder: Are there cheating glowworms that save energy by going dark, and rely on the light of their neighbors to lure victims into their hanging snares? Are there cave insects that have evolved to resist the fatal attraction of these blue lights? Do you get populations of glowworms that switch their cycles to glow at different times of the day, to attract insects during the quiet hours when the main colony has dimmed down? And how do these insects even produce their glow?
That last question, at least, has an answer. Aptly enough, glowworms light up their underworld with a pair of chemicals that have a satanic etymology—luciferins and luciferases, after the Latin for “light bringer.” When luciferins react with oxygen in the presence of luciferases, they produce light. Fireflies, jellyfish, and many other luminescent animals glow in the same way, and each group has its own distinctive luciferins and luciferases. “They’re generic terms, like ‘oven,’” says Kurt Krause from the University of Otago. “I’ve got one, you’ve got another, and they both make heat, but they’re different.” Similarly, Krause and his colleagues have found that the glowworms have their own unique light-making chemistry, which they’re starting to tease apart.
Why bother? Partly, it’s because luciferases are really useful to scientists. Researchers can use them to engineer the cells of non-glowing creatures so that they light up when certain genes are active, allowing them to track cancer cells, detect viral infections, and more. And since different kinds of luciferase glow with their own colors, the more you discover, the more things you can track at once. “We think the glowworm has an interesting biochemical story to tell, and we think that hopefully it’s something that can be commercialized,” says Krause. “But even if it’s not, it’s cool! What’s cooler than organisms that make light? When you get involved with them, you get mesmerized.”
That’s certainly the case when I see the lights for myself. In Waitomo, I sail beneath a field of living stars, produced by the repurposed kidneys of synced-up maggots that are trying to lure other insects into death traps. The true nature of these lights is weird and perhaps even grotesque. But they are strong enough that when I look to my right, I can see my partner’s face in the darkness of the cave. And she is smiling.


The Space Age began on October 4, 1957, exactly 60 years ago. On that date, the Soviet Union used a rocket originally intended for use with ballistic missiles to launch Sputnik 1, a small polished metal sphere weighing 184 pounds. Upon reaching its orbit, it became Earth’s first artificial satellite. Traveling at five miles per second, with an altitude that ranged between about 140 miles at its lowest point and 600 miles at its highest point, it orbited the earth once every 98 minutes. It was very nearly a failure. Telemetry after the fact indicated that an initial booster imbalance came within one second of causing the rocket to pitch so low that it would have veered downward, causing it to crash near the launch site.
Sputnik itself was just a small metal sphere filled with nitrogen, with a radio transmitter, power supply, fan, and antennae. All it did was beep, at two frequencies that could be detected by radio receivers, including ham-radio operators, around the world. That’s it.
On the surface, it would seem it was not much to write home about, and in fact the Soviet government initially didn’t write much about it. The official Communist newspaper Pravda printed only a few paragraphs about Sputnik 1 on launch day. But given the geopolitical situation at the time, outside the Soviet Union, in particular in the United States, the whole landscape of international power and prestige changed.
Five years before Sputik launched, the International Council of Scientific Unions had decided to establish the period July 1, 1957, to December 31, 1958, as the International Geophysical Year (IGY). (The fact that it was actually a period of 18 months somehow didn’t seem to bother them at the time.) To celebrate the IGY the White House announced plans in 1955 to launch an Earth-orbiting satellite during this period. A week later, the Soviets, who had as early as 1954 begun considering a development plan for an artificial satellite, approved plans for their own satellite program.
How Sputnik Launched an Era of Technological Fragility
The initial Soviet program was very ambitious, involving large satellites, with significant scientific payloads, the development of which would have resulted in launches sometime during the IGY. However, it soon became clear that the complexity of the proposed program could not be met in time. Based on a fear that the United States would beat the Soviets to the punch, the Soviet program changed completely. Instead of a complex satellite, a very simple and light satellite would be designed, which, while it could provide some scientific data, would essentially just get into orbit.
They had a rocket designed for ICBMs that was up to the task, and eight months after the new program was approved, and after several embarrassing failures, Sputnik 1 was successfully launched. (In November, the Soviets followed that successful launch with Sputnik 2, which housed a dog, the first living casualty of the new Space Race.)
The initial U.S.-government reaction to the launch of Sputnik was itself subdued. The Soviet program was not secret, and details of Sputnik were made publicly available before the launch, but no one took much notice. Moreover, U.S. spy planes had carefully been monitoring the Soviet rocket program so that the U.S. government was aware of the imminent launch, publicly stating that it did not come as a surprise. The Naval Research Laboratory, among other facilities, tracked Sputnik’s U.S. crossings.
But Sputnik’s launch did trigger concern among the U.S. public and their representatives in Congress. It demolished the notion, nurtured by the U.S. propaganda machine, that the Soviet Union was technically backward. The televised failure of the U.S. government’s first attempted satellite launch of the so-called Vanguard satellite only made the situation worse. Moreover, if a Russian satellite could fly over the United States, Russian missiles carrying nuclear weapons could perhaps also rain down upon the country.
Politics and science had already begun a courtship with the Manhattan Project during World War II, but the Space Race wedded them, a trend that has continued right up to the present time. By 1958 the United States had created two new agencies, NASA and the agency that would eventually become known as DARPA (the Defense Advanced Research-Projects Agency), and the U.S. government dramatically ramped up its support of scientific research and education programs.
The public became galvanized by fears of falling behind the Soviet Communist regime. Suddenly there was a new push for science and technology training in U.S. schools, and Congress enacted a law in 1958 providing low-interest loans to encourage students to study science and math at university. Ultimately the U.S. would build the space infrastructure that led to the moon landing in July of 1969.
Sputnik also had other political impacts. Democrats used the successful launch to argue that there was a dangerous “missile gap” with the Soviets, a central plank of John F. Kennedy’s successful presidential campaign.
Sputnik not only heralded a new era of intense competition between the Soviet Union and the United States, but also spread to many more countries—from Europe to China to India to North Korea—who all used space technology to build research infrastructure and to pursue international prestige.
As a child of the 1950s, I remember how the “Sputnik Moment” jump-started whole new areas of basic and applied research, not to mention the teaching of science and mathematics. And while I continue to find it unfortunate that space exploration is largely driven by national prestige, rather than the progress of science, it is nevertheless true that the side effects have been dramatic, and largely positive. Exactly three months after its launch, Sputnik reentered the atmosphere and burned up. The satellite may have disappeared, but its legacy continues to color almost every aspect of the world in which we now live.
As we face the 21st-century challenges of climate change and nuclear proliferation and much else, it’s worth asking whether we need a terrestrial Sputnik moment to catalyze public and government action to realistically face these challenges. With any luck, the galvanizing act will be as harmless as the small metal sphere the Soviets launched, and not something much more traumatic.


Deep in the Mariana Trench, at depths lower than the Rockies are high, rests a tin of reduced-sodium Spam.
NOAA scientists caught sight of it last year near the mouth of the Mariana’s Sirena Deep. It isn’t an isolated incursion, but it was nevertheless startling, the sight of those timeless golden letters bright against the deep ocean bottom.
Shortly after came news from another team of scientists who had found in the Mariana an innovation less familiar than shelf-stable meat, but far more significant. In the bodies of deep-dwelling creatures were found traces of industrial chemicals responsible for the rise of modern America—polychlorinated biphenyls.
PCBs had been detected in Hirondellea gigas, tiny shrimp-like amphipods scooped up by deepwater trawlers. Results from the expedition, led by Newcastle University’s hadal-zone expert Alan Jamieson, were preliminarily released last year and then published in February.
PCBs have been found the world over—from the bed of the Hudson River to the fat of polar bears roaming the high Arctic—but never before in the creatures of the extreme deep, a bioregion about which science knows relatively little.
How PCBs reached the Mariana is still under investigation. Jamieson and colleagues speculated on multiple, regional sources. A nearby military base. The industrial corridors along the Asian coastline. And the Great Pacific Garbage Patch, where PCBs glom onto plastic particles caught in the current. Over time, the plastic degrades and descends into the depths, ferrying PCBs with them.
But the true origin of PCBs lies in another time and place, in Depression-era Alabama, and before that, 19th-century Germany at the pinnacle of German chemistry.
* * *
PCB production began in late 1929 in a factory east of Birmingham. The same era that gave us New York’s Chrysler Building, The Little Engine That Could, and eventually Spam brought mass-made PCBs to market.
General Electric and Westinghouse were early adopters. Both firms formulated PCBs into dielectric fluids, the insulating liquids added to capacitors and other electrical components to keep them cool and to prevent fires. With PCBs’ aid, the electric grid spread from the industrialized north into the rural regions of the Deep South and the American West. By mid-century, PCBs had a bird’s-eye view of any block in America with a utility pole and PCB-bathed transformer.
Soon PCBs were added to paints, caulks, plastics, even floor finishes and dish detergents. They were branded, and assigned names like Aroclor. That commercial products contained PCBs was never advertised, explained Ellen Griffith Spears, who wrote the definitive book on PCBs’ genesis.
PCBs slipped into the world, becoming ubiquitous while remaining anonymous. Until the mid-1960s, when the Danish-born scientist Sören Jensen detected PCBs in the bodies of pike taken from the waters off Sweden.
In the wake of Rachel Carson’s Silent Spring, published in 1962, Jensen had been dispatched to look for DDT, one of the post-WWII pesticides whose increasing use Carson’s book had questioned. Jensen found DDT. But his data also signaled the presence of unexpected, yet chemically similar, contaminants. It took two years to determine the “ghosts” in his data were PCBs.
After that, everywhere Jensen thought to look, he found their fingerprints: in the feathers of archived white-tailed eagles, and in hair plucked from his own head, and others sampled from his wife and infant daughter. His conclusions, published in 1966, instigated a global investigation into the fate and toxicity of PCBs, research now carried forward (and into the deep oceans) by Jamieson and colleagues.
Today, PCBs are well-characterized pollutants—toxic, extremely persistent, and pervasive. All 209 variations of PCBs are known carcinogens. PCBs can alter liver function, and they can interfere with how humans reproduce, develop, think, and mount an immune response. Based on their cancer-causing potential alone, Congress voted to end American production in 1976 by attaching an amendment to the Toxic-Substances Control Act (TSCA).
“ToSCA,” as the law was called, gave the fledgling Environmental Protection Agency, created six years earlier, the authority to regulate industrial chemicals. PCBs were the only class of chemicals called out by TSCA; about 60,000 others were grandfathered, meaning their use was never questioned.
Another three years passed before Congress’s limits on PCB production took effect. Four decades later, though banned, PCBs live on, including in tiny amphipods swimming in some of the deepest waters of Earth’s biggest ocean.
* * *
PCBs, now endemic to environments everywhere, belong to a class of chemicals called (depending on the era) halocarbons, organohalogens, or halogenated organic compounds.
Organic, in this instance, refers not to foods raised without chemicals but to compounds made with carbon. Halo- (or halogen) signifies the presence of one or more of the halogen elements, the most familiar being chlorine, bromine, and fluorine. From these starting materials, chemists can make an array of compounds. But so can nature. To date, there are more than 5,000 so-called biogenic or naturally-occurring organohalogens.
Nature’s versions are forged in volcanoes or near deep-sea vents where temperatures run high, and chlorine and bromine are abundant. Organisms in all kingdoms can make halocarbons, though in minute quantities and typically for highly specialized purposes such as self-defense or signaling mates.
There are many pathways to making such complex molecules. The Iowa atmospheric chemist Scott Spak keeps a running tally of the “recipes” that might yield a PCB. While there are no known analogs in nature, one does get to wondering whether nature—at some point, somewhere on the planet or deep in the cosmos—could have served up the right mix of raw materials, in the right order and under the right conditions to allow for PCBs’ spontaneous formation. It is conceivable, Spak concedes, though purely hypothetical. Such a discovery, should it even occur, wouldn’t explain PCBs’ global dispersion, nor absolve what humans made with impunity. But it does hint at the complexity of Earth’s chemistry, and the humility with which we still endeavor to understand it.
Into intricate ecological and biological systems human industry introduced PCBs in extraordinary volumes, and in evolutionary terms, rapidly—over the span of three or four human generations, said Spak. But the problem isn’t so much that PCBs are “unnatural,” though one could make that argument. It is that they are molecules nature recognized, familiar enough to be folded into its systems and to confuse them.
Human biology has not adapted to their presence. Species far older than us, microbes mostly, have evolved over millennia to coexist with, and even to synthesize and break down, specific types of biogenic halocarbons. Some strains of bacteria are capable of disassembling PCBs. Other species, such as the Atlantic tomcod, bottom-feeders in the Hudson, have even developed a genetic variation that allows for survival in PCB-polluted waters, though their livers are also loaded with the chemicals.
But for humans, research tracking the health effects of industrial PCB exposures, Spak said, is tantamount to watching evolutionary consequences playing out in real time.
* * *
While sunlight (and some microorganisms) break down PCBs over time, they can be stunningly stable when stored in sediment, glaciers and other so-called “sinks” like the deep ocean. And because PCBs are lipophilic (or fat-loving), they can also accumulate in the fatty tissue of marine life, and in the bodies of mammals like us. Depending on the total load, some measure of PCBs can last a lifetime, or pass between generations through cord blood or milk.
The chemistry that enabled humankind to engineer elements into such durable molecules and enduring pollutants dates to the early 19th century, to a time when the natural world was the chemist’s muse. As chemistry industrialized, chemists were drawn by profit, and later, into geopolitics. In time, chemistry became a tool for nature’s mastery, and—both knowingly and inadvertently—an engine for its alteration.
At the center of this transformation is an elegant molecule called benzene. It is the same carbon-rich compound that lends gas stations their curiously evocative aroma, and PCBs their structural integrity.
A molecule of benzene is comprised of six carbon and six hydrogen atoms. Ask a chemist what benzene looks like, and she will draw a hexagon. It’s a schematic of how benzene’s six carbons circle themselves, as if linked hand in hand, into a ring. The benzene ring.
Benzene rings are also abundant in nature. They were present in Earth’s prebiotic soup, and they float in deep space. Many human hormones have benzene rings. And so do many human-engineered molecules.
Chemical engineers approach benzene as a building block from which to make thousands of useful products, including aspirin, the plastic lid to your takeaway coffee, and the Legos children leave strewn across floors the world over. Though, it should be noted that benzene is rarely made intentionally. Benzene is incidental to other industrial processes, such as refining oil into gasoline, processing coal into coke, or in the making of ethylene, another widely used chemical building block. Which makes benzene an industrial by-product and also a common industrial pollutant, especially following industrial accidents, as happened after Hurricane Harvey struck the plants along Texas’s Gulf coast.
All of which means the benzene ring is something of a paradox. When incorporated into certain molecules, it is essential to life. But in other configurations, slight tweaks in the composition and arrangement of atoms render benzene part of a toxic, possibly carcinogenic molecule. And when on its own, there is ample evidence that benzene causes cancer.
* * *
Until the middle of the 19th century, building on the benzene ring wasn’t possible because, though benzene had been isolated, its structure still eluded chemists. So significant was the “discovery” of benzene’s ring of carbons that in 1890, on the occasion of its 25th anniversary, the German Chemical Society threw a party—the Benzolfest! It was “a festival of magnificence perhaps unparalleled in the history of science,” wrote one commentator.
It was held at Berlin’s City Hall in Germany. The emperor was invited. Dignitaries came. All of the era’s most preeminent chemists gathered in their finest attire. Augustus Hoffman, a bearded and towering figure in the field of organic chemistry, waxed poetic about benzene and the chemist, Friedrich August Kekulé, who had “pluck[ed] the heart out of its mystery.”
The benzene tree, as he called it, was “thronged with blossoms,” its branches heavy with fruit. It was “a blaze of color,” and it gave off “an almost overwhelming fragrance,” a fitting metaphor given benzene’s signature scent.
To Hoffman, the benzene tree was a “giant.” It soared “into the clouds to where the eye cannot yet follow it.” Up the tree scaled “no dearth of industrious workers,” all “busily striving to collect the harvest,” he had said, referring to how entrepreneurial chemists were converting benzene chemistry into industrial products. “Keen climbers have already clambered up to the third or fourth branch,” Hoffman continued, some chemists “working at a dizzy[ing] height.”
With new insight into molecular configurations, chemists, starting around 1865, could better anticipate the steps required to synthesize new molecules. With the benzene ring, “the number of organic compounds all at once,” Hoffman had noted, “increased to infinity.”
Among those in the tree’s farthest reaches was Gustav Schultz, in attendance that night. In 1881, Schutlz, with the chemist Hermann Schmidt, had described the synthesis of a PCB. The pair published the new molecule in a premier German-language journal. And then, like DDT, also achieved during this era, PCBs were left to languish for decades on dust-cloaked shelves of obscure chemical libraries. Their discovery advanced chemical knowledge but was otherwise of little practical value.
* * *
Hoffman was born into the fast-changing world of 19th-century Europe. He grew up in parallel with organic chemistry, and ascended to its highest ranks. Unlike inorganic chemists’ fascination with Earth’s metals and minerals, the first organic chemists studied molecules from living organisms, which were principally comprised of carbon.
Benzene was first isolated from the by-products left after making “portable gas,” which was rendered from fish or whale blubber to fuel lamps. In time, chemists learned to derive benzene and other carbon-rich materials from coal tar instead, a project central to Hoffman’s work and that of his student, Charles Mansfield.
A lump of coal, while not alive, is evidence of life once lived. It is the long-sequestered remnants of ancient flora, from an epoch when trees could grow as tall, or taller, than the one Hoffman had conjured. Coal tar, though, is what remains after human extraction and use. Wherever cities gasified coal to light 19th-century streetlamps, or converted coal into hotter-burning coke to smelt metals, coal tar piled up as waste. Hoffman, curious of its composition, set out to characterize it.
But when organic chemistry adopted coal tar as its primary feedstock, it wedded itself to the residues of industrialization. And so the field became one step removed from the thrum of life that had first inspired it.
By the Benzolfest, organic chemistry was high technology, and the German Empire its Silicon Valley. The field was “the earliest pure science to have a massive impact on technology and on a national economy,” noted the historian Alan Rocke. Germany’s prowess in chemical manufacturing would embolden its nationalistic ambitions. But the war such nationalism inspired would seed the downfall of the German chemical empire, and the rise of a new kind of chemical industry on American soil.
* * *
The original chemical products plucked from the benzene tree had a very specific application: They were textile dyes, what turns humdrum fabrics into every hue imaginable. The first colorfast dye synthesized from coal tar had been an accident. A failed attempt by one of Hoffman’s students to make quinine—a hard-to-source malarial drug derived from the South American fever-bark tree—led to the discovery in 1856 of a dye that could permanently stain silk a rich, red-purple mauve.
Hoffman was based in England, and it was his English student William Perkin who happened into the colorant. Perkin left the lab against Hoffman’s counsel to build a dye-works instead. His factory marked the first attempt to do organic chemistry at scale, wrote Simon Garfield in his history of this world-changing invention. Eventually, after multiple failed attempts, Perkin developed the multistepped process, and proceeded to the equally difficult task of convincing an established, but reluctant, textile industry to adopt his industrial dye.
In time, Garfield noted, a new class of chemists engineered the radiant color palette of Victorian fashion from the dregs of Europe’s Industrial Revolution. Dye-works sprouted across Europe, clustering along Germany and Switzerland’s swift rivers. In time, these ran foul—discolored and odorous. Neighbors complained. Researchers took note. Governments acted, but the coal-tar industry bloomed all the same.
By the close of the 19th century, German dyestuffs dominated the world market, though the first effects of acute exposures were already evident among the earliest generations of dye workers. By 1897, the term chloracne appeared in the German literature to describe a condition unique to chemical workers, “an industrial leprosy,” where the skin is pocked with painful, disfiguring lesions. Not long after, medicine documented new “aniline (dye) tumors” and “dye-workers cancer.” In time, chemists realized benzene, too, was more hazardous in factory quantities than in those used in a laboratory.
And yet, coal-tar chemistry flourished, spreading in multiple directions at once. Chemical innovation led to new dyes, and also to new drugs like aspirin, a synthetic version of a molecule once sourced from willow bark. New classes of pharmaceuticals followed. In time, coal-tar drugs transformed health care.
But coal-tar drugs and coal-tar chemicals thereafter diverged, at least in the popular imagination and in the eyes of states trying to manage these burgeoning technologies. To this day, and despite recent reforms to U.S. policy, drugs and industrial chemicals fall under the purview of separate laws administered by separate agencies. And they are held to separate standards of safety. Forgotten to history are their common origins and chemical ancestry.
Less than a century after the Benzolfest, scientists, led by Theo Colborn, synthesized the growing body of ecological research into a disquieting discovery. At trace levels of exposure, levels lower than workers’ acute exposures and equivalent to dosed drugs, many of these new classes of organic chemicals (including PCBs) could mimic, block, or disrupt the work of hormones, the biochemical signals that coordinate multicellular life.
* * *
Though Perkin missed the mark, making mauve instead, it was no accident that an English chemist sought a synthetic route to quinine. Malarial drugs were essential to colonial expansion into the tropics, one example of how organic chemicals are influenced by prevailing geopolitics and pressed to do the work of empire.
By the second decade of the 20th century, organic chemicals were moved to the front lines of global conflict. Mustard gas, picric acid, and TNT—all organic molecules—made for unimaginably destructive weapons. After Versailles, even peacetime users of organic chemicals were hailed as patriots and power brokers. Germany paid reparations with dyes, while the United States seized German chemical patents and industrial-plant designs as the spoils of war.
Before the Great War, few U.S. firms had ventured into organic chemicals. Only after fighting began, and the British embargo blocked import of German chemicals, did American companies leap into the unknown of organic-chemicals production.
“Americans keenly felt their dependence on German chemicals,” wrote the industrial historian Kathryn Steen. Mastering how to make them domestically was motivated “partly because of the[se] shortages,” she added, but “primarily because of what [the chemicals] represented to Americans—the seemingly inferior industrial and scientific abilities relative to the enemy and rival.”
After the war, growing the nation’s capacity to manufacture chemicals became a national project. The founder of Hooker Chemical (later responsible for Love Canal, the nation’s first Superfund site) argued this point with regard to chlorine (a gruesome war gas). The same case was made for coal-tar chemicals in 1917 by John F. Queeny, the founder of Monsanto Chemical—the company that two decades later would take PCBs global. Factories that made drugs and dyes from coal, Queeny argued, could easily make the materials required by modern warfare. Ramp up peacetime production of coal-tar chemicals and a fleet of war-ready factories would be lying in wait. It would mark “the declaration of American chemical independence,” Queeny said.
For the general public, though, the new coal-tar chemicals were a harder sell.
Coal-tar chemicals smacked of artifice and, in their use as chemical weapons, seemed abhorrent, not the patriots industrialists had painted them to be. Chemistry had a postwar public-relations problem.
In the same editorial where Queeny equated national security with a robust chemical industry, he also pictured coal-tar chemistry as a thickly crowned tree. Like those that chart ancestry, Queeny’s tree arranged chemical products by family and parentage.
Semet-Solvay widely distributed a similar image. Their “coal products tree” rooted the company’s offerings in the nation’s bituminous coal beds. Along boughs of ammonia, tar, and benzene bloomed hundreds of products—Saccharin! Explosives! Mothballs! Perfume!—all promising mastery over nature’s unyielding cycles, its wild swarms and infestations, its off-putting odors and inconvenient secretions.
The collective conscience still carried residues of the real and symbolic horrors of chemical warfare, even at the height of the roaring ’20s. Chemistry had a spectral presence, and it was evident in popular sentiment and the era’s literature. All Quiet on the Western Front, A Farewell to Arms, and Goodbye to All That were postmortems on chemical warfare. All three were published (or translated) in 1929, the same year PCB production began far from the battlefields of Western Europe in Anniston, Alabama, on the lush, southern limits of Appalachia’s rolling hills.
Anniston was a planned, industrial utopia. But in reality, it was a segregated southern city populated by pig-iron and pipe foundries. PCBs were first made by Anniston’s Federal Phosphorus Company, in a plant that belonged to the son of a Confederate solider, a charismatic entrepreneur and a champion of the South’s revival.
Theodore Swann had gotten his start selling locomotives and later electricity. During World War I, he managed a munitions plant in Anniston, but then founded a factory to refine manganese for the steel industry, and when that failed, phosphorous, which is how Swann finally came into his fortune. His electric furnaces captured a pure form of phosphoric acid, which was sold to make fertilizer, detergent, baking soda and even soda pop.
Swann wasn’t a chemist, but a “boom man in a boomtown,” as one biographer put it, always looking for the next thing. And in the mid-1920s, what was on the rise were cars, American crude, and by extension organic chemicals. The success of these ascendant industries depended on better chemical technologies to make better fuels and better car parts. And Swann had seen it coming.
In the mid-1920s, at the invitation of an oil company, Indian Refining (later known as Texaco), he agreed to manufacture a benzene-based chemical called diphenyl (or biphenyl, in today’s parlance). Indian Refining needed a regular supply of diphenyl, a heat transfer fluid, to improve its oil-refining process. Except only one company made diphenyl, Eastman Kodak, and it was only available in small batches at the prohibitive price of $40 per pound.
The structure of diphenyl was simple: just two fused rings of benzene. But it required an entirely different chemistry, and experience and expertise that were in short supply at the time.
As Spears described in Baptized in PCBs, Swann assigned a team to the difficult (and hazardous) task, and issued them a deadline. Development proceeded by trial and error, with error sometimes resulting in explosions, including one that took off a wall and showered Swann’s men with fire, glass, benzene, and other shrapnel.
They eventually mastered diphenyl, a “magic fluid” that brought the nation to the brink of a new chemical age. Not long after, Swann’s chemists scaled another process to add chlorine onto the diphenyl backbone, creating a molecule so stable, it would travel the globe and accumulate in places surely unfathomable to Swann. Swann had financed the conversion of benzene into biphenyl, and now biphenyl into PCBs. The Anniston Works would soon produce 3,000 pounds of PCBs per day.
Production levels climbed higher by the year, as PCBs were put to the project of nation-building. They erected the bases and surveillance equipment that protected the new world order, while at home, PCBs were built into the schools, offices, and factories constructed to accommodate the postwar boom. PCBs would transform U.S. industry in a matter of decades and global ecology before the century was out.
* * *
In the United States, the first batches of PCBs, made at the close of the 1920s, likely left Anniston for Pittsburgh, home of H.H. Robertson Company. Robertson made prefabricated metal siding and roofing from which to erect factories, smelters, refineries and chemical works. The company, like Indian Refining before them, had asked for technical assistance. They needed a new protective coating for their metal sheeting, one that could prevent erosion and the spread of fire better than Halowaxes, or chlorinated naphthalene, one of the earliest classes of industrial halocarbons produced in the United States.
PCBs were unusually durable and durably useful. They were heat-resistant, non-conductive, and excellent as a weatherproof and fireproof coating. And because they were a value-added waste product, PCBs were economically viable.
Flame-resistant factory parts may seem like an insignificant side note, but Robertson’s Protected Metals, later called Galbestos, were instrumental in growing the nation’s manufacturing sector in size and scale. Factories were changing. Small, clustered, brick structures gave way to multistory, steel Goliaths. And these needed to withstand the elements, and to contain the fire and explosion risks that went along with the 20th century’s new methods for transforming nature’s resources into the mass-made materials of modern societies.
Four days after the stock-market crash of 1929, Swann’s firm filed a patent for PCB-laced transformer oils. Westinghouse and GE, like Robertson, would soon find PCBs indispensable, wrote Spears.
But despite these early successes, problems were mounting in Anniston. Within the first years of PCB production, those handling PCBs developed the same chloracne and other debilitating symptoms as dye workers a generation earlier. Three workers at Halowax who handled PCBs died from acute yellow atrophy of the liver (extreme jaundice), fatalities that were studied, but ultimately cast aside by company officials.
And though Swann’s company had weathered the worst years of the Depression, it wouldn’t survive the decade. Roosevelt’s National Recovery Administration had issued economic directives that, as Swann put it, leveled southern industry like Sherman’s March to the Sea. And he had been a profligate spender—living high in the Birmingham hills. But with shifting costs, and loans coming due, Swann went under. His rise to fortune and influence had been fast and somewhat famed. (He had even been profiled in Forbes.) His decline was equally precipitous. Swann sunk into debt, losing his mansion, and all claims to his many factories and technologies.
Queeny’s coal-tar drug company, Monsanto, was also flush from lucrative contracts with Coca-Cola, who bought their caffeine, vanillin, and saccharin. Monsanto took over PCB production in 1935, securing government contracts, growing sales through World War II, and shielding all along the suite of chemicals from regulation until the mid-1970s.
Chemical production in these intervening years soared, owning to the fact that U.S. firms switched their substrate from coal tar to the by-products of the new, advanced crude refineries. In the 1950s, Socony-Vacuum (later ExxonMobil) published the petroleum tree, emblematic of the flowering of American petrochemistry, at the time a “uniquely American phenomenon,” as industrialist Peter Spitz put it.
Two years later, in 1959, Goodrich-Gulf published the rubber tree, as symbolic to this era as Hoffman’s tree had been in his time. Advances in chemical engineering had obviated the need for natural latex, making rubber from oil rather than rubber trees. Drugs, sweeteners, flavorings, fertilizers, fabrics, and furnishings now all had synthetic equivalents. Chemistry, it seemed, had freed humanity from nature altogether.
When chemical trees finally disappeared from popular culture, what was lost was any connection of organic chemicals to their fossil-fuel roots, and of greater significance, to their molecular basis in carbon and the chemistry of life.
* * *
If you bore into the fat-rich bark of a thick-trunked tree, you’ll likely find PCBs, same as you’d find in deep-ocean amphipods, which makes trees like shrimp, and shrimp like us. PCBs are thought to be present in detectable levels in every person on the planet. Though everywhere, the implications are distributed unevenly. PCBs can concentrate, creating hot spots, including in Anniston and other factory towns and regions of the Arctic and subarctic, with significant implications for the indigenous communities living there.
Despite national and international curbs on their production, PCBs now congregate in the deep ocean, raising new concerns. In some areas of the Mariana, PCB levels registered 50 times higher than those found in crabs living in surface waters near heavy industry in China.
By the time the international community stepped in to end global PCB production, well over 1 million metric tons (about 3 billion pounds) had been manufactured worldwide. The 2001 UN Stockholm Convention on Persistent Organic Pollutants that resulted from these negotiations, but which the United States has yet to ratify, initially named DDT, PCBs, and 10 other chemicals (or classes of chemicals), all based on benzene.
“The challenge moving forward is to determine the physiological consequences of such contamination and understand knock-on effects on ecosystem function,” Jamieson and his colleagues concluded in the pages of Nature Ecology and Evolution. Except human activity may be altering the chemistry of the deep before we have had the chance to document it.
The problem is not limited to PCBs, but extends to the larger family of organohalogens to which they belong. In the early 1970s, PCBs were replaced by other organohalogens, one being polybrominated biphenyls, or PBBs, molecules similar to PCBs, but made with bromine instead of chlorine. Soon after their introduction, PBBs got into cattle feed, poisoning the food supply and the people of Michigan, and saddling the state with a long-lived legacy. PBDEs, polybrominated diphenyl ethers, followed as yet another alternative. These were used as flame retardants for two decades before a subsequent generation of Swedish scientists charted rising levels in breast milk.
Besides PCBs, amphipods living in the Mariana also harbor PBDEs, though the most prevalent commercial mixtures have been phased out of U.S. production and named to the UN Stockholm Convention as well.
The deep now archives nearly a century of chemical innovation, and documents the rise and fall of chemical classes, which industry develops and retracts in waves without seeming to absorb the larger lesson.
* * *
There are those who want a bright line to divide what’s natural from what isn’t as a means to make clear what’s safe. But with their origins in Earth’s deep carbon, and their enduring presence in life forms everywhere, such distinctions are murky at best. And yet, PCBs are part of a post-natural state in which industrial chemistry and ecology have become one and the same.
“Nature,” the organic chemist Pat Costner reminded me, “is a chemist, too” and the world its roiling, bubbling, reactive laboratory. Science has only begun to grapple with the complexity of our overlapping chemistries. Costner trained in organic chemistry at the peak of the chemical age. She took her first job as a bench chemist with Shell Oil in the 1960s, though she didn’t stay long. She turned her attention to organic pollution, particularly the chemical by-products industry never intended to make but released into nature anyway.
Her specialty became the family of benzene-based compounds called dioxins, perhaps the most poisonous products of the benzene tree. Some PCBs, she reminded me, are dioxin-like in their toxicity profile. And like dioxins, PCBs can also be inadvertent by-products, made incidentally during the manufacture of titanium dioxide, for example, or a shade of yellow organic inks and dyes reminiscent of the letters that spell out Spam. Try though we may, control over chemistries this complex is something of a chimera.
“We have been so clever at learning to play with atoms and molecules without ever thinking about what they do once they are out,” she told me. “Put a complex molecule into the environment,” said Costner, “and it is going to undergo any number of transitions in hard-to-predict ways.”
The same is true when human-made molecules interact with the exquisite biochemistry of our bodies.
As the biologist Sandra Steingraber explains, the organs of the human system are designed to “shuttle around, break apart, recycle, and reconstruct carbon-containing molecules,” work orchestrated by enzymes and hormones. If carbon molecules come with add-ons like chlorine and bromine, the chemical makeup can influence whether the body stores the molecules, metabolizes them, renders them benign, or makes them inherently more dangerous.
But regulating organic chemicals for their biological activity has been political and controversial because carbon is not just the basis of our biology, but also deeply embedded in our economy.
One hundred and fifty years ago, when Hoffman and his contemporaries gathered to celebrate benzene, humanity’s collective relationship to carbon and nature had already begun to shift. Chemistry, which once mimicked nature’s molecules, had begun to manipulate them. All the more striking, then, is Hoffman’s fantastical tree. Just two years before his death, he rooted chemistry to the Earth. However high chemists might climb, however much industry might harvest, chemistry was grounded in the laws of nature. What goes around comes around. Nature travels in cycles.
But those in attendance that cold March eve, a decade before the new century dawned, could not have known that the branch of chemistry they honored—founded to study living matter!—would spawn an industry so prolific as to irreparably alter the chemistry of life itself.
The chemical innovations of Hoffman’s era would get swept into global conflict—and the American impulse toward maximum and unfettered production—casting PCBs and other fruits of the benzene tree far and wide, into the ocean deep and possibly the depths of time.

