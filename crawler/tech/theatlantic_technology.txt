
It took a terrorist attack for Google to enter the news business.
On September 11, 2001, after hijackers crashed two commercial jets into the World Trade Center as well as a third plane into the Pentagon and another into a field near Shanksville, Pennsylvania, internet users turned to the search engine for information. Again and again, they typed in terms like “New York Twin Towers,” but found nothing about what had happened that morning. Google’s web crawlers hadn’t indexed “Twin Towers” since the month before, which meant every result that Google returned was, given the context, totally and painfully irrelevant.
Google quickly set up a special page for “News and information about attacks in U.S.” with links to the websites of about four dozen newspapers and news networks, along with links to relief funds, resources, and phone numbers for airlines and hospitals. A link to this makeshift news page stayed there for weeks, just below the search bar on Google’s minimalist homepage. Within a year, Google had incorporated a news filter into its search algorithm so that timely headlines appeared atop a list of search results for relevant keywords.
A new era of personalized news products began, in earnest, as a reaction to horrific global news.
Today, a Google search for news runs through the same algorithmic filtration system as any other Google search: A person’s individual search history, geographic location, and other demographic information affects what Google shows you. Exactly how your search results differ from any other person’s is a mystery, however. Not even the computer scientists who developed the algorithm could precisely reverse engineer it, given the fact that the same result can be achieved through numerous paths, and that ranking factors—deciding which results show up first—are constantly changing, as are the algorithms themselves.
We now get our news in real time, on demand, tailored to our interests, across multiple platforms, without knowing just how much is actually personalized. It was technology companies like Google and Facebook, not traditional newsrooms, that made it so. But news organizations are increasingly betting that offering personalized content can help them draw audiences to their sites—and keep them coming back.
Personalization extends beyond how and where news organizations meet their readers. Already, smartphone users can subscribe to push notifications for the specific coverage areas that interest them. On Facebook, users can decide—to some extent—which organizations’ stories they would like to appear in their news feeds. At the same time, devices and platforms that use machine learning to get to know their users will increasingly play a role in shaping ultra-personalized news products. Meanwhile, voice-activated artificially intelligent devices, such as Google Home and Amazon Echo, are poised to redefine the relationship between news consumers and the news.
While news personalization can help people manage information overload by making individuals’ news diets unique, it also threatens to incite filter bubbles and, in turn, bias. This “creates a bit of an echo chamber,” says Judith Donath, author of The Social Machine: Designs for Living Online and a researcher affiliated with Harvard University ’s Berkman Klein Center for Internet and Society. “You get news that is designed to be palatable to you. It feeds into people’s appetite of expecting the news to be entertaining ... [and] the desire to have news that’s reinforcing your beliefs, as opposed to teaching you about what’s happening in the world and helping you predict the future better.”
As data tracking becomes more sophisticated, voice-recognition software advances, and tech companies leverage personalization for profit, personalization will only become more acute. This is potentially alarming given the growth of websites—news-oriented and otherwise—inhabiting the political extremes, which on Facebook are easy to mistake for valid sources of news. When users can customize their news, and customize to these political and social extremes, civic discourse can suffer. “What’s important is how people use the news to have a discussion,” says Donath. “You may have friends or colleagues, and you read the same things in common. You may decide different things about it. Then you debate with those people. If you’re not even seeing the same news story, it leaves you with a much narrower set of people with whom you share that common ground. You’re losing the common ground of news.”
Information-filtering algorithms, whether those of tech giants or news organizations, are the foundation of personalization efforts. But journalists and technologists approach this info-filtering environment in fundamentally different ways. News organizations share information that is true and hopefully engaging. Technology companies like Google and Facebook enable the sharing of information that is engaging and hopefully true. Emerging technologies will only exacerbate the existing problems with algorithmically promoted junk information.
Still, algorithms have a place in responsible journalism. “An algorithm actually is the modern editorial tool,” says Tamar Charney, the managing editor of NPR One, the organization’s customizable mobile-listening app. A handcrafted hub for audio content from both local and national programs as well as podcasts from sources other than NPR, NPR One employs an algorithm to help populate users’ streams with content that is likely to interest them. But Charney assures there’s still a human hand involved: “The whole editorial vision of NPR One was to take the best of what humans do and take the best of what algorithms do and marry them together.”
In an Inside NPR blog post about the editorial ethics driving NPR One’s personalization (cowritten by Charney, chief digital officer Thomas Hjelm, and senior VP of news and editorial director Michael Oreskes), the so-called secret sauce behind the app is “an editorially responsible algorithm.” Metrics track listener behavior so that, over time, the app can offer content catered to individual preferences. Charney declined to describe exactly what data the NPR One app collects—“We’re a little proprietary,” she says—but she gave some examples of how the algorithm personalizes NPR content.
For instance, NPR One knows when you stop listening, which in the future can help producers decide how to keep listeners interested. It can also tell which listeners heard a story that later had a correction appended to it, and deliver that correction to the top of those listeners’ queues. In at least one case, when a correction was significant, NPR One’s algorithm determined who had heard the original segment. NPR then emailed the correction to that list of users.
NPR One can apply that same principle to multipart stories. If a listener misses the first or second part of a story, the app will be sure to offer the missing part to that listener, something those who listen to NPR on the radio often might miss. “Nobody thinks that’s what personalization algorithms are for,” Charney says. “But we can counter both the filter bubble and we can counter false narratives this way.”
Important news stories—both local and national—are presented to all users, with no options for personalization; the app will always provide the lead story of the day and other important stories selected by editors. So while NPR One enables listeners to choose the “nonessential” stories that are more particular to one’s interests—music reviews, for example, or stories about sports or interviews with artists—and decide on the level of depth they hear on certain topics, dialing up or down the frequency of updates, human editors still ultimately decide what you need to hear.
“You may not be interested in Syria. We’ll tell you if this big thing happened and you need to know about it, but we’ll spare you from the incremental news,” Charney says. “The ability to skim across some stories and to dive into other stories, that may be the power of personalization.”
The skimming and diving Charney describes sounds almost exactly like how Apple and Google approach their distributed-content platforms. With Apple News, users can decide which outlets and topics they are most interested in seeing, with Siri offering suggestions as the algorithm gets better at understanding your preferences. Siri now has help from Safari. The personal assistant can now detect browser history and suggest news items based on what someone’s been looking at—for example, if someone is searching Safari for Reykjavík-related travel information, they will then see Iceland-related news on Apple News. But the For You view of Apple News isn’t 100 percent customizable, as it still spotlights top stories of the day, and trending stories that are popular with other users, alongside those curated just for you.
Similarly, with Google’s latest update to Google News, readers can scan fixed headlines, customize sidebars on the page to their core interests and location—and, of course, search. The latest redesign of Google News makes it look newsier than ever, and adds to many of the personalization features Google first introduced in 2010. There’s also a place where you can preprogram your own interests into the algorithm.
Google says this isn’t an attempt to supplant news organizations, nor is it inspired by them. The design is rather an embodiment of Google’s original ethos, the product manager for Google News Anand Paka says: “Just due to the deluge of information, users do want ways to control information overload. In other words, why should I read the news that I don’t care about?”
That is a question news organizations continue to grapple with. If reactions to The New York Times’ efforts to tailor news consumption to individual subscribers are any indication, some people do want all the news that’s fit to print—and aren’t sold on the idea of news personalization.
The Times has recently introduced, or plans to do so later this year, a number of customization features on its homepage involving the placement of various newsletters and editorial features—like California Today, the Morning Briefing, and The Daily podcast—that depend on whether a person has signed up for those services as well as readers being able to choose prioritized placement of preferred topics or writers. Soon, the biggest news headlines may still dominate the top of the homepage, but much of the surrounding content will be customized to cater to individuals’ interests and habits.
The Times’ algorithm, drawing from data like geolocation, will make many of these choices for people. A person reading the news from, say, India might see news relevant to the Indian subcontinent in a more prominent place online than a person reading from New York City. The site already features a “Recommended for You” box, listing articles that you haven’t yet read, also including those suggestions in emails to some subscribers.
The then-public editor Liz Spayd discussed the changes in a March column, noting that she’d heard from several readers unhappy with the newspaper’s efforts to offer a more unique reader experience, and to document and share subscribers’ activity with them. “I pay for a subscription for a reason: the judgment and experience of the editors and writers that make this paper great. Don’t try to be Facebook ... Be The New York Times and do it right,” commented one reader.
“Don’t try to be Facebook” was a common refrain among the commenters. The social network has had its fair share of issues with news curation in its attempts to become “the best personalized newspaper in the world,” as CEO Mark Zuckerberg put it back in 2013. To say nothing of the fake news that proliferates on users’ news feeds, the “trending topics” section had a very rough few months in 2016. First, news “curators” were accused of bias for burying conservative news stories; then, Facebook laid off the entire editorial staff responsible for writing descriptions of items appearing in the section, with some disastrous results, such as when a made-up story—claiming Megyn Kelly was fired from Fox News for being a supporter of Hillary Clinton—showed up at the top of the “trending” list. The story appeared on the blog USPostman.com, a website registered to an address in Macedonia, known for its robust network of information scammers, PolitiFact reported at the time. In January of this year, Facebook gave up on personalized trending topics altogether, filtering topics by users’ geographic regions rather than interests.
Even more troubling than Facebook’s trending-topics woes was the revelation in September that the social network had sold upwards of 3,000 ads—totaling at least $100,000—to a Russian firm connected to the spread of pro-Kremlin propaganda and fake news. The firm, posing as Americans in a myriad of groups and pages, sought to target U.S. voters during the presidential campaign and, while most of the ads didn’t specifically reference the election or any candidates, they “appeared to focus on amplifying divisive social and political messages across the ideological spectrum,” wrote Alex Stamos, Facebook’s chief security officer, in a blog post. The fact that the topics of the ads were so wide-ranging—varying from immigration and gun rights to the LGBT community and Black Lives Matter—is suggestive of how damaging personalization can be and how it isn’t confined to any particular party line. Soon after, Twitter announced it had found and suspended about 200 accounts linked to Russian operatives, many of whom were identified as the same ad buyers active on Facebook.
Meanwhile, in May, Google briefly tested a personalized search filter that would dip into its trove of data about users with personal Google and Gmail accounts and include results exclusively from their emails, photos, calendar items, and other personal data related to their query. The “personal” tab was supposedly “just an experiment,” a Google spokesperson said, and the option was temporarily removed, but seems to have rolled back out for many users as of August.
Now, Google, in seeking to settle a class-action lawsuit alleging that scanning emails to offer targeted ads amounts to illegal wiretapping, is promising that for the next three years it won’t use the content of its users’ emails to serve up targeted ads in Gmail. The move, which will go into effect at an unspecified date, doesn’t mean users won’t see ads, however. Google will continue to collect data from users’ search histories, YouTube, and Chrome browsing habits, and other activity.
The fear that personalization will encourage filter bubbles by narrowing the selection of stories is a valid one, especially considering that the average internet user or news consumer might not even be aware of such efforts. Elia Powers, an assistant professor of journalism and news media at Towson University in Maryland, studied the awareness of news personalization among students after he noticed those in his own classes didn’t seem to realize the extent to which Facebook and Google customized users’ results. “My sense is that they didn’t really understand ... the role that people that were curating the algorithms [had], how influential that was. And they also didn’t understand that they could play a pretty active role on Facebook in telling Facebook what kinds of news they want them to show and how to prioritize [content] on Google,” he says.
The results of Powers’s study, which was published in Digital Journalism in February, showed that the majority of students had no idea that algorithms were filtering the news content they saw on Facebook and Google. When asked if Facebook shows every news item, posted by organizations or people, in a users’ newsfeed, only 24 percent of those surveyed were aware that Facebook prioritizes certain posts and hides others. Similarly, only a quarter of respondents said Google search results would be different for two different people entering the same search terms at the same time.
This, of course, has implications beyond the classroom, says Powers: “People as news consumers need to be aware of what decisions are being made [for them], before they even open their news sites, by algorithms and the people behind them, and also be able to understand how they can counter the effects or maybe even turn off personalization or make tweaks to their feeds or their news sites so they take a more active role in actually seeing what they want to see in their feeds.”
On Google and Facebook, the algorithm that determines what you see is invisible. With voice-activated assistants, the algorithm suddenly has a persona. “We are being trained to have a relationship with the AI,” says Amy Webb, founder of the Future Today Institute and an adjunct professor at New York University Stern School of Business. “This is so much more catastrophically horrible for news organizations than the internet. At least with the internet, I have options. The voice ecosystem is not built that way. It’s being built so I just get the information I need in a pleasing way.”
Webb argues that voice is the next big threat for journalism, but one that presents news organizations with the opportunity to play an even greater role in people’s everyday lives. Soon, we likely will be able to engage with voice-activated assistants such as Siri and Alexa beyond just asking for the day’s news. We’ll be able to interrupt and ask questions—not just in order to put things in context and deepen our understanding of current events, but to personalize them. To ask, “Why should this matter to me?” or even, “What’s the most important news story of today—for me?”
Today, you can ask the Amazon Echo to read you the news—a bit like the way radio broadcasters simply read straight from the newspaper when radio was in its infancy. But technologists, journalists, and scholars believe that in the near future, artificially intelligent voice-activated devices will offer a genuinely interactive and personalized news experience. “Maybe I want to have a conversation with The Atlantic and not USA Today, so I’m willing to pay for that,” Webb says. “This has to do with technology but also organizational management because suddenly there are like 20 different job titles that need to exist that don’t.”
The Echo’s Flash Briefing comes with preloaded default channels—such as NPR, BBC, and the Associated Press—already enabled, but it’s “very much on the consumer to decide” what they want to hear from the Echo, says Amazon spokeswoman Rachel Hass. Any web developer can include a site in the Flash Briefing category the Echo dips into for the news, but being selected as a default outlet by Amazon gives news organizations a huge competitive advantage. Research shows that most people don’t change default settings on their phones, computers, and software—either because they don’t want to, or more likely, they don’t know how to.
Much like a search engine, Amazon isn’t focused on differentiating material from various sources or fact-checking the information the Echo provides. The Echo does, however, read a quick line of attribution during news briefings. “As Alexa reads out your Flash Briefing, she attributes each headline or news piece by saying ‘from NPR’ or ‘from The Daily Show,’” Hass explains. There’s also tremendous incentive for news organizations to play nice with Amazon as a way to get cemented into the device’s default news settings—a relationship that evokes the damaging dependency newsrooms have on Facebook for traffic.
Because Flash Briefings aren’t limited to traditional news outlets, you could conceivably find briefings available from all kinds of sources—including full-fledged newsrooms and individuals. Even former Vice President Joe Biden now delivers daily news briefings, introducing various news articles of his choosing, which are available on Google Home as well as the Echo.
“There are already more than 3,500 Alexa Flash Briefing” skills, the term Amazon uses for the app-like command-driven programs created by developers to use on the Echo. For example, there’s the skill Trump Golf, which offers updates on President Trump’s golf outings whenever prompted by the command, “Alexa, ask Trump Golf for an update.”
“I suspect these devices are the most important thing to emerge since the advent of the iPhone in 2007,” says Kinsey Wilson, former editor for innovation and strategy at The New York Times, “because they open up spaces—principally in the home and in the car—where it allows for a higher, more informal degree of interaction.”
In some ways, voice seems like a natural extension of search. Devices like the Amazon Echo and Google Home will enable people to dip into search engines without having to type. More than that, though, these new devices are meant to be conversational. “It’s not so much asking them a bunch of questions but having a collaborative exploration of some topic,” says Alex Rudnicky, a computer-science professor at Carnegie Mellon University. “This idea of, ‘Wouldn’t it be really nice if you could call up a friend of yours who is very knowledgeable and just have a conversation with them?’”
The personalization element isn’t just the heightened sense of camaraderie one might feel with a conversational robot versus a stack of broadsheets or a talking head on cable television. Personalization is rooted in the fact that devices like the Echo actively learn about the human user with every new interaction and adjust their behavior accordingly. This is the same personalization technique used by Google and Facebook—slurp up myriad individual data, then tailor services to suit—but it uses devices that are always listening, and therefore always learning.
Media organizations that want to create conversational news products for voice-activated devices will have to figure out how to produce and package entirely new kinds of stories, perhaps including advanced tagging systems for snippets of those stories, and be sure their methods integrate with the operating systems these devices use. “The existing SEO methods that we have might need to be rethought completely from scratch,” says Trushar Barot, a member of the digital-development team at BBC World Service. “There may be new methods that emerge that are native to voice recognition.”
Personalized voice assistants face potential obstacles. Sounding too much like a machine is one problem; sounding too much like a human is another. “It’s very easy for people, psychologically, to start anthropomorphizing the device into a real entity and developing genuine human feelings about it,” says Barot. “Plus, the fact that it’s a device that’s in their home and it’s learning more and more about their lives and potentially becoming much more intelligent about proactively offering you suggestions or ideas. That brings up challenging ethical issues.”
News organizations’ use of voice interfaces raise a host of ethical concerns related to data collection, privacy, and security. We don’t know precisely what data these devices collect about individuals (few people read company privacy policies) but, if smartphones have taught us anything, the rough answer is: everything they possibly can. And there’s not an easy answer to who, exactly, owns this data, but one thing’s for sure—it’s not (just) you. This data has immense value, not just to those generating, capturing, and analyzing it, but to a wide range of companies, tech giants and otherwise.
So what do newsrooms do with audience data? “There are potentially ways for newsrooms to use that personalization [data] in a useful way,” says the Berkman Klein Center’s Donath. It largely depends “on what you think the mission of the newsroom is. Is it to inform people as well as to possibly have its own model of what’s important information that people should be aware of? Or is it much more of an entertainment model?” If the latter, that audience data is incredibly valuable for organizations to make sure they’re creating and distributing the type of content people want each day.
Amazon is considering offering developers raw transcripts of what people say to the Echo, according to a July report in The Information. Newsrooms will have to grapple with whether it’s ethical to use data from those transcripts as a way to make money, a move that would certainly enrage some privacy-minded consumers. For publishers, that could be an important revenue stream, but it could also creep audiences out and lessen trust, not enhance it.
What happens to a person’s perception of information, for instance, if the same voice some day is reading headlines from both Breitbart and The Washington Post? “What does that do to your level of trust in that content?” Barot asks. Plus, “there is a lot of evidence that people inherently trust or believe content or news or information shared by their friends. So if this is a similar type of dynamic that’s developing, what does that do for newsrooms?”
Loss of a sense of sources is a big issue, according to Donath: “What’s useful is knowing where something comes from. Depending on what your perspective is, it can cause you to believe it more or believe it less. When you see everything in this generic feed, you have no idea if it’s being reported by something right-leaning or left-leaning. In a lot of ways, the entire significance of what you’re reading is missing.”
These concerns certainly aren’t unique to voice technology. There’s reason to worry that personalization will only exacerbate existing trust issues around news organizations given the gaping partisan disparity found in a September Gallup survey on Americans’ trust in mass media. Though Democrats’ trust and confidence in the media has actually jumped to the highest level it’s been in the past two decades, from 51 percent in 2016 to 72 percent this year, the opposite can be said for Republicans: Only 14 percent of Republicans have a great or fair deal of trust in the mass media, which ties with 2016 as a record low in Gallup’s polling history.
Although some newspaper readers might like being greeted by name each time a major news organization sends a daily roundup of stories, news organizations run the risk of sounding inauthentic, the way campaign emails from politicians seem impersonal despite their attempts to the contrary.
According to Powers, news organizations should share with audiences that the content they’re seeing may not be the same as what their neighbor is seeing—and how they can opt out of personalization. “There needs to be more transparency about what data they’re actually collecting, and how people can manually turn [personalization efforts] on or off or affect what they see,” says Powers.
Perhaps most importantly, it’s essential for news organizations to remember that they can’t leave personalization up to algorithms alone; doing so will likely only narrow people’s news consumption rather than expand it, and could lead to the spread of misinformation. “You still need to have an actual human editor looking to make sure that what’s popular isn’t bogus or hurtful,” says Powers.
Personalization should be a way to enhance news decisions made by human editors, professionals committed to quality journalism as a crucial component of an open society. The news-filtering algorithms made by companies that refuse to admit they are even in the media business—let alone in a position to do great harm—aren’t bound to even the most basic journalistic standards. And yet they are the dominant forces filtering the world around us in real time.
This post appears courtesy of Nieman Reports.
Eryn Carlson contributed reporting.


Emma Perrier spent the summer of 2015 mending a broken heart, after a recent breakup. By September, the restaurant manager had grown tired of watching The Notebook alone in her apartment in Twickenham, a leafy suburb southwest of London, and decided it was time to get back out there. Despite the horror stories she’d heard about online dating, Emma, 33, downloaded a matchmaking app called Zoosk. The second “o” in the Zoosk logo looks like a diamond engagement ring, which suggested that its 38 million members were seeking more than the one-night stands offered by apps like Tinder.
To hear more feature stories, see our full list or get the Audm iPhone app.
She snapped the three selfies the app required to “verify her identity.” Emma, who is from a volcanic city near the French Alps, not far from the source of Perrier mineral water, is petite, and brunette. She found it difficult to meet men, especially as she avoided pubs and nightclubs, and worked such long hours at a coffee shop in the city’s financial district that she met only stockbrokers, who were mostly looking for cappuccinos, not love.
It was a customer who had caused Emma’s heartache, two months earlier. Connor was one of London’s dashing “city boys,” and 11 years her junior. He had telephoned her at work to ask her on a date, which turned into an eight-month romance. They went night-fishing for carp near his parents’ home in Kent, where they sat holding hands in the darkness, their lines dangling in the water. One day at the train station, Connor told her it wasn’t working; he liked nightclubs more than he liked being in a relationship. When she protested, Connor said that he’d never loved her.
To raise her spirits, Emma huffed and puffed her way through a high-energy barbell class called Bodypump, four times a week. Though she now felt prepared to join the 91 million people worldwide who use dating apps, deep down she did not believe that computers were an instrument of fate. “I’m a romantic,” Emma told me, two years after the internet turned her life upside down. “I love to love,” she said, in a thick French accent. “And I want to be loved too.”
As soon as her dating profile went live, Emma’s phone started to bleep and whistle with interest from strangers. The app allowed her to gaze at a vast assortment of suitors like cakes in a coffee-shop window, but not interact with them until she subscribed. That evening, a private message arrived in her inbox. It was from a dark-haired Italian named Ronaldo “Ronnie” Scicluna, who looked to Emma like a high-school crush. But the text was “floue,” Emma told me, not knowing the English word for “blurred.” The app was holding Ronnie’s message ransom.
That night, Emma FaceTimed her sister and showed her Ronnie’s photos: “Oh my God, look at the guy!” she giggled, as they swiped through his profile pictures. He was boyish yet mysterious, like the kind of dangersome male model who steers sailboats through cologne commercials. But according to his profile, Ronnie was a 34-year-old electrician in England’s West Midlands, just 100 miles away.
Gaëlle, Emma’s twin, lived in France and was married with an 11-year-old daughter. The sisters had gossiped on daily video calls since Emma emigrated to the United Kingdom five years earlier. Emma had to learn English “chop-chop”—as Londoners say—and now she too was ready to meet someone special. Ronnie seemed exciting, so she paid the £25 ($34) subscription to Zoosk.
Ronnie’s message materialized. It said: “You look beautiful.”
A rally followed. Emma discovered that she and Ronnie were two lonely Europeans working blue-collar jobs in England. Charming Ronnie attempted a little French, but when Emma wrote to him in Italian, she was surprised that he didn’t speak it. His mother was English, Ronnie explained, his Italian father spoke English too, “except when he swears.”
Their conversation moved from Zoosk onto WhatsApp, a free messaging app. Each morning on the train to work, Emma sat glued to her iPhone. She wondered how a guy like him was interested in her. “I’m very natural,” Emma said. “I mean, I’m nothing. I’m very simple you know ... so I was flattered.” In her favorite photograph, Ronnie wore a leather jacket that made him look like a pop star. As a teenager, Emma had obsessed over the British boy band Take That. But Ronnie was the opposite of a celebrity; he was down-to-earth.
“You could easily have picked someone else,” Ronnie told her one day.
“No. You’re the only one I wanted to talk to ... I paid because of you,” she replied.
“As soon as I saw your picture I wanted you,” he wrote.
“Makes me happy to know that,” Emma replied.
When four red heart emojis appeared on her screen, Emma was thrilled. Unlike her ex-boyfriend, Ronnie seemed mature and attentive. Ronnie was easy on the eyes, funny, and caring, but there was one problem: He did not exist.
* * *
Ronaldo Scicluna was a fictional character created by Alan Stanley, a short, balding, 53-year-old shop fitter—a decorator of retail stores. Alan lived alone in Stratford-upon-Avon, the birthplace of William Shakespeare. Like one of the Bard’s shape-shifting characters, Alan used a disguise to fool women into romance, and to prevent himself from getting hurt. His alter ego “Ronnie” was a ladies’ man, charming, and attractive—everything Alan was not. “I was in a pretty lonely place,” he told me during an emotional interview. “I wasn’t feeling the most attractive of people, I might say. You know, I always struggled with self-confidence and ... I was going through a messy separation and I was just feeling like I needed somebody to talk to.”
When his marriage of 22 years failed, Alan, who has an adult daughter, was devastated and found himself uninterested in the opposite sex. “I’d just had enough,” he explained. For almost a year, he allowed his decorating work to consume him, but boredom set in. Alan wanted to “mix” with new people, he said, but feared public rejection in his close-knit town. Then one day he noticed the online-dating service Zoosk.
Alan elected to bypass the company’s selfie-based verification system, a spokesperson for Zoosk told me, following an internal investigation. He admitted using photographs of a random male model from Google that he had stolen. “I’m always nervous about posting personal images of myself,” he explained. “I just don’t like pictures of me. It goes back a long way, to be honest.” Emma’s profile was the first he saw. He was captivated.
Alan had done it before, at least five times, he admits. He’d become online pen pals with single women from all over the world, but avoided video calls and meetings. He found the thrill of the chase electrifying, with none of the awkward stuff like first dates. Emma was just another mark, and their flirty exchanges were innocent fun, he said. “Catfishing is prevalent across the internet,” he told me, “Everybody does catfishing.”
Catfishing was added to the Merriam-Webster dictionary in 2014. It refers to a person who creates a fake social-media profile, usually with the goal of making a romantic connection. The term was coined during a 2010 documentary, Catfish, when a subject told a story about the journey of live cod from the United States to China. Apparently, to prevent the cod from becoming lazy and their flesh turning to mush, seafood suppliers add to the tanks their natural enemy, the catfish. A predator creates excitement.
Alan was right. Online, catfishing was growing in popularity. “Now you don’t need the imagination of a Tolstoy or Dickens to create a totally believable but fictional identity,” said the cyber-psychologist Mary Aiken, author of The Cyber Effect, “It’s a matter of cut and paste.” The results can be devastating. In 2006, a 13-year-old girl in Missouri was duped into an online relationship with a fake teenage boy created by neighbors. After their online romance soured, Megan Meier committed suicide. By June of this year, catfishing was so prevalent that Facebook announced it is piloting new tools to prevent people from stealing others’ profile pictures, like Alan did.
His flirting with Emma soon progressed from small talk to in-jokes, pet names, and late-night telephone calls. To Emma, his lilting West Midlands accent somehow fit perfectly with the images of the model. In October of 2015, she wrote how happy she had become since “meeting” him.
“Are you not usually happy, stinky?” he asked.
“I am,” she said, “but you changed something.”
They both agreed to delete the dating app. Emma constantly asked for a physical date, but was crestfallen when Ronnie made excuses. This had happened before. Alan knew how to prolong the relationship with a combination of evasion and false promises. He told Emma that decorating new shops took him all over Europe. Any free time was spent drinking whiskey with his father, or on vacation at his parents’ villa in Spain, he said. Maybe one day she could stay in “bedroom three.” Emma just wanted a local dinner—they lived only 100 miles apart.
“It’s hard to keep everyone happy,” Ronnie complained. “Dad loves me working and wants me to keep doing better. Mum wants me to quit. She worries about me. My health. Stress. Dad thinks I handle it well.”
“I think what you need is a [girlfriend] to look after you,” said Emma, before he changed the subject.
“Do you want to know why I started online dating?” she asked him one night. “Because I wanted to ... meet that someone and to start something with that someone ... not to have a broken heart ... which is even more painful when you have never met someone.”
“Me too,” said Ronnie. “We both want the same thing.”
“Give me a date then,” Emma wrote. “I will suit your availability.”
She waited for his reply.
“I don’t think you realize how difficult it is for me to get time off,” he wrote.
“Just a dinner to start with,” Emma begged. “I can do the travel ... then if the connection is really there we will find a way.”
“Do you think it will be there?” he asked.
“I have never been so sure.”
“Do you have faith in us?” he asked.
“It could work perfectly well,” Emma wrote.
“And I love you,” he wrote.
“And I love you too,” she replied.
* * *
Little scientific research exists about catfishing, but experts say that victims tend to be lonely, vulnerable, or missing something in their lives. John Suler, a clinical psychologist and author of Psychology of the Digital Age, said that victims without a real-world social network can overlook what is too good to be true: “It always helps to have friends and family reality-check relationships online,” he said. But Emma had few close friends or family in London. And Emma was looking for love.
Emma met her first boyfriend at age 15. When their high-school romance ended a decade later, she ran away, high into the French Alps, to find seasonal work. She did not find love there, and decided to keep running, this time to England, where she had dreamed of living since visiting as a child. When she arrived, aged 28, there were 127,601 French-born residents in London, and by 2015 that number had doubled, making it the sixth-biggest French city, according to London’s mayor. But the language barrier nearly made Emma quit after two months. “It’s not like the same as you listening to that song in your bedroom when you’re 16,” she said.
She loved talking to Ronnie, whose conversations were full of construction-site bonhomie, British slang, and flirtation. One day, she received a black-and-white modeling photograph of him wearing a tiny pair of Speedos. Emma fired back emojis with laughing faces stained with tears of joy.
“I love that picture thank you,” she replied, “I saved it.”
Alan, who is a fitness fanatic, was now spending his mornings on long-distance runs. Decades of manual labor had kept him fit, but he was resentful about losing his hair at a young age. “In my 30s it started falling out,” he said. “I was exactly like my dad.”
To him, Emma had become not just a friendly voice on the phone, but a project. When he discovered that Emma spent three hours a day commuting to work, Alan encouraged her to find a local job. “I was on her journey in life, trying to guide her,” Alan said.
By January of 2016, Emma was thrilled to receive a job offer three miles from her home at an Italian chain restaurant. As the new assistant manager of Zizzi in Richmond, she managed a team of Poles, Spaniards, and Greeks (there are no real Italians in this story). When Emma boasted about her “long-distance” love, the busboys asked why they’d never met. Emma told them he was “extremely busy.”
Alan was running out of excuses. “It was eating at me because I knew the longer it went on, the more problematic it would become in the long term,” he said. Like Malvolio in Twelfth Night, Alan had donned a ludicrous disguise to win the affections of his Olivia. And in a world where Alan felt ugly and invisible to the opposite sex, Emma showered him in “adoration.” In his mind, Alan minimized his lie: “Everything I told her about me, apart from who I was, and the age, was true.”
One night, after the last customers left Zizzi, Emma closed the restaurant with a popular, baby-faced Spanish waiter named Abraham. As they shut down the huge pizza oven, and packed away the cutlery, Emma revealed how she longed to meet her mysterious boyfriend. Abraham listened for a while, then turned to his manager and said: “But Emma, the guy doesn’t want to meet you ... maybe it’s not even him.”
Emma insisted that they’d talked on the phone.
Abraham said her boyfriend was “probably an old man.”
Then he said he’d heard about an app that could help.
“He could be a psycho,” he added.
Emma was hurt and confused. After Abraham left, she found herself alone in the restaurant. Looking through the window she watched the happy couples walking along the black cobbles of King Street. She longed for the day when Ronnie would appear at Zizzi, sweep her off her feet, and prove them all wrong.
By the spring of 2016, Emma’s family recommended that she cut off all communications with Ronnie. He had refused to meet her after six months, they said. “I didn’t want to listen to them,” Emma said. But one evening after work, she laid on her bed and downloaded to her iPad an app called Reverse Image Search. It is one of many apps that crawls the internet to find the original source of a profile picture.
“Believe me I was scared to use it for the first time,” Emma said. She uploaded the photograph of Ronnie wearing his leather jacket. The results arrived in seconds: The man in the photographs was a model and actor from Turkey, called Adem Guzel. Emma was confused. She found his model-management website, an official Twitter account, and his Facebook. Adem’s closest connection to the United Kingdom was that he had studied at the Gaiety School of Acting in the nearby Republic of Ireland.
“Do you have anything to tell me about Adem Guzel?” she wrote in a text message.
“It is me,” Alan replied, thinking fast. Those were his modeling pictures, he said. He’d once used another name.
“It was a long time ago,” he promised.
Given the opportunity, Alan couldn’t tell the truth. “I would have lost someone that I really treasured,” he told me. But Emma demanded that he reveal himself. FaceTime was “for teenagers,” he said. When she insisted, he yelled: “Stop! Don’t ask me anymore!”
But Emma still wanted to believe in the fantasy, not the truth.
“I couldn’t believe it because, you know ... when you talk to someone every day, and you share your life ... he was my confidente.”
And why would somebody claim to be someone else online?
Julie Albright, a digital sociologist at the University of Southern California, says catfishing can be addictive: “Suddenly finding success with romantic partners online is exciting, and in fact intoxicating for certain people,” she said, adding that catfish often target more than one victim: “Putting several hooks in the water and getting several relationships going is the way to hedge your bets.”
In August of 2016, nearly a year after his and Emma’s relationship began, Alan had computer troubles. He bought a new one, but set it up using his personal email address. When he sent Emma a message, it sounded like Ronnie, but the email address said “Alan Stanley.”
It was his first mistake.
“I lied,” Alan told me. “I said, no, I bought this computer from somebody else and they haven’t changed it yet.”
Emma was now overwhelmed with doubts.
During that summer of 2016, Emma allowed her long-distance relationship to continue as she started what she proudly calls “my investigation.” One day Ronnie sent her a photograph from an aquarium, the fish from Finding Nemo. It was either a False Percula clownfish or a True Percula clownfish—only a saltwater aquarist could tell the difference—but Emma was more interested in uploading it to her app. “This Nemo sent me to TripAdvisor,” she said. It illustrated a review written by “Alan S.”
“I knew,” Emma told me. She typed Alan’s email address into Google.
I asked what she found.
“Everything, everything,” she sighed. “His Twitter accounts. Where I’ve seen his face.”
“It was devastating and I felt sick,” she said. “You have no idea how much I’ve been hurt inside.”
Alan was in early-morning traffic when his cellphone rang.
“Is your real name Alan?” Emma asked.
“No.” he replied.
“But it is, it is, it is!” Emma said, sobbing. Alan accused her of having trust issues.
“Don’t talk to me about trust, Alan Stanley!” Emma yelled. The call, and Alan’s masquerade, was over.
From a quiet corner of a half-decorated shop, Alan called Emma back. “I could not be any more apologetic,” he told me. “I told her everything.” Emma told him she felt like a fool. They both cried. It was, Alan said, a “big error of judgment, the worst and biggest mistake of my life.” But even in his telling of “the truth,” Alan told Emma he was 50, shaving off a few years.
Emma had questions. Was he a pervert? Alan sent her a real photograph of himself, wrinkles and all. “It might sound cruel what I’m going to say,” Emma told me, “but I carried on talking with him, after I knew who he was, only because I wanted to know why he did that to me,” Emma said. “I’m 34 at the time, but maybe another girl, when she finds out, she could maybe go too far, maybe kill herself.” After the big reveal, Emma asked Alan if he wanted to meet her. “I really wanted to go, to end the story,” she said. But was Alan dangerous?
Emma decided that she needed to protect others from his scam. On September 16, 2016, she wrote a Facebook message to the Turkish model:
“Hello Adem, we don’t know each other but a year ago I met a guy online and that man is using your picture and pretends he is you under another name. I wasn’t sure if getting in touch with you was a good idea but I needed you to know, kind regards, Emma.”
* * *
Adem Guzel nearly ignored the message. The shy, 35-year-old model woke up in the Bohemian district of Cihangir, near Istanbul’s famous Taksim Square, suffering from a cold. This was not the first message he had received of this nature. Adem poured a cup of tea in the kitchen of his aparthotel, a type of bed-and-breakfast that had once been popular with travelers, before political instability and terrorist attacks killed off Turkish tourism. He drew a hot bath, undressed, and sank into the water. Maybe it was the head cold, Adem thought, but it was like an invisible person was yelling in his ear: “Pick up the phone!”
Adem toweled off and found his iPhone. Something about the sincerity of Emma’s message stuck in his mind. He wrote back in broken English. “And the conversation just started,” Adem told me, in a gruff, Turkish voice. When he heard how Alan had tricked Emma, Adem was furious. Emma asked him if he wanted to video call.
Emma was on a bus in Richmond when she read the message. She dashed home and showered, with a strange flutter in her stomach. When Adem’s face appeared on her iPhone, Emma was hysterical. “It was crazy,” she said. “I wasn’t sure it was him, I was always in doubt.” But there he was, talking, smiling, nervously running his fingers through his hair. “I never do FaceTiming,” Adem said. “But somehow I wanted to do it with her.”
“You are so real,” Emma said, crying. “You really exist!”
Emma had questions. In English, their shared second language, Adem explained that he had grown up in a coastal Turkish village, then moved to Istanbul and enjoyed a prosperous modeling career. But his plans to become a television actor had stalled when he refused to enter a Turkish reality show, which he said operated on a “casting-couch” basis. Instead, Adem moved into a friend’s deserted aparthotel as a temporary manager.
As they talked, Emma summoned her sister on FaceTime, and showed the iPad to her iPhone. Gaëlle and the Turkish model waved at each other from opposite sides of Europe. After the call, Adem and Emma exchanged text messages, but Adem soon packed his bags and returned to the village whence he came. Şarköy, pop. 17,000, had the cellphone signal of a small Turkish village, and their conversation fizzled out.
* * *
On Friday, November 11, 2016, Alan Stanley stepped off a train at London’s Paddington Station. He strolled to a nearby row of white-pillared Georgian townhouses and checked into the Arbor, a swanky, boutique hotel with views of Hyde Park. That evening, Alan walked out of his hotel, and into the nearby London Hilton, where Emma was nervously waiting in the lounge. She said she needed closure, and to see the truth with her own eyes. Alan “needed to apologize to her face-to-face,” he said.
His face was red with shame. “The hug went on for about a minute,” he told me, “I was just, like, quite tearful.” Emma pulled up an armchair and they sat uneasily side-by-side, making small talk. Then, Alan said he was sorry.
He said he did it to escape the agony of loneliness. When Emma studied him, she saw a man just two years younger than her own father.
Emma and Alan left the Hilton for some fresh air, and strolled along a tree-lined pathway known as Lover’s Walk. In Alan’s telling, they passed Hyde Park’s “Winter Wonderland” where couples were riding a Ferris wheel or whizzing around an ice-skating rink. The walk—20,000 steps, according to his iPhone’s health app—was one of the longest and best of his life.
“We talk, talk, talk,” Emma said. She asked him about drinking whiskey with his father. Was even that true? “He said his dad passed away a few years ago.”
While Alan considered the evening a date, Emma’s memory of the walk was quite the opposite of romance. The park was “empty” she said. Her only memory was pausing at a memorial to the 52 victims of London’s July 2005 bombings.
“It was a perfect night,” Alan said. “She paid for dinner that evening. Italian restaurant in Paddington.”
Alan even insinuated that Emma had stayed the night at his hotel. “As a gentleman I’m very reluctant to talk about this side of it,” he said. Emma flatly denied it.
“I was pleased I met him obviously,” Emma said curtly, “And that was it.”
But that wasn’t it. Emma could not erase Alan from her life. After their meeting in London, they met several times. Just before Christmas of 2016, Alan presented her with a Swarovski bracelet. “She bought me Hugo Boss socks,” Alan told me, “They’re not cheap.”
“It was a relationship that we built ... You develop a friendship, you talk ...” she explained, her voice breaking as she described their toxic relationship. She was helplessly bonded to Alan and he was obsessed with her, high on virtual validation: “She made me feel like I was a teenager again,” he told me.
I wondered if Alan arrived in London hoping that Emma would overlook the difference between him and the model. Maybe his email slipup was just part of a “bait and switch.”
But Emma could tell the difference. “Things started to get a little bit sour between us,” Alan said. “There was a kind of breakdown after Christmas ... her attention suddenly turned more focused toward finding him.” Alan sensed he was competing with the Turkish model for Emma’s affections. He had deleted his fake accounts, and focused his attention on her. Now, he dreaded he would lose her to the man he had unwittingly thrown in her path—an ironic demise worthy of Shakespeare. “I just put two and two together,” Alan said. “I reckoned that they are talking behind the scenes.”
* * *
By January of 2017, the conversation between Emma and Adem had reignited. “I’m not a religious guy,” Adem said, but it felt like fate had pulled them together. They stopped talking about Alan’s scam, and very slowly the conversation between the shy model and Emma, who had so recently been burned, became emotionally charged. But Emma told her sister, Gaëlle, that she felt like she was just starting another long-distance affair. This time, she wouldn’t be played for a fool, and she wouldn’t waste a moment. She invited Adem to London. “It wasn’t to flirt, believe me,” Emma insisted. Adem said yes immediately. He was curious to meet this beautiful French girl, and sure, in London!
On March 31, 2017, Emma sent her catfish a goodbye text message:
“Alan I wanted to tell you that tomorrow I’m going to pick up Adem at the airport. And I still don’t know if it’s good or bad but I’m going to meet ‘my Ronnie.’ You built up all this shit, I’m not sure if I should thank you or detest you for that. But this is happening.”
It was April Fool’s Day, 2017, when Emma stood beneath the giant arrivals board at London’s Heathrow Airport, searching for Adem’s flight. When a lady beside her noticed her shaking hands, Emma explained that she was waiting for a man from the internet, whom she had never met. The woman froze. “You have to be very careful!” She warned, on the internet not everyone is who they say they are.
“Well actually, I know ...” Emma began, but the Turkish passengers were already flooding into the arrivals hall.
“Oh my God, it’s happening,” she thought.
When the crowd parted, she saw him walking toward her in a white T-shirt and a blue cardigan, the man in her photographs, come to life. Adem was taller than she expected, and when he recognized her, she felt breathless. As they hugged in the middle of the airport, Emma thought that he smelled “fantastique.”
In a quiet corner, Emma produced an egg-and-mayonnaise sandwich, which she had bought in case Adem was hungry. When he lifted it to his mouth, she noticed his hands were shaking too. “I was really nervous,” Adem said. They walked into the bitter cold air, and Emma summoned an Uber. It seemed to take forever. Adem was very quiet and there was a nervous energy between them. When he stepped off the curb to look for their car, Adem turned around and found Emma at eye level.
Inexplicably, she kissed him.
“Three minutes later I felt like I know her a long time,” Adem said. The spark was undeniable. She gave him a key to her apartment, and together they discovered the city like tourists, goofing around with a selfie stick. Later, when Adem opened his suitcase, Emma spotted the leather jacket from her favorite photograph, and felt starstruck. And Adem couldn’t believe his luck—his soul mate had appeared in his inbox as if by magic.
On April 23, 2017, their story became a tabloid sensation in England. “My catfish became cupid,” Emma told the Daily Mirror, “And now we’re living happily ever after.” Soon, other victims of Alan Stanley reached out to Emma. One woman from New York said she had been in a relationship with Ronnie for “years.” When the newspapers described Alan as a “love rat,” he endured summits about his behavior with his colleagues and employer, and an “awful” conversation with his daughter.
“These last few months have been beyond stressful,” he told me. “I don’t think I’ve slept properly for three or four months now.” Overwhelmed by shame, he moved to a faraway town. But even Alan felt relieved that the story ended in comedy, not a tragedy.
“I think it’s brilliant Emma and Adem have met,” he said. “It’s almost like fate.” Alan added that he no longer uses fake identities, and has since met someone special, he said, on Twitter: “A European lady, younger than me, younger than Emma.” There is someone out for there for everyone, he added. “I don’t consider myself to be particularly good-looking ... I’m not a David Beckham, or a Tom Cruise, or an Adem Guzel.”
When I spoke to the couple in September of this year, they had been living together in London for six months. “He’s lovely,” Emma said, “He’s a lovely man.” Currently, Adem is chasing his acting dreams in London, and says he recently auditioned for Aladdin, the original, Arabian catfishing story. He read for the lead, a street urchin who uses a genie’s magic to pass himself off as a prince to win over a princess—before realizing that he must be himself.
At home there has been confusion. Emma was making a coffee one day when she looked over and realized: God, this is Adem, not Ronnie. She says Adem is quite different from the gregarious character invented by Alan—he is quiet and sensitive. There are other challenges: Turkey is not yet in the European Union, so Adem can only stay in London for six months at a time, and cannot work. But Emma now admits that the internet is an instrument of fate.
One evening, not long ago, Emma was closing down Zizzi after a busy shift. Night shifts were once her loneliest times, when she would long for “Ronnie” to materialize from the internet and sweep her off her feet. But that night, she noticed Abraham, the disbelieving Spanish waiter, and the rest of the crew, staring at the handsome gentleman waiting in the doorway, ready to take her home.


The latest experiment in a universal basic income will be coming to Stockton, California, in the next year.
With $1 million in funding from the tech industry–affiliated Economic-Security Project, the Stockton Economic-Empowerment Demonstration (SEED) will be the country’s first municipal pilot program. As currently envisioned, some number of people in Stockton will receive $500 per month. That’s not enough to cover all their expenses, but it could help people with rising housing costs, paying student loans, or simply saving for life’s inevitable problems.
Last year, Stockton rents rose more than 10 percent, putting the city’s rental price growth among the top 10 in the nation. This is quite a surprise in what Time called “America’s most miserable city” just three years ago. The average rent remains a modest-by-Bay-standards $1,051, but Stockton has a per-capita income of just $23,046, more than $6,000 less than the U.S. median and a full $8,500 less than the California median. If you made the per-capita income of the city, average rent alone would eat 55 percent of your income.
As the tech boom that began in the mid-00s continues, its financial blast radius keeps expanding. Tech workers have been streaming into the Bay, yet few homes have been built in the Bay Area’s cities. Home prices and rents have exploded. Longtime residents and newcomers alike have been getting pushed ever further out. And in recent years, Stockton—once one of the cheapest cities to live in California—has become the eastmost outpost of the insane Bay housing market.
“There’s not a shortage of housing. There’s a shortage of money to buy housing,” said Fred Sheil, a member of STAND Affordable Housing in Stockton. “Unless you’ve got Bay Area income, they aren’t interesting in talking to you.”
That’s garnered the attention of city leaders, especially Mayor Michael Tubbs, who became the youngest-ever mayor of a medium-sized city when he won a landslide election in 2016. Tall, gregarious, often besuited with a trim beard, Tubbs could become the new face of universal basic income, or as people abbreviate it, UBI.
Stockton won’t be the first UBI project in the Bay (pilots are already in the field in West Oakland and San Francisco), but it would be the first public attempt to show what a basic income can do for people. Unlike the secretive other projects, both the local government and the participants will be reporting what the cash does for them. And the project will be occurring within the context of a regular city government, with all the community engagement that entails.
“The [UBI] conversation is not being had with the people who are going to be impacted,” Tubbs said. “Mark Zuckerberg don’t need $500 a month.”
So, in Stockton, they are planning a six- to nine-month design process to incorporate the city’s residents into the program design, including precisely how the cash stipends will be awarded.
“My bias is that it should go to people who need it the most, but that’s not truly universal. That’s targeted,” he said. “The way our country is now, for something like this to work, everybody has to feel like they are a part of it.”
One idea they’re kicking around is that a specific number of slots would be reserved for what they call their “promise zone” in south Stockton, where they’ve done a lot of existing economic research and development work.
Tubbs, too, approaches the idea of a minimum income from an entirely different place than Silicon Valley’s scions. Most of the tech proponents of UBI have approached the topic through the lens of automation and the massive devaluation of human labor that they think could result from further developments in artificial intelligence. While giving cash to everyone has an egalitarian ring, when the message is delivered by the ultra-wealthy of Menlo Park and San Francisco, it can feel as if UBI is the crumbs being swept off the real-money table to buy off the masses.
But Tubbs referenced a strain of African American thought expressed by no less a leader than Martin Luther King Jr. “The solution to poverty is to abolish it directly by a now widely discussed measure: the guaranteed income,” King argued in 1967. Though Tubbs didn’t mention them, the previous year, the Black Panthers came out with their famous 10-Point Program. And there it is in point number two: “We believe that the federal government is responsible and obligated to give every man employment or a guaranteed income.”
Perhaps it’s not surprising that different black thinkers in the 1960s came to the conclusion that a guaranteed income would be an effective way to fight the poverty that resulted from structural racism. They’d just seen a generation of federal programs make white Americans much, much wealthier, while also seeing how those same policies discriminated against them. The big programs that were created during the New Deal were boxed in by what historian Ira Katznelson calls “the Southern cage.” In exchange for creating socialistic Federal programs, the then-Democrats of the south required policies that would reinforce the racial hierarchy of the country. Black people’s freedom and economic prospects were the bargaining chip that Franklin Delano Roosevelt and the Congresses he worked with slid over to former slave states in exchange for their support of sweeping legislation.
For example, FDR would create the Federal Housing Authority, but segregation and redlining would combine to create disinvestment in increasingly segregated black neighborhoods across the nation. FDR would get Social Security, but many job categories in which black people predominated would be exempted from inclusion. The GI Bill might have helped black people get an education, but they could not take equal advantage of the Veterans Administration housing benefits because of racist real-estate practices. Job and social programs might seem nice, but the experience of what could happen to nice ideas within American bureaucracy might have made simple cash payments seem more racism-proof than the alternatives.
But Tubbs is not a theoretician or activist. He is the mayor of a poor city, and he knows that people in Stockton need money not just to survive, but to try to lever themselves out of the lower-income brackets through education or entrepreneurship.
In preparation for the UBI project, Tubbs had a convening in his old city-council district (where he grew up) in south Stockton with upper-income, middle-income, and poor people.
“We said, ‘What would you do with an extra $500 a month?’” Tubbs said. “One woman said, ‘It’s summer, so that’d be great because my kids are coming back from college and my bills go up. One person said, ‘I’d probably save that up to start a business. One person said, ‘I’d go back to school.’ It wasn’t: ‘I’m gonna buy a TV or a car.’”
For the poorest people in Stockton, it could help them transition from being on the streets into some kind of housing, or from temporary housing into something more permanent. Extra cash could help people stay in their homes, rather than getting evicted. “Don’t get me started talking about Evicted,” he told me, referencing the surprise hit book by Harvard sociologist Matthew Desmond about the lives of poor people in Milwaukee.
“There was one line where he said, ‘Poor black men were locked up. Poor black women were locked out,’” Tubbs said. Locked into prison, locked out of homes from which they’d been evicted.
The lessons of the book hit close to home. He grew up in south Stockton, spending his elementary-school days in Louis Park Estates, a few blocks of nearly identical two-story condos just across the water from Rough and Ready Island. (Yes, that is its real name.)
“I’m not sure why they call it ‘estates.’ It’s a bunch of condominiums with stray cats walking around,” Tubbs jokes. “Growing up, when I'd throw out the trash, I’d toss it and dart because all the cats would come running. That’s why I still don’t like cats.”
On a recent afternoon, there were kids playing in the small and connected front yards, a few older folks perched on plastic chairs. An ancient gentleman in a brown zoot suit that might have been purchased in that cut’s heyday stepped creakily out of a Cadillac. It was closer to idyllic than dystopian, but every window had a set of heavy bars, even the second-story ones. And on one lawn, a family’s possessions were scattered everywhere, around a U-Haul that had been driven up onto the grass. If it was not an eviction, the scene spoke of some kind of hasty retreat.
* * *
Soon, there will be 1,000 more jobs in South Stockton. Amazon recently committed to building a 600,000-square-foot facility in the area.
That’s on top of a million-square-foot facility in a huge and developing logistics hub in Tracy. That’s about 20 minutes down I-205, right at the base of the Altamont Pass, which separates the Central Valley from the East Bay.
Once a sleepy agricultural area, it finds itself a logistics hub for dozens of companies. The wealthy Bay Area is nearby. There is great highway access. The Port of Oakland is through the pass. The land is cheap. And most importantly, the companies want to access “a laborshed” that extends outside the Bay.
A single developer, the logistics-focused real-estate investment trust Prologis, is developing 1,800 acres next to existing facilities for Costco and Safeway. Their first big lease went to Amazon, which snapped up a million-square-foot building that was the first warehouse to be built in the Central Valley after the Great Recession. Now thousands of people work in the warehouse alongside a fleet of robots.
“When I got in the business 10 years ago, people cared about how many truck stalls do you have, how many doors do you have, what’s your clear height,” Ryan George, the Prologis investment officer working on the Tracy project, told me. “That’s all still important, but what drives the discussion now is where is my labor? How do I compete to attract and retain labor?”
Several logistics-industry publications back up George’s assertion. There is a widely acknowledged labor “shortage” in logistics, which has been exacerbated by Amazon’s growth. That’s driven up wages beyond traditional brick-and-mortar retail jobs, but not high enough to retain employees in high-cost regions.
And that’s why Stockton and the surrounding small towns are so attractive. “Some companies are trading transportation advantages for locations that have a desirable labor pool,” wrote Logistics Management in August of this year.
At the same time, a report from the Material Handling Institute and Deloitte Consulting found that many companies expected a major increase in adoption of automation and robotics over the coming years in part because of how hard it is to find the cheap workers that make e-commerce go.
“The fact is that there are 600,000 [warehouse] jobs that are going unfilled in the United States and that gap is getting bigger and bigger,” Fetch Robotics CEO Melonee Wise told me late last year. “The turnover rate for any manufacturing or warehouse job is about 25 percent. And so, there is a need for automation because people aren’t showing up to do the work.”
And ever more e-commerce, which requires a ton of shipping, has added a new wrinkle to the structural problems: It’s highly seasonal. That’s where places like Tracy come into the equation. It’s close enough to serve the Bay Area’s wealthy, but can tap the labor pool not just in Stockton and Sacramento, but all the way out to the migrant workers of the Central Valley.
“The Central Valley in general has a big advantage. To put it in the simplest terms, there are folks out there picking tomatoes in the summertime,” George told me. “They don’t have anything to do in November, December, January. So that’s when they are helping when Amazon triples their employees. And it’s not unique to Amazon.”
Faced with these labor-market conditions, companies have a few options. They can pay out more in wages and offer more perks. They can add tech, in the form of robotics, trying to drive down the amount of labor they need. They can reduce the amount of training and responsibility the average worker needs, so all the people who churn through are roughly interchangeable.
The problem is that that latter two decisions usually make the jobs even worse, exacerbating the wage problem.
George takes me on a driving tour of the vast development. Out here in the back end of e-commerce, drought-tolerant plants line the boulevards, fed only by recycled water. There are bike paths and glassy office parks and little hints of the area’s previous life: an irrigation canal, a railroad crossing.
George stops so that we can watch the construction of a perfectly flat plane onto which concrete will be poured to create the foundation of another enormous building. We talk about how the town of Tracy has received the new development. Though the city has been supportive, some residents don’t want the new development.
“People don’t realize this is where the future is. No one’s going to shopping malls. Shopping malls are going into here, right?” he says, pointing at the soon-to-be building.
Looking around, this does seem like the perfect place for a warehouse. We’re surrounded by highways, a wind farm, huge transmission lines, aqueducts. This is the shadow infrastructure of the Bay Area, the place where the physical systems that underly even the most phone-dependent life take shape. There are jobs in making those systems work, but they may not be ones that people want to do.
It’s a fascinating paradox. While Mayor Tubbs worries about how to structure UBI and get decent jobs into his city, the logistics people are fretting about not having enough workers to fill the slots and how to purchase more robots to reduce the need for human labor.
Even out here, two hours from Silicon Valley on a good day, the tech industry is shaking up civic and economic life. Would a truly universal UBI make hiring even more difficult, thereby driving even more automation? Given that not enough people seem to want warehouse jobs, is that necessarily a bad thing?
In San Francisco, the idea of a universal basic income can drive derisive snorts as a payoff from the tech overlords, but in Stockton, they’ll take all the help they can get.


A skeleton is a human being in its most naked form. A life stripped down to its essence. As the foundation of our bodies—indeed, of our very being—skeletons provoke equal measures of fascination and terror.
As an archaeologist excavating burials, I’ve felt connected to another person—separated by centuries of time—by touching their remains. I’ve observed how exhibits of Egyptian mummies and plastinated bodies inspire wonder for others. But as a museum curator, I’ve also learned that for many cultures, human remains are not organic material to be exploited for science, but rather the sacred remnants of ancestors to be revered.
Our physical bodies will exist as motionless bones far longer than as animate flesh. And human skeletons evoke powerful reactions, from reverence to fear, when they’re encountered. Those features imbue skeletons with a surprising power. Through them, people can live on through their earthly remains.
* * *
Bones are an ancient obsession. Archaeologists recently revealed an 11,000-year-old “skull cult” in Turkey. Humanity’s first farmers also ritually de-fleshed, carved, and displayed human crania. For a thousand years, Japanese folklore has warned of the gashadokuro, a colossal starving skeleton who feasts on the living in the dark of night. The Chimbu tribe of Papua New Guinea intimidates enemies by painting their entire bodies in frightening versions of skeletons, becoming an army of the dead. In medieval Europe, the skeleton was commonly portrayed as a memento mori—a reminder of the inevitability of death.
From Hamlet’s gaze into the eye sockets of the departed court jester, to Paris’s underground catacombs (where there are 6 million skeletons for the public to view), to the laughing skulls carved on pumpkins for Halloween, human bones continue to haunt the collective imagination.
My own imagination was stirred when I excavated my first grave along Highway 188 in central Arizona more than 20 years ago. The road needed to be realigned, but more than 300 burials stood in its way, left some 750 years ago by a Native American group scholars call the Salado. As cars whizzed by, I dug into the soft dirt to reveal pearl-white bones. The archaeological work was slow and painstaking—not only because of the sheer number of burials, but also because of their dazzling contents. In the Southwest, ancient graves typically consist of the bones of the dead along with a few nonperishable artifacts, such as pottery or stone. Here, the graves were loaded with shell and turquoise jewelry, stone animal carvings, bone hairpins, and whole jars and stone points.
I had already uncovered two bodies in the shallow pit, and now a third skull appeared. When I finished exposing the left hand of this third individual, I gasped. Her hand was situated just below the right hand of the second. I realized that the pair likely died at the same time and were placed in the grave side-by-side, holding hands.
It was a moment that would shape my view of what human remains mean. Seeing those two ancients tenderly touching each other in death, I had an immediate link to their history, previously lost to the past. But I also felt their humanity surround me in the present.
* * *
Science tends to take a cold view of the dead. Bones, which The Anatomy and Biology of the Human Skeleton describes as the “remnants of mineralized connective tissue,” are made up of cells arranged in a matrix like a spiderweb. When living, they are a bank of salts, calcium, and red blood cells. Adult humans have 206 bones, which shelter vital organs while also working in concert with muscles to give humans their characteristic fluid rigidity. Though made from soft tissue, bones are tremendously strong. They can heal themselves. The human skeleton is a brilliant feat of evolution.
It took centuries for humans to understand it. More than 2,000 years ago, in what is now Turkey, the physician and philosopher Galen undertook one of the first systematic studies of human anatomy. He got it mostly right, but also seeded myths—such as the theory that bones consist of the same matter as semen because they share a similar color.
Later, the Persians took great interest in anatomy, advancing its knowledge along with scholars in China, Japan, and India. But it wasn’t until the cusp of the European Renaissance that a renewed interest in human dissection led to detailed studies of the body’s architecture. The greatest researcher of this period was Leonardo da Vinci, whose detailed illustrations accurately revealed the body’s inner workings. By the 16th century, articulated human skeletons hung in anatomy theaters across Europe.
During the centuries that followed, the science of the human skeleton took a darker turn. Between 1839 and 1849, Samuel G. Morton published his three-volume Crania Americana, which purported to prove the superiority or inferiority of races based on measurements of their skulls. Based on these racist ideas, museums collected thousands of skeletons—mostly of Native Americans, since their graveyards were easiest to pillage.
Today’s researchers reject such views, of course. Biological inheritance is intertwined with behavior, environment, and culture. People are born with bones, but those bones respond to the world that contains them and bodies that live atop their scaffolding. This is why the skeleton continues to be so valuable to archaeologists. Excavated remains tell the stories of the dead—a person’s sex and age at death, along with their disorders and diseases, traumas and infections, clues to their diet, what hand they used most, how hard they worked. Bones are also a vessel for DNA, which allows scientists to trace the migrations of ancient humans and even discover who they had sex with.
* * *
Some cultures intentionally display their dead. The Torajans on the Indonesian island of Sulawesi, for example, mummify deceased relatives and keep them in their homes, talking to them and feeding them. Yet many people around the world are distraught that their ancestors lie as specimens on museum shelves.
Some years ago, a group of Native Americans came to visit their ancestors’ remains in the storage area of the museum where I work. They asked me to turn off the lights. We were engulfed in darkness when an elder struck a match and lit a bundle of sage, the sweet smoke filling the air. He then sang a song so loudly that the metal of the cabinets reverberated like an accompanying drumbeat. He said he wanted to be sure that his ancestors’ spirits knew he was there—that he remembered them and cared for them.
Bones are not the same as shards of pottery or beaded moccasins. In 1990, after years of protest, Native Americans secured a federal law that established a process for the return of human remains, funerary offerings, and other cultural items from museums. In the years since, more than 57,000 Native American skeletons and 1.7 million burial goods have been repatriated (although more than 100,000 skeletons and millions more artifacts are still in U.S. museums). This movement has become global, as indigenous peoples in New Zealand, Australia, Canada, and parts of Africa have demanded the return of their dead.
While some scientists and museums have pushed back against such claims, the native peoples and the scientists agree more than they might realize. Most Native Americans and indigenous peoples do not oppose science; they object to the form of science that robs bodies of their humanity, especially without consent. Likewise, Westerners also respect skeletons when given the opportunity. In 2012, workers discovered a shallow, unmarked grave under a parking lot in Leicester, England. The skeleton it held, scholars soon confirmed, belonged to King Richard III. For more than 500 years, no one had known the exact fate of the English monarch, long portrayed as a tyrant and murderer. The discovery was a revelation. In his bones lay vital clues about the monarch’s life and last days.
Unlike Native Americans, however, the king didn’t go into a museum. His remains sparked an outpouring of grief and love. Locals raised more than $250,000 for a funeral. The body was laid in an oak coffin in Leicester’s Anglican cathedral. Thousands came to view him. After three days, in an intricate ceremony, 10 British Army soldiers carried Richard III to a marble tomb.
In this moment, Richard III was made a king once again, given a fleeting but vitalized second life. His skeleton provoked new ideas about his biography and England’s history. The mere presence of the bones got the living to fund and attend a burial with the pomp and circumstance befitting royalty.
* * *
A fork does not eat. A painting does not gaze. A book cannot think. But objects do induce humans to act and feel. A fork affords nourishment; a painting creates the experience of beauty; a book stimulates learning. Through their form, cultural function, historical role, or inherent qualities, objects exert their influence and power.
Perhaps nothing does this more profoundly than human bones. They are the medium through which people live on after death. The sight of skeletons can draw or repel. When used for historical purposes, they provide answers about life. When used spiritually, they provoke questions about what lies after death. Perhaps this is why people feel the power of skeletons so viscerally. They seem alive and dead all at once. That’s why they live on so vibrantly, and why people can’t help to react to them with both awe and fear. You and I and everyone else will surely die, but our bones will live on without us.
This article appears courtesy of Object Lessons.


On Monday, the reporter Taylor Lorenz noticed that Google Maps had a new feature: Walking distances were delivered in terms of calories.
Instead of simply telling her that a walk would take 13 minutes, the app also converted that to an amount of energy, 59 calories. Then a click on that calorie count gave a further conversion, from calories to food.
Specifically, mini cupcakes with pink frosting.
This was not well received.
Responses varied narrowly. An ostensible measure to promote health was interpreted as a tech corporation policing women’s bodies.
The writer Rachel Joy Larris noted: “‘Cupcake?’ Let’s talk about all the signifiers that contains about assumptions of gender, culture, and food.”
The writer Dana Cass said, referring to the Harvey Weinstein-induced Me Too movement: “Lol every woman I know has been sexually assaulted and Google Maps is telling me how many calories I’ll burn on my walk to work.”
The app offered no option to convert calorie counts into Budweiser or raw venison.
Within hours, BuzzFeed News reported that Google was simply testing the change, and that it “is removing this feature due to strong user feedback.”
Despite a boom in fitness apps and $1,200 watches that track physical activity, many people do not want to be reminded of calories unless requested. While this sort of nudge may benefit some people, among others the concern is that overwhelming focus on intake and output can drive bulimia or anorexia. In either case, unsolicited calorie counts and cupcake equivalents have an air of body policing and guilt inducement that do not pair well with a culture that assiduously regulates women’s appearances. As writer Casey Johnston offered, “Any woman could have told you this is a supremely bad thing a) to do b) to not be able to turn off.”
In the spirit of no-one-size-fits-all solutions in health, there is more logic in Google considering this as an opt-in feature rather than a default. Tailoring the experience to users in ways safe and driven by evidence would mean more thought than simply forcing pink-cupcake counts on unsuspecting people.
For instance, Google estimated, “The average person burns 90 calories by walking one mile.” Calorie counts vary widely from person to person—walking a mile is a much less energy-intensive endeavor for a professional endurance athlete than a veteran of World War II. Google presumably has the personal data on most of us to make a much more precise calculation—and to suggest more specific incentives than cupcakes or burning calories.
I’ve argued many times that calorie bartering is not usually an effective approach to weight loss or health. Calories offer no insight into the nutritional value of a food, and they are often used by sellers of junk to convince people that they can eat junk if they simply exercise the calories away. But the metabolic effects of 100 calories of Coke on future hunger and energy storage are not the same as a 100 calorie salad, any more than introducing any two 100-pound people would have the same effect on a dinner party.
All of this is part of the consistent theme that obesity prevention is much less straightforward than other public-health challenges. Metabolic syndrome is unique among deadly preventable conditions—it is not the equivalent to if Google Maps were able to track swarms of Zika-infected mosquitoes and suggest alternate routes.
As our behavior is shaped more and more by interactions with phones, our health is shaped by the world that comes to us through apps. The effects can be beneficial or otherwise, but they will not be neutral. This means a serious burden/opportunity on designers to advocate responsibly and strategically for health. That means reckoning with the individual and societal stigma of states of health that affect our outward appearances, and those which are tied to ideas of guilt and moral judgment, and finding ways to make health easy without compromising any individual’s sense of agency in deciding what degree of health they choose to pursue.


The first recorded example in Western literature of men telling women to shut up and stay in the house, writes classicist Mary Beard in her 2014 essay, “The Public Voice of Women,” is in the Odyssey. Not-yet-grown Telemachus tells his mother, Penelope, to “go back up into your quarters, and take up your own work, the loom and the distaff ... speech will be the business of men, all men, and of me most of all.”
As Beard noted in her essay, centuries on, the voices of women are still considered illegitimate in the public sphere, including the new spaces of social media. That manifests as verbal harassment, death threats, and doxing online; as complaints about the sound of women’s literal voices on the radio, giving talks, or in podcasts; as sexual harassment in the workplace; as catcalls on the street. All of these can be seen as ways to drive women out of the public sphere, and back to their proper domain of Kinder, Küche, Kirche (children, kitchen, church). On Friday, many Twitter users boycotted the platform, in response to the suspension of the actress Rose McGowan’s account for speaking out about sexual harassment by the film executive Harvey Weinstein. The driving force for the boycott was women outraged that hate speech, including misogynist and racial harassment and threats, routinely go unchecked, and yet McGowan’s account was suspended.
These women did indeed remove themselves from a public sphere. Twitter, with its more than 300 million active monthly users, is a communal space in a new and extraordinary way that’s driven by the specific technological decisions of the site, which carry with them specific affordances. “Affordances,” a term popularized in the world of design and user interaction by Donald Norman, is a way of describing the perceived possibilities of how the user can interact with the product. These affordances shape how users behave.
Much of the power of Twitter comes from retweets, which can carry the words of a user to an audience far beyond their own followers (for comparison, see Instagram, where no such function exists—it makes it much more difficult for a specific image to “go viral” on the site). But retweeting also allows for what social-media researchers such as danah boyd and Alice Marwick refer to as “context collapse”: removing tweets from not only their temporal and geographic context, but also their original social and cultural milieu, which is very different from most public spaces. I described it to a friend once on a New York City subway—“we’re talking in public, in that everyone near us in this subway car can hear what we’re saying, but that’s a very different ‘public’ than hearing ourselves on NPR tomorrow.” While readers may literally know nothing about the poster or the context except for what is said in that one tweet, they can still just hit “reply” and their response will likely be seen by the poster.
While nothing is stopping people from finding out more information before responding, the clearest affordance Twitter has is for these “drive-by” responses (I’ve been mansplained to by many people who I presume haven’t even looked at my bio to see the “engineering professor” there before trying to school me on my research field—per Telemachus, “of me most of all”). This amplification and context collapse, coupled with the ease of replying and of creating bots, makes targeted harassment trivially easy, particularly in an environment where users can both mostly live in their own ideological bubble by following people who share their views, however abhorrent, and who can easily forget that there is a real person behind the 140 characters of text.
So while Twitter may consider itself to be merely reflecting the discourse, these technological affordances ease the way for certain types of hostile behavior. If you think of the experience of the generalized, systemic misogyny and racism of our culture as being bathed in sunlight on a scorching hot day, Twitter might say it’s just a mirror. But it’s actually handing out magnifying glasses that can focus the already painful ambient sunlight into a killing ray. The targets of this ire, in our society and on Twitter, are disproportionately not just women but people of color. (Imagine how Telemachus would have responded if, rather than his mother, one of the non-Greek household slaves chose to speak up in visiting company.)
One of the most profound social changes of the last few decades is opening up public discourse to a broader range of speakers than ever before, and social media has been a large part of that. The specific affordances of Twitter make it powerful—it can amplify marginalized voices but it can also amplify harassment. Friday’s boycott was intended to be a unified stand against that.
But the point of harassment is to shut women up, either by self-censorship through fear or by driving them away from Twitter, making it simply the newest wrinkle in that long history of exclusion from public spaces and conversations. Many women, especially women of color, therefore found a protest that mandated their silence to be ironic, if not outright misguided: It takes a certain amount of social power to genuinely believe that your absence would be remarked upon and lamented. After 3,000 years of denying the public sphere to all but a small set of voices, some of the new voices are rightly considering their presence to be a sit-in, an occupation, and they are rightly refusing to be driven away. Ultimately, if Twitter wants to be the public sphere, it needs to act like it, by working to create an environment where all voices can be safely heard. Twitter’s social problems are exacerbated by the affordances of technology; they’ll need to bring both ongoing human effort and better design decisions to improve the experiences of marginalized people, and therefore everyone, in their public sphere.


Last week, Sotheby’s auctioned off 140 little black dresses. The event, “Les Petites Robes Noires, 1921–2010,” featured vintage dresses collected by the fashion antiquarian Didier Ludot. A dazzling mix of silk faille, velvet, jersey, and tulle—all in black—cut simple silhouettes. The collection included iconic pieces from Chanel, Givenchy, and Hermès. The more expensive lots fetched over 20,000 euros.
To introduce the collection, Ludot wrote, “Today I pay tribute to the astonishing story of the little black dress and to the designers who wrote its story, a dizzying tale ... from the Roaring Twenties to the new millennium.” But the most astonishing part of the little black dress’s story might be its prologue, the backstory left out of the auction catalogue, the glossy coffee-table books, and the fashion magazines. The most important acolytes of the little black dress were not designers nor aristocrats, but masses of working-class women.
* * *
In October 1926, Vogue featured a sketch of a long-sleeved, calf-length, black sheath dress by a plucky young designer named Coco Chanel. Dubbed “Chanel’s Ford,” the dress was promoted as equivalent in egalitarianism to the Model T.
At the time, Vogue’s editors wrote that Chanel’s little black dress would “become sort of a uniform for all women of taste.” That seems like an astute prediction, in hindsight. But in 1926, the proclamation was tone-deaf at best, as the little black dress was already the actual uniform of many working-class women. The little black dress (or LBD, as it is commonly abbreviated) was a uniform designed to keep certain women in their place. Only later was it co-opted as haute couture for women of taste.
When the lower classes adopt the fashions of the elite, the elites often respond by changing course abruptly—a neckline or a hemline rises or falls dramatically, perhaps, or a voluminous silhouette narrows. But sometimes, rather than quickly changing styles, the upper classes simply wear the clothes the poor have discarded.
For example, as towns populated in the 14th century, a merchant class arose within them. This middle class had some discretionary income, and they spent it on the most conspicuous consumer good: clothing. Finally, they could afford jewel-studded velvets, gold and silver trimmings, brightly colored coats, and sumptuous furs. As the fashion historian Anne Hollander has explained, when the aristocracy couldn’t outlaw or outspend these medieval nouveau riche, they started wearing baggy and threadbare clothing. This new fashion—looking like one had thrown on any old thing—served as a not-so-subtle reminder to the upstarts that, while money could buy clothes, it couldn’t buy class.
Blue jeans offer a more recent example. Jeans began as cheap and durable work pants for miners and farmers. They were the de facto uniform of the rural working class. But once working-class men had access to ready-to-wear trousers, their jeans started showing up on postwar suburban youths, and then in trendy boutiques. Recently, Nordstrom even sold a $425 pair of jeans with fake mud stains—the ultimate blue-collar costume. Once more, the wealthy turn the tables by appropriating the clothing of the poor.
The LBD also finds its origins among the poor. Before the 19th century, domestic servants wore whatever they could—homemade dresses, often, but also their employers’ hand-me-downs. But in the 1860s, the British upper classes required their maids to wear a common uniform: a white mobcap, an apron, and a simple black dress. Soon after, wealthy American and French families followed suit.
Relationships between upper-class women and their servants had changed, becoming “less intimate and more authoritarian,” as the sociologist Diana Crane puts it. At this time, servants ceased to be “the help,” a somewhat collegial characterization, and became known as “domestics.” And domestics wearing upper-class castoffs, especially young and pretty ones, led to embarrassing mix-ups. A caller mistaking the maid for the mistress of the house raised uncomfortable questions about recently erected class barriers.
Cassell’s Household Guide, which billed itself as an encyclopedia of domestic and social economy, summed up the problem like this, circa 1880: “As a general rule, ladies do not like to see their maids dressed in the clothes they themselves have worn—the difference in the social scale of mistress and maid renders this unpleasing.”
But Cassell’s made one exception: “a black or a dark-colored silk.” Previously, a simple black dress meant a wealthy woman was “dressing down.” But by the 19th century, the black dress had become a staple of the lower and middle classes. It was the perfect hand-me-down for the help.
* * *
There was a time when black signified wealth. It was favored by 15th-century Spanish aristocrats and wealthy Dutch merchants. Later, Baldassare Castiglione’s 1528 The Book of the Courtier advised others to follow their lead, to appear above the petty fads of commoners. Black clothing conveyed plainness and piety, for one thing. But it was also incredibly expensive to produce, requiring vast quantities of imported “oak apples”—a bulbous growth left behind on oak leaves from insect egg sacs. By the early 19th century, a newer dye made from logwood and ferrous sulfate made the color cheap to produce. In 1863, an even cheaper synthetic aniline black dye was developed.
By the 1880s, most awkward maid-or-mistress mix-ups had been eliminated thanks to the trusty black dress. But another sort of working-class woman now had the opportunity to dress above her station. Rapid industrialization gave consumers more disposable income, and they wanted places to spend it. More shops opened in urban centers, and cheap labor was needed to staff them. Unmarried young women began pouring into the cities to work as “shopgirls” in dry-good establishments, dress stores, hat and glove shops, and department stores.
The shopgirl enjoyed more freedom and less supervision than domestic servants did. Often, for the first time in her life, she also enjoyed some disposable income of her own. The sewing machine, invented in 1846 and mass-produced in the 1870s, made it easier than ever to imitate these fashions. Mated to the precut paper pattern, devised by the upscale American designer Ellen Curtis Demorest, women could duplicate the latest fashions from Paris with relative ease. And advances in efficiency at textile factories made a wider variety of fabrics and trims available with which to do so.
The new cheap aniline dyes that made the domestic’s black uniform possible also made brightly colored dresses—the vivid scarlets, blues, and greens that were once only for the upper classes—affordable, too. With a few dollars and a few nights’ work, an enterprising shopgirl could create a passable imitation of a dress from the society pages. Or instead, she could shop the sale rack at her place of employment—one of the large, new department stores—and purchase a ready-to-wear dress. She could then alter and trim the dress with lace, sequins, or buttons to make it appear custom-made.
So attired, she might successfully blend in with a store’s clientele—or even outshine them. This wasn’t a desirable state of affairs. Writing in the June 4, 1910, edition of the International Gazette, a Methodist minister urged that “the craze of the shopgirl ... as fashionably attired as the rich woman she waits on had become a menace.” Even earlier, in response to customer complaints, employers had brainstormed ways to neutralize the threat. In 1890, The Sun declared there was a “revolution in dress” underway, “not by the fashionable folk, but by New York’s army of shopgirls.”
In response, many employers began requiring their female employees to dress like domestic servants, in simple black dresses. An 1892 San Francisco Call headline summarized the reaction among the labor pool: “The Shopgirls Hate It.” Sometimes they even went on strike in response. But threatened with termination, most shopgirls buckled, and by the 1890s the little black dress was the required uniform in New York, London, and Paris.
In the summer of 1894, wearing a black dress became a condition of employment for Jersey City telephone operators, too. The “‘hello’ girls,” as they were called, also protested. Newspapers presented their case sympathetically; in 1892, for example, the Reading Times pointed out that the women were opposed not to the dress itself, but “to the idea of showing by their dress that they are working girls.”
For these reasons, the little black dress became a marker of class. When young working-class women complained that being forced into uniform was “inconsistent with our ideals of freedom and independence,” as the The San Francisco Call reported in 1892, they weren’t just complaining about self-expression. Embedded in their ideals was the promise of social mobility.
These women were the fin de siècle equivalent of medieval merchants. They mixed with the upper classes, whether in drawing rooms or on retail shop floors, and they saw what the wealthy wore up close. Thanks to the sewing machine, the paper pattern, and affordable fabrics, the working classes could finally, feasibly, dress like high society—even if they were now only permitted to do so after work hours.
* * *
Society matrons exacted their revenge by dressing like shopgirls and maids, reappropriating their little black dresses for the upper crust.
Lillie Langtry, a famous British beauty who would go on to become a successful actress, conquered London society in 1886 “dressed in a simple little black frock,” as the Emporia Daily News described it. By the early 1900s, socialites who wanted to appear especially youthful and edgy donned little black dresses. The LBD appeared in fashion magazines and society pages decades before Chanel’s dress appeared in Vogue. It was such an established trend by 1915 that even the wife of the U.S. Secretary of the Treasury appeared in public looking “like a college girl, in her short little black dress.”
While Coco Chanel didn’t invent the little black dress, she was astute enough to pick up on the underlying trend that made it popular—la pauvreté de luxe, she called it, or “luxurious poverty.” It was a look reserved exclusively for those who could “afford” to look poor by pretending that they simply couldn’t be bothered with fashion. But while a rich woman might now better blend into the crowd, on closer inspection, there would be some small detail in her seemingly anonymous garment—a certain cut or fabric or label—that acted as a secret handshake for those in the know.
Today, the fashion industry sometimes celebrates the little black dress as an equal-opportunity fashion—versatile, classic, and chic. But this neutral garment was never ideologically neutral—nor was it the democratic creation of a visionary designer. The little black dress marked and mediated social boundaries, a collaboration between cutting-edge technology and age-old class politics.
Today, in addition to little black-dress auctions, there are LBD-themed dinner parties and wine tastings, galas and charity balls. A little black dress has become a shorthand for instant glamour, promising to disguise both figure flaws and mundane lives. This blue-collar costume has successfully crossed over. Women wear little black dresses to feel more like Audrey Hepburn or Princess Diana or even a model in a Robert Palmer music video. But when they do, those women also conjure other predecessors: the women who wore them while they balanced trays, stocked shelves, folded shirts, worked the switchboards, and wrung out the laundry.
This article appears courtesy of Object Lessons.




Few journalists have gotten a peek inside X, the secretive lab run by Google's parent company Alphabet. Its scientists are researching cold fusion, hover boards, and stratosphere-surfing balloons. Derek Thompson, staff writer at The Atlantic, spent several days with the staff of X. In this episode, he tells Matt and Alex all about what he found, and what it suggests about the future of technological invention.
Links:


There’s a famous viral video in which a diver slowly swims up to a clump of rock and seaweed, only for part of that clump to turn white, open its eye, and jet away, squirting ink behind it. Few videos so dramatically illustrate an octopus’s mastery of camouflage. But ignore, if you can, the creature’s color, and focus on its texture. As its skin shifts from mottled brown to spectral white, it also goes from lumpy to smooth. In literally the blink of an eye, all those little bumps, spikes, and protuberances disappear.
The project was entirely funded by the U.S. Army Research Office—and it’s not hard to imagine why. There are obvious benefits to having materials that can adaptively hide the outlines of vehicles and robots by breaking up their outlines. But there are other applications beyond military ones, Shepherd says. It might cut down on shipping costs if you could deliver materials as flat sheets, and then readily transform them into three-dimensional shapes—like flat-pack furniture, but without the frustrating assembly. Or, as the roboticist Cecilia Laschi notes in a related commentary, biologists could use camouflaged robots to better spy on animals in their natural habitats.
“I don’t see this being implemented in any real application for quite some time,” says Shepherd. Instead, he mainly wants to learn more about how octopuses themselves work, by attempting to duplicate their biological feats with synthetic materials. “I’m just a big nerd who likes biology,” he says.
Octopuses change their texture using small regions in their skin known as papillae. In these structures, muscle fibers run in a spiderweb pattern, with both radial spokes and concentric circles. When these fibers contract, they draw the soft tissue in the papillae towards the center. And since that tissue doesn’t compress very well, the only direction it can go is up. By arranging the muscle fibers in different patterns, the octopus can turn flat, two-dimensional skin into all manner of three-dimensional shapes, including round bumps, sharp spikes, and even branching structures.
Shepherd’s team—which includes the postdoc James Pikul and the octopus expert Roger Hanlon, who took the famous video at the start of this piece—designed their material to work in a similar way. In place of the octopus’s soft flesh, they used a stretchy silicone sheet. And in place of the muscles, they used a mesh of synthetic fibers that were laid down in concentric rings. Normally, the silicone membrane would balloon outward into a sphere when inflated. But the rings of fibers constrain it, limiting its ability to expand and forcing it to shoot upward instead.
By changing the layout of the fibers, the team could create structures that would inflate into various shapes, like round bumps and pointy cones. Pikul grabbed a stone from a local riverbed and programmed the material to mimic its contours. He set the material to create hierarchical shapes—lumps on lumps. He even programmed it to duplicate the more complicated contours of a field of stones, and a plant with spiraling leaves.
For the moment, the material can only be programmed to mimic one predetermined shape at a time. Still, “the results are impressive,” writes Laschi, and “represent a first step toward more general camouflage abilities.” Indeed, Shepherd is now adapting the material so it can transform more flexibly—just like a real octopus. For example, the team could replace the fixed mesh of fibers with rubber tubes, parts of which could be inflated or deflated at whim. That way, they could change which bits of the surface are flexible, to determine how it will eventually inflate.
Shepherd’s team is just one of many groups who are attempting to build soft robots, which eschew the traditional hard surfaces of most machines in favor of materials that are soft, bouncy, and floppy. Such bots would theoretically be better at navigating tough terrain, resisting shocks and injuries, and even caring for people. Often, these researchers use the octopus as an inspiration. Last year, Harvard researchers 3-D printed a soft, autonomous “octobot” that moved by burning small amounts of onboard fuel, and channeling the resulting gas into its arms. Laschi, meanwhile, has built a robot with soft floppy arms that can wiggle through the water.
The robots are certainly cool, but they’re nowhere near as versatile as the real deal. Shepherd’s material, for example, can change texture about as fast as an actual octopus, but it can only make one rough shape at a time. The animal, meanwhile, can produce far finer undulations in its skin, which are tuned to whatever it sees in its environment. For now, nothing we produce comes anywhere close.


In the media world, as in so many other realms, there is a sharp discontinuity in the timeline: before the 2016 election, and after.
Things we thought we understood—narratives, data, software, news events—have had to be reinterpreted in light of Donald Trump’s surprising win as well as the continuing questions about the role that misinformation and disinformation played in his election.
To hear more feature stories, see our full list or get the Audm iPhone app.
Tech journalists covering Facebook had a duty to cover what was happening before, during, and after the election. Reporters tried to see past their often liberal political orientations and the unprecedented actions of Donald Trump to see how 2016 was playing out on the internet. Every component of the chaotic digital campaign has been reported on, here at The Atlantic, and elsewhere: Facebook’s enormous distribution power for political information, rapacious partisanship reinforced by distinct media information spheres, the increasing scourge of “viral” hoaxes and other kinds of misinformation that could propagate through those networks, and the Russian information ops agency.
But no one delivered the synthesis that could have tied together all these disparate threads. It’s not that this hypothetical perfect story would have changed the outcome of the election. The real problem—for all political stripes—is understanding the set of conditions that led to Trump’s victory. The informational underpinnings of democracy have eroded, and no one has explained precisely how.
* * *
We’ve known since at least 2012 that Facebook was a powerful, non-neutral force in electoral politics. In that year, a combined University of California, San Diego and Facebook research team led by James Fowler published a study in Nature, which argued that Facebook’s “I Voted” button had driven a small but measurable increase in turnout, primarily among young people.
Rebecca Rosen’s 2012 story, “Did Facebook Give Democrats the Upper Hand?” relied on new research from Fowler, et al., about the presidential election that year. Again, the conclusion of their work was that Facebook’s get-out-the-vote message could have driven a substantial chunk of the increase in youth voter participation in the 2012 general election. Fowler told Rosen that it was “even possible that Facebook is completely responsible” for the youth voter increase. And because a higher proportion of young people vote Democratic than the general population, the net effect of Facebook’s GOTV effort would have been to help the Dems.
The research showed that a small design change by Facebook could have electoral repercussions, especially with America’s electoral-college format in which a few hotly contested states have a disproportionate impact on the national outcome. And the pro-liberal effect it implied became enshrined as an axiom of how campaign staffers, reporters, and academics viewed social media.
In June 2014, Harvard Law scholar Jonathan Zittrain wrote an essay in New Republic called, “Facebook Could Decide an Election Without Anyone Ever Finding Out,” in which he called attention to the possibility of Facebook selectively depressing voter turnout. (He also suggested that Facebook be seen as an “information fiduciary,” charged with certain special roles and responsibilities because it controls so much personal data.)
In late 2014, The Daily Dot called attention to an obscure Facebook-produced case study on how strategists defeated a statewide measure in Florida by relentlessly focusing Facebook ads on Broward and Dade counties, Democratic strongholds. Working with a tiny budget that would have allowed them to send a single mailer to just 150,000 households, the digital-advertising firm Chong and Koster was able to obtain remarkable results. “Where the Facebook ads appeared, we did almost 20 percentage points better than where they didn’t,” testified a leader of the firm. “Within that area, the people who saw the ads were 17 percent more likely to vote our way than the people who didn’t. Within that group, the people who voted the way we wanted them to, when asked why, often cited the messages they learned from the Facebook ads.”
In April 2016, Rob Meyer published “How Facebook Could Tilt the 2016 Election” after a company meeting in which some employees apparently put the stopping-Trump question to Mark Zuckerberg. Based on Fowler’s research, Meyer reimagined Zittrain’s hypothetical as a direct Facebook intervention to depress turnout among non-college graduates, who leaned Trump as a whole.
Facebook, of course, said it would never do such a thing. “Voting is a core value of democracy and we believe that supporting civic participation is an important contribution we can make to the community,” a spokesperson said. “We as a company are neutral—we have not and will not use our products in a way that attempts to influence how people vote.”
They wouldn’t do it intentionally, at least.
As all these examples show, though, the potential for Facebook to have an impact on an election was clear for at least half a decade before Donald Trump was elected. But rather than focusing specifically on the integrity of elections, most writers—myself included, some observers like Sasha Issenberg, Zeynep Tufekci, and Daniel Kreiss excepted—bundled electoral problems inside other, broader concerns like privacy, surveillance, tech ideology, media-industry competition, or the psychological effects of social media.
The same was true even of people inside Facebook. “If you’d come to me in 2012, when the last presidential election was raging and we were cooking up ever more complicated ways to monetize Facebook data, and told me that Russian agents in the Kremlin’s employ would be buying Facebook ads to subvert American democracy, I’d have asked where your tin-foil hat was,” wrote Antonio García Martínez, who managed ad targeting for Facebook back then. “And yet, now we live in that otherworldly political reality.”
Not to excuse us, but this was back on the Old Earth, too, when electoral politics was not the thing that every single person talked about all the time. There were other important dynamics to Facebook’s growing power that needed to be covered.
* * *
Facebook’s draw is its ability to give you what you want. Like a page, get more of that page’s posts; like a story, get more stories like that; interact with a person, get more of their updates. The way Facebook determines the ranking of the News Feed is the probability that you’ll like, comment on, or share a story. Shares are worth more than comments, which are both worth more than likes, but in all cases, the more likely you are to interact with a post, the higher up it will show in your News Feed. Two thousand kinds of data (or “features” in the industry parlance) get smelted in Facebook’s machine-learning system to make those predictions.
What’s crucial to understand is that, from the system’s perspective, success is correctly predicting what you’ll like, comment on, or share. That’s what matters. People call this “engagement.” There are other factors, as Slate’s Will Oremus noted in this rare story about the News Feed ranking team. But who knows how much weight they actually receive and for how long as the system evolves. For example, one change that Facebook highlighted to Oremus in early 2016—taking into account how long people look at a story, even if they don’t click it—was subsequently dismissed by Lars Backstrom, the VP of engineering in charge of News Feed ranking, as a “noisy” signal that’s also “biased in a few ways” making it “hard to use” in a May 2017 technical talk.
Facebook’s engineers do not want to introduce noise into the system. Because the News Feed, this machine for generating engagement, is Facebook’s most important technical system. Their success predicting what you’ll like is why users spend an average of more than 50 minutes a day on the site, and why even the former creator of the “like” button worries about how well the site captures attention. News Feed works really well.
But as far as “personalized newspapers” go, this one’s editorial sensibilities are limited. Most people are far less likely to engage with viewpoints that they find confusing, annoying, incorrect, or abhorrent. And this is true not just in politics, but the broader culture.
That this could be a problem was apparent to many. Eli Pariser’s The Filter Bubble, which came out in the summer of 2011, became the most widely cited distillation of the effects Facebook and other internet platforms could have on public discourse.
Pariser began the book research when he noticed conservative people, whom he’d befriended on the platform despite his left-leaning politics, had disappeared from his News Feed. “I was still clicking my progressive friends’ links more than my conservative friends’— and links to the latest Lady Gaga videos more than either,” he wrote. “So no conservative links for me.”
Through the book, he traces the many potential problems that the “personalization” of media might bring. Most germane to this discussion, he raised the point that if every one of the billion News Feeds is different, how can anyone understand what other people are seeing and responding to?
“The most serious political problem posed by filter bubbles is that they make it increasingly difficult to have a public argument. As the number of different segments and messages increases, it becomes harder and harder for the campaigns to track who’s saying what to whom,” Pariser wrote. “How does a [political] campaign know what its opponent is saying if ads are only targeted to white Jewish men between 28 and 34 who have expressed a fondness for U2 on Facebook and who donated to Barack Obama’s campaign?”
This did, indeed, become an enormous problem. When I was editor in chief of Fusion, we set about trying to track the “digital campaign” with several dedicated people. What we quickly realized was that there was both too much data—the noisiness of all the different posts by the various candidates and their associates—as well as too little. Targeting made tracking the actual messaging that the campaigns were paying for impossible to track. On Facebook, the campaigns could show ads only to the people they targeted. We couldn’t actually see the messages that were actually reaching people in battleground areas. From the outside, it was a technical impossibility to know what ads were running on Facebook, one that the company had fought to keep intact.
Pariser suggests in his book, “one simple solution to this problem would simply be to require campaigns to immediately disclose all of their online advertising materials and to whom each ad is targeted.” Which could happen in future campaigns.
Imagine if this had happened in 2016. If there were data sets of all the ads that the campaigns and others had run, we’d know a lot more about what actually happened last year. The Filter Bubble is obviously prescient work, but there was one thing that Pariser and most other people did not foresee. And that’s that Facebook became completely dominant as a media distributor.
* * *
About two years after Pariser published his book, Facebook took over the news-media ecosystem. They’ve never publicly admitted it, but in late 2013, they began to serve ads inviting users to “like” media pages. This caused a massive increase in the amount of traffic that Facebook sent to media companies. At The Atlantic and other publishers across the media landscape, it was like a tide was carrying us to new traffic records. Without hiring anyone else, without changing strategy or tactics, without publishing more, suddenly everything was easier.
While traffic to The Atlantic from Facebook.com increased, at the time, most of the new traffic did not look like it was coming from Facebook within The Atlantic’s analytics. It showed up as “direct/bookmarked” or some variation, depending on the software. It looked like what I called “dark social” back in 2012. But as BuzzFeed’s Charlie Warzel pointed out at the time, and as I came to believe, it was primarily Facebook traffic in disguise. Between August and October of 2013, BuzzFeed’s “partner network” of hundreds of websites saw a jump in traffic from Facebook of 69 percent.
At The Atlantic, we ran a series of experiments that showed, pretty definitively from our perspective, that most of the stuff that looked like “dark social” was, in fact, traffic coming from within Facebook’s mobile app. Across the landscape, it began to dawn on people who thought about these kinds of things: Damn, Facebook owns us. They had taken over media distribution.
Why? This is a best guess, proffered by Robinson Meyer as it was happening: Facebook wanted to crush Twitter, which had drawn a disproportionate share of media and media-figure attention. Just as Instagram borrowed Snapchat’s “Stories” to help crush the site’s growth, Facebook decided it needed to own “news” to take the wind out of the newly IPO’d Twitter.
The first sign that this new system had some kinks came with “Upworthy-style” headlines. (And you’ll never guess what happened next!) Things didn’t just go kind of viral, they went ViralNova, a site which, like Upworthy itself, Facebook eventually smacked down. Many of the new sites had, like Upworthy, which was cofounded by Pariser, a progressive bent.
Less noticed was that a right-wing media was developing in opposition to and alongside these left-leaning sites. “By 2014, the outlines of the Facebook-native hard-right voice and grievance spectrum were there,” The New York Times’ media and tech writer John Herrman told me, “and I tricked myself into thinking they were a reaction/counterpart to the wave of soft progressive/inspirational content that had just crested. It ended up a Reaction in a much bigger and destabilizing sense.”
The other sign of algorithmic trouble was the wild swings that Facebook Video underwent. In the early days, just about any old video was likely to generate many, many, many views. The numbers were insane in the early days. Just as an example, a Fortune article noted that BuzzFeed’s video views “grew 80-fold in a year, reaching more than 500 million in April.” Suddenly, all kinds of video—good, bad, and ugly—were doing 1-2-3 million views.
As with news, Facebook’s video push was a direct assault on a competitor, YouTube. Videos changed the dynamics of the News Feed for individuals, for media companies, and for anyone trying to understand what the hell was going on.
Individuals were suddenly inundated with video. Media companies, despite no business model, were forced to crank out video somehow or risk their pages/brands losing relevance as video posts crowded others out.
And on top of all that, scholars and industry observers were used to looking at what was happening in articles to understand how information was flowing. Now, by far the most viewed media objects on Facebook, and therefore on the internet, were videos without transcripts or centralized repositories. In the early days, many successful videos were just “freebooted” (i.e., stolen) videos from other places or reposts. All of which served to confuse and obfuscate the transport mechanisms for information and ideas on Facebook.
Through this messy, chaotic, dynamic situation, a new media rose up through the Facebook burst to occupy the big filter bubbles. On the right, Breitbart is the center of a new conservative network. A study of 1.25 million election news articles found “a right-wing media network anchored around Breitbart developed as a distinct and insulated media system, using social media as a backbone to transmit a hyper-partisan perspective to the world.”
Breitbart, of course, also lent Steve Bannon, its chief, to the Trump campaign, creating another feedback loop between the candidate and a rabid partisan press. Through 2015, Breitbart went from a medium-sized site with a small Facebook page of 100,000 likes into a powerful force shaping the election with almost 1.5 million likes. In the key metric for Facebook’s News Feed, its posts got 886,000 interactions from Facebook users in January. By July, Breitbart had surpassed The New York Times’ main account in interactions. By December, it was doing 10 million interactions per month, about 50 percent of Fox News, which had 11.5 million likes on its main page. Breitbart’s audience was hyper-engaged.
There is no precise equivalent to the Breitbart phenomenon on the left. Rather the big news organizations are classified as center-left, basically, with fringier left-wing sites showing far smaller followings than Breitbart on the right.
And this new, hyperpartisan media created the perfect conditions for another dynamic that influenced the 2016 election, the rise of fake news.
* * *
In a December 2015 article for BuzzFeed, Joseph Bernstein argued that “the dark forces of the internet became a counterculture.” He called it “Chanterculture” after the trolls who gathered at the meme-creating, often-racist 4chan message board. Others ended up calling it the “alt-right.” This culture combined a bunch of people who loved to perpetuate hoaxes with angry Gamergaters with “free-speech” advocates like Milo Yiannopoulos with honest-to-God neo-Nazis and white supremacists. And these people loved Donald Trump.
“This year Chanterculture found its true hero, who makes it plain that what we’re seeing is a genuine movement: the current master of American resentment, Donald Trump,” Bernstein wrote. “Everywhere you look on ‘politically incorrect’ subforums and random chans, he looms.”
When you combine hyper-partisan media with a group of people who love to clown “normies,” you end up with things like Pizzagate, a patently ridiculous and widely debunked conspiracy theory that held there was a child-pedophilia ring linked to Hillary Clinton somehow. It was just the most bizarre thing in the entire world. And many of the figures in Bernstein’s story were all over it, including several who the current president has consorted with on social media.
But Pizzagate was but the most Pynchonian of all the crazy misinformation and hoaxes that spread in the run-up to the election.
BuzzFeed, deeply attuned to the flows of the social web, was all over the story through reporter Craig Silverman. His best-known analysis happened after the election, when he showed that “in the final three months of the U.S. presidential campaign, the top-performing fake election-news stories on Facebook generated more engagement than the top stories from major news outlets such as The New York Times, The Washington Post, The Huffington Post, NBC News, and others.”
But he also tracked fake news before the election, as did other outlets such as The Washington Post, including showing that Facebook’s “Trending” algorithm regularly promoted fake news. By September of 2016, even the Pope himself was talking about fake news, by which we mean actual hoaxes or lies perpetuated by a variety of actors.
The longevity of Snopes shows that hoaxes are nothing new to the internet. Already in January 2015, Robinson Meyer reported about how Facebook was “cracking down on the fake news stories that plague News Feeds everywhere.”
What made the election cycle different was that all of these changes to the information ecosystem had made it possible to develop weird businesses around fake news. Some random website posting aggregated news about the election could not drive a lot of traffic. But some random website announcing that the Pope had endorsed Donald Trump definitely could. The fake news generated a ton of engagement, which meant that it spread far and wide.
A few days before the election Silverman and fellow BuzzFeed contributor Lawrence Alexander traced 100 pro–Donald Trump sites to a town of 45,000 in Macedonia. Some teens there realized they could make money off the election, and just like that, became a node in the information network that helped Trump beat Clinton.
Whatever weird thing you imagine might happen, something weirder probably did happen. Reporters tried to keep up, but it was too strange. As Max Read put it in New York Magazine, Facebook is “like a four-dimensional object, we catch slices of it when it passes through the three-dimensional world we recognize.” No one can quite wrap their heads around what this thing has become, or all the things this thing has become.
“Not even President-Pope-Viceroy Zuckerberg himself seemed prepared for the role Facebook has played in global politics this past year,” Read wrote.
And we haven’t even gotten to the Russians.
* * *
Russia’s disinformation campaigns are well known. During his reporting for a story in The New York Times Magazine, Adrian Chen sat across the street from the headquarters of the Internet Research Agency, watching workaday Russian agents/internet trolls head inside. He heard how the place had “industrialized the art of trolling” from a former employee. “Management was obsessed with statistics—page views, number of posts, a blog’s place on LiveJournal’s traffic charts—and team leaders compelled hard work through a system of bonuses and fines,” he wrote. Of course they wanted to maximize engagement, too!
There were reports that Russian trolls were commenting on American news sites. There were many, many reports of Russia’s propaganda offensive in Ukraine. Ukrainian journalists run a website dedicated to cataloging these disinformation attempts called StopFake. It has hundreds of posts reaching back into 2014.
A Guardian reporter who looked into Russian military doctrine around information war found a handbook that described how it might work. “The deployment of information weapons, [the book] suggests, ‘acts like an invisible radiation’ upon its targets: ‘The population doesn’t even feel it is being acted upon. So the state doesn’t switch on its self-defense mechanisms,’” wrote Peter Pomerantsev.
As more details about the Russian disinformation campaign come to the surface through Facebook’s continued digging, it’s fair to say that it’s not just the state that did not switch on its self-defense mechanisms. The influence campaign just happened on Facebook without anyone noticing.
As many people have noted, the 3,000 ads that have been linked to Russia are a drop in the bucket, even if they did reach millions of people. The real game is simply that Russian operatives created pages that reached people “organically,” as the saying goes. Jonathan Albright, research director of the Tow Center for Digital Journalism at Columbia University, pulled data on the six publicly known Russia-linked Facebook pages. He found that their posts had been shared 340 million times. And those were six of 470 pages that Facebook has linked to Russian operatives. You’re probably talking billions of shares, with who knows how many views, and with what kind of specific targeting.
The Russians are good at engagement! Yet, before the U.S. election, even after Hillary Clinton and intelligence agencies fingered Russian intelligence meddling in the election, even after news reports suggested that a disinformation campaign was afoot, nothing about the actual operations on Facebook came out.
In the aftermath of these discoveries, three Facebook security researchers, Jen Weedon, William Nuland, and Alex Stamos, released a white paper called Information Operations and Facebook. “We have had to expand our security focus from traditional abusive behavior, such as account hacking, malware, spam, and financial scams, to include more subtle and insidious forms of misuse, including attempts to manipulate civic discourse and deceive people,” they wrote.
One key theme of the paper is that they were used to dealing with economic actors, who responded to costs and incentives. When it comes to Russian operatives paid to Facebook, those constraints no longer hold. “The area of information operations does provide a unique challenge,” they wrote, “in that those sponsoring such operations are often not constrained by per-unit economic realities in the same way as spammers and click fraudsters, which increases the complexity of deterrence.” They were not expecting that.
Add everything up. The chaos of a billion-person platform that competitively dominated media distribution. The known electoral efficacy of Facebook. The wild fake news and misinformation rampaging across the internet generally and Facebook specifically. The Russian info operations. All of these things were known.
And yet no one could quite put it all together: The dominant social network had altered the information and persuasion environment of the election beyond recognition while taking a very big chunk of the estimated $1.4 billion worth of digital advertising purchased during the election. There were hundreds of millions of dollars of dark ads doing their work. Fake news all over the place. Macedonian teens campaigning for Trump. Ragingly partisan media infospheres serving up only the news you wanted to hear. Who could believe anything? What room was there for policy positions when all this stuff was eating up News Feed space? Who the hell knew what was going on?
As late as August 20, 2016, the The Washington Post could say this of the campaigns:
Hillary Clinton is running arguably the most digital presidential campaign in U.S. history. Donald Trump is running one of the most analog campaigns in recent memory. The Clinton team is bent on finding more effective ways to identify supporters and ensure they cast ballots; Trump is, famously and unapologetically, sticking to a 1980s-era focus on courting attention and voters via television.
Just a week earlier, Trump’s campaign had hired Cambridge Analytica. Soon, they’d ramped up to $70 million a month in Facebook advertising spending. And the next thing you knew, Brad Parscale, Trump’s digital director, is doing the postmortem rounds talking up his win.
“These social platforms are all invented by very liberal people on the west and east coasts,” Parscale said. “And we figure out how to use it to push conservative values. I don’t think they thought that would ever happen.”
And that was part of the media’s problem, too.
* * *
Before Trump’s election, the impact of internet technology generally and Facebook specifically was seen as favoring Democrats. Even a TechCrunch critique of Rosen’s 2012 article about Facebook’s electoral power argued, “the internet inherently advantages liberals because, on average, their greater psychological embrace of disruption leads to more innovation (after all, nearly every major digital breakthrough, from online fundraising to the use of big data, was pioneered by Democrats).”
Certainly, the Obama tech team that I profiled in 2012 thought this was the case. Of course, social media would benefit the (youthful, diverse, internet-savvy) left. And the political bent of just about all Silicon Valley companies runs Democratic. For all the talk about Facebook employees embedding with the Trump campaign, the former CEO of Google, Eric Schmidt, sat with the Obama tech team on Election Day 2012.
In June 2015, The New York Times ran an article about Republicans trying to ramp up their digital campaigns that began like this: “The criticism after the 2012 presidential election was swift and harsh: Democrats were light-years ahead of Republicans when it came to digital strategy and tactics, and Republicans had serious work to do on the technology front if they ever hoped to win back the White House.”
It cited Sasha Issenberg, the most astute reporter on political technology. “The Republicans have a particular challenge,” Issenberg said, “which is, in these areas they don’t have many people with either the hard skills or the experience to go out and take on this type of work.”
University of North Carolina journalism professor Daniel Kreiss wrote a whole (good) book, Prototype Politics, showing that Democrats had an incredible personnel advantage.  “Drawing on an innovative data set of the professional careers of 629 staffers working in technology on presidential campaigns from 2004 to 2012 and data from interviews with more than 60 party and campaign staffers,” Kriess wrote, “the book details how and explains why the Democrats have invested more in technology, attracted staffers with specialized expertise to work in electoral politics, and founded an array of firms and organizations to diffuse technological innovations down ballot and across election cycles.”
Which is to say: It’s not that no journalists, internet-focused lawyers, or technologists saw Facebook’s looming electoral presence—it was undeniable—but all the evidence pointed to the structural change benefitting Democrats. And let’s just state the obvious: Most reporters and professors are probably about as liberal as your standard Silicon Valley technologist, so this conclusion fit into the comfort zone of those in the field.
By late October, the role that Facebook might be playing in the Trump campaign—and more broadly—was emerging. Joshua Green and Issenberg reported a long feature on the data operation then in motion. The Trump campaign was working to suppress “idealistic white liberals, young women, and African Americans,” and they’d be doing it with targeted, “dark” Facebook ads. These ads are only visible to the buyer, the ad recipients, and Facebook. No one who hasn’t been targeted by then can see them. How was anyone supposed to know what was going on, when the key campaign terrain was literally invisible to outside observers?
Steve Bannon was confident in the operation. “I wouldn’t have come aboard, even for Trump, if I hadn’t known they were building this massive Facebook and data engine,” Bannon told them. “Facebook is what propelled Breitbart to a massive audience. We know its power.”
Issenberg and Green called it “an odd gambit” which had “no scientific basis.” Then again, Trump’s whole campaign had seemed like an odd gambit with no scientific basis. The conventional wisdom was that Trump was going to lose and lose badly. In the days before the election, The Huffington Post’s data team had Clinton’s election probability at 98.3 percent. A member of the team, Ryan Grim, went after Nate Silver for his more conservative probability of 64.7 percent, accusing him of skewing his data for “punditry” reasons. Grim ended his post on the topic, “If you want to put your faith in the numbers, you can relax. She’s got this.”
Narrator: She did not have this.
But the point isn’t that a Republican beat a Democrat. The point is that the very roots of the electoral system—the news people see, the events they think happened, the information they digest—had been destabilized.
In the middle of the summer of the election, the former Facebook ad-targeting product manager, Antonio García Martínez, released an autobiography called Chaos Monkeys. He called his colleagues “chaos monkeys,” messing with industry after industry in their company-creating fervor. “The question for society,” he wrote, “is whether it can survive these entrepreneurial chaos monkeys intact, and at what human cost.” This is the real epitaph of the election.
The information systems that people use to process news have been rerouted through Facebook, and in the process, mostly broken and hidden from view. It wasn’t just liberal bias that kept the media from putting everything together. Much of the hundreds of millions of dollars that was spent during the election cycle came in the form of “dark ads.”
The truth is that while many reporters knew some things that were going on on Facebook, no one knew everything that was going on on Facebook, not even Facebook. And so, during the most significant shift in the technology of politics since the television, the first draft of history is filled with undecipherable whorls and empty pages. Meanwhile, the 2018 midterms loom.
Update: After publication, Adam Mosseri, head of News Feed, sent an email
describing some of the work that Facebook is doing in response to the
problems during the election. They include new software and processes
"to stop the spread of misinformation, click-bait and otherproblematic content on Facebook."
"The truth is we’ve learned things since the election, and we take our
responsibility to protect the community of people who use Facebook
seriously. As a result, we’ve launched a company-wide effort to
improve the integrity of information on our service," he wrote. "It’s
already translated into new products, new protections, and the
commitment of thousands of new people to enforce our policies and
standards... We know there is a lot more work to do, but I’ve never
seen this company more engaged on a single challenge since I joined
almost 10 years ago."


Is there a pillow as useless as the U-shaped travel neck pillow? There is not. This half-ovate, toilet-seat cover-esque object reigns as King of Travel Accessories, while failing miserably at its intended sole use. It is a scourge for reasons that I will outline in this essay and of which, by the end, I will convince you without question.
This past summer, I had occasion to travel by plane with such a pillow—memory foam in a pleasant maroon—and did so thoughtlessly, stuffing it into my carry-on as if it were my passport, or a book to ignore while watching, God willing, episodes of Sex and the City on the tiny television. When it came time to attempt sleep I, like many of my fellow passengers, dutifully placed the U-shaped pillow on my shoulders. As my neck protruded an uncomfortable distance from the seat back, I let my head fall to my left. No good. I let my head fall to my right. No good. I scrunched the pillow up, so it was more like a tiny, oddly-shaped normal pillow, but the damn thing kept bouncing back to U-shape, which, by design, has a hole in it, so that was definitely no good.
This damn pillow was no good.
It might come as a shock to you to hear someone speak the truth about U-shaped neck pillows so plainly, as this sort of pillow has been allowed to exist unchecked since it was patented in 1929. I understand and will allow you a moment to compose yourself. Have you taken it? Okay. The U-shaped neck pillow is an unsupportive abomination; a pernicious, deceitful, recklessly ubiquitous travel trinket lulling the masses not to sleep but to a zombielike restlessness for which they have been trained to blame themselves, i.e., “I can’t sleep on airplanes.” The U-shaped travel neck pillow is a useless trash pillow for nobody.
But not everyone agrees. “I bought this pillow for the long-weekend holiday trip. The memory foam is the perfect firmness, and it is so soft and comfortable,” says someone named Ivan in an Amazon review of a neck pillow similar to that which failed me on my recent flight. Okay, Ivan. Someone named Allen says, “I use this in the car. I fall asleep very easy. This keeps my neck comfortable and I don't wake up with neck pain.” Okay, Allen. Someone named Cass says, “I returned it as it had a horrible chemical smell, plus whatever was inside was a solid piece. I wanted something that had little pellets.” Well. This one seems like more of a “Cass” issue, actually.
Brad John, the cofounder of Flight 001, a popular chain of travel stores about which Martha Stewart has allegedly commented, “I love this store, it looks like an airplane,” told me the U-shaped travel pillow sells very well, even though there hasn’t been much innovation in the market. “They’re basically the same as they’ve always been. We sell the heated ones, the inflatable ones, the foam ones.” The main advancement, he said, and the top seller at the moment, is a convertible travel pillow “which you can either make into a regular pillow or a U-neck.” Very interesting that the top-selling U-shaped neck pillow is one that has the ability to function as a normal, non-U-shaped neck pillow.
Brad John himself uses a normal pillow on flights. “I just don’t find the neck pillow comfortable,” he said, “but that’s just personal preference.”
Everyone I spoke with agreed that the U-shaped neck pillow stinks, notably my friend Megan Reynolds who said, “We have one in the house but the boy cat uses it for sex.” My friend Lindsay Robertson, to whom I was referred explicitly because she regularly uses a U-shaped neck pillow on flights, proved to secretly be a member of the U-shaped-neck-pillow resistance: “I never actually use it as a neck pillow, because I can't sleep that way—I'm not sure anyone can,” she told me. Instead, she puts her neck pillow on the tray table in front of her, takes off her glasses, puts her hands in her lap, and “[lets her] face fall completely forward into the pillow, as if [she has] expired.”
What accounts for why some derive comfort from the U-shaped neck pillow—(liars)—and some do not? I asked Mary O’Connor, who is a professor of orthopedics and rehabilitation and the director of the Center for Musculoskeletal Care at Yale. “I’m unaware that there is any clinical data that shows they’re effective in reducing neck strain or neck discomfort,” she said, “However, many of us who travel have experienced falling asleep with our neck in a weird position and it bothering us thereafter. So, I think they can be helpful, but that depends on how they’re used and whether they support the neck.”
The ideal pillow, she said, would keep your head and neck in neutral alignment with your spine, so you’re not too far forward, or backward, or too far to one side or the other. “But how do you know, when you’re in the airport, that the pillow you’re going to purchase is going to give you the right support?” O’Connor asks. “The pillows are all the same. Some people have short necks, some people have long necks, and there’s no ability to look and say, ‘I need this design or this size pillow for my neck, to really work well for me.’ And that’s part of the challenge. Could one of those pillows help someone? Yes, they could. Will they help everyone? Probably not.”
I attempted to find research pointing to the uselessness or usefulness of the dreaded U-shaped neck pillow, and came up empty-handed. However I did find a study titled “The Use of Neck-Support Pillows and Postural Exercises in the Management of Chronic Neck Pain,” which was published in The Journal of Rheumatology in 2016 and dealt with the positive effects of bed-specific neck-support pillows for people with chronic neck pain. I spoke to the study’s coauthor Brian Feldman, a senior scientist and head of the Division of Rheumatology at Toronto’s Hospital for Sick Children, who made sure I understood that his study was not, actually, about the U-shaped travel pillows people use on planes. I understand. I thought he might be able to offer some insight, anyway.
In, he stressed, his own opinion of U-shaped travel pillows, he said, “I can’t stand them. I never use them. They’re not built strongly enough or firm enough. There are all kinds of new gizmos that people have been developing for pillows for sleep in transportation, and they tend to be more like straps that hold your head in place, or boxlike structures that you can sit forward and place your head in, or neck collars, which give you much more support around your neck. Those kinds of things are probably all much better than the typical U-shaped pillow.”
Keeping your neck in a nice physiological position while sleeping is a wonderful thing to do, he said, but the issue with U-shaped pillows is that they aren’t built to be firm enough or high enough to help most people, plus they don’t circle around the neck properly. “They just don’t do the job they’re supposed to do,” Feldman says. In order to work, he thinks they’d have to look more like the kind of rigid neck collar you see on someone who has recently injured their neck, one “that presses up into the head and keeps the chin up and supported so the head doesn’t flop over in any way once you’ve fallen asleep” while sitting up.
Also, don’t they look like the the first-ever stone pillow used by Mesopotamians in 7,000 BC? Seems like we should not still be using a pillow that looks like the first-ever stone pillow used by Mesopotamians in 7,000 BC, but that’s just my opinion.
If I could leave you with one piece of advice, it would be: Take a hard look at whether or not your U-shaped travel pillow is worth toting on your next flight. Are you stuffing it into your carry-on out of usefulness, or out of habit? Is it taking up precious storage space because it will help you sleep, or because you thought you should buy it even though there you’ve encountered no evidence, either personal or scientific, to suggest that this thought is correct? Are you wrong, or do you agree with me? Ask yourself these questions, and then leave the U-shaped pillow behind.
(Unless you’re a boy cat and you’d like to use it for sex.)


ADELPHI, Maryland—In a quiet voice and in her native Spanish, the woman explained to Dr. Shantanu Nundy that she had been feeling dizzy whenever she stood up.
She cleaned houses and worked in a store. There was a lot going on at home—and now this. She choked up describing it all.
Nundy’s clinic, called Mary’s Center, is a primary-care practice, and hers was a classic primary-care problem: common, yet strange; vague, yet worrisome—troubling enough to send the woman to the emergency room the day before, sticking her with a $200 bill. Still, the dizzy spells were not definitive enough for the ER to do anything about.
Nundy suspected she had something deep inside her ear that was throwing off her balance. To make sure, he had her perform something called the Dix-Hallpike test: From a sitting position, he asked her to fall back onto the exam table, then toss her head to one side. That would help determine whether the source of the dizziness was a problem in the inner ear.
It didn’t really work. When she sat back up, she felt fine.
Nundy stepped into the hallway and wrote up her case in the clinic’s electronic medical record. But he still wanted to be sure the cause of the dizziness wasn’t a small stroke or something more serious.
He opened a new tab on his computer and went to a new website that he helps design and run: the Human Diagnosis Project, or Human Dx. The project allows primary-care doctors to ask for assistance on difficult cases from an online network of physicians all over the world.
He clicked “get help on a case” and, on a checklist-style page, input that she was “43f”—a 43-year-old female—with episodic dizziness for the past two months. He then submitted the case to a doctor at another Mary’s Center clinic, as well as to Human Dx’s entire database of nearly 7,000 doctors.
Trained in internal medicine, Nundy now leads the nonprofit arm of Human Dx, but he spends Fridays at the clinic as its only provider for adults. (Other doctors and nurses see children there the other days of the week.)
Mary’s Center is a safety-net clinic, so its patients pay according to their income. At just after 8:30, the waiting room was bustling. The staff issued each patient a number and called them back in English and Spanish—“Twenty-six ... veintiseis!”
Nundy says about 80 percent of his patients are uninsured, in some cases because of their immigration status. Even for those with insurance, a specialist might be out of reach because of high deductibles and co-pays or long wait times.
“For you and me, someone who has insurance, the standard of care is that you see an expert who lives and breathes ... your diagnosis,” Nundy says. But for the 28 million uninsured Americans, seeing, say, a dermatologist or a neurologist usually means getting on long waiting lists for a doctor who is willing to volunteer his or her time.
Human Dx might help doctors confirm their suspected diagnoses or think of things to rule out. At Mary’s Center, one man came in complaining of headaches and nausea, and the Human Dx physicians suggested a blood test called an ESR. Another time, Nundy used it to confirm a suspected case of rheumatoid arthritis before putting a low-income patient on a heavy-duty course of medications.
Experienced doctors use Human Dx for their most difficult cases, and newer providers use it to hone their skills. Johns Hopkins Hospital and other teaching hospitals are now using it to train medical residents. Georgia Lewis, a nurse practitioner who works with Nundy, used Human Dx when, two months into her stint at Mary’s Center, all the other providers went on vacation. Rashes can be confounding, so she’ll upload them to Human Dx along with a photo.
The contributors to the project are vetted based on how accurately they’ve solved past cases. Human Dx uses machine learning, which means that eventually the algorithms powering the diagnosis suggestions will become “smarter” based on the input of the doctors using it. The hope is that, over time, Human Dx can help reduce misdiagnoses, which according to studies happen up to 20 percent of the time.
Human Dx hopes to soon roll out to all 1,300 safety-net clinics in the United States. Ron Yee, the chief medical officer of the National Association of Community Health Centers, is helping clinics like Mary’s Center start using the platform. “We thought we can really help our communities because we have challenges getting specialty care,” he said.
Yee and his colleagues are still figuring out how to fit Human Dx into so many primary-care doctors’ workflows. They’re also puzzling through that eternal health-care question: how to get paid for it. “Does insurance accept this?” Yee said. “I don’t know what it looks like.”
Nundy acknowledges that Human Dx adds time to a doctor’s day. But he says researching difficult cases already adds time, as does reading reference materials or calling his med-school friends for their advice. He hopes that as the project progresses, it could count toward doctors’ continuing medical education, licensing requirements, or student loans. Eventually, he hopes to get all the area’s specialists who treat the uninsured on Human Dx, so they can offer their counsel digitally and save their charity care for those who really need to be seen in person.
It usually takes about six hours to get a response through Human Dx, but a little over an hour after Nundy had seen the woman with the dizzy spells, a few responses had already trickled in. The relative likelihood of the doctors’ guesses were represented by little green bars, like a Wi-Fi signal. The most common suggested diagnosis was dehydration, followed by stress, a ministroke, or Ménière’s disease, a disorder of the inner ear. “Now when I see a person with dizziness,” Nundy said, “I’ll think about Ménière’s disease.”
Most likely, the woman was just stressed and tired. But for Nundy and other primary-care doctors using Human Dx, it’s worth carefully considering “the consequences of being wrong. If this was my mom or my sister ... that’s what we would want,” he said. “That’s what patients deserve.”


The morning of January 27, 1967, Gus Grissom and his Apollo 1 crew put on their flight suits. Foil-clad, with breathing masks, they looked like a mid-century vision of the future brought to you by Reynolds Wrap. The crew of three were to perform a launch test that day, to see what it would be like when they rode a metal cone to space.*
Grissom had been to space before during the Gemini program. That day’s practice wasn’t going great, not like one would hope an actual launch would go. First, the astronauts smelled something rotten in their oxygen. That delayed them by more than an hour. Then, their communications system began to fritz out. Of this, Grissom famously groused, “How are we going to get to the moon if we can't talk between two or three buildings?”
Later, though—into that same microphone and over those same lines—came a single word: “fire.”
It was true: Damaged wires had likely ignited a spark, which fed on the all-oxygen air, growing with its consumption of space-age material—nylon, foam.
The crew tried to escape the capsule. But the hatch wouldn’t open. All three astronauts suffocated inside the vessel that was supposed to carry them—and with them, us—into the future.**
The agency’s two other fatal accidents occurred during the same January week as Apollo’s: Challenger 19 years later, Columbia 17 years after that.*** And just three years ago, the private-spaceflight industry endured its first human loss, when Virgin Galactic’s SpaceShipTwo lost its copilot.****
After each fatal incident, the nation has responded with shock and grief. These explorers—our explorers, Earth’s explorers—paid for that exploration with their lives. Questions arose. Some—How did this happen?—are left to inspectors and investigators. But others—How big a cost are humans willing to bear to leave the planet?—lie in the public domain. The answers seem to have changed throughout the decades, as space travel seemed to evolve from something novel to something routine.
Today, industry and government are both upshifting gears, back into novelty, which means the public’s attitudes toward space travel and its inevitable accidents may return to what they were in NASA’s early, more adventurous days. After decades in a stable and predictable orbit, American spaceflight will return to new vehicles and, maybe, new destinations. The country is deciding which far-off world to point ships toward next, with the moon and Mars the most likely candidates. Private companies are doing the same, and preparing to take high rollers on suborbital romps. And with that leap into the unknown, Americans may become more tolerant of the loss of astronaut life. If they don’t, the government and private industry might not be able to make the leap at all.
We all know people probably will die on these new missions, especially if they become commonplace, as many hope. What no one knows is how we will all respond to those losses.
* * *
When Grissom and his compatriots signed on to the astronaut corps, times were different. They were different: cowboy test pilots—military men, mostly, with that rightest of stuff. Space, and the flight paths to and through it, was basically uncharted. Rockets blew up—a lot—listing sideways, spinning tail over teakettle, exploding heads in the ground like ostriches.
And the astronauts themselves were, for the most part, inured to their mortality. In The Right Stuff, Tom Wolfe repeatedly references the euphemism the early astronauts used to describe fatal crashes: The fliers had “screwed the pooch.” It’s gallows humor: The pilots and astronauts couldn’t completely control their survival—but they could at once face death, distance themselves from it, and use tone to strip it of power.
The public perceived these guys (and they were all guys) as all-American swaggerers, laying their lives on the line for the primacy of the country.
“It was a battle in the Cold War,” says Rand Simberg, author of Safe Is Not An Option: Overcoming the Futile Obsession With Getting Everyone Back Alive That Is Killing Our Expansion Into Space.
The nation, of course, mourned the Apollo 1 crew’s loss—especially given its gruesome nature. But the public and the government were perhaps not surprised, or philosophically disturbed, that people had to die if Americans were to get to the moon in a decade. In an article called “Space Travel: Risk, Ethics, and Governance in Commercial Human Spaceflight,” space strategist Sara Langston looks to other fields to understand attitudes and regulations about space exploration. “In the Encyclopedia of Public Health, [Daniel] Krewski defines acceptable risk as the likelihood of an event whose probability of occurrence is small, whose consequences are so slight, or whose benefits (perceived or real) are so great that individuals or groups in society are willing to take or be subjected to the risk that the event might occur,” she writes. The risk of space accidents, by inference, are subject to the same kind of cost-benefit analysis.
After Apollo, though, came the staid shuttle program. And with it, the tenor of spaceflight changed. The Cold War ended in the ’90s. The spacecraft was called a shuttle. You know, like the bus that takes you to the airport. The Americans had already conquered spaceflight—we got to the moon, which was very hard and very far away and involved orbiting other bodies and sometimes landing. Spinning ellipses around our own planet in a sturdy vehicle? Easy.
The shuttle program left Americans—and perhaps the world—with the false sense that the space-pioneer days were over.
* * *
In technical terms, as the shuttle program developed, people began to think of its flights as operational rather than experimental. In experimental mode, engineers are still figuring the details out, fingering the edges of a craft’s envelope and seeing how hard and fast they can press before they get a cut. In operational mode, though, engineers are supposed to know most everything—the ups, downs, ins, and outs of performance given sundry contingencies.
While the shuttle mostly functioned well, that performance was never actually a given. The vehicle remained, to its last days, experimental, a status reflected in its success/failure rate. “I think people that know our industry kind of understand the edge we're on here, because these systems are tremendously complex,” says David Bearden, general manager of the NASA and civil-space division at the Aerospace Corporation. “If you look back, empirically or historically, at any launch system, about the best you're ever going to get is 1 in 200. On an airline it is a one-in-a-million chance. People who know the industry and know the way those systems operate understand that, I think.”
* * *
I was only six months old when space-shuttle mission STS-51-L sat on the launchpad on January 28, 1986. Aboard were six astronauts and Christa McAuliffe, a teacher from Concord, New Hampshire. The shuttle lifted off on the cold Florida morning. But then, nine miles above Earth’s surface, that seemingly reliable spacecraft broke apart, undone by the uncharacteristic chill at Cape Canaveral that day.
As a Miami Herald Tropic investigation later detailed, the astronauts didn’t die right away: The crew vehicle stayed intact, and continued to go up, before tipping back toward Earth, traveling 12 miles of sky before crashing into the cold ocean water—hard as the cement on the launchpad. The astronauts, the article said, were very likely alive until the very end and might have even been conscious.
Coverage from the days after the tragedy expresses, of course, sadness. “The Shuttle Explosion; Suddenly, Flash of Fear Dashes Watchers’ Hopes,” read a New York Times headline.
“What Went Wrong? Shuttle Loss in Ball of Fire Stuns Nation,” went one from the local Orlando Sentinel.
Both papers, though, declared that the show must go on: “Reflecting on Loss: Welling of Tears, a Desire to Press On,” said the Times.
“Three shuttle veterans lose peers but not faith in program,” said the Sentinel.
The losses, while tragic and (as the Rogers Commission Report would later reveal) avoidable, shouldn’t squash the program. Sacrifices, after all, must be made, for a new program whose utility the nation was still proving.
* * *
I was 17 when NASA lost the space shuttle Columbia in 2003. I’d grown up in Central Florida, not far from Kennedy Space Center. I’d seen almost every shuttle launch in person—with my classmates outside on the sidewalks of my schools, with my sisters in the backyard, and very occasionally from the far side of Cape Canaveral, with my parents. The sonic booms from landings sometimes set off the burglar alarms that hung from our door handles.
But in 2003, I had things to do that didn’t include watching out for spacecraft. I was on my way to a band (marching band, not the cool kind of band) rehearsal session when I heard about Columbia. News of the accident came slow and halting over whatever alt-rock station I was blasting from my Grand Am.
Later, investigators would reveal that a piece of foam had fallen from the shuttle’s wing during launch, leaving a hole that let gas come through when the shuttle re-entered the atmosphere. The shuttle was going Mach 18, 37 miles above the ground, when it broke apart, shaking debris across thousands of square miles.
I remember sitting in my car in a church parking lot, thinking how it couldn’t be real. I remember thinking the radio host didn’t sound like he thought it was real. We’d probably both watched shuttles launch and land safely for much of our lives. To us, the whole program seemed routine—operational. It had moved into that realm of seeming safety, and risks seemed not just less likely but also less justified. And while we always knew this could happen, we never thought it would.
The Columbia disaster represented, unlike the Challenger explosion before it, the start of the finish for the shuttle program. NASA announced its end the very next year. Two strikes, shuttle’s out.
* * *
Sometimes, you hear the phrase “Failure is not an option” associated with NASA. But it was never a slogan at the agency; no one in mission control, that we know of, ever said it, and no manager passed it down. It was just a line in the movie Apollo 13. Failure is always an option: It has to be.
Of course, no one wants a rocket to blow up or a crew capsule to fall to Earth. But to undertake space travel, the undertakers have to acknowledge those possibilities and mitigate the risks. As NASA administrator William Gerstenmaier said in his paper “Staying Hungry: the Interminable Management of Risk in Human Spaceflight,” “We never simply accept it, but NASA, our stakeholders, and the public must acknowledge the risk as we move forward.”
The public, to some extent, also knows that’s the equation. But a 1/200 mission-failure rate means that one doesn’t happen very often, which means that every one comes as a shock.
Still, astronauts’ deaths don’t always cause communal moral outrage. “A particularly risky venture can become socially acceptable in correlation with the value placed on it,” Langston wrote in her risk paper. If people value a space-exploration program, in other words, they’re okay with others risking their lives to push it forward.
Simberg contends that wasn’t fully true with the shuttle, as compared to Apollo—an inspiring and aspiring mission with political importance. “The reason we were so upset about losing these seven astronauts was that what they were doing was kind of trivial,” he says of Columbia.
We don’t always demand, though, that people be doing something Valuable that Benefits Humanity to let them risk their lives (and there were lots of ways the shuttle and, in particular, its trips to the always-peaceful International Space Station did benefit humanity). About 1 percent of people who try to climb Mount Everest historically die in the attempt, for example. And this despite the fact that topping Everest is not exactly exploration, with its $40,000 price tag and Sherpa guides and commercial expeditions. And it’s been done before.
Shuttle astronauts, meanwhile, have a 1.5 percent chance of dying on a given trip to space. And trying to keep them at least as safe as that—or safer—means the agency can’t go as boldly as private industry can.
* * *
The major players in the crewed-commercial space are SpaceX, which wants to eventually build a martian colony; Blue Origin, whose Jeff Bezos envisions an outer space full of industry and long-term habitation; and Virgin Galactic, which wants to democratize access to space closer to home, with a carrier plane that rides up to 50,000 feet, then travels up on its own and glides back down at the behest and guidance of its pilots.
On October 31, 2014, Virgin Galactic paid a human price for that system. During that October test flight, copilot Michael Alsbury unlocked SpaceShip’s feathering system, which changes the shape of the plane to aid reentry, early. The wind then pushed the system open, and the vehicle destabilized. While pilot Peter Siebold parachuted to safety, Alsbury remained with the ship, and died on impact.
After the accident, Virgin allowed its already-booked customers to back out, but just 3 percent did.
SpaceX, meanwhile, has had its own explosive setbacks, and yet the company and leader Elon Musk still remain the industry’s darlings. SpaceX blew up an uncrewed Falcon 9 rocket in September 2016. In June of the year before, the company lost another Falcon that was supposed to resupply the Space Station. In test launches and landings of its reusable rockets, SpaceX has also had a vessel tip over into the ocean and explode (January 2016); crash into a ship (January 2015); and land “too hard for survival” (April 2015).
Based on this, the NewSpace industry seems to exist firmly in the experimental phase. But, more than that, the public seems to know—and accept—that status. “You understand that you're in a test-pilot phase,” says Bearden. “The public can process that and say, ‘That's not me. By the time I fly, they're going to have worked it out.’”
The public permits mistakes for the private space companies—because they produce rockets and results on non-geologic timescales, and lay out visions like “you can go to space” and “you can have a house on Mars.”
The FAA, which regulates commercial space activity via the Human Spaceflight Requirements for Crew and Spaceflight Participants, is also relatively forgiving. “Congress mandated these regulations in the Commercial Space-Launch Amendments Act of 2004,” says the FAA’s description of this law. “Recognizing that this is a fledgling industry, the law required a phased approach in regulating commercial human spaceflight, with regulatory standards evolving as the industry matures,” attempting not to crush innovation with regulation. Flight providers do, though, have to get extremely informed consent from would-be astronauts.
NASA recognizes the value in this model, and in its different posture toward risk. The agency has teamed up with such space companies—letting them, among other things, shuttle cargo and crew to low Earth orbit. NASA no longer has to be all things to all people and missions, and can let those experimental upstarts do a little legwork.
The agency may also see, though, that the public perceives New Space cadets as pioneers—a lens through which they don’t see NASA like they used to—and so forgive their mistakes, tallying them as the cost of innovation, rather than a cost not worth bearing. And perhaps the agency hopes the same thing for itself, as it turned those duties over to private companies so that it can focus on its own bold goals, its own new risky, experimental phase of operations with both the costs and the benefits that come with that.
* This article originally stated that there were four crew members aboard Apollo 1.
** This article originally misstated the cause of death for the Apollo 1 crew.
*** This article originally implied that the Columbia disaster occurred 36 years after the Challenger explosion.
**** This article originally stated that the Virgin Galactic crash resulted in the death of the craft’s pilot. We regret the errors.


You kids don’t understand. You could never understand.
You walk around in habitats of text, pop-up cathedrals of social language whose cornerstone is the rectangle in your pocket. The words and the alert sounds swirl around you and you know how to read them and hear them because our culture—that we made—taught you how. We were the first generation to spend two hours typing at our closest friends instead of finishing our homework, parsing and analyzing and worrying over “u were so funny in class today” or “nah lol youre pretty cool.”
That thing you know how to do, that cerebellum-wracking attentiveness to every character of the text message and what it might mean—we invented that. But when we invented it, we didn’t have text messages, we didn’t have Snapchat, we didn’t have group chats or Instagram DMs or school-provided Gmail accounts. We had AIM. We had AOL Instant Messenger.
“How did AIM work?” you ask. It was like Gchat or iMessage, but you could only do it from a desktop computer. (Since we didn’t have smartphones back then, its desktop-delimited-ness was self-explanatory.) You could set lengthy status messages with animated icons in them. And iconic alert noises played at certain actions: the door-opening squeak when someone logged on, the door-closing click when they logged off, the boodleoop for every new message.
“Those status messages,” you say. “What were they like?” As thunderous piano-accompanied art songs were to the sad young men of Romantic Germany, so were status messages to us. They might have a succinct description of our emotional state. Often they consisted of the quotation of vitally important song lyrics: from The Postal Service, from Dashboard Confessional, from blink-182, from Green Day, from The Beatles (only after Across the Universe came out), from RENT and Spring Awakening and The Last Five Years. (We didn’t have Hamilton back then—I shudder to imagine what 2008 would’ve been like if we had.) From Brand New or Taking Back Sunday if you were pissed at your crush.
And then there were, sometimes concurrently with the song lyrics, the pained, cryptic, and egocentric recountings of the emotional trials of the day. Our parents wronged us. Our best friend wronged us. Our chemistry teacher wronged us. But we never actually said that outright; instead, we hinted at their sins and petty slights through suggestion and understatement. That’s right: AIM was so fertile and life-giving that we invented subtweeting to use it. (Gen X-ers: Don’t @ me about how you all proto-subtweeted on CompuServe or Usenet or ENIAC or whatever.)
But status messages were just the golden filigree of the gorgeous AIM tapestry. AIM was everything to us. I really mean that: As 9/11-jittered American parents were restricting access to the places where we could meet in public—the sociologist danah boyd writes about this in her book, It’s Complicated—we had to turn to AIM. So AIM became the original public-private space. AIM was the mall. AIM was the study carrel. AIM was our best friend’s finished basement. AIM was the side of the library where everyone smoked. AIM was the club (see, Hobbes, Calvin and) and da club (see Cent, Fifty). AIM was the original dark social.
We didn’t ask for someone’s number, at least not then—an errant month of texting in 2005 could still cost $45, an exorbitant figure to the teenage mind—so we asked for their AIM. Or we got their AIM from someone else. (We usually had to tread carefully around the ask.) And over a couple months, we assembled buddy lists of our friends and teammates and crushes and classmates. Their away lights twinkled in a constellation of teenage social possibility.
“What did you even talk about?” All the same stuff you text about now. We asked if they had copied down the math problem sets. We asked how far you were supposed to read tonight in Gatsby. (Then we didn’t do the reading.) We complained about how Mr. O’Brien was mean to freshmen. We talked about the high-school musical, about the ending of Donnie Darko, about God and religion. We used lol to stand in not only for laughter or humor, but for any inarticulable mass of any emotion at all. We talked about who had sex with who. We talked a lot about love. We felt the world shiver and transform when our crush logged on and—boodleoop—started messaging us.
We made our first attempts, on AIM, of transfiguring our mysterious and unpredictable thoughts into lively and personable textual performances. We were witty and dramatic. We invented our online selves—we invented ourselves.
We got bored. Myspace and Xanga helped us set up temporary and ramshackle museums of our tastes. Then Facebook came along, with all the frisson of “only college students use it,” and we drifted there. Its pseudo-maturity and time-delayed interactions allured us. Our AIM status messages went to Facebook instead: It was where we mourned the end of the field-hockey season or the final showing of the winter musical. We posted photos of each other on Facebook and liked them and commented on them—but sometimes still chatted about them on AIM. We asked homework questions via each other’s walls. We wrote subtweety openings as our Facebook status, hoping our crush would comment there instead. Eventually Facebook had its own chat product too, and it made more sense to use that, or Gchat, or to just text.
And then we graduated from high school, and some of us moved far away, and as mobile semi-adults spread across campus, AIM didn’t make logistical sense anymore. Our usernames, laden with Harry Potter and Hot Topic references, were kind of embarrassing anyway. We got bored with the sweet and secret internet of our youth, and we began the hard adult work of building our personal brands, watching prestige television, and purchasing different forms of financial insurance (renter’s, medical, dental, life).
But for years AIM was still there—simply, silently, warmly beckoning for anyone to return. You didn’t hear it. You texted instead, or made Instagram stories. We texted instead, too. It’s how we navigate our lives now.
So now, on December 15, AIM will leave us forever. “AIM is signing off for the last time,” said the product team in a tweet on Friday. “Thanks to our buddies for making chat history with us!”
AIM showed us how to live online, for good and for ill. We all live our whole lives in text chains and group threads now. We plan every hangout, we send every news article, we proclaim every relationship in the river of text it taught us to sail. Honestly, that river has been a little scary lately. Instant messaging, once a special thrill, now sets the texture of our common life. But AIM taught us how to live online first. So AIM, my old buddy, don’t feel bad if you see us shedding a tear. We know what you have to do. For we’ll see you waving from such great heights—
“Come down now,” we’ll say.
But everything looks perfect from far away.
“Come down now,” but you’ll stay.


“Death spiral!” President Trump tweeted in May, about the Affordable Care Act. It had been a common accusation of Republicans even earlier. Media, pundits, and think tanks all weighed in on whether or not the label applies to Obamacare and its health-care exchanges.
Today, death spiral means “a marketplace spinning out of control,” as FiveThirtyEight’s Anna Maria Barry-Jester puts it. It’s an accusation that demands an urgent response. In a death spiral, destruction is so near and so inevitable that any attempt to avoid it becomes valid. By evoking the dwindling seconds before a plane crash, every other option looks better by comparison.
Yet death spirals have another story to tell. Before the death spiral was a figure of speech, it was a physical problem aviators needed to solve: how to keep from crashing when they flew through clouds or fog. How they solved real death spirals in the air might help explain how to resist the narrowed choices metaphorical death spirals impose.
* * *
In the early decades of flight, aviators were bedeviled by bad weather. Those who encountered poor visibility mid-flight told harrowing tales of disorientation and confusion. Surrounded on all sides by milk-white fog or hazy darkness, pilots entered a world where nothing behaved as it should. When they observed the plane slipping into a gentle descent, they corrected to gain altitude, only to find the plane diving downward faster. Or, when they were certain the plane was flying level, the turn indicator would register a turn to the right. What the instrument registered as level, meanwhile, felt like a turn to the left.
Under these conditions, bailing out often became the best option. Those who didn’t often joined their plane as it crashed into the ground.
Lost in the clouds, these pilots had fallen prey to a form of sensory disorientation known as a death spiral, or, more commonly, a graveyard spiral. The term describes an almost instinctive set of maneuvers pilots undertake when they lose sight of the horizon. The graveyard spiral begins when a plane flying in these conditions enters a gentle turn. As it turns, the plane will begin to descend, picking up speed.
Death spirals occur because the pilot feels the descent but not the turn. That has to do with the way the human body relies both on the visual and vestibular systems to perceive its orientation in space. As fluid moves through the small canals in the inner ear, the brain registers the body’s shifts in position. The fluid moves when the head turns, creating the sensation that the vessel under control is doing the turning. In mid-flight, though, the fluid can settle in place. If this happens, a turn can feel like level flight. In this situation, a pilot who follows the instruments and levels the plane’s wings feels, with absolute certainty, that the craft is turning in the opposite direction.
A pilot who recorrects to what feels level in his or her body simply reinitiates the spiral dive. Likewise, pulling back on the yoke to gain altitude without leveling the wings only tightens the plane’s downward spiral. Without a clear view of the horizon to correct against, the pilot can become so disoriented that a total loss of control results, ending in a crash. An Air Safety Institute scare-tactic training video, “178 Seconds to Live,” follows a pilot through the disorientation of a classic graveyard spiral.
Once it became clear that aviators were becoming disoriented in the clouds, they set themselves to the task of figuring out how to avoid it. This was the birth of what is known as “instrument flight.” Planes already carried basic instruments, such as turn and bank indicators, but these were primarily seen as navigational devices—implements that helped pilots reach a destination rather than keep the plane in the air. To tame the death spiral, these devices had to become part of how aviators kept control of the plane.
In 1932, William Ocker and Carl Crane published Blind Flight in Theory and Practice, a detailed guide to flying by instruments through darkness and fog. Ocker and Crane’s method relied on giving the pilot a visual reference against which to double-check the body’s fallible sensations. A turn and bank indicator shows the wings’ departure from level flight, and an artificial horizon visually represents the plane’s relation to the ground.
But designing and implementing instruments was the easy part. It was harder to teach pilots to believe what their instruments reported over (and against) the persuasive sensations they felt in their bodies. Here Ocker and Crane ran up against aviators’ long-standing belief that they controlled the plane, at least in part, through their superior “air sense”—their body’s special ability to maintain its equilibrium in flight. The idea that skillful flight depended on the body’s perception of its own weight and relative position, sometimes called “deep sensibility” or kinesthesia, was a truism among pilots. (Aviators referred to this skill as their ability to “fly by the seat of the pants,” a phrase that connoted, perhaps falsely, skill more than luck.)
Ocker and Crane started demonstrating the limits of the pilot’s body, spinning skeptical pilots in chairs until they were dizzy, or showing them the curves their bodies traced when they tried to walk a straight line without the aid of vision. They even blindfolded homing pigeons and threw them out of a plane to demonstrate that even nature’s best fliers would lose all sense of orientation without sight. (The pigeons spiraled helplessly, Ocker and Crane reported, until they finally spread their wings parachute-style and floated, unharmed, to the ground.) This “inherent spiral tendency” lived in everyone, Ocker and Crane argued, and it would show itself if not restrained by a competing vision of the horizon. Hence the aviator’s need for instruments: They gave back the horizon clouds and fog had obscured.
A wary stance toward bodily perceptions would become a guiding principle for instrument flight. Early U.S. military training documents instructed pilots that their inner ears provided information that was “not at all reliable,” for instance. Ocker and Crane gave pilots a set of practical lessons in how to reference them to keep control of the plane. As pilots learned to trust their instruments, flight through clouds and fog became commonplace, safe, and mundane. The death spiral, meanwhile, was replaced by a simpler imperative: Check your instruments, and believe them.
* * *
Pilots still talk about death spirals, especially to warn amateurs of the dangers of flying into fog and haze. More commonly, though, the term claims that a social organization is on the brink of collapse: small towns, department stores, utility markets, liberal-arts colleges, Apple before Steve Jobs’s return as CEO, the island of Puerto Rico (pre- and post-Hurricane Maria), even the State Department under Rex Tillerson.
The use that is most resonant today—the death spiral as what ails insurance markets—traces back to a 1998 article by two economists describing an “adverse-selection death spiral,” in which insurance plans become financially unsustainable when there are too few healthy, low-cost subscribers enrolled. Economists and businesspeople have played a leading role in the death spiral’s transition to metaphor, converting the individual danger pilots faced into a shorthand for market forces endowed with the inevitability of natural law. They draw on the death spiral’s sense of urgency, meanwhile, to heighten the stakes of corporate failures. The term demands drastic action while rationalizing choices that might follow that imperative.
But its metaphorical life abandons the work that made death spirals in aviation avoidable—the steady, mundane habit of cross-referencing one’s fallible perceptions to the reality of the horizon. As a metaphor, the death spiral is all problem and no solution; it preserves the original’s diagnosis but abandons its cure.
This absence seems particularly lamentable in current discussions of the ACA, given how intensely felt most people’s policy positions seem to be. The death spiral works as a metaphor in this case because it fits neatly into a larger narrative of scarcity. That young healthy people are not buying health insurance on the exchanges seems a rational choice, given their precarious financial state. When the majority of Americans worry they will be unable to maintain their standard of living, the idea that benefits like Obamacare are about to collapse under their own weight makes intuitive sense similarly to how aviators’ bodies rationalize their false perceptions in the air.
The death spiral’s lesson is that logic that seems intuitive needs to be calibrated against measured reality. The perception that the ACA is in a death spiral, for example, requires calibration against the realities of spending decisions and wealth distribution. America pays more than any other industrialized country for its health care, which nevertheless does less to extend its citizens’ lives. About half of the nation’s discretionary spending goes to the military. Great wealth is concentrating in the hands of a diminishing few.
Against this horizon, the urgency and narrowness implied by the ACA’s supposed death spiral looks less insistent. If the aviators win more options when checking their bodily impulses against the horizon, so too the citizenry might find more room to maneuver by expanding its view of possible maneuvers.
I don’t mean to make this process sound easy; it’s not. There’s a reason that the aviator and writer Wolfgang Langewiesche, writing in Harper’s in 1941, described instrument flight as “the castigation of the flesh translated into aeronautical terms.” Orienting their bodies to a horizon that was obscured required pilots to resist the sensations that keep humans upright at every moment. Likewise, resisting the death spiral as metaphor requires pushing back against the normal and the everyday.
Metaphorical death spirals lure people toward forced (and false) choices—choices that endorse actions in concord with fear. It’s not that it feels good to believe disaster is imminent; it’s that it feels real—the perceptions bodies and minds feel intuitively ground people’s thoughts and actions. Perhaps this is why the death spiral is such a powerful metaphor today, when catastrophe feels like the background to everyday life. But there’s also hope in the death spiral: Crashes aren’t inevitable—so long as there’s instrumentation to help find a horizon.
This article appears courtesy of Object Lessons.


The surveillance state has a Blogspot.
At least that’s what it looks like in the opening credits of Gossip Girl, when the titular website flashes on the screen, and Kristen Bell, the narrating omniscient voice of Gossip Girl herself, intones: “Gossip Girl here: your one and only source into the scandalous lives of Manhattan’s elite.”
The site that obsessively monitors, and regularly ruins, the characters’ lives looks like it was made on the classic Blogger platform: There’s a header, a series of bordered posts that run straight down the middle, and a left rail full of links. It’s the most iconic of the many ways that the show, which turns 10 years old this year, is a perfect time capsule of the technology of its time. And while it now feels dated in some respects, it was remarkably prescient about the compulsive relationship people would end up having with their devices.
Gossip Girl was a show about ultra-privileged teens and their infinitely morphing romantic entanglements and high-society social battles. But it was also a show about lives lived in the spotlight of the internet, in the liminal era just before most of America dove headfirst into palm-sized screens.
Technology was integral to Gossip Girl’s premise and plots. Without cameraphone-wielding looky-loos invading the privacy of Serena, Blair, Dan, Chuck, and Nate, there would be no show. So many plotlines hinged on secrets, but it usually only took a couple episodes before Gossip Girl ensured those secrets were revealed, and the writers had to find something new for the characters to hide.
The show’s creators treated technology with the detailed attention befitting its central role, to the point that “we would have companies like Verizon come in and show us prototypes of new models coming up in the future,” Joshua Safran, the show’s executive producer, told Vulture. “We would come up with plotlines based on what we knew would be tech coming out in the future.” Nothing but the newest and shiniest for Manhattan’s elite.
One interesting thing about the Gossip Girl era was the sheer variety of phones available. Before we all coalesced around a touch-screen rectangle as the best possible mobile-phone design, there were BlackBerries with their full keyboards; the Motorola Razr, a super-skinny flip phone; the LG Chocolate, which came in fun colors and slid open to reveal its number pad. All of these appear in the show’s first season, a reflection of the technological diversity of the time.
If the show were filmed today, all the Constance Billard/St. Jude’s students would have iPhones. (Serena’s would be gold and Blair’s would be rose gold. I’m certain of this.)
This was a show in which text messages were often major plot points, but this was before anyone had thought to depict texts as free-floating typography in a shot (an idea widely credited to Sherlock), which meant there were a lot of close-ups of cellphone screens.
The attention the show paid to technology was both incredible production design and a great opportunity for product placement. Watching it today, it feels extremely evocative of 2007, in a good way, but also sometimes in a hilariously dated way.
There is a plot point mid-first season that revolves around a videotape. A literal tape, from a camcorder:
Blair studies for the SATs with this handheld Princeton Review device:
And at one point Serena bonds with her boyfriend Dan’s best friend, Vanessa, over a round of Guitar Hero. Is there anything more 2007 than Guitar Hero?
Sure, one can certainly get one’s jollies by watching Blake Lively pretend to be totally crushing it playing “Free Bird” on that plastic guitar. (Her fingers barely move! As someone who devoted way too much time to getting good at Guitar Hero, I’m offended by this shallow performance.) But where these Upper East Siders were ahead of the curve was in the tightness of the grip technology had on their lives.
The teens of Gossip Girl had codependent, toxic relationships with their phones in a way that would be intimately familiar to many people now, even those who aren’t constantly living in fear of their personal lives being blogged about. Though it was possible in the late ’00s to subscribe to text-message updates from RSS feeds, or SMS alerts from news organizations, for the most part cellphones were still thought to be just for calling and texting people you knew. But Gossip Girl’s characters were using their phones to monitor the news. (By “news” I mean rumors about their very small social circle, but still.) It was unclear whether they’d signed up to get notifications from the Gossip Girl blog, or whether the anonymous blogger just had everyone’s numbers to send “e-blasts” to. These e-blasts were also inconsistent in form—sometimes they appeared as emails:
And sometimes as texts:
It was not uncommon for all the characters to be in a room together, probably at a lavish penthouse party, and for all their phones to go off simultaneously. Then they’d all check them at once, creating a tableau that was strikingly similar to a modern group of people reacting to a breaking-news notification:
If I encountered this in real life today I’d be more likely to expect that North Korea had launched a missile than that my friend’s ex had been spotted with another woman.
Several of the characters—well, let’s be real, mostly Blair Waldorf—exhibited a double standard around privacy. Blair fiercely protected her own secrets, and was devastated when Gossip Girl revealed embarrassing facts about her private life. But she also frequently sent tips in to the blog about others, for her own ends. And all the characters, however they may have hated the blog, still read it regularly. This is a more extreme version of how anybody today might engage in Facebook-stalking, or other digital dirt-gathering, on people in their lives, even as they might worry about what’s discoverable about themselves online.
People have only entrusted more of their personal information to the internet—especially to their smartphones—over time. “It was once said that a person’s eyes were a window to their soul,” Blair says at one point in season one, as she’s forwarding messages from a stolen phone to herself. “That was before people had cellphones.” That certainly hasn’t become less true since then.
The role of the actual Gossip Girl blog diminished as the seasons went on, and the show’s quality declined as well. At the end, the nonsensical reveal of which main character was behind the blog entirely missed the point. That wasn’t a mystery that needed to be solved. The point of Gossip Girl wasn’t who she was; it was that she was watching.
The show was about scandal, and privilege, and the greatest love affair in 21st-century television history (Blair + Chuck 4eva), but it was also about the ways a person’s public and private life can blur in the internet age, with or without their consent. And that’s a theme that feels more relevant than ever. XOXO.


At just about the halfway point of Lynn Novick and Ken Burns’s monumental documentary on the Vietnam War, an army advisor tells an anecdote that seems to sum up the relationship between the military and computers during the mid-1960s.
“There’s the old apocryphal story that in 1967, they went to the basement of the Pentagon, when the mainframe computers took up the whole basement, and they put on the old punch cards everything you could quantify. Numbers of ships, numbers of tanks, numbers of helicopters, artillery, machine gun, ammo—everything you could quantify,” says James Willbanks, the chair of military history at U.S. Army Command and General Staff College. “They put it in the hopper and said, ‘When will we win in Vietnam?’ They went away on Friday and the thing ground away all weekend. [They] came back on Monday and there was one card in the output tray. And it said, 'You won in 1965.’”
This is, first and foremost, a joke. But given the emphasis that Secretary of Defense Robert McNamara placed on data and running the number—I began to wonder if there was actually some software that tried to calculate precisely when the United States would win the war. And if it was possible that it once gave such an answer.
The most prominent citation for the apocryphal story comes in Harry G. Summers’ study of the war, American Strategy in Vietnam: A Critical Analysis. In this telling, however, it is not the Johnson administration doing the calculation but the incoming Nixon officials:
When the Nixon Administration took over in 1969 all the data on North Vietnam and on the United States was fed into a Pentagon computer—population, gross national product, manufacturing capability, number of tanks, ships, and aircraft, size of the armed forces, and the like. The computer was then asked, “When will we win?” It took only a moment to give the answer: “You won in 1964!”
He said “the bitter little story” circulated “during the closing days of the Vietnam War.” It made the point that there “was more to war, even limited war, than those things that could be measured, quantified, and computerized.”
There’s no doubt that Vietnam was quantified in new ways. McNamara had brought what a historian called “computer-based quantitative business-analysis techniques” that “offered new and ingenious procedures for the collection, manipulation, and analysis of military data.”
In practice, this meant creating vast amounts of data, which had to be sent to computing centers and entered on punch cards. One massive program was the Hamlet Evaluation System, which sought to quantify how the American program of “pacification” was proceeding by surveying 12,000 villages in the Vietnamese countryside. “Every month, the HES produced approximately 90,000 pages of data and reports,” a RAND report found. “This means that over the course of just four of the years in which the system was fully functional, it produced more than 4.3 million pages of information.”
Once a baseline was established, decision makers could see progress. And they wanted to see progress, which created pressure on data gatherers to paint a rosy picture of the portrait on the ground. The slippage between reality and the model of reality based on data became one of the key themes of the war.
“The crucial factors were always the intentions of Hanoi, the will of the Viet Cong, the state of South Vietnamese politics, and the loyalties of the peasants. Not only were we deeply ignorant of these factors, but because they could never be reduced to charts and calculations, no serious effort was made to explore them,” wrote Richard N. Goodwin, a speechwriter for Presidents Kennedy and Johnson. “No expert on Vietnamese culture sat at the conference table. Intoxicated by charts and computers, the Pentagon and war games, we have been publicly relying on and calculating the incalculable.”
All of which the “apocryphal story” condenses into a biting joke.
But was there actually a computer somewhere in the Pentagon that was cranking out “When will we win the war?” calculations?
On October 27, 1967, The Wall Street Journal ran an un-bylined blurb from its Washington, D.C., bureau on the front page talking about a “victory index.”
U.S. strategists seek a “victory index” to measure progress in the Vietnam War. They want a single statistic reflecting enemy infiltration, casualties, recruiting, hamlets pacified, roads cleared. Top U.S. intelligence officials, in a recent secret huddle, couldn’t work out an index; they get orders to keep trying.
Now, a victory index is not quite a computer program you can ask “When will we win the war?” But it’s pretty close! A chart could be plotted. Projections could be made from current progress to future ultimate success. At the very least, we can say that officials tried to build a system that could be the kernel of truth at the center of a certainly embellished story.
And it doesn’t seem out of the question that the specific error—showing the United States had already won—could have actually occurred. As the intelligence officials tried different models to make sense of all their numbers, it certainly seems possible that some statistical runs would, in fact, return the result that the peak of the victory index had already occurred. That the war had been won.
In a world besotted by data, the apocryphal story about the Pentagon computers reminds us that the model is not the world, and that ignoring that reality can have terrible consequences.


On October 4, 1957, a beach ball-shaped satellite launched into space from the Kazakh desert. The satellite joined Earth’s journey around the sun, which is why its creators named it Sputnik, Russian for “traveling companion.” Sputnik circled the planet about every hour-and-a-half, traveling at 18,000 miles per hour as it emitted a steady beep, beep, beep. On the ground, people watched Sputnik through binoculars or listened to its pings on ham radios. By January of the following year, Earth’s traveling companion fell out of its orbit and burned up in the planet’s atmosphere.  
Sputnik’s spectators could not have anticipated that this event—the launch of the first human-made satellite into space—would ignite a race to the stars between the United States and the Soviet Union. Nor could they have known that they were, all of them, standing at the precipice of a new era in human history of near-complete reliance on satellite technology. For them, Sputnik was a sudden flash of innovation, something at which to marvel briefly. For their children and grandchildren and generations after, satellites would become the quiet infrastructure that powered the technology that runs their world.
“Many people grasp that satellites are important in our lives, but they may not see exactly in what ways,” said Martin Collins, a curator at the space-history department of the Smithsonian National Air and Space Museum.
So what would happen if all the satellites orbiting Earth suddenly, all at once, stopped working?
The effects would be felt unevenly around the world, Collins said. In communities that don’t rely on satellite technology, particularly in the developing world, potential disruptions to daily life likely would be less severe. In other places, like in the United States, the results would be severe at best. If the blackout persisted long enough, they’d be catastrophic.
If the satellites shut down, “tentacles of disruption,” as Collins put it, would begin to unfurl.
Without operational communications satellites, most television would disappear. People in one country would be cut off from the news reports in another. The satellite phones used by people in remote areas, like at a research station in Antarctica or on a cargo ship in the Atlantic, would be useless. Space agencies would be unable to talk to the International Space Station, leaving six people effectively stranded in space. Militaries around the world would lose contact with troops in conflict zones. Air-traffic controllers couldn’t talk to pilots flying aircraft over oceans.
Richard Hollingham described how this loss of would feel in a Wellesian story in the BBC in 2013: “The rapid-communications systems that tied the world together were unraveling. Rather than shrinking, it seemed as if the Earth was getting larger.”
Without global navigation satellites, the Global Positioning System (GPS)—the network of satellites and ground stations that tell us exactly where we are—would crumble. Some of the immediate effects would be frustrating, but not debilitating, like not being able to use a smartphone to find your way around a new city or track a run in a fitness app. Other effects would have far-reaching consequences. Millions of truckers and other workers in the delivery industry rely on GPS to crisscross the country each day, delivering food, medicine, and important goods.
The loss of GPS also would have disastrous results for our sense of time. GPS satellites are equipped with atomic clocks, which provide the very precise time that satellites need to calculate distance on Earth and tell GPS-enabled devices about their location. Satellites transmit this time to receivers on the ground, where power companies, banks, computer networks, and other institutions synchronize their operations to it. Without these clocks, the electrical grid, financial transactions, and, yes, the internet would start to fall apart. So too would the internet of things, the vast web of devices that talk to each other on our behalf.
“GPS is staggeringly integrated into our lives,” Collins said.
The shutdown of weather and remote-sensing satellites would gravely hamper our ability to predict weather events, like the major hurricanes that have swept across the Caribbean and southeastern United States this year. Farmers couldn’t get information that informs their crop and water management, and scientists wouldn’t have data for their studies of Earth’s features or climate change.
The World Needs a Terrestrial Sputnik Moment
The disruption of every one of the hundreds of operational satellites orbiting Earth is unlikely, but even the loss of one or a few satellites could have powerful effects. When one communications satellite fell out of its orbit in 1998, 80 percent of pager users in the United States——about 45 million people—lost service. An article in The Los Angeles Times a couple of days later sought to emphasize the fragility of the nation’s behind-the-scenes satellite infrastructure. “Paging is hardly the only consumer convenience delivered via satellite technology,” it warned.
Satellite operations could get knocked out by natural phenomena, like powerful solar storms, or human activity, like one nation’s intentional destruction of another’s  fleet of satellites, or an all-out global war. Space junk could also set off a series of collisions that damage any satellites in their path. Collins said that the cause of a complete blackout of satellites would likely determine how people respond to it. Chaos, for a time, is likely inevitable, and there are plenty of suggestions for this doomsday scenario in apocalyptic science-fiction writing.
“Would it severely disrupt the way we live right now? Yes,” Collins said. “Would people be starving in the streets or would there be civil disobedience? That’s hard to say. Potentially.”
Would anything good come out of it? Perhaps, Collins said, when the power grid fails and people are left in the darkness, they could see, many of them for the first time, the unobstructed night sky, with the stars of the Milky Way stretching out before them. They could look up and gaze at the place where their traveling companions, now silent, float along with them.


Facebook’s greatest strength—its ability to identify and connect like-minded people—is also a major vulnerability. Over the past month, the company has revealed that Russia-linked accounts purchased thousands of fake political ads on its platform around the 2016 U.S. election. These ads “microtargeted” Americans based on their divisions along political, racial, and religious lines. Some, as CNN recently reported, specifically targeted voters in Michigan and Wisconsin, two of the most heavily contested states.
The apparent goal was to sow distrust among voters, perhaps even shape how they voted.
As an initial response, Facebook announced that it will close the loopholes that allow Russian-backed sources—or any other foreign powers—to open fake accounts. While a productive start, this doesn’t go after the underlying problem that Russian operatives capitalized on: the extreme polarization of Americans on political issues. Wittingly or not, Facebook has taken on a central role in American democracy. Now the company has to decide how proactive it wants to be to become “a force for good,” as Mark Zuckerberg has promised.
One step Facebook could take in this direction: reverse-engineer the very algorithms used by the Russians. Facebook could try an experiment of matching Americans across political lines to help bridge the country’s deep divide.
Key to understanding why the Russian operatives’ efforts worked is looking at the way in which people build social networks online and the value they get from them. In Bowling Alone, the Harvard professor Robert Putnam uses the phrase “social capital” to describe this process, which he explains happens in two ways: “Bonding” is social capital built by connecting within exclusive homogenous groups; “bridging” is social capital built by connecting with inclusive heterogeneous groups. Both are valuable—while bonding offers support and solidarity, bridging helps people expand their perspectives and creates trust across diverse groups.
“Bonding social capital constitutes a kind of sociological superglue,” Putnam writes, “whereas bridging social capital provides a sociological WD-40.”
Facebook is primarily a mechanism for bonding, not bridging. Studies show that in the vast majority of cases, people live in self-made echo chambers on Facebook that reinforce their existing views of the world. You need look no further than the “red feeds” and “blue feeds” on any given issue to see that in general, when people connect on Facebook, they are mostly connecting with others who have similar political beliefs, educational backgrounds, and religious outlooks.
Although bridging is possible—say, when your old high-school friend who stayed local while you flew across the country for college offers to connect with you—the ability to choose your network and “hide,” “unfriend,” or even “block” people with whom you no longer want to engage makes it essentially an exclusive network. Facebook further amplifies this segregation by using data from a user’s social network and activities on the platform to custom-tailor a News Feed that aggregates posts it knows that user wants to see, often reinforcing worldviews. This insularity allowed Russia’s $100,000 investment in “dark ads” to reach roughly 10 million Americans before and after the election in discrete demographic and geographic circles.
Facebook’s emphasis on bonding over bridging also has consequences for how people build trust. The relationship researcher John Gottman has found that successful romantic relationships depend on making frequent deposits in each partner’s “emotional bank account.”  Consistent positive interactions increase levels of trust in the relationship, so that when conflict arises, there are enough “reserves” in place to make a withdrawal, but still leave the relationship in a net-positive place. In fact, Gottman estimates that every relationship needs at least five positive interactions to maintain equilibrium with a single negative interaction.
Applying Gottman’s “bank account” model to social relationships can help explain why it’s difficult to have meaningful disagreements on political issues. Americans today spend an average of six-and-a-half hours each day online, with almost a third of that time on social media. If their social-media diets include relatively insular circles like Facebook, their daily positive interactions are likely occurring more with people they already agree with, and less with people from across groups with different perspectives. In fact, in 2017, odds are that Americans will most likely interact with someone who holds different political views when they’re screaming at them from the other side of a protest line, or inside an angry internet forum.
Without a way to make regular, positive deposits in social relationships that bridge political lines, every civic debate is a withdrawal without social reserves, leaving people perpetually overdrawn.
Some research supports the idea that frequent and meaningful interactions between diverse Facebook users can promote the flow of new ideas across otherwise unconnected groups. Jonny Thaw, a spokesperson for the company, pointed out a 2014 study that looked at how the platform creates the “bridging social capital” described by Putnam. The study, which was conducted by researchers unaffiliated with Facebook, found that “weaker ties” in someone’s network (like a friend of a friend, or someone with whom you would not have other offline connections) offered the platform’s users the most potential for users to expand their worldview, because these connections opened the door to new information and diverse perspectives.
More importantly, however, was that the users who benefited the most from their weak social ties—in terms of expanding their outlook—were those who actively engaged in what the study’s authors call “Facebook Relationship Maintenance Behaviors,” like “responding to questions, congratulating or sympathizing with others, and noting the passing of a meaningful day.”
In other words, simply being connected to Facebook users from different backgrounds isn’t enough to make people open to new perspectives and ideas; users need to actively make deposits in each other’s social bank accounts in order to truly benefit from those diverse connections. The study notes that facilitating bridging among its users “may lie in technical features of the site that lower the cost of maintaining and communicating with a larger network of weak ties.”
This study points to some creative ways that Facebook can promote political bridging among its users—and develop some WD-40 against threats to democracy in the process. Let’s say that Facebook created a new feature called “Friend Swap” for users interested in creating connections with people outside of their political bubble. The company could use its powerful algorithms to match users with someone who, based on their individual preferences and posts, they disagree with politically, but have some things in common with personally. What’s important is that the users don’t engage over political issues, at least until they’ve had time to build some social trust. If you’re a liberal, you might not be so open to being thrown whole-hog into a conservative stranger’s feed and reading their posts from Fox News. But you may find some common ground around, say, rooting for the same sports team, or shared musical tastes or experiences, like being a veteran.
A feature like Friend Swap would selectively share only the posts of each user’s feed in an area they have in common with their political counterpart, and allow them to interact on that topic. After a trial period, the “swapped” posts might include ones on another common interest, and so on, until the users elect, if they eventually choose, to actually be “friends.” By creating connections around common interests or experiences, users would make deposits in each other’s social bank accounts over time. If they do become full-on friends, they would be more likely, at least in theory, to be open to a dialogue on differing viewpoints on political issues from someone they’ve come to trust based on bonding in other areas. Hopefully, at the very least, they could agree to disagree while maintaining their connection, which is still a win in today’s climate.
Of course, Friend Swap won’t be a panacea for political differences. It requires people to view online relationships—with strangers—as being valuable enough to invest their time. And the self-selection of people who opt in to this kind of feature might be the same people who would be more open to different viewpoints anyway. But even as an experiment, Friend Swap would be an opportunity for Facebook to gather data on how it can bridge its red and blue silos over shared values like civility, openness, tolerance, and respect. It would also offer a new way to connect people from politically polarized geographic regions, like the Rust Belt and the coasts.
Trying to “socially engineer” relationships, even for the purposes of political cross-pollination, might go against the grain of a company that has been built upon a principle of fierce neutrality. But Russian operatives’ attempts to use Facebook to disrupt American democracy demonstrates that neutrality no longer seems be an option, if it ever really was one in the first place. On Yom Kippur, the Jewish day of atonement, Zuckerberg acknowledged as much, asking for forgiveness for “the ways [Facebook] was used to divide people rather than bring us together.” Facebook has the talent and the resources to help unite people in defense of democratic values, if it has the will to do it.


It’s just after sunrise in New York City. The sky is bathed in pinks and orange as people walk along a long dock toward a white ship. They board the vessel and it sails out to a launchpad further out in the water, where a spaceship strapped to a giant rocket awaits. After they pile in, the rocket blasts off into the atmosphere. About 39 minutes later, they land halfway around the world, in Shanghai.
This is the scenario imagined by SpaceX founder Elon Musk, who discussed the futurist transport system in a speech in Australia last week about the company’s long-term ambitions. The not-yet-built system—which Musk nicknamed BFR, for “big fucking rocket”—would, someday, ferry passengers from one major city to another. Long-distance trips from Bangkok to Dubai, or from Honolulu to Tokyo, for example, would take about 30 minutes and cost about as much as an economy airline ticket.
(The BFR would also subsume the duties of SpaceX’s current fleet of rockets and spacecraft, like the Falcon 9 and Dragon capsule, by launching satellites into orbit, transporting astronauts and cargo to the International Space Station, and even bringing humans to the moon and Mars.)
The news of the Earth transport system was thrilling for Musk’s fans, for whom a speech from the entrepreneur about space exploration is akin to an Apple launch event. The future is really here, or at least quickly approaching! Imagine setting your smartphone to rocket mode instead of airplane mode. Rocket travel, an animated video of this future seemed to suggest, would be a breeze.
Well, not necessarily. To make a half-hour trip, the BFR would have to travel thousands of miles per hour, with a maximum speed of about 16,700 miles per hour, according to SpaceX. The flight would expose passengers to sensations they don’t usually encounter while traveling, like intense gravitational forces and weightlessness. The spaceship would definitely need to stock barf bags.
Musk explained in a tweet that travelers would experience g-forces between 2 and 3, which means twice or three times their body weight. “Will feel like a mild to moderate amusement park ride on ascent and then smooth, peaceful, and silent in zero gravity for most of the trip until landing,” he said.
“That may not be a very comfortable way to travel,” said Ge-Cheng Zha, a mechanical and aerospace professor at the University of Miami who studies supersonic flight. “Not everyone can take it.”
The ride will be most intense during landing and takeoff. The rapid acceleration and deceleration could lead to motion sickness. So could a quick peek out the window during a particularly twisty maneuver. “There’s a disconnect between the g-force and what the person sees, which can lead to severe motion sickness,” said Andy Feinberg, a geneticist at Johns Hopkins University in Maryland who studies astronaut health (and applied to be an astronaut himself in 1979).
Feinberg has flown aboard NASA’s now-retired zero-gravity plane, which simulates weightlessness by taking a series of dives. The speed of the aircraft matches the speed of the passengers as they fall, which creates the experience of free-falling. Feinberg remembers not everyone on board could handle the shifts.“The NASA people, the astronauts, and I were having the time of our lives,” he said. Everyone else around them was throwing up.
Aside from the flight experience and the discomfort it may bring, there’s a host of other factors Musk and his engineers will need to consider before the BFR becomes reality. While the actual trip may indeed take about a half-hour, preparing and unloading the passengers could take hours. The nature of the travel could increase the time required for security checks, luggage checks, and whatever new safety procedures flight attendants may have to present. (At least some flight attendants, for what it’s worth, seem game for the BFR. While it’s too soon to give a “definitive opinion” on rocket travel, Sara Nelson, the president of the Association of Flight Attendants-CWA International, said flight attendants have the “flexibility to adapt to new conditions.”)
There’s also the question of fuel. Launching the BFR, which will stand 106 meters tall, nearly double the height of the Falcon 9, will require tremendous energy, Zha said. A rocket is much harder to get off the ground than, say, a supersonic plane, he said. The Concorde, a now-retired commercial supersonic airliner that carried passengers from New York to London in under three hours, among other destinations, used about three times as much fuel as a Boeing 747. “The reason we use rockets for space delivery is because there’s no other options,” Zha said. “On Earth, airplanes are way more efficient.”
The transport system will also face questions from the U.S. government and other nations in the BFR’s flight path about the rocket’s safety risks and environmental impact. The rocket’s introduction would require the regulation of an entirely new commercial industry.
Still, many won’t be deterred. The BFR doesn’t exist yet, so the coolness factor outweighs all others. Asked whether he would hitch a ride on the BFR, Feinberg said, “in a second.”


In the crucial early hours after the Las Vegas mass shooting, it happened again: Hoaxes, completely unverified rumors, failed witch hunts, and blatant falsehoods spread across the internet.
But they did not do so by themselves: They used the infrastructure that Google and Facebook and YouTube have built to achieve wide distribution. These companies are the most powerful information gatekeepers that the world has ever known, and yet they refuse to take responsibility for their active role in damaging the quality of information reaching the public.
BuzzFeed’s Ryan Broderick found that Google’s “top stories” results surfaced 4chan forum posts about a man that right-wing amateur sleuths had incorrectly identified as the Las Vegas shooter.
4chan is a known source not just of racism, but hoaxes and deliberate misinformation. In any list a human might make of sites to exclude from being labeled as “news,” 4chan would be near the very top.
Yet, there Google was surfacing 4chan as people desperately searched for information about this wrongly accused man, adding fuel to the fire, amplifying the rumor. This is playing an active role in the spread of bad information, poisoning the news ecosystem.
The problem can be traced back to a change Google made in October 2014 to include non-journalistic sites in the “In the News” box instead of pulling from Google News.
But one might have imagined that not every forum site could be included. The idea that 4chan would be within the universe that Google might scrape is horrifying.
Worse, when I asked Google about this, and indicated why I thought it was a severe problem, they sent back boilerplate.
Unfortunately, early this morning we were briefly surfacing an inaccurate 4chan website in our Search results for a small number of queries. Within hours, the 4chan story was algorithmically replaced by relevant results. This should not have appeared for any queries, and we’ll continue to make algorithmic improvements to prevent this from happening in the future.
It’s no longer good enough to note that something was algorithmically surfaced and then replaced. It’s no longer good enough to shrug off (“briefly,” “for a small number of queries”) the problems in the system simply because it has computers in the decision loop.
After I followed up with Google, they sent a more detailed response, which I cannot directly quote, but can describe. It was primarily an attempt to minimize the mistake Google had made, while acknowledging that they had made a mistake.
4chan results, they said, had not shown up for general searches about Las Vegas, but only for the name of the misidentified shooter. The reason the 4chan forum post showed up was that it was “fresh” and there were relatively few searches for the falsely accused man. Basically, the algorithms controlling what to show didn’t have a lot to go on, and when something new popped up as searches for the name were ramping up, it was happy to slot it as the first result.
The note further explained that what shows up in “In the News” derives from the “authoritativeness” of a site as well as the “freshness” of the content on it. And Google acknowledged they’d made a mistake in this case.
The thing is: This is a predictable problem. In fact, there is already a similar example in the extant record. After the Boston bombings, we saw a very similar “misinformation disaster.”
Gabe Rivera, who runs a tech-news service called Techmeme that uses humans and algorithms to identify important stories, addressed the problem in a tweet. Google, he said, couldn’t be asked to hand-sift all content but “they do have the resources to moderate the head,” i.e., the most important searches.
The truth is that machines need many examples to learn from. That’s something we know from all the current artificial-intelligence research. They’re not good at “one-shot” learning. But humans are very good at dealing with new and unexpected situations. Why are there not more humans inside Google who are tasked with basic information filtering? How can this not be part of the system, given that we know the machines will struggle with rare, breaking-news situations?
Google is too important, and from what I’ve seen reporting on them for 10 years, the company does care about information quality. Even from a pure corporate-trust and brand perspective, wouldn’t it be worth it to have a large enough team to make sure they get these situations right across the globe?
Of course, it is not just Google.
On Facebook, a simple search for “Las Vegas” yields a Group called “Las Vegas Shooting /Massacre,” which sprung up after the shooting and already has more than 5,000 members.
The group is run by Jonathan Lee Riches, who gained notoriety by filing 3,000 frivolous lawsuits while serving a 10 year prison sentence after being convicted for stealing money by impersonating people whose bank credentials had been phished. Now, he calls himself an “investigative journalist” with Infowars, though there is no indication he’s been published on the site, and given that he also lists himself as a former male underwear model at Victoria’s Secret, a former nuclear scientist at Chernobyl, and a former bodyguard at Buckingham Palace, his work history may not be reliable.
The problems with surfacing this man’s group to Facebook users is obvious to literally any human. But to Facebook’s algorithms, it’s just a fast-growing group with an engaged community.
Most people who joined the group looking for information presumably don’t know that the founder is notorious for legal and informational hijinks.
Meanwhile, Kevin Roose of The New York Times pointed out that Facebook’s Trending Stories page was surfacing stories about the shooting from Sputnik, a known source of Russian propaganda. Their statement was, like Google’s, designed to minimize what had happened.
“Our Global Security Operations Center spotted these posts this morning and we have removed them. However, their removal was delayed, allowing them to be screen-captured and circulated online,” a spokesperson responded. “We are working to fix the issue that allowed this to happen in the first place and deeply regret the confusion this caused.”
All across the information landscape, looking for news about the shooting within the dominant platforms delivered horrifying results. “Managing breaking news is an extremely difficult problem but it's incredible that asking the search box of *every major platform* returns raw toxic sewage,” wrote John Hermann, who covers the platforms for The New York Times.
For example, he noted that Google’s conglomerate mate at Alphabet, YouTube, was also surfacing absolutely wild things and no respected news organization.
managing breaking news is an extremely difficult problem but it's incredible that asking the search box of *every major platform* returns raw toxic sewage
As news consumers, we can say this: It does not have to be like this. Imagine a newspaper posting unverified rumors about a shooter from a bunch of readers who had been known to perpetuate hoaxes. There would be hell to pay—and for good reason. The standards of journalism are a set of tools for helping to make sense of chaotic situations, in which bad and good information about an event coexist. These technology companies need to borrow our tools—and hire the people to execute on the principles—or stop saying that they care about the quality of information that they deliver to people.
There’s no hiding behind algorithms anymore. The problems cannot be minimized. The machines have shown they are not up to the task of dealing with rare, breaking news events, and it is unlikely that they will be in the near future. More humans must be added to the decision-making process, and the sooner the better.


In one corner of the internet, World War II is just getting started.
The battle is unfolding at @RealTimeWWII, a Twitter account that “live-tweets” the events of each day of the war, hour by hour, as they happened more than 70 years ago. Here, the conflict that ensnared most of the world’s nations and claimed the lives of millions is broken down into 140-character dispatches. Right now, it’s September 1939, and German bombs are shattering Warsaw. The city is in flames. Its residents are out of food and clean water. “Human wreckage is laid on the table, the surgeon vainly endeavoring to save the lives that are slipping through his hands,” one nurse says of the scene.
@RealTimeWWII will spend the next six years live-tweeting the rest of the war to its nearly 500,000 followers. The account is run by Alwyn Collinson, a digital editor at the Museum of London. Collinson actually spent the last six years tweeting the war, too. His first run started in 2011 and ended last month, and he decided to start all over again in September, writing and tweeting the same dispatches, with some new additions.
“It’s incredible to have hundreds of thousands of people reading and hundreds responding,” Collinson said. “I think it’s a shame just to walk away from that, though perhaps it would have been easier to do so.”
Collinson spends about an hour each day planning, curating, and posting tweets. The preparation for major events—like the German invasion of the Soviet Union, the attack on Pearl Harbor, and D-Day—can take as long as 10 hours. It helps that World War II is one of the most well-studied and documented events in human history; Collinson has at his disposal countless books, newspaper archives, historical records, and other sources. In addition to tweeting the basics, like military operations, Collinson shares quotes from firsthand accounts gleaned from letters and diaries.
“In many ways, it’s like trying to report on the news but years and years ago,” he said. “I’m not just trying to get a sense of who invaded who, which cities fell when, but more of what people thought and how they were trying to make a narrative out of it at the time.”
Collinson was inspired to retrace World War II after watching the events of the Arab Spring play out across social-media platforms in late 2010. Back then, for perhaps the first time, people were using digital means like Twitter and other websites to announce, coordinate, and document widespread political demonstrations in multiple countries. Social media elevated and carried the protests over international borders, reaching anyone with a Twitter handle. The internet made the far-flung events of the movement into something “real and tangible and immediate,” Collinson said.
“I remember being really profoundly moved by the way that you could see history unfolding,” he said. “We didn’t know necessarily how things were going to turn out.”
As @RealTimeWWII’s following grew, users began to reach out to share personal stories, letters, or diaries of family members who lived through the war. Some helpfully corrected Collinson when he tweeted something incorrect, like the wrong name for a military tank in a picture. ( “One of the wonderful advantages of the internet is that people can correct you almost immediately,” he said with a laugh.) Then there were the vitriolic comments. In response to tweets about the Holocaust, Collinson said,“I had neo-Nazis saying, why are you reporting on this Jew news? Get back to real news.”
For the most part, Collinson is tweeting the same kind of content today as he did when he started in 2011. But the atmosphere—of Twitter, of the internet in general—feels different to him. In the last decade, it seems like the discourse on social media has gotten increasingly ugly.
“I suppose now it seems like a less innocent landscape,” Collinson said. “The internet and social media are no longer an escape. It’s just one more way of reflecting, perhaps even in a more disconcerting way, the world that we live in.”
These days, Twitter users are finding more parallels between the events of World War II and current political and economic affairs, particularly in the rise of nationalist sentiment in the United States and Europe. Some people react to the tweets with warnings or lamentations that the world is regressing to early-20th-century ideologies. “Obviously there were terrible events going on [in 2011] as there are terrible events going on now,” Collinson said. “But there was less talk of there being Nazis on the streets of the Western world.”
The account serves as yet another reminder that history is hindsight. Users scrolling through @RealTimeWWII’s feed enjoy an omniscience they cannot possess in their own time. But they are living through history just as much as people during the war did, and perhaps someday their own lives will become a historical narrative told on the internet—or through some technology that doesn’t yet exist.
“It’s reminded us that actually no one lives outside of history,” Collinson said. “We could be living in the calm before some terrible storm. One of the reasons to study history, other than that it’s fascinating, is that it reminds you not to be complacent about your own time and not to think that you can live unaffected by the world.”


Not long ago, Glenn Thrush, a White House reporter for The New York Times, sat down at 7:30 a.m. to plan his day. His schedule and laptop sat arrayed before him. With so much time to spare, he checked Twitter—an easy mistake to make—where he noticed someone saying something vile and ill-considered.
And then, somehow, 90 minutes passed. It was 9 a.m. No planning had gotten done.
“Someone had said something nasty and it had totally hijacked my day,” he said on Thursday. Twitter’s addictiveness, and its mysterious ability to speed the passage of time, were part of the reason that Thrush deactivated his own account on the platform earlier this month—an interesting choice for a White House reporter during the administration of a president who loves to break news on Twitter. On September 19, Thrush posted his last tweet and logged off. At least for now.
Now, he feels more in control of his day, though he lamented that he couldn’t promote his Times colleagues’ work anymore. But there were other workplace benefits to leaving the site. “I’ll just say straight-out, I think my bosses are pretty pleased with my decision to do this,” he said.
Twitter was just one of the topics that Thrush and two other politics reporters covered on Thursday at the Washington Ideas Forum, which is sponsored by the Aspen Institute and The Atlantic. Thrush was joined by Katy Tur, a correspondent for NBC News; and Robert Costa, a national political reporter at The Washington Post and the moderator of  Washington Week on PBS. During the forum, they discussed the many roles the media—social and otherwise—have played in the early Trump era.
All three of the journalists have achieved new recognition in the Trump era—though only Thrush was honored with a regular portrayal on Saturday Night Live. All three have also experienced the topsy-turvy—and, sometimes, inappropriate—assaults of the new administration and its most prominent figure.
Tur was one of the first national reporters tasked to cover Trump full-time. Often, she said, it was just her and a bunch of local reporters at any one campaign event. She came to be one of the first reporters that Trump recognized—so much so that by June 2015, Trump shouted, mid-rally, “Katy, you haven’t even looked up at me once.”
“I yelled back, ‘I’m tweeting what you’re saying!’” Tur said. “And he liked that and moved on.”
The president did not always move on. In a now widely reported moment, Trump shocked Tur by kissing her on the cheek in November 2015 while on an MSNBC set in New Hampshire.  “Before I know what’s happening, his hands are on my shoulders and his lips are on my cheek. My eyes widen. My body freezes. My heart stops,” Tur writes in her new book, Unbelievable.
“F—k,” she thought. “I hope the cameras didn’t see that. My bosses are never going to take me seriously. I didn't have time to duck!”
A few minutes later, Trump mentioned the kiss on-air. “But actually, Katy Tur — what happened? She was so great,” he told Joe Scarborough and Mika Brzezinski. “I just saw her back there. I gave her a big kiss. She was fantastic.”
For Tur, the incident—aside from being inappropriate—signified the president’s ignorance if not active rejection of the courtesies normally expected from presidential candidates (or, for that matter, adult professionals).
Tur and Thrush described Trump as a hybrid figure, a creation and impresario of every kind of media.
“Prior to covering his presidential campaign, my final two interactions with Donald Trump were not taking his phone calls when he was trying to get an exit ramp built [in New York City],” said Thrush.
Like Ed Koch, a former New York mayor, Trump was for decades best described as “unavoidable for comment,” he said. “I think it’s notable that most of the people who cover him right now started at tabloids in New York City.”
The president hasn’t fully given up his old act: He called Costa’s cellphone minutes after an early Obamacare repeal bill failed.
But if his roots are inky, his power is televisual. Trump supporters across the country have mentioned The Apprentice when explaining their support to Tur. “His supporters would say, Donald Trump will know who to hire. And I’d say, how do you know that?” she said. They would reply: “I’ve seen him on The Apprentice.”
In other words, the press—whom Trump has described as “the enemy of the people”—emerged as less like a target of his ire than just another tool to be exploited. “I think it’s way overstated, their hostility to the press,” said Thrush, referring to Trump and his administration.
“It really was, walking into that building, a very hostile environment for the first three to five months. Now,” he said, “it’s settled into the usual trench warfare.”


Donald Trump and Mark Zuckerberg each told partial truths yesterday.
First, Trump tweeted that “Facebook was always anti-Trump.” From all available information, it does seem true that the vast majority of Facebook’s employees did not want Donald Trump elected president of the United States. They are disproportionately young, urban, and socially liberal, living in California’s most left-wing region. Trump lost all these demographic groups.
Zuckerberg, Facebook’s CEO, responded to Trump with a post about the company’s role in the election. “Trump says Facebook is against him,” he wrote. “Liberals say we helped Trump. Both sides are upset about ideas and content they don’t like. That’s what running a platform for all ideas looks like.”
Trump wants Facebook to be seen as having a traditional anti-Trump bias. Mark Zuckerberg wants the service to be seen as neutral. And they’re both wrong.
Zuckerberg’s statement begins with a play right out of the D.C. congressional playbook: The tough-minded, get-things-done pragmatist knows in his heart that if everyone is mad, he must have done something right.
But the sophisticated critiques of Facebook are not about ideas and content that people don’t like, but rather the new structural forces that Facebook has created. News and information flow differently now than they did before Facebook; capturing the human attention that constitutes that flow is Facebook's raison d’être (and value to advertisers). Now that it has done so, Zuckerberg would like to pretend that his software is a pure conduit through which social and political truths can flow.
The conduit must be pure. The platform must be neutral. Because Mark Zuckerberg wants his company’s role in the election to be seen like this: Facebook had a huge effect on voting—and no impact on votes.
Zuckerberg describes Facebook’s central role in the election himself. “More people had a voice in this election than ever before. There were billions of interactions discussing the issues that may have never happened offline. Every topic was discussed, not just what the media covered,” he wrote. “This was the first U.S. election where the internet was a primary way candidates communicated. Every candidate had a Facebook page to communicate directly with tens of millions of followers every day.”
Facebook even registered 2 million people to vote, which Zuckerberg notes was “bigger than the get-out-the-vote efforts of the Trump and Clinton campaigns put together.”
Half apologizing for calling the idea that the spread of misinformation on his platform swung the election “crazy,” he continued, “the data we have has always shown that our broader impact—from giving people a voice to enabling candidates to communicate directly to helping millions of people vote—played a far bigger role in this election.”
But given all that, couldn’t even small structural wrinkles in Facebook have provided more support for one candidate over another? Are we to believe that despite this admittedly enormous impact on 2016, the platform somehow maintained perfect neutrality with respect to the candidates?
One of the foundational documents of the academic field of science and technology studies is a talk given by Melvin Kranzberg, of the Georgia Institute of Technology. In it, he declared six incisive, playful “laws” of technology. The first one is: “Technology is neither good nor bad; nor is it neutral.”
He explains:
“Technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.”
And this is the rub with Facebook right now. The technology that has, more or less, created Facebook’s current status as the world’s leading purveyor of information is News Feed.
News Feed had to solve a basic problem: There were always too many posts by people you know and organizations you follow to show them all to you. So, Facebook’s engineers decided to find a way to rank them that wasn’t just chronological. It makes some sense: Why show you four posts from Sports Illustrated before showing you a post from your father?
The News Feed, then, takes many pieces of data about each story and the way that story is playing in your social network to order your personal feed. As a technology, it is one of the most successful products of all time. People spend countless hours on Facebook precisely because News Feed keeps showing them stuff that they want to interact with.
During this time, the actions they take on the platform signal to the News Feed that they’re interested in something. The industry calls this engagement. Reading, watching, sharing, commenting, reposting to your own page: That’s all engagement. Posts that generate a lot of it (within one’s network and beyond) are more likely to show up on your feed.
There are a lot of factors to this, Facebook’s engineers would tell you. But the complexity can’t hide the most basic fact: The goals of News Feed have nothing to do with democracy. They might overlap sometimes. But they are not the same.
Zuckerberg’s note ends saying that he wants Facebook to be a “force for good in democracy.” To recognize that one’s massive platform can do good, however, requires an understanding that it could also do bad. You can’t have one without the other. This sin of omission runs throughout Silicon Valley: “Change the world.” “Have an impact.” These are incomplete phrases that render incomplete thoughts.
If Facebook wants to be a force for good in democracy, it needs to answer some questions. Does maximizing engagement, as it is understood through News Feed’s automated analysis, create structural problems in the information ecosystem? More broadly, do the tools that people use to communicate on Facebook influence what they actually talk about?
Facebook might offer the defense that any changes would reflect equally across partisan lines or that there is no systemic bias that gets baked into the system. But let’s just say that one candidate, in a hypothetical election, was very good at driving engagement on Facebook. Perhaps this candidate was hyperbolic and prone to extreme statements that generated controversy. Perhaps this candidate hit hot-button issues and denigrated opponents personally. Perhaps this candidate used the preexisting fractures among the country’s polity to drive a lot of shares and comments, positive and negative.
The other candidate in this hypothetical election was more measured. The remarks the candidate made were primarily about policy. The candidate tried to calm the passions of political followers. Does anyone doubt that this candidate’s engagement would not be as good?
Now multiply that by all the media that both these candidates generate. Multiply that by the people on Facebook who come to understand that posting an anti-Trump meme gets more engagement than a pro-Clinton meme.
The fake news that ran rampant on Facebook was a symptom of a larger issue. The real problem lies at the very heart of Facebook’s most successful product: Perhaps virality and engagement cannot be the basis for a ubiquitous information service that acts as a “force for good in democracy.”
And if this is true, how much is Facebook willing to change?


Tuesday, Twitter announced that it would test 280-character tweets, a doubling of the 140-character standard on the social network.
If you can still read this while gasping for breath and looking for a place to sit down while you absorb this news, I have something to tell you: You broke it, Twitter’s just trying to fix it.
There was a time when Twitter was 140 characters of text. That was it. That was nice and information-dense, but you wanted to add pictures. You wanted to add more text in one tweet than it was possible to write, so you screenshotted the text of stories or even your own notepad.
You started numbering your tweets, tweetstormin’.
Hell, you wanted to add 14-minute videos. You wanted to add livestreaming video of protests and football games and AM2DM.
Whatever measly restriction Twitter tried to hold onto, for the purity of the product, you just routed around. And now, no one can seriously argue that Twitter is only a place for 140-character bits of text. Like, who are we kidding here? And why do we think that the text-message character limits of the early ’00s somehow magically stumbled onto the platonic ideal of message length for a social network built primarily from small chunks of text?
How many of you even remember the time when you could only send 140 characters in a text message on your smartphone? What keeps texts short, and what will keep most Twitter messages short, is the culture of messaging on a phone, which like all cultures is viscous and will change more slowly than the product. You’ll have time to adjust.
So who cares about a doubling of the character limit? It was arbitrary then and it’s arbitrary now. My guess is you’ll see very little difference in the platform, and maybe the success of the change will give Twitter the confidence to focus on what really matters: the communities that have gathered on the service.
People think that Twitter’s brand is built around the 140-characters. But if you ask me, Twitter’s core identity is contained in the @username. The @ is about the people inside Twitter: people who tweet things they shouldn’t, people who tweet about hurricanes all night, people who love books, people who have rare expertise.
You see it best at conferences, around TV shows, and when there is a major local breaking-news event. It’s not that Twitter, as a whole, becomes awesome during these moments. Sometimes, in fact, the user experience breaks. But no other product on the internet quickly sorts out who the important individuals to follow for a given event are.
Twitter’s value has always been in these little pro-am micro-networks, hived off from the larger feed, where anyone with knowledge, wit, or skills can become central to the perception of a moment. That’s not going to change now.


On Tuesday evening, Twitter announced that it is experimenting with doubling the length of tweets, allowing users to post up to 280 characters per message.
To start, the feature will only be available to a random set of users on the service. But if adopted by the platform as a whole, the change will constitute one of the most fundamental changes to Twitter’s core product in years.
“This is a small change, but a big move for us,” said Jack Dorsey, Twitter’s chief executive officer, in one of the first supersized tweets. “One hundred forty was an arbitrary choice based on the 160-character SMS limit.”
Biz Stone, who cofounded the company and returned to it full-time earlier this year, provided more context in another mega tweet:
Originally, our constraint was 160 (limit of a text) minus username. But we noticed @biz got 1 more than @jack. For fairness, we chose 140. Now texts are unlimited. Also, we realize that 140 isn't fair—there are differences between languages. We're testing the limits. Hello 280!
Twitter’s users responded predominantly with jokes, dread, and pandemonium, which is in line with how they respond to most changes to the service. Tech critics seemed more sanguine. Will Oremus, a reporter at Slate and a reliable defender of Twitter’s leadership, predicted that the change would barely alter the service.
Some users questioned why the company was doing anything other than addressing its ongoing problem with user harassment. In July, Twitter said it had ratcheted up enforcement of its rules, bragging that it now takes down “10 times the number of abusive accounts every day compared to the same time last year.”
Because the big-boy tweet feature has yet to debut at scale—as of Tuesday night, I saw no one outside of Twitter’s leadership cabal posting 280-character tweets—most of the coverage remains speculative. So here are seven questions about the hefty tweets—and a first attempt at answering them.
Why would Twitter do such a thing?
Money. Since it went public in late 2013, Twitter has basically never found a firm financial footing. The number of Americans who log on to Twitter every month stayed flat for years—and then, this summer, it began to decline. (U.S. users have outsize importance here: They are the company’s most lucrative demographic.) For years, Twitter dealt with the crisis of its stalling user numbers by wringing more revenue out of the same number of people by selling better-targeted ads.
Now that project has also slowed. Almost four years after debuting on Wall Street at $44, Twitter’s shares linger at $16. The company has never turned a profit.
Once, analysts hoped Twitter would be another Facebook. But Mark Zuckerberg’s monster soared out of its IPO, and it has now come to gobble one out of every five dollars spent on digital ads in the United States. No one would confuse the companies now.
What do 280 characters have to do with money?
Twitter seems to hope that enlivening the service—and dropping one of its trademark constraints—will bring users back to the service. In fact, it explicitly says as much.
In Twitter’s blog post announcing the change, two of its employees compare American Twitter users to Japanese users. The average English-language tweet is 34 characters, they say; the average Japanese-language tweet is only 15. There are specific reasons for this: As a language, Japanese requires many fewer characters per thought; some nouns can be expressed in only one symbol.
“Our research shows us that the character limit is a major cause of frustration for people tweeting in English, but it is not for those tweeting in Japanese,” they say. “Also, in all [language] markets, when people don’t have to cram their thoughts into 140 characters and actually have some to spare, we see more people tweeting—which is awesome!”
If you are a Twitter executive, board member, or long-suffering investor, that does indeed sound awesome.
Is there any forewarning for this?
There’s some. Critics called for it to double its character limit as early as 2011. But from 2010 to 2015, the company’s former chief executive officer, Dick Costolo, seemed to have little appetite to mess with its core product. (It was always unclear if this was because he revered Twitter or because he was never sure how it worked.)
Jack Dorsey, who helped found Twitter, took over the company later that year. He seemed more confident in messing with it.
In early 2016, for instance, he flirted with the idea of allowing 10,000-character addenda to normal-size tweets. This would have essentially given every tweet an optional embedded blog post, mirroring how users can already attach photos, videos, or a poll to their tweet. (I liked this idea, because it would make screenshots of text more accessible to blind tweeters.)
That idea apparently came to naught, but its reception played into the titanic tweets which debuted on Tuesday.
omg tweets are now long enough for signatures!
best regards,
isaac
--
Isaac Hepworth
Product Manager, Google
+1 303 555 4787
What does this mean for thread culture?
Since the election of Donald Trump (roughly), thousands of progressive Twitter users have strung their isolated, anxious thoughts into extended, panicked manifestos by linking them together into “threads.” The company helped promote this behavior by allowing users to string their own tweets together, forming a list of tweets down the page.
This isn’t a new behavior. Twitter users have been doing it since at least 2010. The venture capitalist Marc Andreessen famously made a habit of it during his 2014 Twitter binge. (Back then, the BuzzFeed tech journalist Charlie Warzel dubbed it “tweetstorming.”) Yet its current resurgence is a defining feature of 2017 Twitter—and a threat, by the way, to Medium’s business model.
If director’s-cut tweets roll out to users en masse, I’d expect little change to the length of the longest threads. But perhaps some of the two- or three-tweet threads will be edited down, instead, into 280-character opuses. Concision will live again.
Will this make Twitter better?
It all depends on what you mean by “better.”
Back in 2015, I wrote that Twitter’s greatest cultural ailment (at least for its English-speaking American users) was the onset of metastasized context collapse. Context collapse is what happens when the audience for any online post becomes unstable and untrustworthy—what happens when you don’t know if an offhand Twitter reply, sent to your friend, will wind up on the front page of Breitbart.
This outbreak of context collapse weakened the good faith of Twitter users, I said: It turned the service, which had flourished as something speech-like, where people could have conversations and test out ideas in public; into something print-like, where someone’s tweets were taken as a lasting statement about their core identity.
Then the 2016 election happened—a mass test of internet users’ good faith if there ever was one. And while it’s hard to envision Twitter ever returning to its speechlike roots, the print-like expectations around the service have only hardened. So too have the incentives around setting clear “in” and “out” groups, especially for the service’s highest-profile users in media and entertainment.
As my friend Charlie Loyd put it, in a mock version of Twitter’s terms of service:
2. Content on the Services
You understand and agree to the following stands of content on the Services. Every morning, you will be presented with one of the worst op-eds, blog posts, or other topical views published in your language of choice, anywhere in the world, on the previous day (the “Take”). If you do not tweet about the Take, you implicitly agree with the Take. If you do tweet about the Take, your criticism will be based on one of the following criticisms: (1) that it is neoliberal; or (2) that it is virtue signaling. If at any time you are not addressing a Take, one will be assigned to you in your mentions by an account with a name like @2ndAmdmtLenin.
It’s hard to see XXL tweets making Twitter more speechlike. I anticipate just about the opposite: The prolix tweets will encourage people to attach disclaimers, footnotes, and other dependent clauses to their messages such that tweets are taken as testaments to political identity. But perhaps that’s a good thing: If Twitter is already print-y, why not make it print-ier? (Or, for that matter, more like Facebook statuses? Which is also what this beta test accomplishes.)
That said, Twitter’s most pressing public-relations problem—as far as I can tell—is not context collapse, but the platform’s reputation for abusive unpleasantness and viral controversy. It can’t fix the latter, and it has a long way to go to sufficiently addressing the former.  I doubt its affiliation with President Trump will much improve the perception, popular among Americans of all stripes, that Twitter simply isn’t very fun anymore.
What does it mean for Twitter’s most (in)famous user?
President Trump has not yet commented on the proposed change. However, the president’s distinct philosophy of tweet threading—he sometimes cuts a sentence off at the end of a tweet, then waits minutes to finish the thought—suggests that he bristles at the 140-character count. In July, for instance, he tweeted:
After consultation with my Generals and military experts, please be advised that the United States Government will not accept or allow......
Then he did not immediately supply a direct object. Unnamed Pentagon officials told BuzzFeed News that for several minutes they worried the president was declaring a military action on North Korea. Nine minutes after the original tweet, Trump finished the thought:
....Transgender individuals to serve in any capacity in the U.S. Military. Our military must be focused on decisive and overwhelming.....
....victory and cannot be burdened with the tremendous medical costs and disruption that transgender in the military would entail. Thank you
However, Trump has also touted his skillful use of Twitter’s concision in the past.
“Thanks—many are saying I’m the best 140-character writer in the world,” he tweeted a few days after the 2012 presidential election. “It’s easy when it’s fun.”
He posted a nearly identical thought less than two years later, in 2014. It remains to be seen whether the same people will praise him at 280 characters.
Can you fit an entire Frank O’Hara poem in a tweet now?
Yes, you can. With thanks to Erik Hinton, a designer at The Outline:
Good news! The new Twitter character limit will accommodate the entirety of Frank O'Hara's very appropriate "Spleen". pic.twitter.com/asolmTQPQG
Alas, not all poets are as concise.


There were six hours during the night of April 10, 2014, when the entire population of Washington State had no 911 service. People who called for help got a busy signal. One Seattle woman dialed 911 at least 37 times while a stranger was trying to break into her house. When he finally crawled into her living room through a window, she picked up a kitchen knife. The man fled.
To hear more feature stories, see our full list or get the Audm iPhone app.
The 911 outage, at the time the largest ever reported, was traced to software running on a server in Englewood, Colorado. Operated by a systems provider named Intrado, the server kept a running counter of how many calls it had routed to 911 dispatchers around the country. Intrado programmers had set a threshold for how high the counter could go. They picked a number in the millions.
Shortly before midnight on April 10, the counter exceeded that number, resulting in chaos. Because the counter was used to generate a unique identifier for each call, new calls were rejected. And because the programmers hadn’t anticipated the problem, they hadn’t created alarms to call attention to it. Nobody knew what was happening. Dispatch centers in Washington, California, Florida, the Carolinas, and Minnesota, serving 11 million Americans, struggled to make sense of reports that callers were getting busy signals. It took until morning to realize that Intrado’s software in Englewood was responsible, and that the fix was to change a single number.
Not long ago, emergency calls were handled locally. Outages were small and easily diagnosed and fixed. The rise of cellphones and the promise of new capabilities—what if you could text 911? or send videos to the dispatcher?—drove the development of a more complex system that relied on the internet. For the first time, there could be such a thing as a national 911 outage. There have now been four in as many years.
It’s been said that software is “eating the world.” More and more, critical systems that were once controlled mechanically, or by people, are coming to depend on code. This was perhaps never clearer than in the summer of 2015, when on a single day, United Airlines grounded its fleet because of a problem with its departure-management system; trading was suspended on the New York Stock Exchange after an upgrade; the front page of The Wall Street Journal’s website crashed; and Seattle’s 911 system went down again, this time because a different router failed. The simultaneous failure of so many software systems smelled at first of a coordinated cyberattack. Almost more frightening was the realization, late in the day, that it was just a coincidence.
“When we had electromechanical systems, we used to be able to test them exhaustively,” says Nancy Leveson, a professor of aeronautics and astronautics at the Massachusetts Institute of Technology who has been studying software safety for 35 years. She became known for her report on the Therac-25, a radiation-therapy machine that killed six patients because of a software error. “We used to be able to think through all the things it could do, all the states it could get into.” The electromechanical interlockings that controlled train movements at railroad crossings, for instance, only had so many configurations; a few sheets of paper could describe the whole system, and you could run physical trains against each configuration to see how it would behave. Once you’d built and tested it, you knew exactly what you were dealing with.
Software is different. Just by editing the text in a file somewhere, the same hunk of silicon can become an autopilot or an inventory-control system. This flexibility is software’s miracle, and its curse. Because it can be changed cheaply, software is constantly changed; and because it’s unmoored from anything physical—a program that is a thousand times more complex than another takes up the same actual space—it tends to grow without bound. “The problem,” Leveson wrote in a book, “is that we are attempting to build systems that are beyond our ability to intellectually manage.”
Our standard framework for thinking about engineering failures—reflected, for instance, in regulations for medical devices—was developed shortly after World War II, before the advent of software, for electromechanical systems. The idea was that you make something reliable by making its parts reliable (say, you build your engine to withstand 40,000 takeoff-and-landing cycles) and by planning for the breakdown of those parts (you have two engines). But software doesn’t break. Intrado’s faulty threshold is not like the faulty rivet that leads to the crash of an airliner. The software did exactly what it was told to do. In fact it did it perfectly. The reason it failed is that it was told to do the wrong thing. Software failures are failures of understanding, and of imagination. Intrado actually had a backup router, which, had it been switched to automatically, would have restored 911 service almost immediately. But, as described in a report to the FCC, “the situation occurred at a point in the application logic that was not designed to perform any automated corrective actions.”
This is the trouble with making things out of code, as opposed to something physical. “The complexity,” as Leveson puts it, “is invisible to the eye.”
The attempts now underway to change how we make software all seem to start with the same premise: Code is too hard to think about. Before trying to understand the attempts themselves, then, it’s worth understanding why this might be: what it is about code that makes it so foreign to the mind, and so unlike anything that came before it.
Technological progress used to change the way the world looked—you could watch the roads getting paved; you could see the skylines rise. Today you can hardly tell when something is remade, because so often it is remade by code. When you press your foot down on your car’s accelerator, for instance, you’re no longer controlling anything directly; there’s no mechanical link from the pedal to the throttle. Instead, you’re issuing a command to a piece of software that decides how much air to give the engine. The car is a computer you can sit inside of. The steering wheel and pedals might as well be keyboard keys.
Like everything else, the car has been computerized to enable new features. When a program is in charge of the throttle and brakes, it can slow you down when you’re too close to another car, or precisely control the fuel injection to help you save on gas. When it controls the steering, it can keep you in your lane as you start to drift, or guide you into a parking space. You couldn’t build these features without code. If you tried, a car might weigh 40,000 pounds, an immovable mass of clockwork.
Software has enabled us to make the most intricate machines that have ever existed. And yet we have hardly noticed, because all of that complexity is packed into tiny silicon chips as millions and millions of lines of code. But just because we can’t see the complexity doesn’t mean that it has gone away.
The programmer, the renowned Dutch computer scientist Edsger Dijkstra wrote in 1988, “has to be able to think in terms of conceptual hierarchies that are much deeper than a single mind ever needed to face before.” Dijkstra meant this as a warning. As programmers eagerly poured software into critical systems, they became, more and more, the linchpins of the built world—and Dijkstra thought they had perhaps overestimated themselves.
What made programming so difficult was that it required you to think like a computer. The strangeness of it was in some sense more vivid in the early days of computing, when code took the form of literal ones and zeros. Anyone looking over a programmer’s shoulder as they pored over line after line like “100001010011” and “000010011110” would have seen just how alienated the programmer was from the actual problems they were trying to solve; it would have been impossible to tell whether they were trying to calculate artillery trajectories or simulate a game of tic-tac-toe. The introduction of programming languages like Fortran and C, which resemble English, and tools, known as “integrated development environments,” or IDEs, that help correct simple mistakes (like Microsoft Word’s grammar checker but for code), obscured, though did little to actually change, this basic alienation—the fact that the programmer didn’t work on a problem directly, but rather spent their days writing out instructions for a machine.
“The problem is that software engineers don’t understand the problem they’re trying to solve, and don’t care to,” says Leveson, the MIT software-safety expert. The reason is that they’re too wrapped up in getting their code to work. “Software engineers like to provide all kinds of tools and stuff for coding errors,” she says, referring to IDEs. “The serious problems that have happened with software have to do with requirements, not coding errors.” When you’re writing code that controls a car’s throttle, for instance, what’s important is the rules about when and how and by how much to open it. But these systems have become so complicated that hardly anyone can keep them straight in their head. “There’s 100 million lines of code in cars now,” Leveson says. “You just cannot anticipate all these things.”
In September 2007, Jean Bookout was driving on the highway with her best friend in a Toyota Camry when the accelerator seemed to get stuck. When she took her foot off the pedal, the car didn’t slow down. She tried the brakes but they seemed to have lost their power. As she swerved toward an off-ramp going 50 miles per hour, she pulled the emergency brake. The car left a skid mark 150 feet long before running into an embankment by the side of the road. The passenger was killed. Bookout woke up in a hospital a month later.
The incident was one of many in a nearly decade-long investigation into claims of so-called unintended acceleration in Toyota cars. Toyota blamed the incidents on poorly designed floor mats, “sticky” pedals, and driver error, but outsiders suspected that faulty software might be responsible. The National Highway Traffic Safety Administration enlisted software experts from NASA to perform an intensive review of Toyota’s code. After nearly 10 months, the NASA team hadn’t found evidence that software was the cause—but said they couldn’t prove it wasn’t.
It was during litigation of the Bookout accident that someone finally found a convincing connection. Michael Barr, an expert witness for the plaintiff, had a team of software experts spend 18 months with the Toyota code, picking up where NASA left off. Barr described what they found as “spaghetti code,” programmer lingo for software that has become a tangled mess. Code turns to spaghetti when it accretes over many years, with feature after feature piling on top of, and being woven around, what’s already there; eventually the code becomes impossible to follow, let alone to test exhaustively for flaws.
Using the same model as the Camry involved in the accident, Barr’s team demonstrated that there were more than 10 million ways for key tasks on the onboard computer to fail, potentially leading to unintended acceleration.* They showed that as little as a single bit flip—a one in the computer’s memory becoming a zero or vice versa—could make a car run out of control. The fail-safe code that Toyota had put in place wasn’t enough to stop it. “You have software watching the software,” Barr testified. “If the software malfunctions and the same program or same app that is crashed is supposed to save the day, it can’t save the day because it is not working.”
Barr’s testimony made the case for the plaintiff, resulting in $3 million in damages for Bookout and her friend’s family. According to The New York Times, it was the first of many similar cases against Toyota to bring to trial problems with the electronic throttle-control system, and the first time Toyota was found responsible by a jury for an accident involving unintended acceleration. The parties decided to settle the case before punitive damages could be awarded. In all, Toyota recalled more than 9 million cars, and paid nearly $3 billion in settlements and fines related to unintended acceleration.
There will be more bad days for software. It's important that we get better at making it, because if we don't, and as software becomes more sophisticated and connected—as it takes control of more critical functions—those days could get worse.
The problem is that programmers are having a hard time keeping up with their own creations. Since the 1980s, the way programmers work and the tools they use have changed remarkably little. There is a small but growing chorus that worries the status quo is unsustainable. “Even very good programmers are struggling to make sense of the systems that they are working with,” says Chris Granger, a software developer who worked as a lead at Microsoft on Visual Studio, an IDE that costs $1,199 a year and is used by nearly a third of all professional programmers. He told me that while he was at Microsoft, he arranged an end-to-end study of Visual Studio, the only one that had ever been done. For a month and a half, he watched behind a one-way mirror as people wrote code. “How do they use tools? How do they think?” he said. “How do they sit at the computer, do they touch the mouse, do they not touch the mouse? All these things that we have dogma around that we haven’t actually tested empirically.”
The findings surprised him. “Visual Studio is one of the single largest pieces of software in the world,” he said. “It’s over 55 million lines of code. And one of the things that I found out in this study is more than 98 percent of it is completely irrelevant. All this work had been put into this thing, but it missed the fundamental problems that people faced. And the biggest one that I took away from it was that basically people are playing computer inside their head.” Programmers were like chess players trying to play with a blindfold on—so much of their mental energy is spent just trying to picture where the pieces are that there’s hardly any left over to think about the game itself.
John Resig had been noticing the same thing among his students. Resig is a celebrated programmer of JavaScript—software he wrote powers over half of all websites—and a tech lead at the online-education site Khan Academy. In early 2012, he had been struggling with the site’s computer-science curriculum. Why was it so hard to learn to program? The essential problem seemed to be that code was so abstract. Writing software was not like making a bridge out of popsicle sticks, where you could see the sticks and touch the glue. To “make” a program, you typed words. When you wanted to change the behavior of the program, be it a game, or a website, or a simulation of physics, what you actually changed was text. So the students who did well—in fact the only ones who survived at all—were those who could step through that text one instruction at a time in their head, thinking the way a computer would, trying to keep track of every intermediate calculation. Resig, like Granger, started to wonder if it had to be that way. Computers had doubled in power every 18 months for the last 40 years. Why hadn’t programming changed?
The fact that the two of them were thinking about the same problem in the same terms, at the same time, was not a coincidence. They had both just seen the same remarkable talk, given to a group of software-engineering students in a Montreal hotel by a computer researcher named Bret Victor. The talk, which went viral when it was posted online in February 2012, seemed to be making two bold claims. The first was that the way we make software is fundamentally broken. The second was that Victor knew how to fix it.
Bret Victor does not like to write code. “It sounds weird,” he says. “When I want to make a thing, especially when I want to create something in software, there’s this initial layer of disgust that I have to push through, where I’m not manipulating the thing that I want to make, I’m writing a bunch of text into a text editor.”
“There’s a pretty strong conviction that that’s the wrong way of doing things.”
Victor has the mien of David Foster Wallace, with a lightning intelligence that lingers beneath a patina of aw-shucks shyness. He is 40 years old, with traces of gray and a thin, undeliberate beard. His voice is gentle, mournful almost, but he wants to share what’s in his head, and when he gets on a roll he’ll seem to skip syllables, as though outrunning his own vocal machinery.
Though he runs a lab that studies the future of computing, he seems less interested in technology per se than in the minds of the people who use it. Like any good toolmaker, he has a way of looking at the world that is equal parts technical and humane. He graduated top of his class at the California Institute of Technology for electrical engineering, and then went on, after grad school at the University of California, Berkeley, to work at a company that develops music synthesizers. It was a problem perfectly matched to his dual personality: He could spend as much time thinking about the way a performer makes music with a keyboard—the way it becomes an extension of their hands—as he could thinking about the mathematics of digital signal processing.
By the time he gave the talk that made his name, the one that Resig and Granger saw in early 2012, Victor had finally landed upon the principle that seemed to thread through all of his work. (He actually called the talk “Inventing on Principle.”) The principle was this: “Creators need an immediate connection to what they’re creating.” The problem with programming was that it violated the principle. That’s why software systems were so hard to think about, and so rife with bugs: The programmer, staring at a page of text, was abstracted from whatever it was they were actually making.
“Our current conception of what a computer program is,” he said, is “derived straight from Fortran and ALGOL in the late ’50s. Those languages were designed for punch cards.” That code now takes the form of letters on a screen in a language like C or Java (derivatives of Fortran and ALGOL), instead of a stack of cards with holes in it, doesn’t make it any less dead, any less indirect.
There is an analogy to word processing. It used to be that all you could see in a program for writing documents was the text itself, and to change the layout or font or margins, you had to write special “control codes,” or commands that would tell the computer that, for instance, “this part of the text should be in italics.” The trouble was that you couldn’t see the effect of those codes until you printed the document. It was hard to predict what you were going to get. You had to imagine how the codes were going to be interpreted by the computer—that is, you had to play computer in your head.
Then WYSIWYG (pronounced “wizzywig”) came along. It stood for “What You See Is What You Get.” When you marked a passage as being in italics, the letters tilted right there on the screen. If you wanted to change the margin, you could drag a ruler at the top of the screen—and see the effect of that change. The document thereby came to feel like something real, something you could poke and prod at. Just by looking you could tell if you’d done something wrong. Control of a sophisticated system—the document’s layout and formatting engine—was made accessible to anyone who could click around on a page.
Victor’s point was that programming itself should be like that. For him, the idea that people were doing important work, like designing adaptive cruise-control systems or trying to understand cancer, by staring at a text editor, was appalling. And it was the proper job of programmers to ensure that someday they wouldn’t have to.
There was precedent enough to suggest that this wasn’t a crazy idea. Photoshop, for instance, puts powerful image-processing algorithms in the hands of people who might not even know what an algorithm is. It’s a complicated piece of software, but complicated in the way a good synth is complicated, with knobs and buttons and sliders that the user learns to play like an instrument. Squarespace, a company that is perhaps best known for advertising aggressively on podcasts, makes a tool that lets users build websites by pointing and clicking, instead of by writing code in HTML and CSS. It is powerful enough to do work that once would have been done by a professional web designer.
But those were just a handful of examples. The overwhelming reality was that when someone wanted to do something interesting with a computer, they had to write code. Victor, who is something of an idealist, saw this not so much as an opportunity but as a moral failing of programmers at large. His talk was a call to arms.
At the heart of it was a series of demos that tried to show just how primitive the available tools were for various problems—circuit design, computer animation, debugging algorithms—and what better ones might look like. His demos were virtuosic. The one that captured everyone’s imagination was, ironically enough, the one that on its face was the most trivial. It showed a split screen with a game that looked like Mario on one side and the code that controlled it on the other. As Victor changed the code, things in the game world changed: He decreased one number, the strength of gravity, and the Mario character floated; he increased another, the player’s speed, and Mario raced across the screen.
Suppose you wanted to design a level where Mario, jumping and bouncing off of a turtle, would just make it into a small passageway. Game programmers were used to solving this kind of problem in two stages: First, you stared at your code—the code controlling how high Mario jumped, how fast he ran, how bouncy the turtle’s back was—and made some changes to it in your text editor, using your imagination to predict what effect they’d have. Then, you’d replay the game to see what actually happened.
Victor wanted something more immediate. “If you have a process in time,” he said, referring to Mario’s path through the level, “and you want to see changes immediately, you have to map time to space.” He hit a button that showed not just where Mario was right now, but where he would be at every moment in the future: a curve of shadow Marios stretching off into the far distance. What’s more, this projected path was reactive: When Victor changed the game’s parameters, now controlled by a quick drag of the mouse, the path’s shape changed. It was like having a god’s-eye view of the game. The whole problem had been reduced to playing with different parameters, as if adjusting levels on a stereo receiver, until you got Mario to thread the needle. With the right interface, it was almost as if you weren’t working with code at all; you were manipulating the game’s behavior directly.
When the audience first saw this in action, they literally gasped. They knew they weren’t looking at a kid’s game, but rather the future of their industry. Most software involved behavior that unfolded, in complex ways, over time, and Victor had shown that if you were imaginative enough, you could develop ways to see that behavior and change it, as if playing with it in your hands. One programmer who saw the talk wrote later: “Suddenly all of my tools feel obsolete.”
When John Resig saw the “Inventing on Principle” talk, he scrapped his plans for the Khan Academy programming curriculum. He wanted the site’s programming exercises to work just like Victor’s demos. On the left-hand side you’d have the code, and on the right, the running program: a picture or game or simulation. If you changed the code, it’d instantly change the picture. “In an environment that is truly responsive,” Resig wrote about the approach, “you can completely change the model of how a student learns ... [They] can now immediately see the result and intuit how underlying systems inherently work without ever following an explicit explanation.” Khan Academy has become perhaps the largest computer-programming class in the world, with a million students, on average, actively using the program each month.
Chris Granger, who had worked at Microsoft on Visual Studio, was likewise inspired. Within days of seeing a video of Victor’s talk, in January of 2012, he built a prototype of a new programming environment. Its key capability was that it would give you instant feedback on your program’s behavior. You’d see what your system was doing right next to the code that controlled it. It was like taking off a blindfold. Granger called the project “Light Table.”
In April of 2012, he sought funding for Light Table on Kickstarter. In programming circles, it was a sensation. Within a month, the project raised more than $200,000. The ideas spread. The notion of liveness, of being able to see data flowing through your program instantly, made its way into flagship programming tools offered by Google and Apple. The default language for making new iPhone and Mac apps, called Swift, was developed by Apple from the ground up to support an environment, called Playgrounds, that was directly inspired by Light Table.
But seeing the impact that his talk ended up having, Bret Victor was disillusioned. “A lot of those things seemed like misinterpretations of what I was saying,” he said later. He knew something was wrong when people began to invite him to conferences to talk about programming tools. “Everyone thought I was interested in programming environments,” he said. Really he was interested in how people see and understand systems—as he puts it, in the “visual representation of dynamic behavior.” Although code had increasingly become the tool of choice for creating dynamic behavior, it remained one of the worst tools for understanding it. The point of “Inventing on Principle” was to show that you could mitigate that problem by making the connection between a system’s behavior and its code immediate.
In a pair of later talks, “Stop Drawing Dead Fish” and “Drawing Dynamic Visualizations,” Victor went one further. He demoed two programs he’d built—the first for animators, the second for scientists trying to visualize their data—each of which took a process that used to involve writing lots of custom code and reduced it to playing around in a WYSIWYG interface. Victor suggested that the same trick could be pulled for nearly every problem where code was being written today. “I’m not sure that programming has to exist at all,” he told me. “Or at least software developers.” In his mind, a software developer’s proper role was to create tools that removed the need for software developers. Only then would people with the most urgent computational problems be able to grasp those problems directly, without the intermediate muck of code.
Of course, to do that, you’d have to get programmers themselves on board. In a recent essay, Victor implored professional software developers to stop pouring their talent into tools for building apps like Snapchat and Uber. “The inconveniences of daily life are not the significant problems,” he wrote. Instead, they should focus on scientists and engineers—as he put it to me, “these people that are doing work that actually matters, and critically matters, and using really, really bad tools.” Exciting work of this sort, in particular a class of tools for “model-based design,” was already underway, he wrote, and had been for years, but most programmers knew nothing about it.
“If you really look hard at all the industrial goods that you’ve got out there, that you’re using, that companies are using, the only non-industrial stuff that you have inside this is the code.” Eric Bantégnie is the founder of Esterel Technologies (now owned by ANSYS), a French company that makes tools for building safety-critical software. Like Victor, Bantégnie doesn’t think engineers should develop large systems by typing millions of lines of code into an IDE. “Nobody would build a car by hand,” he says. “Code is still, in many places, handicraft. When you’re crafting manually 10,000 lines of code, that’s okay. But you have systems that have 30 million lines of code, like an Airbus, or 100 million lines of code, like your Tesla or high-end cars—that’s becoming very, very complicated.”
Bantégnie’s company is one of the pioneers in the industrial use of model-based design, in which you no longer write code directly. Instead, you create a kind of flowchart that describes the rules your program should follow (the “model”), and the computer generates code for you based on those rules. If you were making the control system for an elevator, for instance, one rule might be that when the door is open, and someone presses the button for the lobby, you should close the door and start moving the car. In a model-based design tool, you’d represent this rule with a small diagram, as though drawing the logic out on a whiteboard, made of boxes that represent different states—like “door open,” “moving,” and “door closed”—and lines that define how you can get from one state to the other. The diagrams make the system’s rules obvious: Just by looking, you can see that the only way to get the elevator moving is to close the door, or that the only way to get the door open is to stop.
It’s not quite Photoshop. The beauty of Photoshop, of course, is that the picture you’re manipulating on the screen is the final product. In model-based design, by contrast, the picture on your screen is more like a blueprint. Still, making software this way is qualitatively different than traditional programming. In traditional programming, your task is to take complex rules and translate them into code; most of your energy is spent doing the translating, rather than thinking about the rules themselves. In the model-based approach, all you have is the rules. So that’s what you spend your time thinking about. It’s a way of focusing less on the machine and more on the problem you’re trying to get it to solve.
“Typically the main problem with software coding—and I’m a coder myself,” Bantégnie says, “is not the skills of the coders. The people know how to code. The problem is what to code. Because most of the requirements are kind of natural language, ambiguous, and a requirement is never extremely precise, it’s often understood differently by the guy who’s supposed to code.”
On this view, software becomes unruly because the media for describing what software should do—conversations, prose descriptions, drawings on a sheet of paper—are too different from the media describing what software does do, namely, code itself. Too much is lost going from one to the other. The idea behind model-based design is to close the gap. The very same model is used both by system designers to express what they want and by the computer to automatically generate code.
Of course, for this approach to succeed, much of the work has to be done well before the project even begins. Someone first has to build a tool for developing models that are natural for people—that feel just like the notes and drawings they’d make on their own—while still being unambiguous enough for a computer to understand. They have to make a program that turns these models into real code. And finally they have to prove that the generated code will always do what it’s supposed to. “We have benefited from fortunately 20 years of initial background work,” Bantégnie says.
Esterel Technologies, which was acquired by ANSYS in 2012, grew out of research begun in the 1980s by the French nuclear and aerospace industries, who worried that as safety-critical code ballooned in complexity, it was getting harder and harder to keep it free of bugs. “I started in 1988,” says Emmanuel Ledinot, the Head of Scientific Studies for Dassault Aviation, a French manufacturer of fighter jets and business aircraft. “At the time, I was working on military avionics systems. And the people in charge of integrating the systems, and debugging them, had noticed that the number of bugs was increasing.” The 80s had seen a surge in the number of onboard computers on planes. Instead of a single flight computer, there were now dozens, each responsible for highly specialized tasks related to control, navigation, and communications. Coordinating these systems to fly the plane as data poured in from sensors and as pilots entered commands required a symphony of perfectly timed reactions. “The handling of these hundreds of and even thousands of possible events in the right order, at the right time,” Ledinot says, “was diagnosed as the main cause of the bug inflation.”
Ledinot decided that writing such convoluted code by hand was no longer sustainable. It was too hard to understand what it was doing, and almost impossible to verify that it would work correctly. He went looking for something new. “You must understand that to change tools is extremely expensive in a process like this,” he said in a talk. “You don’t take this type of decision unless your back is against the wall.”
He began collaborating with Gerard Berry, a computer scientist at INRIA, the French computing-research center, on a tool called Esterel—a portmanteau of the French for “real-time.” The idea behind Esterel was that while traditional programming languages might be good for describing simple procedures that happened in a predetermined order—like a recipe—if you tried to use them in systems where lots of events could happen at nearly any time, in nearly any order—like in the cockpit of a plane—you inevitably got a mess. And a mess in control software was dangerous. In a paper, Berry went as far as to predict that “low-level programming techniques will not remain acceptable for large safety-critical programs, since they make behavior understanding and analysis almost impracticable.”
Esterel was designed to make the computer handle this complexity for you. That was the promise of the model-based approach: Instead of writing normal programming code, you created a model of the system’s behavior—in this case, a model focused on how individual events should be handled, how to prioritize events, which events depended on which others, and so on. The model becomes the detailed blueprint that the computer would use to do the actual programming.
Ledinot and Berry worked for nearly 10 years to get Esterel to the point where it could be used in production. “It was in 2002 that we had the first operational software-modeling environment with automatic code generation,” Ledinot told me, “and the first embedded module in Rafale, the combat aircraft.” Today, the ANSYS SCADE product family (for “safety-critical application development environment”) is used to generate code by companies in the aerospace and defense industries, in nuclear power plants, transit systems, heavy industry, and medical devices. “My initial dream was to have SCADE-generated code in every plane in the world,” Bantégnie, the founder of Esterel Technologies, says, “and we’re not very far off from that objective.” Nearly all safety-critical code on the Airbus A380, including the system controlling the plane’s flight surfaces, was generated with ANSYS SCADE products.
Part of the draw for customers, especially in aviation, is that while it is possible to build highly reliable software by hand, it can be a Herculean effort. Ravi Shivappa, the VP of group software engineering at Meggitt PLC, an ANSYS customer which builds components for airplanes, like pneumatic fire detectors for engines, explains that traditional projects begin with a massive requirements document in English, which specifies everything the software should do. (A requirement might be something like, “When the pressure in this section rises above a threshold, open the safety valve, unless the manual-override switch is turned on.”) The problem with describing the requirements this way is that when you implement them in code, you have to painstakingly check that each one is satisfied. And when the customer changes the requirements, the code has to be changed, too, and tested extensively to make sure that nothing else was broken in the process.
The cost is compounded by exacting regulatory standards. The FAA is fanatical about software safety. The agency mandates that every requirement for a piece of safety-critical software be traceable to the lines of code that implement it, and vice versa. So every time a line of code changes, it must be retraced to the corresponding requirement in the design document, and you must be able to demonstrate that the code actually satisfies the requirement. The idea is that if something goes wrong, you’re able to figure out why; the practice brings order and accountability to large codebases. But, Shivappa says, “it’s a very labor-intensive process.” He estimates that before they used model-based design, on a two-year-long project only two to three months was spent writing code—the rest was spent working on the documentation.
As Bantégnie explains, the beauty of having a computer turn your requirements into code, rather than a human, is that you can be sure—in fact you can mathematically prove—that the generated code actually satisfies those requirements. Much of the benefit of the model-based approach comes from being able to add requirements on the fly while still ensuring that existing ones are met; with every change, the computer can verify that your program still works. You’re free to tweak your blueprint without fear of introducing new bugs. Your code is, in FAA parlance, “correct by construction.”
Still, most software, even in the safety-obsessed world of aviation, is made the old-fashioned way, with engineers writing their requirements in prose and programmers coding them up in a programming language like C. As Bret Victor made clear in his essay, model-based design is relatively unusual. “A lot of people in the FAA think code generation is magic, and hence call for greater scrutiny,” Shivappa told me.
Most programmers feel the same way. They like code. At least they understand it. Tools that write your code for you and verify its correctness using the mathematics of “finite-state machines” and “recurrent systems” sound esoteric and hard to use, if not just too good to be true.
It is a pattern that has played itself out before. Whenever programming has taken a step away from the writing of literal ones and zeros, the loudest objections have come from programmers. Margaret Hamilton, a celebrated software engineer on the Apollo missions—in fact the coiner of the phrase “software engineering”—told me that during her first year at the Draper lab at MIT, in 1964, she remembers a meeting where one faction was fighting the other about transitioning away from “some very low machine language,” as close to ones and zeros as you could get, to “assembly language.” “The people at the lowest level were fighting to keep it. And the arguments were so similar: ‘Well how do we know assembly language is going to do it right?’”
“Guys on one side, their faces got red, and they started screaming,” she said. She said she was “amazed how emotional they got.”
Emmanuel Ledinot, of Dassault Aviation, pointed out that when assembly language was itself phased out in favor of the programming languages still popular today, like C, it was the assembly programmers who were skeptical this time. No wonder, he said, that “people are not so easily transitioning to model-based software development: They perceive it as another opportunity to lose control, even more than they have already.”
The bias against model-based design, sometimes known as model-driven engineering, or MDE, is in fact so ingrained that according to a recent paper, “Some even argue that there is a stronger need to investigate people’s perception of MDE than to research new MDE technologies.”
Which sounds almost like a joke, but for proponents of the model-based approach, it’s an important point: We already know how to make complex software reliable, but in so many places, we’re choosing not to. Why?
In 2011, Chris Newcombe had been working at Amazon for almost seven years, and had risen to be a principal engineer. He had worked on some of the company’s most critical systems, including the retail-product catalog and the infrastructure that managed every Kindle device in the world. He was a leader on the highly prized Amazon Web Services team, which maintains cloud servers for some of the web’s biggest properties, like Netflix, Pinterest, and Reddit. Before Amazon, he’d helped build the backbone of Steam, the world’s largest online-gaming service. He is one of those engineers whose work quietly keeps the internet running. The products he’d worked on were considered massive successes. But all he could think about was that buried deep in the designs of those systems were disasters waiting to happen.
“Human intuition is poor at estimating the true probability of supposedly ‘extremely rare’ combinations of events in systems operating at a scale of millions of requests per second,” he wrote in a paper. “That human fallibility means that some of the more subtle, dangerous bugs turn out to be errors in design; the code faithfully implements the intended design, but the design fails to correctly handle a particular ‘rare’ scenario.”
Newcombe was convinced that the algorithms behind truly critical systems—systems storing a significant portion of the web’s data, for instance—ought to be not just good, but perfect. A single subtle bug could be catastrophic. But he knew how hard bugs were to find, especially as an algorithm grew more complex. You could do all the testing you wanted and you’d never find them all.
This is why he was so intrigued when, in the appendix of a paper he’d been reading, he came across a strange mixture of math and code—or what looked like code—that described an algorithm in something called “TLA+.” The surprising part was that this description was said to be mathematically precise: An algorithm written in TLA+ could in principle be proven correct. In practice, it allowed you to create a realistic model of your problem and test it not just thoroughly, but exhaustively. This was exactly what he’d been looking for: a language for writing perfect algorithms.
TLA+, which stands for “Temporal Logic of Actions,” is similar in spirit to model-based design: It’s a language for writing down the requirements—TLA+ calls them “specifications”—of computer programs. These specifications can then be completely verified by a computer. That is, before you write any code, you write a concise outline of your program’s logic, along with the constraints you need it to satisfy (say, if you were programming an ATM, a constraint might be that you can never withdraw the same money twice from your checking account). TLA+ then exhaustively checks that your logic does, in fact, satisfy those constraints. If not, it will show you exactly how they could be violated.
The language was invented by Leslie Lamport, a Turing Award–winning computer scientist. With a big white beard and scruffy white hair, and kind eyes behind large glasses, Lamport looks like he might be one of the friendlier professors at the American Hogwarts. Now at Microsoft Research, he is known as one of the pioneers of the theory of “distributed systems,” which describes any computer system made of multiple parts that communicate with each other. Lamport’s work laid the foundation for many of the systems that power the modern web.
For Lamport, a major reason today’s software is so full of bugs is that programmers jump straight into writing code. “Architects draw detailed plans before a brick is laid or a nail is hammered,” he wrote in an article. “But few programmers write even a rough sketch of what their programs will do before they start coding.” Programmers are drawn to the nitty-gritty of coding because code is what makes programs go; spending time on anything else can seem like a distraction. And there is a patient joy, a meditative kind of satisfaction, to be had from puzzling out the micro-mechanics of code. But code, Lamport argues, was never meant to be a medium for thought. “It really does constrain your ability to think when you’re thinking in terms of a programming language,” he says. Code makes you miss the forest for the trees: It draws your attention to the working of individual pieces, rather than to the bigger picture of how your program fits together, or what it’s supposed to do—and whether it actually does what you think. This is why Lamport created TLA+. As with model-based design, TLA+ draws your focus to the high-level structure of a system, its essential logic, rather than to the code that implements it.
Newcombe and his colleagues at Amazon would go on to use TLA+ to find subtle, critical bugs in major systems, including bugs in the core algorithms behind S3, regarded as perhaps the most reliable storage engine in the world. It is now used widely at the company. In the tiny universe of people who had ever used TLA+, their success was not so unusual. An intern at Microsoft used TLA+ to catch a bug that could have caused every Xbox in the world to crash after four hours of use. Engineers at the European Space Agency used it to rewrite, with 10 times less code, the operating system of a probe that was the first to ever land softly on a comet. Intel uses it regularly to verify its chips.
But TLA+ occupies just a small, far corner of the mainstream, if it can be said to take up any space there at all. Even to a seasoned engineer like Newcombe, the language read at first as bizarre and esoteric—a zoo of symbols. For Lamport, this is a failure of education. Though programming was born in mathematics, it has since largely been divorced from it. Most programmers aren’t very fluent in the kind of math—logic and set theory, mostly—that you need to work with TLA+. “Very few programmers—and including very few teachers of programming—understand the very basic concepts and how they’re applied in practice. And they seem to think that all they need is code,” Lamport says. “The idea that there’s some higher level than the code in which you need to be able to think precisely, and that mathematics actually allows you to think precisely about it, is just completely foreign. Because they never learned it.”
Lamport sees this failure to think mathematically about what they’re doing as the problem of modern software development in a nutshell: The stakes keep rising, but programmers aren’t stepping up—they haven’t developed the chops required to handle increasingly complex problems. “In the 15th century,” he said, “people used to build cathedrals without knowing calculus, and nowadays I don’t think you’d allow anyone to build a cathedral without knowing calculus. And I would hope that after some suitably long period of time, people won’t be allowed to write programs if they don’t understand these simple things.”
Newcombe isn’t so sure that it’s the programmer who is to blame. “I’ve heard from Leslie that he thinks programmers are afraid of math. I’ve found that programmers aren’t aware—or don’t believe—that math can help them handle complexity. Complexity is the biggest challenge for programmers.” The real problem in getting people to use TLA+, he said, was convincing them it wouldn’t be a waste of their time. Programmers, as a species, are relentlessly pragmatic. Tools like TLA+ reek of the ivory tower. When programmers encounter “formal methods” (so called because they involve mathematical, “formally” precise descriptions of programs), their deep-seated instinct is to recoil.
Most programmers who took computer science in college have briefly encountered formal methods. Usually they’re demonstrated on something trivial, like a program that counts up from zero; the student’s job is to mathematically prove that the program does, in fact, count up from zero.
“I needed to change people’s perceptions on what formal methods were,” Newcombe told me. Even Lamport himself didn’t seem to fully grasp this point: Formal methods had an image problem. And the way to fix it wasn’t to implore programmers to change—it was to change yourself. Newcombe realized that to bring tools like TLA+ to the programming mainstream, you had to start speaking their language.
For one thing, he said that when he was introducing colleagues at Amazon to TLA+ he would avoid telling them what it stood for, because he was afraid the name made it seem unnecessarily forbidding: “Temporal Logic of Actions” has exactly the kind of highfalutin ring to it that plays well in academia, but puts off most practicing programmers. He tried also not to use the terms “formal,” “verification,” or “proof,” which reminded programmers of tedious classroom exercises. Instead, he presented TLA+ as a new kind of “pseudocode,” a stepping-stone to real code that allowed you to exhaustively test your algorithms—and that got you thinking precisely early on in the design process. “Engineers think in terms of debugging rather than ‘verification,’” he wrote, so he titled his internal talk on the subject to fellow Amazon engineers “Debugging Designs.” Rather than bemoan the fact that programmers see the world in code, Newcombe embraced it. He knew he’d lose them otherwise. “I’ve had a bunch of people say, ‘Now I get it,’” Newcombe says.
He has since left Amazon for Oracle, where he’s been able to convince his new colleagues to give TLA+ a try. For him, using these tools is now a matter of responsibility. “We need to get better at this,” he said.
“I’m self-taught, been coding since I was nine, so my instincts were to start coding. That was my only—that was my way of thinking: You’d sketch something, try something, you’d organically evolve it.” In his view, this is what many programmers today still do. “They google, and they look on Stack Overflow” (a popular website where programmers answer each other’s technical questions) “and they get snippets of code to solve their tactical concern in this little function, and they glue it together, and iterate.”
“And that’s completely fine until you run smack into a real problem.”
In the summer of 2015, a pair of American security researchers, Charlie Miller and Chris Valasek, convinced that car manufacturers weren’t taking software flaws seriously enough, demonstrated that a 2014 Jeep Cherokee could be remotely controlled by hackers. They took advantage of the fact that the car’s entertainment system, which has a cellular connection (so that, for instance, you can start your car with your iPhone), was connected to more central systems, like the one that controls the windshield wipers, steering, acceleration, and brakes (so that, for instance, you can see guidelines on the rearview screen that respond as you turn the wheel). As proof of their attack, which they developed on nights and weekends, they hacked into Miller’s car while a journalist was driving it on the highway, and made it go haywire; the journalist, who knew what was coming, panicked when they cut the engines, forcing him to a slow crawl on a stretch of road with no shoulder to escape to.
Although they didn’t actually create one, they showed that it was possible to write a clever piece of software, a “vehicle worm,” that would use the onboard computer of a hacked Jeep Cherokee to scan for and hack others; had they wanted to, they could have had simultaneous access to a nationwide fleet of vulnerable cars and SUVs. (There were at least five Fiat Chrysler models affected, including the Jeep Cherokee.) One day they could have told them all to, say, suddenly veer left or cut the engines at high speed.
“We need to think about software differently,” Valasek told me. Car companies have long assembled their final product from parts made by hundreds of different suppliers. But where those parts were once purely mechanical, they now, as often as not, come with millions of lines of code. And while some of this code—for adaptive cruise control, for auto braking and lane assist—has indeed made cars safer (“The safety features on my Jeep have already saved me countless times,” says Miller), it has also created a level of complexity that is entirely new. And it has made possible a new kind of failure.
“There are lots of bugs in cars,” Gerard Berry, the French researcher behind Esterel, said in a talk. “It’s not like avionics—in avionics it’s taken very seriously. And it’s admitted that software is different from mechanics.” The automotive industry is perhaps among those that haven’t yet realized they are actually in the software business.
“We don’t in the automaker industry have a regulator for software safety that knows what it’s doing,” says Michael Barr, the software expert who testified in the Toyota case. NHTSA, he says, “has only limited software expertise. They’ve come at this from a mechanical history.” The same regulatory pressures that have made model-based design and code generation attractive to the aviation industry have been slower to come to car manufacturing. Emmanuel Ledinot, of Dassault Aviation, speculates that there might be economic reasons for the difference, too. Automakers simply can’t afford to increase the price of a component by even a few cents, since it is multiplied so many millionfold; the computers embedded in cars therefore have to be slimmed down to the bare minimum, with little room to run code that hasn’t been hand-tuned to be as lean as possible. “Introducing model-based software development was, I think, for the last decade, too costly for them.”
One suspects the incentives are changing. “I think the autonomous car might push them,” Ledinot told me—“ISO 26262 and the autonomous car might slowly push them to adopt this kind of approach on critical parts.” (ISO 26262 is a safety standard for cars published in 2011.) Barr said much the same thing: In the world of the self-driving car, software can’t be an afterthought. It can’t be built like today’s airline-reservation systems or 911 systems or stock-trading systems. Code will be put in charge of hundreds of millions of lives on the road and it has to work. That is no small task.
“Computing is fundamentally invisible,” Gerard Berry said in his talk. “When your tires are flat, you look at your tires, they are flat. When your software is broken, you look at your software, you see nothing.”
“So that’s a big problem.”
* This article originally stated that there were 10 million ways for the Toyota Camry to cause unintended acceleration. We regret the error.


Perhaps the grimmest aspect of the ongoing emergency in Puerto Rico is that the knowledge of the aftermath of Hurricane Maria—including 10 deaths, the devastation of entire swathes of the island, and dire shortages of food, water, and fuel—come from the teaspoonfuls of information that have dribbled out of the island. Most places don’t have power, and won’t for weeks, if not months. Less than 300 of the island’s 1,600 cellphone towers are functional, 85 percent of all above-ground cable and phone infrastructure was knocked out, and the more remote areas that make up the majority of Puerto Rico’s landmass have mostly gone dark, with brief calls from satellite phones providing sporadic updates. There’s no way to know for certain how bad the crisis is, and that dearth of information in turn has become part of the crisis itself.
For the millions of people in the Puerto Rican diaspora, both on the island and on the mainland, the lack of information is one of the most disruptive parts of the disaster. For people living on the island, it’s often impossible to get word out as conditions deteriorate, to know when aid might be coming, or to coordinate the delivery and access to life-saving services. Their relatives scattered across the United States and elsewhere have been left entirely in the dark as to the status of family members. While there’s no shortage of stories on the long-term implications of Hurricanes Irma and Maria on Puerto Rico and its existing humanitarian problems, at the moment many Puerto Ricans haven’t yet settled a more basic question: Who made it through the storm and its aftermath?
On the mainland, Puerto Ricans are still waiting for any news about loved ones on the island. For Alexandra Gates, a graduate student at the University of Chicago, communication with extended family in Puerto Rico has been limited. “We have literally gotten two text messages and one or two very spotty phone calls from them,” Gates says. “All they were saying is 'hey we're alive,’ and that's it. Then it all goes dark again.” Since most wi-fi on the island is down, the WhatsApp groups that the family usually uses to keep tabs seamlessly with people on both sides of the Caribbean have mostly gone dark, and family members have had to rely on pilgrimages to working cell towers in order to send brief status updates by text.
For cousins in or near the capital of San Juan, this is an inconvenient, but not terribly difficult proposition. For cousins further out in more remote areas, updating family means using limited gasoline supplies to take long trips across washed out roads, mud, and debris in order to stand in line outside a working tower.
But in more remote areas, even those options aren’t available. The tiny, rural island municipality of Vieques, which used to house a Navy munitions test facility, only had tenuous infrastructure connections to the larger island before it suffered a direct hit from Maria. Now, according to residents, the only power on Vieques is supplied by generators, which must themselves be resupplied regularly with gasoline from the main island, which itself might face gasoline supply problems in the near future. Power isn’t expected to be restored for months, and most telephone lines and trees on the island are down. Cell service hasn’t been working, and only a handful of satellite phones—including some owned and operated by FEMA officials—are available to provide updates to the authorities or to families. According to Steven Mueller, a resident of Vieques currently working out of Washington, D.C., “they're entirely dependent on one power line and one water line that we have to the main island.”
Still, residents of Vieques are working to get around those barriers. The day after Hurricane Maria hit, Vieques resident Brittany Roush began using Facebook to coordinate communications and aid. “This effort started with the Facebook page Vieques Peeps that most residents use to connect with each other,” Roush says. “The morning after the storm, there were a few hours where people had lost contact and frantic posts were starting to pile up.” Roush and friends who were not on Vieques when it was hit began taking calls and posts about family members and working to provide up-to-date information.
Those efforts evolved into the ViequesLove project, which Roush runs along with Mueller, her husband, and which has raised over $200,000 via GoFundMe. The group has worked to maintain a list of people verified as safe on Vieques by communicating with the few residents with satellite phones, having coordinated supply runs with government officials, and on Sunday and Monday managing to charter a plane to drop off more supplies on the island. Those flights helped resupply the hospital, where doctors and nurses are currently operating in tents, relying on a gasoline stockpile that can keep generators going for five days.
Although FEMA and the Red Cross are present on Vieques, as they are across Puerto Rico, with federal assets often slow to arrive and difficult to manage, social media and efforts by concerned citizens have been vital to keeping lines of communication open and coordinating relief. The “Puerto Rico Maria Updates” Facebook page boasts over 150,000 members, and has developed in a matter of days into perhaps the quickest real-time news service about developments in Puerto Rico, as well as one of the primary ways for Puerto Ricans on the mainland or in different municipalities to find out about relatives elsewhere.
Patricia Pichardo, who was born and raised in Puerto Rico, but lives and works in Atlanta, Georgia, started the Puerto Rico Maria Updates group the morning after Maria made landfall. “Wednesday morning when they were hit, I was expecting for communications to be lost, but not right away,” Pichardo says. “But the news outlets were only communicating with very broad information,” with very little in the way of news about individual towns. Pichardo was looking for news about her sister, Carolina, in Caguas, which is located about 20 miles inland from San Juan.
In her search for news about her sister and relative in Caguas, Pichardo came across posts from family friends on Facebook verifying that they were safe. She realized that a large-enough moderated Facebook group could amplify the sparse news from outlying municipalities, and could help connect Puerto Ricans with family and with resources.  “I said maybe if I can get enough friends and enough friends' friends to join, then maybe I can reach a critical mass where I'm able to hear more inside news than just by browsing my newsfeed alone,” Pichardo says. So on Wednesday, she created posts exhorting people on the island and on the mainland to join, and paid to sponsor the posts. By the evening the group boasted almost a thousand people. At its most rapid growth rate, the page added 1,000 new members an hour, and generated hundreds of new posts and dozens of documents detailing logistics, supplies, and safe lists.
The Puerto Rico Maria Updates page has become such an indispensable source of knowledge to people seeking any sense of what’s happening on the ground that it’s become a key source for a tool from Univision News that provides municipality-by-municipality updates. That tool is powered by a mixture of official news reports and social media chatter, much of which comes from Pichardo’s group.
The tool’s usefulness is reliant on the quality of information from the Facebook group, which now features an extensive cast of moderators who regularly call out and ban false reports and posts that don’t meet their strict guidelines for quality. Moderators upvote seemingly reliable content, use hundreds of volunteers to scour posts and discern whether information is corroborated internally or with news reports, block trolls and unauthorized fundraisers, and even issue corrections or warnings on faulty posts. All of those activities are directed by what Pichardo describes as “self-organization,” with people volunteering for fact-checking duties. “The group itself has a very strong identity and a very strong desire for real information,” Pichardo says. “But we’re not sleeping very much.”
The collection of efforts launched by activists on the island and family members on the mainland are similar in spirit to the deployment of the “Cajun Navy” in Houston after Hurricane Harvey. Then, residents used WhatsApp and the Talkie app, along with dozens of hours of work from unpaid volunteer dispatchers, to triage relief efforts in real time, get boats and trucks to people the most in need, and provide information to victims, their families, and authorities. In that example, the efforts of private citizens proved perhaps just as valuable as and more trusted than those of first responders and federal entities.
For Puerto Rico, where migration to the mainland is common, and where a recession and economic crisis have forced a wave of millions to move in the past few years in search of greater job prospects, political status, or health care, the connection is deeper. Crowdfunding and crowdsourcing simply mirror existing familial and community lines that have been maintained even as families are spread across the continental United States. And those lines are more important in Puerto Rico, which was singled out over Texas and Florida on Monday by President Trump as being “in deep trouble,” and where official aid and federal authorities have been criticized for their slow response and lack of communication.
“We are doing things everywhere we are, and we're doing it because maybe our aspirations go beyond what the island can provide us in terms of infrastructure,” Pichardo says. “We're everywhere. We're no longer all in Puerto Rico. But not because we don't love them and not because we are disconnected.”


Many questions remain about the ads purchased by Russian-linked accounts during the 2016 presidential election.
Earlier this month, the company announced that Russian-linked accounts had purchased $100,000 worth of advertising.
The scale of this advertising buy is mysterious. In an election where billions of dollars were spent, why even bother to spend $100,000? It seems like a drop in the bucket, but also more than nothing. For comparison, in 2015 and 2016, all campaigns directly paid Facebook a collective $11,313,483.59 across all races, according to Federal Election Commission numbers. The Trump campaign paid Facebook $261,685 directly for ads. But those numbers are only lower bounds for the amount of money spent on Facebook because many campaigns pay consultants, who then purchase ads on their behalf. (For example, Cambridge Analytica, which worked with the Cruz and then Trump campaigns, took in $15.4 million during the cycle, including $5 million in one payment from the Trump campaign on September 1.)
So, the Russian ad buy is a significant Facebook purchase, but not one that seems scaled to the ambition of interfering with a national U.S. election.
That could be because: 1) Not all the ads have been discovered, so the $100,000 is a significant undercount. 2) That was the right number, and the ads worked to aid distribution of disinformation. 3) The ads were part of a message-testing protocol to improve the reach of posts posted natively by other accounts. Think of it as a real-time focus group to test for the most viral content and framing. 4) That $100,000 was a test that didn’t work well, so it didn’t get more resources. 5) That $100,000 was merely a calling card, spent primarily to cause trouble for Facebook and the election system.
Let’s walk through these branching possibilities for what this advertising buy could mean.
We don’t know much about how Facebook conducted its investigation. We do know that they repeatedly denied there was Russian influence during the election, and then copped to it in early September.
A Washington Post article fleshed out a few details, including that President Obama personally spoke with Mark Zuckerberg after the election to get him to take the misinformation campaign seriously.
The problem appears to have been that Facebook’s spam- and fraud-tuned machine-learning systems could not see any differences between the “legitimate” speech of Americans discussing the election and the work of Russian operatives.
Here’s the description of the process that eventually found the ad purchases:
Instead of searching through impossibly large batches of data, Facebook decided to focus on a subset of political ads. Technicians then searched for “indicators” that would link those ads to Russia. To narrow down the search further, Facebook zeroed in on a Russian entity known as the Internet Research Agency, which had been publicly identified as a troll farm. “They worked backward,” a U.S. official said of the process at Facebook.
I take this to mean that they identified known Internet Research Agency trolls, looked at the ads they posted, and then looked for similar ads being run, liked, or shared by other accounts.
Why this would have taken several months is unclear. Journalist Adrian Chen was able to build out a network of Russian operative–run pages without any of the data that Facebook has. Given that the story he wrote ran in The New York Times Magazine in 2015, you’d think that particular agency would have been the first place Facebook would have looked.
That could be one reason a Congressional investigator told the Washington Post that Facebook had only hit “the tip of the iceberg.”
But that’s only one possibility. The ads could have done exactly what the Russians intended, even at this limited scale, as part of a broader information campaign.
Some context: Facebook ads can do several different things. They can promote a piece of existing content somewhere on the internet. They can be used to try to drive “likes” to a page. They can be used to get people to watch a video.
With the right (salacious/truthy/fake) material, even a little money can go a long way. The Daily Beast had a Facebook ad specialist calculate how far $100,000 worth of Facebook spending would go and came up with a range of 23-70 million people, depending on how they were targeted.
Vice News talked with the owner of a right-wing Facebook page who uses Facebook ads to juice conservative content. After spending $34,100, the man controlled pages with 1.8 million likes. With that distribution base, he was able to push out content that could, on occasion, do serious numbers. “With a few advertising dollars, one April video ... received more than 27 million views and over 450,000 shares, spreading so pervasively into the conservative media universe that Donald Trump’s official Facebook page shared it two days later,” Vice wrote.
So maybe that’s it. The ads were simply a smallish part of growing the distribution network for disinformation and propaganda.
Looking back at Adrian Chen’s reporting on the Russian troll farm known most commonly as the Internet Research Agency, there’s a mix of skill and blundering. The Agency was smart enough to set up Chen with a neo-Nazi, surreptitiously photograph it, get stories written about the encounter, and then promote those via social media. But they also struggled to find English speakers who could write with proper grammar. The Agency could orchestrate a very complicated hoax about a chemical plant, but also let a known activist and journalist slip inside the company as new hires. The Agency was playing in international geopolitics, perhaps funded by a billionaire oligarch, but Chen reported the agency’s rumored budget back in 2015 was a mere $400,000 a month, or $4.8 million a year.
Perhaps the best mental model is simply a digital-advertising agency. In that case, there are some other intriguing possibilities.
Regular digital agencies (and media companies) routinely use Facebook ad buys to test whether stories and their attached “packaging” will fly on the social network. You run a bunch of different variations and find the one that the most people share. If the Internet Research Agency is basically a small digital agency, it would be quite reasonable that there was a small testing budget to see what content the operatives should push. In this case, the buys wouldn’t be about direct distribution of content—they aren’t trying to drive clicks or page likes—but merely to learn about what messages work.
And there’s a variation on these two scenarios, too. It could be that only $100,000 got spent simply because the ads were ineffective. Facebook itself has a case study on the reelection bid of Senator Pat Toomey that showed substantial increases in “voter intent” for key demographic groups. But the Toomey campaign spent $2.8 million on digital strategy.
That said, there is certainly reasonable doubt that even millions of dollars of Facebook spending could change the outcome of even a state in the U.S. presidential election. And perhaps the digital agency came to the conclusion that its budget was better spent elsewhere. Or maybe one group within the Internet Research Agency began buying ads—we do know the place is obsessed with metrics—to make itself look better to superiors for some period of time.
But it seems possible, from Chen’s description, that this was just a small thing for the Agency, which never gained institutional support.
And the last possibility is that the Internet Research Agency wanted to make a buy that it knew would get Facebook in trouble with the government once it was revealed. Think of it as corporate kompromat. Surely the Internet Research Agency would know that buying Facebook ads would look bad for Facebook, not to mention sowing the discord that seems to have been the primary motivation for the information campaign.
Some of these questions will be solved by simply seeing the ads. If they were testing the same content with different headlines or a bunch of different videos or posts, that’d tell us something about their operation. If the messaging ended up outside the ads, that might tell us something. In all cases, seeing the ads will be a major part of deciding what the known world of Russian influence was up to, which seems important for other social networks and political campaigns to defend themselves against future operations.
Maybe all we’ll see is bungling and a scattershot, silly approach. That would be useful and interesting information, too.
In any case, the Russian effort remained hidden in plain sight, which could be due to sophistication. Or it could be that having a nonfinancial motivation essentially served as an exploit for Facebook’s security systems, which are tuned to fighting fraud.
“Various groups regularly attempt to use such techniques to further financial goals, and Facebook continues to innovate in this area to detect such inauthentic activity,” wrote Chief Security Officer Alex Stamos and two Facebook coauthors in a white paper on information operations. “The area of information operations does provide a unique challenge, however, in that those sponsoring such operations are often not constrained by per-unit economic realities in the same way as spammers and click fraudsters, which increases the complexity of deterrence.”
The bottom line is that Facebook was not prepared for the threat. And I highly doubt that we won’t see a lot more come out about Russian operations on Facebook.


On Friday, more than 3 million London commuters learned that they would soon have fewer options for the commute home, after the city’s transport authority announced Uber will no longer be allowed to operate within the sprawling capital’s city limits.
Uber Did What!?
“TfL has concluded that Uber London Limited is not fit and proper to hold a private hire operator license,” Transport for London said in a statement, citing a range of concerns with the ride-sharing app, from its history of reporting serious criminal offenses to its policies on obtaining background checks for its drivers. Uber’s current operating license expires in eight days, after which the app will no longer be permitted to operate within the city. TfL, however, said Uber would have three weeks to appeal the decision, during which time the company may continue to operate.
The leader of London’s Licensed Taxi Drivers’ Association praised the transport authority’s decision, as did London Mayor Sadiq Khan, who noted that “providing an innovative service must not be at the expense of customer safety and security.” Tom Elvidge, Uber London’s general manager, accused the city of attempting to restrict consumer choice. “Not only will this decision deprive you of the choice of a convenient way of getting about town,” Elvidge said in an email to customers, “it will also put more than 40,000 licensed drivers who rely on our app out of work.” The email did not make mention of the safety concerns raised by the city.
Overall, the move will affect millions. Since Uber first made its London debut in 2012, some 3.5 million have downloaded the app—many of them seeking a cheaper alternative to the traditional black cabs, which can be as much as twice as costly. In the five-year period Uber has operated in London, its fleet of cars has nearly doubled the number of traditional cabs.
Though rivalries between Uber and the traditional taxi companies where they operate are hardly new (they’ve been well documented in Brazil, Canada, and China), Uber’s relationship to London—and the iconic cabs that roam its streets—is in a different category. As The New York Times reported in July, the clash is less about “the disruptive power of an app, or a new business model, than about the disruption of Britain. London’s cabbie wars echo the culture wars that fueled Britain’s vote last summer to leave the European Union—and that have brutally flared up again in recent weeks: immigrant versus native, old versus new, global versus national.”
Part of this divide can be seen in the demographic makeup of those who drive for both services. According to 2017 statistics by TfL, 68 percent of London’s taxi drivers identified as white and British, compared to only 6 percent of private-hire drivers, under which Uber qualifies. Other factors the Times noted, such as those who regard Uber as emblematic of the threat of globalization to the hundreds of complaints Uber drivers have reported involving offensive or racist abuse from other cab drivers, make this division even more stark.
The rift between Uber and London’s cabs may not be resolved anytime soon. Since TfL announced its decision, more than 300,000 people have signed an online petition announcing their opposition. Uber, meanwhile, has said it will immediately challenge the decision.


In the last week, one of Facebook’s greatest business strengths became a source of tremendous embarrassment to the company. Its famed money-making engine—which ceaselessly converts user data and content into advertising dollars, all underpinned by algorithmic plumbing—was found to have some glaring issues.
Specifically, as ProPublica revealed Thursday, advertisers could target self-described anti-Semites as an audience for their ads. Facebook’s algorithmic ad tool allowed buyers to target users who publicly entered phrases like “Jew hater,” “How to burn the Jews,” or “History of ‘why Jews ruin the world’” into their Facebook profile as their educational background or professional interest.
Facebook apologized for the feature last week, pulling every algorithmically defined ad-targeting category. On Wednesday afternoon, Sheryl Sandberg, the company’s chief operating officer, outlined further steps.
Facebook would be tightening its “enforcement processes” to make sure that content in violation of its community standards could not be used to target ads, Sandberg said in a public note on her profile. The company would also make it easier for users to report offensive or abusive ads directly to corporate workers.
Facebook’s community standards, which have been in effect for years, already prohibit “anything that directly attacks people based on their race, ethnicity, national origin, religious affiliation, sexual orientation, sex, gender or gender identity, or disabilities or diseases.”
The company also reactivated more than 5,000 audience-targeting demographics after checking them individually. “After manually reviewing existing targeting options, we are reinstating the roughly 5,000 most commonly used targeting terms—such as ‘nurse,’ ‘teacher,’ or ‘dentistry,’” Sandberg said.
She also suggested that the company would manually vet future target-audience terms in the future.
This final change—that Facebook should check the audience categories that underpin its central business—was one of the main reforms that experts called for after last week.
“We don’t need to be too awed by this problem,” Aaron Rieke, a technologist at the civil-rights firm Upturn, told me last week, about the anti-Semitic targeting. “You have a finite list of categories, many of which were generated automatically. Take a look, and see what matches up with your community standards and the values of the company. Facebook can monitor the things it does that make it money.”
Other suggested changes—like a public clearinghouse of all ad categories available to purchasers—were not embraced by the company.
Sandberg, writing on the eve of Rosh Hashanah, the Jewish new year, took an unusually personal and contrite tone in her note of apology. “The fact that hateful terms were even offered as options was totally inappropriate and a fail on our part,” she said:
Seeing those words made me disgusted and disappointed—disgusted by these sentiments and disappointed that our systems allowed this. Hate has no place on Facebook—and as a Jew, as a mother, and as a human being, I know the damage that can come from hate. [...]
We never intended or anticipated this functionality being used this way—and that is on us. And we did not find it ourselves—and that is also on us.
Like so many of Facebook’s recent struggles, the whole story turns on the intersection of automation, algorithms, and users acting in bad faith. It also turns on the company’s consistent inability to imagine the worst applications of its software.
Facebook, like many other tech companies, allows advertisers to “self-serve” their own ad purchases. There’s little human interference with—or oversight of—this process: A buyer can write a post or upload a video, select a target audience, and take out an ad with no humans otherwise involved. Many of the systems that maintain Facebook’s plumbing are essentially algorithmic like this, too.
This automation has come back to bite the company twice in the past week—at both the targeting and the purchasing ends of its pipeline. In addition to the anti-Semitic problem with the targeting end, Facebook also appears to have allowed Russian shell firms to buy political ads, attempting to influence the 2016 election.
Facebook may be able to plug some of the simplest holes in its targeting software. But the Russia-purchased ads—and the surprise with which Facebook discovered the anti-Semitic targeting in the first place—suggests that it is facing an arduous, important, and possibly Sisyphean task in trying to understand the darkest corners of its own business.


Decades after the National Science Foundation took over the old McMurdo naval base in Antarctica, and five years after design work began, there are renderings of the plan for a new McMurdo.
The OZ Architecture design will consolidate the current 105 buildings into six large, insulated, modern buildings. The new McMurdo is of this moment: It looks like an Apple Store.
Perched on volcanic gravel near the Ross Ice Shelf, McMurdo is the largest community in Antarctica with over 1,200 residents during summer and about a quarter as many during the winter. Many are scientists, but the support staff includes workers from all the trades—plumbers, carpenters, HVAC specialists, IT folks.
The old McMurdo was constructed for the International Geophysical Year of 1957 by a special group from the U.S. Naval Mobile Construction Battalion called “Task Force 43.” It was part of Operation Deep Freeze, the overarching U.S. military movement into Antarctica. Film from the time shows what a brutal task it was. Tractors dragged sleds laden with construction materials bumping over sea ice. It was bleak.
The most salient fact from the construction process is that McMurdo was never intended as a permanent settlement. And yet, there it is, sitting by the volcano, pressed into service by the “beakers,” as the Navy guys called the scientists taking over.
As one might imagine, there are problems with adapting a transient military facility into a permanent scientific research base. In 2012, a Blue Ribbon Panel catalogued all the facilities’ needs in Antarctica. That led to the Antarctic Infrastructure Modernization for Science, which selected OZ Architecture to do a master plan for the site.
“This is a career highlight to be challenged to create a community that is new and remote and in a hostile environment where you want to provide well-being,” said Rick Petersen, a principal at OZ. “This is the main logistical hub to support really groundbreaking science that affects the whole globe.”
Petersen said the design was an attempt to maximize energy efficiency and increase the logistical logic of the site while also providing for the mental and physical needs of the people at the base.
The walls have an R value, a measurement of insulation, of 72, five to seven times what you probably have in your house. The windows are triple-pane and the best insulated (or “highest performance”) out there. That should reduce the use of diesel generators on site to some degree.
The buildings, like many Antarctic structures such as the newish Amundsen-Scott South Pole Station and the British Halley VI, are lifted off the ground, even though (unlike most others) they aren’t designed to be moved. The situation at McMurdo is different: There is “a matrix of ice” in the ground, which they don’t want to warm with heat from the buildings. So, OZ designed insulated boxes lofted above the permafrost. Skirts around the bottom of the building prevent ice and snow from building up underneath them. (Petersen and Halley VI architect Hugh Broughton know and consult with each other.)
Logistics is a core consideration of the new McMurdo plan. Right now, warehouses and facilities that need the cargo in them are not well-placed. “There are 22 different warehouses scattered about the community, so there is a challenge in getting equipment or food from one to the other,” Petersen told me. “Things are spread out. It requires pickup trucks and vehicles to move things from one warehouse to another.”
Just to carry out the basics of camp life, like making food, requires schlepping to a warehouse and back to the production facility. In the new design, there are interior routes to key locations, although there are also ways to pop outside to escape the relentlessness of confinement.
The current design sounds great. It was built with input from years of listening to experienced personnel. But Petersen knows that they can’t get everything right, so they designed the buildings to be adaptable to changing needs and circumstances.
“The different work groups adapted the current buildings not only to optimize how they work, but also to reflect their culture,” Petersen said. “There are basically neighborhoods” that correspond to the type of workers who spend time there, from science outfitters to carpenters.
“We have to understand that the buildings we’re creating today and building in the next few years are going to outlast the people and programs that will first be using them,” he said.
The new buildings will be prefabricated and shipped to the site in containers. Construction will begin in 2019 and should take about five years.


Take a video of a birthday cake’s candles sparkling in an Instagram story, then tap the sticker button. Near the top of the list you’ll see a slice of birthday cake.
It’s a little thing. This simple trick is not breathtaking nor magical. But it is the beginning of something transformative. Smartphones already changed how most people take pictures. The latest Silicon Valley quest is to reimagine what a camera is, applying the recent progress in artificial intelligence to allow your phone to read the physical world as easily as Google read the web.
With 2 billion users, Facebook has reorganized the teams responsible for coding the camera software in Instagram, Facebook, and Messenger into a new unit it calls “AI Camera.” The group started last year with a single person. Now, it has grown to 60 people. That includes Rick Szeliski and Michael Cohen who worked on Photosynth (among other things) at Microsoft. The AI Camera team also can draw on the expertise of top neural-network researchers like Yann LeCun and Yangqing Jia in other parts of the company.
The AI Camera team is responsible for giving the cameras inside these apps an understanding of what you’re pointing them at. In the near future, your camera will understand its location, recognize the people in the frame, and be able to seamlessly augment the reality you see.
Right now, the team’s work has shipped in small ways, like the birthday sticker trick. But that is just the beginning of a development program that wants to transform the way you use the camera on your phone.
AI Camera combines many of the most important technological developments of the last several decades: neural networks, robotics, camera systems, and social-network data. This underlying basket of technologies—more adjacent to each other than in “a stack,” as software developers might conceive it—are converging into the smartphone's ability to take and display pictures.
Perhaps that seems absurd. But the human desire to capture, understand, and share images of the physical world has proven to be nearly insatiable, which is why this is the one domain where Facebook, Apple, Google, Samsung, Snapchat, and Microsoft directly compete.
Facebook’s work mirrors what’s happening at the other tech giants. Snap calls itself a camera company, and its realization of “lenses” are the best embodiment of augmented reality outside of Pokémon Go. At Google’s developer conference in May, Sundar Pichai showed off Google Lens, software that can detect what a camera is seeing and do something with that information, from entering a password to identifying a flower.
Prodded by Snap, the tech giants have begun to piece together what can be accomplished with the whole imaging and display system that a smartphone is. Every millisecond a phone’s camera is engaged is a moment when data can be captured, processed, understood, and looped back to the user for viewing.
“We’re basically looking at what pieces of technology we need to build amazing augmented-reality products,” said John Barnett, product manager on the AI Camera team.
Imagine, he said, a persistent, shareable social layer on the physical world, a spatialized Facebook that’s escaped the feed.
“Everyone got so excited about Pokémon Go when it was just one thing. What if there are 1,000 things like that?” Barnett asked. “All these layers of information that are spatially situated and relevant to what you care about.”
This is a radically different notion from the Facebook we’ve come to know, which, even though it made the leap from the desktop to “mobile,” rarely engages with the physical space where your hand clutches the phone.
“In the existing Facebook structure, we’re giving you everything that’s happening right now in the world, collapsing space to give you a a slice of time,” said Barnett. “This is talking about collapsing time to give you this piece of space.”
Facebook would take on two modes: The News Feed, in the company’s terms, would show you what you care about now, and the spatial Facebook would tell you what’s happening here. One could read from, and write to, the world. Your world, at least.
* * *
On one of the many decks at Facebook’s Menlo Park campus, overlooking the mudflats of the south Bay, there’s a nondescript corner. Pipes run along it. A surveillance camera sits on the east-facing wall. To the naked eye, there’s nothing to distinguish it from the hundreds of others that help form Facebook’s gargantuan ark.
However, pull out a phone loaded with an app Facebook has in development and point it at the wall, and you get a beautiful piece of art, created primarily by Heather Day, a San Francisco artist. It made a brief appearance during Mark Zuckerberg’s keynote at the company’s F8 conference.

Brilliant blues, cyans, teals drip from the pipes, pooling away from the wall. It’s cool, this thing hanging in the air.
Put the app away, open it back up and point it at the corner again, and the art is back. Move around it, move through it, and the ghostly remains of Day’s paint strokes and pours remain there. What if there were thousands of things like this all over the world? Next to burrito recommendations and Strava segment records and pictures of your friends, mugging for the camera, in situ.
This is one vision for augmented reality, the name for this layering of digital information on top of imagery of the physical world. AR has gotten a big push in recent months by Apple’s announcement of ARKit, a framework for developers to enable AR in apps. They’ve been showing it off, and Google recently announced a similar (though not as widely lauded) set of tools called ARCore.
No matter what, AR is a ridiculously complicated task for a smartphone. Alvaro Collet is a computer-vision Ph.D. from Carnegie Mellon University who came to AI Camera from Microsoft. He’s standing next to me looking at the wall. “This is actually a pretty challenging scene because it is very plain,” Collet tells me.
The basic task mirrors what robots have had to do for decades. Researchers call it SLAM (simultaneous localization and mapping).
The theory and practice of SLAM were developed over the past 30 years by robotics researchers like SRI’s Randall Smith and Peter Cheeseman, the University of Sydney’s Hugh Durrant-Whyte, Sebastian Thrun, and Carnegie Mellon’s Martial Hebert, who was Collet’s advisor. Most of these people were working on real robots, largely autonomous vehicles loaded up with all kinds of sensors. But as smartphones began to roll out, researchers realized that their systems might be able to reach hundreds of millions of people, not a few dozen.
The problem of SLAM is that you need to build a map of the world in which to place the robot (or phone), but the position of the robot (or phone) and the world are both uncertain.
“If you had all the features of the world already in 3-D, it would be very easy to place the position of the camera. And, conversely, if you had all the camera positions, it’s very easy to create the 3-D map of the world,” Collet said. “The problem with SLAM is that when you start, you don’t have a 3-D map and you don’t know where the camera is. That’s the simultaneous part.”
There are many ways to go about the problem that are encoded in different algorithms. Each has tradeoffs. Some provide excellent precision, but are computationally expensive. Others might consider the images from a sensor less extensively, but work quickly and without much computing.
Facebook finds itself building across both iOS and Android, which introduces many challenges. Facebook’s advantage, though, is its tremendous scale, 2 billion users and counting. But to use that scale, Facebook must make AR work on all kinds of crappy phones, not just Pixel 2s and Samsung Galaxy Note 8s and iPhone Xs. And that means that they actually deploy multiple algorithms to do SLAM. For lower-end phones, they do rougher, faster calculations. Higher-end phones get better performance because they can handle the processing.
Down on the low end, the AI Camera team must try to account for a bunch of mostly invisible hardware problems. Inside the phone, there is a camera, but there’s also an inertial-measurement unit, which they can use to tell how the phone is moving. The IMU contains gyroscopes and accelerometers. And all of these components, on low-end devices, have to be calibrated. Their clocks must be synchronized. And each device, because of lower manufacturing quality, might show more variance than one iPhone to the next.
Once all that electronic work has been done, and the phone knows roughly where it is, and the geometry of the scene, the next layer of technology gets piled on top: deep neural networks. The “neural” part means that this kind of software is “trained,” not programmed with traditional rules. After being shown large amounts of labeled data, the neural network can label new data based on what it has seen. The deep part refers to the neural network’s number of layers, which correspond to the complexity of features in a dataset.
Over the past five-ish years, this type of machine-learning system has transformed the way image recognition, among other things, is done. If you’ve ever used Google Photos to find pictures of business cards or mountains or people, you’ve made use of a deep neural network’s power.
Imagine, though, the next step: Instead of merely recognizing objects in artifacts, the phone can recognize objects live within the model of a scene that the device has already built. That’s only become possible in the last year.
“For the first time, you can run SLAM and deep networks on a cellphone,” Collet said. “We have two big teams: SLAM geometric teams and the other is deep nets. And the goal is these two things are going to combine.”
That’s the only way you get to augmented reality of the kind that Facebook imagines. Then they’d just have to get people to populate all the layers of spatial information.
“One thing we really are about is giving everyday users—maybe a year from now, maybe two years from now—the ability to recreate that Heather Day scene with just tools you have on your phone,” Collet said.
Anyone with a Facebook account could create media and fix it to a spot in the world. There will be food recommendations and wedding photos and paintings dripping in the air. A globe of ghostly art and burrito spots.
***
But we do know one thing from the history of every social platform: People will make their own uses of the tools. They’ll find new, unforeseen uses and abuses. There will be unintended consequences to spatializing Facebook.
Some of these could be predictable. There is already spatial information out there, just not displayed in the way AI Camera imagines or running through Facebook. Yelp, for example, has struggled with troll reviewers. (Here’s a nice list of some common variants.) Restaurants have been struggling to deal with the digital signs that lovers and haters affix to their doors for a decade now.
Another cautionary example comes from Pokémon Go. Omari Akil wrote a post describing his experience playing the game as a black man. He spent more time worrying about whether other people would find him suspicious—and bring him into contact with police—than he did actually engaging with the app. “When my brain started combining the complexity of being black in America with the real-world proposal of wandering and exploration that is designed into the game play of Pokémon Go, there was only one conclusion,” he wrote. “I might die if I keep playing.”
The realities of race and gender in America, which already play out in ugly ways across the internet, will be amplified by the physicality of augmented reality. Not everyone will be able to access the same spaces with the same ease.
In 2016, Waze rolled out a high-crime alert in Brazil to let people navigate around “bad neighborhoods.” Microsoft ran into trouble for a similar 2012 patent that got termed the “avoid ghetto” feature.
Even in more benign examples, the imperfect fit of spatial information on top of space can cause problems. Near my home in North Oakland, navigation apps lead many people to make a dangerous left just past an overpass and before a much larger intersection that the urban plan and human driving intuition both discourage. Nearly every time I drive past the intersection, that left is causing problems in the traffic flow up Claremont Avenue.
It’s not that Facebook can or should be expected to fix all of trolling or American antiblackness or the complexities of layering digital on physical. But as they develop this world, they can build with these problems in mind.
There’s even an analogy within the AI Camera project. Collet, the computer-vision specialist, was describing all the work that they have to do to make their systems work with the weird and wild world of phones across the globe. The calibrations, the algorithms, the fault tolerance of the systems.
“If you don’t think about them from the start, it’s very hard, once you have a system to say, ‘Oh, maybe we should tolerate this better,’” he said.
And as it goes with the reality of physical components, so it should be with the reality of the ethical and behavioral aspects of augmented reality. It’s gonna be more work to consider the misuses and biases in the system, but considering those things now will make the system more robust later.
If the AI Camera team succeeds, they will open up a new and basically infinite space on top of the land. The open question is what that will do the places under this new digital layer.


In 1804, a young naturalist named John James Audubon tied silver threads to the legs of the eastern phoebes, tiny white-and-brown songbirds, that lived in a nest near his home near Philadelphia. The birds soon flew away for the winter. The following spring, two returned with threads still attached. The experiment marked the first recorded use of bird banding in America, a technique for studying migration patterns.
More than two centuries later, technology, particularly the wonder of GPS, has turned silver threads into tags, sensors, and other devices capable of tracking all kinds of species around the world. Today, researchers get text-message alerts from collars worn by elephants in Kenya. They can stick tags on the shells of turtles or attach sensors to the fur of seals that transmit information with the help of satellites. They can even glue tiny barcodes to the backs of carpenter ants.
These are just a few of the projects described in Where the Animals Go, a book by geographer James Cheshire and designer Oliver Uberti, out this week in the United States. Cheshire and Uberti spoke to dozens of scientists tracking animals, from owls and elk to pythons and hyenas, and turned their data into a collection of 50 beautiful maps.
The maps show the paths the animals take as they cross desert, forest, ice, and ocean to feed, breed, and survive. The maps reveal what Audubon couldn’t see when he tied his silver thread to the birds: a journey. Some are especially quirky, as in the case of the seagulls who made daily trips to a city in France that was 40 miles away from their breeding colony. When researchers visited the site to investigate, they found the gulls feasting on discarded food outside of a potato-chip factory.
I spoke with Uberti and Cheshire about animal-tracking technology and the strange places it takes us. Our conversation has been edited for length and clarity.
Marina Koren: So your book introduces us to a fairly new era of tracking animals using technology. Can you tell me about the era that we’ve left behind? How did humans track animals before they could stick GPS tags on them?
Oliver Uberti: Until fairly recently, tracking involves looking for footprints, looking for fallen feathers, broken branches, droppings—any sign that an animal has passed through. Then around the past couple centuries, you start to get people like John James Audubon, who are tying threads to the legs of songbirds to prove that they’re actually returning to the same place every year. And then in the 20th century, you start to get people doing some real DIY tracking where they’re attaching cameras to pigeons or radio transmitters to a duck. But it’s only really been in the past 20 to 30 years that GPS has come on the scene, and then after that the miniaturization of computing power and the internet and satellites—it’s literally just exploded what you can do in a really tiny device.
James Cheshire: We’ve really transitioned from monitoring animals as a resource to be exploited to monitoring them as individuals within a species that we can collect a myriad of data from. Prior to the animal-tracking revolution, one of the biggest data sets collected on whales came from where we killed them, from whaling ship logs. There’s now sensors that they’ve got on whales that collect more data points than the sum total of the data collected in the previous five decades or more of research.
Koren: What kind of technology do we use today?
Uberti: There’s really no “one size fits all.” Scientists tailor tracking tags to the species and to the study, and that all depends on the environment that the animal lives in. GPS doesn’t penetrate underwater, so if you’re tracking a marine animal like a shark or a turtle, you need to have a device that can transmit when the animal comes to the surface and gets a brief, sometimes split-second window to shoot up to a satellite and transmit. Or you need a device that can release off the animal and float to the surface and then transmit. If you’re talking about songbirds, there’s protections in place to make sure they’re not taking tiny little songbirds that weigh only a few grams and saddling them with a giant computer. You talk about tracking plankton—you’re not even using a computer. You’re setting up an environment in an aquarium, you turn off all the lights, and you inject the plankton with a fluorescent particle like they use to track cancer in some medical technologies. And in the darkness, the individual plankton fluoresce and, by recording that with cameras, [scientists] can watch the illuminated animals move up and down the water column in response to UV light.
[At the Save the Elephants organization,] they can get text messages if they think elephants are in danger, if they’re moving too slowly, which is often a result of being shot by poachers or herders who’ve been scared because the livestock are coming close to these elephants and they don’t know what to do. If the animal is moving slower than it’s expected to move, the GPS tags notice and their accelerometers inside notice. It’s much like the way Google Maps can track different traffic patterns on the road by how fast they’re expecting you to be moving. If the elephant moves below a threshold, then it sends out an alert and Save the Elephants can dispatch rangers and law enforcement to check on the animal immediately.
Koren: How did you go about visualizing the immense data sets from the sicentists you talked to?
Uberti: When the data comes out raw from the tags, it’s an immense hairball. Tracks of many, many mountain lions, for instance, are all tangled on top of each other. To really tell any story of what the animals experienced or what the scientists are investigating, we try to turn big data into small data, and highlight one or two individual animals and walk you through or swim you through or fly you through what that animal experiences. It was a lot of editing down, taking out extraneous tracks, and really zeroing in on the lives of a few individuals.
Cheshire: Getting the right base-mapping information, all of the contextual stuff that goes around the tracks—that was a huge amount of effort, actually. It was probably more work than dealing with the actual animal data itself because we needed to make sure that what we were showing on those maps was relevant to that particular animal. We wanted to create maps that conveyed some of the environmental conditions. For example, when we mapped snowy owls over the Great Lakes, the satellite imagery we used for that roughly comes from the same time as when the owls themselves were flying over them, so you could see the same ice floes that the owls are seeing.
Koren: What was it like to look at a completed map, see a bunch of squiggles, and know that that’s an animal going about its life?
Uberti: This all goes back to an elephant named Annie, who I first mapped years ago. I saw where her GPS tracker stopped recording in [southeastern Chad], where she and a number of her companions had been killed by poachers. Mapping her moving through her environment, waiting till nighttime to cross the roads, to avoid interaction with humans, and then just watching her life stop on a map—that was the first time a map had connected me to an individual animal. It’s easy to think of animals when you hear about them, when you watch nature specials on TV, as furry robots that are just kind of preprogrammed, moving about the earth, doing what they as a species are assigned to do. And when you look at these tracks, you see in each individual path animals making decisions that are unique to them—what they like to eat, where they like to go.
Koren: The maps in the book really make clear how animal migrations transcend borders, whether it’s of national parks or nations. Has your idea of borders changed after seeing just how irrelevant they are for animals?
Uberti: It confirmed my own assumptions that borders don’t exist in the natural world. A great example of that goes back to the first GPS tracking study in Africa, where Cynthia Moss was doing research on elephants in Amboseli [in Kenya]. Some of her elephants were leaving and crossing the border into Tanzania into wildlife hunting ground and were being shot. When they confronted the Tanzanian government and the hunters about it, the hunters said, oh, no, these are our elephants in Tanzania, they’re not yours from Kenya. So Moss got Iain Douglas-Hamilton, the founder of Save the Elephants, and he put two [sensors] on two elephants, and sure enough, one of them crossed over the border into Tanzania. Here you had this undeniable proof that animals don’t follow human borders.
Border walls and pipelines make headlines, but far more insidious in their effect on animal behavior are freeways and fences. Mountain lions in Los Angeles are completely marooned on genetic islands in the Santa Monica and the Santa Ana Mountains because of this web of freeways, many of them eight, 10 lanes long, that the animal can’t get across. And if they can’t get an influx of DNA from animals from outside their population, they’re doomed by inbreeding.
Cheshire: It’s certainly true that animals don’t care about national borders, but the national borders themselves and the geopolitics around them still have big impacts on animals. We have a story about a wolf named Slavc who walked across Europe and starts in Slovenia and finishes in Italy and goes via Austria. Each of those countries had different laws and different attitudes toward large carnivores and whether they could be hunted. So for the researcher who was tracking Slavc’s progress, it was a real roller coaster, because as soon as Slavc stepped over the Austrian border, the chances of him getting hunted were bigger because there are laws there that mean you can shoot stray dogs. An animal doesn’t have to show a passport to cross from one country to another, but the impacts, when they do, can still have a big difference on their life chances.
Koren: What does the future of animal-tracking technology look like?
Cheshire: Stuff is going to get smaller and more powerful and cheaper to deploy. But the real interesting stuff is going to be what the researchers are able to do with the data when they get back to their labs, and start crunching through these big sets of numbers they’ve got, and see if they can look at the health of an ecosystem almost in real time. Researchers are now looking into how animals interact with one another, with predators, with prey. It’s those interactions that dictate how successful an animal is—are they getting enough food, are they able to hunt, are they able to breed? [Researchers] are beginning to tag enough animals within an ecosystem to see how those interactions are playing out.
Koren: Has working on this book changed the way you look at animals in your everyday life, even just a squirrel on the street?
Cheshire: For sure. There’s a tree in my backyard that has songbirds in it that come and go. Before, I didn’t pay them much attention, but now I’m wondering how long they’re going to hang around for, where they’re going to go, where they’ve been.
Uberti: The Wallace Stevens stanza that we excerpt kind of sums it up for me. “When the blackbird flew out of sight, it marked the edge of one of many circles.” Working on this book just made me think that way about all the animals I see in my day, whether it was a roebuck that jumped past me while hiking in Slovenia, to a lizard that skirted out from underneath a trash can here in Los Angeles, or the brown widow spider I saw last night outside of my flowerpots in my backyard. So much is happening outside of what we can see for ourselves.


Updated at 8:04 a.m. ET
More than 200 people are dead after a magnitude 7.1 earthquake struck Mexico on Tuesday, the anniversary of the 1985 quake that killed at least 5,000 people. It was the second major earthquake in the region this month. Just 12 days ago, Mexico City’s earthquake-warning sirens blared ahead of an 8.1-magnitude temblor—and many Mexicans reacted as they are by now accustomed to when the alarm went off Tuesday.
The warning system, which has been in place since the early 1990s, is linked to a complementary smartphone app used by millions of Mexicans.
The U.S. Geological Survey said the quake’s epicenter was five miles southeast of Atencingo, in central Puebla state, at a depth of 32 miles. Details of Tuesday’s quake are still emerging, but videos and images posted on social media showed fleeing people, collapsed buildings, and smoke emerging from structures. The mayor of Mexico City, where the quake was also felt, said many people are trapped in the rubble.
In an odd coincidence, the quake struck just hours after Mexicans participated in earthquake drills to mark the anniversary of the 1985 Mexico City earthquake. It was also the second powerful quake to strike this month: an 8.1-magnitude temblor that hit Chiapas on September 7 killed 98 people. Mexico has a rash of major quakes in its recent history. Tuesday’s was the 14th earthquake greater than 7.0 magnitude in Mexico since 1985. None of the quakes since that period have had the same devastating impact as the one more than three decades ago, even if several of them, including the one on September 7, were at least as powerful, if not more so. One reason for this: improved building codes.
The codes are strictly enforced in Mexico City—which is particularly vulnerable to the impact of faraway quakes because it’s built on a dried-out lake bed—and other major cities. As the Christian Science Monitor noted in 2003, after a deadly quake killed 25 people:
Millions of dollars have been spent to reinforce centuries-old structures across the city, and any new construction must comply with strict quake protection measures.
Teams of rescue workers, police, hospital staff, and firemen practice earthquake simulations throughout the year so they can respond immediately.
City workers from the subway system to the power grid are assigned specific checklists they must work through the instant any seismic activity occurs.
They are understanding orders to repair any damage immediately, even in the event that telecommunications are down and they can not reach their superiors.
Meanwhile, a state-of-the-art alert system beams emergency messages to the capital every time major seismic activity is detected along coastal fault lines. Warning sirens then sound, giving city residents up to a minute's warning that a quake is coming.
Perhaps most important, massive public-awareness campaigns have trained Mexico City residents how to react when they hear the sirens: Get outside quickly and stand clear of any structure or power cables that could fall.
Many of those systems put in place have proved invaluable in the years since the 1985 quake. For instance, during the September 7 quake, the early-warning system, called SASMEX, which relies on sensors installed along Mexico’s western coast, gave residents more than a minute’s warning of the impending quake, likely saving lives. That system has been in place since the early 1990s; a complementary smartphone app is used by millions of Mexicans.
This sort of early-warning system is particularly useful outside the major cities where building codes may not be as well enforced. But as CityLab pointed out, poorer states like Chiapas, where 16 people were killed in the September 7 quake, aren’t part of the system. (The CityLab piece also noted that the SASMEX system needs an overhaul.)
The southern state of Oaxaca bore the brunt of the earlier September quake, and the damage in the town of Ixtaltepec prompted one state official to tell The New York Times: “We have no idea how we are going to rebuild the entire town.” There have been warnings for years about this looser enforcement of building codes in some parts of the country. The Guardian pointed out in 2015:
Yet, in the peripheral zones, beyond the tourist’s reach, informal settlements continue to proliferate. There, codes and regulations are irrelevant. Residents build with whatever materials they can find and afford, and they continue to struggle to gain access to basic services such as water and electricity. For all the improvements of the last 30 years, roughly 60% of the city is made up of these unregulated, informal and vulnerable zones.
The newspaper continued: “The growth of these settlements over the last three decades attests to the persistence of poverty and inequality in Mexico City—and as a result, its vulnerability to its next big earthquake.” This month’s two quakes, the accompany devastation, and deaths illustrate just how vulnerable it is.


In the beginning, it wasn’t the heat, but the humidity. In 1902, the workers at Sackett & Wilhelms Lithographing & Printing Company in New York City were fed up with the muggy summer air, which kept morphing their paper and ruining their prints. To fix the problem, they needed a humidity-control system. The challenge fell to a young engineer named Willis Carrier. He devised a system to circulate air over coils that were cooled by compressed ammonia. The machine worked beautifully, alleviating the humidity and allowing New York’s lithographers to print without fear of sweaty pages and runny ink.
But Carrier had a bigger idea. He recognized that a weather-making device to control humidity had even more potential to control heat. He went on to mass-manufacture the first modern air-conditioning unit at the Carrier Corporation (yes, that Carrier Corporation), which is still one of the largest HVAC manufacturers in the world. Air-conditioning went on to change far more than modern printing—it shaped global productivity, migration, and even politics.
The story of air-conditioning—and 49 other breakthroughs—is the subject of a new book, Fifty Inventions That Shaped the Modern Economy, by the economist and Financial Times columnist Tim Harford. I spoke to him recently about some of the book’s biggest ideas, and the following conversation has been edited for clarity and brevity.
Derek Thompson: Humans wanted to keep cool long before Carrier’s invention. But it’s sort of pathetic how we tried to do it. You describe the early 19th-century business of New England companies shipping large carved ice cubes insulated with sawdust around the country. New England literally exported ice the way Georgia exports peaches. There were even shortages during mild winters—“ice famines.”
Tim Harford: It was really hard to cool things! Before the invention of air-conditioning, you had to take something that was very cold and move it to places that were hot. And there were fascinating problems. For example, when the bodies of water that supplied the ice, like lakes, started getting polluted, the pollutants would be trapped in the pieces of ice. When they melted at their destination, it filled the air with unpleasant smells.
Thompson: Truly, thank God for Willis Carrier. The global effects of air-conditioning that you describe are mind-blowing. Air-conditioning transformed cities’ skylines, allowing for tall glassy skyscrapers that didn’t broil people in the top floors. It transformed demographics, allowing for migration in the U.S. to the Sun Belt, to Atlanta and Phoenix. By allowing politically conservative retirees to move south and west, you quote the author Steven Johnson saying that air-conditioning elected Ronald Reagan.
Harford: Yes, and it’s key to have a global perspective, too. This didn’t just reshape America. Air-conditioning reshaped the world. Places like Singapore and Shanghai are miserable when they’re hot and humid, but today they are global metropolises. There are studies saying that human productivity peaks around 70 degrees. That means that air-conditioning made us more productive, but also, by creating density in Singapore, it allows people to work longer and keep making the world a rich place. There is also the dark side of air-conditioning. You cool the temperature inside, but these units are energy-hungry, and they contribute to global warming.
Thompson: The first invention in the book is the plow, which facilitated the agricultural revolution. You write that “agricultural abundance creates rulers and the ruled, masters and servants, and inequality of wealth unheard of in hunter-gatherer societies.”
The second invention in the book is the gramophone, which allowed the most popular singers to reverberate in houses around the world, turning local stars into global superstars. That, too, creates inequality of wealth unheard of in pre-gramophone societies. Do you think technology inherently creates superstar effects and inequality?
Harford: There are two questions in there. First, does tech always increase inequality? Second, can politics offset that?
First, no. The famous example of technology displacing workers is the mechanized loom, famously smashed by the Luddites. These machines made their owners rich. But I think it’s possible that those looms actually decreased income inequality, because the new looms could be used by lower-skilled workers, who earned more money. On the other hand, I do think that much technology today is biased toward people who already have skills, which exacerbates winner-take-all effects.
But the winners of technology are often determined by politics and law. Bill Gates’s wealth could not exist without the U.S. property system that protected his software from being copied. Political choices have made him one of the richest people in the world.
Can politics offset inequality, too? Yes, of course. The welfare state can offset it.
Thompson: In fact, “the welfare state” is an invention that you discuss in the book, which is an impressively expansive definition of invention. The founders of the modern welfare state, in your telling, were much more interested in warfare than welfare. Otto Van Bismarck needed a dedicated army when he created pensions. To suppress his socialist rivals and secure popular support, Mussolini offered a fleet of social-welfare programs, including paid vacations and infant care. It seems to me that political necessity was the mother of political invention.
Harford: The welfare state was designed to strengthen the role of—it’s right there in the name—the state. It was initially designed to make the state more powerful and make people rely on the state.
More generally, “necessity as the mother of invention” is an intriguing cliché. Sometimes it’s not true. Take an invention like public-key cryptography. Here you had mathematicians tinkering around with prime numbers for a long time, in what might have seemed like useless work. Then some computer scientists realized that they could use the research behind prime numbers to encrypt messages. But it was unimaginable where the prime-number research would inevitably go.
But then, take barbed wire. Here, necessity was clearly the mother of invention. Americans could see for years that there was a burning need for a cheap material, which didn’t require a lot of wood, to fence in large areas in the western United States.
Thompson: In 1875, barbed wire was marketed as “The Greatest Discovery of the Age.” After the President Lincoln opened up the West by signing the Homestead Act of 1862, the American West was difficult to settle because settlers couldn’t enforce the boundaries of their property. You explain that barbed wire created the modern definition of private property.
Harford: Barbed wire allowed farmers after the Homestead Act to enforce their property rights. In any business, if you can’t practically enforce property rights, it doesn’t matter what the law says. You see the importance of enforcement in the music industry. The law says that the people who create music own the rights to it. But property rights meant something very different in the streaming world, when people could easily illegally download songs. Digital rights management was an attempt to create barbed wire for digital products. It tried to solve a “fencing” problem in music.
Thompson: Do you have a favorite invention in the book—or perhaps, an invention that you considered most underrated?
Harford: My favorite invention was paper. It didn’t occur to me at first. People said to me, “You must put the printing press in the book. It revolutionized the way we think; it led to the Reformation and rise of Europe.” But without paper, the press isn’t very useful. Yes, you can print on animal skin. But 5,000 copies of a book will require a quarter of a million sheep. Paper is what makes the printing press economically feasible. It’s an extremely underrated invention. Also, it has other practical uses, like toilet paper.
Thompson: As you write, the first example of paper money in world history was developed by merchants in Sichuan, China, around 1000 AD. It was an IOU. It wasn’t supposed to be a formal government currency. But the promise of value is one of the definitions of money, and an IOU is the promise of money. So people started trading their IOUs rather than carry around iron coins. They accidentally invented a private currency.
Harford: The paper money was a tradable IOU, which had been separated from the original debt. And if the debt can be traded, that’s what money is.
Thompson: It’s a beautiful lesson in the definition of money. And you write that when the Chinese government got wind of this, they tried to control the supply of IOU paper.
Harford: It wasn’t so much that the government wanted to control the money—they wanted to monopolize it. Here’s why. Let’s say you have an IOU from me. You go down to the corner store. You say, “Well, I could write you an IOU, or I could give you this IOU from Tim, who is a really good guy.” The corner store might say, “Actually, I trust Tim more than Derek anyway, so pass it over.” So now, my debt—Tim’s IOU—is circulating as money.
Here’s the magic: Tim’s IOU can be traded, over and over and over again, just like you traded it at the corner store. But nobody is ever coming back to me for the money. It’s more useful if it’s never cashed in. It’s pure profit. So, of course the government wants to be involved in that!
Thompson: In writing this book, did you learn anything about where invention comes from?
Harford: This is not a book about where ideas come from, or how inventions happen. Some of the inventions in this book come from private-sector geniuses, and there is stuff that is funded by the state. The biggest pattern is that it’s hard to find a pattern.


Facebook’s fact-checking efforts are on the rocks. Five months after the social-media giant debuted a third-party tool to stop the spread of dubious news stories on its platform, some of its fact-checker partners have begun expressing frustration that the company won’t share data on whether or not the program has been effective.
In the absence of that official data, a study by Yale researchers made waves last week by suggesting that flagging a post as “disputed” makes readers just a slim 3.7 percent less likely to believe its claim. Among Trump supporters and young people, the fact-checking program could even backfire: Those respondents were more likely to believe unflagged posts after they saw flags on others.* That concern was echoed earlier this year by the actor James Woods, who tweeted that a disputed tag on Facebook was the “best endorsement a story could have.”
The study—as well as ongoing revelations about how Russian troll farms might have used Facebook ads to meddle with the U.S. presidential election—has been stirring up the debate about whether and how social-media companies ought to police misinformation and propaganda on their platforms. Facebook claims that its efforts are working, and criticized the Yale researchers’ methodology, but a growing body of scholarship shows how difficult fact-checking has become online. With roots in old-fashioned cognitive biases that are amplified by social-media echo chambers, the problem is revealing itself to be extraordinarily difficult to fight at an institutional level.
Take Walter Quattrociocchi, a computer scientist at the University of Venice who has published a torrent of research over the past few years that examines how Facebook users consume information and self-segregate into online communities. In one recent paper, Quattrociocchi’s team looked at five years’ worth of Facebook posts, along with likes and comments, from a group of 413 public pages. These pages ranged from science-themed fare like “ScienceDaily” to ominously-titled conspiracy pages like “I Don’t Trust The Government.”
What Quattrociocchi found may have deep implications for the future of online fact-checking. Facebook users who cluster around conspiracy-related content tend to interact only with material that affirms their preexisting worldview, but in the rare cases when they do come into contact with dissenting information that attempts to debunk conspiracy theories—in the form of public posts by science-related pages—the conspiracy theorists become more, rather than less, likely to interact with conspiracy-related content in the future. In fact, conspiracy theorists who never interact with dissenting viewpoints are almost twice as likely as those who do to eventually drift away from conspiracy-themed content.
In other words, attempting to correct wrongheaded beliefs on Facebook appears to accomplish the precise opposite. Instead of alerting readers to the post’s factual inaccuracy, it entrenches them further in their erroneous beliefs. That’s not the same as studying the effect of a “disputed” tag on an article’s virality—only Facebook has access to that information—but it appears to be a good proxy.
Quattrociocchi doesn’t equivocate about his own feelings. He calls any promise that fact-checking can stomp out the spread of misinformation on social media a “hoax” and “bullshit.”
Though this issue predates the 2016 presidential election, the problem came into focus during that time. There were those Macedonian teens who discovered they could make a quick buck by publishing fictitious news reports designed to outrage conservative Americans. There was the rise of fringe media outlets like Infowars, whose figurehead Alex Jones has refused to retract conspiracy theories about how the Sandy Hook shooting was staged by paid actors. Donald Trump acknowledged what was already coming to be called “fake news” during the campaign by appropriating the term as a diss he still often lobs at CNN and The New York Times in the wake of unfriendly reports.
Before the internet, Quattrociocchi says, information had to make it past various gatekeepers before it could be widely disseminated. Those sentinels were often flawed, but they tended to filter out the most outrageous misinformation. Now, he suspects, information propagates via the same mechanisms as selfies or memes, leading to a crisis of authority.
None of that puts Facebook in an easy position. In the wake of the 2016 election, the platform faced a wave of criticism for having allowed misinformation to go unchecked on its platform during the campaign. Mark Zuckerberg, the company’s CEO, initially went on the defensive, but eventually acquiesced. In response to criticism of the new fact-checking program’s implementation, the company argues that flagging posts is only one part of a larger effort to fight misinformation on the platform. (Facebook declined to provide any further information on the record.)
Regardless, the difficulty of online fact-checking presents a grave challenge to public discourse. On the open web, biases can lead to vast, ingrown communities that reinforce preposterous beliefs. Kate Starbird, a researcher at the University of Washington, set out to study how useful, accurate information about safety and breaking news spreads on Twitter during bombings and mass shootings. But she started to notice an unnerving trend: As each disaster unfolded, a group of fringe accounts would start to promulgate paranoid theories about how the tragedy had been a “false flag,” carried out by the government or some other shadowy cabal.
Earlier this year, she published a provocative paper arguing that these strange networks of conspiracy posters are deeply connected to white nationalism, the alt-right, and the associated media ecosystem of sites like Infowars.
“We have ideas about how we make sense of the world,” Starbird said. “Our current information environment makes us newly vulnerable to things like filter bubbles and purposeful manipulation. People who understand how we think will try to influence us with ads and propaganda.”
Starbird did point out that not all findings are as somber as Quattrociocchi’s. One bright spot, for example, is that users may be more receptive to fact-checking if it comes from a friend—though that sort of engagement can be exhausting in these fraught times.
Starbird’s work has led her to reevaluate her outlook on the role of social media in the world. “When I started doing research about social media, I was very optimistic about its role in crisis response,” Starbird said. “But after the last year and a half, I’ve become increasingly depressed. I have a much more negative outlook.”
* This article originally stated that young people and Trump supporters were more likely to believe flagged posts than unflagged posts. We regret the error.


Nine months after Donald Trump won the presidency by unexpectedly swinging key states in the upper Midwest by slim margins, Facebook’s role in the 2016 election is still not clear.
Just in the last week, Facebook’s advertising has come under new scrutiny. Friday evening, The Wall Street Journal reported and CNN confirmed that special prosecutor Robert Mueller served the company with a search warrant to gather information on Russia-linked accounts that Facebook said purchased $150,000 worth of ads on the platform.
Earlier in the week, ProPublica found that anti-Semitic terms could be used to target advertising, prompting Facebook to scramble to patch up its advertising-keyword system.
Both incidents highlighted a basic question: What should Facebook know about the automated systems that it uses to make money?
Facebook’s advertising platform allows anyone, really, to purchase advertising targeted at particular populations. That could be people who recently got engaged, graduates of San Jose State, Red Sox fans, or as ProPublica discovered, bigots.
Facebook’s response to the ProPublica investigation was that few people actually targeted ads to people who listed themselves as “Jew haters.” It was a capability that was merely latent in the system.
And with the Russian ad buy, $150,000 of ad purchases is a drop in the bucket for Facebook, which had $8.8 billion of revenue in the fourth quarter of 2016 alone.
But American elections are not “Facebook-scale.” They can be swung by thousands of people. And within a massive automated system like Facebook’s, it was possible to stash a targeted disinformation campaign around the 2016 election.
The great irony of Facebook is that a system built to connect people makes high-margin advertising dollars by doing away not just with the middleman, but with any man or woman. There doesn’t have to be any human between an advertiser and the people who are targeted.
In the past, salespeople and production teams would have provided a check on the advertising that ran. Now, the people are gone, but the algorithmic systems that Facebook has created are not yet up to the task.
And not just with the advertising ecosystem. Earlier this year, a New York Times Magazine story asked, more or less, whether Facebook’s News Feed was, on balance, bad for American democracy. The aperture of the critique keeps widening, too: a new Times story highlights that governments all over the world are passing laws to regulate Facebook and other internet platforms.
The new zeitgeist has forced Facebook’s leadership to accept some responsibility for the way it shapes the political information people receive.
“Giving everyone a voice has historically been a very positive force for public discourse because it increases the diversity of ideas shared,” Mark Zuckerberg himself admitted. “But the past year has also shown it may fragment our shared sense of reality.”
That followed the burst of “fake news ” in the original sense of the coinage around the election: straight-up hoaxes perpetrated by a variety of actors like the viral story that the Pope endorsed Donald Trump. (He did not.)
The real problem of “fake news” wasn’t merely that it could work on Facebook’s system, but that the incentives of News Feed seemed to make fake news perform substantially better than real news. URLs filled with nonsense aren’t the problem, but the actual distribution system that Facebook’s News Feed represents. If lies, hoaxes, or “truthiness” are so effective, that does not portend good things for factual accounts. The system, whatever else it might be, does not seem to be as neutral as Facebook would like it to be.
It’s possible that the Russian ad buy will end up connected to this larger complex of problems. The way that many media organizations use Facebook ads is to test their content, not to distribute it. It could be that the ads were merely a good way to mark certain kinds of users or find the most viral, divisive content.
Both kinds of knowledge could have multiplied the impact of any kind of disinformation campaign.
One thing seems clear: Facebook will end up before Congress in one way or another. Before congresspeople, demurrals won’t cut it. Facebook has to know itself better, even if that means hiring a lot more people. Facebook wants these things to be artificial-intelligence problems, but counterintelligence may be the more relevant field right now.


Wiffle ball is a variant of baseball played with a plastic perforated ball. Eight three-quarter-inch, oblong holes cover half the ball’s surface area, while the other hemisphere is uninterrupted. Originally designed to relieve the arm of a young baseball pitcher (the son of its inventor, David N. Mullany), the ball achieves a curving trajectory without requiring the pitcher to impart spin or hurl at top speed. Each ball is packaged with instructions for how to release it in order to achieve various effects—with the perforations up for a straight ball, toward the pitcher’s thumb for a curve, and toward the outer fingers for a slider.
The inventor’s grandsons still run the family enterprise, with a product unchanged since its 1953 launch. Their dad, the pitcher for whom the ball was designed, told The Atlantic in 2002 that the Mullany family believed cutting the holes might create a “weight imbalance” that would cause the ball to curve. To this day, the company insists, “we don’t know exactly why it works—it just does!”
That folksy answer is charming, but a scientific one can foster even greater admiration for this curious ball and the sport that makes use of it.
* * *
Before Wiffle ball, uncertainty over the baseball curveball motivated investigation from both the media and the scientific community. Life magazine commissioned photographic studies of curve balls in 1941, to determine whether the phenomenon was real or an optical illusion. The magazine’s editors concluded it was illusory, enraging pitchers of the era.
In 1953, Igor Sikorsky, the inventor of the first viable helicopter, performed experiments demonstrating that a spinning sphere did indeed experience lateral deflection (that is, “curve”). Although Sikorsky didn’t publish the data, his collaborators did share a summary of findings—among them, that four-seam pitches curved more than two-seam. By 1959, the University of Notre Dame’s F.N.M. Brown produced gorgeous wind-tunnel images of smoke streamlines showing the way a ball with backspin deflected its wake. And that same year, the National Institute of Standards and Technology engineer Lyman Briggs published his own study, concluding that a baseball could indeed arc up to 17.5 inches on its way to the plate. The spinning ball lowered air pressures on one hemisphere, pulling the ball in that direction.
And so, physicists confirmed that a curveball really does curve. But even so, the batter’s perception is different. At the plate, a pitch appears to “break”—jumping or dropping suddenly, rather than smoothly arcing. The neuroscientist Arthur Shapiro has shown that this optical illusion may be due to the way our visual system processes information.
That’s for baseballs, which are made from rubber or cork wrapped in yarn and leather. What about Wiffle balls?
Wiffle balls wouldn’t be possible without the ubiquity of plastic. In postwar America, lab-synthesized plastics flooded consumer markets once they were no longer needed for wartime duties in mortar fuses, parachutes, soldiers’ service-issued combs, aircraft components, or in the Teflon containers used for the Manhattan Project’s most volatile gases. The first Wiffle-ball prototypes were made by cutting holes into the plastic packaging for Coty perfume. Today’s mass-produced Wiffle balls begin life as polyethylene pellets, melted and injection-molded into hemispheres that are then pressure-sealed together.
The asymmetric flow field caused by the Wiffle-ball holes might yield the same result as does the effect on a spinning baseball: a trajectory that curves or bends in the direction of the resulting pressure force. Still, whether the ball tends to curve toward, or away from, those holes is a matter of some contention, actively debated in Wiffle chat rooms and on the field.
Robert Adair, a Yale physicist and the author of The Physics of Baseball, has speculated that the holes, like the stitching on a baseball, accelerate turbulence on the perforated side of the Wiffle ball. Faster airflow may lower the pressure and cause the ball to move toward the holes. However, the Brooklyn College professor Peter Brancazio has countered that scuffing a Wiffle ball “essentially takes the holes out of the equation.” If the smooth, unperforated side of the ball were sufficiently roughened, it might disturb the air more than the holes, reversing the pressure asymmetry and causing the ball to curve away from the holes.
* * *
My own study of baseball aerodynamics began as a trap meant to lure my undergraduate students toward the beauty of fluid mechanics. We skewered some baseballs and Wiffle balls and used a wind tunnel to measure the forces—lift, drag, and side or lateral forces—as functions of things like the airspeed and spin rate. For the Wiffle ball, we also varied the orientation of the ball with respect to the airflow, making our own version of the manufacturer’s pitching instructions.
With perforations on either side of the ball, we found that the Wiffle balls experienced a lateral force that generally acted to push the ball toward the position of the holes. Things got more complicated when the perforations were on the upstream portion of the ball. As shown in the first image below, fog traces the airflow over a ball with the holes facing the flow, with a symmetric wake pattern suggesting that if we untethered the ball it would fly straight. The second image shows the flow over a ball with its holes facing up, and a wake that is deflected upward, meaning that the ball is experiencing a downward force.
Unlike a baseball, air can flow through a Wiffle ball. Our results suggested that some airflow is captured within the ball, and that this captured air creates a “trapped vortex” effect that also induces a force on the ball. This effect can either compete with or complement the asymmetric pressure distribution outside the ball due to the perforations. So, we measured the airflow inside the balls in the wind tunnel, and also performed computational fluid-dynamics calculations to confirm this (below).
Which effect—internal or external flow—dominates the ball trajectory depends on the pitching speed, and how much spin the pitcher puts on the ball. Especially at moderate speeds, nuances such as ball scuffing can be decisive.
Publishing a study of Wiffle-ball aerodynamics reveals just how many people care about Wiffle ball, and how deeply. I learned about Wiffle leagues full of adult players, and a tournament in which pitchers hurl lightweight Wiffle balls at 80 to 90 miles per hour, with wicked curves and drops. This isn’t just tweens trying to avoid elbow injuries or broken windows; Wiffle ball is serious. Players sent me modified Wiffle balls with handwritten notes describing the effects the alterations had on their pitches. Some of the notes implied (or insisted on) a challenge: See if your fancy wind tunnel can figure this out.
There’s a hot-rod culture at work in the customization of Wiffle balls. It’s legal in many adult leagues, too. Scuffing will rough up the surface, creating a locally turbulent airflow and a reduced pressure that pulls the ball toward the scuff. Some players use a knife to change the shape of the Wiffle perforations, demonstrating their intuition that the airflow into the ball might be important. Indeed, this alteration can reroute or accelerate the entering air, changing the ball’s eventual trajectory due to the trapped vortices inside the ball. Blocking some of the perforations will have similar effects.
Pitching with top, back, or sidespin—as a former baseball pitcher is likely to do when taking up Wiffle ball later in life—makes things even more interesting. And the unhittable knuckleball resulting from a Wiffle ball lobbed with its perforations toward the batter is a natural effect of the uneasy balancing act being performed by these competing effects of internal and external aerodynamics.
The Mullany family and their company have kept the ball itself above the fray of modifications, never varying the features of the product or marketing any “air effects” kits. Players have full ownership of their modifications, and the company’s lack of involvement lends a sense that the player scuffing or knifing a ball is a bit of an outlaw, rebelling against the rigidity of baseball and its regulations.
* * *
The Wiffle ball was designed to increase access to the thrills of baseball. It made trick pitches available to younger players, and also made the game accessible to adults whose ball-playing glory days are now behind them, offering an experience similar to baseball without the risk of rotator-cuff injury. The Wiffle Ball company has cultivated this all-American, family-run story with their “it just works, who could possibly explain it?” mystique. There is innocent nostalgia baked into the game. And yet, understanding how the ball works its magic takes none of that folk glory away. Instead, it increases respect for the sport, by showing how much is possible within it.
This article appears courtesy of Object Lessons.


Facebook lives and dies by its algorithms. They decide the order of posts in your News Feed, the ads you see when you open the app, and which which news topics are trending. Algorithms make its vast platform possible, and Facebook can often seem to trust them completely—or at least thoughtlessly.
On Thursday, a pitfall of that approach became clear. ProPublica revealed that people who buy ads on Facebook can choose to target them at self-described anti-Semites. The company’s ad-targeting tool allows companies to show ads specifically to people who use the language “Jew hater” or “How to burn Jews” on their profile.
Should Facebook Ads Be Regulated Like TV Commercials?
The number of people in those groups alone isn’t large enough to target for an ad, as Facebook will only let advertisers focus on groups of several thousand people. The platform’s ad-selling portal reported, at most, 2,200 self-interested “Jew haters.”
But the anti-Semitic categories can be lumped up into larger groups, which can be targeted. When ProPublica first tried buying an ad, the service suggested that it add “Second Amendment,” presumably because self-described “Jew haters” also expressed an interest in that law. ProPublica declined its suggestion, ultimately targeting fans of Germany’s National Democratic Party, an ultranationalist organization often described as neo-Nazi.
ProPublica does not argue that Facebook actually set up an anti-Semitic demographic that can be targeted by advertising. Rather, it suggests that algorithms—which developed the list of targetable demographics—saw enough people self-describing as “Jew hater” to lump them into a single group.
“Facebook does not know what its own algorithm is doing,” said Susan Benesch, a faculty associate at the Berkman Klein Center for Internet and Society and the director of the Dangerous Speech Project. “It is not the case that somebody at Facebook is sitting in a dark room cackling, and said, ‘Let’s encourage people to sell Nazi ads to Nazi sympathizers.’”
She continued: “In some ways, that would be much easier to correct, because there would be some intention, some bad person doing this on purpose somewhere. The algorithm doesn’t have a bad intention.”
“It’s not surprising, sadly, that people are expressing hateful views on Facebook, but it was surprising to see that hate reflected back in the categories that Facebook uses to sell ads,” said Aaron Rieke, a technologist at Upturn, a civil-rights and technology consulting firm based in Washington, D.C.
He suggested that human oversight may be the best mechanism to avoid this kind of mistake. “We don’t need to be too awed by this problem,” he told me: Even if its list of algorithmic categories is tens of thousands of entries long, it would take relatively little time for a couple paid, full-time employees to go through it.
“This is not a situation where you’re asking Facebook to monitor the ocean of user content,” he said. “This is saying: Okay, you have a finite list of categories, many of which were generated automatically. Take a look, and see what matches up with your community standards and the values of the company. Facebook can monitor the things it does that make it money.”
After ProPublica published its story, Facebook said that it had removed the offending ad categories. “We know we have more work to do, so we're also building new guardrails in our product and review processes to prevent other issues like this from happening in the future,” Rob Leathern, a product-management director at Facebook, said in a statement.
He also reiterated that Facebook does not allow hate speech. “Our community standards strictly prohibit attacking people based on their protected characteristics, including religion, and we prohibit advertisers from discriminating against people based on religion and other attributes.”
Last October, another ProPublica investigation revealed that Facebook allowed landlords and other housing providers to omit certain races when selling advertising, a practice that violates the Fair Housing Act.
To Jonathan Zittrain, a professor of law at Harvard University, that story suggests the entire way that tech companies currently sell ads online might need an overhaul. “For categories with tiny audiences, with titles drawn from data that Facebook users themselves enter—such as education and interests—it may amount to a tree falling in a forest that no one hears,” he said. “But should anything more than negligible ad traffic begin with categories, there’s a responsibility to see what the platform is supporting.”
He continued:
More generally, there should be a public clearinghouse of ad categories —and ads—searchable by anyone interested in what’s being shown to audiences large and small. Unscrupulous lenders ought not to be able to exquisitely target vulnerable people at their greatest moments of weakness with no one else ever knowing, and real accountability takes place through public channels, not only through adding another detachment of “ad checkers” internal to the company.
Indeed, Facebook might not be alone in permitting unscrupulous ads to get through. The journalist Josh Benton demonstrated on Thursday that many of the same anti-Semitic keywords used by Facebook can also be used to buy ads on Google.
For Facebook, this entire episode comes as the platform comes under unprecedented political scrutiny. Last week, Facebook admitted to selling $100,000 in internet ads to a Kremlin-connected troll farm during last year’s U.S. presidential election. The ads, which the company did not previously reveal, could have been seen by tens of millions of Americans.
On Wednesday, Facebook announced new rules governing what kinds of content can and can’t be monetized on its platform, prohibiting any violent, pornographic, or drug- or alcohol-related content. The guidelines followed the U.S.-wide debut of a video tab in its mobile app.
Zittrain said the episode reminded him of the T-shirt vendor Solid Gold Bomb. In 2011, that firm used software to auto-generate more than 10 million possible T-shirts, almost all variations on a couple themes. Shirts were only produced when someone actually ordered one. But after the Amazon listing for one shirt, which read “KEEP CALM AND RAPE A LOT,” went viral in 2013, Amazon shut down the company’s accounts, and it saw its business collapse.
His point: Algorithms can seem ingenious at making money, or making T-shirts, or doing any task, until they suddenly don’t. But Facebook is more important than T-shirts: After all, the average American spends 50 minutes of their time there every day. Facebook’s algorithms do more than make the platform possible. They also serve as the country’s daily school, town crier, and newspaper editor. With great scale comes great responsibility.


Suddenly, everything is a computer. Phones, of course, and televisions. Also toasters and door locks, baby monitors and juicers, doorbells and gas grills. Even faucets. Even garden hoses. Even fidget spinners. Supposedly “smart” gadgets are everywhere, spreading the gospel of computation to everyday objects.
It’s enough to make the mundane seem new—for a time anyway. But quickly, doubts arise. Nobody really needs smartphone-operated bike locks or propane tanks. And they certainly don’t need gadgets that are less trustworthy than the “dumb” ones they replace, a sin many smart devices commit. But people do seem to want them—and in increasing numbers. There are now billions of connected devices, representing a market that might reach $250 billion in value by 2020.
Why? One answer is that consumers buy what is on offer, and manufacturers are eager to turn their dumb devices smart. Doing so allows them more revenue, more control, and more opportunity for planned obsolescence. It also creates a secondary market for data collected by means of these devices. Roomba, for example, hopes to deduce floor plans from the movement of its robotic home vacuums so that it can sell them as business intelligence.
But market coercion isn’t a sufficient explanation. More so, the computational aspects of ordinary things have become goals unto themselves, rather than just a means to an end. As it spreads from desktops and back-offices to pockets, cameras, cars, and door locks, the affection people have with computers transfers onto other, even more ordinary objects. And the more people love using computers for everything, the more life feels incomplete unless it takes place inside them.
* * *
A while back, I wrote about a device called GasWatch, a propane-tank scale that connects to a smartphone app. It promises to avert the threat of cookouts ruined by depleted gas tanks.
When seeing devices like this one, I used to be struck by how ridiculous they seemed, and how little their creators and customers appeared to notice, or care. Why use a computer to keep tabs on propane levels when a cheap gauge would suffice?
But now that internet-connected devices and services are increasingly the norm, ridicule seems toothless. Connected toasters promise to help people “toast smarter.” Smartphone-connected bike locks vow to “Eliminate the hassle and frustration of lost keys and forgotten combinations,” at the low price of just $149.99. There’s Nest, the smart thermostat made by the former designer of the iPod and later bought by Google for $3.2 billion. The company also makes home security cameras, which connect to the network to transmit video to their owners’ smartphones. Once self-contained, gizmos like baby monitors now boast internet access as an essential benefit.
The trend has spread faster than I expected. Several years ago, a stylish hotel I stayed at boasted that its keycards would soon be made obsolete by smartphones. Today, even the most humdrum Hampton Inn room can be opened with Hilton’s app. Home versions are available, too. One even keeps analytics on how long doors have been locked—data I didn’t realize I might ever need.
These devices pose numerous problems. Cost is one. Like a cheap propane gauge, a traditional bike lock is a commodity. It can be had for $10 to $15, a tenth of the price of Nokē’s connected version. Security and privacy are others. The CIA was rumored to have a back door into Samsung TVs for spying. Disturbed people have been caught speaking to children over hacked baby monitors. A botnet commandeered thousands of poorly secured internet-of-things devices to launch a massive distributed denial-of-service attack against the domain-name system.
Reliability plagues internet-connected gadgets, too. When the network is down, or the app’s service isn’t reachable, or some other software behavior gets in the way, the products often cease to function properly—or at all.
Take doorbells. An ordinary doorbell closes a circuit that activates an electromagnet, which moves a piston to sound a bell. A smart doorbell called Ring replaces the button with a box containing a motion sensor and camera. Nice idea. But according to some users, Ring sometimes fails to sound the bell, or does so after a substantial delay, or even absent any visitor, like a poltergeist. This sort of thing is so common that there’s a popular Twitter account, Internet of Shit, which catalogs connected gadgets’ shortcomings.
As the technology critic Nicholas Carr recently wisecracked, these are not the robots we were promised. Flying cars, robotic homes, and faster-than-light travel still haven’t arrived. Meanwhile, newer dreams of what’s to come predict that humans and machines might meld, either through biohacking or simulated consciousness. That future also feels very far away—and perhaps impossible. Its remoteness might lessen the fear of an AI apocalypse, but it also obscures a certain truth about machines’ role in humankind’s destiny: Computers already are predominant, human life already plays out mostly within them, and people are satisfied with the results.
* * *
The chasm between the ordinary and extraordinary uses of computers started almost 70 years ago, when Alan Turing proposed a gimmick that accidentally helped found the field of artificial intelligence. Turing guessed that machines would become most compelling when they became convincing companions, which is essentially what today’s smartphones (and smart toasters) do. But computer scientists missed the point by contorting Turing’s thought experiment into a challenge to simulate or replace the human mind.
In his 1950 paper, Turing described a party game, which he called the imitation game. Two people, a man and a woman, would go behind closed doors, and another person outside would ask questions in an attempt to guess which one was which. Turing then imagined a version in which one of the players behind the door is a human and the other a machine, like a computer. The computer passes the test if the human interlocutor can’t tell which is which. As it institutionalized, the Turing test, as it is known, has come to focus on computer characters—the precursors of the chatbots now popular on Twitter and Facebook Messenger. There’s even an annual competition for them. Some still cite the test as a legitimate way to validate machine intelligence.
But Turing never claimed that machines could think, let alone that they might equal the human mind. Rather, he surmised that machines might be able to exhibit convincing behavior. For Turing, that involves a machine’s ability to pass as something else. As computer science progressed, “passing” the Turing test came to imply success as if on a licensure exam, rather than accurately portraying a role.
That misinterpretation might have marked the end of Turing’s vision of computers as convincing machines. But he also baked persuasion into the design of computer hardware itself. In 1936, Turing proposed a conceptual machine that manipulates symbols on a strip of tape according to a finite series of rules. The machine positions a head that can read and write symbols on discrete cells of the tape. Each symbol corresponds with an instruction, like writing or erasing, which the machine executes before moving to another cell on the tape.
The design, known as the universal Turing machine, became an influential model for computer processing. After a series of revisions by John von Neumann and others, it evolved into the stored-programming technique—a computer that keeps its program instructions as well as its data in memory.
In the history of computing, the Turing machine is usually considered an innovation independent from the Turing test. But they’re connected. General computation entails a machine’s ability to simulate any Turing machine (computer scientists call this feat Turing completeness). A Turing machine, and therefore a computer, is a machine that pretends to be another machine.
Think about the computing systems you use every day. All of them represent attempts to simulate something else. Like how Turing’s original thinking machine strived to pass as a man or woman, a computer tries to pass, in a way, as another thing. As a calculator, for example, or a ledger, or a typewriter, or a telephone, or a camera, or a storefront, or a café.
After a while, successful simulated machines displace and overtake the machines they originally imitated. The word processor is no longer just a simulated typewriter or secretary, but a first-order tool for producing written materials of all kinds. Eventually, if they thrive, simulated machines become just machines.
Today, computation overall is doing this. There’s not much work and play left that computers don’t handle. And so, the computer is splitting from its origins as a means of symbol manipulation for productive and creative ends, and becoming an activity in its own right. Today, people don’t seek out computers in order to get things done; they do the things that let them use computers.
* * *
When the use of computers decouples from its ends and becomes a way of life, goals and problems only seem valid when they can be addressed and solved by computational systems. Internet-of-things gadgets offer one example of that new ideal. Another can be found in how Silicon Valley technology companies conceive of their products and services in the first place.
Take abusive behavior on a social networks as an example. Earlier this year, Chris Moody, Twitter’s vice president of data strategy, admitted, “We have had some abuse on the platform.” Moody cited stopping abuse as the company’s first priority, and then added, “But it’s a very, very hard challenge.” To address it, Twitter resolved to deploy IBM’s Watson AI to scan for hate speech. Google has a similar effort. One of its labs has developed Perspective, an “API that uses machine learning to spot abuse and harassment online.”
Sometimes tech firms will make efforts like these a matter of business viability—the search for “scalable” solutions to products and services. When I asked Twitter about Moody’s comments, a spokesperson told me that the company uses a combination of computational and human systems when reviewing safety content, but they couldn’t share many specifics.
It sounds promising, but the results are mixed. Twitter claims it’s getting better at fighting abuse, but it still seems to ignore even the worst cases. And Google’s Perspective can be fooled by simple typos and negations.
Even though it’s in these companies’ immediate business interest to solve abuse problems now, making online spaces safer is assumed to require a computational answer. Human content moderation is notoriously hard, admittedly. And the volume of content is so high, a matter Twitter emphasizes, that computational systems are needed to manage and operate them.
But perhaps managing abuse is “a very, very hard challenge” largely on account of that assumption. The very idea of a global network of people able to interact with one another anonymously precludes efficient means of human intervention. Twitter’s answer assumes that their service, which is almost entirely automated by apps and servers, is perfectly fine—they just need to find the right method of computational management to build atop it. If computer automation is assumed to be the best or only answer, then of course only engineering solutions seem viable.
Ultimately, it’s the same reason the GasWatch user wouldn’t choose a cheap, analog gauge to manage cookout planning. Why would anyone ever choose a solution that doesn’t involve computers, when computers are available? Propane tanks and bike locks are still edge cases, but ordinary digital services work similarly: The services people seek out are the ones that allow them to use computers to do things—from finding information to hailing a cab to ordering takeout. This is a feat of aesthetics as much as it is one of business. People choose computers as intermediaries for the sensual delight of using computers, not just as practical, efficient means for solving problems.
That’s how to understand the purpose of all those seemingly purposeless or broken services, apps, and internet-of-things devices: They place a computer where one was previously missing. They transform worldly experiences into experiences of computing. Instead of machines trying to convince humans that they are people, machines now hope to convince humans that they are really computers. It’s the Turing test flipped on its head.
* * *
There’s a name for that, as it happens: the “reverse Turing test.” CAPTCHAs, those codes in online forms that filter out automated bots, are reverse Turing tests in which the computer judges whether a user is a human. There are also reverse Turing tests in which people try to guess which actor is a human among a group of computers.
These works use the Turing test as an experience in its own right, rather than a measure of intelligence. That precedent dates to the early days of computing. One of the most famous examples of imitation game-style chatterbots is Joseph Weizenbaum’s 1966 program ELIZA. The program acts like a Rogerian therapist—a kind of psychotherapy built on posing clients’ questions back to them. It’s an easy pattern to model, even in the mid-1960s (“What would it mean to you if you got a new line printer?” ELIZA responds heroically to a user pretending to be an IBM 370 mainframe), but it hardly counts as intelligence, artificial or otherwise. The Turing test works best when everyone knows the interlocutor is a computer but delights in that fact anyway.
“Being a computer” means something different today than in 1950, when Turing proposed the imitation game. Contra the technical prerequisites of artificial intelligence, acting like a computer often involves little more than moving bits of data around, or acting as a controller or actuator. Grill as computer, bike lock as computer, television as computer. An intermediary.
Take Uber. The ride-hailing giant’s main business success comes from end-running labor and livery policy. But its aesthetic success comes from allowing people to hail cars by means of smartphones. Not having to talk to anyone on the phone is a part of this appeal. But so is seeing the car approach on a handheld, digital map. Likewise, to those who embrace them, autonomous vehicles appeal not only because they might release people from the burden and danger of driving, but also because they make cars more like computers. Of course, computers have helped cars run for years. But self-driving cars turn vehicles into machines people know are run by computers.
Or consider doorbells once more. Forget Ring, the doorbell has already retired in favor of the computer. When my kids’ friends visit, they just text a request to come open the door. The doorbell has become computerized without even being connected to an app or to the internet. Call it “disruption” if you must, but doorbells and cars and taxis hardly vanish in the process. Instead, they just get moved inside of computers, where they can produce new affections.
One such affection is the pleasure of connectivity. You don’t want to be offline. Why would you want your toaster or doorbell to suffer the same fate? Today, computational absorption is an ideal. The ultimate dream is to be online all the time, or at least connected to a computational machine of some kind.
This is not where anyone thought computing would end up. Early dystopic scenarios cautioned that the computer could become a bureaucrat or a fascist, reducing human behavior to the predetermined capacities of a dumb machine. Or else, that obsessive computer use would be deadening, sucking humans into narcotic detachment.
Those fears persist to some extent, partly because they have been somewhat realized. But they have also been inverted. Being away from them now feels deadening, rather than being attached to them without end. And thus, the actions computers take become self-referential: to turn more and more things into computers to prolong that connection.
* * *
This new cyberpunk dystopia is more Stepford Wives, less William Gibson. Everything continues as it was before, but people treat reality as if it were in a computer.
When seen in this light, all the issues of contemporary technology culture—corporate data aggregation, privacy, what I’ve previously called hyperemployment (the invisible, free labor people donate to Facebook and Google and others)—these are not exploitations anymore, but just the outcomes people have chosen, whether through deliberation or accident.
Among futurists, the promise (or threat) of computer revolution has often been pegged to massive advances in artificial intelligence. The philosopher Nick Bostrom has a name for AI beyond human capacity: “superintelligence.” Once superintelligence is achieved, humanity is either one step away from rescuing itself from the drudgery of work forever, or it’s one step away from annihilation via robot apocalypse. Another take, advocated by the philosopher of mind David Chalmers and the computer scientist Ray Kurzweil, is the “singularity,” the idea that with a sufficient processing power, computers will be able to simulate human minds. If this were the case, people could upload their consciousness into machines and, in theory, live forever—at least for a certain definition of living. Kurzweil now works at Google, which operates a division devoted to human immortality.
Some even believe that superintelligence is a technology of the past rather than the future. Over millions of years, a computer simulation of sufficient size and complexity might have been developed, encompassing the entirety of what Earthly humans call the universe. The simulation hypothesis, as this theory is known, is of a piece with many ancient takes on the possibility that reality is an illusion.
But the real present status of intelligent machines is both humdrum and more powerful than any future robot apocalypse. Turing is often called the father of AI, but he only implied that machines might become compelling enough to inspire interaction. That hardly counts as intelligence, artificial or real. It’s also far easier to achieve. Computers already have persuaded people to move their lives inside of them. The machines didn’t need to make people immortal, or promise to serve their every whim, or to threaten to destroy them absent assent. They just needed to become a sufficient part of everything human beings do such that they can’t—or won’t—imagine doing those things without them.
There’s some tragedy in this future. And it’s not that people might fail to plan for the robot apocalypse, or that they might die instead of uploading. The real threat of computers isn’t that they might overtake and destroy humanity with their future power and intelligence. It’s that they might remain just as ordinary and impotent as they are today, and yet overtake us anyway.


Last week, Facebook disclosed to congressional investigators that it sold $100,000 worth of advertisements to a troll farm connected to the Kremlin surrounding the U.S. presidential election. These advertisements, which targeted voters with divisive political content, added even more evidence of Russia’s attempts to meddle with the election. But they also contributed to a larger conversation about free speech in an era where social-media posts replace political pamphlets and the public square has increasingly moved into cyberspace.
Tech giants like Facebook are largely left to decide for themselves how to arbitrate what is said on their platforms: what speech is permissible or not, whether to flag propaganda or not, and how to regulate advertisements. While, across mediums, it is illegal for foreigners to financially influence U.S. elections, last week’s news also means a new reckoning with the specific responsibility of tech companies to regulate the ads they sell and the content they host.
Legal and tech experts have growing concerns about individual private companies’ massive power over public discourse on the internet. The ability of any single entity to significantly arbitrate speech has previously belonged to the government alone. Now, Facebook, with a small number of its peers, has assumed much of the responsibility for regulating this influential realm online.
Jay Stanley, a privacy expert at the American Civil Liberties Union, sees danger in steps toward censorship on social media. “We would ideally like to see companies that provide a forum in which people communicate with each other to be free-speech zones, especially companies that play important roles in our national discourse,” he said. “Once companies go down the path of engaging in censorship, line-drawing decisions are often impossible, inconsistent, capricious, or downright silly.”
But Andrew McLaughlin, the cofounder of Higher Ground Labs, a company that invests in technology to help progressive candidates, believes that platforms should suppress propaganda in ad space. “Despite their best intentions, tech companies have built systems that are so open to manipulation by bots and trolls and other techniques that they effectively reward propaganda,” he says. “Failing to tackle that problem means ceding the terrain to fraudsters, fake-news pushers, and other kinds of propagandists, who easily gain the upper hand.”
Susan Benesch, a faculty associate at Harvard’s Berkman Klein Center for Internet and Society and the founding director of the Dangerous Speech Project, likewise falls in this camp. “If you deceive people consistently and on a large scale, you are probably damaging their willingness to engage as citizens in our democracy,” she says. She believes that the public should continue to pressure tech companies to create some mechanism for oversight as to what content is taken offline.
Facebook’s ad policy already prohibits some forms of messaging, such as use of politically or socially controversial material for commercial benefits. And on Wednesday, Facebook announced new guidelines for monetized content—including new steps to verify the authenticity of buyers, which could deter trolls and bots. One of its provisions is a warning against making money off some forms of deception: Users who “share clickbait or sensationalism, or post misinformation and false news, may be ineligible or may lose their eligibility to monetize.”
As far as non-ad content, tech companies censor certain disagreeable speech as well. Facebook, Twitter, and YouTube have removed ISIS-linked propaganda and accounts from their platforms. Following The Daily Stormer’s inflammatory coverage of Charlottesville, Google and the web-hosting company GoDaddy refused to provide service to the neo-Nazi website. Meanwhile, the internet company Cloudflare revoked the site’s DDoS-attack protections. And the chat app Discord banned other alt-right groups.
Eric Goldman, a codirector of Santa Clara University’s High-Tech Law Institute, sees these cases as “inextricably linked” to the recent controversy over Facebook advertisements. “On the one hand, I’m excited when I see social-media companies and other online services being thoughtful about what kind of content they want on their services,” he said. “On the other hand, whenever we see online services tinkering with political ads, we have the risk they might be adding their own biases into the mix.”
Experts seem to agree on two principles with respect to free speech and tech company arbitration. First, social-media companies, like other private publishers and unlike the government, are not bound by the First Amendment—meaning that they have discretion over what kind of speech is allowed on their platforms. And second, it is dangerous for the government to play a role in censoring content and advertisements on social media—beyond requiring companies to ban illegal activity from their platforms.
Further, many experts believe that mandating the disclosure of political-advertisement sources on Facebook, like the requirement for television or radio to identify ad sponsors on air, could help solve the problem. “Even if nothing else is done, it should be possible to require that political advertisers on Facebook embed the financing information in the ad, and it should be possible for Facebook to archive a copy of the ad with state elections officials or the FEC,” says Philip Howard, the director of research at the Oxford Internet Institute.
Senators Mark Warner and Martin Heinrich have suggested that social-media ads should be regulated like TV ads. And Warner, who vice-chairs the Senate Intelligence Committee, is calling for Facebook and Twitter to testify about Russian election meddling, as he sees the current landscape of social media and political campaigns as “the Wild West.”
Of course, with any new disclosure requirements, defining the realm of political speech for Facebook and other online entities won’t be simple. Most of the Russian ads disclosed last week were aimed at increasing the divisiveness across contested issues in America rather than overtly endorsing a candidate. Similarly, lawmakers would need to define the extent (if any) to which digital platforms should be required to investigate the underlying identities of its ad buyers.
Facebook, for its part, is “always exploring ways to make our ad policies more effective and transparent,” a press representative claims.
The story of Russian Facebook advertisements also fuels the debate about the responsibilities of American companies that serve international populations. For example, India has more active Facebook users than the United States. Chinmayi Arun, an assistant professor at the National Law University in Delhi, sees the need for additional mechanisms to both flag harmful content and to contest unreasonable censorship by platforms.
It remains unclear if Facebook will adopt this or other measures. Morgan Weiland, an attorney and scholar affiliated with Stanford Law School’s Center for Internet and Society, believes that along with previous problematic content—like untrue news and hoax videos—last week’s revelations may help force tech companies to identify their values and define their role in the public discourse.
“Right now they are operating in an arena where they have some, but very few, legal responsibilities,” she says. “We are going to keep seeing examples of this kind, and at some point the jig is going to be up and the regulators are going to act. Will the tech companies be the first movers and decide what they [themselves] will get to say, or will the regulators be the ones that move first?”
Neil Richards, a law professor at Washington University in St. Louis, likens the present moment to a Facebook relationship status: “It’s complicated,” he says. “The way these advertising models subject all of us and our attention to advertising, a kind of mental pollution, is unprecedented in human history. The decisions we make regarding the way that tech companies are a market for advertising affect, in a very real sense, what kind of digital democracy we are going to build.”


Customers who shell out $999 for an iPhone X when it comes out in November will have a new party trick in their pockets: They’ll be able to unlock the phone with nothing more than a quick glance at the screen. When they look away, it will lock up again.
When new features like this one, which Apple is calling Face ID, make it easier to unlock a phone, they save time; Apple says iPhone users unlock their phones an average of 80 times a day. But make it too easy to get into a phone and people start to get nervous.
Apple executives emphasized the security of Face ID as they announced the latest generation of iPhones on Tuesday. Once a phone learns to recognize its owner, using a 3-D map made up of more than 30,000 infrared dots projected onto a person’s face, it’ll only unlock itself when it senses that the owner is looking straight at the device. The map can’t be fooled by photos or masks, said Phil Schiller, Apple’s senior vice president for marketing, at Tuesday’s event, and it’s stored locally in a secure part of the device. Locking the phone just requires closing your eyes or glancing away, so waving a phone in front of its sleeping owner won’t unlock it.
Even if Face ID is advanced enough to keep pranksters out, many wondered Tuesday if it would actually make it easier for police to get in. Could officers force someone they’ve arrested to look into their phone to unlock it?
That’s a question with no easy answer. Technologists had a similar question a few years ago when smartphones started rolling out fingerprint readers: Could cops make someone scan their thumbprint to unlock their phone? The answer, it turned out, is ... maybe. In several cases since 2014, state and federal judges have signed search warrants that compelled fingerprint unlocks. The Fifth Amendment protects people from having to give up information that could incriminate them, like a password or PIN code. But a thumbprint isn’t something you know, which would be protected by the Constitution; it’s something you are. Like DNA or your handwriting, physical attributes are usually considered outside the boundaries of Fifth Amendment protections.
Despite the judges’ decisions, some legal scholars disagree with the idea that the government should be able to use a search warrant to force people to unlock phones secured with biometric authentication, which relies on physical characteristics. The fact that a physical attribute is used for unlocking a device changes how it should be treated under law, the argument goes.
“When you put your fingerprint on the phone, you’re actually communicating something,” Albert Gidari, the director of privacy at Stanford University’s Center for Internet and Society, told me last year. “You’re saying, ‘Hi, it’s me. Please open up.’” That communication should be protected under the Fifth Amendment, just like a password, he said—and the same would hold for any other way of unlocking your phone using physical characteristics, including facial recognition.
Whether law enforcement could legally get someone to unlock their phone with their face will remain an open question until it’s been litigated. Presumably, this uncertainty wouldn’t be the deciding factor for most people buying the new phone, especially those who are already comfortable using their fingerprints to unlock their devices. (It’s far more likely the deciding factor would be the phone’s nearly four-digit price tag.)
In fact, the most meaningful changes in Apple’s digital security won’t arrive with its new flagship phone, but with iOS 11, the company’s new mobile operating system, which will be available for free next week on newer iPhone models.
With this upgrade, users will be able to click the power button five times fast to enter an emergency mode, with options to make an SOS call or display a medical ID. It also disables the fingerprint reader, immediately requiring a password to get back into the phone. (In the past, disabling Touch ID required restarting the phone, or waiting 48 hours.) It’s not clear yet whether emergency mode will also disable Face ID, but it seems likely that it would. And since constitutional protection for passwords is set in stone, those five clicks would quickly move an iPhone user out of the gray area of biometrics.
A second change, which was discovered last week by ElcomSoft, a Russian cybersecurity company, adds a simple step when a person connects their device to a computer. Before, the phone only required a user to unlock it—which could be done with a fingerprint scan—and then tap “trust” to establish a connection. Now, the process requires the user to enter the phone’s PIN or passcode, again avoiding the legal gray area.
This extra layer of protection may be especially useful at the U.S. border, writes Nicholas Weaver, a computer-security researcher at the International Computer Science Institute in Berkeley, in Lawfare. Taking advantage of a broad exception at the border to the Fourth Amendment, which protects against warrantless searches, border agents regularly look through passengers’ devices as they enter or even exit the country. Often, they copy the entire contents of a device to a computer using forensic software, where they can scan it thoroughly for keywords or contacts.
But at the border, the Fifth Amendment also isn’t always honored the way it would be elsewhere. Agents have shown that they’re more than willing to ask passengers for their phone PINs, which would nullify the protection of iOS 11’s extra password prompt.


Look, it’s true: People gather in Apple Stores. Kids play games. Design nerds fondle iPad Pros. Befuddled people seek refuge at the Genius Bar.
The stores have good vibes. Everything is clean. There are no sounds of commerce. No clanging till. No specials on an aisle. No mechanical belt sliding products toward a beeping scanner. People will tell you they like your new shoes. I love Apple Stores.
But there is one problem with calling an Apple Store an Apple “Town Square”—which the company announced it’s now doing at Tuesday’s iPhone event. Namely, the Apple Store is a store and not a town square.
Furthermore, they are truly explicitly not a public space. They are the opposite of a public space.
Since the 19th century, stores have served as gathering places for people. American department stores had cafés and gardens and all sorts of places for people to hang out. But they would have never had the audacity to confuse themselves with town squares. The nice stuff was just a way of bringing customers to the store to purchase goods.
And most surreally, a dominant problem for democracy at this moment is that truly public space doesn’t exist on the internet you access through your phone.
Internet platforms, as John Herrman has argued, merely masquerade as democratic spaces. But they are not. They are private, as private as an Apple Store.
In adopting the faux democratic language of Facebook and Twitter, Apple has made the perfect physical metaphor for the largely ineffable problem the internet poses to democracy.
Maybe that will make people realize how absurd it is to expect fundamentally commercial entities to build community or to serve liberal democracy or to make your voice heard or to act as an agora or whatever else.
These are businesses. They sell stuff. People buy it. That’s great.
Bringing these democratic ideas inside private enterprises seems nice, but it warps the very idea of “the public.” Who is excluded from the Apple Town Square that should have equal access to the soapbox?


For two decades now, Apple has been fighting a battle between attention and disregard. In 1997, when Steve Jobs returned as interim CEO, the company was a struggling maker of personal computers with limited market share. Then came the iMac, a Mac computer people finally wanted to own again. Then the iPod, which transformed the company into a maker of high-design personal electronics and accessories. Then, of course, the iPhone, which made Apple the most valuable company in the world—and changed forever the way people live, work, and play.
Apple’s latest plans, announced on Tuesday at its Cupertino headquarters in a new theater bearing Jobs’s name, suggest that the company has entered a new phase. The iPhone has become so popular that it’s almost hard to notice, like the air people breathe. So now Apple has a new job: to make the iPhone just as important when it’s old and banal as it was when new and extraordinary.
* * *
As Apple’s success has magnified, so have expectations for the company. Investors want assurances of continued growth, which has become a concern as the smartphone market has matured. Now everyone has one, and sales growth is stagnating.
For years, both the market and the public have been waiting on Apple’s response. The assumption has been that this would come in the form of a new product (or products). The iPad was the first candidate. Then the Apple Watch. Both have been successful, but not nearly as successful as the iPhone. Expectations have remained transfixed on new, more dramatic future products, among them the mythical Apple Car, whose future is uncertain.
This approach—lining up one new, killer product after another—seems almost impossible, even for Apple. But the company’s latest announcement points toward a new way of culturing attention, one that’s much more subtle than just getting people to buy or rent a glass rectangle year after year.
Attention is a strange thing. It’s often thought that the way to retain power or influence is to hold onto people’s attention—to keep it active, front and center. That’s how iPhone rose to prominence, after all: by ripping a hole in popular understanding of mobile telephony and introducing a totally new paradigm.
But over time, active attention recedes into the background. It has to. Extraordinary events, products, and ideas cannot survive as wild curiosities. They must be made ordinary. Such is the fate of every influential media form, from the electric light to the automobile to the refrigerator to the television to the smartphone.
Media’s true power comes from this habituation. When everyone relies on electricity. When everyone unloads a dishwasher. When everyone commutes by personal automobile. When everyone connects and reads and works and plays on a smartphone.
* * *
Apple’s announcement revealed two new approaches to manage its fall into habituation. One is technical and one is social.
The first approach changes the way an iPhone turns on. Apple’s new, flagship smartphone, the iPhone X, has an OLED display from bezel to bezel, supplanting the home button from the device’s front. To replace TouchID, the fingerprint sensor that provided security for device use and payment, Apple has introduced a new facial recognition technology called Face ID.
Its technical implementation is impressive, using a front-facing camera and dot projector to map and model an iPhone user’s facial features. Apple claims that it's secure and reliable, with one in a million odds of being cracked.
But to invoke Face ID to unlock a phone, the user must look at the device, so that the phone can see its owner’s eyes. This has a convenient side effect: People must pay deliberate attention to their iPhones again. No longer can one fail to notice the fact of the iPhone as a device mediating life in the background, even if it is about to be put to that use. First, the user must acknowledge it in the foreground. iPhone, see me. Recognize me, as I recognize you doing so.
It’s a small thing—a setup, perhaps, for something yet to come. But temporarily, at least, it reactivates the iPhone as a thing that demands and receives active, rather than just passive, attention.
The second approach is far stranger. Angela Ahrendts, Apple’s senior vice president of retail, announced that the company would rebrand its retail locations. Instead of stores, they would become “town squares, because they are gathering places.” She’s creating “plazas” in Apple’s largest stores, adding “boardrooms” for entrepreneurs, and recasting aisles as “avenues,” which are “like shop windows around a town square.” Apple’s Fifth Avenue store in Manhattan will become such a town square, as will a new one in the Carnegie Library in Washington D.C.—a controversial use of an architectural landmark that once served as a real public sphere rather than one remixed out via capitalism.
Whether successful or not, Apple’s town-square plan acknowledges that the company’s devices have had a substantial impact on the public sphere—and not always a good one. Access to information, people, services, and ideas now route frequently—sometimes exclusively—through rectangles. We know that the apps and services that provide that material, including Twitter and Facebook, have had a profound and disturbing effect on political reality.
Apple’s new, if deranged, interest in civics appears to respond to this reality. Apple town squares make physical and tangible a reality otherwise left immaterial: that screens have turned public spaces into private ones, and that those private spaces rumble with quiet threat.
* * *
In Tuesday’s address, Apple’s CEO, Tim Cook, said something so trite as to pass by without notice. “It’s truly amazing how much iPhone impacts the world each and every day,” he noted as he prepared to introduce the new iPhone models. “Our intention with iPhone has always been to create something so powerful, so immersive, and so magical that the hardware virtually disappears.”
For years, lines like this were understood as matters of usability. The Macintosh was a personal computer so intuitive anyone could use it. Later, the iPod and iPhone followed suit. But Cook is also talking about another kind of disappearance: the vanishing act that media perform when they become mature, mass media.
The future of Apple isn’t in tablets or watches or even cars. It’s in how well, or how poorly, it manages global life run by its smartphones. And how willingly the public lets it.


The iPhone is the single most successful consumer product of all time. It’s generated $762 billion of revenue for Apple, the most valuable company listed on the American stock exchanges. It has made Apple, more or less, the iPhone company.
And the model that the iPhone established—phone-on-glass, apps-on-phone—is “eating the world.” All of which which makes Tuesday’s upcoming announcement of the next generation of the phone an important event for the technology industry, generally.
But it’s an even bigger deal for Apple at this particular moment. Looking back at all the quarters since the company launched the iPhone in the fall of 2007, it is clear that this set of phones, this announcement, will determine if Apple can return to major unit and revenue growth or if Apple’s tremendous run building an already huge and profitable user base is over.
With the benefit of hindsight, it’s possible to discern several clear quantitative periods for the iPhone business. Here they are in a chart of the number of iPhone units sold each quarter since launch (note that Apple, like many companies, begins its fiscal year in October, so the extremely important holiday season falls into Q1):
First, there was the launch phase, in which Apple was selling just single-digit millions of phones, but its growth rates were phenomenal. Let’s call this The Launch Hyper Growth phase. It lasted from the third quarter of 2007 until the first quarter of 2010. Comparing quarters with the year prior (e.g. Q3 2009 vs. Q3 2008), the company posted annual growth of between 470 percent and 8,280 percent, despite supply problems.
Then, for three full years, Apple entered the The Great Scaling epoch. During this time, some notable things happened; Steve Jobs died in what Apple would call the first quarter of 2012. But nothing stopped the upward movement of the iPhone. Apple went from selling 8 to 9 million phones each quarter to selling between 26 and 37 million phones. The average year-on-year revenue growth through this period was 87.3 percent.
Next was the blowout first quarter of fiscal year 2013. Apple sold 47.79 million iPhones. And for two years, the company struggled to show the kind of growth in revenue or unit sales that it had in the previous three years. They were getting huge, and it made sense to many industry analysts that the “law of large numbers” would drop their growth rates considerably. It was The First Plateau. For these two years, Apple’s iPhone revenue averaged a year-on-year increase of a much more modest 14.1 percent.
Then came 2015. The Annus Mirabilis, or miraculous year. It started with selling nearly 75 million units in the first quarter and continued with huge unit and revenue leaps over the previous year. After many quarters of slowing growth, Apple blew the doors out with its new large-screen phones and they almost quadrupled their average year-on-year quarterly revenue growth to 51.7 percent.
In context, it has to be seen as one of the greatest corporate performances the world has ever seen. It was truly shocking.
Now, there’s the fiscal year 2016 and 2017 so far. Having posted such enormous numbers in The Annus Mirabilis, Apple has found itself unable to show growth without some exciting new species of phone. In the second quarter of 2016, revenue and unit sales fell on a year-over-year basis for the first time. Across the board, the average unit and revenue growth fell by 4.4 percent and 6.2 percent, respectively.
If you’re looking at the chart above and wondering how anyone could be worried about Apple, take a look at the chart below showing revenue growth rate.
Now the question is: As we roll the clock forward, will The No-Growth Era become simply The Second Plateau, or has Apple reached the edge of what’s possible for its high-price, high-margin mobile phone?
It’s all riding on this next generation of iPhones. Which could make Tuesday’s keynote/marketing speech a much more interesting affair than the last several Apple events.


Hurricane Irma slammed the west coast of Florida on Sunday, making landfall first in the Keys and then at Marco Island, 15 miles south of Naples. Since then, it’s been making its way northward, visiting destruction on the state as it weakens.
As the storm progressed through Florida, it knocked out the lights all over the state. In a press conference Monday morning, Eric Silagy, the president of the state’s largest electric utility, Florida Power and Light, estimated that more than half the state is without power. That’s more than 10 million people, which dwarfs the number who lost electricity during Hurricane Sandy, which had been the record holder for hurricane-related power problems with 6.2 million affected.
Florida Power and Light is the nation’s third-largest utility and provides power to 4.9 million homes and businesses. Early Monday morning, 4.4 million of those customers had lost power, some multiple times, as the utility restored service and then it was knocked out again. “We’ve had over 5 million outages across our territory. That is unprecedented,” Silagy said. “We’ve never had that many outages. I don’t think any utility across the country has. It is, by far, the largest in the history of our company.”
Already, the company has restored 1 million connections, though some only temporarily.
On Sunday, the utility’s VP of communications, Rob Gould, told ABC that residents on the east coast could expect a standard post-storm restoration timeline, but that the west coast’s electrical grid would need a “wholesale rebuild.”
“This is going to be a very, very lengthy restoration, arguably the longest and most complex in U.S. history,” Gould said.
That task will begin very soon. The company plans to have 16,000 people, including thousands from other utilities, working out on the lines.
The restoration of power to western Florida will be a test of the resilience of Florida Light and Power’s vaunted smart-grid infrastructure. The utility says it has invested over $3 billion in making its grid “stronger, smarter, and more storm-resilient.”
It was standing with FPL’s CEO that President Obama announced $3.4 billion in smart-grid grants through the Department of Energy as part of the stimulus package, and when the utility finished its smart-grid installation in 2013, it was lauded as smart-grid technology’s coming-of-age moment.
In other words, FPL’s grid was about the best the country could have brought to the table. And now, apparently, Irma has laid waste to at least a large chunk of that system.
What could a “wholesale rebuild” mean?
An electric grid is a complex technical system. There are power plants that feed electricity onto the grid and there are consumer loads that take the power. In between, there is connective tissue that allows for long-distance transmission of power as well as for the local distribution of that electricity: high-voltage transmission lines, substations, transformers, and regular local power lines.
Generally speaking, power outages usually happen toward the edges of the network, when local power lines get snapped or their poles are felled.
FPL’s statement indicates that the west-coast grid has sustained damage beyond the standard downed power lines. Pieces of the system’s core have been compromised. However, given how early they are in the effort to bring power back, they haven’t had a chance to inspect all their facilities.
“We haven’t seen structural damage,” the utility’s president Silagy said Monday morning, “but I am sure we will see some.”
This happened to several utilities during Hurricane Sandy in 2013—which, until today, had caused more people to lose power than any other in history. In a deep postmortem, Greentech Media detailed some of the carnage inflicted on one utility, Public Service Enterprise Group. Sandy had damaged 16 substations, one-third of its transmission circuits, multiple power plants, and thousands of distribution lines and poles. Two million of their 2.2 million customers lost service.
By November 3, a few days after Sandy struck, 65 percent of PSEG’s customers had power. A week later, 98 percent of service had been restored.
Given that the aftermath of Sandy is one of the parables of the electric-utility industry, it’s fair to say that Gould means that the restoration project on the western coast of Florida will take longer and be harder, despite FPL’s state-of-the-art grid.
FPL operates 6,926 miles of transmission lines and 605 transmission, subtransmission, and distribution substations. Major ones are shown on the map below, which highlights that most of FPL’s infrastructure is on the eastern seaboard. The red lines are key power-import links, which run into Georgia and tap the Southern Company’s power plants. Completed in 1982, they substantially upgraded peninsular’s Florida’s electrical reliability.
This Homeland Security map of substations shows the relative density of electrical infrastructure within the bottom half of the state.
FPL’s storm preparations are a matter of the public record. Each year, they submit a report to the Florida Public Service Commission detailing their distribution reliability and storm preparedness. Two things are clear from reading this document: First, FPL’s overall reliability was excellent both in absolute terms and relative to other utilities in the state. Second, the utility learned from the northeastern utilities’ experience with Sandy. For example, it installed flood monitors in 223 substations, which are supposed to help protect critical components, though how well they worked during Irma is unknown. They’ve also been investing hundreds of millions a year into hardening the system near key locations like police, fire, and sewage stations. And they established an initiative to coordinate more closely with local governments, an area that became a flashpoint after Sandy.
As the storm continues northward and the restoration effort begins in earnest, FPL’s ability to bounce back will become a test of how resilient the smart-grid and post-Sandy systems really are.
Despite the huge numbers of people who have lost power, the early results have portend a faster restoration of service. Silagy, in particular, noted that the substation flood monitors seem to have performed well.
“Frankly those flood monitors [at substations] saved 3 or 4 days of work and millions of dollars worth of equipment that would have had to be be replaced rather than simply re-energized,” the FPL president told reporters at the Monday press conference.


The U.S. oil industry pumps more than 3 billion barrels of crude per year. Oil crosses continents in pipelines like the Keystone, which moves 1.3 million barrels per day. It travels between them on tanker ships, the largest of which can carry 3.7 million barrels. When oil leaks, the disaster is quantified in barrels spilled—more than 250,000 from the Exxon Valdez, and at least 3 million from Deepwater Horizon. When oil sells, it is priced per barrel, and when it burns, its energy output is measured in “barrel-of-oil equivalents” (5.8 × 106 BTUs). The world of oil is a world of barrels.
And yet less and less of the oil trade requires actual barrels. In the movies they make good historical set pieces and symbols of future apocalypse. But there aren’t any barrels in the Dakota Access Pipeline. No barrels rolled off the Exxon Valdez. And the oil that spewed from Deepwater Horizon never had a chance to reach a barrel in the first place.
So why do people still talk about barrels when they talk about oil? Because the oil barrel became a concept rather than a physical thing.
* * *
The oil barrel almost didn’t survive the 19th century. Oil companies hated them: Barrels were leaky, costly, and cumbersome. But they were also necessary. When the first Pennsylvania wells began gushing in the late 1850s, prospectors scrambled to catch the erupting crude. Any container would do—whiskey or ale jugs, salt or turpentine vats. The best option was an old one: the casks still used today to age wines and whiskies. They dated to the Romans and Celts, who designed them to replace clay pots for moving wine and olive oil. In the first Pennsylvania oil fields, demand for those barrels rose so quickly that at times their price exceeded the value of oil itself.
The first oil barrels held between 31.5 and 45 gallons, but Pennsylvania producers settled on a common standard by the late 1860s. They based their new system on another old-world model. In 1482, King Edward IV had moved to eliminate shady dealing in the English herring industry by imposing a 42-gallon standard on shipping containers. Oil companies promised similar market consistency with an added bonus. They would sell oil in 40-gallon units, but buyers would also get “an allowance of two gallons” as a measure of good faith. The measurement stuck.
The barrel requirement was hard to solve. John D. Rockefeller fought the battle on multiple fronts. If Standard Oil had to use barrels, it would do it on the cheap. Barrel manufacturing became part of the oil monopoly—one tentacle of Rockefeller’s octopoid oil empire, as an editorial cartoon of the era cast his business. To cut prices, Standard cut trees, and acres of oaks became stacks of barrels. By the 20th century, Standard had developed steel containers that eliminated the need for trees (and Rockefeller made another fortune selling iron ore to steel plants). But the basic problems remained: The barrels were still hard to move, and Standard’s mass-produced steel ones had worse seals, meaning more leaks.
Standard and its competitors wanted to eliminate the barrels altogether. They developed railway tanks to replace barrel-filled boxcars and sent horse-drawn kegs to distribute kerosene to local retailers. Water transport was desirable but inefficient, because the barrels were so heavy (and leaky). But in the early 1870s, small tanker boats made shipment by river possible without barrels. By the end of the decade, Ludvig Nobel (brother of Alfred, the Nobel Prize founder) had developed a tanker called the Zoroaster to ship Russian oil across the Caspian Sea. And in the 1890s, similar ships sent Standard Oil all around the world.
These years also witnessed the emergence of the pipeline system. If oil could get from the well to the refinery in a single container, the barrels and the teamsters who moved them could be eliminated. From the earliest days of drilling, short pipelines built using wooden ducts and gravity offered one approach—at least if they didn’t leak, which they usually did. Soon, iron pipes and pumps made longer lines viable. In 1865, the first commercial line ran five miles from Pithole City, Pennsylvania, to the railroad terminal in Oil Creek. In 1881, Standard connected the Pennsylvania oil fields to New York City, and over the next decade it created similar lines to link fields in Ohio with refineries in Chicago. By the 1890s, longer lines and larger pipes linked new fields from Oklahoma and the Gulf Coast to Illinois. The Keystone XL and Dakota Access Pipeline are just the latest versions of the same saga—the dream of an oil world without barrels.
* * *
The barrels finally changed in 1905, when Nellie Bly patented the steel drum still used today. Bly designed her barrels to carry more oil, 55 gallons, and to leak less. Her invention also caused the barrel’s meaning to separate from its physical reality. Despite adoption of the new, larger barrels, the old 42-gallon quantity remained the industry unit of measure for a “barrel”—as it still does. The barrels people used to hold oil were no longer the barrels they used to talk about it. A barrel became a quantity of oil futures sold, or crude spilled, or latent energy stored.
The discrepancy between the barrel’s physical reality and its meaning in industry parlance didn’t do anything to solve the barrel problem. Bly had improved the barrels, but no barrels at all would still have been better for the oil business. Even better would be symbolic barrels that existed in name only, used to measure production and sales, but hidden from sight. To present a good public face, the barrels that did exist were painted in corporate colors and logos. Like advertisements, promotional films, and glossy service stations, colorful containers like Standard’s “holy blue barrels,” as the muckraker Ida Tarbell called them, sold as pleasant and diverting an image as possible.
Talking about barrels that weren’t real made perfect sense. They were part of how the industry presented its contribution, measured in the number of barrels produced, to the economy, national security, and the everyday happiness of a nation of drivers. Could there ever be too many “barrels” in the Strategic Petroleum Reserve?
By the 1950s, the spread of pipelines, rail tanks, and tanker trucks meant that less and less oil needed to move in barrels. They found alternate use—in ports, for instance, where they transported raw materials and carried fuel for ships. But they also piled up in suburban junkyards as evidence of their looming obsolescence.
As waste objects, the barrels found new lives in the art world. As a young artist working in Paris, Christo Javacheff would drag them from the junkyard to his studio, where he learned to turn industrial objects into modern art by wrapping and stacking them. In 1962, less than a year after the construction of the Berlin Wall, he and his partner Jeanne-Claude used barrels to barricade Paris’s narrowest street. They called it the “Iron Curtain.” Their barrels—once icons of the global oil trade—became symbols of a world divided.
As objects, oil barrels have had many fates in the last half-century. Hobos and hippies turned the discarded ones into fireplaces. Trinidadian drummers turned them into steelpans. And a DIY manual for making your own barrel-bottom fire pit, trash can, planter, or kegerator is just clicks away.
For the oil industry, the 19th-century barrel problem has taken on new forms. In places like Nigeria, for instance, they support a thriving black market. Siphoned from leaky pipelines, this illicit oil moves in barrels again. Meanwhile scientists continue to search for better ways to move the stuff, perhaps even in a future without pipelines.
In authorized settings, the oil barrel continues to find uses in the industrial hinterlands. The last time I saw an oil barrel was in a Tuscan marble mine. I was touring the quarries in Carrara, home of the famed white marble that gave us Michelangelo’s “David”—along with countless kitchen countertops and bank facades. Near the end of the tour, something caught my eye. Tucked off in a corner was a pile of oil barrels, dented and dirty but with the brand names still visible. It was one of those unusual places where oil barrels remain more than just words.
The oil barrel tells a story of struggle between what industries need and what they want. Oil companies needed barrels, but they didn’t want them. They were dirty, wasteful objects. When the industry got rid of them, it became a little less dirty, and a little less wasteful. But mostly it became easier to trade the mess and the waste for the image the industry wanted: a world of energy, vitality, and security.
This article appears courtesy of Object Lessons.


Modern cargo ships are huge, slow machines that ply the world’s oceans, delivering fuel, raw materials, and products to power the economy.
Sailing the seas is dangerous. Thanks to safer ships and weather forecasting, it’s not as dangerous as it used to be, but rough seas can sink a modern ship, as we saw when the freighter El Faro sank during an Atlantic storm in October 2015, killing all 33 seafarers aboard.
This September, Irma has been beating its way across the Atlantic Ocean toward the edge of the Caribbean Sea. And cargo ships have been powering away from a broad swath of ocean to avoid the storm. In this case, many ships are seeking shelter on the west side of the islands that mark the eastern boundary of the Caribbean. Their specific routes are chosen by captains with the aid of company headquarters, usually relying on specialized forecasters who work with shippers.
MarineTraffic is a service that tracks the movement of ships using Automatic Identification System (AIS) beacons on board. They sent me images of ship positions from early on September 4th through September 5th. I was able to roughly match those frames with wind-track data provided by the National Hurricane Center, so you can see what it is that the ships are avoiding.
In the animation below, the maroon represents the route that hurricane-speed winds took. The yellow is the envelope of tropical-storm-level winds.
The ship colors represent different types of vessels: Tankers are red, cargo ships green. Pleasure craft are pink.
Already, ships are returning to routes that are behind the storm and clearing out of the areas that remain in the path. This map shows MarineTraffic data overlaid with a roughly contemporaneous image of Irma from the GOES satellite on Thursday.
As you can see, many ships are still hiding from the storm behind the islands, waiting for it to pass by on its way to making landfall in Florida.


Consumer data breaches have become so frequent, the anger and worry once associated with them has turned to apathy. So when Equifax revealed late Thursday that a breach exposed personal data, including social-security numbers, for 143 million Americans, public shock was diluted by resignation.
There are reasons for the increased prevalence and severity of these breaches. More data is being collected and stored, for one, as more people use more connected services. Corporate cybersecurity policy is lax, for another, and sensitive data isn’t sufficiently protected. Websites and apps, which are demanded by consumers as much as they serve the interests of corporations, expose paths to data that should be better firewalled. Software development has become easy and popular, making security an afterthought, and software engineering has failed to adopt the attitude of civil service that might treat security as a first-order design problem. And hacking and data theft have risen in popularity and benefit, both as an illicit business affair and as a new kind of cold warfare.
People have started to experience data loss and theft in a new way. Breaches have settled into a kind of modern malaise, akin to traffic or errands. They are so frequent and so massive that the whole process has become a routine.
Online data, like usernames and passwords, have been leaked and hacked with such frequency and in such great quantities (a hacker stole more than a billion Yahoo! email accounts in 2013), that savvy people treat their credentials as violated in advance. Breaches of more sensitive data, like bank, social-security, address, and health or employment records, have also become common. Home Depot, Target, Sony, Anthem, the U.S. Office of Personnel Management, and other recent violations felt shocking and violating at first, but over time that sensation has waned. With over half of the entire U.S. adult population potentially exposed by the Equifax breach, what’s left to do but shrug and sigh? I’ve got so many stacked-up subscriptions to credit-monitoring services from previous consumer breaches, adding another one would be superfluous.
Most organizations affected by hacks and leaks have treated the matter with great seriousness and care, understanding that their reputations were on the line. But whether intentionally or not, Equifax appears to have leaned into the new malaise, treating this massive breach with the bureaucratic apathy one might expect from a big, faceless credit-reporting agency—a company everyone must use, but no one chooses to.
The announcement of the breach, which came after hours on Thursday, offered the first sign of indifference. Media outlets, including The Atlantic, rushed to cover the matter, but details were slim. When my colleague Gillian White contacted them, Equifax offered no further comment beyond the materials they had published on an informational website. Other outlets experienced similar silence.
Those websites confused the matter more than they clarified it. The company had launched a new domain, equifaxsecurity2017.com, to communicate about the breach. That site appeared, to some users, like a phishing effort. Given the option to assuage concern, why set up a new domain that would only instill more of it? Once inside, this sensation only amplified. The site offers a tool to “determine if your personal information may have been impacted by this incident,” but accessing it requires submitting a last name and the last six digits of a social-security number. That’s a lot of data to hand over to anyone, especially an organization that has just demonstrated that it cannot be trusted with it.
Once submitted, the website either confirms no impact, or it offers an ambiguous response, inviting the supposedly impacted person to sign up for credit-monitoring services from TrustedID Premier, an Equifax subsidiary. Even that task cannot be performed immediately; the user is presented with a date on which the process can continue. The website also warns that no further notice will be provided to the user. It recommends marking your calendar. Even those who were not affected, according to Equifax’s confusing tool, are invited to sign up for TrustedID, making the whole affair feel like a grotesque marketing campaign.
In press coverage and on social media, some have speculated that submission of the personal information requires the individual to agree to Equifax terms of service that mandate arbitration in the case of dispute. If true, such an agreement would prohibit affected parties from suing Equifax, including via class-action lawsuits. But even this ambiguity seems unclear. TrustedID Premier’s terms of use do require agreeing to arbitration to use the service, but TrustedID’s services are separate from Equifax’s. The terms page itself is identical to the one that appears on TrustedID’s stand-alone website, although it was updated the day before the breach was made public, suggesting that the company buttoned up in anticipation.
Ultimately, not only is it unclear if one must agree to arbitration for access to the free credit-monitoring services—it’s also uncertain if consumers even learn the fact and details of their breached data without signing up for TrustedID, with or without agreeing to arbitration with Equifax. The whole affair is permeated with unknowable rules, some of which feel like traps.
In the end, the truth of the Equifax breach—who was affected, and how, and what the company will do to help, and what the terms of such assistance entail—might not be the most important lesson from this incident. More than anything, it suggests that a corner has been turned in corporate consumer data responsibility. Like severe weather, breaches have become so frequent and severe that they can begin receding from prominence. No matter their grievous effects, Equifax’s response suggests that fatalism might replace responsibility, planning, and foresight. This is just what happens now.


Updated at 8:15 p.m.
On Thursday, Equifax, one of three major credit reporting agencies, revealed that highly sensitive personal and financial information for around 143 million U.S. consumers was compromised in a cybersecurity breach that began in late spring. There are only around 125 million households in the U.S.
Two Major Credit Reporting Agencies Have Been Lying to Consumers
According to the company’s statement, the cybersecurity breach started in May of this year and continued until it was discovered on July 29. While criminals did not appear to have accessed what Equifax describes as “core consumer or commercial credit reporting databases,” which help in the generating of credit scores, some pretty important personal information was accessed. According to the company, criminals were able to access the social security numbers, birth dates, and addresses for a massive—but as yet unspecified—number of U.S. consumers. The hack also included credit card numbers for more than 200,000 Americans and documentation related to disputes, which contain personal and identifying information, for some 180,000 Americans. On top of that, financial disclosures show that three top Equifax executives sold $1.8 million worth of company stock in the days after the breach was discovered, according to Bloomberg.
"This is clearly a disappointing event for our company, and one that strikes at the heart of who we are and what we do. I apologize to consumers and our business customers for the concern and frustration this causes," said the company’s CEO Richard F. Smith in a statement. Equifax declined to comment further.
As Sarah Jeong has written before for The Atlantic, new technologies have resurfaced old problems related to the collection—and protection—of financial data. The circumstances in present times are reminiscent of the ones that precipitated the creation of the Fair Credit Reporting Act of 1970, she explains. But even with rules in place about how to separate and collect financial data from individuals, the transition to digital has brought those problems back, and they haven’t yet been satisfactorily addressed. Add to that the ongoing challenge of securing important information online—one that just about every organization faces—and the ability to harm the public in the course of normal operations for businesses built to collect and create crucial, personal, highly sensitive data becomes enormous.
This breach comes on the heels of a recent finding by the Consumer Financial Protection Bureau, the government agency responsible for monitoring and regulating the financial industry, that Equifax had been deceiving American consumers, signing them up for costly products without their knowledge, misrepresenting credit scores, and violating the Fair Credit Reporting Act. At the start of 2017, Equifax, along with another credit reporting agency, Transunion, were ordered to pay $23 million in fines and restitution by the CFPB. The credit-reporting industry is controlled largely by three companies: Equifax, Experian, and Transunion. Their culling and dissemination of financial data is what allows—or prevents—people from being able to buy or rent houses, get auto loans, have credit cards, and a host of other everyday necessities.
The Equifax breach, in its size, duration, and scope, is more than an unfortunate mishap. Part of the tragedy in all of this is that, those whose information has been compromised never asked to have their information collected in the first place—all major credit reporting agencies receive data directly from a host of financial companies, such as banks and credit card companies, in order to build credit reports. For Americans who want to protect their personal financial information, there is no way, in our current system, to do so.


“Toxicity is a relative thing.” So Arkema executive Richard Rennard described the noxious fumes emanating from a plant that had been flooded by Hurricane Harvey last week. Locals had claimed that the plant, which is dangerously close to residential areas, had caught fire—with some containers possibly exploding—and sent potentially poisonous chemicals across the area. Rennard and other Arkema officials vehemently denied those claims, claiming that the “pops” residents heard were not explosions, and that the chemical fumes leaking from the plant were “noxious,” but not necessarily poisonous.
A new lawsuit filed in a Harris County district court not only directly contradicts those claims from Arkema, but paints a much more harrowing picture of the facility’s meltdown following the flood. The suit alleges that a series of explosions on August 31 spread dangerous fumes to a perimeter 1.5 miles around the plant, where it incapacitated police officers charged with maintaining that perimeter, and then even overwhelmed medical professionals responding to their calls. The suit—filed by some of those first responders who say they were made ill by the fumes—also alleges that a series of negligent decisions by Arkema and operators at the plant led directly to those explosions, and to planned explosions on September 3 that it claims spread contaminated material into the surrounding neighborhoods.
The Crosby, Texas, facility, which French-owned Arkema operates, manufactured organic peroxides for the creation of plastics. Hurricane Harvey, which made landfall in Texas on August 25 and stalled over the gulf region of the state for the week after, dumped up to 40 inches of rain on the Crosby plant. Its main power source went out on August 27, and backup generators failed on August 28. Since organic peroxides need refrigeration to avoid ignition, Arkema feared explosions were imminent. On August 30, concerned about such explosions, the Harris County fire marshal evacuated people within that 1.5 mile perimeter, citing “a potential for a chemical reaction leading to a fire at the facility, which could produce a large amount of black smoke.”
The lawsuit alleges that at least some of that buildup was avoidable. The plaintiffs cite the frequency of flooding in Houston—which has seen disastrous floods in each of the past three years—as a reason for Arkema officials to have been better prepared for the flood. According to the plaintiffs, Arkema “never heeded the warnings and ignored the foreseeable consequences of failing to prepare.” Those preparations might have included the adequate provision of backup refrigeration services in the case of loss of power, and the evacuation of critical personnel.
On the day of the fires, the plaintiffs allege that first responders were required to secure the perimeter and wait for the “inevitable” explosions that would come. According to the suit, the fumes immediately sickened police officers, then sickened emergency medical personnel who were called to tend to them, and left several individuals hospitalized. Further, the suit claims that those responders had been put in the position to fall ill—and suffer potential unknown future health issues—because of the misrepresentations of Arkema executives, who have maintained that the substances leaking into the air and exploding were not toxic.
Just days after the last planned explosions at the Crosby facility, residents near the plant began moving back into their homes. But the lawsuit by the first responders has some implications for them as well. Residents have been wary of Arkema’s assurances that the plants fumes weren’t anything more than unpleasant, and of claims that other forms of chemical contamination hadn’t affected their homes. If during discovery—during which the plaintiffs have sought to obtain a restraining order on Arkema from tampering with evidence—and the course of the trial, the plaintiffs find that the chemicals first responders were exposed to were more dangerous than advertised, and that negligence on behalf of the company led to bodily harm, then residents of homes within and even outside the explosion may have similar concerns about exposure.
The suit, in which plaintiffs are seeking damages over $1 million, is one of a few proceedings that will probe the nature and genesis of the Crosby plant explosion. In a statement on August 31, the U.S. Chemical Safety Board announced it would launch an investigation into the Arkema processes and protocols as soon as “the emergency response activities have been completed and the facility is deemed safe for entry.”
Additionally, the Environmental Protection Agency, which apparently hasn’t inspected the Arkema facility in 14 years, is working with the Texas Commission on Environmental Quality to monitor the site. On September 6, TCEQ announced “an open investigation into the Arkema incident that will include an evaluation of any impacts due to the fires at the site.”


On my final night on Powder Mountain in Eden, Utah, I joined a yoga class in a window-walled lodge with dream catchers dangling from the rafters. The class’s attendees were blissed out from spending several days in late February skiing. As we stretched, a man wearing gray athleisure gave us all high fives before unfurling his own mat. It was unclear what we were being congratulated for—perhaps our luck. Whatever our stories, we’d landed here, in the snow globe that is the Wasatch Mountain Range.
Lying on my back, I could see the area’s surrounding mountains and the lights inside a dense thicket of town house–style condos—all that seemed to stand between the lodge and Paradise. Literally, Eden is a 58-mile drive south of the town of Paradise, Utah, making Powder Mountain some kind of mecca.
Since the 1970s, the mountain’s pilgrims have been skiers, drawn to its unpretentiousness and unbeatable conditions. Today, it’s also a Zion for a different kind of seeker. Construction began this summer on a public mountain town that will straddle a 10,000-acre site between three skiing bowls. In 2013, Powder Mountain was purchased by Summit, a company—or, perhaps more accurately, a collective—founded in 2008 by five 20-something friends who want to “catalyze entrepreneurship” and “create global change.”
With $40 million raised from its “members”—mainly tech entrepreneurs recruited through days-long events on the mountain, on cruise ships, or, this fall, in downtown Los Angeles—Summit plans to build 500 single-family houses, a village for amenities, and a home for the organization’s nonprofit arm, Summit Institute. By 2022, Summit says much of the village will be operational and occupied, with full build-out occurring over the next 20 years.
Despite having some of the trappings of an exclusive retreat for start-ups, Summit Powder Mountain is not a gated community or a resort. Its founders say they are creating a year-round community for innovators, thought leaders, artists, scientists, and others to solve the world’s most pressing challenges, from environmental catastrophe to access to basic medical care.
They plan to do this, in large part, by physical design. Mapped out over years by a coterie of prestigious architects and planners, Summit adheres to a logical grid and strict aesthetic guidelines specifically meant to avoid a dissolution into a ski resort of McMansions and Gucci outposts. If you attract the right people, the founders envision, solutions to global problems will come on a shared chairlift ride up the mountain, or during a fireside chat between strangers.
While Summit doesn’t like to associate itself with the word “utopia,” its turn to architecture to reflect and contain its ideals puts it squarely in line with the hundred or so utopian societies, secular and religious, that were founded in the United States in the 19th century, and also the hippie communes of the 1960s and ’70s. Many of these societies failed because of change—in religion, politics, technology, war. But when they did succeed—as Summit hopes to—it often had a lot to do with how successfully their physical world matched their spiritual and intellectual goals.
* * *
I flew out to Powder Mountain during one of Summit’s weekend retreats, where 150 or so attendees paid $1,500 for skiing, farm-to-table meals, guest speakers, and entertainment. The retreats are aimed mostly at entrepreneurs, though the organization says anyone “kind and open-hearted” and “doing innovative work regardless of your discipline” can attend.
That’s a tricky line to toe when also offering a rarefied experience. At a communal dinner on my first night, guests gathered at one of the mountain’s original ’70s-era lodges. The banquet tables were set with tea lights, and we shared braised beef from a cow that had grazed the mountain over the previous summer. A spoken-word poet asked everyone to shout our names at the same time.
Later, while most people were skiing, I traveled between three lodges on the mountain, one of which is a yurt-like structure called Sky Lodge on the very top. Whiteout conditions prevented me from looking out over the deck and better envisioning Summit’s snaking site plan, so I worked on my laptop instead, sipping something called Saffron Elixir and listening to the deaf-blind advocate Haben Girma discuss her disability-rights work. When lunch was served, Summiters clambered in on their boots. “This is next-level, man,” I overheard one of them say. (Summit has values—and it has perks.)
One neighbor, as Summit calls future residents of the town, is Bryan Meehan, the CEO of Blue Bottle Coffee. “There’s a lot of amazing progress happening in California in technology,” says Meehan, who lives in Marin, “but sometimes we don’t pay attention to the impacts of progress socially and environmentally. The conversation can be too focused on how beautiful this bottle of wine is, or ‘have you seen my new Tesla?’” At Summit, the discussions might be about poetry or regenerative agriculture. Pass out your business card and you’ll get the cold shoulder from the Summit community.
Christiana Moss, a cofounder of an architecture firm involved in design and planning on the mountain, refers to Summit’s founders as “compassionate capitalists”—businessmen with a heart, who want Powder Mountain to seed new companies, philanthropic endeavors, and creativity. “You have young people with revolutionary ideas hanging out with people with a lot of money who can make their ideas happen,” Moss says of Summit’s design. If it works like its supporters hope it will, Powder Mountain will become a new center for thought leadership, and one that isn’t located on either of the coasts: a new American campus.
* * *
As Chris Jennings writes in Paradise Now: The Story of American Utopianism, the experimental communities that were born in the 19th century “intended to catalyze a global revolution by building a working prototype of the ideal society.” The Shakers built more than 20 communities in the United States and are known for their distinct architectural style, with its identical sets of doors, gates, and stairways—everything in perfect symmetry and broom-swept starkness. Others, like Brook Farm, inspired by transcendentalism and founded in the 1840s in West Roxbury, Massachusetts, attempted to construct a single building to house their entire community and bind it together spiritually.
In the 1960s and ’70s, rather than create communities meant to transform the world, a new wave of DIY counterculturists escaped it. They constructed camps like Drop City, established in 1965 by four art students and filmmakers who bought seven acres of land in southeastern Colorado and erected Buckminster Fuller–esque dome structures made of junkyard scraps. By 1977, Drop City had been abandoned, but not before a period of intense creativity ensued.
One of Summit’s inspirations is Aspen, Colorado, as the Aspen Institute’s founder Walter Paepcke originally conceived it. In the late 1940s, Paepcke began buying up property in the defunct mining town with the intent to make it not just a vacation retreat, but “a Kulturstaat, a civilized state organized around culture and thriving on it,” writes James Sloan Allen in The Romance of Commerce and Culture: Capitalism, Modernism, and the Chicago-Aspen Crusade for Cultural Reform. By the 1950s, Paepcke had succeeded in opening a ski resort, the Institute, the Aspen Music Festival and School, and the International Design Conference. But Greg Mauro, a venture capitalist who purchased Powder Mountain with the other founders, believes the opulence of Aspen of today would be anathema to Paepcke.
Sensitive to potential criticism that Summit Powder Mountain could become a wealth ghetto, the leadership is baking equal opportunity into the design of the town: subsidizing artists-in-residence, building cabins and hostels, adding low-income housing, offering fellowships to artists and entrepreneurs, and creating programming for the town of Eden’s 600 residents. It would be a fake town otherwise, says Mauro. As a managing partner of Learn Capital, a venture-capital firm focused on educational technology, he hopes to bring schools to the mountain, too, aiming to grow the permanent population one or two percentage points per year.
In part, Summit’s owners are banking on a 125-page book of design guidelines to discourage potential residents who would want to steer the town toward luxury. Private houses designed by architects such as Brian MacKay-Lyons (who, for years, created his own utopian experiment on his family’s farm, calling it the Ghost Architectural Laboratory) and Todd Saunders (whose work, like MacKay-Lyons’s, often feels like a dramatic extension of its surroundings) must adhere to the guidelines, which dictate what can and can’t be done on the mountain, down to a list of acceptable shrubs, houses capped at 4,500 square feet, and a “modern mountain” style.
* * *
No one at Summit mentioned Salt Lake City in any of our discussions, but the Mormon-founded city is only an hour’s drive south of Eden, and it’s an obvious example of urban planning and architecture as physical embodiments of ideals—and one that survives, in an evolved form, today. Just as Summit’s founders knew that Powder Mountain was their future home as soon as Mauro introduced them to it, Brigham Young supposedly declared “this is the place” about Salt Lake City in 1847, kicking off nearly a century of impressive architecture, industry, and growth for Mormons and non-Mormons alike.
Other utopias can look alluring on paper, but would have been dystopian in practice. Frank Lloyd Wright’s Broadacre City, an “anti-city” plan the architect introduced in 1932, laid out a gridded suburbia that isolated families within their own one-acre plots of land, and made the automobile central to survival. With no true center, Broadacre City would have siloed the programs that create a sense of community: schools, public transportation, retail, recreation.
Summit, too, has risks. The project has been criticized by some Eden residents who are wary about what is being built and how it will affect their town. The organization recently clashed with some people over water rights; “Summit Sucks Water” signs still dot some lawns. (Echoing Mauro, Jeff Rosenthal, another of Summit’s cofounders, says that if the town of Eden doesn’t accept the mountain town, he’ll deem the project a failure.)
There’s also the issue of diversity—of great importance to Summit. While Summit’s attendance is around 55 percent men and 45 percent women, the organization was founded by young, white men. For some members, that translates into a tangible feeling of a boys’ club. Personally and professionally, Summit has been a boon for Rebecca Clyde, a cofounder of a digital-marketing agency and a chatbot startup: She’s met friends and investors, and has considered buying property on Powder Mountain. But she chuckled when she attended a session about the development plans on Powder Mountain. “There were three men up there on this panel, and they were talking about why they loved that mountain: because the land has a feminine energy and the village and concept is trying to harness it,” she recalls. “Where are all the women who are going to bring all this feminine energy to life?”
The French philosopher Michel Foucault once wrote that utopias “present society itself in a perfected form, or else society turned upside down, but in any case these utopias are fundamentally unreal spaces.” Summit has money, time, sensitive architects, and a mission of goodwill—perhaps all the raw ingredients needed to actually succeed in creating a paradisiacal mix of leisure, thought leadership, philanthropy, and education.
Still, the intentional communities in the United States have tended to fall short of their initial feverish inspiration, or else survived in isolation from the rest of the world. Summit’s brand of utopianism is not that of the 19th century: Its community isn’t interested in multiplying Powder Mountains across the globe, nor is it cut off from the world in an outcropping of geodesic domes in the desert. As an idea, Summit Powder Mountain is some next-gen hybrid. Time will tell if it can become a real place—and not just another resort town.


In an effort to understand the politics of tech’s wealthy elites, Stanford researchers surveyed 600 technology-company founders, asking them about different policies.
The caricature of Silicon Valley people, which we’ve pilloried before, are that they are simple libertarians. The new survey continues to deconstruct this silly portrait. Fewer tech founders (24 percent) than Republicans (63 percent) or Democrats (44 percent) generally agree with a simple statement of libertarian philosophy: “I would like to live in a society where government does nothing except provide national defense and police protection, so that people could be left alone to earn whatever they could.”
But tech founder politics are different from either political party in unusual ways. Tech founders are developing a new, strange kind of politics that’s opposed to regulation, but for redistribution through taxation.
The New York Times has a really excellent visualization of the core ideological findings:
Look at the big bubble in the yellow tech founder column: Don’t regulate and do redistribute. None of the other large groups they surveyed showed a comparable interest in this political philosophy.
The “do not regulate” category was formed from responses to questions about regulating Uber, how the gig economy should be structured, whether it is too hard to fire workers, and the general proposition of whether “government regulation of business does more harm than good,” as well as specific questions about regulating drones, self-driving cars, and internet companies.
For example, 80 percent of tech founders think economic inequality is fine if it means the economy grows faster and 75 percent of tech founders think labor unions should lose influence. “They look like Republican donors when we ask them these questions,” said David Broockman, a Stanford Graduate School of Business professor who coauthored the study with fellow academic Neil Malhotra and journalist Greg Ferenstein.
And yet, when the researchers asked the tech founders about taxation and redistribution policies, they expressed major support for things like “universal healthcare, even if it means raising taxes,” increases in spending on the poor, and taxes on high-income individuals.
If tech founders had their way, government regulation might not stop you from financially falling through market action, but it’d bounce you back up. This is government not as a shaper of markets, but as a market-failure compensator. It’s not quite a “social safety net,” but maybe it’s a social trampoline.
This in part explains the interest of Silicon Valley in a universal basic income, which would distribute a cash stipend that’s enough to cover the basic necessities of life. While the nominal recent interest in UBI stems from fears over job displacement by robots, the UBI solution squares perfectly with the underlying ideology of tech founders. This is a group of people who don’t want government or organized labor to check their power over workers or restrict the disruptions their products may have in existing industries. But their liberal social politics lead them to wanting some kind of societal cushion for capitalism.
Which is one reason my colleague Ian Bogost suggested this name for the tech-founder political philosophy: “Trust me.”


Facebook claims it can reach more young people in the United States with advertisements than are actually alive, according to a new report.
Last week, a trade magazine in Australia looked at the “reach” statistics that Facebook gave for that country, and found that the company estimated it could reach more young people than Australia’s census.
Now, the firm Pivotal Research has done the same thing for America. “Through Facebook’s Ads Manager we can see that Facebook claims a potential reach within the United States of 41 million 18- to 24-year-olds, 60 million 25- to 34-year-olds, and 61 million 35- to 49-year-olds,” they wrote. “By contrast, U.S. Census data indicates that last year there were a total of 31 million 18- to 24-year-olds, 45 million 25- to 34-year-olds, and 61 million 35- to 49-year-olds.”
That’s an overstatement of 10 million people among 18- to 24-year-olds, and 15 million among 25- to 34-year-olds.
A Facebook spokesperson told The Wall Street Journal that the reach estimates “are not designed to match population or census estimates. We are always working to improve our estimates.”
Which, sure. Except that you’d probably want your estimates to match reality as closely as possible, and showing 30 percent more people than could possibly exist seems like a problem.
The larger context is that Facebook has been eating ever larger chunks of the digital-advertising market, soaking up ad dollars in large part because of the scale of the company’s user base (a representative example of coverage: “Facebook’s Scale Casts a Shadow Over Publishers”).
If just some of that scale is not real, that appears significant, even if, as the Journal notes, “the impact of the discrepancy isn’t clear” for advertisers—at least not yet. (Several advertising associations did not immediately respond to requests for comment.)
It’s easy to see how such estimates could go wrong. People create more than one account. They switch devices. They lie about their ages.
But that strikes at the heart of the other big lure of Facebook. This is the company that’s supposed to have the most and best user data. They’re supposed to have solved the problem of matching up real people with online personae. That was the point of a huge London Review of Books essay on the company. “Facebook, in fact, is the biggest surveillance-based enterprise in the history of mankind,” writes the journalist John Lanchester.
And yet, on Instagram, which shares data with Facebook and uses its parent company’s back-end advertising infrastructure, the company’s algorithms continue to believe that I live in Washington, D.C., and occasionally serve me local advertising to that effect.
This makes some sense, given that The Atlantic is headquartered in D.C. But I only lived there for 18 months, and I left in 2011.
Granted, I’m just one person, and anecdotes aren’t data. But I’ve been posting pictures from and receiving bills in Oakland for six years. Shouldn’t the “biggest surveillance-based enterprise in the history of mankind” know that?
Maybe these demographic notes matter far less than it’s thought for advertising—age, location, that kind of thing. Because the truth is, I’ve even purchased hundreds of dollars worth of sneakers and clothes through Instagram ads. They do understand what I want to buy, even if they don’t know where I live.


The weight of water can deform the Earth’s crust, if there’s enough of it. And we can measure that change with the ultraprecise global-positioning satellites humans have launched into orbit.
On Monday, Chris Milliner of the Jet Propulsion Laboratory tweeted a simple map visualizing data from the Nevada Geodetic Laboratory. It showed that the GPS data from special stations around Houston detected that the whole area had been pushed down roughly two centimeters by the weight of the water that fell during Hurricane Harvey.
GPS data show #Harveyflood was so large it flexed Earth's crust, pushing #Houston down by ~2 cm! #EarthScience #HurricaneHarvey #txflood pic.twitter.com/88lNScJBq9
Why this would happen is simpler than you might think. A gallon of water weighs about 8.34 pounds. And by one estimate, Harvey dropped 33 trillion gallons of water across the area it hit. So that’s roughly 275 trillion pounds.
And it turns out that scientists have measured the effects of loading a bunch of water onto land many times. For example, a 2012 study of the Himalayas detected a seasonal flux in the height of the mountains as water fell, and then ran off those mountains into Asia’s rivers.
A 2017 study in Science found that the Sierra Nevada exhibits “vertical surface displacement [with] peak-to-peak amplitudes” of 0.5 to one centimeter. More water is more mass. Less water is less mass. And the crust underneath that water responds to the changes.
One could see Harvey’s deposition of water as a fast-action version of what happens in the Earth’s mountain ranges each year.
There are some caveats, Milliner later explained. Some of the change could come from the soil underneath the GPS stations becoming compacted by the water’s weight. But because some stations located on bedrock also experienced the depression, he believed that the key mechanism was crust deformation. It’s also possible, he elaborated, that some areas outside Houston were pushed up by the way the water squished the Houston crust.
Perhaps a hurricane seems a match for the crust of the Earth, in mental scale. But humans can also have this sort of effect (even without calculating how much of Harvey’s water was due to climate change). Huge dams can impound water on a scale of Hurricane Harvey’s rains. The Hoover Dam and Three Gorges Dam both created reservoirs with a capacity of roughly 10 trillion gallons of water.
The effect of the filling of the Three Gorges Dam has experienced substantial attention from Chinese researchers, in part because there was a marked increase in small earthquakes in the region as the reservoir was filled. That’s not a concern in Houston, which is not a seismically active region.
It also remains to be seen how quickly the Earth rebounds in Houston, given that the water is rushing back to the sea, rather than sitting on mountaintops as glaciers or in reservoirs as hydrologic storage. And one assumes there’s an interesting paper in that for a geodetic researcher.
And there are (at least) two more ways humans are changing the crust of the Earth. Because the climate is warming, there’s less ice—and therefore less mass—attached to the world’s mountaintops. “The Earth, behaving like an elastic body, uplifts in a response to the load loss,” the Himalayan researchers write.
Meanwhile, in Houston, the ground had already been sinking—scientists call it subsidence—because humans have pumped the groundwater out of the aquifers under the city.
With each passing year, the combined and uncoordinated efforts of humanity force the mountains a little higher and the flats a bit lower.


In 2009, with little attention from abroad, the government of India launched a new identification program that has gone on to become the largest biometric database in the world. The program, known as Aadhaar, has collected the names, addresses, phone numbers—and perhaps more significantly, fingerprints, photographs, and iris scans—of more than 1 billion people. In the process, Aadhaar has taken on a role in virtually all parts of day-to-day life in India, from schools to hospitals to banks, and has opened up pathways to a kind of large-scale data collection that has never existed before.
The Indian government views Aadhaar as a key solution for a myriad number of societal challenges, but critics see it as a step toward a surveillance state. Now, the Aadhaar experiment faces a significant threat from the Indian Supreme Court—one that may prove to be existential.
In late August, the Supreme Court issued a unanimous decision that found, for the first time, a fundamental right to privacy in the Indian Constitution. The decision has been widely celebrated by Aadhaar’s opponents, who believe that the program is in conflict with the newly enshrined right. Soon, the Supreme Court will direct its attention to this very issue, and if they find that Aadhaar violates privacy rights, it will be up to lawmakers to rethink the entire program. But if the Supreme Court rules that the program is constitutional, then Aadhaar, already staggering in scope and ambition, will continue to grow.
When the Indian government first launched Aadhaar, it saw an opportunity to harness the country’s burgeoning technology sector to reduce corruption and streamline the delivery of government services. Prior to the advent of Aadhaar, the government has said it was plagued by problems managing its welfare programs, and lost millions of dollars each year as Indian residents either inserted fake names or their own names multiple times into the system in order to withdraw more than their fair share of benefits. With Aadhaar, the practice of accessing benefits became a simple matter of touching a fingerprint scanner. If the fingerprint matches the one on file, the benefit can be approved and administered. When it is working well, the process is comparable to unlocking an iPhone, and ensures that government benefits go only to the people who qualify.
Open to all Indian residents, Aadhaar was optional at first and associated with only a handful of government subsidies, including those for food and liquefied petroleum gas for cooking. It was targeted at those who needed help the most, particularly rural villagers who lacked official forms of identification, and were therefore unable to open bank accounts or access welfare programs in the past.
But over time, mission creep set in. Under the leadership of Nandan Nilekani, the cofounder of the outsourcing firm Infosys (whom Jon Stewart once welcomed as his new overlord on The Daily Show), Aadhaar was used as a way to apply data-driven improvements to a wide range of government and private-sector services. Aadhaar was soon linked to so many activities that it has now become almost impossible to live in India without enrolling. Participation in the program is a requirement, or will be soon, for filing taxes, opening bank accounts, receiving school lunch in the state of Uttar Pradesh, purchasing railway tickets online, accessing some public Wi-Fi, participating in the state of Karnataka’s universal health-care coverage, and benefiting from a wide range of welfare programs. The Indian Member of Parliament Jairam Ramesh has sarcastically described the program as “compulsorily mandatorily voluntary.”
The government authority responsible for administering Aadhaar declined to comment, but in an op-ed for The Indian Express, its CEO, Ajay Bhushan Pandey, wrote that the program has saved the government approximately $8 billion in the past two years alone (the World Bank has estimated that the figure is closer to $1 billion per year). Pandey says that the program has succeeded in improving the government’s capacity for reaching and serving people directly.
For many of the communities Aadhaar was designed to help—particularly the poor and the underserved—the technology hasn’t lived up to the sunny rhetoric, however. In a country with inconsistent internet outside of its large cities, remote towns struggle to get online to authenticate peoples’ fingerprints with the central database. Some enrollees insist that their satellite-internet access works only on cloudy days; others say it functions best when it’s sunny.
According to an analysis of government data by Reetika Khera, a professor of economics at the Indian Institute of Technology in Delhi, millions of people have missed out on government benefits because of Aadhaar. In some cases, that’s because those who are elderly or disabled are unable to walk to the distribution sites to verify their identities. Others, who do manual labor, find that their fingerprints are too weathered from years of physical exertion to scan correctly, and so are denied their food rations.
Nikhil Dey, one of the founders of the grassroots organization Mazdoor Kisan Shakti Sangathan, also studied the government’s data. He found that approximately 1 million people in the state of Rajasthan had been unfairly dropped from the government lists for food subsidies due to Aadhaar, and more than 3 million were unable to collect their designated grain allocations. In one district alone, Dey says, 1,350 out of approximately 2,900 people marked “dead” or “duplicate” were actually neither, but lost access to their pensions anyway.
Despite these implementation challenges, the scariest parts about the program for privacy advocates are its ubiquity and lax security. According to the technology engineer Anand Venkatanarayanan, when biometric information is used to access a service via Aadhaar, such as purchasing a new cell phone, the service provider receives that person’s demographic data (name, address, phone number), and the government receives the metadata—specifically, the date and time of the transaction, the form of identification used, and the company with which the transaction was carried out. That information can paint a fuzzy but intimate long-term picture of a person’s life, and raises concerns about both government surveillance and private-sector abuse.
There is already ample evidence of misuse. High-profile examples from the past several months have dominated news cycles: 210 government agencies published full names, addresses, and Aadhaar numbers of welfare beneficiaries; 120 million users’ Aadhaar information appears to have been leaked from the telecommunications company Reliance Jio (the company claimed the data was inauthentic); bank-account and Aadhaar details of more than 100 million people were disclosed through certain open-government portals; the government’s e-hospital database was hacked to access confidential Aadhaar information.
These disclosures may be most damaging for those who are already vulnerable. Apar Gupta, a lawyer on the team that challenged Aadhaar before the Supreme Court, is particularly concerned about many Dalits (previously the “untouchables” in the caste system) and migrant laborers who work as manual scavengers, entering sewers without protection to clean them by hand. It’s a dangerous occupation with a high fatality rate, and it can also bring immense social stigma. Gupta worries that Aadhaar will permanently stigmatize these individuals by allowing future employers, schools, banks, and new acquaintances to view their database information and judge them based on their socioeconomic standing. Social mobility in India could become even more difficult. So could hiding a pregnancy or a gender-reassignment surgery, or failing the eighth grade. In many of the objections raised about Aadhaar, there’s a kernel of fear that the program could turn a person’s identity into a prison.
The August 24 Supreme Court ruling seemed to address these concerns, making the case that privacy is essential for an individual to function in society. “Privacy ensures that a human being can lead a life of dignity by securing the inner recesses of the human personality from unwanted intrusion,” Justice Dhananjaya Y. Chandrachud wrote. In arriving at its decision, the Supreme Court rejected two previous decisions from the 1950s and 1960s that denied a right to privacy, and instead framed privacy as a “primordial” right that must be understood in the context of an interconnected world. The justices further emphasized the point by referencing international jurisprudence about privacy from the United States, Canada, South Africa, and the European Union.
That didn’t surprise Mishi Choudhary, the legal director of the Software Freedom Law Center, who noted that “we’re at a stage where technology is sweeping the planet in almost the same way. A lot of countries are looking to each other for guidance on how to adapt their jurisprudence to suit the current world.”
Over the last few years, Russia, Tunisia, Morocco, and Algeria have all expressed interest in the Aadhaar program, and according to reports, representatives from Tanzania, Afghanistan, and Bangladesh recently visited India to learn more about implementing an Aadhaar system of their own. As the Supreme Court once again prepares for hearings about Aadhaar, the world will be paying close attention.


A series of satellite images provides an expected yet still breathtaking look at the flooding caused by Hurricane Harvey in Texas.
Taken by DigitalGlobe, the primary private provider of spy-satellite imagery to the U.S. government, they add to the stockpile of data that illustrate the storm’s record-setting rainfall.
The most striking combination of images, at the top of this story, shows a piece of Simonton, Texas, completely swallowed by brown flood waters.
At times, as in the images of Wharton below, the mud from the flood reveals the underlying topology of the land, showing how the bayou might flow without human intervention.
Other images show flooding in Angleton, Brookshire, Holiday Lakes, and Rosenberg.



Benjamin Franklin was attracted to electricity. Given its similar color, crackle, and configuration, he suspected that lightning itself was electricity. Noting that a pointed metal needle could draw electricity from a charged metal sphere, Franklin became convinced that a metal rod could coax lightning from the sky. Why? So it would strike the rod instead of buildings or passersby.
As legend has it, Franklin hopped on a horse in 1752 with key-adorned kite in hand, determined to prove his conviction. The two pranced about under stormy skies until the charged-filled atmosphere energized the key and confirmed his suspicions.
More than two-and-a-half centuries later, lighting rods persist—as decorative architectural pieces, as vestiges of the past, and as mitigators of lightning’s power.
* * *
Franklin later extended his lightning-rod idea to ships, including British warships, which were eventually outfitted with anchor chains that stretched from the top of their wooden masts to the sea. They aimed to dissipate electrical energy so the masts would stay intact if lightning struck. Soon, lightning rods were widely adopted in the northeastern United States, and elsewhere during the mid-1700s.
But not without resistance from some quarters, including the clergy. In fact, the Reverend Thomas Prince, pastor of Boston’s Old South Church, asserted that the Cape Ann earthquake of 1755 could be attributed to the ubiquitous placement of lightning rods in New England, especially in Boston. Centered off the coast of what’s now Massachusetts, the earthquake, Reverend Prince seemed to imply, was no accident given man’s unwise attempts to deflect the hand of God.
Today, Franklin’s lightning rods are known by many names: air terminals, finials, lightning conductors, or strike-termination devices among them. To me, calling them strike-termination devices makes it sound as if once a lightning bolt strikes the rods, the danger is averted. Instead, the rods, typically a half-inch in diameter, are connected to a metal cable hidden within the building or structure. The diameter of both the rod and cable vary depending on the height of the building and the type of metal. In general, the higher the building, the heavier the rods and cables. No matter the size, the cables make their way down to Earth, where they are anchored. Grounded, the lightning rod dissipates the lightning strike’s energy.
Without this seemingly simple system, damage to a structure can range from a minor insult to a complete loss. Parker M. Willard Jr. has seen just that. “We see a lot of damage from indirect strikes that come in through the utility lines,” he says. “The average insurance claim is $7,400, and I’ve seen some in excess of $700,000.”
Willard is the co-owner of Boston Lightning Rod Company, along with his father, Parker M. Willard Sr. Willard Jr.’s great-great-grandfather, Henry Willard, founded the company, which is based in Dedham, Massachusetts, 144 years ago. Now 40, Willard Jr. started working for Boston Lightning Rod when he was 16. The lighting-rod industry is “family oriented,” according to Willard. Multigenerational, really. “We’re one of the oldest [lightning rod companies] in the United States,” he tells me. “It’s not unusual to go to trade seminars and meet the next generation. There are a lot of lightning-protection families out there.”
When it comes to lightning and its stupendous energy, the bottom line, says Willard, is that lighting rods, when installed correctly, provide an effective path to ground for electrical energy, thus mitigating or avoiding damage to buildings. Especially when surge protection for incoming telecommunications, electric lines, and the internet is added to the setup.
“A lot of times people will put lightning rods up on their home or business and think they’re protected, but the structure can take an indirect strike to a utility line or a transformer outside the structure and the lightning rod is defenseless against that kind of strike,” Willard says. That’s why surge protection for telecommunications and cable have become a bigger and bigger part of his business: “Twenty years ago, people had a telephone, a TV, and an electrical line. Now they have high-end electronics, stuff that’s highly susceptible to any kind of electrical surge. A lightning rod system protects against a direct strike. Surge protection protects against an indirect strike.”
* * *
In fact, most structures need more than one lighting rod, Willard explains. The rods should be spaced across the structure’s main ridge, with a maximum distance of 20 feet between them. The average house needs three or four rods, and ideally other rods should be placed on prominent points such as chimneys and dormers. All are interconnected with that all-important cable that runs to ground.
“The idea is that when lightning strikes your house, or any building, you want it to strike in a safe place that takes the current down to the ground without damaging your house, your television set, or you,” says Joseph Dwyer, a professor of physics at the University of New Hampshire. “You can’t prevent the lightning from striking, but if it’s going to strike a certain area, it gives it a safe place to do it.”
In the past, people favored what are known as Franklin air terminals—rods that come to a point at the top. Willard says that until recently they were used almost exclusively in the United States, because their points were believed to conduct electricity more effectively. Now, most newly installed lightning-protection systems use blunt, or rounded, rods. Studies have shown that blunt terminals work just as well as their pointy brethren, if not better. Plus, they’re safer to install—no risk of impalement.
“Safety is always a concern, and you can’t be afraid of heights in our business,” Willard says. He and his crew have worked on some of the tallest buildings in Boston, including the landmark Prudential tower in the heart of the city. Lightning rods or no, Dwyer says humans should have a healthy fear of lightning, especially when outside. “There’s no safe place outside during a thunderstorm,” he says.
That’s because most lightning never leaves the storm. Often, it never strikes the ground, traveling upward and branching out a bit in the sky. It’s the lightning that travels down toward the Earth—cloud-to-ground lightning, as it’s called—that can threaten life and property. “If you ever see a nasty-looking cloud up there, you might think, ‘I’m okay, maybe there’s no lighting,’” says Dwyer. “But it could be that that storm has been making lightning for a while. It just hasn’t bothered to send one down to the ground. The lightning you see first may be just the tip of the iceberg.”
Dwyer describes lightning as “a really big spark that is measured on the scale of miles or kilometers and is about as wide as a human finger.” It can travel 100 miles through a storm by breaking down the air in front of it and transferring charges.
What’s more, lightning doesn’t travel in a smooth line; it zigzags. “It will go maybe 50 yards, and then pause as if it has run out of steam, then it will suddenly leap forward another 50 yards, sometimes in a new direction, sometimes it will branch,” Dwyer says.
Lightning is erratic, so people try to impose order—even if that order leads to more disorder in the form of myths. As it happens, lightning does strike in the same place twice—and sometimes more. Take the Empire State Building, for instance. The New York City art-deco landmark is struck nearly 100 times a year.
* * *
Some people think that lightning rods actively draw lighting. That, too, is a myth; they help dissipate electrical energy if it forms. But get this: Lightning rods did attract attention from European fashionistas in the late 18th century. According to the Lubbock Morning Avalanche’s May 13, 1933, edition, ladies of the haute mode were wearing lightning rods attached to their hats, hats known as chapeau paratonnerre. The so-called rods, the paper said, “consisted of a woven metal ribbon which encircled the hat and terminated in a long silver cord trailing on the ground.”
Because they are so prominent, real lighting rods have also inspired decoration. I appreciate good design, so I decided to search for lighting rods with fine form. I came across some striking designs: an antique copper lightning rod with a starburst tip, a vintage lightning rod with a wind indicator from Maryville, Missouri, and a vintage lightning rod in ornate hammered aluminum with a cobalt-blue ball and a roof mount. Prices ranged from $49.99 to $145.
All lovely, but it was the cobalt-blue ball that caught my eye. To my disappointment, it was made from hard plastic, not glass. Some people say glass balls were added to lightning rods so people would know if a structure had been hit should they find the ball shattered following a storm. Others say this is not so. No matter, the glass ball has become a decorative feature.
According to Willard, nowadays the glass balls are purely ornamental. He sees them on old barns in Maine, New Hampshire, and Massachusetts. “They don’t explode, and they don’t glow,” he says. “Lightning passes harmlessly right through them. They’ve returned over the years because some people want them as an architectural point.”
I asked Willard if he has installed lightning rods on his home. “That’s only a recent development,” he confided. “I was just doing the siding and the roof on my house before I put the lightning rods on.” After a roofing job, homeowners need to have someone qualified reinstall the lightning rods. They may look simple to install to the untrained eye, but they have idiosyncrasies, and if installed incorrectly they can be dangerous.
That’s why I’ve decided to purchase a single lightning rod, a dramatic one, and install it in my living room, so I can enjoy its sculptural qualities absent the storms. I suspect it will make for a striking piece of art.
This article appears courtesy of Object Lessons.


On Friday, Muslims around the world will buy a sacrificial animal, have it slaughtered in accordance with Islamic law, and divide up the meat between their family, their relatives, and the poor. The ritual slaughter is among the central traditions of a joyous feast called Eid al-Adha, the larger of the two annual festivals in Islam, which marks the end of the annual hajj, or pilgrimage to Mecca. In Islamic tradition, the ritual slaughter honors Abraham’s willingness to sacrifice his son in the name of God, although God at the last moment gave Abraham a ram to sacrifice instead.
In some countries like Pakistan, Yemen, and Afghanistan, celebrants will head to bustling outdoor markets to buy a choice animal—usually a sheep, but people also sacrifice goats, cows, and even camels—directly from the farmer who raised it, and have it slaughtered on the spot. But elsewhere in the Muslim world, the supply chain isn’t so straightforward. Many countries in the Middle East import the majority of their Eid livestock from places like Somalia, India, Romania, and as far as Australia. Some 12,500 Syrian sheep will even be airlifted from Lebanon to Qatar, owing to an ongoing political standoff between Qatar and several Gulf countries led by Saudi Arabia, which has shut down the only roads into Qatar.
In Dubai, if you fancy a nice Australian steer, there’s no need to block off an afternoon to find one and have it slaughtered: An app from a Kuwaiti company called al-Mawashi (“livestock” in Arabic) lets you buy as many as you like online, for pickup at the “smart” abattoir of your choice, already slaughtered in accordance with Muslim law and cut up to order.
What Does It Mean to Be a ‘Secular Muslim’?
Outside the Eid rush, al-Mawashi will even deliver to your door. In a 30-second ad spot for the service, a portly kid tumbles down the stairs in an ostentatious, gold-trimmed home, imploring his mother to cook him some meat. Busy on the phone, she sends him to the kitchen to ask the South Asian cook, who declares that there’s no meat at all. He calls for his father, who sends him to Raju, the Indian driver, who tells him to buzz off. Down at the soccer field, he’s struck with inspiration when he sees an al-Mawashi delivery truck. Moments later, a delivery man hands his mother a box of meat at the door. “Who ordered this?” she wonders aloud, until the kid bounds up with a sly smile. The whole transaction is a far cry from Abraham’s sacrifice in the desert.
An app for reserving a sacrificial animal for Eid isn’t out of place in the sea of programs that have sprouted up to help the smartphone-toting Muslim go about his or her religious duties. Apps abound that calculate prayer times and send push notifications when they’re about to start, like a modern call to prayer. Many include a compass that points toward Mecca, to show Muslims which way to face as they pray. (Even Google got into the qibla-finding game this summer.) An app called Zabihah is like a Yelp for Halal food—except its desktop version has been around six years longer than Yelp itself. And hundreds of Koran apps help users memorize the holy book with translations, audio recordings, and games.
Social networks, forums, and messaging apps allow Muslims to connect with one another, and share information about Islam with curious non-Muslims. The internet is also a popular destination for believers searching for guidance from Islamic scholars about the finer points of living religiously in the modern world. Can the Koran be read on a smartphone or laptop? Yes, says a fatwa, or religious decree, from the United Arab Emirates—but reading it in print and memorizing it is best. Can a recorded recitation of a Koranic verse be used as a cellphone ringtone? No, said a panel of clerics in India in 2009, because the ringtone may be cut off before the verse is over, which would be a sin. What’s more, it would be inappropriate for the recitation to be heard by an individual in a bathroom.
These innumerable “new media” fatwas, which touch on topics large and small, have garnered some pushback from the religious old guard, some of whom see them as a threat to their authority. The proliferation of new technology also poses ethical conundrums for Muslims, who debate how it should and shouldn’t be allowed to impact their lifestyle and religion.
In a 2005 lecture titled “Islam, Muslims, and Modern Technology,” Seyyed Hossein Nasr, a celebrated Iranian scholar of Islam, explores Muslims’ relationship with Western technology. “We cannot be naïve and think it is simply neutral,” Nasr said, pointing to technology’s deleterious effects on health and mental well-being. Nasr isn’t calling for cutting out television and internet and cellphones entirely; he advocates instead for casting a critical eye on Western technology to discern its value and objectives. He emphasizes tradition: Mass production shouldn’t be allowed to take over traditionally handmade goods like Persian carpets and Indian saris, and Islamic architecture and urban design should be taught alongside their Western equivalents in the Muslim world.
Amana Raquib, a professor of philosophy and religion at the Institute of Business Administration in Karachi, Pakistan, argues for the development of an Islamic ethical framework for evaluating technology. She proposes a handful of ways to apply Islamic values to modern technology in a 2016 paper submitted to a conference called “Islamic Perspectives on Science and Technology.” Technology that promotes greed and selfishness doesn’t align with Islamic values, she writes; nor does technology that could “enhance” humans (such as gene therapy), because it contradicts our nature as divinely created beings.
Apps that order up religious sacrifices, however, would seem to be a prime example of technology imbued with Islamic values. Dubai’s ordering service may seem newfangled, but Muslims have been arranging Eid al-Adha sacrifices remotely for a long time. Numerous services allow Muslims to offer their ritual sacrifice to a family in need abroad, and some have had online order forms since the internet was young.
“These are interesting ideas because they take out some of the traditional processes,” said Gary Bunt, a professor of Islamic Studies at University of Wales Trinity Saint David. “Now it’s another product on your phone.” But for big, busy families, he said, it’s a natural progression.
Even as technology alters every facet of daily life, the core practices of Eid al-Adha haven’t changed dramatically over the years, said Hilal Khashan, a political-science professor at the American University of Beirut. The first morning of the four-day celebration involves an early visit to the mosque for prayer. Muslims make family visits, trade presents, cook traditional dishes and sweets, and slaughter and share a sacrificial animal, if they can afford to do so.
The most tangible encroachment of modernity may be an uptick in buzzing smartphone notifications around the holidays. “In the past, people would send postcards and letters,” Khashan said. “Now, everybody just exchanges holiday wishes on WhatsApp.”


Washington, D.C., woke up to a humdinger of a story today, a flash portrait that shows the relationship between money, power, and ideas—and highlights the potential for intellectual corruption that has accompanied the flood of Big Tech money into the capital.
The New York Times reported that the New America Foundation, the digital-savvy center-left think tank, might have pushed out Barry Lynn, a ferocious and influential critic of “platform monopolies” like, for example, Alphabet (née Google). After Google was hit with a 2.42 billion–euro fine by the European Commission in June, Lynn posted a congratulatory note to the regulators and a call for action by American anti-trust officials.
New America, meanwhile, has received more than $20 million since its founding in 1999 from Alphabet companies and the foundation established by Eric Schmidt. Schmidt, currently executive chairman of Alphabet, also previously served as chairman of New America’s board.
Before Lynn began his Open Markets program, New America had been closely, deeply associated with technology- and market-friendly Silicon Valley progressivism.
Open Markets took a completely different tack, “researching and reporting on the political and economic dangers posed by monopolization,” largely by technology companies. And they were good at it. They were gaining adherents with influential essays by Lynn, Lina Khan, and Matt Stoller. They’d gotten the ears of Senators like Elizabeth Warren and even one of Silicon Valley’s own representatives, Ro Khanna. Some have given them credit for the anti-monopoly plank in the Democrats’ platform. (New America also has two Atlantic people on its board: the Atlantic Media chairman David Bradley and The Atlantic national correspondent Jim Fallows, neither of whom contributed to this story. Fallows was the chairman of the New America board from its founding until 2008, when Schmidt took over. New America’s CEO, Anne-Marie Slaughter, has written for this magazine. As have Lynn, Matt Stoller, and Phillip Longman of the Open Markets team. And we’ve cited Open Markets work repeatedly in our coverage.)
Soon, Open Markets—and its staff of 10—will be spun out of New America and will stand on its own.
Those are the well-established public facts. What happened next is what’s up for debate.
The Times article says that Schmidt saw the June note Lynn posted to the New America website and “communicated his displeasure” to Slaughter. The statement came down, then went back up without explanation. A few days later, Slaughter asked to meet with Lynn, and—somehow—it was decided his group would leave New America. She then sent him an email that he shared with the Times.
The Times wrote that the email said “the decision was ‘in no way based on the content of your work,’” but but also noted that “Slaughter accused Lynn of ‘imperiling the institution as a whole.’”
While the Times leaves out the context of the “imperiling” remark, a plain reading of the excerpts—and the fact that the email was sent at all—suggests a leader panicked about the loss of the institution’s most important constellation of funders.
Shortly after the story broke, Slaughter said on Twitter, “This story is false. New America will issue statement shortly. We are proud of Open Markets work.”
Slaughter’s longer statement emphasized that the spin-out was occasioned by Lynn’s “repeated refusal to adhere to New America’s standards of openness and institutional collegiality,” which is how educated people say, “He was an asshole.” (Neither Slaughter nor Lynn immediately replied to my request for an interview Wednesday. Nor did a representative for New America.)
Update: New America has released the full text of three emails from Slaughter to Lynn. A charitable reading of Slaughter’s position is that it was not Lynn’s work itself—indeed she offers to keep his team on doing their same research—but his behavior around the presentation of his work. Slaughter tells Lynn that he’s being pushed out not “based on any response from Google” but because of a “pattern” of “telling me one thing and doing another.”
In the Times, Google disputed that it had anything to do with Open Markets leaving New America.
If it all sounds like an inside-baseball Washington-power story, that’s because it is. But that doesn’t make it unimportant. This is one way that the future of the nation’s economy is being decided.
Whether Alphabet, Amazon, and others are treated as special entities, regular old companies, or monopolists is a key factor in determining what the internet will look like in the coming years.
All of which makes the undisputed facts of the case enough to tell the story. Maybe Eric Schmidt sent Anne-Marie Slaughter an email or a text message saying, “WTF?” with a link to Lynn’s post. Or even just an email with the link. But even if he didn’t, is anyone to believe that the institutional apparatus of New America would not be aware that one of its units was directly targeting the business model of a major funder and allied entity?
Working with just the dumb facts of the money involved, one might imagine that Lynn’s time at New America would come to a close, sooner or later.
The scale of Silicon Valley money and Washington money are so different that the introduction of the former into the latter is almost comical. You’ve got companies amassing tens of billions of dollars in cash with mechanics that are linked to particular regulatory and tax regimes. And those regimes are held up by people who measure donations in the tens or hundreds of thousands.
This is a key component of what Robert Reich (a decade ago) called supercapitalism. Many times the most efficient way to make money is to change the rules governing how that money can be made.
“Supercapitalism has not stopped at the artificial boundary separating economics from politics. The goal of the modern corporation—goaded by consumers and investors—is to do whatever is necessary to gain competitive advantage,” Reich wrote. “That includes entering any battleground where such gains can be made. Washington—and other capital cities around the world where public policies are devised—has become a competitive battleground because public policies often help some companies or industries while putting rivals at a comparative disadvantage.”
And one of the best ways to change (or freeze) policy making and regulation is to change the conversations that people are having in Washington, D.C., through funding research at think tanks.
If Alphabet were Monsanto or General Motors or Pfizer, most people probably wouldn’t even feign surprise. But for many years, just the word Google conferred a special halo on otherwise anodyne projects. They were the standard-bearers for the future and were afforded a generosity of intention that no other company received. But no matter the color of its logo, Alphabet’s money is green, too, and it exerts the same power as anyone else’s.
The details of how this all happened are significant for the individuals involved. But for the country at large, outside Washington policy circles, the essence of the story will remain the same: At some number of millions, even the invisible, digital kind of money takes on a tremendous weight. No matter where it falls, or even could potentially fall, it shapes the structures of power.
Which might just be the point that the Open Markets program has been trying to make.


In the wake of Hurricane Harvey, the nation has once again seen a Gulf Coast city flooded, its residents in peril. A new book of essays, due to be published next January, Environmental Disaster in the Gulf South, provides important social context for the many “natural” disasters that have plagued the region for 200 years.
The book’s editor, Cindy Ermus, a professor at the University of Lethbridge, argues in her introduction that the Gulf South shares underlying characteristics that connect its disasters across time and space.
“The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.”
The cities of the Gulf Coast exist where they do because the ocean and rivers provide economic opportunities and scenic landscapes. But those same waters threaten to undermine the growing cities' safety during storms. Ermus cites the geographer Kent Mathewson, who maintained that the “workings and movements of streams, soils, seas, storms, and continental substrata may ultimately play a larger and more immediate role on this region than almost anywhere else on earth.”
But the environmental processes are only one component of the region’s vulnerability to disaster.
In these acute crises, we see inspiring stories over and over: Communities come together. People risk their lives to save their neighbors as well as strangers. It is heartwarming. It reinforces everything that we want to believe about ourselves as Americans, as Texans, as Floridians, as Louisianans.
But examining why these moments of heroism become necessary tells a darker story about America. People don’t just find themselves in places vulnerable to flooding. They are pushed there by racial injustice, economic inequality, and short-term, profit-driven development practices. The long-term decay of the nation’s infrastructure is a direct result of policy decisions that politicians and communities make time and again. The Gulf Coast is an extreme example of this, a laboratory for what happens when you combine lax planning policies, aging flood-control mechanisms, and a geography that channels storms from the warm (and warming) waters of the Gulf into the cities that line it.
Wetland cities sitting on or near a gulf that generates some of the fiercest storms on earth are becoming more vulnerable to the “natural” hazards they’ve long battled. Their development is booming, but in the process the cities have torn out the wetlands, paved over the prairies, and built an economy (and politics) around the carbon-heavy oil and gas industries. The cities grow. Local disaster managers do the best they can to prepare. And then they wait, hoping the circulation of wind and water does not bring the worst case to them. But it will, eventually, and everyone knows this, except when they manage to forget it. This is a slow tragedy in innumerable acts.
“It is hard to escape the conclusion that there is something exceptional—exceptionally dangerous, that is—about the South’s approach to the sea,” writes Ted Steinberg, a historian and author of Acts of God: The Unnatural History of Natural Disaster in America, writes in the book’s final chapter
I talked with the editor of Environmental Disaster in the Gulf South, Cindy Ermus, about what the book could teach us about Houston and Harvey, disaster management, and racial inequality.
Alexis Madrigal: So your academic field is the history of disasters?
Cindy Ermus: I looked at disasters and crises throughout graduate school. My manuscript looks at a plague in 1720 in southern France. And I edited a volume about disasters in the Gulf South.
Madrigal: Do you see through-lines in this long history of disaster management from back then to now?
Ermus: There are certainly a lot of parallels in disaster management, the ways that societies and governments come together or don’t come together to manage a crisis or prevent a crisis before it happens. I argue that prior to the 18th century, disaster management was more local. It depended more on the responses and reactions of municipal authorities rather than a capital of an emerging nation-state. And what we see emerging then is the centralization of disaster management. It’s coming out of these capitals, or to use the U.S. term, federal bodies, like FEMA or international ones like the UN.
Madrigal: Thinking about the Cajun Navy or the people organizing themselves on Twitter, do you think we’re seeing a move away from that central government control?
Ermus: I would say that there does appear to be a kind of coming back to the community in terms of mitigating these crises. In the 18th and 19th centuries, communities begin to rely more and more on the state, whether it is for welfare, public health, or assistance in terms of disaster.
One of the things that has come with increased awareness of the role of governments and corporations in disasters—the neoliberalization of disaster responses—is a sense of distrust.
The community is thinking: We can’t rely on the state right now. We can’t wait around for Bush—in the case of Katrina—to finally respond and send in assistance. So, let’s mobilize ourselves. Maybe the state isn’t doing what it should. I do wonder. As a historian, all of this is very ongoing.
Madrigal: What is one concept from your field that you wish people out there knew?
Ermus: A natural disaster implies that human activity doesn’t have anything to do with it. One phrase that we use in disaster studies in place of “natural disaster” is a “natural hazard.” It’s not a disaster until it affects a human population and the degree to which it affects that population is oftentimes a human problem.
Why did Houston pave over their floodplains—knowing or potentially knowing the fact that they would flood if a major storm hit?
Madrigal: But even the concept of a quote-unquote “natural hazard” seems to have undergone a weirding because of climate change. It’s clearly a very difficult task to disentangle precisely or even roughly how much of the rain that fell from Harvey was linked to global warming and how much would have happened under “natural” conditions. (One climate scientist estimated about 30 percent.)
Ermus: To use Harvey as an example, the degree to which climate change is at fault should be clear. It should be clear that’s not the main cause of the storm. Climate change is not solely responsible for the storm. These kinds of storms have happened. What climate change might have influenced and very much did influence is the strength of the storm. We know that the Gulf of Mexico serves as the brewing pot, if you will. It’s this creator of storms. The water in the Gulf of Mexico remained warm year-round. The average sea-surface temperature never fell below 73 degrees, even in winter. Climate change is undeniably taking place. And it is influencing storms like Harvey.
So how natural is a natural hazard even, given the effects of climate change? That’s something we’ll be talking about a lot.
Madrigal: Where did your interest in the Gulf South come from?
Ermus: I grew up in the Gulf South. I experienced Hurricane Andrew [in 1992]. I do 18th-century [history] but I find myself always applying things from the 18th century to today and vice versa.
Madrigal: Just in the last 10 or 15 years, it seems like the Gulf has been hit with several region-altering environmental disasters from acute ones like Katrina, Ike, and the Deepwater Horizon oil spill to the slow-burn of the oxygen-deprived dead zone and loss of coastal wetlands. Did you do the book because it’s such a disaster-prone region?
Ermus: It may not be the only capital of disaster but it is one among many. It has hundreds of years of experience with disasters and mitigation. And looking at this area will help us learn from the past as we look into a future where there will be more and more crises like this.
Madrigal: Aside from vulnerability to the same kinds of weather events, what unites the region?
Ermus: Infrastructure. I should mention Ted Steinberg’s afterword to Environmental Disaster in the Gulf South sums things up by looking at the slow disaster of infrastructure in Florida. In the case of infrastructure, this negligence takes decades and eventually comes to a head when Hurricane Katrina or Harvey makes landfall.
For example, infrastructure is what made Katrina the kind of disaster that it was. Decisions had been made on the ground. Things that could have easily been done and were instead neglected in terms of the construction of levees and the decisions of the Army Corps of Engineers in previous decades. Then there is the disappearance of the wetlands on the coast of Louisiana, which were disposed of to increase traffic to the Port of New Orleans. The wetlands are nature’s way of serving as protection against big storms.
Madrigal: How do the region’s politics and long history of racial inequality play into disaster management?
Ermus: Florida was the first state that I’m aware of to ban the use of the phrase “climate change.” The denial of climate change is going to significantly slow the progress of any infrastructure development or decisions that could help prevent crises like this.
In terms of the social history of the region, there are several chapters in the volume that address the environmental injustice that has this long history in the region. For instance, Andy Horowitz, a professor at Tulane University, looks at the 1900 Galveston storm in Texas as a “part of the ongoing disaster of racial terror in Texas at the turn of the 20th century.”
Christopher Church, a professor at University of Nevada, Reno, looks at hurricanes throughout the 20th century to see how lower socioeconomic communities are almost always the most vulnerable to disaster because the most affordable part of any city is usually the part that’s most vulnerable to disaster. In some ways, really, all of the chapters touch on this.
Madrigal: Taking the long view, do you think the way we look at disasters has changed?
Ermus: The way that we view disasters and crises has changed. Disasters were viewed as acts of god. Divine punishment. In the 18th century we start to get away from that. We see an exponential increase in people looking at geological and “natural”—that’s in quotations—causes. And today, increasingly, we see them as the result of human activity.
Even in 1755, during the Lisbon earthquake, Jean-Jacques Rousseau cites human activity for creating a disaster. It’s kind of an evolution of the way we view disasters from acts of god to natural occurrences to human events.
Madrigal: What does focusing on the humans allow us to see?
Ermus: Andy Horowitz, one of the contributors to my volume, recently tweeted something to the effect of: As you observe what’s going on in Houston, you observe the effects of this storm, ask yourself what human decisions contributed to these ill effects. And that’s the bottom line. We’re seeing what’s going on with these storms. We’re seeing that these effects are becoming worse and affecting more people than ever before. So, what can we do differently? Are we gonna do nothing? At what point are we going to decide as a society that climate change is real and that infrastructure matters?


The fallout from the rainiest storm in (at least) Texas history continues. No one has ever seen a storm dump this much water over so wide an area. Or as the National Weather Center’s prediction team put it, “The breadth and intensity of this rainfall are beyond anything experienced before.” As the situation on the ground continues to evolve, we’ve compiled a short list of significant resources, and helpful stories, to stay abreast of what’s happening. It will be updated.
We’ve gathered up the Twitter folks mentioned by name here (and several others) into this list.
Weather
The driving force for the event is, of course, the storm. As in many cases, the National Weather Service’s various feeds are essential. The NWS websites are a bit inscrutable, so the Twitter feeds are a good way to access their updates:
Two meteorologists have started up their own site, Space City Weather, for “hype-free forecasts.” They provide very in-depth analysis. For example, they have done an excellent job providing historical perspective on the storms that have flooded Houston:
Two other meteorologists have been heavily involved in filtering and amplifying significant information about the storm: Eric Holthaus and Brian McNoldy.
For a local perspective, Jeff Lindner is a meteorologist for the Harris County Flood-Control District, which manages the area’s flood mitigation infrastructure. He’s been tweeting, too.
Officials
The Harris County Flood-Control District manages the different pieces of infrastructure that try to protect Houston from flooding. They take a more traditional news approach, releasing press releases here. Their in-depth reports serve as excellent backgrounders:
The Harris County Sheriff’s Office has been tweeting extensively, including Sheriff Ed Gonzalez himself.
The City of Houston has an active Twitter feed including live broadcasts of some news conferences. Mayor Sylvester Turner, too.
Media
The Houston Chronicle has a large team of reporters on the ground with local knowledge. Many are tweeting in addition to filing stories. The investigative reporter Susan Carroll has been doing an excellent job of highlighting coverage. They’ve also, through time, created some of the best contextual explainers, including these two, which are the best journalistic accounts of the city’s flood history:
ProPublica, The Texas Tribune, and Reveal put together a remarkable explainer about the the previous two years’ worth of flooding. It’s essential background:
The reporters from that story are worth following: Neena Satija, Kiah Collier, and Al Shaw.
The alt-weekly Houston Press is also doing a very good job on the ground. Dianna Wray’s explainer on the (troubled, in-need-of-upgrades) dams holding a ton of rainwater is an excellent starting point for their coverage:
Community Organizations
Texas Monthly has compiled a list of organizations that can help people on the ground. And the Greater Houston Community Foundation, which works with and vets nonprofits, has set up a Harvey fund.
Scholars and Writers
Cindy Ermus is a history professor at the University of Lethbridge. She has a book coming out next January about environmental disaster in the Gulf South. She tweets.
Scott Knowles is a disaster historian at Drexel University. He also tweets: “In American disaster we ask responders and emergency managers to save lives and to fix generations of poverty and deferred maintenance. Impossible.”
Rebecca Solnit is not a scholar, per se, but her work on the 1906 San Francisco earthquake, A Paradise Built in Hell, is a rich (and heartening) exploration of what people do under the worst possible conditions. Here’s an excerpt in The New York Times.


Floods cause greater property damage and more deaths than tornadoes or hurricanes. And Houston’s flood is truly a disaster of biblical proportions: The sky unloaded 9 trillion gallons of water on the city within two days, and much more might fall before Harvey dissipates, producing as much as 60 inches of rain.
Pictures of Harvey’s runoff are harrowing, with interstates turned to sturdy and mature rivers. From Katrina to Sandy, Rita to Tōhoku, it’s easier to imagine the flooding caused by storm surges wrought by hurricanes and tsunamis. In these cases, the flooding problem appears to be caused by water breaching shores, seawalls, or levees. Those examples reinforce the idea that flooding is a problem of keeping water out—either through fortunate avoidance or engineering foresight.
But the impact of flooding, particularly in densely developed areas like cities, is far more constant than a massive, natural disaster like Harvey exposes. The reason cities flood isn’t because the water comes in, not exactly. It’s because the pavement of civilization forces the water to get back out again.
* * *
There are different kinds of floods. There’s the storm surge from hurricanes, the runoff from snowmelt, the inundation of riverbanks. But all these examples cast flooding as an occasional foe out to damage human civilization. In truth, flooding happens constantly, in small and large quantities, every time precipitation falls to earth. People just don’t tend to notice it until it reaches the proportions of disaster.
Under normal circumstances, rain or snowfall soaks back into the earth after falling. It gets absorbed by grasslands, by parks, by residential lawns, by anywhere the soil is exposed. Two factors can impede that absorption. One is large quantities of rain in a short period of time. The ground becomes inundated, and the water spreads out in accordance with the topography. The second is covering over the ground so it cannot soak up water in the first place. And that’s exactly what cities do—they transform the land into developed civilization.
Roads, parking lots, sidewalks, and other pavements, along with asphalt, concrete, brick, stone, and other building materials, combine to create impervious surfaces that resist the natural absorption of water. In most of the United States, about 75 percent of its land area, less than 1 percent of the land is hardscape. In cities, up to 40 percent is impervious.
The natural system is very good at accepting rainfall. But when water hits pavement, it creates runoff immediately. That water has to go somewhere. So it flows wherever the grade takes it. To account for that runoff, people engineer systems to move the water away from where it is originally deposited, or to house it in situ, or even to reuse it. This process—the policy, planning, engineering, implementation, and maintenance of urban water systems—is called stormwater management.
According to my Georgia Institute of Technology colleague Bruce Stiftel, who is chair of the school of city and regional planning and an expert in environmental and water policy governance, stormwater management usually entails channeling water away from impervious surfaces and the structures built atop them. In other words, cities are built on the assumption that the water that would have been absorbed back into the land they occupy can be transported away instead.
Like bridges or skyscrapers designed to bear certain loads, stormwater management systems are conceived within the limits of expected behavior—such as rainfall or riverbank overrun events that might happen every 10 or 25 years. When these intervals are exceeded, and the infrastructure can’t handle the rate and volume of water, flooding is the result.
Houston poses both a typical and an unusual situation for stormwater management. The city is enormous, stretching out over 600 square miles. It’s an epitome of the urban sprawl characterized by American exurbanism, where available land made development easy at the edges. Unlike New Orleans, Houston is well above sea level, so flooding risk from storm surge inundation is low. Instead, it’s rainfall that poses the biggest threat.
A series of slow-moving rivers, called bayous, provide natural drainage for the area. To account for the certainty of flooding, Houston has built drainage channels, sewers, outfalls, on- and off-road ditches, and detention ponds to hold or move water away from local areas. When they fill, the roadways provide overrun. The dramatic images from Houston that show wide, interstate freeways transformed into rivers look like the cause of the disaster, but they are also its solution, if not an ideal one. This is also why evacuating Houston, a metropolitan area of 6.5 million people, would have been a terrible idea. This is a city run by cars, and sending its residents to sit in gridlock on the thoroughfares and freeways designed to become rivers during flooding would have doomed them to death by water.
* * *
Accounting for a 100-year, 500-year, or “million-year” flood, as some are calling Harvey’s aftermath, is difficult and costly. Stiftel confirms that it’s almost impossible to design for these “maximal probable flood events,” as planners call them. Instead, the hope is to design communities such that when they flood, they can withstand the ill effects and support effective evacuations to keep people safe. “The Houston event seems like an illustration that we haven’t figured it out,” Stiftel says.
Many planners contend that impervious surface itself is the problem. The more of it there is, the less absorption takes place and the more runoff has to be managed. Reducing development, then, is one of the best ways to manage urban flooding. The problem is, urban development hasn’t slowed in the last half-century. Cities have only become more desirable, spreading outward over the plentiful land available in the United States.
The National Flood Insurance Program, established in 1968, offered one attempt at a compromise. It was meant to protect and indemnify people without creating economic catastrophe. Instead of avoiding the floodplain, insurance allowed people to build within it, within management constraints recommended by FEMA. In theory, flood-hazard mitigation hoped to direct development away from flood-prone areas through the disincentives of risk insurance and regulatory complexity.
Since then, attitudes have changed. For one part, initial avoidance of floodplains created desirable targets for development, especially in the middle of cities. But for another, Stiftel tells me that attitudes about development in floodplains have changed, too. “It’s more about living with water than it is about discouraging development in areas prone to risk.”
Sometimes “living with water” means sidestepping the consequences. Developers working in flood zones might not care what happens after they sell a property. That’s where governmental oversight is supposed to take over. Some are more strict than others. After the global financial crisis of 2008, for example, degraded local economies sometimes spurred relaxed land-use policy in exchange for new tax bases, particularly commercial ones.
In other cases, floodplains have been managed through redevelopment that reduces impervious surfaces. Natural ground cover, permeable or semi-permeable pavers, and vegetation that supports the movement of water offer examples. These efforts dovetail with urban redevelopment efforts that privilege mixed-use and green space, associated with both new urbanism and gentrification. Recreation lands, conservation lands and easements, dry washes, and other approaches attempt to counterbalance pavement when possible. Stiftel cites China’s “sponge cities” as a dramatic example—a government-funded effort to engineer new, permeable materials to anticipate and mitigate the flooding common to that nation.
* * *
But Thomas Debo, an emeritus professor of city planning at Georgia Tech who also wrote a popular textbook on stormwater management, takes issue with pavement reduction as a viable cure for urban flooding. “We focus too much on impervious surface and not enough on the conveyance of water,” he tells me. Even when reduced in quantity, the water still ends up in in pipes and concrete channels, speeding fast toward larger channels. “It’s like taking an aspirin to cure an ailment,” he scoffs. Houston’s flooding demonstrates the impact.
Instead, Debo advocates that urban design mimic rural hydrology as much as possible. Reducing impervious surface and improving water conveyance has a role to play, but the most important step in sparing cities from flooding is to reduce the velocity of water when it is channelized, so that it doesn’t deluge other sites. And then to stop moving water away from buildings and structures entirely, and to start finding new uses for it in place.
That can be done by collecting water into cisterns for processing and reuse—in some cases, Debo explains, the result can even save money by reducing the need to rely on utility-provided water. Adding vegetation, reclaiming stormwater, and building local conveyance systems for delivery of this water offer more promising solutions.
Though retired from Georgia Tech, Debo still consults on the campus’s local stormwater management efforts. In one case, the institute took a soccer field and made it into an infiltration basin. Water permeates the field, where it is channeled into pipes and then into local cisterns.
In Houston’s case, catastrophic floods have been anticipated for some time. The combination of climate change, which produces more intense and unpredictable storms, and aggressive development made an event like this week’s almost inevitable. The Association of State Floodplain Managers has called for a national flood risk-management strategy, and the Houston Chronicle has called flood control the city’s “most pressing infrastructure need.” A lack of funding is often blamed, and relaxed FEMA regulations under the Trump Administration won’t help either.
But for Debo and others, waiting for a holistic, centralized approach to stormwater management is a pipe dream anyway. Just as limiting impervious surface is not the solution to urban stormwater management, so government-run, singular infrastructure might not be either. “It’s much more difficult, and a much bigger picture,” Debo insists to me. “There is no silver bullet for stormwater management.”
* * *
One problem is that people care about flooding, because it’s dramatic and catastrophic. They don’t care about stormwater management, which is where the real issue lies. Even if it takes weeks or months, after Harvey subsides, public interest will decay too. Debo notes that traffic policy is an easier urban planning problem for ordinary folk, because it happens every day.
So does stormwater—it just isn’t treated that way. Instead of looking for holistic answers, site-specific ones must be pursued instead. Rather than putting a straight channel through a subdivision, for example, Debo suggests designing one to meander through it, to decrease the velocity of the water as it exits.
The hardest part of managing urban flooding is reconciling it with Americans’ insistence that they can and should be able to live, work, and play anywhere. Waterborne transit was a key driver of urban development, and it’s inevitable that cities have grown where flooding is prevalent. But there are some regions that just shouldn’t become cities. “Parts of Houston in the floodway, parts of New Orleans submerged during Katrina, parts of Florida—these places never should have been developed in the first place,” Debo concludes. Add sea-level rise and climate-change superstorms, and something has to give.
Debo is not optimistic about resisting the urge toward development. “I don’t think any of it’s going to happen,” he concedes. “Until we get people in Congress and in the White House who care about the environment, it’s just going to get worse and worse.”
Even so, there’s reason for optimism. If good stormwater management means good, site-specific design, then ordinary people have a role to play, too. Residential homeowners who install a new cement patio or driveway might not even realize that they are channeling water down-grade to their neighbors, or overwhelming a local storm drain. Citizens can also influence stormwater issues within their municipalities. Many folks know that they have a local city council and school board, but local planning, zoning, and urban design agencies also hold regular public meetings—unfortunately, most people only participate in this aspect of local governance when they have an axe to grind. For the average American concerned with the deluge, the best answer is to replace an occasional, morbid curiosity with flooding with a more sophisticated, long-term interest in stormwater management.


Texas continues to battle the ravages of Hurricane Harvey, which has dumped more than 30 inches of rain across the greater Houston area. The record-setting rains have caused devastating floods for the third straight year in the area, following the Tax Day Flood of 2016 and the Memorial Day Flood of 2015.
Houston keeps flooding, but local authorities have long tried to mitigate the risks of living in the Bayou City. Houston is located inside Harris County, and after a series of floods in the 1930s, the county created a flood-control district to regionally manage the area’s flood-preparedness system. Over the years, the Harris County Flood-Control District worked with the Army Corps of Engineers to make the entire city of Houston into a hydraulic machine that can direct water into a series of bayous, canals, and reservoirs, then on into the Gulf of Mexico. There are now 2,500 miles of channels for moving water and a total of $4 billion of flood-mitigation infrastructure.
As bad as things are in Houston right now, they’d be much worse without all this infrastructure in the ground. So, to keep these mitigation measures working, the Harris County Flood Control District spends roughly $100 million per year.
That sounds like a lot, but Houston is now a metropolitan powerhouse. The fourth-largest city in the country, it’s also the primary home of the nation’s oil and gas industry. The area’s real GDP is approaching half a trillion dollars a year.
And consider what some other agencies spend. The South Florida Water Management District spends $664 million per year on its operations. The California Department of Water Resources, which operates the state’s dams and canals, spends more than $500 million per year on “public safety and prevention of damages.”
Or compare to road-infrastructure spending: In Houston itself, the public-works budget is $2 billion this year. In California, the state spends $1.9 billion on highway maintenance alone.
Infrastructure is expensive, which is one reason that even an additional $1 trillion would only be a down payment on what the United States needs to maintain its functioning basic systems. But it’s expensive because it serves the needs of whole populations. Multiply anything by 350 million (or 7 million or 1 million) and you get a pretty big number.
In 2016, the Harris County Water District’s former chief estimated that $26 billion would be necessary to really harden the city against flooding. That would be done by, among other things, widening existing channels to increase the throughput of the water-moving machine.
At the same time, however, the city has continued to pave away the land’s natural capacity to absorb rainwater. One study found that over the last four decades, rainfall had increased 26 percent in the Brays Bayou watershed, but “runoff has skyrocketed by 204 percent.”
The city’s human inhabitants must race to protect themselves from ... themselves. The system was designed for a much smaller city with fewer people and much less runoff.
The system was also designed for a Houston climate that featured less extreme rains. The Texas state climatologist John Nielsen-Gammon has shown that rains have gotten more extreme in the city. “Heavy precipitation of any particular magnitude are twice as likely to fall in the Bayou City today as they were in the early 20th century,” the Houston Chronicle reported.
And though improvements to planning and mitigation would be actual improvements, it’s not clear that even a system with a $26 billion upgrade—or more—could have handled what Harvey brought to the area. The 2016 Tax Day Flood dumped 240 billion gallons of water on the county. Already, Harvey has sent 734 billion gallons to the ground in the same area; according to Flood District calculations, there have been 23.7 inches of rainfall on average across the whole county in the 48 hours since the storm made landfall.
Tropical Storm Allison, which struck in 2001, also brought tremendous rainfall, but only a tiny portion of the county crossed 25 inches of precipitation over the five days of that event.
Harvey will be remembered for the breadth of its rainfall, a burden which shows the cracks in Houston’s infrastructure. To survive on the Gulf Coast in the coming century, the city and its surrounding county are going to have to make the kinds of huge investments in flood mitigation that their forbears did after previous disasters.


As torrential rain continued and floods swelled in Texas this weekend, thousands of people trapped in Harvey’s path started calling 911 and other emergency lines for help. When they couldn’t get through, some started tweeting.
“Please help I’m stranded with my kids I need help fast,” a user wrote to the Houston Police’s account on Twitter.
“Our apartments are surrounded with water like an island we need rescue,” a woman pleaded.
“Please help us she a new born,” a woman wrote above photo of a sleeping baby.
Dozens of pleas like this spread across Twitter and other social-media networks on Sunday as Harvey continued its assault in southeastern Texas. Users included their street addresses, mostly in Houston, which received 25 inches of rain in the last two days. Some took to Twitter because they said they couldn’t get through to 911 and other helplines. Officials in Houston said 911 operators received more than 56,000 calls between Saturday and Sunday nights, a span of time that usually gets 8,000 calls. Emergency services received nearly 6,000 calls for high-water rescues, and more than 1,000 people have been rescued, officials said. In a tweet on Sunday, Houston Police asked anyone with a boat who can help to call the department, providing a phone number that was immediately swarmed with calls.
Social media provided stranded people with a digital megaphone that could be carried far beyond Harvey’s path. Many other users joined the digital search-and-rescue operation and retweeted the voices of the stranded, hoping to buoy their pleas in fast-moving feeds. People offered words of comfort and replied with numbers or Twitter handles for local and national emergency services. Friends and family tweeted on their loved one’s behalf. “Anyone in NE #Houston able to rescue my Aunt? Just had surgery & home filling up w/ water,” a man pleaded. Conversations between flooding victims and concerned Twitter users, some nowhere near Houston, unfolded in real-time as the victims shared updates about water rising around them.
A crowdsourcing rescue effort appeared in the form of @HarveyRescue, a “remote volunteer logging requests for rescues seen on social media,” according to the account’s bio. The account started a Google spreadsheet Sunday and encouraged people to fill in requests for help. By Sunday night, they’d set up a more formal Google form, asking people for their names, phone numbers, and addresses, as well as information about the number of people and pets stranded and whether they had any urgent medical needs. The spreadsheet no longer appears to be visible to users, but on Monday morning there were about 400 requests for rescue. “Mother has 2 small kids & handicap son.” “Family w/ water pouring into home.” “One person is paralyzed.” “5-week-old baby.” About 40 were listed as “resolved.”
Some calls for help in Houston eventually resulted in good news. “Rescued! Thank you one and all!” one woman wrote Sunday night, after sharing an address for three stranded women in their 70s. Others continued to broadcast updates, while some fell silent.
Ed Gonzalez, the sheriff for Harris County, one of the most affected areas, joined in, responding to users seeking help and asking for updates on their situations. But other officials, seeing pleas pile up in their comments, advised people against using social media to register their requests. “Please do not use the HPD social media accounts for rescue requests,” Houston Police tweeted, telling people to 911 for life-threatening emergencies and 311 for help leaving their homes. The U.S. Coast Guard urged the same, tweeting, “Do not report distress on social media.” If phone lines are busy, “please keep trying.”
The use of social media during natural disasters like Harvey has become the norm in the last decade, allowing people to broadcast their safety or peril beyond the confines of a catastrophe. When phone networks crashed during the earthquake and tsunami in Japan in 2011, many people turned to Twitter, Facebook, Skype, and Mixi, a social-networking website in the country. When Manila, the capital of the Philippines, flooded after heavy rains in 2012, residents circulated a Google spreadsheet and crowdsourced aid. When a 7.8-magnitude earthquake shook Nepal in 2015, people around the world combed through Twitter and Facebook posts from the disaster zone and collected reports for help to help guide first responders and charities. More than 8.5 million people in Nepal used Facebook’s Safety Check feature, launched a year earlier, to say they were safe.
The calls for help in Texas, online and offline, will likely grow as Harvey continues its slow churn this week, producing effects that the National Weather Service described in a tweet as “unknown & beyond anything experienced.” Harvey struck the state’s Gulf Coast late Saturday as a Category 4 hurricane and weakened to a tropical storm as it moved inland, but has unleashed nonstop rains, flooding whole neighborhoods and turning roads and highways into rivers that rise by the hour. The storm has resulted in at least five deaths and the displacement of thousands. The National Hurricane Center said Sunday night Texas could see between 15 to 25 inches of rain through Friday, with some parts getting as much as 50 inches—the amount Houston usually sees in an entire year.



It's been ten years since the iPhone came out, and now the first generation to grow up with smartphones is coming of age. Jean Twenge, a psychologist who has studied generational behaviors, has found troubling signals that these devices seem to be taking a visible toll on the mental health of post-Millennials. In the September 2017 issue of The Atlantic, Twenge shares her findings in a story adapted from her new book, iGen: Why Today’s Super-Connected Kids Are Growing Up Less Rebellious, More Tolerant, Less Happy—and Completely Unprepared for Adulthood—and What That Means for the Rest of Us.
In this episode, we talk with Twenge about her findings, hear from a few members of the post-Millennial generation about their relationships with their devices, and discuss what the research means for parents.
Links:


Physicians call it the 5,000-hour problem. If you have a common chronic condition such as cardiovascular disease or diabetes, the expert in charge of your health for almost all of your 5,000 waking hours annually is—you. And, frankly, you won’t always make the best choices.
“The behavior changes that are necessary to address chronic disease are much more in your hands than in the doctor’s,” points out Stacey Chang, executive director of the Design Institute for Health at Dell Medical School in Austin, Texas. “To cede that control to the doctor sometimes is actually counterproductive.”
With that in mind, a rapidly evolving set of new digital health tools is angling to help patients engage better with their own care. Wearable health monitors already on the market help to track heart rate, footsteps, or blood glucose levels; sophisticated home health sensors can report on weight and blood pressure; and phone apps can present key feedback and maybe even offer personalized advice.
The only problem: It has thus far proved very difficult to know what really works.
Indeed, despite a veritable avalanche of “digital health” products, from Fitbits to telehealth heart sensors, and despite floods of data flowing both to the people who use them and to their physicians—and even despite clear evidence that many doctors very much want these new gadgets to work—there is still precious little clinical data proving that they are providing major patient benefits or delivering more cost-effective care.
As with everything else in health care, the reasons for this gap are many, but one major factor is the difficulty of getting patients to embrace new forms of self-monitoring and self-care beyond the novelty period—and to a point where such technology might actually make a difference.
Many early attempts to truly test the efficacy of such digital technologies have shown them to be a flop in clinical trials—in large part because participants drop out. An analysis of five health apps built with Apple iPhone software, for example, found that only about one-eighth of participants, or less, were still hanging in after 10 weeks. Another recent study out of Singapore found that about 200 people outfitted with fitness trackers showed no better health outcomes than a similar control group after a year. And when Cedars-Sinai Medical Center in Los Angeles invited about 66,000 patients registered on its portal to share data from their fitness trackers, less than 1 percent did so, according to a paper published last year in the journal PLOS One, part of the open-access Public Library of Science.
“If you build it,” quipped Brennan Spiegel, an internal-medicine physician and head of the Cedars-Sinai Center for Outcomes Research and Education, “they will not come.”
That’s not entirely surprising, according to Kevin Volpp, director of the University of Pennsylvania’s Center for Health Incentives and Behavioral Economics. “Studies that have looked at ongoing engagement with devices have found that basically people stop using devices that are given to them over time,” he said. “That’s one of the central challenges in efforts to improve health through technology and digital devices. While there have been enormous advances in technology, there’s still a lot of work to be done with the science of habit formation.”
Some clinicians hope that simply doing a better job of educating patients on these new technologies—and on their personal role in using them to improve their own health—can help bridge this gap, but others suggest that won’t be enough. “It’s not a very potent way of changing behavior,” said David Asch, executive director of the University of Pennsylvania Perelman School of Medicine’s Center for Health Care Innovation. Most people on a diet, after all, know what they shouldn’t eat.
One example is a “digital therapeutics” service from Omada Health of San Francisco that targets people at high risk of developing cardiovascular disease or diabetes. In one study, among 1,121 overweight or obese seniors, participants averaged a 6.8 percent reduction in body weight within 26 weeks, and 89 percent of them completed 9 of 16 core lessons—high marks of achievement in this field.
“It’s a very high-tech and high-touch program,” said Sean Duffy, Omada’s co-founder and chief executive. “We mail people a welcome kit that contains a digital scale that has a cellphone chip in it so it’s preregistered to you as a participant. You step on the scale, there’s a beep and the data is in the system.” The system also forms patient-support groups based on location and demographics, and those groups are then paired with health coaches. With patient data flowing continually to Omada, the health coaches can be alerted if someone starts backsliding.
That flow of patient data also allows the service to act as a continuous learning system, with the company’s data scientists studying ways to optimize and personalize interventions to improve behaviors, Duffy added.
Another app, developed by researchers at Johns Hopkins Bayview Medical Center in Baltimore, tackles a very different problem in cardiovascular health: assisting patients recovering from a heart attack.
Traditionally, hospitals saddle patients with a pile of discharge-instruction papers just before they are wheeled down to their cars—“when they couldn’t care less about what we’re giving them,” said Francoise Marvel, an internal medicine physician at Bayview. That failure to communicate can add to patients’ confusion about how to care for themselves during the crucial early days of recovery—one main reason that about a fifth of heart attack patients nationally are rehospitalized within a month after discharge.
The Johns Hopkins “Corrie” app aims to address that by integrating educational videos, medication information, medical appointment tracking, and other crucial aids in an easy-to-use mobile app. Marvel says that early results from a first clinical study of 50 patients have been promising: None were rehospitalized in the first month.
The Corrie app taps not only smartphones and smartwatches, but smart timing. “We can actually enroll patients who are six or seven hours out of having a stent placed in the ICU,” Marvel said. “We’re giving it to patients when they have the time to spend watching the videos, asking questions about their medications, and following their blood pressure and their vital signs—which they will likely need to be taking when they go home.
“We’re getting them to buy in and learn the skills,” she added, “while they care the most.”
Family members can also follow along with a companion Corrie app. “If the adult daughter knows that her dad’s not taking his blood pressure medicine every day,” Marvel said, “she’ll rain down on him in a way that no doctor or no nurse follow-up call could ever compete with.”
* * *
Still, for all the enthusiasm surrounding these technologies, limited independent real-world testing makes it difficult to categorically say what works and what doesn’t—though researchers do suggest that one thing is abundantly clear: Patient acceptance of high-tech self-care and monitoring devices cannot be taken for granted.
“We have to think about how we intervene on behaviors in really different ways, ones that are much more consumer-like,” said Chang. Social networks such as Facebook, for instance, can provide much more pervasive, much more consistent touchpoints than traditional health-care practices and can shift behaviors in striking ways, he says.
In chronic disease, people lack instant gratification for, say, dropping that slice of pizza and eating their damn broccoli. Individual patients need to find personally meaningful ways to motivate themselves, like picturing themselves playing actively with their grandchildren, Chang said. A trained human health coach can help them build this motivational framework, but it remains a real challenge for a mobile app.
Research in behavioral economics can help to optimize more concrete healthy-habit incentives offered by employers or health plans, Volpp said. A clinical study published last year in the Annals of Internal Medicine, for example, either paid participants $1.40 for each day they took 7,000 steps, or gave them a virtual bank account holding 30 days’ worth of those payments—or $42—and then subtracted $1.40 for each day they didn’t walk 7,000 steps. The second method worked appreciably better.
Overall, as our understanding of how to apply digital health technologies matures, the importance of active patient engagement can only grow. “The conventional model is that we can use technology to bring patients closer to doctors,” Asch said. “I think that’s wrong. We have a shortage of doctors, and doctors are one of the most expensive ways of delivering health care.”
“A better and more sustainable solution,” he added, “is to learn how to be less reliant on your doctor and still get good outcomes.”
This article appears courtesy of Undark Magazine.


Playing cards are known and used the world over—and almost every corner of the globe has laid claim to their invention. The Chinese assert the longest pedigree for card playing (the “game of leaves” was played as early as the 9th century). The French avow their standardization of the carte à jouer and its ancestor, the tarot. And the British allege the earliest mention of a card game in any authenticated register.
Today, the public might know how to play blackjack or bridge, but few stop to consider that a deck of cards is a marvel of engineering, design, and history. Cards have served as amusing pastimes, high-stakes gambles, tools of occult practice, magic tricks, and mathematical probability models—even, at times, as currency and as a medium for secret messages.
In the process, decks of cards reveal peculiarities of their origins. Card names, colors, emblems, and designs change according to their provenance and the whims of card players themselves. These graphic tablets aren’t just toys, or tools. They are cultural imprints that reveal popular custom.
* * *
The birthplace of ordinary playing cards is shrouded in obscurity and conjecture, but—like gunpowder or tea or porcelain—they almost certainly have Eastern origins. “Scholars and historians are divided on the exact origins of playing cards,” explains Gejus Van Diggele, the chairman of the International Playing-Card Society, or IPCS, in London. “But they generally agree that cards spread from East to West.”
Scrolls from China’s Tang Dynasty mention a game of paper tiles (though these more closely resembled modern dominoes than cards), and experts consider this the first written documentation of card playing. A handful of European literary references in the late 14th century point to the sudden arrival of a “Saracen’s game,” suggesting that cards came not from China but from Arabia. Yet another hypothesis argues that nomads brought fortune-telling cards with them from India, assigning an even longer antiquity to card playing. Either way, commercial opportunities likely enabled card playing’s transmission between the Far East and Europe, as printing technology sped their production across borders.
In medieval Europe, card games occasioned drinking, gambling, and a host of other vices that drew cheats and charlatans to the table. Card playing became so widespread and disruptive that authorities banned it. In his book The Game of Tarot, the historian Michael Dummett explains that a 1377 ordinance forbade card games on workdays in Paris. Similar bans were enacted throughout Europe as preachers sought to regulate card playing, convinced that “the Devil’s picture book” led only to a life of depravity.
Everybody played cards: kings and dukes, clerics, friars and noblewomen, prostitutes, sailors, prisoners. But the gamblers were responsible for some of the most notable features of modern decks.
Today’s 52-card deck preserves the four original French suits of centuries ago: clubs (♣), diamonds (♦), hearts (♥), and spades (♠). These graphic symbols, or “pips,” bear little resemblance to the items they represent, but they were much easier to copy than more lavish motifs. Historically, pips were highly variable, giving way to different sets of symbols rooted in geography and culture. From stars and birds to goblets and sorcerers, pips bore symbolic meaning, much like the trump cards of older tarot decks. Unlike tarot, however, pips were surely meant as diversion instead of divination. Even so, these cards preserved much of the iconography that had fascinated 16th-century Europe: astronomy, alchemy, mysticism, and history.
Some historians have suggested that suits in a deck were meant to represent the four classes of Medieval society. Cups and chalices (modern hearts) might have stood for the clergy; swords (spades) for the nobility or the military; coins (diamonds) for the merchants; and batons (clubs) for peasants. But the disparity in pips from one deck to the next resists such pat categorization. Bells, for example, were found in early German “hunting cards.” These pips would have been a more fitting symbol of German nobility than spades, because bells were often attached to the jesses of a hawk in falconry, a sport reserved for the Rhineland’s wealthiest. Diamonds, by contrast, could have represented the upper class in French decks, as paving stones used in the chancels of churches were diamond shaped, and such stones marked the graves of the aristocratic dead.
But how to account for the use of clover, acorns, leaves, pikes, shields, coins, roses, and countless other imagery? “This is part of the folklore of the subject,” Paul Bostock, an IPCS council member, tells me. “I don’t believe the early cards were so logically planned.” A more likely explanation for suit marks, he says, is that they were commissioned by wealthy families. The choice of pips is thus partly a reflection of noblemen’s tastes and interests.
* * *
While pips were highly variable, courtesan cards—called “face cards” today—have remained largely unchanged for centuries. British and French decks, for example, always feature the same four legendary kings: Charles, David, Caesar, and Alexander the Great. Bostock notes that queens have not enjoyed similar reverence. Pallas, Judith, Rachel, and Argine variously ruled each of the four suits, with frequent interruption. As the Spanish adopted playing cards, they replaced queens with mounted knights or caballeros. And the Germans excluded queens entirely from their decks, dividing face cards into könig (king), obermann (upper man), and untermann (lower man)—today’s Jacks. The French reintroduced the queen, while the British were so fond of theirs they instituted the “British Rule,” a variation that swaps the values of the king and queen cards if the reigning monarch of England is a woman.
The ace rose to prominence in 1765, according to the IPCS. That was the year England began to tax sales of playing cards. The ace was stamped to indicate that the tax had been paid, and forging an ace was a crime punishable by death. To this day, the ace is boldly designed to stand out.
The king of hearts offers another curiosity: The only king without a mustache, he appears to be killing himself by means of a sword to the head. The explanation for the “suicide-king” is less dramatic. As printing spurred rapid reproduction of decks, the integrity of the original artwork declined. When printing blocks wore out, Paul Bostock explains, card makers would create new sets by copying either the blocks or the cards. This process amplified previous errors. Eventually, the far edge of our poor king’s sword disappeared.
Hand craftsmanship and high taxation made each deck of playing cards an investment. As such, cards became a feast for the eye. Fanciful, highly specialized decks offered artists a chance to design a kind of collectible, visual essay. Playing-card manufacturers produced decks meant for other uses beyond simple card playing, including instruction, propaganda, and advertising. Perhaps because they were so prized, cards were often repurposed: as invitations, entrance tickets, obituary notes, wedding announcements, music scores, invoices—even as notes between lovers or from mothers who had abandoned their babies. In this way, the humble playing card sometimes becomes an important historical document, one that offers both scholars and amateur collectors a window into the past.
While collectors favored ornate designs, gamblers insisted on standard, symmetrical cards, because any variety or gimmickry served to distract from the game. For nearly 500 years, the backs of cards were plain. But in the early 19th century, Thomas De La Rue & Company, a British stationer and printer, introduced lithographic designs such as dots, stars, and other simple prints to the backs of playing cards. The innovation offered advantages. Plain backs easily pick up smudges, which “mark” the cards and make them useless to gamblers. By contrast, pattern-backed cards can withstand wear and tear without betraying a cardholder’s secrets.
Years later, Bostock tells me, card makers added corner indices (numbers and letters), which told the cardholder the numerical value of any card and its suit. This simple innovation, patented during the Civil War, was revolutionary: Indices allowed players to hold their cards in one hand, tightly fanned. A furtive glance offered the skilled gambler a quick tally of his holdings, that he might bid or fold or raise the ante, all the while broadcasting the most resolute of poker faces.
Standard decks normally contain two extra “wild” cards, each depicting a traditional court jester that can be used to trump any natural card. Jokers first appeared in printed American decks in 1867, and by 1880, British card makers had followed suit, as it were. Curiously, few games employ them. For this reason, perhaps, the Joker is the only card that lacks a standard, industry-wide design. He appears by turns the wily trickster, the seducer, the wicked imp—a true calling card for the debauchery and pleasure that is card playing’s promise.
This article appears courtesy of Object Lessons.


On Tuesday, one Robert Lee was punished for the actions of another. When ESPN decided to remove the sports broadcaster Robert Lee from covering the first University of Virginia football game of the season, it was, per the network’s own statement “simply because of the coincidence of his name.” UVA, and its home of Charlottesville, have been embroiled in turmoil over the removal of a statue of Robert E. Lee, and the network thought it best to not evoke the obvious comparisons.
According to a statement from an ESPN executive shared by the journalist Yashar Ali, the move was made to protect Lee from potential “memes and jokes” and Lee himself was involved in the decision.
Regardless, all of this happened because, presumably, Lee’s parents liked the sound of the name Robert.
Both Robert and Lee are extremely common names. According to the website HowManyofMe.com, which searches a database of U.S. Census data, there are 5,128,282 Roberts in the United States, 731,046 people with the last name Lee, and a whopping 11,518 Robert Lees.
“I assume there’s a reason that even Robert E. Lee, the original general, went by his initial,” says Laura Wattenberg, the creator of BabyNameWizard.com, who researches names. “He’s never referred to without that initial.”
Surely some of them were named explicitly for Robert E. Lee, but many—probably most—were not. Wattenberg says that there used to be many people named for General Lee, but nowadays, “homage names are just an endangered species.” If someone chooses to go by the full “Robert E. Lee,” you might reasonably presume that they are trying to play up the Confederate connection, Wattenberg says. But the sports broadcaster Robert Lee is Asian American, and “one knows that broadcaster is not from a family proud of its Confederate ancestry,” she says.
Lee is the 22nd most common last name in the United States, according to the U.S. Census Bureau, and the people who share it are a fairly diverse group. White people make up 40.1 percent of Lees, 37.8 percent are Asian or Pacific Islander, 17.4 percent are black, 1.3 percent are Hispanic, and 1 percent are Native American.
One Robert Lee, who lives in San Francisco and works as a business analyst, didn’t fully understand the significance his name holds in the United States until he went to college. He lived in Hong Kong until he was 18, and then went to Brown University.
Before that, “I knew that [Robert E. Lee] was a general, but I didn’t really know anything about the Civil War or American history at all,” he says. But when people he met at college would casually mention the connection, he asked them about it and learned that way. He says people ask him often what his middle initial is.
“At the beginning I was confused as to why people were asking that question all the time until somebody finally told me,” he says.
In his sophomore year, he became the director of an a cappella group. “I had really high standards and I was used to a strict musical environment,” he says, “so I got the nickname ‘The General.’” He says he didn’t mind it at the time because he didn’t know much about Robert E. Lee. “But after that year I started moving away from it,” he says.
Sharing a name with someone famous (or infamous) means that person will already cast a bit of a shadow over your name. But unlike the 699 Jennifer Lawrences in the United States who are probably just tired of jokey Hunger Games references, the Robert Lees have the larger shadow of America’s fraught history of racism, which endures to the present.
Names shape how we move through the world; they are a shorthand that is used to refer to a person in all their complex humanity. “Robert E. Lee” is shorthand not only for the man himself, but for the Confederacy and all the connotations that carries. It is a way of referring to all the people who served under him who fought to maintain slavery; it is a way of referring to the false alternative history which some would like to preserve, in which Lee was a hero, the Confederacy a romantic lost cause, and the Civil War fought over “states’ rights.” His name has become inextricably linked to the movement to remove statues not only of Lee, but other Confederate figures—and to the white supremacists who violently opposed this removal in Charlottesville.
It might seem silly to think that simply sharing a name with a figure like Lee could have real consequences, but it did for Robert Lee the sports broadcaster. Even without that middle “E.”
“I think ironically ESPN did more harm to the name than the statue did,” Wattenberg says. “Because ESPN just sort of fired a shot across the bow saying that the name Robert Lee—in all of its forms—is now linked to the general and I suspect that a lot of Bob Lees are cursing them right now.”
When I wrote earlier this year about interviewing people who shared my name, Wattenberg told me that names alone really do shape how other people see you. “Your name forms this little pocket of identity around you,” she said. And sometimes, we carry things in that pocket that someone else placed there.
Robert Lee from San Francisco says lately “I have become more aware of the name when I introduce myself. Saying the name out loud, it brings a bunch of different conflicting emotions.”
But “I do like my name,” he says. “I’m named after my godfather, and the last name Lee also runs in the family and represents a lot. I think the importance of your name is really up to you.”


In a corner of Alphabet’s campus, there is a team working on a piece of software that may be the key to self-driving cars. No journalist has ever seen it in action until now. They call it Carcraft, after the popular game World of Warcraft.
The software’s creator, a shaggy-haired, baby-faced young engineer named James Stout, is sitting next to me in the headphones-on quiet of the open-plan office. On the screen is a virtual representation of a roundabout. To human eyes, it is not much to look at: a simple line drawing rendered onto a road-textured background. We see a self-driving Chrysler Pacifica at medium resolution and a simple wireframe box indicating the presence of another vehicle.
Months ago, a self-driving car team encountered a roundabout like this in Texas. The speed and complexity of the situation flummoxed the car, so they decided to build a look-alike strip of physical pavement at a test facility. And what I’m looking at is the third step in the learning process: the digitization of the real-world driving. Here, a single real-world driving maneuver—like one car cutting off the other on a roundabout—can be amplified into thousands of simulated scenarios that probe the edges of the car’s capabilities.
Listen to the audio version of this article:Feature stories, read aloud: download the Audm app for your iPhone.
Scenarios like this form the base for the company’s powerful simulation apparatus. “The vast majority of work done—new feature work—is motivated by stuff seen in simulation,” Stout tells me. This is the tool that’s accelerated the development of autonomous vehicles at Waymo, which Alphabet (née Google) spun out of its “moon-shot” research wing, X, in December of 2016.
If Waymo can deliver fully autonomous vehicles in the next few years, Carcraft should be remembered as a virtual world that had an outsized role in reshaping the actual world on which it is based.
Originally developed as a way to “play back” scenes that the cars experienced while driving on public roads, Carcraft, and simulation generally, have taken on an ever-larger role within the self-driving program.
At any time, there are now 25,000 virtual self-driving cars making their way through fully modeled versions of Austin, Mountain View, and Phoenix, as well as test-track scenarios. Waymo might simulate driving down a particularly tricky road hundreds of thousands of times in a single day. Collectively, they now drive 8 million miles per day in the virtual world. In 2016, they logged 2.5 billion virtual miles versus a little over 3 million miles by Google’s IRL self-driving cars that run on public roads. And crucially, the virtual miles focus on what Waymo people invariably call “interesting” miles in which they might learn something new. These are not boring highway commuter miles.
The simulations are part of an intricate process that Waymo has developed. They’ve tightly interwoven the millions of miles their cars have traveled on public roads with a “structured testing” program they conduct at a secret base in the Central Valley they call Castle.
Waymo has never unveiled this system before. The miles they drive on regular roads show them areas where they need extra practice. They carve the spaces they need into the earth at Castle, which lets them run thousands of different scenarios in situ. And in both kinds of real-world testing, their cars capture enough data to create full digital recreations at any point in the future. In that virtual space, they can unhitch from the limits of real life and create thousands of variations of any single scenario, and then run a digital car through all of them. As the driving software improves, it’s downloaded back into the physical cars, which can drive more and harder miles, and the loop begins again.
To get to Castle, you drive east from San Francisco Bay and south on 99, the Central Valley highway that runs south to Fresno. Cornfields abut subdevelopments; the horizon disappears behind agricultural haze. It’s 30 degrees hotter than San Francisco and so flat that the grade of this “earthen sea,” as John McPhee called it, can only be measured with lasers. You exit near the small town of Atwater, once the home of the Castle Air Force Base, which used to employ 6,000 people to service the B-52 program. Now, it’s on the northern edge of the small Merced metro area, where unemployment broke 20 percent in the early 2010s, and still rarely dips below 10 percent. Forty percent of the people around here speak Spanish. We cross some railroad tracks and swing onto the 1,621 acres of the old base, which now hosts everything from Merced County Animal Control to the U.S. Penitentiary, Atwater.
The directions in my phone are not pointed to an address, but a set of GPS coordinates. We proceed along a tall opaque green fence until Google Maps tells us to stop. There’s nothing to indicate that there’s even a gate. It just looks like another section of fence, but my Waymo host is confident. And sure enough: A security guard appears and slips out a widening crack in the fence to check our credentials.
The fence parts and we drive into a bustling little campus. Young people in shorts and hats walk to and fro. There are portable buildings, domed garages, and—in the parking lot of the main building—self-driving cars. This is a place where there are several types of autonomous vehicle: the Lexus models that you’re most likely to see on public roads, the Priuses that they’ve retired, and the new Chrysler Pacifica minivans.
The self-driving cars are easy to pick out. They’re studded with sensors. The most prominent are the laser scanners (usually called LIDARs) on the tops of the cars. But the Pacificas also have smaller beer-can-sized LIDARs spinning near their side mirrors. And they have radars at the back which look disturbingly like white Shrek ears.
When a car’s sensors are engaged, even while parked, the spinning LIDARs make an odd sound. It’s somewhere between a whine and a whomp, unpleasant only because it’s so novel that my ears can’t filter it out like the rest of the car noises that I’ve grown up with.
There is one even more special car parked across the street from the main building. All over it, there are X’s of different sizes applied in red duct tape. That’s the Level Four car. The levels are Society of Automotive Engineers designations for the amount of autonomy that the car has. Most of what we hear about on the roads is Level One or Level Two, meant to allow for smart cruise control on highways. But the red-X car is a whole other animal. Not only is it fully autonomous, but it cannot be driven by the humans inside it, so they don’t want to get it mixed up with their other cars.
As we pull into the parking lot, there are whiffs of Manhattan Project, of scientific outpost, of tech startup. Inside the main building, a classroom-sized portable, I meet the motive force behind this remarkable place. Her name is Steph Villegas.
Villegas wears a long, fitted white collared shirt, artfully torn jeans, and gray knit sneakers, every bit as fashionable as her pre-Google job at the San Francisco boutique Azalea might suggest. She grew up in the East Bay suburbs on the other side of the hills from Berkeley and was a fine-arts major at University of California, Berkeley before finding her way into the self-driving car program in 2011.
“You were a driver?” I ask.
“Always a driver,” Villegas says.
She spent countless hours going up and down 101 and 280, the highways that lead between San Francisco and Mountain View. Like the rest of the drivers, she came to develop a feel for how the cars performed on the open road. And this came to be seen as an important kind of knowledge within the self-driving program. They developed an intuition about what might be hard for the cars. “Doing some testing on newer software and having a bit of tenure on the team, I began to think about ways that we could potentially challenge the system,” she tells me.
So, Villegas and some engineers began to cook up and stage rare scenarios that might allow them to test new behaviors in a controlled way. They started to commandeer the parking lot across from Shoreline Amphitheater, stationing people at all the entrances to make sure only approved Googlers were there.
“That’s where it started,” she says. “It was me and a few drivers every week. We’d come up with a group of things that we wanted to test, get our supplies in a truck, and drive the truck down to the lot and run the tests.”
These became the first structured tests in the self-driving program. It turns out that the hard part is not really the what-if-a-zombie-is-eating-a-person-in-the-road scenarios people dream up, but proceeding confidently and reliably like a human driver within the endless variation of normal traffic.
Villegas started gathering props from wherever she could find them: dummies, cones, fake plants, kids’ toys, skateboards, tricycles, dolls, balls, doodads. All of them went into the prop stash. (Eventually, the props were stored in a tent, and now at Castle, in a whole storage unit.)
But there were problems. They wanted to drive faster and use streetlights and stop signs. And the concert season at Shoreline Amphitheater regularly threw kinks in their plans. “It was like, ‘Well, Metallica is coming, so we’re gonna have to hit the road,’” she says.
They needed a base, a secret base. And that’s what Castle provided. They signed a lease and started to build out their dream fake city. “We made conscious decisions in designing to make residential streets, expressway-style streets, cul-de-sacs, parking lots, things like that,” she says, “so we’d have a representative concentration of features that we could drive around.”
We walk from the main trailer office to her car. She hands me a map as we pull away to travel the site. “Like at Disneyland, so you can follow along,” she says. The map has been meticulously constructed. In one corner, there is a Vegas-style sign that says, “Welcome to Fabulous Castle, California.” The different sections of the campus even have their own naming conventions. In the piece we’re traveling through, each road is named after a famous car (DeLorean, Bullitt) or after a car (e.g., Barbaro) from the original Prius fleet in the early days of the program.
We pass by a cluster of pinkish buildings, the old military dormitories, one of which has been renovated: That’s where the Waymo people sleep when they can’t make it back to the Bay. Other than that, there are no buildings in the testing area. It is truly a city for robotic cars: All that matters is what’s on and directly abutting the asphalt.
As a human, it feels like a video-game level without the non-player characters. It’s uncanny to pass from boulevards to neighborhood-ish streets with cement driveways to suburban intersections, minus the buildings we associate with these places. I keep catching glimpses of roads I feel like I’ve traveled.
We pull up to a large, two-lane roundabout. In the center, there is a circle of white fencing. “This roundabout was specifically installed after we experienced a multilane roundabout in Austin, Texas,” Villegas says. “We initially had a single-lane roundabout and were like, ‘Oh, we’ve got it. We’ve got it covered.’ And then we encountered a multi-lane and were like, ‘Horse of a different color! Thanks, Texas.’ So, we installed this bad boy.”
We stop as Villegas gazes at one piece of the new addition: Two car lanes and a bike lane run past parallel parking abutting a grass patch. “I was really keen on installing something with parallel parking along it. Something like this happens in suburban downtowns. Walnut Creek. Mountain View. Palo Alto,” she says. “People are coming out of storefronts or a park. People are walking between cars, maybe crossing the street carrying stuff.” The lane was like a shard of her own memory that she’s embedded in the earth in asphalt and concrete, which will make its way into a more abstract form, an improved ability for a robot to handle her home terrain.
She drives me back to the main office and we hop into a self-driving van, one of the Chrysler Pacificas. Our “left-seat” driver is Brandon Cain. His “right-seat” co-driver in the passenger seat will track the car’s performance on a laptop using software called XView.
And then there are the test assistants, who they call “foxes,” a sobriquet that evolved from the word “faux.” They drive cars, create traffic, act as pedestrians, ride bikes, hold stop signs. They are actors, more or less, whose audience is the car.
The first test we’re gonna do is a “simple pass and cut-in,” but at high speed, which in this context means 45 miles per hour. We set up going straight on a wide road they call Autobahn.
After the fox cuts us off, the Waymo car will brake and the team will check a key data point: our deceleration. They are trying to generate scenarios that cause the car to have to brake hard. How hard? Somewhere between a “rats, not gonna make the light” hard stop and “my armpits started involuntarily sweating and my phone flew onto the floor” really hard stop.
Let me say something ridiculous: This is not my first trip in a self-driving vehicle. In the past, I’ve taken two different autonomous rides: first, in one of the Lexus SUVs, which drove me through the streets of Mountain View, and second, in Google’s cute little Firefly, which bopped around the roof of a Google building. They were both unremarkable rides, which was the point.
But, this is different. These are two fast-moving cars, one of which is supposed to cut us off with a move that will be, to use the Waymo term of art, “spicy.”
It’s time to go. Cain gets us moving and with a little chime, the car says, “Autodriving.” The other car approaches and cuts us off like a Porsche driver trying to beat us to an exit. We brake hard and fast and smooth. I’m impressed.
Then they check the deceleration numbers and realize that we had not braked nearly hard enough. We have to do it again. And again. And again. The other car cuts us off at different angles and with different approaches. They call this getting “coverage.”
We go through three other tests: high-speed merges, encountering a car that’s backing out of a driveway while a third blocks the autonomous vehicle’s view, and smoothly rolling to a stop when pedestrians toss a basketball into our path. Each is impressive in its own way, but that cut-off test is the one that sticks with me.
As we line up for another run, Cain shifts in his seat. “Have you ever seen Pacific Rim?” Cain asks me. You know the Guillermo del Toro movie where the guys get synced up with huge robot suits to battle monsters. “I’m trying to get in sync with the car. We share some thoughts.”
I ask Cain to explain what he actually means by syncing with the car. “I’m trying to adjust to the weight difference of people in the car,” he says. “Being in the car a lot, I can feel what the car is doing—it sounds weird, but—with my butt. I kinda know what it wants to do.”
Far from the haze and heat of Castle, there is Google’s comfy headquarters in Mountain View. I’ve come to visit Waymo’s engineers, who are technically housed inside X, which you may know as Google X, the long-term, high-risk research wing of the company. In 2015, when Google restructured itself into a conglomerate called Alphabet, X dropped the Google from its name (their website is literally X.company). A year after the big restructuring, X/Alphabet decided to “graduate” the autonomous vehicle program into its own company as it had done with several other projects before, and that company is Waymo. Waymo is like Google’s child, once removed, or something.
So, Waymo’s offices are still inside the mother ship, though, like two cliques slowly sorting themselves out, the Waymo people all sit together now, I’m told.
The X/Waymo building is large and airy. There are prototypes of Project Wing’s flying drones hanging around. I catch a bit of the cute little Firefly car the company built. (“There’s something sweet about something you build yourself,” Villegas had said back at Castle. “But they had no A/C, so I don’t miss them.”)
Up from the cafeteria, tucked in a corner of a wing, is the Waymo simulation cluster. Here, everyone seems to have Carcraft and XView on their screens. Polygons on black backgrounds abound. These are the people creating the virtual worlds that Waymo’s cars drive through.
Waiting for me is James Stout, Carcraft’s creator. He’s never gotten to speak publicly about his project and his enthusiasm spills out. Carcraft is his child.
“I was just browsing through job posts and I saw that the self-driving car team was hiring,” he says. “I couldn’t believe that they just had a job posting up.” He got on the team and immediately started building the tool that now powers 8 million virtual miles per day.
Back then, they primarily used the tool to see what their cars would have done in tricky situations in which human drivers have taken over control of the car. And they started making scenarios from these moments. “It quickly became clear that this was a really useful thing and we could build a lot out of this,” Stout says. The spatial extent of Carcraft’s capabilities grew to include whole cities, the number of cars grew into a huge virtual fleet.
Stout brings in Elena Kolarov, the head of what they call their “scenario maintenance” team to run the controls. She’s got two screens in front of her. On the right, she has up XView, the screen that shows what the car is “seeing.” The car uses cameras, radar, and laser scanning to identify objects in its field of view—and it represents them in the software as little wireframe shapes, outlines of the real world.
Green lines run out from the shapes to show the possible ways the car anticipates the objects could move. At the bottom, there is an image strip that displays what the regular (i.e., visible-light) cameras on the car captured. Kolarov can also turn on the data returned by the laser scanner (LIDAR), which is displayed in orange and purple points.
We see a playback of a real merge on the roundabout at Castle. Kolarov switches into a simulated version. It looks the same, but it’s no longer a data log but a new situation the car has to solve. The only difference is that at the top of the XView screen it says “Simulation” in big red letters. Stout says that they had to add that in because people were confusing simulation for reality.
They load up another scenario. This one is in Phoenix. Kolarov zooms out to show the model they have of the city. For the whole place, they’ve got “where all the lanes are, which lanes lead into other lanes, where stop signs are, where traffic lights are, where curbs are, where the center of the lane is, sort of everything you need to know,” Stout says.
We zoom back in on a single four-way stop somewhere near Phoenix. Then Kolarov starts dropping in synthetic cars and pedestrians and cyclists.
With a hot key press, the objects on the screen begin to move. Cars act like cars, driving in their lanes, turning. Cyclists act like cyclists. Their logic has been modeled from the millions of miles of public-road driving the team has done. Underneath it all, there is that hyper-detailed map of the world and models for the physics of the different agents in the scene. They have modeled both the rubber and the road.
Not surprisingly, the hardest thing to simulate is the behavior of the other people. It’s like the old parental saw: “I’m not worried about you driving. I’m worried about the other people on the road.”
“Our cars see the world. They understand the world. And then for anything that is a dynamic actor in the environment—a car, a pedestrian, a cyclist, a motorcycle—our cars understand intent. It’s not enough to just track a thing through a space. You have to understand what it is doing,” Dmitri Dolgov, Waymo’s vice president of engineering, tells me. “This is a key problem in building a capable and safe self-driving car. And that sort of modeling, that sort of understanding of the behaviors of other participants in the world, is very similar to this task of modeling them in simulation.”
There is one key difference: In the real world, they have to take in fresh, real-time data about the environment and convert it into an understanding of the scene, which they then navigate. But now, after years of work on the program, they feel confident that they can do that because they’ve run “a bunch of tests that show that we can recognize a wide variety of pedestrians,” Stout says.
So, for most simulations, they skip that object-recognition step. Instead of feeding the car raw data it has to identify as a pedestrian, they simply tell the car: A pedestrian is here.
At the four-way stop, Kolarov is making things harder for the self-driving car. She hits V, a hot key for vehicle, and a new object appears in Carcraft. Then she mouses over to a drop-down menu on the righthand side, which has a bunch of different vehicle types, including my favorite: bird_squirrel.
The different objects can be told to follow the logic Waymo has modeled for them or the Carcraft scenario builder can program them to move in a precise way, in order to test specific behaviors. “There’s a nice spectrum between having control of a scenario and just dropping stuff in and letting them go,” Stout says.
Once they have the basic structure of a scenario, they can test all the important variations it contains. So, imagine, for a four-way stop, you might want to test the arrival times of the various cars and pedestrians and bicyclists, how long they stop for, how fast they are moving, and whatever else. They simply put in reasonable ranges for those values and then the software creates and runs all the combinations of those scenarios.
They call it “fuzzing,” and in this case, there are 800 scenarios generated by this four-way stop. It creates a beautiful, lacy chart—and engineers can go in and see how different combinations of variables change the path that the car would decide to take.
The problem really becomes analyzing all these scenarios and simulations to find the interesting data that can guide engineers to be able to drive better. The first step might just be: Does the car get stuck? If it does, that’s an interesting scenario to work on.
Here we see a video that shows exactly such a situation. It’s a complex four-way stop that occurred in real life in Mountain View. As the car went to make a left, a bicycle approached, causing the car to stop in the road. Engineers took that class of problem and reworked the software to yield correctly. What the video shows is the real situation and then the simulation running atop it. As the two situations diverge, you’ll see the simulated car keep driving and then a dashed box appear with the label “shadow_vehicle_pose.” That dashed box shows what happened in real life. To Waymo people, this is the clearest visualization of progress.
But they don’t just have to look for when the car gets stuck. They might want to look for too-long decision times or braking profiles outside the right range. Anything that engineers are working on learning or tuning, they will simulate looking for problems.
Both Stout and the Waymo software lead Dolgov stressed that there were three core facets to simulation. One, they drive a lot more miles than would be possible with a physical fleet—and experience is good. Two, those miles focus on the interesting and still-difficult interactions for the cars rather than boring miles. And three, the development cycles for the software can be much, much faster.
“That iteration cycle is tremendously important to us and all the work we’ve done on simulation allows us to shrink it dramatically,” Dolgov told me. “The cycle that would take us weeks in the early days of the program now is on the order of minutes.”
Well, I asked him, what about oil slicks on the road? Or blown tires, weird birds, sinkhole-sized potholes, general craziness. Did they simulate those? Dolgov was sanguine. He said, sure, they could, but “how high do you push the fidelity of the simulator along that axis? Maybe some of those problems you get better value or you get confirmation of your simulator by running a bunch of tests in the physical world.” (See: Castle.)
The power of the virtual worlds of Carcraft is not that they are a beautiful, perfect, photorealistic renderings of the real world. The power is that they mirror the real world in the ways that are significant to the self-driving car and allow it to get billions more miles than physical testing would allow. For the driving software running the simulation, it is not like making decisions out there in the real world. It is the same as making decisions out there in the real world.
And it’s working. The California DMV requires that companies report the miles that they’ve driven autonomously each year along with disengagements that test drivers make. Not only has Waymo driven three orders of magnitude more miles than anyone else, but their number of disengagements have fallen quickly.
Waymo drove 635,868 autonomous miles from December 2015 to November 2016. In all those miles, they only disengaged 124 times, for an average of about once every 5,000 miles, or 0.20 disengagements per 1,000 miles. The previous year, they drove 424,331 autonomous miles and had 272 disengagements, for an average of once every 890 miles, or 0.80 disengagements per 1,000 miles.
While everyone takes pains to note that these are not exactly apples-to-apples numbers, let’s be real here: These are the best comparisons we’ve got and in California, at least, everybody else drove about 20,000 miles. Combined.

The tack that Waymo has taken is not surprising to outside experts. “Right now, you can almost measure the sophistication of an autonomy team—a drone team, a car team—by how seriously they take simulation,” said Chris Dixon, a venture capitalist at Andreessen Horowitz who led the firm’s investment in the simulation company Improbable. “And Waymo is at the very top, the most sophisticated.”
I asked Allstate Insurance’s head of innovation, Sunil Chintakindi, about Waymo’s program. “Without a robust simulation infrastructure, there is no way you can build [higher levels of autonomy into vehicles].” he said. “And I would not engage in conversation with anyone who thinks otherwise.”
Other self-driving car researchers are also pursuing similar paths. Huei Peng is the director of Mcity, the University of Michigan’s autonomous- and connected- vehicle lab. Peng said that any system that works for self driving cars will be “a combination of more than 99 percent simulation plus some carefully designed structured testing plus some on-road testing.”
He and a graduate student proposed a system for interweaving road miles with simulation to rapidly accelerate testing. It’s not unlike what Waymo has executed. “So what we are arguing is just cut off the boring part of driving and focus on the interesting part,” Peng said. “And that can let you accelerate hundreds of times: A thousand miles becomes a million miles.”
What is surprising is the scale, organization, and intensity of Waymo’s project. I described the structured testing that Google had done to Peng, including the 20,000 scenarios that had made it into simulation from the structured testing team at Castle. But he misheard me and began to say, “Those 2,000 scenarios are impressive,”—when I cut in and corrected him—“It was 20,000 scenarios.” He paused. “20,000,” he said, thinking it over. “That’s impressive.”
And in reality, those 20,000 scenarios only represent a fraction of the total scenarios that Waymo has tested. They’re just what’s been created from structured tests. They have even more scenarios than that derived from public driving and imagination.
“They are doing really well,” Peng said. “They are far ahead of everyone else in terms of Level Four,” using the jargon shorthand for full autonomy in a car.
But Peng also presented the position of the traditional automakers. He said that they are trying to do something fundamentally different. Instead of aiming for the full autonomy moon shot, they are trying to add driver-assistance technologies, “make a little money,” and then step forward toward full autonomy. It’s not fair to compare Waymo, which has the resources and corporate freedom to put a $70,000 laser range finder on top of a car, with an automaker like Chevy that might see $40,000 as its price ceiling for mass-market adoption.
“GM, Ford, Toyota, and others are saying ‘Let me reduce the number of crashes and fatalities and increase safety for the mass market.’ Their target is totally different,” Peng said. “We need to think about the millions of vehicles, not just a few thousand.”
And even just within the race for full autonomy, Waymo now has more challengers than it used to, Tesla in particular. Chris Gerdes is the director of the Center for Automotive Research at Stanford. Eighteen months ago, he told my colleague Adrienne LaFrance that Waymo “has much greater insight into the depth of the problems and how close we are [to solving them] than anyone else.” When I asked him last week if he still thought that was true, he said that “a lot has changed.”
“Auto manufacturers such as Ford and GM have deployed their own vehicles and built on-road data sets,” he said. “Tesla has now amassed an extraordinary amount of data from Autopilot deployment, learning how the system operates in exactly the conditions its customers experience. Their ability to test algorithms on board in a silent mode and their rapidly expanding base of vehicles combine to form an amazing testbed.”
In the realm of simulation, Gerdes said that he had seen multiple competitors with substantial programs. “I am sure there is quite a range of simulation capabilities but I have seen a number of things that look solid,” he said. “Waymo no longer looks so unique in this respect. They certainly jumped out to an early lead but there are now a lot of groups looking at similar approaches. So it is now more of a question of who can do this best.”
This is not a low-stakes demonstration of a neural network’s “brain-like” capacities. This is making a massive leap forward in artificial intelligence, even for a company inside Alphabet, which has been aggressive in adopting AI. This is not Google Photos, where a mistake doesn’t mean much. This is a system that will live and interact in the human world completely autonomously. It will understand our rules, communicate its desires, be legible to our eyes and minds.
Waymo seems like it has driving as a technical skill—the speed and direction parts of it—down. It is driving as a human social activity that they’re working on now. What is it to drive “normally,” not just “legally”? And how does one teach an artificial intelligence what that means?
It turns out that building this kind of artificial intelligence does not simply require endless data and engineering prowess. Those are necessary, but not sufficient. Instead, building this AI requires humans to sync with the cars, understanding the world as they do. As much as anyone can, the drivers out at Castle know what it is to be one of these cars, to see and make decisions like them. Maybe that goes both ways, too: The deeper humans understand the cars, the deeper the cars understand humans.
A memory of a roundabout in Austin becomes a piece of Castle becomes a self-driving car data log becomes a Carcraft scenario becomes a web of simulations becomes new software that finally heads back out on a physical self-driving car to that roundabout in Texas.
Even within the polygon abstraction of the simulation the AI uses to know the world, there are traces of human dreams, fragments of recollections, feelings of drivers. And these components are not mistakes or a human stain to be scrubbed off, but necessary pieces of the system that could revolutionize transportation, cities, and damn near everything else.


If you use Instagram, you have seen an exhortation from a friend or colleague to check out some article or event. These calls to action inevitably end, “Link in Bio.”  
That’s shorthand, of course, for the single link that Instagram allows users to drop into their profiles. Because other links can’t be added to posts, that single link is an endorsement: It must be the one URL in the world that you are willing to attach to yourself at that moment.
For years, I’ve wondered why Instagram doesn’t allow links elsewhere. It would be so simple. (I reached out to Instagram for comment, but they didn’t respond.) The Ringer’s Alyssa Bereznak dressed down the company in a post titled, appropriately, “‘Link in Bio’ Is the Worst Thing About Instagram.”
“A network that hosts millions of people won’t let them do something that is second nature for digital natives. So its users have concocted their own clunky loophole to get around the problem,” Bereznak writes. “It’s as if there were a permanent snowstorm in a city, and the mayor refused to clear the sidewalks. Inevitably, pedestrians would just stomp out their own inelegant roundabout paths to navigate the dirty, urine-filled slush.”
Other writers have called Link in Bio “dreaded,” “clumsy,” and “clunky at best.”
And yet, Instagram crushes on, adding users by the hundred millions. The reason is simple: People, like myself, like Instagram. It is a plain like, uncomplicated. In 2015, when my colleague Rob Meyer wrote the definitive post about liking Instagram, “I Like Instagram,” he laid out its excellent lack of features:
It is a silly, idiosyncratic piece of software, but so simple. It says: Here is a picture. Here is a picture of a weird bird my friend saw. Here is a picture of my friend celebrating Eid with her brother. Here is a picture of an acquaintance flying over the city where I used to live.
With every photo, I have two options. I can scroll by, or I can say “I saw this and liked it.” Either way, then I scroll some more. It is a place to look at pictures and, maybe, video. It does not do much else. It doesn’t need to. It is so simple as to be almost serene.
It was true back then! The app’s simplicity seemed to be its heart. But many things have changed about Instagram in the intervening two years. There are now stories, daring you to step into their circles. The formerly chronological feed has been Facebooked. Even back then, Instagram had already added private messaging to its basic function of picture posting.
But the basic feeling people have about Instagram remains the same. They might be annoyed by certain aspects of the app, but they still like it, basically, and in the flat way that Meyer captured perfectly. “Oh, that was nice” is something one could imagine saying after looking at Instagram.
Not so for Facebook or Twitter. On those platforms, I feel like I’ve been snookered into emoting. The reason for this, I would argue, is simple: Instagram has never become a full participant in the web. By refusing to allow hyperlinks, it has maintained a distinct space on the internet. Twitter and Facebook expanded to become a messy, permeable front end for the whole of the web (even as they try to claw ever more video minutes/ads into their players).
And what is on the web right now is, more or less, politics. The Trump era has meant that Americans are talking about politics perpetually, endlessly, circuitously, directly, boringly, excitedly. Given the circumstances and stakes, that’s not necessarily a bad thing. But as “the conversation” goes, so follows the content, bucketing arguments, relating to your feelings, connecting popular culture with political theater. I don’t know anyone who isn’t exhausted, at least some of the time.
Then there is Instagram, where the documentation that life goes on doesn’t feel out of place. Like a recipe book written in 1944, Instagram declares: Still gotta eat! Like sunset pics from 1968: Still the world turns!
You can trace this right back to Link in Bio. Mobile apps are supposed to let the users do what they want, what they demand. Their paths of desire must be seamless and easy. Friction is the enemy.
But what if friction is necessary for the long-term health of these social systems? What if the platforms sometimes need to do the thing that generates lower short-run “engagement”? What if social networks now need dampening, not amplification?
The outright denial of user desire is also a good reminder of what these spaces actually are. The New York Times’ John Herrman calls the big platforms a “commercial simulation” of freedom. Instagram does not pretend to be part of the public sphere. It is not the natural home of #theresistance. It’s a place for the Sunday’s-best version of your personal life to have space on the internet.
In recent months, Instagram has experimented with letting verified users link out to the web from inside their stories. It solves the Link-in-Bio problem like that. But once the web starts to creep in, will its exhausting dynamics follow?


After Charlottesville, white supremacists’ physical and digital presences—and the tactics used to combat them—are under renewed scrutiny.
There have been attempts, most prominently by Logan Smith, who runs the Twitter account Yes, You’re Racist, to tack real names and identities onto the pictures of people who showed up to rally for white supremacy in the city. Some people have called this “doxxing,” referring to the practice of figuring out the identity of an anonymous or pseudonymous person online and publishing their IRL details, usually to spur harassment.
The technique has been used online for decades. It gained widespread notoriety during the Gamergate saga, when a variety of unsavory far-right-wing figures doxxed women in the video-game media, but people all over the political spectrum have done their fair share. That’s led a lot of reasonable people to declare the tactic verboten, more or less, no matter the target. It’s just beyond the pale.
What Smith is doing, however, is not doxxing as it has been understood. His targets showed up at a public rally. They made no attempt to hide their identities.
The merging of the digital and physical worlds does make things more complicated. One can imagine that the rally participants did not expect national scrutiny before the event. They showed up with a few hundred people and have ended up in pictures seen by tens of millions online. This is a variation on what academics have called “context collapse.” Smith is doing that collapsing, making white supremacists accountable for their views and speech in a greater portion of their lives.
At the same time, their outing digitally opens them up to the dynamics of online vigilantism: jeers, threats, and more.
“If you’re going to go out in public and advocate for Nazi ideas, you have to be prepared for people to say, ‘You’re a terrible person,’” said Sasha Costanza-Chock, a Massachusetts Institute of Technology associate professor of civic media. “I don’t think there is much defensible in saying we shouldn’t do that. We might want to preserve the term ‘doxxing’ having a specific meaning, but identifying and mapping extreme-right networks—we should agree that’s reasonable to do.”
Smith’s actions are certainly on the same continuum as doxxing, but not precisely in the same spot, and Costanza-Chock asked, what else is on that continuum? Perhaps the Southern Poverty Law Center’s research into hate groups (or Political Research Associates’ report on the rise of the “alt-right”)? Maybe a local reporter digging into a government official who is secretly in the Ku Klux Klan?
When people say “the solution to hate speech is more speech”—might this fit the bill as the “more speech” part? It’s imperfect, sometimes painful, but ultimately part of the universe of speech that free-speech advocates hope will result in a better society.
Maybe.
But there are established rules and norms, as well as (some) accountability processes for journalists and researchers, which don’t tend to exist among doxxers.
And online searches like this are very prone to error. This sort of thing has been happening for years—and the internet sleuths, as local news stories love to tag them, have often gone awry. Smith has misidentified some of the people in the photos, with predictably bad consequences. A poor tissue engineer at the University of Arkansas got randomly dragged into the mess.
Lucas Wright and Susan Benesch, of the Dangerous Speech Project, write on the project’s blog that searches like this will always encounter similar problems, by malfeasance or actual bad actors feeding false information into the system. “These false positives are inevitable since the strategy relies on imperfect information—yet can seriously disrupt a target’s life,” they write. “As it becomes easier to forge realistic fake videos, such errors will only become more common.”
Costanza-Chock argued that some guide should exist for people wishing to engage in Nazi doxxing that would instruct them on how to do so as responsibly as possible and offer other routes to grapple with their online accounts.
But Wright and Benesch reject the tactic more completely, at least from the perspective of trying to de-radicalize white supremacists. “Disrupting their lives—getting them fired from their jobs, disowned by their parents, or dogpiled with threats on Twitter—may give a satisfying jolt of schadenfreude, but it also cuts them off from the remaining moderating forces in their lives,” they write. “When that happens, they will not learn to love; they will only commit further to the dangerous communities that are willing to embrace them.”
And that may be true. There may be a tradeoff between raising the costs of doing white supremacy out there in the world—and increasing the radicalization of those who Smith and others identify.


A young girl stands alongside a wood-canvas canoe. She wraps the tails of a long, leather strap around one of the canoe’s horizontal struts in preparation for a two-mile portage to the next lake. She swings the 100-pound, waterlogged canoe up over her head in a single fluid motion, resting it upside down on her shoulders, one end pointed toward the sky. Then she pulls the leather strap behind her hairline like a bandanna, adjusts her stance so the weight of the canoe is channeled smoothly down her neck and spine, and starts walking the rocky trail.
Methods of “head carry,” or weighting a load on top of the head, are standard across the developing world. The best-known images of the technique show African women trekking miles to and from water sources. Despite 10-gallon buckets balanced precariously above them, their backs are straight as a rod and their chins lifted. Often, heavy loads aren’t only carried on the heads, but suspended from it by a strap, called a tumpline. Light, internal-frame backpacks have largely replaced tumplines among modern folk who carry loads outdoors. But when done properly, head carry can be safer, more efficient, and more functional than supposedly better, newer technologies.
* * *
Looking for the inventor of head-carry devices and techniques is like asking who invented shoes. No single origin story exists for a piece of leather, fabric, or rope that is knotted, looped, or buckled around a load and worn across the top of the head. The tumpline precedes even the backpack. It has been used across every populated continent. Congolese women use tumplines to carry charcoal and firewood. Sherpas, perhaps the most famous tumpline users today, have been known to eschew modern packs even when they are offered, preferring the simple strap when carrying gear in the Himalayas. They call tumplines namlo and carry up to their own body weight in baskets called doko.
The English word “tumpline” is thought to be a shortening of the Algonquian words mattump or metump, and it entered the Western lexicon alongside trade. Fur-trapping voyageurs and coureurs de bois learned the method from their Native allies in the 17th and early 18th centuries. It was the only feasible way to carry large bundles of furs and gear through the dense forests between navigable bodies of water in what is now Canada and the northernmost United States.
In 1882, the French Canadian shoemaker Camille Poirier brought the tumpline to the North American masses when he created the Duluth pack. This pack kept the familiar shape of the backpack, with shoulder straps and a fixed-volume storage compartment, but added a tumpline attachment that allowed for the pack to be weighted on the shoulders, the neck, or both. The Duluth pack quickly became a classic; it is still manufactured in Duluth, Minnesota, and imitated by outdoor outfitters around the world.
By the early 20th century, the tumpline’s influence had spread from outfitters to the military. During World War I, in the trenches of the Somme, Staff Captain F.R. Phelan formed the 11th Canadian Brigade Tumpline Company after showing how much manpower and time could be saved if resupplies of the muddy trenches were conducted by tumpline. Phelan had learned the technique while hunting and fishing in the wilderness of Quebec, mirroring the journeys and challenges of the voyageurs.
Phelan’s men were issued an oiled leather tumpline with two long tails. The tails could be tied around larger loads than would fit in a backpack or in the hands. It also could be worn without interfering with their helmets. The men were trained in knots and proper posture, and they slowly increased the weight of their loads until they were carrying twice what had previously been possible, and with equal or greater efficiency. The tumpline made the dangerous process of resupplying the trenches faster and safer.
By 1944, tumpline companies had become standard across the Canadian ground forces. A Popular Mechanics article from that year shows Canadian soldiers using tumplines to carry medical supplies, machine guns, and even to pull sled-style stretchers designed for moving casualties while under fire during World War II.
* * *
In the second half of the 20th century, the rise of performance outdoor gear threatened to extinguish the tumpline outside of traditional communities. Backpacks are not a modern creation, but the internal-frame pack, invented by Greg Lowe in 1967, was a watershed moment in modern outdoorsmanship. Tumplines had never been ubiquitous on packs, but the creation of a less bulky pack that comfortably loaded weight on the hips rather than the shoulders made it easy to justify removing tumpline attachments from backpack designs.
Advances in padding made packs more comfortable. Improved textiles increased waterproofness. And better design made them more space-efficient. In the process, backpacks became commodities—mass-produced, but also one-size-fits-most. By contrast, tumplines are precision tools that need to be adjusted carefully to fit each user. Wearers need to be trained in proper posture and technique. They must carefully ease into heavier loads as they build up muscles in their necks and backs. Backpacks can be treated as precision tools as well, but the barrier to entry is much lower. Shoulder straps are simple and intuitive; they slip right on. Hip belts are equally self-explanatory, and the flaws of the modern backpack have mostly been overlooked in favor of convenience and fashion.
This state of affairs makes Yvon Chouinard an outlier. He is the founder of Patagonia, a company that makes some of the fanciest gear in the outdoor sports world. And even though he built an empire on $900 parkas and $500 sleeping bags, Chouinard still swears by the tumpline. His company sells a simple nylon version of the product for under $20.
Chouinard took up the tumpline in 1968, after sustaining a neck injury in the jungles of Colombia that resulted in severe recurring back pain. A decade later, during an expedition to Nepal, he saw that the porters were carrying twice as much as the climbers with much simpler gear. Chouinard began training with the tumpline, and it proved an effective a solution. To this day, the founder of a company some critics have nicknamed “Patagucci” makes use of an ancient tool, claiming that he’d never go back to using a conventional pack without adding on a tumpline.
Despite the advocacy of industry leaders like Chouinard, the proven efficacy by some of the world’s strongest athletes and Sherpas, and hundreds of years of documented use, the most common argument against tumplines is that they aren’t safe. The claim is not without merit, but impatience is more to blame than intrinsic risk. If a runner were suddenly to run barefoot on a dirt trail, they’d likely get injured. Feet accustomed to structured cocoons of cushioned safety do not take well to the shock of having to work in new ways. Likewise, hoisting 50 pounds atop an unstrengthened spine without training guidance can also lead to harm.
Numerous studies show that tumplines and other head-carry techniques are more metabolically efficient and physically healthy than the supposedly high-tech successors that fill today’s gear shops. African women have been found to carry loads of up to 60 percent of their body weight on their heads more economically than army recruits with a backpack of an equivalent weight. Nepalese porters with a tumpline have been found to be 60 percent faster and 39 percent more powerful than their clients carrying modern packs.
In 2007, the Outside magazine contributor Eric Hansen wrote about how he had tested out the economy of the tumpline for himself. After convincing a team of Nepalese porters to let him into their ranks, he was fitted with a rope tumpline and loaded up. The experience wasn’t fun or comfortable, but it did question prevailing norms in the world of outdoor sports. Whereas a client might struggle with a 55-pound load, that was the absolute minimum that a porter would carry for the same distance.
* * *
Back in Northern Ontario, a group of kids from Keewaydin sets out on a canoe trip. Their camp is committed to preserving “the old ways”: They are portaging a month’s worth of gear, food, and the canoes needed to carry it all by tumpline. It’s not just for the sake of nostalgia or grit-building, either. The old technology makes efficient use of the limited space inside of a canoe, for one thing. For another, it’s more logical than the complicated contraptions people have dreamt up for portaging boats. There are rolling carts, backpack-style frames, and yokes—but a simple strap still suffices.
The wilderness-canoe guide and writer Cliff Jacobson recommends that backpackers and canoe-trippers take a piece from each tradition—a hip belt for added support when going downhill, shoulder straps for stability, and a tumpline modification for uphill climbs. Like Chouinard, he claims that he’d never own a pack without a tumpline. “Everest Sherpas use tumplines,” he writes, “but Americans still insist on backpacks with hip belts. You tell me: Who’s the professional?”
With simple technologies staging heroic comebacks, maybe the tumpline will enjoy a 21st-century resurgence. In the meantime, native communities and passionate advocates who choose function over fashion and long-term safety over short-term comfort keep this traditional method of carrying things on the head from going extinct.
This article appears courtesy of Object Lessons.


Last night, Tucker Carlson took on the subject of slavery on his Fox News show. Slavery is evil, he noted. However, slavery permeated the ancient world, he said, as reflected in the on-screen graphics.
On Twitter, recent University of Toronto English Ph.D. graduate Anthony Oliveira noted, “Here's Tucker Carlson right now on Fox making the *exact* pro-slavery case (bad but status-quo and well-precedented) made 160 years ago.”
It sounds like a particular variety of Twitter gallows humor, not meant to be taken quite seriously. But it is not a joke.
This precise series of ostensible mitigating factors around the institution of American slavery were, in fact, advanced by pro-slavery forces through the 19th century. And it got me wondering: Given that The Atlantic was founded as an abolitionist magazine before the Civil War, might there be an article or two that might address Carlson’s warmed-over proto-Confederate arguments?
And indeed, there are.
Take Carlson’s bullet point, “Until 150 years ago, slavery was rule.”
Well, yes. Slavery was legal in some American states. But how did this happen, especially when other countries began abolishing slavery early in the 19th century? In our second issue, Edmund Quincy put his pen to “Where Will It End?” And he doesn’t mess around. Slavers had power because they went on bloody conquests to open up new territory for slavery.
The baleful influence thus ever shed by Slavery on our national history and our public men has not yet spent its malignant forces. It has, indeed, reached a height which a few years ago it was thought the wildest fanaticism to predict; but its fatal power will not be stayed in the mid-sweep of its career ... Slavery presiding in the Cabinet, seated on the Supreme Bench, absolute in the halls of Congress—no man can say what shape its next aggression may not take to itself. A direct attack on the freedom of the press and the liberty of speech at the North, where alone either exists, were no more incredible than the later insolences of its tyranny ... The rehabilitation of the African slave-trade is seriously proposed and will be furiously urged, and nothing can hinder its accomplishment but its interference with the domestic manufactures of the breeding Slave States ... Mighty events are at hand, even at the door; and the mission of them all will be to fix Slavery firmly and forever on the throne of this nation.
Indeed, in the early days of The Atlantic, the violent battle over whether Kansas would become a slave state raged. In the “Kansas Usurpation,” from Issue 4, our author details the endless skulduggery that slavers perpetrated “to force the evils of slavery upon a people who cannot and will not endure them.”
And how about the idea that ancient peoples also held slaves? The Atlantic didn’t address Greek slaveholding, but takes on their admirers, the Romans. In a piece called “Spartacus,” published in Issue 3 in 1858, the author explicitly differentiates the Roman version of slavery from the American.
“Fowell Buxton has happily translated [the Roman motto], ‘They murdered all who resisted and enslaved the rest.’ But it was as slaveholders that the Romans most clearly exhibited their impartiality,” the piece states. “They were above those miserable subterfuges that are so common with Americans. They made slaves of all, of the high as well as the low—of Thracians, as well as Sardinians, of Greeks and of Syrians as readily as of Scythians and Cappadocians.”
With ever-increasing rigor from colonial times, the American system explicitly made only people with African ancestry subject to chattel slavery, i.e. they were the only people whose children were born enslaved and who would die enslaved, absent an extraordinary circumstance. American slavery was different.
To be clear, this isn’t just about Carlson. My target is the implicit idea that American slavery was not historically, distinctly terrible. It was. There is no parallel. While other countries—and states within the Union—were banning slavery, the South was intensifying slavery in several different ways.
First, the ideological and theological interpretation of slavery in the South began to change. The specific and perpetual enslavement of African people had seemed to Jeffersonian Americans as an evil that was ebbing away. “In the late 18th century, most Americans believed that slavery, as institutionalized dependence, was neither good nor practical, and so would fade before the action of natural forces under the new, free political system,” writes John Patrick Daly in When Slavery Was Called Freedom: Evangelicalism, Proslavery, and the Causes of the Civil War.
But as abolitionists began to succeed in the northern states, chattel slavery of black human beings began to be theologically promoted as something to be proud of, possibly even holy, in the South. “Good slaveholders, they maintained, gave the institution its character—that is, goodness,” Daly writes. “This formulation allowed proslavery spokesmen to denounce the historically evil institution of slavery while defending Southern practices: Slaveholders in the evil form of slavery were bad men; the Southerners were good, and the source of their wealth untainted. Good—and especially evangelical—slaveholders supposedly redeemed the institution of slavery.”
Second, the old colonial state slaveowners were making a business out of selling the people they enslaved south and west. This became a lynchpin of the region’s wealth as agriculture declined there. Black people were chained together in Virginia and the Carolinas and marched to Georgia, to Florida, to Mississippi, to Texas. Whatever networks of family and community they’d been able to build within the oppressive violence of slavery were destroyed (again).
Ed Baptist tells this story in The Half Has Never Been Told: Slavery and the Making of American Capitalism. “The massive and cruel engineering required to rip a million people from their homes, brutally drive them to new, disease-ridden places, and make them live in terror and hunger as they continually built and rebuilt a commodity-generating empire,” he writes, “this vanished in the story of a slavery that was supposedly focused primarily not on producing profit but on maintaining its status as a quasi-feudal elite, or producing modern ideas about race in order to maintain white unity and elite power.”
Third, the gin-powered cotton economy relied on huge financial investments to open up new cotton land ever farther south and west. A series of financial bubbles ran in those directions, with literal securities issued to slaveowners secured by the bodies of enslaved people.
“African American bodies and childbearing potential collateralized massive amounts of credit, the use of which made slaveowners the wealthiest people in the country,” write Ned and Constance Sublette in The American Slave Coast: A History of the Slave-Breeding Industry. “When the Southern states seceded to form the Confederacy they partitioned off, and declared independence for, their economic system in which people were money.”
To make their loan payments, these speculator-slavers created the brutal “whipping machine,” which drove massive productivity gains at the expense of the health and well-being of the already oppressed people working in the fields.
”The returns from cotton monopoly powered the modernization of the rest of the American economy, and by the time of the Civil War, the United States had become the second nation to undergo large-scale industrialization,” Baptist writes. “In fact, slavery’s expansion shaped every crucial aspect of the economy and politics of the new nation—not only increasing its power and size, but also, eventually, dividing U.S. politics, differentiating regional identities and interests, and helping to make civil war possible. The idea that the commodification and suffering and forced labor of African Americans is what made the United States powerful and rich is not an idea that people necessarily are happy to hear. Yet it is the truth.”
It was this marriage of new ideological underpinning, the incredible profits the gin-powered cotton industry could produce, and the new modes of capitalization and management that American slaveowners developed that make American slavery different and worse from those that preceded it.
The drive to keep opening up cotton land to feed the slaver-speculator economy also led to genocidal atrocities against Native Americans, as well as the imperial project of snatching the western part of the continent from Mexico, which had abolished slavery in the 1820s.
In April 1861, with the slaveholder’s rebellion beginning, The Atlantic published an essay by Charles Francis Adams Jr., the grandson of John Quincy Adams, called “The Reign of King Cotton.”
“Throughout the South, whether justly or not, it is considered as well settled that cotton can be profitably raised only by a forced system of labor,” Adams wrote. “With this theory, the Southern States are under a direct inducement, in the nature of a bribe, to the amount of the annual profit on their cotton-crop, to see as many perfections and as few imperfections as possible in the system of African slavery.”
But the bribe didn’t stop getting paid at the Mason-Dixon line. Even New England, hotbed of abolitionism and birthplace of this magazine, got rich on textiles spun in the factories along the Merrimack. Where do you think they got the cotton for the City of Spindles? Baptist tells the story of the Collins Axe Works, which sold hundreds of thousands of axes into the western parts of the South, where they were given to enslaved black people to clear the forests. Hundreds of millions of trees fell through black labor performed with these axes. And back on the Farmington River, a white factory owner and his associates got rich.
“All told, more than $600 million, or almost half of the economic activity in the United States in 1836, derived directly or indirectly from cotton produced by the million-odd slaves—6 percent of the total U.S. population—who in that year toiled in labor camps on slavery’s frontier,” Baptist calculates.
There is no escaping the basic facts of our history. Plato, Muhammad, and the Aztec empire did not have the cotton gin or the luxuries that came from the securitization of enslaved people. Native American slaveholders didn’t shape and take advantage of emergent American capitalism to subdue a continent.
Given all this, no wonder the neo-Confederates keep fighting to keep their heroic monuments. Understanding the breadth and depth of the American slavery’s evil would undermine not just their dedication to busts of Robert E. Lee, but the whole moral project of seeing whiteness as a sign of virtue.
This is what Confederate flag wavers mean when they say they are “fighting for their heritage.” They are fighting for the right to declare their ancestors good, despite the evidence of the horrors they perpetrated, which rival anything that happened in the 20th century.
And what they’re counting on is that Americans, no matter when their families arrived across seas or rivers, will excuse the Confederate flag-wavers because they want to believe only the best stories about our country, too.
There is no excuse. That other people at other times owned slaves—Greek, African, or Native American—does not excuse the system of oppression that we erected on this continent to build this country.
“Many of those people were there to protest the taking down of the statue of Robert E. Lee. So this week, it’s Robert E. Lee, I noticed that Stonewall Jackson’s coming down,” President Trump said yesterday at a press conference. “I wonder, is it George Washington next week? And is it Thomas Jefferson the week after? You know, you really do have to ask yourself, where does it stop?”
What if the answer is that it doesn’t? The evil of slavery and the white supremacy it embedded in the fabric of the country go all the way back to the beginning. And our history needs to honestly tell the story of James Madison dying without freeing a single one of the 100 enslaved people who worked for him right alongside his call, quoted in The Atlantic in 1861, to leave the words slavery out of the Constitution so that it would “be the great charter of Human Liberty to the unborn millions who shall enjoy its protection, and who should never see that such an institution as slavery was ever known in our midst.”
We can excise the words, but we can never scrub the blood from the soil.


Both as a service and a company, Google has always been a convenient stand-in for the greater internet. “Googling” became shorthand for searching for something on the web. The “don’t be evil” line from its IPO prospectus became a catch-all politics for Silicon Valley.
So it’s not surprising that the outbreak of a new strain of reactionary politics has found its way to Google’s doorstep. First, Google fired the engineer James Damore for writing a memo explicitly opposing the company’s diversity and inclusion policies through a specious reading of the biology literature on gender and IQ differences in humans. Damore became an instant celebrity on the right and among free-speech absolutists.
Then, so-called “New Right” figure and noted conspiracy theorist Jack Posobiec—who President Trump retweeted this week—called for a march on Google offices in cities across the country for this Saturday, August 19. “Google is a monopoly, and its [sic] abusing its power to silence dissent and manipulate election results,” the announcement reads. “Their company YouTube is censoring and silencing dissenting voices by creating ‘ghettos’ for videos questioning the dominant narrative. We will thus be Marching on Google!”
This week, too, the Daily Stormer, a long-running neo-Nazi site, got cut off from its original hosting and went to Google, who also cut off the site.
Looking at this news in isolation, you might say: Google has become a right-wing target because it is a liberal institution. Certainly, it has not gone unnoticed on the right that Googlers overwhelmingly supported Hillary Clinton, both financially and operationally. And more generally, the Bay Area technology industry has become a key base of support for Democratic candidates.
But the ideology underpinning Silicon Valley does not fall on a strict left/right spectrum. For many years, scholars who study the internet’s dynamics have been saying that the public sphere—the place where civic society is supposed to play out, the place “free speech” advocates desire to see preserved—has been privatized by Facebook, Google, and the other big internet companies. Most on the left saw this privatization as part of a larger conservative (or—GASP—neoliberal) movement that was sapping the strength of the government-secured commons.
Zeynep Tufekci, a sociologist at Princeton University, made this connection in 2010. “This is about the fact that increasing portions of our sociality are now conducted in privately owned spaces. The implications of this are still playing out,” she wrote.
She cited a litany of examples running long before Google and Facebook: the outsourcing of key government functions to private contractors, the “dominance of corporate-owned media over the civic public sphere,” and the replacement of public parks with malls and “privately owned town squares.”
By moving our speech online, we entered a mall, where Facebook or Twitter or Google control the rules, not the U.S. government. Companies have different imperatives, the first of which is—for these companies, above all—to make money on advertising. By and large, this has led to a maximally permissive informational environment. On the occasions they have intervened to censor nipples or ban particular kinds of sites, their response has been: Hey, it’s a free market and you can always post photos to your LiveJournal, search with Bing, and chat on Gab.
These attitudes are why sometimes people describe Silicon Valley as “libertarian.” But most Silicon Valley people are wealthy Democrats who support progressive social causes, but do not want entrepreneurship and business restrained by the government (too much). They are the classic “socially liberal, fiscally conservative” people who the left (dirtbag and otherwise) loves to pillory.
Can the U.S. Government Seize an Anti-Trump Website's Visitor Logs?
Take the Los Angeles–based web host and domain-name registrar DreamHost, for example. On the one hand, they are fighting the Department of Justice, which has requested details on 1.3 million visitors to an anti-Trump website. “That information could be used to identify any individuals who used this site to exercise and express political speech protected under the Constitution’s First Amendment,” the company wrote on its blog. On the other, they are part of the cloud infrastructure of neo-Nazi groups like the American Nazi Party.
All of which puts both left-wing critics and right-wing marchers into different kinds of binds.
For the right, Google and Facebook are private actors who obey the market. Last I checked, strong-arming companies into doing the will of the people by extra-market means is not high on the list of conservative principles. Why can’t the market just decide? If people don’t like Google’s or Facebook’s decisions, they can head for the digital exits.
And on the left, Google and Facebook and the rest are, in fact, manifestations of neoliberalism. But because they’ve privatized the public sphere of discourse, they now possess the power to shut down white-supremacist sites, stop Daily Stormer links from circulating, and do it all without anyone having a legal or constitutional basis for stopping the “censorship.” The biggest de-platforming of all would come from getting the internet platforms themselves to use their power to stop the speech of white supremacists, neo-Nazis, birthers, “ironic racist” 4channers, and anti-Semites. And, indeed, a message projected onto Twitter headquarters this week called for just such a move:
In #Charlottesville, Nazis are marching, many with Twitter accounts. At #twitter HQ, I'm asking a simple question. pic.twitter.com/v3gkoaKU8B
As more and more of daily life online is consumed by the political storm in America, the “socially liberal, fiscally conservative” position that the tech companies have staked out is getting harder and harder to hold.


Suppose you were to click on this link: This one, right here.
It will take you to the website of Disrupt J20, which organized some of the “direct action” protests on the day of President Donald Trump’s inauguration in Washington, D.C. The site contains general information about civil disobedience and political protests, and it advertises several Washington-specific events.
Some of the protests on Inauguration Day turned violent, and the U.S. government has since charged more than 200 people with felony rioting or destruction of property in connection to events on January 20. It alleges that some of the suspects were connected to the Disrupt J20 effort.
Yet if you clicked that link above—even if you were nowhere near Washington on Inauguration Day—the government is now allegedly interested in you.
The U.S. Department of Justice is attempting to seize the visitor logs and IP addresses of anyone who has visited DisruptJ20.org, as well as any email addresses, user logs, and photos collected by the website, according to DreamHost, a Los Angeles–based web host and domain-name registrar.
This data encompasses more than 1.3 million IP addresses, as well as the email addresses and photos of thousands of people, the company said. DreamHost is not politically connected to DisruptJ20, but it provided paid web-hosting services for the group.
DreamHost has so far refused to comply with the government’s search warrant, arguing that it constitutes “investigatory overreach and a clear abuse of government authority.”
“That information could be used to identify any individuals who used this site to exercise and express political speech protected under the Constitution’s First Amendment. That should be enough to set alarm bells off in anyone’s mind,” said a blog post published to the company’s website on Monday.
A spokesperson for the U.S. Attorney’s Office for the District of Columbia did not respond before publication. A spokesman for the U.S. Department of Justice declined to comment.
Digital-privacy and civil-rights advocates were quick to criticize the scope of the government’s warrant. But experts in computer crime law said it wasn’t immediately obvious that the warrant was illegal.
“The Department of Justice isn’t just seeking communications by the defendants in its case. It’s seeking the records of every single contact with the site—the IP address and other details of every American opposed enough to Trump to visit the site and explore political activism,” wrote Ken White, a criminal-defense lawyer and former assistant U.S. attorney.
He continued:
The government has made no effort whatsoever to limit the warrant to actual evidence of any particular crime. If you visited the site, if you left a message, they want to know who and where you are—whether or not you did anything but watch TV on inauguration day. This is chilling, particularly when it comes from an administration that has expressed so much overt hostility to protesters, so relentlessly conflated all protesters with those who break the law, and so deliberately framed America as being at war with the administration’s domestic enemies.
“No plausible explanation exists for a search warrant of this breadth, other than to cast a digital dragnet as broadly as possible,” said Mark Rumold, a senior staff attorney at the Electronic Frontier Foundation, in a blog post. The EFF is assisting DreamHost in its opposition to the warrant.
“The Fourth Amendment was designed to prohibit fishing expeditions like this. Those concerns are especially relevant here, where [the Department of Justice] is investigating a website that served as a hub for the planning and exercise of First Amendment–protected activities,” he said.
In an email, Rumold added that the government had successfully seized visitor logs for other websites in the past. “But I’ve never seen anything on this scale, where we’re talking about millions of users and there’s no attempt whatsoever to narrow the scope (either by date, time, or user),” he told me.
“I don’t think there are precedents one way or another on this,” Orin Kerr, a law professor at George Washington University, told me.
“It’s not obvious to me whether the warrant is problematic,” he elaborated in an article at The Washington Post. The government’s search warrant instructs DreamHost to turn over all its records about DisruptJ20.org. As Kerr understands it, DreamHost wants the government to only legally be able to ask for certain records about the website. He continues:
There’s an interesting and unresolved issue presented here: What’s the correct level of particularity for a website? Courts have allowed the government to get a suspect’s entire email account, which the government can then search through for evidence. But is the collective set of records concerning a website itself so extensive that it goes beyond what the Fourth Amendment allows? In the physical world, the government can search only one apartment in an apartment building with a single warrant; it can’t search the entire apartment building. Are the collective records of a website more like an apartment building or a single apartment? I don’t know of any caselaw on this.
A hearing in D.C. Superior Court is scheduled for Friday.
President Trump has addressed the January 20 protests directly at least twice. Two days after they occurred, he belittled them and the Women’s March, on January 21, in a tweet: “Watched protests yesterday but was under the impression that we just had an election!” he said. “Why didn’t these people vote? Celebs hurt cause badly.”
Two hours later, he tweeted an addendum: “Peaceful protests are a hallmark of our democracy. Even if I don’t always agree, I recognize the rights of people to express their views.”


On August 21, the moon's orbit will bring it directly between the Earth and the sun, creating a total solar eclipse in the United States for the first time since 1979. One of the first towns perfectly positioned for the most dramatic view is Keizer, Oregon. Resident Matt Rasmussen is one of many people living along the eclipse’s “path of totality” looking to make the most of this once-in-a-lifetime opportunity—and not just to take in the sights.
Rasmussen said a friend living in nearby Portland, which will only see a relatively mundane partial eclipse, casually suggested he try to rent out his house for the weekend of the eclipse. “She said we should post on Airbnb because she bet we could get a mortgage payment out of it,” he says. “I laughed, and randomly set up our house at what we thought was a large amount, never expecting to have a taker. We were booked within two days.”
Rasmussen charged $2,000 for a single night, as much as 10 times the typical price, which he guesses would be around $200 or $300. Other Oregonians contacted through Airbnb’s messaging system for this article tell similar stories.
The eclipse has been a hot topic along its path for months, with preparations taking many forms. California is prepping its solar-heavy power grid to deal with the temporary drop in sunlight. School districts situated on the eclipse path in Illinois and Missouri are canceling class on August 21. Kentucky officials are stocking up on a drug to treat heroin overdoses, and NASA is readying a raft of science experiments. In Oregon, as with other states on the eclipse’s cross-country path, government officials and locals are preparing for the swarm of visitors to these otherwise quiet, out-of-the-way towns. Residents have been told to prepare for power outages, cell-tower failures, internet-service outages, and other headaches. Gridlock figures to be a problem on roads across Oregon, straining emergency services.
“We’re planning for a very busy few days,” Peter Murphy, a spokesperson for the Oregon Department of Transportation, said in an email. He said 23 two-person crews will be stationed every few miles along U.S. Route 97, ready to remove any vehicles causing delays. “A lot will depend upon weather that day. Clouds on the coast or Willamette Valley may send travelers our way, and that will be a challenge to manage. Our message is ‘Arrive early, stay put, and leave late.’”
Rasmussen's father-in-law works on the coast, where officials are also anticipating traffic will slow to a halt. “From what I hear about the mid-Oregon coast, 50 miles west of us, they are going to station ambulances with supplies every few blocks along U.S. 101,” Rasmussen says, because authorities anticipate the roads, campgrounds, and beaches will be so jammed that ambulances may have a hard time getting through.
With a forecast like that, it’s no wonder some people are heading out of town—and hoping to profit in the process.
Zachary Burns of Redmond, a town located about 100 miles farther inland than Keizer, recalls his telescope-hobbyist father first told him about the eclipse about a year ago. He has heard varying estimates for how many visitors are coming to his part of central Oregon, but even the most conservative guesses are in the hundreds of thousands.
“I originally had the thought to leave town and camp to avoid the crowds,” he says. “Then I realized every camping spot was full. Then I heard every hotel was full. This was months ago, almost six months prior to the eclipse. That’s when I realized it might be a good way to bring in a little extra income.”
Burns rented his home for the night before the eclipse for $1,200. He figures it would likely go for $300 a night on any other weekend. Four times the typical rate isn’t bad, but he says he may have actually underbid himself. “As the date draws closer and prices rise, I’ve realized I could have listed the house for possibly $2,000 a night,” he says.
Others are opening up their property as a campground for multiple visitors. Bethany Stelzer from the town of Kimberly, another 100 miles inland from Redmond, said her family is offering campsites on the grounds of her orchard for $1,500 a night—each.
“I’ve heard from several locals that they are renting their homes or fields as campsites,” she says. “It was my understanding that the Oregon state parks sold out all of their campsites in the path of the eclipse fairly quickly, so the community stepped up to offer more options.”
For his part, Burns says he ultimately decided against camping during the eclipse, instead opting to stay with his parents. He made the decision in part because anywhere he could hope to find will be teeming with people. Then there are safety considerations; he worries any natural disasters, like late-summer wildfires, could make traffic jams even more dangerous.
Extra money from a night’s rental or a guided tour isn’t life-changing, but any extra bit helps given central Oregon’s relatively high cost of living, residents say. The money is also a nice tradeoff for the disruption the eclipse—or, more accurately, all the people headed to see it—will bring.
Some residents of central Oregon are more optimistic about the small-town welcome visitors will receive. Shawn Stanfill lives in Madras, about 25 miles north of Redmond. The father of Madras police chief Tanner Stanfill, he says law enforcement have taken the necessary steps to be ready to help in any situation, and he insists residents of small towns are always willing to lend a hand. The elder Stanfill is listing a couple properties for $1,500 a night, though he cautions those looking to view the eclipse from a relatively remote location like Madras need to be prepared to spend an extra two days getting in and another two getting out, so extreme will be the glut of visitors.
Zachary Burns says his employer, a restaurant supply company, has been ordering supplies weeks in advance to ensure eateries have what they need—though no one really knows what to expect. “I’ve heard varying estimates of the number of people coming to the area, but even lower estimates seem to think in the hundreds of thousands,” he says. “Should be an adventure.”
While the eclipse has become a serious moneymaking opportunity for many Oregonians, as well as those in states eastward along the eclipse’s path, those contacted for this article say they still intend to make the most of the eclipse itself. One resident said he plans to hike up a mountain trail known only to locals, where he’ll be unlikely to run into any out-of-town sky-gazers. Considering all the commotion and chaos that the eclipse could bring, they’ve probably earned at least that small measure of solitude.


How should Americans fight against a resurgent white-nationalist movement in the United States? This weekend, they returned to an artifact from an earlier era of anti-Nazism. Tens of thousands of people rediscovered—and promptly shared and retweeted—a clip from Don’t Be a Sucker, a short propaganda film made by the U.S. War Department in 1943.
When it first debuted, Don’t Be a Sucker would have played in movie theaters. Now it has made its 21st-century premiere thanks to a network of smaller screens and the Internet Archive, where it is available in full. Almost 75 years after it was first shown, Don’t Be a Sucker lives again as a public object in a new and strange context.
1947 anti-fascist video made by US military to teach citizens how to avoid falling for people like Trump is relevant again. pic.twitter.com/vkTDD1Tplh
Its opening clip is a direct and plain-language parable in anti-fascism. It begins as a flushed man brandishes a pamphlet and addresses a crowd: “I see negroes holding jobs that belong to me and you. Now I ask you, if we allow this thing to go on, what’s going to happen to us real Americans?” He proceeds to blame blacks, Catholics, Freemasons, and immigrants for the nation’s ills.
“I’ve heard this kind of talk before, but I never expected to hear it in America,” says an older man with an Eastern European accent.
He introduces himself to a younger man next to him: “I was born in Hungary but now I am an American citizen. And I have seen what this kind of talk can do—I saw it in Berlin. I was a professor at the university. I heard the same words we have heard today.”
“But I was a fool then,” he continues. “I thought Nazis were crazy people, stupid fanatics. Unfortunately it was not so. They knew they were not strong enough to conquer a unified country, so they split Germany into small groups. They used prejudice as a practical weapon to cripple the nation.”
There ends the viral clip. But the original, 17-minute film Don’t Be a Sucker—which can be viewed in full below—continues, slipping into a short history of the rise of the Nazi Party in Germany. We see the movement evolve from an angry group of men in the streets to a party organization armed with an official state paramilitary. There’s a montage of Nazi crimes: A Jewish shop owner is carried away by police officers, a group of union members are attacked, and a college professor is arrested after telling his students that there is no scientific basis for the existence of a “master race.” (The version below is from the film’s 1947 rerelease.)*
Michael Oman-Reagan, an anthropologist and researcher in British Columbia, was the first to post the clip on Saturday evening, in a tweet comparing the orator’s rhetoric to President Donald Trump’s. His post has since been retweeted more than 85,000 times.
But he was not alone in linking the events in Charlottesville to the Second World War. Orrin Hatch, a Republican of Utah and the president pro tempore of the Senate, said in a tweet on Saturday: “We should call evil by its name. My brother didn’t give his life fighting Hitler for Nazi ideas to go unchallenged here at home.”
What makes the film so remarkable? It’s not as if Don’t Be a Sucker encapsulates some lost golden age of American anti-racism. Indeed the contradictions of the 1940s are inseparable from the film. In its opening montage, it shows a multiethnic group of kids—white, black, and East Asian—playing baseball. Yet in 1943, the same year it was released, the U.S. federal government kept more than 100,000 Americans imprisoned solely for the crime of being Japanese. And it was on its way to implementing one of the great anti-black wealth transfers of American history.
Still, Don’t Be a Sucker seems wise. It seems to know how democratic solidarity falters, how prejudice and factionalism can fracture a nation, and how all these forces might manifest in the United States of America. This wisdom may have emerged from simple practicality: Though the U.S. Army and Navy remained segregated for another five years, they were already vast and diverse enterprises by 1943. Simply put, different people had to work together to win the Second World War. The same was true of the whole country.
And in that, Don’t Be a Sucker may point to a deeper driver of the American experiment in multi-ethnic democracy. Building a diverse commonwealth has never been just an idealistic aspiration or moral avocation. It has been a requirement of the republic’s survival—the sole remedy to the cancer of white supremacy.
* This article has been updated to clarify that the clip is from the 1947 rerelease of the film.


Opening their paper on Friday morning, readers of The Wall Street Journal encountered a financial item of unusually wide interest. “Here’s a question that’s probably not on the CFA exam,” write Mike Bird and Riva Gold. “What happens to financial markets if two nuclear-armed nations go to war?”
What, indeed? We soon learn the consequences could be dire. Short-term interest rates would rise and long-term rates would fall. In a small skirmish between North Korea and the United States, the S&P 500 Index might post 20-percent losses “before it became clear that the United States would prevail.” But were another nuclear-armed power like Russia or China to get involved, the European Central Bank would have to take extreme action and issue “highly dovish forward guidance.”
Yet even amid this market turmoil, the savvy broker might still protect their investment. Sure, it’s true that the Japanese yen—a traditional safe haven—makes for a tricky bet when Tokyo is 800 miles downwind of Pyongyang. But there’s at least one good option left, according to analysts at the Nordea Group:
German bunds, the perennial refuge of panicked investors, would be good to own during a nuclear conflict too, with aggressive buying pushing the spread between German two- and 10-year bunds to 0.5 percentage point, from above one percentage point now.
At last, a good spread between German bonds. What a relief.
Nowhere does the story mention several other consequences of nuclear war: the urban firestorms; the plumes of sun-blotting black smoke; the crop die-offs across Asia, Africa, and North America; and the breakdown in the global communication network, whose destruction would render the German bund meaningless (no matter how favorable its yield curve). Nor did the story pause to note the millions of dead.
In the second week of August 2017, the American public began to do something that felt distinctly 20th-century: consider the consequences of a nuclear war. Two things became clear. First, nuclear anxiety had arrived again as a mass cultural force in American life—or, at least, in the accelerated internet-era version of it. Second, the public (and the American president) was obviously out of practice in thinking about it.
The episode began in earnest on Wednesday, when The Washington Post reported that at least one intelligence agency believed that North Korea could now miniaturize its nuclear weapons to fit into an intercontinental ballistic missile. If true, it represents an alarming technological breakthrough for the nation.
Then the president spoke. At an unrelated event at his private golf course in New Jersey, President Donald Trump warned of “fire and fury like the world has never seen” if North Korea continued to make threats against the United States. The next day—after aides tried to signal that his comments were improvised—he repeated them, saying maybe “fire and fury” was not “tough enough.”
Finally, on Friday morning, Trump tweeted that the U.S. military was “locked and loaded should North Korea act unwisely.”
Nuclear war—suddenly, everyone was talking about it, because the president was talking about it, in ways he isn’t supposed to.
Every late-night host riffed on the apocalypse. “Even Trump is scared by what he’s saying—look at him, he’s literally hugging himself,” quipped Seth Meyers, host of Late Night. (Trump gripped his torso as he uttered “fire and fury.”) A set of Democratic-connected advocacy groups, most of them not particularly radical, held an “emergency rally against nuclear war” at the White House.
And every professor or researcher of nuclear-weapons policy—normally confined to the dusty corners of university libraries and international security conferences—found themselves on a treadmill of radio and TV interviews. “[Nuclear weapons] are this kind of layer over the world, this abstract, intangible thing. We don’t talk or think about them,” says Lovely Umayam, who researches nuclear weapons at the Stimson Center, a think tank in Washington, D.C.
She said she felt glad there was renewed interest in the one technology that hangs over all U.S. international relations. But she also worried at how reactive the attention seemed. For the past week, she told me, she’s heard one constant question during TV and radio interviews: “Should we be concerned?”
“As an expert, I say, no, not quite,” she said. “We could really walk back on these words and develop de-escalation mechanisms. It’s horrible [Trump and Kim Jong Un] are talking this way, but it’s not the end of the world yet.”
“But then, as an anthropologist, I want to say: Yes, you should be concerned! You should always be concerned. And that you have to ask an expert that question—what does it say about your literacy of [nuclear] issues?” she said.
Kristyn Karl, a professor of political science at Stevens Institute of Technology, agreed that the public’s interest in nuclear weapons was way up—even if their understanding wasn’t. “The public is currently more aware of nuclear threats than they have been since the end of the Cold War,” she told me in an email.
That doesn’t mean they know much about them.
Americans flunk questions about basic nuclear security, Karl said, “such as identifying nuclear states, the scale of nuclear arsenals, etc.” Younger Americans also have little experience with nuclear weapons, especially compared with Baby Boomers.
Alex Wellerstein, a historian of nuclear weapons, also at the Stevens Institute, agreed that people seem more interested now. But he worries that they won’t stay that way once this crisis passes.
“It’s clear there is a sharp uptick of interest on nuclear questions,” he said in an email. “The question is, what kind of interest is it? Is it the kind of interest that will lead to a more sustained public interest on these topics? Or is it an ephemeral fear of the sort that comes and goes in a crisis?”
“American nuclear anxiety seems almost totally focused on foreign policy issues from small states—specifically Iran and North Korea. In that sense it is somewhat different than the period of the Cold War when the threat was much larger,” he said:
What I fear is that Americans will erroneously think that a war with either Iran or North Korea would be “no big deal” whereas we are (and were) much more aware that a war with Russia was totally unthinkable. War with Iran should be considered unthinkable (one need only look at what our war with Iraq has cost us, what monsters it created), and war with North Korea would come at a dearer cost than I think most people appreciate.
But when it comes to the prospect of nuclear annihilation, what is unthinkable and what isn’t? Americans are finding themselves back in the uneasy practice of imagining not the end of the world, but all the intermediate steps between now and then—the first warnings on the news, the orange streaks in the sky, the agony of waiting for ignition.
Writing three decades ago, the essayist and physician Lewis Thomas imagined a war with Russia and fell into despair. “My mind swarms with images of a world in which the thermonuclear bombs have begun to explode, in New York and San Francisco, in Moscow and Leningrad, in Paris, in Paris, in Paris. In Oxford and Cambridge, in Edinburgh,” he wrote:
This is a bad enough thing for the people in my generation. We can put up with it, I suppose, since we must.
What I cannot imagine, what I cannot put up with ... is what it would be like to be young. How do the young stand it? How can they keep their sanity? If I were very young, 16 or 17 years old, I think I would begin, perhaps very slowly and imperceptibly, to go crazy.
For today’s young people, looking to an uncertain future, at least there are German bonds to buy.


When CEO Sundar Pichai addressed a controversial memo about diversity that circulated inside Google, culminating in the termination of its author, James Damore, he began by telling the company’s 72,053 employees that “we strongly support the right of Googlers to express themselves, and much of what was in that memo is fair to debate, regardless of whether a vast majority of Googlers disagree with it.”
“However,” he added, “portions of the memo violate our Code of Conduct and cross the line by advancing harmful gender stereotypes in our workplace. Our job is to build great products for users that make a difference in their lives. To suggest a group of our colleagues have traits that make them less biologically suited to that work is offensive and not okay. It is contrary to our basic values and our Code of Conduct.”
I have a question for the CEO.
Given that the full text of the memo is public, that it is the subject of a national debate on an important subject, that many educated people disagree with one another about what claims it made, and that clarity can only help Google employees adhere to the company’s rules going forward, would you be willing to highlight the memo using green to indicate the “much” that you identified as “fair to debate” and red to flag the “portions” that you deemed Code-of-Conduct violations?
Absent that, it seems to me that Google employees will remain as uncertain as ever about what they can and cannot say at the company. As an illustration, consider Alan Jacobs, an English professor at Baylor University who declares himself confused about your meaning:
Google’s position could be:
I think those are the chief options.
Actually, I can think of still more options—especially if only tiny “portions” of the memo crossed Google’s line—which only underscores the dearth of clarity available to your employees. As a general matter, for example, I wonder if you believe the truth of a proposition is relevant to whether it violates the Code of Conduct. Can something be both the scientific consensus on a subject and unmentionable?
Jacobs adds, “I seriously doubt that Google will get much more specific. Their goal will be to create a climate of maximal fear-of-offending, and that is best done by never allowing employees to know where the uncrossable lines are. That is, after all, corporate SOP.” I’d guess legal incentives are a more powerful motivator of strategic vagueness. Are we being too cynical? Over the course of its history Google has often struck me as a unique company. And surely elevating clarity here would fulfill the mission of making all pertinent information universally accessible and useful.


DNA is fundamentally a way of storing information. Usually, it encodes instructions for making living things—but it can be conscripted for other purposes. Scientists have used DNA to store books, recordings, GIFs, and even an Amazon gift card. And now, for the first time, researchers from the University of Washington have managed to take over a computer by encoding a malicious program in DNA.
Strands of DNA are made from four building blocks, represented by the letters A, C, G, and T. These letters can be used to represent the 1s and 0s of computer programs. That’s what the Washington team did—they converted a piece of malware into physical DNA strands. When those strands were sequenced, the malware launched and compromised the computer that was analyzing the sequences, allowing the team to take control of it.
“The present-day threat is very small, and people don’t need to lose sleep immediately,” says Tadayoshi Kohno, a computer security expert who led the team. “But we wanted to know what was possible and what the issues are down the line.” The consequences of such attacks will become more severe as sequencing becomes more commonplace. In the early 2000s, it cost around $100 million to sequence a single human genome. Now, you can do it for less than $1,000. The technology is not just cheaper, but also simpler and more portable. There are even pocket-sized sequencers that allow people to analyze DNA in space stations, classrooms, and jungle camps.
But with great ubiquity comes great vulnerability. DNA is commonly used in forensics, so if troublemakers could hack sequencing machines or software, they could change the course of an investigation by altering genetic data. Or, if machines are processing confidential data about genetically modified organisms, hackers could steal intellectual property.
There’s also the matter of personal genetic data. The United States is currently trying to sequence the DNA of at least 1 million Americans to pave the way for precision medicine, where treatments are tailored to an individual’s genes. “That data is very sensitive,” says Peter Ney, a student in Kohno’s lab. “If you can compromise [the sequencing pipeline], you could steal that data, or manipulate it to make it seem like people have genetic diseases they don’t have.”
“We want to understand and anticipate what the hot new technologies will be over the next 10 to 15 years, to stay one step ahead of the bad guys,” says Kohno. In 2008, his team showed that they could wirelessly hack their way into a heart implant, and reprogram it to either shut down or deliver debilitating jolts. In 2010, they showed that they could hack into the control system of a Chevrolet Impala, taking control of the car. Then, they turned their attention to DNA sequencing. “It’s an emerging field that other security researchers haven’t looked at, so the intrigue was there,” says Kohno. “Could we compromise a computer system with DNA biomolecules?”
They could, but reassuringly, it wasn’t easy. To make their malware work, the team introduced a vulnerability into a program that’s commonly used to analyze DNA data files. They then exploited that weakness. That’s a bit of a cheat, but the team also showed that such vulnerabilities are common in software for analyzing DNA. The people who created these programs didn’t really have hacking in mind, and so their products tend to be insecure, and rarely follow best practices for digital security. With the right molecular malware, it could be possible for adversaries to compromise these programs and the computers that run them.
“I liked the creativity a lot, but their exploit is unrealistic,” says Yaniv Erlich, a geneticist at Columbia University and the New York Genome Center. (Earlier this year, Erlich encoded a computer virus in DNA, but he didn’t code it so that it would launch on its own when the DNA was sequenced.) In practice, the team’s malware would create a glitch that most sequencing centers would spot and fix. An adversary could only assume control of a compromised computer if they had impeccable timing, and struck immediately after the strands were sequenced.
Still, Erlich agrees that programs for analyzing DNA have “relatively relaxed security standards.” There are rumors, he says, that one big research institution was hit by ransomware, because they used the default admin passwords on their sequencing machines.
“My hope is that over the next 5 to 10 years, people take a strong interest in DNA security, and proactively harden their systems against adversarial threats,” says Kohno. “We don’t know of such threats arising yet and we hope that they’ll never manifest.”


Until the 20th century, only the wealthy or dying might have witnessed someone trying to cool the air indoors—even though building a fire to keep warm in the winter would have been perfectly reasonable. Extreme heat was seen as a force that humans shouldn’t tamper with, and the idea that a machine could control the weather was deemed sinful. Even into the early 1900s, the U.S. Congress avoided the use of manufactured air in the Capitol, afraid voters would mock them for not being able to sweat like everyone else.
While adoption of air-conditioning demanded industrial ingenuity, it also required renouncing the vice of cooling the inside air. But in the process of shedding its hypothetical moral slight against the heavens, the air conditioner has perpetrated worse, actual sins against the Earth.
* * *
Despite the shadow of immorality, breakthroughs in air-conditioning developed out of desperation. Doctors scrambling to heal the sick took particular interest. In 1851, a Florida doctor named John Gorrie received a patent for the first ice machine. According to Salvatore Basile, the author of Cool: How Air-Conditioning Changed Everything, Gorrie hadn’t initially sought to invent such an apparatus. He’d been trying to alleviate high fevers in malaria patients with cooled air. To this end, he designed an engine that could pull in air, compress it, then run it through pipes, allowing the air to cool as it expanded.
Outside of his office though, people saw no practical need for this achievement. It wasn’t until the pipes on Gorrie’s machine unexpectedly froze and began to develop ice that he found a new opportunity. Still, this accomplishment was lampooned as sacrilege in The New York Globe: “There is Dr. Gorrie, a crank ... that thinks he can make ice by his machine as good as God Almighty.”
The use of ice and snow to chill drinks or to help cool a room was nothing new. In the 17th century, the inventor Cornelius Drebbel used snow that had been stored underground during the summer to perform an act he called “turning summer into winter.” In his book Absolute Zero and the Conquest of Cold, Tom Shachtman speculates that Drebbel achieved his effect by mixing snow with water, salt, and potassium nitrate, which formed ice crystals and significantly cooled the space. King James, who invited Drebbel to demonstrate his innovation, reportedly ran from the demonstration in Westminster Abbey, shivering.
Ice would be used two centuries later to cool another man in power, U.S. President James A. Garfield. On July 2, 1881, Charles Guiteau fired two shots from his revolver into Garfield’s back. The aftermath inspired naval engineers to develop a method to keep a president cool, as he slowly died that summer.
The astronomer Simon Newcomb oversaw development of the apparatus that struggled to chill Garfield’s sickroom. Newcomb rigged together an engine connected to pipes that powered a fan to blow over a giant bucket of ice. In written reports, Newcomb explained that his apparatus held “some six tons [of ice] in all, through which the air might pass in one direction and return in the other.” The device lowered the room’s temperature from 95 to 75 degrees—and ate up hundreds of pounds of ice an hour.
As news of Newcomb’s machine slowly grabbed the public interest, distrust of cooling the air began to wane. Inventors developed fanciful schemes to beat the heat. One believed he could take a balloon connected to a fire hydrant and a hose and create personal rainstorms. Another came up with the idea of towers with carbon dioxide bombs at the top that would explode above a neighborhood and cool the air upon detonation. Some of these curiosities managed to win patents, but few proved useful in practice.
* * *
Two decades after Garfield’s death, Willis Carrier coined the term “air-conditioning.” Although it wasn’t an overnight sensation, Carrier’s breakthrough came in July 1902, when he designed his Apparatus for Treating Air, first installed in the Sackett Williams Publishing building in Brooklyn, New York. The device blew air over tubes containing a coolant. Its purpose was to reduce humidity more than to reduce air temperature; excess water in the air warped the publishing house’s paper.
In 1899, Alfred R. Wolff had preceded Carrier with an air-cooling device, installed in the dissecting room of Cornell Medical College in New York City. Later, the same year Carrier installed his first apparatus in Brooklyn, Wolff placed his machine at the New York Stock Exchange. Instead of keeping cadavers fresh for study, it brought comfort to the horde of men at work.
The technology began to spread. Frigidaire sold the first “room cooler” for the home in 1929. H.H. Schultz and J.Q. Sherman marketed an air conditioner that leaned against the windowsill, but the first window-mounted unit, as we know it today, was the 1932 Thorne Room Air Conditioner. It looked like the grill of an old car shoved through a window. In her book Cool Comfort: America’s Romance with Air-Conditioning, Marsha Ackermann recounts a radio interview in which Carrier announced his vision. He imagined a world in which “the average businessman will rise, pleasantly refreshed, having slept in an air-conditioned room. He will travel in an air-conditioned train, and toil in an air-conditioned office.”
Air-conditioning’s major public debut was at the 1939 World’s Fair. Carrier hosted the Carrier Igloo of Tomorrow expo, where 65,000 visitors would experience air-conditioning for the first time, boosting consumer interest. Over the next decade, as the air conditioner shrank in size, advertisements for the machine shifted their appeals from men in the workplace to women at home. In some early ads the air conditioner sits in the window among a proud family admiring their machine like a spacecraft that had landed in the living room.
Basile points out another, less obvious move that increased the device’s popularity: In 1959, the U.S. Weather Bureau created its “discomfort index”—we know it today as the heat index, a measure of temperature and humidity combined. The discomfort index gave an unexpected boost to air-conditioning by, as Basile says in his book, putting “people in mind of cooled air.” Now the public could gauge if it was too hot to go outside. If they could afford it, there were plenty of air-conditioner manufacturers offering solace from the weather.
By the 1960s, millions of air conditioners were being sold every year in the United States. Windows across cities and suburbs were being plugged with the machines. As of 2011, the Energy Information Administration’s Residential Energy Consumption Survey says that 87 percent of households in the United States have an air conditioner or central air. That’s compared to 11 percent in Brazil and only 2 percent in India.
* * *
While the public’s reluctance to air-conditioning might have hampered the initial development of air-conditioning technologies, its eventual popularity has proved detrimental to the Earth’s atmosphere.
By 1989, the Montreal Protocol was enacted in an effort to cut the release of chlorofluorocarbons, or CFCs, into the atmosphere. Freon, a CFC used in the early A/Cs, was among the features of older air-conditioning units that contributed to ozone depletion.
Even though refrigerants have been modified to use fluorine instead of chlorine, and thereby to avoid impacting ozone, air-conditioning still exerts enormous environmental impact. According to Daniel Morrison, the acting deputy director of communications at the U.S. Department of Energy, residential and commercial buildings used more than 500 billion kilowatt-hours of electricity for air-conditioning in 2015 alone. That’s almost 20 percent of the total electricity used in buildings, amounting to $60 billion in electricity costs annually. Air-conditioning is also one of the main contributors to peak electric power demand, one symptom of which is rolling summer blackouts.
Of all the devices people use today, the air conditioner has not experienced major design makeovers like the television or the telephone. But there are companies trying to revolutionize the future of air-conditioning—both in aesthetics and efficiency. Some of these efforts rehearse earlier qualms about the unseemliness of interior cooling, making air-conditioning more personal. CoolWare, for example, makes an A/C collar, which wraps around the neck and delivers water-cooled air via small fans. Wristify offers a similar product as a bracelet. Kuchofuku makes an air-conditioned work shirt of a similar design.
A Cyprus-based company called Evapolar has introduced what it calls “the world’s first personal air cooler.” It’s a small cube with a water reservoir and a fan that creates a breeze and purifies the air. Evapolar promotes the idea of a “microclimate” designed to cool a single person’s work or sleep space, and thereby to avoid wasting energy by cooling entire rooms or buildings. “Just as our phones became personalized, we believe that the climate device should also become personalized,” Evapolar spokesperson Ksenia Shults tells me.
Dyson and Xiaomi are also introducing small, personalized air purifiers into the market. All these devices remain niche (and fairly uncool, as it were), but stranger things have become mainstream.
Even today, air-conditioning remains controversial. Due to their environmental impact, some advocates call for disuse of these machines. Others accuse the air conditioner of chauvinism, forcing women in the workplace to dress one way inside and another outside. It has become both a symbol of human ingenuity and of weakness, acclimatizing human bodies so that they are less resilient against natural heat without the aid of machines.
More than just an appliance, the air conditioner is a memento mori. It was a device people invented to avoid a few individual deaths, and yet one whose adoption might have a role to play in the passing of a temperate climate for everyone. As summer proceeds, listen to the chorus of machines humming in the windows, outside the houses, atop the office buildings. They offer a reminder that humanity’s ingenuity can come at a cost. Maybe our forebears weren’t entirely wrong to see peril in the act of cooling the air.
This article appears courtesy of Object Lessons.


Popular imagery of the atom bomb is oddly sterile.
For all we know of the horrors of nuclear weapons, the visual that’s most often evoked is ethereal, if ominous: a silent, billowing cloud, aloft in black and white.
The reasons for this are understandable. Nuclear weapons have been tested far more often than they’ve been used against people. And the only two times they were used in warfare—in Hiroshima, then Nagasaki, 72 years ago—photographers captured many scenes of devastation, yet video recording was scant.
The People Who Built the Atomic Bomb
Survivors of the bombings have shared what they saw and heard before the terror. John Hersey’s famous report, published in 1946 by The New Yorker, describes a “noiseless flash.” Blinding light and intense pressure, yes, but sound? “Almost no one in Hiroshima recalls hearing any noise of the bomb,” Hersey wrote at the time. There was one person,  a fisherman in his sampan on the Inland Sea at the time of the bombing, who “saw the flash and heard a tremendous explosion,” Hersey said. The fisherman was some 20 miles outside of Hiroshima, but “the thunder was greater than when the B-29s hit Iwakuni, only five miles away.”
There is at least some testing footage from the era that features sound. It is jarring to hear. The boom is more like a shotgun than a thunderclap, and it’s followed by a sustained roar. Here’s one example, from a March 1953 test at Yucca Flat, the nuclear test site in the Nevada desert.

The National Archives description of the footage is matter-of-fact—which is the purpose of archival descriptions, but which seems strangely detached, considering: There’s the mountain ridge in early morning. An atom bomb is exploded. Burning. Pan of the mushroom against darkened sky. The cloud dissipates as the sky lightens. A yucca plant and Joshua trees in foreground. Hiller-Copters buzz in. And, finally, General John R. Hodge standing at a microphone, blinking into the morning sun.
“This test, I think, went very well,” he said. “I was quite interested in how the troops reacted. I didn’t find any soldier there who was afraid.”
“They took it in stride,” he added “as American soldiers take all things.”


A federal court has unsealed new documents in the case against an Israeli teenager, Michael Kadar, who has been accused of making at least 245 threatening calls to Jewish Community Centers and schools around the United States. According to the documents, Kadar advertised a “School Email Bomb Threat Service” on AlphaBay, an online marketplace for illicit goods and services that was shut down by the federal government in July. Authorities have identified an individual in California who allegedly ordered and paid for at least some of Kadar’s threats.
A newly unsealed search warrant alleges that Kadar charged $30 for an email bomb threat to a school, plus a $15 surcharge if the buyer wanted to frame someone for it. “There is no guarantee that the police will question or arrest the framed person,” Kadar allegedly wrote in his ad.
I just add the persons name to the email. In addition my experience of doing bomb threats putting someones name in the emailed threat will reduce the chance of the threat being successful. But it’s up to you if you would like me to frame someone.
Kadar charged double for a threatening email to a school district or multiple schools, but districts with more than 12 schools required a “custom listing.” He noted that he was available “almost 24/7 to make emails,” and he promised to refund non-successful threats.
An Israeli American Teen Has Been Arrested in the JCC Bomb Threats Case
Kadar got good reviews. One AlphaBay user wrote that the threats were “Amazing on time and on target. We got evacuated and got the day cut short.” Based on the date when the comment was posted, it appeared to refer to a threat made to Rancho Cotate High School in Rohnert Park, California, north of San Francisco.
The Justice Department seized AlphaBay in late July—Attorney General Jeff Sessions called it “the largest dark net marketplace in history.” The documents in the Kadar case suggest that authorities had been tracking AlphaBay for a while: The search-warrant application alludes to screenshots of Kadar’s activity on the marketplace taken in mid-March.
It’s possible that the information discovered in the Kadar case contributed to the AlphaBay investigation. The Kadar documents were unsealed on July 19, the day before the Justice Department announced that AlphaBay had been shut down. Previously, the search warrant had been sealed because it was “relevant to an ongoing investigation into the criminal organizations as not all of the targets of this investigation will be searched at this time.” The search warrant and related legal documents were unsealed because the FBI and local authorities in California may need them to pursue criminal charges against the suspected buyer or buyers, or they may eventually be producible in the discovery phase of a criminal proceeding. The filings were first publicly flagged by Seamus Hughes, the deputy director of the Program on Extremism at George Washington University.
When Kadar was arrested in late March, members of the Jewish Community were shocked that an Israeli teenager appeared to responsible for many of the bomb threats that had forced Jewish Community Centers and schools to repeatedly evacuate their buildings last winter. Authorities arrested another suspect, Juan Thompson, in connection with some of the threats, but he appeared to make only a handful of the calls and was allegedly attempting to get revenge on an ex-girlfriend. The new documents suggest that even more people may have been involved as buyers—but how many, who, and why they did it are all not yet clear, and the document does not specifically state that any of the threats to Jewish institutions were issued at the behest of clients. So far, the investigation has led to a surprising pair of suspects. It’s not clear what kind of person will emerge as a suspect next.


As Uber battles taxis and other ride-hailing apps in cities across the world, the company is beginning to move quickly into a much larger transportation market: trucking.
This spring, Uber unveiled Uber Freight, a brokerage service connecting shippers and truckers through a new app. Conceptually, “Uber for trucking” seems like a logical extension of the passenger transport business.
But the logistics industry has totally different dynamics. For one, it’s business to business. Most truckers are owner-operators or they’re part of very small companies with a handful of vehicles. The industry has well-established ways of doing things. Truckers basically work in the places where Uber’s ride-hailing service doesn’t. And unlike Uber’s ride-hailing service, the company can’t bring a huge new supply of drivers onto the market to change the dynamics of transportation. As it is, there are somewhere north of 3 million truck drivers in America, between long-haul and delivery.
A Burdensome Regulation Screening Truck Drivers for a Sleep Disorder
Uber Freight was born out of the marriage of an internal team with members of Otto, after Uber acquired the latter company early last year. Since then, the teams have split up into self-driving research and development, managed by Alden Woodrow, formerly of Google X, and the Uber Freight team. Freight has a floor of one of Uber’s offices in downtown San Francisco and a large operations team in Chicago.
Uber has had a brutal last year. The company's culture has been critiqued from the inside and outside as sexist and fratty. The problems led to the ouster of a series of top executives, including founder Travis Kalanick. Even in trucking, Uber's acquisition of Otto has led to a lawsuit filed by Alphabet's self-driving car division, Waymo, related to the alleged theft of sensor technology. One Uber employee I know recently joked, "Uber's become a four-letter word."
I visited the company’s San Francisco office with Uber Freight’s product lead, Eric Berdinis. He’d come to Uber via Otto after a stint at Motorola working on the Moto 360 smartwatch, among other things. He graduated from the University of Pennsylvania in 2013, which makes him roughly 27 years old.
We walked the floor that is Berdinis’s domain. The engineering team is on the west side of the building, ops on the east. In the ops room, heat maps of America glowed on mounted televisions, showing where Uber is doing the most business. Texas was hot. This is certainly one of the places where software is nibbling away at the world.
Then we tucked into a conference room for an extensive interview. We talked through how to actually build “Uber for trucking,” what really hurts truckers, whether Otto oversold the speed at which self-driving trucks would arrive, and what drivers think of Travis Kalanick.
Alexis Madrigal: Let’s talk about Uber Freight and self-driving trucks. When Uber started, self-driving cars were pretty far away. When Uber Freight starts, perhaps self-driving trucks are not that far away. How much do you think self-driving trucks would change the economics for you guys?
Eric Berdinis: In my time at Otto, we did spend a decent amount of time thinking about the economics of trucking once it happens, even if it is a decade out. Now that I have been spending more time on the freight side, I haven’t been as close to that. But the teams are in communication about how these things might work together at some point.
Madrigal: And what is the relationship between the self-driving and Freight teams now?
Berdinis: They were born from a similar origin story. At least, I came from that team. The day-to-day workings are pretty separate. They are going down the path of finding their first customers and we’re scaling up the business and building the network. We’re in sync on what’s happening, but no active workstreams together like that.
Madrigal: Are you hearing from drivers that they are worried about it?
Berdinis: You see that come up every once in a while.
Madrigal: I know this isn’t what you’re doing on a day-to-day basis anymore, but how could you see the automation playing out?
Berdinis: I’ll first start by saying that one of the last things I worked on on the Otto side was the Otto-Budweister partnership and the video and the whole thing around that. Once I joined Uber Freight full time, I was thinking to myself, “We really made it seem like this thing was coming sooner than it is. We probably scared a lot of people. We kind of hyped this thing up.”
And it is showing what the future will be like. But it won’t be coming as fast as the video made it seem. The reality is that the transition to any kind of self-driving truck future is quite a ways away.
But in terms of how we think about that future, we actually do see a future where jobs don’t get impacted in the way that people expect them to. We wouldn’t be doing Uber Freight, which is a human-driven product, if we didn’t think that there was a responsible way for the future to look with humans and self-driving trucks.
Madrigal: Can you describe the future you see where there are autonomous trucks but jobs are not negatively impacted?
Berdinis: The answers aren’t perfectly clear yet, but the way that we’re building out this product is heading toward a direction that is the most driver-friendly possible. Once we have a more defined plan for how self-driving trucks and Uber Freight could work together, the specific will be more clear. There are lots of path that that could happen. Nothing to go into detail on now.
Madrigal: Has the recent trouble at Uber affected you all more or less than the standard employee at the company?
Berdinis: Uber Freight, because it has been incubated from the beginning with the Otto acquisition, we’ve always had really strong leadership internally. So, there has not been a huge impact from any of the searches for COO or CEO. The board is extremely excited about freight. They love having Uber with a diverse set of business opportunities. It hasn’t affected shippers. It hasn’t affected drivers. If you asked a driver, “Did you hear about Travis Kalanick?” They’d be like, “What are you talking about?”
Madrigal: But you did have a big departure from Otto in [founder Anthony] Levandowski. And there’s the Waymo lawsuit. Does that affect you guys on the Uber Freight ops side?
Berdinis: It really doesn’t. Because there are no self-driving components to Uber Freight. We definitely get questions like you’re asking me now. But it’s not like our technologies have anything to do with self-driving.
Madrigal: How did Uber Freight get started?
Berdinis: Curtis Chambers, who I think was the #7 employee at Uber, was tasked with exploring new opportunities in transportation. He was there for the start of uberX. He started UberEATS. Then, around the time that Otto was started, which was January/February of last year, Curtis was off with a few salespeople and engineers talking to trucking companies and starting to figure out if Uber should get into trucking. With the Otto acquisition, that solidified. The team we had created and the team Curtis had created—3 or 4 people on each side—we said, okay, let’s build out Uber Freight.
Madrigal: And the model you settled on is that Uber Freight essentially works as a broker between people with stuff to ship and truckers?
Berdinis: There is a defined model for how you build a company in the brokerage industry, which is the middle man between shippers and carriers. There have been a lot of brokers that have come along since the 1980s, when brokers became a formal thing.
Madrigal: Because of deregulation.
Berdinis: Right. So, there was a playbook for that. But it was completely unknown for how we do this in a tech-forward way that doesn’t totally follow the normal step-by-step that a brokerage would go after.
Madrigal: Which would just be lining up both sides of the marketplace, getting loads and getting trucks.
Berdinis: You make a promise to a shipper and hustle to find a driver and then, boom, that’s your first load. You just keep doing that at scale. It’s very easy to do it manually because you’re just calling and negotiating. You can muscle through that. But how do you get drivers to use an app or embrace a new way of doing things, especially when: 1) These drivers don’t really use technology in their day-to-day lives, and 2) when we’re really small, they log in and there are like five loads. That’s not a very useful product. So how do we get past the chicken-and-egg problem to the point where we are today when drivers come back every single day. And some of them are 100 percent on Uber Freight, like they completely transitioned their business.
Madrigal: How did you do that?
Berdinis: We didn’t actually put the apps out into the stores, the point where you can log in and book a load. That wasn’t until February for Android and March for iOS. So between September when we moved the first load until February, it was a lot of manual work, old-school hustle, get the loads, get the drivers.
Madrigal: Did you hire people from the other brokers?
Berdinis: Yeah, for sure. Uber has a very specific kind of ops executor. The Uber-style ops executor is very analytical, lot of them from finance backgrounds. They can work very hard and think through problems in a very analytical, data-driven way. And then there is the brokerage-style ops person, who is much more on the execution side. They know the industry really well. They can hear in the driver’s tone of voice if they are lying about a flat tire or just delayed from their previous shipment. All that kind of stuff. So, marrying the two kinds of operators together helped us build that ops team.
Madrigal: You guys decided to regionally build out. So the first market is ... Dallas?
Berdinis: We call it the Golden Triangle: Dallas, Houston, San Antonio. There are pretty even flows of freight in and out of each of those cities. So if we can capture that triangle, as soon as you drop off a load in Dallas, you can pick up a load to either Houston or San Antonio. There are other kinds of natural triangles around the country, but just within that triangle area, that makes up about 10 percent of the country’s freight.
Madrigal: Relative to other brokerages, you’re better capitalized and possibly better organized, and you don’t have to make money right away. There are a lot of advantages you guys have in going into a market like that. But what were the hard things about it?
Berdinis: When we we were starting up, before we publicly launched, most of the drivers we had talked to had never heard of Uber. They operate between cities and between cities, Uber doesn’t exist. So, it’s not top of mind. Once we did launch publicly, we started to see the camaraderie with their taxi friends. But you also hear other drivers coming and pushing back against them, saying, “With taxis, they created new supply. And that’s why there is competition. With trucking, Uber Freight is not creating new truck drivers.” We’re actually just giving loads in a more efficient way. We’re paying quickly. Over the last nine months, we’ve gotten pretty deep in the crazy pain points that drivers have and are going one by one to knock them off.
Madrigal: What are those?
Berdinis: It all comes back to earnings at the end of the day.
Madrigal: Because they are small business owners.
Berdinis: And drivers get paid from shippers, net 30– and net 60–day terms. [Meaning, the people shipping stuff have 30 or 60 days from the work being completed to pay the truckers.] If their truck breaks down, they struggle with the 60-day terms because they are working week to week. As a result, there is this huge industry called “factoring,” it’s kind of like payday loans. The trucker says, “Give me 95 percent of this receivable but today, versus making me wait 60 days.” That’s just 2–5 percent skimmed off of every single load. And when these drivers are only making a few percent profit margin, that could be all the profit they are making. The whole payment process does not work and it is causing a lot of trucking companies to go out of business.
Madrigal: What else has surprised you in making this foray into logistics?
Berdinis: I’ll start with the app itself. There was lots of apprehension at the beginning when we started calling drivers. A, they’d never heard of Uber. So the sell was hard. And B, a lot of them had never downloaded an app. They might have an iPhone, but we’d say, “Go to the App Store.” And they’d say, “What is that?” It was 45 minutes per driver walking them through the download process and password. We started to think that if we had to do this for tens of thousands of drivers, we could never scale.
But as time went on, drivers started showing up to us, instead of us going to them, it started self-selecting for drivers who know how to use apps and get it. The usage of the app was far exceeding what our expectations were.
We are seeing that not only are the ones who booked loads with us booking more loads every single week that they come back. But drivers who never booked a load with us, continued opening up the app almost every single day to check for new opportunities. We saw this crazy engagement. There’s not a lot of ways for drivers to see what loads are available out there. And just having the list and the price—that visibility in and of itself—is a huge mental shift.
Madrigal: It’s almost like the early stories around cell phones and farmers in whatever country being able to check the prices at market.
Berdinis: This is like that for a lot of these truck drivers. We were super skeptical that the divers would know how to use the app. But whether it is self-selection or whatever, we found this incredible affinity to come back and check more. We were pleasantly surprised by that.
Madrigal: Where does all this go from here? What are the next steps?
Berdinis: Texas was our original focus and yesterday, we announced six new states or regions we’ll do our same kind of density play in. That’s gonna help us understand if we can replicate the success we’ve seen in Texas in these other markets.
Madrigal: Do you have a GM for those markets, the way Uber’s passenger business would?
Berdinis: We don’t have a GM for those markets. It is all centralized from the ops team. When Uber launches new cities, they have a GM. They have a pretty standard process. For Freight, it’s not exactly like that. Because freight moves between cities. And the lines are not as clean as “Here’s Los Angeles that’s launched.”
Madrigal: Okay, that’s one shift. What’s the other big thing?
Berdinis: Up until now, the way drivers interacted with the app, they have to go into the app and search for what they want. We’ve learned that drivers have pretty specific preferences. Uber drivers don’t really have preferences. Maybe they can use this tool that helps them drive home at the end of the day. But during the day, the whole city is where they are working. With truck drivers, you can’t tell a local driver, someone driving within 100 miles of their home, to go take a load from Houston to New York. There is no point to surfacing that. So, now the app is a lot more proactive about personalizing that search-and-discovery experience.
So, we’re sending out push notifications. Hey, this load is one you’ve taken before. It just showed up on our system. Do you wanna book it? And then when they get into an app, there is a whole For You section. It’ll say: Recommended Because The Load Will Take You Home. Recommended Because You’ve Done This Load Before.
Madrigal: Netflix-style.
Berdinis: Right.
Madrigal: Are there other companies that want to digitize the brokerage business?
Berdinis: There’s a few “Uber for trucking companies.” They don’t call themselves that anymore, but they used to. There’s Transfix out of New York, Convoy out of Seattle. And if you search “Uber for trucking” you’ll see dozens that came and went.


An anonymous Google software engineer’s 10-page fulmination against workplace diversity was leaked from internal company communications systems, including an internal version of Google+, the company’s social network, and another service that Gizmodo, which published the full memo, called an “internal meme network.”
“I’m simply stating that the distribution of preferences and abilities of men and women differ in part due to biological causes,” the Googler writes, “and that these differences may explain why we don’t see equal representation of women in tech and leadership.”
Why Is Silicon Valley So Awful to Women?
The memo has drawn rage and dismay since its appearance Saturday, when it was first reported by Motherboard. It seemed to dash hopes that much progress has been made in unraveling the systemic conditions that produce and perpetuate inequity in the technology industry. That includes increasing the distribution of women and minorities in technical jobs, equalizing pay, breaking the glass ceiling, and improving the quality of life in workplaces that sometimes resemble frat houses more than businesses.
These reactions to the screed are sound, but they risk missing a larger problem: The kind of computing systems that get made and used by people outside the industry, and with serious consequences, are a direct byproduct of the gross machismo of computing writ large. More women and minorities are needed in computing because the world would be better for their contributions—and because it might be much worse without them.
* * *
Workplace equity has become a more visible issue in general, but it has reached fever pitch in the technology sector, especially with respect to women. When the former Uber engineer Susan Fowler published an explosive accusation of sexism at that company earlier this year, people took notice. When combined with a series of other scandals, not to mention with Uber’s longstanding, dubious behavior toward drivers and municipalities, the company was forced to act. CEO Travis Kalanick was ousted (although he remains on the board, where he retains substantial control).
Given the context, it’s reasonable to sneer at the anonymous Googler’s simple grievances against workplace diversity. Supposedly natural differences between men and women make them suited for different kinds of work, he argues. Failure to accept this condition casts the result as inequality, he contends, and then as oppression. Seeking to correct for it amounts to discrimination. Rejecting these premises constitutes bias, or stymies open discourse. The Googler does not reject the idea of increasing diversity in some way. However, he laments what he considers discriminatory practices instituted to accomplish those goals, among them hiring methods designed to increase the diversity of candidate pools and training or mentoring efforts meant to better support underrepresented groups.
Efforts like these are necessary in the first place because diversity is so bad in the technology industry to begin with. Google publishes a diversity report, which reveals that the company’s workforce is currently composed of 31 percent women, with 20 percent working in technical fields. Those numbers are roughly on par with the tech sector as a whole, where about a quarter of workers are women.
Racial and ethnic diversity are even worse—and so invisible that they barely register as a problem for the anonymous Googler. I was chatting about the memo with my Georgia Tech colleague Charles Isbell, who is the executive associate dean of the College of Computing and the only black tenure-track faculty member among more than 80 in this top 10–ranking program.
“Nothing about why black and Hispanic men aren’t software engineers?” he asked me after reading the letter, paraphrasing another black computer scientist, Duke’s Jeffrey R.N. Forbes. “Did I glaze over that bit?” Isbell knows that Google’s meager distribution of women far outshines its terrible racial diversity. Only 2 percent of all U.S. Googlers are black, and only 4 percent are Hispanic. In tech-oriented positions, the numbers fall to 1 percent and 3 percent, respectively. (Unlike the gender data, which is global, the ethnic diversity data is for the United States only.)
These figures track computing talent more broadly, even at the highest levels. According to data from the Integrated Postsecondary Education Data System, for example, less than 3 percent of the doctoral graduates from the top-10 ranked computer science programs came from African American, Hispanic, Native American, and Pacific Islander communities during the decade ending in 2015.
Given these abysmal figures, the idea that diversity at Google (or most other tech firms) is even modestly encroaching on computing’s incumbents is laughable. To object to Google’s diversity efforts is to ignore that they are already feeble to begin with.
* * *
The Googler’s complaints assume that all is well in the world of computing technology, such that any efforts to introduce different voices into the field only risk undermining its incontrovertible success and effectiveness. But is the world that companies like Google have brought about really one worthy of blind praise, such that anyone should be tempted to believe that the status quo is worth maintaining, let alone celebrating?
Many things are easier and even better thanks to Google search (or maps, or docs)—or Facebook, or smartphones, or any of the other wares technology companies put on offer. But overall, the contemporary, technologized world is also in many ways a hellscape whose repetitive delights have blinded the public to its ills.
Products have been transformed into services given away “free” as an excuse to extract data from users. That data is woven into an invisible lattice of coercion and control—not to mention as a source of enormous profit when sold to advertisers or other interested parties. Apps and websites are designed for maximum compulsion, because more attention means more content, and more content means more data and thereby more value. All that data is kept forever on servers corporations control, and which are engineered—if that’s the right word for it—in a way that makes them susceptible to attack and theft.
Thanks to the global accessibility of the internet, these services strive for universal deployment. Google and Facebook have billions of “customers” who are also the source of their actual products: the data they resell or broker. The leverage of scale also demands that everyone use the same service, which dumps millions together in unholy community. Online abuse is one consequence, as are the campaigns of misdirection and “fake news” that have become the front for a new cold war.
Because of that universal leverage, work of all kinds has also been upset by or consolidated in computing services. Retail, travel, entertainment, and transportation, of course, but even professions like real estate, law, and education appear to be at risk of dismantlement via automation and global dissemination. This sea change is excused by platitudes about “innovation” and “disruption.”
All told, the business of computing is infiltrated with a fantasy of global power and wealth that naturally coheres to the entrenched power of men over generations. To mistake such good fortune for inborn ability is to ignore the existence of history.
Men—mostly white, but sometimes Asian—have so dominated technology that it’s difficult even to ponder turning the tables. If you rolled back the clock and computing were as black as hip-hop, if it had been built from the ground up by African American culture, what would it feel like to live in that alternate future—in today’s alternate present? Now run the same thought experiment for a computing forged by a group that represents the general population, brown of average color, even of sex, and multitudinous of gender identity.
Something tells me the outcome wouldn’t be Google and Twitter and Uber and Facebook. It’s depressing that it takes a determined exercise in speculative fiction even to ponder how things might be different were its works made by different hands.
Not just the services or apps, either. Given that the fundamentals of computing arose from a long legacy of ideas mostly forged by white men, it’s hard to imagine how the fundamental operation of computers at the lowest level might have been different had ideas from alternative sources underwritten it.
The business of computing is also bound to incumbents. Failing to acknowledge this truth hamstrings earnest efforts to overcome that power through diversification. For example, advocating for more women entrepreneurs (about 17 percent of start-ups have a woman founder) or venture-capital partners (about 7 percent are women) seems like a terrific path toward diversity and equity. But the venture-backed start-up itself is still a slave to the marketplace design that its mostly male precursors had already created and entrenched. Change in established companies faces the same challenges. A search for a new Uber chief executive is underway, although it remains unclear whether Uber’s culture can be changed, even with a new leader.
Even the fateful Googler’s memo enjoys the spoils of a world already designed for male supremacy. What is this letter, after all, but a displaced Reddit post? Certain but non-evidential. Feigning structure, but meandering. Long and tedious, with inept prose and dead manner. This false confidence underwrites all the claims the memo contains, from its facile defense of jingoism as political conservatism to its easy dismissal of anyone not predetermined to be of use.
And Google built an “internal meme network” expressly for the purpose of sharing material like the memo in question! How to interpret such a thing except as Google’s own private Reddit, where the bravado of the white man’s internet comes home to roost at the office? Even worse, in her statement responding to the anti-diversity memo, Google’s vice president of diversity, integrity, and governance, Danielle Brown, appears to celebrate this offering as one among “so many platforms for employees to express themselves,” such that “this conversation doesn’t end with my email today.” The problem, it seems, is also its own solution.
As my colleague Mark Guzdial puts it, women used to avoid computer science because they didn’t know what it is. Now they avoid it because they know exactly what it is.
* * *
Soon, the fall term will commence at Georgia Tech. I will take to the lectern in the introductory course to our bachelor of science degree in computational media. The program also hopes to make headway against the diversity struggle. Conceived after the dot-com crash and inaugurated in 2004, the degree draws half its courses, faculty, and management from computing and half from the liberal arts. The goal was to address the increased connection between computing, expression, and communication.
The results have been promising. Computational media has achieved consistently high gender equity, for example. As of spring 2017, computer science was composed of only 24 percent women, whereas women made up 52 percent of the computational media students. That might give it the greatest proportion of women among accredited computing undergraduate majors in the country. Ethnic diversity is also better: 11 percent of computational media students are black and 9 percent are Hispanic, compared with 6 and 5 percent, respectively, in CS.
But that apparent victory might be a Pyrrhic one. All the anxieties that plague the anonymous Googler also afflict programs like ours, which provide part of the funnel to tech companies like Google. As computing rose from the dot-com ashes in the mid-2000s, enrollments skyrocketed. But computational media remains small—a tenth the size of computer science, and shrinking in total number and percentage of overall computing students during the same years CS has been on the rise. As a part of that decline, it appears to be losing men to computer science in particular, and perhaps falsely inflating the program’s claims to gender equity in the process.
When it was designed, computational media hoped to attract students with an interest in areas that blend computing and creativity, among them film, the web, television, games, and so on. That move failed to anticipate the foundational grievance that courses through the Google memo: that of “dumbing down” computing with interlopers. Students, more anxious and more professionally oriented than ever, seek the surety of the computer science degree. Academic faculty and industrial managers, meanwhile, fear yielding to “CS Lite,” a derogatory name for compromising technical expertise.
We should have known that for some computational media inevitably would threaten to feminize computing, relegating technical creativity to service work at best, emotional labor at worst. And so, while Georgia Tech can lay claim to an impressively gender-equal accredited computing degree, it’s not clear that such an accomplishment does anything more than pay lip service to diversity, distracting attention from our ever-growing contribution to the perverted reality of a world run by the computer programmers we graduate into companies like Google.
Darkened under the shadow of this Google jeremiad, I’m not sure what to say to my students when I stand before them later this month. Computation ended up having a much more widespread and much more sinister impact on media writ large—not just traditional media forms like music and news, but also on media as a name for every interaction between people and the world, from socializing to health, education to transit. It’s not possible to rewind the clock on the past, nor to burn it all down and start anew. But training up more women and minorities to service technological power’s existing ends—founding start-ups, working at Google—only transfers the lip service from educational programs to tech companies. They process diversity into glossy reports that placate shareholders and the public, all the while putting on the same show with a slightly different cast of characters.
Reader, I want so desperately to leave you with an alternative. A better option, a new strategy. One that would anticipate and defang the inevitable maws crying, “Well, what’s your solution, then?” But facile answers spun off-the-cuff by white men in power—aren’t these the things that brought trouble in the first place?
Maybe there is an answer, then, after all: Just to shut up for a minute. To stop, and to listen, and even to step out of the way. Not entirely, and not forever. But long enough, at least, to imagine how some of the lost futures of pasts left unpursued might have made for different, actual presents—and that might yet fashion new futures. Only a coward would conclude that none of them are better than the one that’s become naturalized as inevitable.


In their “hot mic” moment last week, Senators Susan Collins and Jack Reed gave cold bipartisan voice to a deep fear: The president of the United States is stunningly unprepared for his job and just may be—to use a technical political science term I learned in graduate school—two cans short of a six pack. Between the Senate’s late-night “damn the torpedoes” voting frenzy to repeal something, anything, from Obamacare, and the president’s early morning tweets proclaiming his “complete power” to pardon himself and his relatives, what used to be business as usual in Washington never looked so good.
It is comforting to think that Trump is the only thing standing between us and the good old dysfunctional ways of Washington. But I have my doubts. The president’s disruption engine is powered by three paradoxes. Each was made possible by technological innovations. All will endure long after this ringmaster moves his circus to another town.
Paradox #1: More information, less credibility
Trump’s cries about fake news get receptive audiences in part because we live in the most complex information age in human history. The volume of data is exploding, and yet credible information is harder to find. The scale of this information universe is staggering. In 2010, Eric Schmidt, the chairman of Google’s parent company Alphabet, noted that every two days, we create as much information as we did from the dawn of civilization up to 2003. Today Google processes close to 62,000 search queries a second. That’s more than 5.3 billion queries a day.
Information is everywhere, but good information is not. Why? Because the barriers to entry are so low. In the Middle Ages, when paper was a sign of wealth and books were locked up in monasteries, knowledge was considered valuable and creating it was costly. To be sure, there was some flat-earthy nonsense locked up in those tomes and religious and political rulers used their claims to knowledge as political weapons. Today the challenge is different. We now live at the opposite extreme, where anyone—from foreign adversaries to any crackpot with a conspiracy theory—can post original “research” online. And they do. Telling the difference between fact and fiction isn’t so easy. A few months ago, one of my graduate student researchers included information in a paper that I found oddly inaccurate, so I checked the footnotes. The source was “RT”—as in the outlet formerly known as Russia Today, a propaganda arm of the Kremlin. Stanford students aren’t the only ones struggling with real fake news. In December, the Pakistani defense minister rattled his nuclear saber in response to an Israeli tweet. Except the Israeli tweet wasn’t real.
Meanwhile, attitudes toward traditional information sources like the mainstream media and universities are souring, particularly among Republicans. Confidence in newspapers has declined by more than 20 points since 1977. Last month, a Pew survey found that for the first time, a majority of Republicans had a negative view of American universities.
The antidote to bad information used to be more information. Not anymore. What good is more information if people don't trust it—or if the traditional methods of sorting the good information from the bad (including the weighty brands of certain news organizations) don't work anymore? The marketplace of ideas is experiencing market failure. When information proliferates and credibility shrinks, reasoned argument suffers and democratic society decays.
Paradox #2: More connectivity, less civility
Today nearly half the world is online. By 2020 more people are expected to have cell phones than running water. But civility has not accelerated in tandem. In earlier times, it took some effort to deliver hurtful messages. In the U.K.’s Parliament building, seating in the House of Commons is designed to space the opposition at least two sword lengths apart from the ruling party—just in case.  Distance has its benefits. Years ago, I got a letter from a federal inmate claiming I was part of a 9/11 conspiracy and the murder for which he had been convicted. He went to a lot of trouble to write me with all that multi-colored ink. He even had to pay for the stamp. Now, I can get anonymous vitriol on Twitter, or in my email, or in the comments section of something I write—instantly, for free.
Sure, connectivity has created tremendous positive changes, including new markets in developing nations and new bonds among kindred spirits across vast distances. But connectivity has also made nasty discourse more convenient and socially acceptable. The step between harmful speech and chilled speech is a small one. Civility is not a convenience. It is a cornerstone of free speech in a liberal society. Trump may personify America’s descent into coarse discourse and amplify its spread. But it didn’t start and will not stop in Trump Tower or the White House. The root causes lie deeper.
Paradox #3: The wisdom of crowds, the duplicity of crowds
Technology has unleashed the wisdom of crowds. Now you can find an app harnessing the experiences and ratings of likeminded users for just about anything. The best taco truck in Los Angeles? Yelp. The highest rated puppy crate? Amazon. Youth hostels in Barcelona? TripAdvisor. Researchers are even using the wisdom of crowds to better predict which internet users may have pancreatic cancer and not even know it yet—based on the search histories of other cancer patients.
But the 2016 presidential election revealed that not all crowds are wise, or even real. The wisdom of crowds can be transformed into the duplicity of crowds. Deception is going viral.
On social media, one person can masquerade as hundreds, even thousands, with fake personas. Thanks to advances in artificial intelligence, it’s also possible to create armies of automated social media bots to develop, manipulate, and spread deceptive information at speeds and scales unimaginable before now. Facebook is so concerned about the duplicity of crowds, in April the company issued a “call to arms” report about what it’s doing to stop bad actors from manipulating public discourse and deceiving people.
Disruption used to be a good word, signifying creativity and innovation—shaking up things in a good way. The Founding Fathers were disruptive, imagining a nation ruled by laws and not kings. Their great American experiment inspired generations and helped transform half the world into democracies. NASA was disruptive, pushing the frontiers of science to land a man on the moon. Silicon Valley tech companies have disrupted all sorts of industries to become the engines of the global economy.
But disruption often has unintended consequences. More information, connectivity, and crowdsourcing are also shrinking credibility, eroding civility, and empowering the duplicity of crowds. These technological chickens are coming home to roost, and they’re likely to stay here even when Trump is gone.



Our increasingly smart machines aren’t just changing the workforce; they’re changing us. Already, algorithms are directing human activity in all sorts of ways, from choosing what news people see to highlighting new gigs for workers in the gig economy. What will human life look like as machine learning overtakes more aspects of our society?
Alexis Madrigal, who covers technology for The Atlantic, shares what he’s learned from his reporting on the past, present, and future of automation with our Radio Atlantic co-hosts, Jeffrey Goldberg (editor in chief), Alex Wagner (contributing editor and CBS anchor), and Matt Thompson (executive editor).
Links:
Further Reading:


I sit down at the table, move my napkin to my lap, and put my phone on the table face-down. I am at a restaurant, I am relaxed, and I am about to start lying to myself. I’m not going to check my phone, I tell myself. (My companion’s phone has appeared face-down on the table, too.) I’m just going to have this right here in case something comes up.
Of course, something will not come up. But over the course of the next 90 minutes I will check my phone for texts, likes, and New York Times push alerts at every pang of boredom, anxiety, relaxation, satiety, frustration, or weariness. I will check it in the bathroom and when I return from the bathroom. I don’t really enjoy this, but it is very interesting, even if some indignant and submerged part of my psyche moans that I am making myself dumber every time I look at it. As, in fact, I am.
A smartphone can tax its user’s cognition simply by sitting next to them on a table, or being anywhere in the same room with them, suggests a study published recently in the Journal of the Association for Consumer Research. It finds that a smartphone can demand its user’s attention even when the person isn’t using it or consciously thinking about it. Even if a phone’s out of sight in a bag, even if it’s set to silent, even if it’s powered off, its mere presence will reduce someone’s working memory and problem-solving skills.
These effects are strongest for people who depend on their smartphones, such as those who affirm a statement like, “I would have trouble getting through a normal day without my cell phone.”
Have Smartphones Destroyed a Generation?
But few people also know they’re paying this cognitive smartphone tax as it plays out. Few participants in the study reported feeling distracted by their phone during the exam, even if the data suggested their attention was not at full capacity.
“We have limited attentional resources, and we use some of them to point the rest of those resources in the right direction. Usually different things are important in different contexts, but some things—like your name—have a really privileged status,” says Adrian Ward, an author of the study and a psychologist who researches consumer decision-making at the University of Texas at Austin.
“This idea with smartphones is that it’s similarly relevant all of the time, and it gets this privileged attentional space. That’s not the default for other things,” Ward told me. “In a situation where you’re doing something other than, say, using your name, there’s a pretty good chance that whatever your phone represents is more likely to be relevant to you than whatever else is going on.”
In other words: If you grow dependent on your smartphone, it becomes a magical device that silently shouts your name at your brain at all times. (Now remember that this magical shouting device is the most popular consumer product ever made. In the developed world, almost everyone owns one of these magical shouting devices and carries it around with them everywhere.)
In the study, Ward and his colleagues examined the performance of more than 500 undergraduates on two different common psychological tests of memory and attention. In the first experiment, some participants were told to set their phones to silent without vibration and either leave them in their bag or put them on their desk. Other participants were asked to leave all their possessions, including their cell phone, outside the testing room.
In the second experiment, students were asked to leave their phones on their desk, in their bag, or out in the hall, just as in the first experiment. But some students were also asked to power their phone off, regardless of location.
In both experiments, students who left their phones outside the room seemed to do best on the test. They also found the trials easier—though, in follow-up interviews, they did not attribute this to their smartphone’s absence or presence. Throughout the study, in fact, respondents rarely attributed their success or failure on a certain test to their smartphone, and they almost never reported thinking they were underperforming on the tests.
Daniel Oppenheimer, a professor of psychology at the University of California, Los Angeles, noted that this effect is well-documented for enticing objects that aren’t smartphones. He was not connected to this research, though his research has focused on other vagaries of digital life. Several years ago, he and his colleagues suggested that students remember far more of a lecture when they take notes by hand rather than with a laptop.
“Attractive objects draw attention, and it takes mental energy to keep your attention focused when a desirable distractor is nearby,” Oppenheimer told me in an email. “Put a chocolate cake on the table next to a dieter, a pack of cigarettes on the table next to a smoker, or a supermodel in a room with pretty much anybody, and we would expect them to have a bit more trouble on whatever they’re supposed to be doing.”
He continued: “We know that cell phones are highly desirable, and that lots of people are addicted to their phones, so in that sense it’s not so surprising that having one visible nearby would be a drain on mental resources. But this study is the first to actually demonstrate the effect, and given the prevalence of phones in modern society, that has important implications,” he said.
Ward will continue researching the psychological costs and benefits of the new technologies that have permeated everyday life. His dissertation at Harvard looked at the implications of delegating cognitive tasks to the cloud. “Big things are happening so quickly. It’s the 10th anniversary of the iPhone, and the internet’s only been around for 25 years, yet already we can’t imagine our lives without these technologies,” he said. “The joyful aspects, or positive aspects—or the addictive aspects—are so powerful, and we don’t really know the negative aspects yet.”
“We can yell our opinions at each other, and people are going to agree or disagree with them, and set up luddites-versus-technolovers debates. But I wanted to get data,” he told me.
It’s worth noting that the type of psychological research Ward conducts—trials on willing, Western undergrads, often participating in studies to fulfill course credit—has suffered a crisis of confidence in recent years. Psychologists have had difficulty replicating some of the most famous experiments in their field, leading some to argue that all psychology experiments should be replicated before they are published. This study has not yet been replicated.
One possible consequence of Ward’s work extends beyond smartphones. Most office workers now know that “multi-tasking” is a fallacy. The brain isn’t doing two tasks at once as much as it’s making constant, costly switches between tasks. But Ward says that assiduously not multi-tasking isn’t very helpful, either.
“When you’re succeeding at not multitasking—that is, when you’re doing a ‘good job’—that’s not exactly positive as well,” he said. That’s because it takes mental work, and uses up attentional resources, to avoid distraction.
Instead, he recommends that the most dependent users just put their smartphone in another room.



I used to scandalize my friends with this confession: “I don’t own a hair dryer.”
It was as if I’d told them I ride a horse to work. But their surprise was justified: 90 percent of U.S. homes own a hair dryer. They come standard in most hotel rooms. The hair dryer is tangled up with the history of fashion, the evolution of women’s roles, and the development of gendered social spaces.
In the beginning, the hair dryer wasn’t a home appliance. In 1888, Alexandre-Ferdinand Godefroy debuted his “hair dressing device” in a French salon. It wasn’t pretty: His dryer was a clumsy, seated machine, resembling a vacuum cleaner—essentially a giant hose connected to a heat source. At the time, women wore their hair long and looped, or curled into elaborate updos. For formal occasions, they might have ribbons, feathers, or flowers woven into their locks. Godefroy’s invention aimed to speed up the labor involved with these creations. But his machine failed to circulate air effectively, so the time saved wasn’t significant. The prototype was far too unwieldy to become widespread anyway.
Hair dryers didn’t take off until the first handheld units became available, in the early 1920s. These metal, gun-shaped models arrived right when women’s hairstyles were shifting from the mountainous piles of Gibson Girl curls that required dozens of bobby pins to the tidy, easier-to-shape bobs of flappers. It was a radical break from past styles. As Rachel Maines, a technology historian at Cornell University, explained to The New York Times, “Having clean, shiny, fluffy hair—that’s a 20th-century thing.” This new trend was also happy news for the hair dryer. Dirty hair could hide in a pompadour, but a shorter ’do that hung free would reveal limp or stringy hair.
Early handheld hair dryers were still difficult to use. Their metallic (often aluminum) casings made them hard to wield. Also, drying times were far longer than today’s norm, as the devices drew only 100 watts of electricity compared to the 2,000 watts of modern versions. That made them exhausting to use over the long periods of time required for drying. Some early versions had pedestals to give tired arms a rest. Nevertheless, these dryers were considered a marvel of convenience, marketed as having “loads of hot or cold air instantly. Just by pressing the handle button.”
The handheld versions for the home were joined by hooded models for the salon. Made of metal and later of plastic, and applying an even, all-over heat, hooded dryers entered widespread use in 1930s. In the decades that followed, they became a defining trait of the salon scene.
This was an unsettled time for American women. First they joined the workforce during the war effort, in the 1940s. Later, they were driven back into the home. During these postwar years, the salon became a cherished second space for women outside the home. The task of “setting” hair into the molded hairdos popular in the day, such as Veronica Lake’s cascading S-shaped waves or Grace Kelly’s sculpted bob, required regular appointments at the salon, establishing it as a popular weekly meeting spot. The image of a row of women idly flipping through magazines under a hair dryer hood became a symbol of postwar prosperity and of women’s new leisure time.
In an effort to bring that salon cachet into the home, the bonnet hair dryer debuted in 1951. This model had a soft, shower cap-style headpiece that the user would attach to a motor via a hose. In a Sunbeam commercial from the 1960s, the bonnet dryer was advertised to be “so fast that it actually dries hair in an average of 22 minutes.” These models were also meant to mimic the salon experience: “Just select any one of four temperatures. Then, relax,” the commercial suggested. They came in little handled carrying cases that could be toted around, but typically the user would stay seated in a single spot while hot air circulated. Advertisements frequently showed models chatting on the phone, suggesting that salon-level socializing and the community it inspired wouldn’t be lost if women did their own grooming at home.
Another invention that sprang from hooded hair dryers was the “wave machine.” The hairstylist Marjorie Joyner, known for her talent in creating marcel waves, connected pot-roast rods to a dryer, and mechanized marcelling was born. Hair salons were racially segregated in these years, but the wave device became popular in both black and white salons alike. With this machine, Joyner appears to have become the first African-American woman to secure a patent.
* * *
In the 1960s and ’70s, the sexual revolution left its mark on fashion—and hair. The rigid gender divisions of the previous decades began to soften. Icons like the Beatles and the Monkees were wearing their hair longer in mod “mop tops,” and influencing other men to do the same. That helped spur the counterculture trend of long, hippie locks. Companies moved quickly to capitalize on this potential new hair-dryer market. As one Clairol ad said to its male reader: “Congratulations. You have more hair today than a year ago.” But then it explained to men that the “secret” to mastering this new look “isn’t just more hair. It’s cleaner hair, blown dry—to give it bulk and body it out.”
Hairstylists gained celebrity status in these decades, thanks to stylist-to-the-stars Vidal Sassoon and films like Shampoo, which starred Warren Beatty as a hunky hairdresser irresistible to his customers. Suddenly, a functional grooming tool had sex appeal. The stylist-as-Casanova persona can still be found today, in celebrity stylists like Harry Josh. Miranda Kerr promoted his signature dryer by blowing it across her décolletage during a photo shoot, treating it more like a seductive bottle of perfume than an appliance.
During the ’60s, plastics began to dominate consumer goods, and hair dryers were no exception. Once made from metal or occasionally Bakelite, now hair dryers joined a flood of “fantastic plastic” products facilitated by companies like DuPont and Dow Chemical. But apart from an alteration to its materials and the addition of various attachments and heat conductors, like ceramic and tourmaline, the hair dryer has changed very little since its birth. Writing for Fast Company in 2011, James Gaddy lamented the device’s boring uniformity, complaining that they “all look the same.” Gaddy denounced all models as little more than “a holding pen for the small motor-driven fan and heater inside.”
It wasn’t until the 1970s that regulations were drafted to improve dryer safety. And only as recently as 1991 were these devices legally required to contain ground fault circuit interrupters, which greatly decrease the danger of high-voltage injury or death. Older models still resurface in the news for plunking into bathtubs and electrocuting their owners, as in the case of the young Palomera sisters (ages 7 and 9), who were cooling off in the tub when their old dryer dropped in.
* * *
Hair dryers weave in and out of public and private spaces, making them different from other grooming tools. Hair depilators and eyelash curlers remain hidden behind closed doors. But hair dryers began in public and continue to occupy public space. Some salons will even place a chair in their picture window, putting the hair-drying experience on full display and marketing it to passersby.
In the last decade, hair dryers have taken up public real estate anew thanks to an explosion of hair-drying bars in urban areas that deal exclusively with the washing and drying of hair (no cuts or dyeing treatments). The company Drybar, one of the most popular, has more than 70 locations across the United States and Canada. Styles are modeled on the extremely coiffed looks paraded on the red carpets of award shows and on reality TV. These ultra-manicured hairdos are a status symbol akin to a handbag or diamond ring. Maintaining them requires a commitment of $40 to $50 per week on the impermanence of hairdos that one humid day can dismantle.
But the hair dryer may now be at a crossroads. In 2016, Dyson, the maker of vacuums, fans, and hand dryers, set out to remodeling the hair dryer. As it had done with its Airblade hand dryers, Dyson hopes to revolutionize the market, encouraging more women to take their hair back into their own hands. The company shifted the motor to the base of the dryer, making it smaller and supposedly improving drying time. Though many of Dyson’s changes are more aesthetic than functional, this is a market where looks matter.
At the same time, the fashion pendulum has begun to swing away from high-polish, TV-ready looks toward a more relaxed, no-effort appearance. Celebrities like Alicia Keys have embraced the no-makeup look, and the #iwokeuplikethis movement has reinvigorated a fresher, less preened appearance. Hair might become less conforming and more free and breezy again—which could push hot air out of the public eye and back behind the bathroom door.
I finally succumbed and bought a hair dryer. I had spent years flying out the door with a damp head of hair, but I decided my soggy morning appearance was doing me a disservice. It communicated a certain young, relaxed attitude that went against the professional adult I wanted to become. Years later, I still feel awed that after 10 minutes of fanning a dryer around, my hair can be tamed. Now I see why ads for hair dryers were once laced with a million exclamation points, and showed women who were smitten over their new grooming gadgetry. As one reads, you can store your hair dryer away “or you can keep it out in the open and make a pet out of it.”
This article appears courtesy of Object Lessons.


Over the past year, the old idea of enforcing market competition has gained renewed life in American politics. The basic idea is that the structure of the modern market economy has failed: There are too few companies, most of them are too big, and they’re stifling competition. Its supporters argue that the government should do something about it, reviving what in the United States we call antitrust laws and what in Europe is called competition policy.
Stronger antitrust enforcement—it’s enough of a thing, now, that Vox is explaining it.
The loudest supporters of this idea, so far, have been from the left. But this week, a newer and more secretive voice endorsed a stronger antitrust policy.
Steve Bannon, the chief strategist to President Donald Trump, believes Facebook and Google should be regulated as public utilities, according to an anonymously sourced report in The Intercept. This means they would get treated less like a book publisher and more like a telephone company. The government would shorten their leash, treating them as privately owned firms that provide an important public service.
What’s going on here, and why is Bannon speaking up?
First, the idea itself: If implemented, it’s unclear exactly how this regime would change how Facebook and Google run their business. Both would likely have to be more generous and permissive with user data. If Facebook is really a social utility, as Mark Zuckerberg has said it is, then maybe it should allow users to export their friend networks and import them on another service.
Both companies would also likely have to change how they sell advertising online. Right now, Facebook and Google capture half of all global ad spending combined. They capture even more global ad growth, earning more than three quarters of every new dollar spent in the market. Except for a couple Chinese firms, which have a lock on their domestic market but little reach abroad, no other company controls more than 3 percent of worldwide ad spending.
So if the idea were implemented, it would be interesting, to say the least—but it’s not going to become law. The plan is a prototypical alleged Bannonism: iconoclastic, anti-establishment, and unlikely to result in meaningful policy change. It follows another odd alleged Bannon policy proposal, leaked last week: He reportedly wants all income above $5 million to be taxed at a 44-percent rate.
Which bring me to the second point: Bannon’s proposal is disconnected from the White House policy that he is, at least on paper, officially helping to strategize. The current chairman of the Federal Communications Commission, Ajit Pai, is working to undo the rule that broadband internet is a public utility (which itself guarantees the idea of “net neutrality”). Trump named Pai chairman of the FCC in January.
Bannon’s endorsement of stronger antitrust enforcement (not to mention a higher top marginal tax rate) could very well be the advisor trying to signal that he is still different from Trump. Bannon came in as the avatar of Trump’s pro-worker, anti-immigration populism; he represented the Trump that tweeted things like:
I was the first & only potential GOP candidate to state there will be no cuts to Social Security, Medicare & Medicaid. Huckabee copied me.
As the president endorses Medicaid cuts and drifts closer to a Paul Ryan-inflected fiscal conservatism, Bannon may be looking for a way to preserve his authenticity.
Third, it’s the first time I’ve seen support for stronger antitrust enforcement from the right. So far, the idea’s strongest supporters have been Congressional Democrats. Chuck Schumer has elevated the idea to the center of the “Better Deal” policy agenda in 2018. Before that, its biggest supporters included Bernie Sanders, who railed against “Too Big to Fail” banks in his presidential campaign; and Elizabeth Warren, who endorsed a stronger competition policy across the economy last year.
Finally, while antitrust enforcement has been a niche issue, its supporters have managed to put many different policies under the same tent. Eventually they may have to make choices: Does Congress want a competition ombudsman, as exists in the European Union? Should antitrust law be used to spread the wealth around regional economies, as it was during the middle 20th century? Should antitrust enforcement target all concentrated corporate power or just the most dysfunctional sectors, like the pharmaceutical industry?
And should antitrust law seek to treat the biggest technology firms—like Google, Facebook, and perhaps also Amazon—like powerful but interchangeable firms, or like the old telegraph and telephone companies?
There will never be one single answer to these questions. But as support grows for competition policy across the political spectrum, they’ll have to be answered. Americans will have to examine the most fraught tensions in our mixed system, as we weigh the balance of local power and national power, the deliberate benefits of central planning with the mindless wisdom of the free market, and the many conflicting meanings of freedom.


Lavanderia, one of San Francisco’s largest laundromats, is an urban relic. Its peeling aquamarine walls house some 110 machines. Telenovelas play on a TV and arcade games from the 1990s are tucked into unexpected nooks. After opening in 1991, Lavanderia—like so many other laundromats in big cities—became a social hub in a neighborhood where renters lacked the space or funds for their own machines.
But, again like so many other laundromats in big cities, Lavanderia’s future is uncertain. While families have been hauling their dirty towels, sheets, and underwear there for decades, the business’s future earnings now pale in comparison to the value of the land it sits on—rents have skyrocketed in recent
The Poverty Just Over the Hills From Silicon Valley
years in the Mission District, the historically Latino neighborhood where Lavanderia, whose name means laundromat in Spanish, is located.
In its heyday, the 5,200-square-foot laundromat brought in over $1,000 a day in quarters. But in the past decade, its owner, a wealthy tech entrepreneur named Robert Tillman, has seen revenues dry up. Business was so bad at his nine other Bay Area laundromats that he sold them off over the years. Lavanderia is the only one Tillman has left, and he’d like to turn it into a 75-unit apartment building, with some units generating as much as $55,000 each year.
The erosion of Tillman’s laundromat business is a side effect of a national trend: Developers are remaking urban neighborhoods across the country, constructing apartment buildings for waves of young, wealthy workers and installing washers and dryers in each unit, leaving local laundromats without clientele. “Offering a washer and dryer in-unit is a trend we’re certainly seeing,” says Paula Munger, the director of industry research for the National Apartment Association. A recent survey by the industry group found the addition of washers and dryers to be one of the most common upgrades to apartments in recent years.
That has posed a problem for laundromats. According to data from the Census Bureau, the number of laundry facilities in the U.S. has declined by almost 20 percent since 2005, with especially precipitous drops in metropolitan areas such as Los Angeles (17 percent) and Chicago (23 percent). (While that data includes both laundromats and dry cleaners, laundromats account for the bulk of the drop.) In the disappearance of laundromats, a longtime staple of urban living, one can detect yet another way that cities have changed in response to an influx of higher-earning residents.
Collectively earning $5 billion each year, as estimated by the Coin Laundry Association, the U.S.’s coin-operated laundromats are overwhelmingly mom-and-pop operations and share a tightly knit history with the American city. While the first self-serve laundromat opened in 1934 in Fort Worth, Texas, the industry didn’t really take off until the ’50s, after many cities became more densely populated. “Like our mantra goes, ‘The more people, the more dirty clothes,’” Brian Wallace, the president of the Coin Laundry Association told me. Technological leaps in washer and dryer efficiency during the ’80s allowed the industry to expand even more. Laundromats, communal spaces that brought people together to perform a mundane chore, became a fixture of the urban experience, with Hollywood using them to stage serendipitous meetings, as it did in the 1985 film The Laundromat.
By the ’90s, the industry was strong enough to attract Tillman’s attention. Tillman, who’s now 61, earned his first fortune from tech ventures—among other things, he was behind DigitalGlobe, a satellite-imaging company that has supplied orbital views of earth to Google and the U.S. government. Back on the ground, Tillman, a graduate of Stanford’s business school, recognized that people still needed clean clothes, the age of satellites notwithstanding. Soon, he came to own 18 laundromats. “The ’90s were a great time for the laundry business,” Tillman recalls.
After the dot-com bubble burst, San Francisco’s rapid inflow of wealth has hurt his businesses. Lavanderia’s revenue has slid 33 percent since 2004, according to the business’s accounting records. Terry Smith, who repairs machines and collects the quarters at Lavanderia and other Bay Area laundromats, anecdotally reports that lately he’s been dumping fewer coins into his jingling collections sack as he makes his rounds. Even Tillman’s eight laundromats in Albuquerque were impacted by the urban transformations seen across the country. “I saw where the business was headed,” Tillman says. By 2007, he sold off all his laundromats except for Lavanderia—part of a 15-percent drop in the number of the Bay Area’s laundry facilities since the early 2000s, according to Census Bureau data.
Laundromats’ margins are further thinning as the price of water and sewage services have risen across the country. Utilities make up by far the heftiest of Lavanderia’s expenses, costing over $100,000 each year. Add to that the roughly $30,000 Tillman spends fixing his aging washers and dryers, and the laundromat is left with about $140,000 of profit each year, a number that continues to dwindle.
At the same time, laundromats were never a great bargain for low-income customers. Families can do multiple loads of laundry a week, and at a laundromat, that can cost $100 or more per month. There is rarely an alternative: Landlords are typically reluctant to install the right plumbing and hookups in already cramped apartments in what are often older buildings.
With this calculus in mind, Tillman would like to turn Lavanderia into a six-story apartment building that few of his current customers could likely afford to live in. He is hardly the first laundromat owner to conceive of such a plan. As Adam Lesser, the owner of Fiesta Laundromat, just a few blocks from Lavanderia, puts it, “I’m over here cleaning out lint. What the hell am I doing?”
Tillman filed a proposal with San Francisco’s planning department outlining his intentions three and a half years ago, but the project has stalled in the face of anti-development activism. Erick Arguello, a longtime Mission resident, heads one of the groups opposing Tillman’s project. He has seen one laundromat after another close in the neighborhood in the past several years: Super Lavar, where his family of seven used to go, turned into an upscale restaurant. Cleaner Wash, a small laundromat also in the Mission, was bought for over $1.5 million and turned into a high-end gym. “We have large families and you have to walk three or four blocks to go do your laundry,” Arguello says. “You also lose that sense of community. The laundromat was a family affair growing up.”
Meanwhile, as the project is on hold, Tillman has been putting off much-needed repairs. These days he’s trying to drum up local support at community meetings in the Mission so he can finally raze Lavanderia. Not all laundromat owners are pursuing Tillman’s route, though. In June, at the Coin Laundry Association’s conference, held every two years in Las Vegas, owners explored ways to make laundromats more appealing to a hipper clientele, such as by offering wi-fi. “The representations have been more positive and younger—a place where young people meet,” Wallace says, adding that the industry faces further challenges from on-demand laundry apps. Some laundromats have already morphed into cafes where customers can drink craft beer or sip a latte while waiting for their loads to finish. That may be a positive turn for the coin-laundry industry, but it does sound a whole lot like what’s happening to Lavanderia’s neighborhood anyway.


The New York Times likes to think of itself as a family newspaper. It is also the self-described paper of record. It may not be either, but it’s definitely not both all the time.
Take, for example, the moment when the Times had to choose whether to quote the new White House communications director in a particularly colorful tirade against his colleagues. Anthony Scaramucci, who joined the Trump administration last week, eviscerated the White House chief of staff, Reince Priebus, and the administration’s chief strategist, Stephen Bannon, in an interview with a New Yorker reporter on Wednesday.
“Reince is a fucking paranoid schizophrenic, a paranoiac,” Scaramucci said.
And also: “I’m not Steve Bannon. I’m not trying to suck my own cock.”
Then, for good measure: “I’m not trying to build my own brand off the fucking strength of the president. I’m here to serve the country.”
In this case, the Times really went for it, publishing all three quotes verbatim. Maybe not every journalist would make the same call, but most would understand why the Times went this route. Many publications try to avoid gratuitous foul language, even in quotes, unless the meaning of the thing being conveyed depends on it. Otherwise, matters of taste notwithstanding, bad language is often just distracting. Plenty of people curse in casual conversation; rarely is it actually meaningful.
But when the White House director of communications uses language like, well, you know, to describe the president’s inner circle, it’s in the public interest to know exactly what was said. (The Atlantic quoted Scaramucci, too, by the way.) The Times didn’t immediately grant my request to speak with an editor Thursday night, but a spokesperson did direct me to comments by the paper’s deputy managing editor, which he’d published to Twitter.
The Times published Scaramucci’s profanity only after top editors, including the executive editor Dean Baquet, “discussed whether it was proper,” Clifford Levy wrote. “We concluded that it was newsworthy that a top Trump aide used such language. And we didn’t want our readers to have to search elsewhere to find out what Scaramucci said.” Given what the newspaper has had to navigate before, it’s likely the vulgar reference to Bannon was the most difficult call among the three. Indeed, wrote one of the top editors at the Times, Sam Dolnick, the debate was “one for the ages.”
“A couple of years ago I got in trouble for ‘hand job.’ In a quote,” tweeted Emily Bazelon, a staff writer for The New York Times Magazine. In fact, Bazelon’s reference to hand jobs, at least the reference that appears in her 2014 magazine story about college romance, was not a direct quote but a line she paraphrased.
Either way, it’s not like the Times never prints vulgar language.
There was the Access Hollywood tape last fall, which featured Trump bragging about being able to grab women without their consent. The Times repeatedly printed the vulgar terms he used. It also published an offensive term—uh, rhymes with “blunt”—that a Trump adviser had used to describe Hillary Clinton, only to remove the word from an op-ed after the fact with a brief editor’s note flagging the change.
The Evolution of Slang
There have been other instances in which obscenities found their way into the Times. F-bombs are sprinkled throughout book excerpts, for example, and in web-only extras—quoting the poet Allen Ginsberg, in the case of a 2007 blog post. The word “fuck” also appeared in the full text of the Starr Report, which detailed President Bill Clinton’s sexual relationship with a 22-year-old White House intern, Monica Lewinsky, and which the Times printed in 1998. The report included, for instance, a quote from Lewinsky saying she wished the president would “acknowledge ... that he helped fuck up my life.” In a separate story that day, the paper described how the graphic language in the report was making things challenging for newscasters, in particular. “On CBS, Bob Schieffer looked profoundly embarrassed as he read cold from the report,” the Times wrote. Another Clinton-era curse word that made it into the paper? “Dumb-ass,” which Rolling Stone had mistakenly quoted Clinton as having said, a dispute that the Times covered.
It isn’t so easy to track the vulgarities the Times has printed, however, in part because it has used text-reading software to digitize much of its archival material. On one hand, this is why the newspaper’s archival presentation is so impressive. But it’s also why a search for any given curse word is liable to turn up a ton of false positives. To a computer, for instance, the 1975 headline “Court Shift on Sanity Debated” scans as “Court Shit on Sanity Debated.” Which is funny, sure, but not actually what the paper printed at the time. The Times online archive is full of this sort of thing.
The newspaper’s reporters have not infrequently written themselves into contortions to avoid foul language. “Barnyard expletive” is a favorite cop-out. (Personally, I prefer “baloney” if you want to get cute about it, but maybe that’s just me.) Mostly, they end up describing unsavory words in vague terms like “a vulgarity that refers to part of the male anatomy” or “a vulgarism for a part of the female anatomy.” Countless Times articles about George Carlin, the comedian who was famous for his bit about the “seven words you can never say on television,” dutifully avoided printing them. (A blessing, perhaps, in the YouTube age, as they’re best delivered by Carlin himself.) “A Master of Words, Including Some You Can’t Use in a Headline,” one article’s headline cheekily acknowledged.
These days, the paper tends to find creative workarounds for foul language. In previous eras, however, they’ve avoided covering a story altogether on account of vulgarity. A 1901 story described a trial with testimony that was “of such a character” that the Times could not print it. (They may not have had a choice; the Times noted that the papers in London, where the trial was underway, had refused to publish the testimony first.)
In a 2007 story about a hardcore punk band, the writer Kelefa Sanneh laid out clearly what it would take for the Times to print the group’s colorful name. “Well, the name won’t be printed in these pages,” Sanneh wrote, “not unless an American president, or someone similar, says it by mistake.”
“I made a mistake in trusting in a reporter,” Scaramucci tweeted on Thursday night. “It won’t happen again.”



Elon Musk and Mark Zuckerberg are having a spat about whether or not artificial intelligence is going to kill us all.
Musk, the chief of Tesla and SpaceX who has longstanding worries about the potentially apocalyptic future of AI, recently returned to that soapbox, making an appeal for proactive regulations on AI. “I keep sounding the alarm bell,” he told attendees at a National Governors Association meeting this month. “But until people see robots going down the street killing people, they don’t know how to react.”
In a Facebook Live broadcast, Zuckerberg, Facebook’s CEO, offered riposte. He called Musk a “naysayer” and accused his doomsday fears of unnecessary negativity. “In some ways I actually think it is pretty irresponsible,” Zuck scolded. Musk then retorted on Twitter: “I’ve talked to Mark about this. His understanding of the subject is limited.”
Seeing the CEOs of publicly traded tech companies go at it like Tay and Kanye is unfamiliar territory. Open sneers between public figures is normally reserved for tabloid socialites or feuding celebrities. But this is 2017—the president attempts to enact policy via Twitter, after all—so expectations must be adjusted. Rappers and reality-television stars feud because their prosperity is directly yoked to their public image. That’s true for tech business leaders now, too. Musk and Zuckerberg aren’t engaged in a debate about ideas. They are peacocking their personal identities in order to serve their future interests.
* * *
I’ve argued before that “artificial intelligence” has become so overused that the term is almost meaningless. Like “algorithm” before it, technologists, businesspeople, and journalists wield the idea like a magic wand that turns ordinary computer software and devices into world-saving (or world-ending) marvels. And given AI’s long history of wonder and dread in science fiction, people are primed to expect it to usher in utopia or dystopia.
When a term has a wealth of possible meanings, it is easy to ascribe one’s favorite meaning to it. “Disruption” is like this, as is “fake news.” The term “climate change” is now used by the right and left alike for opposite purposes. The Republican talking-points pollster Frank Luntz advocated for it over “global warming” to the G.W. Bush administration, because it sounded less severe. Change can be good, the reasoning goes.
Artificial intelligence has left the orbit of computer science, and even science fiction, and become an abstract talking point. When people make use of it, especially powerful actors like Musk and Zuckerberg, it serves a perlocutionary function: as personal branding.
When it comes to personal brands, Musk’s is easier to characterize. He’s long been compared to Tony Stark, the fictional industrialist and alter ego of Iron Man in Marvel comics. After Musk sold his first company, an online publishing service called Zip2, to AltaVista for $307 million in 1999, he co-founded X.com, which was eventually renamed PayPal and sold to eBay for $1.5 billion in 2002. Musk’s PayPal partner Peter Thiel turned to venture investing with the spoils, but Musk decided to make space rockets instead and SpaceX was born. His subsequent ventures, including electric/autonomous car maker Tesla, solar-cell manufacturer SolarCity, the Hyperloop tube-transit concept, and the new, associated tunneling-equipment firm The Boring Company—all of these ventures represent infrastructural invention of the Tony Stark variety.
As the statistician Mark Palko recently noted, Musk has a material interest in maintaining the Tony Stark alter-ego persona. When Musk waxes futuristic on self-driving cars, underground transit, brain-embedded computers, or Mars colonies, he reinforces the current and future value of his various ventures.
Portraying AI as an existential threat to humanity is consistent with this interest. If intelligent machines might strip humanity of its unmatched leverage over the natural and artificial environment, then industrial solutions must be pursued in order to stop them. Even if the threat of a robot apocalypse is unlikely, Musk has reason to advocate for aggressive contingency plans.
It’s difficult to match Zuckerberg’s business persona to a specific comic-book hero (Peter Parker? Reed Richards?). But unlike Musk, Zuck’s business and personal interests reside at the level of ideas rather than materials. Facebook is his singular venture, an enormously successful company that deals entirely in digitized text, images, video, and sound. These are representations—ideas and concepts—rather than concrete goods.
When Zuckerberg has looked beyond these immaterial representations, he has always done so in order to corner the market on more opportunities for symbol-creation and dissemination. Facebook’s purchase of Instagram and WhatsApp offer examples. And Zuckerberg’s big hardware acquisition, the VR-headset maker Oculus, represents a new terrain for virtual experience, not a new means of taming cities, continents, or the cosmos.
From this vantage point, software is always friendly and tame—or at least domesticable. Zuckerberg has billions of users and millions of advertisers who want to reach them, and terror about the future of computers only alienates those ordinary people from the friendly future he hopes to deliver to them. Zuckerberg learned this lesson the hard way, when his demonstration of a home-grown AI for his house, which he named Jarvis, was met with sneers and mockery. His recent tour of ordinary people and places in the United States shows just how completely he learned this lesson. The man is newly serious about reinforcing computing as a friendly, or at least innocuous, force on everyday life.
Especially given Facebook’s undeniable impact on the 2016 election—a feat that is hardly benign, but which Zuckerberg seems to have defused expertly anyway. This is why Zuckerberg is an “optimist,” as he puts it, when it comes to artificial intelligence. To say otherwise would suggest that computers are intrinsically risky. That fear, even if hypothetical, has potentially dire consequences for Zuckerberg’s business and personal future.
* * *
When figures like Musk and Zuckerberg talk about artificial intelligence, they aren’t really talking about AI—not as in the software and hardware and robots that might produce delight or horror when implemented. Instead they are talking about words, and ideas. They are framing their individual and corporate hopes, dreams, and strategies. And given Musk and Zuck’s personal connection to the companies they run, and thereby those companies’ fates, they use that reasoning to help lay the groundwork for future support among investors, policymakers, and the general public.
On this front, it’s hard not to root for Musk’s materialism. In an age when almost everything has become intangible, delivered as electrons and consumed via flat screens, launching rockets and digging tunnels and colonizing planets and harnessing the energy of the sun feel like welcome relief. But the fact that AI itself is an idea more than it is a set of apparatuses suggests that Zuckerberg might have the upper hand. Even if it might eventually become necessary to bend the physical world to make human life continuously viable, the belief in that value starts as a concept, not a machine.


Toddlers crave power. Too bad for them, they have none. Hence the tantrums and absurd demands. (No, I want this banana, not that one, which looks identical in every way but which you just started peeling and is therefore worthless to me now.)
They just want to be in charge! This desire for autonomy clarifies so much about the behavior of a very small human. It also begins to explain the popularity of YouTube among toddlers and preschoolers, several developmental psychologists told me.
If you don’t have a 3-year-old in your life, you may not be aware of YouTube Kids, an app that’s essentially a stripped-down version of the original video blogging site, with videos filtered by the target audience’s age. And because the mobile app is designed for use on a phone or tablet, kids can tap their way across a digital ecosystem populated by countless videos—all conceived with them in mind.
The videos that surface on the app are generated by YouTube’s recommendation algorithm, which takes into account a user’s search history, viewing history, and other data.* The algorithm is basically a funnel through which every YouTube video is poured—with only a few making it onto a person’s screen.
This recommendation engine poses a difficult task, simply because of the scale of the platform. “YouTube recommendations are responsible for helping more than a billion users discover personalized content from an ever-growing corpus of videos,” researchers at Google, which owns YouTube, wrote in a 2016 paper about the algorithm. That includes many hours of video uploaded to the site every second of every day. Making a recommendation system that’s worthwhile is “extremely challenging,” they wrote, because the algorithm has to continuously sift through a mind-boggling trove of content and instantly identify the freshest and most relevant videos—all while knowing how to ignore the noise.
And here’s where the ouroboros factor comes in: Kids watch the same kinds of videos over and over. Videomakers take notice of what’s most popular, then mimic it, hoping that kids will click on their stuff. When they do, YouTube’s algorithm takes notice, and recommends those videos to kids. Kids keep clicking on them, and keep being offered more of the same. Which means video makers keep making those kinds of videos—hoping kids will click.
This is, in essence, how all algorithms work. It’s how filter bubbles are made. A little bit of computer code tracks what you find engaging—what sorts of videos do you watch most often, and for the longest periods of time?—then sends you more of that kind of stuff. Viewed a certain way, YouTube Kids is offering programming that’s very specifically tailored to what children want to see. Kids are actually selecting it themselves, right down to the second they lose interest and choose to tap on something else. The YouTube app, in other words, is a giant reflection of what kids want. In this way, it opens a special kind of window into a child’s psyche.
But what does it reveal?
“Up until very recently, surprisingly few people were looking at this,” says Heather Kirkorian, an assistant professor of human development in the School of Human Ecology at the University of Wisconsin-Madison. “In the last year or so, we’re actually seeing some research into apps and touchscreens. It’s just starting to come out.”
Kids’ videos are among the most watched content in YouTube history. This video, for example, has been viewed more than 2.3 billion times, according to YouTube’s count:

You can find some high-quality animation on YouTube Kids, plus clips from television shows like Peppa Pig, and sing-along nursery rhymes. “Daddy Finger” is basically the YouTube Kids anthem, and ChuChu TV’s dynamic interpretations of popular kid songs are inescapable.

Many of the most popular videos have an amateur feel. Toy demonstrations like surprise-egg videos are huge. These videos are just what they sound like: Adults narrate as they play with various toys, often by pulling them out of plastic eggs or peeling away layers of slime or Play-Doh to reveal a hidden figurine.
Kids go nuts for these things.
Here’s a video from the YouTube Kids vloggers Toys Unlimited that’s logged more than 25 million views, for example:

The vague weirdness of these videos aside, it’s actually easy to see why kids like them. “Who doesn’t want to get a surprise? That’s sort of how all of us operate,” says Sandra Calvert, the director of the Children’s Digital Media Center at Georgetown University. In addition to surprises being fun, many of the videos are basically toy commercials. (This video of a person pressing sparkly Play-Doh onto chintzy Disney princess figurines has been viewed 550 million times.) And they let kids tap into a whole internet’s worth of plastic eggs and perceived power. They get to choose what they watch. And kids love being in charge, even in superficial ways.
“It’s sort of like rapid-fire channel surfing,” says Michael Rich, a professor of pediatrics at Harvard Medical School and the director of the Center on Media and Child Health. “In many ways YouTube Kids is better suited to the attention span of a young child—just by virtue of its length—than something like a half-hour or hour broadcast program can be.”
Rich and others compare the app to predecessors like Sesame Street, which introduced short segments within a longer program, in part to keep the attention of the young children watching. For decades, researchers have looked at how kids respond to television. Now they’re examining the way children use mobile apps—how many hours they’re spending, which apps they’re using, and so on.
It makes sense that researchers have begun to take notice. In the mobile internet age, the same millennials who have ditched cable television en masse are now having babies, which makes apps like YouTube Kids the screentime option du jour. Instead of being treated to a 28-minute episode of Mr. Rogers’s Neighborhood, a toddler or preschooler might be offered 28 minutes of phone time to play with the Daniel Tiger’s Neighborhood app. Daniel Tiger’s Neighborhood is a television program, too—a spin-off of Mr. Rogers’s—aimed at viewers aged 2 years old to 4 years old.
But toddlers and preschoolers are actually pretty separate groups, as far researchers are concerned. A 2-year-old and a 4-year-old might both like watching Daniel Tiger, or the same YouTube Kids video, but their takeaway is apt to be much different, Kirkorian told me. Children under the age of 3 tend to have difficulty taking information relayed to them through a screen and applying it to real-life situations. Many studies have reached similar conclusions, with a few notable exceptions. Researchers recently discovered that when a screentime experience becomes interactive—Facetiming with Grandmère, let’s say—kids under 3 years old actually can make strong connections between what’s happening onscreen and offscreen.
Kirkorian’s lab designed a series of experiments to see how much of a role interactivity plays in helping a young child transfer information this way. She and her colleagues found striking learning differences among what young children learned—even kids under 2 years old—when they could interact with an app versus when they were just watching a screen. Other researchers, too, have found that incorporating some sort of interactivity helps children retain information better. Researchers at different institutions have different definitions of “interactivity,” but in one experiment it was an act as simple as pressing a spacebar.
“So there does seem to be something about the act of choosing, having some kind of agency, that makes a difference for little kids,” Kirkorian says. “The speculative part is why that makes a difference.”
One idea is that kids, especially, like to watch the same things over and over and over again until they really understand it. I watched the Dumbo VHS so many times as a little kid that I would recite the movie on long car rides. Apparently, this is not unusual—at least not since the age of VCRs and, subsequently, on-demand programming and apps. “If they have the opportunity to choose what they’re watching, then they’re likely to interact in a way that meets their learning goals,” Kirkorian says. “We know the act of learning new information is rewarding, so they’re likely to pick the information or videos that are in that sweet spot.”
“Children like to watch the same thing over and over,” says Calvert, of Georgetown. “Some of that is a comprehension issue, so they’ll repeatedly look at it so they can understand the story. Kids often don’t understand people’s motives, and that’s a major driver for a story. They don’t often understand the link between actions and consequences.”
Young kids are also just predisposed to becoming obsessive about relatively narrow interests. (Elephants! Trains! The moon! Ice cream!) Around the 18-month mark, many toddlers develop “extremely intense interests,” says Georgene Troseth, an associate professor of psychology at Vanderbilt University. Which is part of why kids using apps like YouTube Kids often select videos that portray familiar concepts—ones that feature a cartoon character or topic they’re already drawn to. This presents a research challenge, however. If kids are just tapping a thumbnail of a video because they recognize it, it’s hard to say how much they’re learning—or how different the app environment really is from other forms of play.
Even the surprise-egg craze isn’t really novel, says Rachel Barr, a developmental psychologist at Georgetown. “They are relatively fast-paced and they include something that young children really like: things being enclosed and unwrapped,” she told me. “I have not tested it, but it seems unlikely that children are learning from these videos since they are not clearly constructed.”
“Interactivity is not always a good thing,” she added.
Researchers differ on the degree to which YouTube Kids is a valuable educational tool. Obviously, it depends on the video and the involvement of a caregiver to help contextualize what’s on screen. But questions about how the algorithm works also play a role. It’s not clear, for instance, how heavily YouTube weighs previous watching behaviors in its recommendation engine. If a kid binge-watches a bunch of videos that are lower quality in terms of learning potential, are they then stuck in a filter bubble where they’ll only see similarly low-quality programming?
There isn’t a human handpicking the best videos for kids to watch. The only human input on YouTube’s side is to monitor the app for inappropriate content, a spokesperson for YouTube told me. Quality control has still been an issue, however. YouTube Kids last year featured a video that showed Mickey Mouse-esque characters shooting one another in the head with guns, Today reported.
“The available content is not curated but rather filtered into the app via the algorithm,” said Nina Knight, a YouTube spokesperson. “So unlike traditional TV, where the content is being selected for you at a specified time, the YouTube Kids app gives each child and family more of the type of content they love and anytime they want it, which is incredibly unique.”
At the same time, the creators of YouTube Kids videos spend countless hours trying to game the algorithm so that their videos are viewed as many times as possible—more views translate into more advertising dollars for them. Here’s a video by Toys AndMe that’s logged more than 125 million views since it was posted in September 2016:

“You have to do what the algorithm wants for you,” says Nathalie Clark, the co-creator of a similarly popular channel, Toys Unlimited, and a former ICU nurse who quit her job to make videos full-time. “You can’t really jump back and forth between themes.”
What she means is, once YouTube’s algorithm has determined that a certain channel is a source of videos about slime, or colors, or shapes, or whatever else—and especially once a channel has had a hit video on a given topic—videomakers stray from that classification at their peril. “Honestly, YouTube picks for you,” she says. “Trending right now is Paw Patrol, so we do a lot of Paw Patrol.”
There are other key strategies for making a YouTube Kids video go viral. Make enough of these things and you start to get a sense of what children want to see, she says. “I wish I could tell you more,” she added, “But I don’t want to introduce competition. And, honestly, nobody really understands it. ”
The other thing people don’t yet understand is how growing up in the mobile internet age will change the way children think about storytelling. “There’s a rich set of literature showing kids who are reading more books are more imaginative,” says Calvert, of the Children’s Digital Media Center. “But in the age of interactivity, it’s no longer just consuming what somebody else makes. It’s also making your own thing.”
In other words, the youngest generation of app users is developing new expectations about narrative structure and informational environments. Beyond the thrill a preschooler gets from tapping a screen, or watching The Bing Bong Song video for the umpteenth time, the long-term implications for cellphone-toting toddlers are tangled up with all the other complexities of living in a highly networked on-demand world.

* Unlike YouTube’s main website, YouTube Kids does not use an individual child’s geographic location, gender, or age to make recommendations, a spokesperson told me. YouTube Kids does, however, ask for a user’s age range. The YouTube spokeswoman cited the Children's Online Privacy Protection Rule, a Federal Trade Commission requirement for operators of websites aimed at kids under 13 years old, but declined to answer repeated questions about why the YouTube Kids algorithm used different inputs than the original site’s algorithm.


On Monday, the editorial staff of Snopes.com wrote a short plea for help. The post said that the site needed money to fund its operations because another company that Snopes had contracted with “continues to essentially hold the Snopes.com web site hostage.”
“Our legal team is fighting hard for us, but, having been cut off from all revenue, we are facing the prospect of having no financial means to continue operating the site and paying our staff (not to mention covering our legal fees) in the meanwhile,” the note continued.
It was a shocking message from a website that’s been around for more than 20 years—and that’s become a vital part of internet infrastructure in the #fakenews era. The site’s readers have responded. Already, more than $92,000 has been donated to a GoFundMe with a goal of $500,000.
So, what’s going on? Well, it probably won’t surprise you that there’s a startup tech company and a lawsuit involved. There are claims and counterclaims. But if you want to know the gory details that are available in the court filings, here we go.
Snopes began in the early 1990s as a small website built by the husband-and-wife team of David and Barbara Mikkelson. Snopes was what you sent to your cousins who circulated crazy conspiracy theories from their Hotmail accounts. In 2003, the Mikkelsons  founded a parent company, Bardav, for the site.
All the way up into the 2010s, it had that look and feel, too, of a previous era of the internet. And perhaps because of that, its pronouncements on the veracity of subjects had a kind of authority that other media fact-checkers lacked. People, at least as many as possible in today’s crazed informational environment, trusted Snopes.
The founders divorced in 2015, some titillating details of which became public. Both founders received 50 percent of the company.
In the summer of that year, Bardav had entered into an agreement with a newish San Diego company called Proper Media to “provide content and website development services as well as advertising sales and trafficking” to Snopes. Proper Media’s principals were Chris Richmond, who co-owns a wiki called TV Tropes, and Drew Schoentrup, both now described in court filings as residents of Puerto Rico (more on that shortly).* Each of these men had a 40 percent share in the company. They were joined by three other people who had smaller equity stakes: Tyler Dunn, Ryan Miller, and Vincent Green.
In July 2016, Barbara Mikkelson sold her half of Bardav to these five men, leaving her ex-husband with five new partners in the company. Because Bardav was an S corporation, its shareholders had to be people, not other companies. So, the stock purchase agreement between Mikkelson and the men assigned them each equity on the same split that they had in Proper Media.
Diamond Creek Capital financed a big chunk of the deal with help from Barbara Mikkelson herself. Each of the five men in on the deal from the Proper Media side signed personal-liability notes with Diamond Creek Capital.
For a time, it seemed as if the arrangement was working out. The San Diego Union Tribune visited the Proper Media offices, out of which Snopes employees were working. The story featured Vincent Green, a former Marine who’d been an intern only months before, and Brooke Binkowski, the site’s managing editor and a long-time journalist.
“Before we came on board, there was not even a content-management system for the site,” Green told the paper. “It was an excruciating process for developing content. What you see now is our quick and dirty change-over from 20 years of bad code to something more responsive and functional.”
But behind the scenes, there was trouble. Proper Media’s CEO and president had moved to Puerto Rico, according to a cross-complaint filed by Green, and corroborated by their own filings. They set up a separate company there, which Green claims was a tax-avoidance scheme that he told them he was uncomfortable with.
Meanwhile, in a story as old as media, the site’s editors worried that the co-owners didn’t understand what Snopes was, and that they only wanted to juice its revenues, so they could sell it.
On February 18—in a much disputed series of events—Green and Proper Media’s largest shareholders, Richmond and Schoentrup, had a contentious meeting. In the weeks that followed, Green either left or was forced out, and he went to work at Bardav, which is to say Snopes, where he remains.
On March 10, in an action that Proper Media disputes, David Mikkelson canceled the contract that had been in place governing interactions between Bardav and Proper Media. Mikkelson claims that he had a right to do so as CEO and sole director. Proper Media says that he could not because it was understood that Drew Schoentrup was a director of the company as well, even though he had not been elected through a formal process.
Also in dispute are Green’s shares, which when combined with Mikkelson’s, would give the two of them putative control of the company. Proper Media contends that, more or less, the shares belonged to Green and he was just holding them on behalf of the company.
There are many other claims and counterclaims flying around the filings related to the lawsuit. It’s not interesting to go through all of them in detail, but what can be said: This is a mess.
Proper Media’s lawyer, Karl Kronenberger, told me that they’ve alleged that “David Mikkelson has engaged in gross financial, technical, and corporate mismanagement.” Mikkelson told me that Proper Media “continue to hold themselves out as authorized advertising representatives. They have continued to collect the revenue and they have not paid us any advertising revenue.”
What does the future hold for Snopes? That could become slightly more clear next Friday, when there is a hearing in San Diego to address competing motions. Mikkelson is seeking an injunction to force Proper Media to hand over control of the site. Meanwhile, Proper Media is seeking to remove Mikkelson as a director of Bardav.
In the meantime, it looks like the GoFundMe will at least cover the site running for a while longer, but based on conversations with those who know the site’s financial picture, Snopes’s operating expenses are close to $100,000 a month. If a resolution to the dispute isn’t reached soon, it could mean the end of both Proper Media and Snopes.
Which would be a terrible end for the kind of website that bracingly defied the logic of corporate digital media. It hadn’t pivoted to video. It was a site people trusted. It was technologically unsophisticated. It was profitable.
Stay tuned.
* This article originally stated that Chris Richmond founded TV Tropes. We regret the error.


You have to be a certain level of Twitter-obsessed to know what I mean when I say we’ve entered another Taye Diggs situation, so allow me to explain.
A few years ago, the actor Taye Diggs started following a lot of journalists on Twitter. And journalists being journalists—not exactly a humble bunch—started to casually brag to one another about it, only to find out that Diggs was following seemingly everyone on Twitter. (Same thing with the actress Melissa Joan Hart. Sorry to be the one to break it to you.)
It finally came out that Diggs was following so many random people (699,000 at last count) because he’d hired a “social network dude” to do it for him, as The Washington Post reported in 2014. In other words, it was a branding thing.
It’s fair to guess that something similar is going on with Anthony Scaramucci, the new White House communications director. Amid the news that Scaramucci would join the White House (and that press secretary Sean Spicer had resigned in protest), people began noticing that Scaramucci was following them on Twitter. More than a dozen of my colleagues at The Atlantic, at least, are among the followed.
Plus this person:
I just noticed that that scaramucci fellow is following me on these here twitters. 😯
And this one:
I'm confused. Not sure when Anthony Scaramucci began following me ?? pic.twitter.com/GCJL7Uh7lc
And this one:
just doing some research on anthony scaramucci and he's already following me? pic.twitter.com/j07JmPdc9x
And this one:
Jesus, @Scaramucci is following me...
But not this person:
At this point I'm kinda offended now that Scaramucci *isn't* following me tbh
What’s the advantage of following so many randos, one might ask? In some cases, it’s a strategy to increase a person’s follower count—operating on the principle that if you follow someone on a social platform, that person will follow you back. Get enough followers and you may even grab the attention of an algorithm that will suggest you as someone to follow. (Scaramucci is following 168,000 accounts; he has 487,000 followers.)
In other words, it’s a way of being opportunistic and garnering people’s attention—without actually ever engaging with them. So, the perfect strategy for someone in politics, a cynic might say.
Incidentally, Scaramucci is following me, too. So I sent him a message to ask about it. He didn’t write back.


Last year I fell in love with a toaster.
It looks like most others. A brushed, stainless-steel housing. Four slots, to accommodate the whole family’s bread-provisioning needs. It is alluring but modest, perched atop the counter on proud haunches.
But at a time when industry promises disruptive innovation, Breville, the Australian manufacturer of my toaster, offers something truly new and useful through humility rather than pride.
The mechanism that raises and lowers the bread from the chassis is motorized. After I press a button atop the frame, the basket silently lowers the bread into the device to become toast. On its own, this feature seems doomed to mechanical failure. But the risk is worthwhile to facilitate the toaster’s star ability: the “A Bit More” button. That modest attribute offers a lesson for design of all stripes—one that could make every designed object and experience better.
* * *
Toast is an imperfect art. Different breads brown at different rates. Even with the very same bread, similar toaster settings can produce varied results. When my bread doesn’t come up dark enough, I dial in a guess for another browning run. Usually I go overboard and burn the toast in the process. It’s toaster telephone game.
The “A Bit More” button enters here, at the friction point between good and great toast. When the toast reveals itself to me above the Breville’s chassis, I visually gauge its browness. If insufficient, I press the button, which actuates the basket motor. Down it goes for a brief, return visit to the coil. Then back up again, having been toasted, well, just a bit more.
The button also makes toasting bread, normally a quantitative act, more qualitative. The lever dials in numerical levels of browning, and the “A Bit More” button cuts it with you-know-what-I-mean ambiguity. That dance between numbers and feelings apologizes even for a slightly over-browned slice of toast by endearing the eater to the result the button helped produce.
Sure, I’m talking about toast. But Breville’s “A Bit More” Button is nothing short of brilliant. It highlights an obvious but still unseen problem with electric toasters, devices that have been around for more than a century. And then it solves that problem in an elegant way that is also delightful to use. It’s just the kind of solution that designers desperately hope to replicate, and users hope to discover in ordinary products. But agreeing on a method for accomplishing such achievements is harder.
The “A Bit More” Button was conceived by the industrial designer Keith Hensel, who worked for Sunbeam and then as Breville’s principal designer until his unexpected death in 2013, at the age of 47. His specialty was household products, like toasters, kettles, and blenders.
Breville’s head designer, Richard Hoare, tells me that Hensel, with whom he worked closely, fell upon the idea by “focusing on user empathy.” Hensel had been pondering the problem people have with toasters. “Your bread comes up too light, so you put it back down, then get distracted and forget, and it goes through a full cycle and burns,” Hoare relates. “Keith thought, why can’t the consumer have more control? Why can’t they have ‘A Bit More?’”
According to Hoare, the design team called the button by that name from the start. Some people within Breville thought it was too colloquial, and other options were considered. “Extra Darkness” was one, and “10% Extra” another. “These were confusing and clunky,” says Hoare. “In the end ‘A Bit More’ was the clearest.” Breville, which holds several patents in motorized toaster basket tech, started selling toasters with the feature in 2008.
When it came to persuading Breville to adopt and manufacture the idea, Hoare admits that it took some time for people to see the significance. I imagined that manufacturing cost and complexity might have been a factor, but Hoare waves that off. Instead, he tells me that describing features in a way ordinary users might, rather than by means of brand-speak, had sparked debate between design and marketing. The design team insisted that the colloquial version would resonate with users. “We have had so many say, ‘I love that it’s actually called A Bit More,’” says Hoare. “‘It was so cool when I saw it printed there next to the button.’”
* * *
The explanation elicits nods of respect. This, it seems, is how good design happens. But how would a designer, or a business, or even an ordinary person replicate the triumph of “A Bit More” in other contexts?
Hoare’s recollection corresponds with a trend in contemporary design practice—and one that claims to be particularly adept at producing outcomes like “A Bit More.” It’s called user-experience, or UX, design, a discipline that strives to craft pleasurable and useful encounters between people and things. Originally derived from human-computer interaction, or HCI, where user-interface design was its ancestor, UX purports to offer a general approach to design of all kinds, from software design to product design to architecture and urban planning.
But UX practice talks out of both sides of its mouth. On the one hand, UX fancies itself an empirical discipline. Its processes include ethnographic user research, specification drafting, iterative design, user testing, and so forth. UX inherits mid-century form-follows-function design ideals. It also embraces more recent trends, like participatory design, which deeply integrates stakeholders into the design process. Data are often incorporated into UX for affirming, denying, or directing elsewhere a design team’s attention.
On the other hand, UX design also privileges out-of-the-box genius to solve design problems. Apple, often considered to typify UX, is famous for conducting design in secret via a small cadre of geniuses. Steve Jobs is the ultimate example, a figure who held that “people don’t know what they want until you show it to them.” In the design-genius mentality, how a toaster (or smartphone, or building) ought to work becomes a type of soothsaying, whereby the designer earns the status of mastermind. Research becomes retrospective justification, the designer’s ingenuity validated by user adoption of the product—irrespective of how well it really serves their goals or interests.
Neither polarity of UX-style design really helps explain how one might best arrive at Breville’s “A Bit More” button. On one side is intuition. Keith Hensel, the genius who died too soon, possessed a sixth sense for taming the Maillard reaction and a congenial manner for proselytizing his solution. On the other side is evidence, via the research and participant observation conducted to cash out the “user empathy” Hoare cites as a compass bearing.
UX proponents tell tall tales about how good design really takes place. Bottom-up, evidentiary design implies that the designer is ultimately unnecessary, a mere facilitator who draws out a solution from the collective. The designer becomes a bureaucrat. And top-down, genius design becomes indistinguishable from salesmanship. As a result, design dissolves into other, more established disciplines like business intelligence, product marketing, and corporate evangelism. It’s an error that makes good design look far easier and more replicable than it really is. And worse, it allows people to conclude that their own expertise—from data analytics to advertising to illustration—is a sufficient stand-in for design.
* * *
The error comes from two assumptions. The first holds that human beings are the only arbiters of designed objects and experiences. A design is good if it works for humans, even if individual humans might disagree about what that means. In this conception, design is a purely instrumental process, carrying out human will with ever-increasing refinement.
But design-by-genius proves that this approach is often flawed. Jobs was right: People don’t seem to know what they want, or need. Who could imagine verbalizing even the relatively simple challenge of toasting such that the “A Bit More” button would emerge as a remedy? Some UX proponents have realized this trap and reframed their design practice as a dialectic between the gifted and the rabble. But soon enough, the resulting compromises again feel like a smorgasbord of every possible influence, from management to technology to sociology. Design becomes a nickname for any possible approach to design, which reduces it to a shibboleth for designers.
The second erroneous assumption is that contemporary designers believe they are reformers. Agents of change. It could be social or political change. It could be aesthetic or cultural change. It could be the selfish change of professional aspiration and its related station. It could be the change associated with progress. Designers are ambitious sorts of folk—arrogant, even—and none would want to be associated with stasis, or even with mere cyclicality. What a waste, just to mow lawns or brown bread every day! Let us instead reinvent lawn care! Let us reinvent breakfast!
But neither Hensel nor Hoare adopt such hubris over toasters. Breville’s toaster is not remarkable because it reinvents toasting, nor because it resolves an obvious user grievance, nor because it changes the world. What makes the device compelling is that it makes toasting what it always was, but even more so. Everything remains the same, but ever so slightly adjusted. Trim-tab-tweaked to more adeptly sail the current tack, the same one all bread-browners have navigated since the advent of the electric toaster.
Counterintuitively, design is a process conducted for the benefit of human use, but which takes place outside the domain of human experience. Unlike politics, entertainment, health, and other domains that also impact human life directly, design exerts force on the material world before the human one. (Design shares this quirk with engineering, but engineers are fortunate enough to work at technical depths, where ordinary people can’t see.)
To design a toaster (or a building, or a smartphone) effectively, the designer must depart the realm of the human, at least temporarily, and enter the universe of the object. Despite what the UX folk might say, designers are not diplomats negotiating treaties between humanity and toasterdom. Instead, they are explorers who breach the alien frontier of toasterness. Certainly, that cosmos does not exclude the human agents who would brown bread for sustenance. But it does not rely upon them exclusively, either. Toasting is a practice bigger than people and toasters both, otherwise it would be possible without one party or the other.
* * *
Allow me to indulge an analogy from philosophy. In both the genius and consensus registers, UX design predicates its success on knowledge: either the second sight of the designer, or the negotiated consensus of the user. Philosophers call the study of knowledge epistemology, and this approach to design is entirely epistemological. Just find the proper knowledge and the right design will emerge.
But when conducted best—including in Breville’s case, and despite Hoare’s insistence otherwise—design is more related to the philosophy of what things are, called ontology. It is a discipline of essence, that great bugbear of contemporary life, not of knowledge. Pursuing greater compatibility with a thing’s essence requires that the designer focus on the abstraction formed by the designed object and its human users together—whether it be toasting, dwelling, publishing, socializing, or anything else.
The designer’s job is not to please or comfort the user, but to make an object even more what it already is. Design is the stewardship of essence—not the pursuit of utility, or delight, or form. This is the orientation that produces solutions like the Breville “A Bit More” button. The design opportunities that would otherwise go unnoticed emerge not from what people know about or desire for toasting, but from deeply pursuing the nature of toasting itself.
The distinction might seem academic. Isn’t the UX-oriented toaster designer simply seeking deeper knowledge of the essence of toasting? Doesn’t the designer-genius simply have special, immediate access to that deep nature? Indeed, that’s possible. But mostly through accident. The designer who starts from the problem of knowledge will only ever find essence by happenstance. And that is why so few things feel like the “A Bit More” button. They are struggling so hard to meet requirements, please people, or exalt the arbitrary choices of their creators that they fail to pay respect to the thing in question—and thereby to see what it is, even, let alone to change it for the better.


Photography owes much of its early flourishing to death. Not in images depicting the aftermath of violent crimes or industrial accidents. Instead, through quiet pictures used to comfort grieving friends and relatives. These postmortem photographs, as they are known, were popular from the mid-19th through the early-20th centuries—common enough to grace mantelpieces. Many can be viewed anew at online resources like the Thanatos Archive.
Historians estimate that during the 1840s, the medium’s first decade, as cholera swept through Britain and America, photographers recorded deaths and marriages by a ratio of three to one. Budding practitioners had barely learned to handle the bulky machinery and explosive chemicals before they were asked to take likenesses of the dead: to bend lifeless limbs into natural poses and mask tell-tale signs of sickness, racing against rigor mortis.
Many people find photos of the dead creepy or morbid. No question, postmortem photographs are sorrowful images. They capture the ravages of illness. They depict grieving parents. They show wives caressing the faces of lost husbands, just for a chance to be tender toward them one last time. And they portray unbearably beautiful children, poised as if asleep, surrounded by the toys they played with while alive. But today, the sorrow of these images lies elsewhere: in treating pictures of the dead like obscenities rather than as memento mori.
* * *
Photography extended the centuries-old traditions of death masks and mortuary paintings, which commemorate the dead by fixing them in an illusion of life. But compared to these earlier media, photographs possessed an almost magical verisimilitude. “It is not merely the likeness which is precious,” wrote Elizabeth Barrett Browning of a postmortem portrait, “but the association and the sense of nearness involved in the thing ... the very shadow of the person lying there fixed forever!” For many, procuring a postmortem photo must have felt like a funerary ritual—a way of allowing the dead to become fully dead. But this new invention also had something of resurrection about it. It animated a body, astonishing viewers each time they gazed upon it.
During the 1840s and early 1850s, a postmortem photo would likely have been the first and only portrait of someone. At $2 each (roughly $60 today), photographs were costly, and in America’s open expanses, studios were miles away from most households. But death changes things. People who had never given a thought to the medium now turned to it in desperation. Decades later, in trade journals like The Philadelphia Photographer, veteran practitioners wrote of how parents would arrive at their doorsteps with stillborn infants, to whom they hadn’t even given a name. “Can you photograph this?” implored one young mother, opening a wooden basket to reveal “a tiny face like waxwork.”
Almost all the postmortem photographs from this period are daguerreotypes. The dominant mode of photography for its first 15 years, the daguerreotype was rendered on a copper sheet burnished to look like a mirror. When held at the right angle, a grieving widow would have seen her image meld with that of her husband, a striking reunion after death. Daguerreotypes were produced as three-dimensional objects, meant for the hand as much as the eye. They came in small cases of leather or ebony, opened by a delicate handle. Inside, the image lay cuddled in velvet. Like tiny reliquaries, daguerreotypes kept safe the image of one’s beloved. They kept other things, too, like a baby’s silken curl or a piece of a girl’s ribbon.
Many postmortem pictures show parents cradling their children, or wives alongside their deceased husbands. The corpse figures prominently, but so do the shattered expressions of those left behind. A surprising number of fathers appear—at this time, men could openly admit their grief. There are parents so young they look like children themselves. Many subjects make trembling attempts at self-composure.
Rituals help the living overcome the desire to die with the dead. As a ritual, postmortem photography helped check grief. By pressing subjects to execute specific poses and gestures, death photos helped the living externalize personal loss. The faces of many mourners evidence the struggle. How else to interpret a daguerreotype of a mother lying next to her child?







Many photographs from the 1840s and ’50s depict a corpse posed in a semblance of sleep. The convention makes death look easy and gentle—a rest from labor. “It has a heavenly calm in it,” the English author Mary Russell Mitford remarked of her father’s cast in 1842. But this conceit has an ulterior motive: to trick the viewer into believing that death is sleep, no metaphor about it. Consider the image above, of a boy who bears no trace of decay in his luscious round face. And yet for every photo like this one, a dozen more exist in which photography’s irrepressible realism exposes the charade, in the form of fever sores or sunken eyes. Such images mix comfort with a kind of cruelty.
Postmortem daguerreotypes are piercingly intimate. They bring the viewer close enough to the face of the dead to see a boy’s long lashes, or a girl’s spray of freckles. Many were taken at home. No props here: These are the chairs the dead once sat in, the toys their living bodies held. It is in these daguerreotypes especially that we discover what the French critic Roland Barthes called the “punctum” of a photograph: the accidental element that “wounds” a viewer with its poignancy. In a daguerreotype labeled “Our Darling,” for example, the humble detail of the girl’s dirty fingernails reveals the truth of every postmortem photograph: the life that the dead left behind.
* * *
Beginning in 1851, daguerreotypy gave way to the wet collodion process, which made photography cheaper, faster, and reproducible. The medium soared in popularity, and the market for postmortem photography expanded. As it did, the aspirations for postmortem photos also rose. By the 1860s, death photos began explicit attempts to animate the corpse. Dead bodies sit in chairs, posed in the act of playing or reading. In one striking tintype dated 1859, a young boy perches on a seat, eyes open, holding a rattle. A close look reveals a wrinkle on the left side of the backdrop: a clue that someone, most likely the photographer’s assistant, is propping the child up. In a cabinet card from the 1890s, a young girl holds a plaything in one hand and a doll in the other. Parents and photographers engage in a nostalgic game of make-believe. But the dead children refuse to play along, looking more inanimate, somehow, than their toys.
This slide into sentimentality, even if grotesque, coincides with a profound shift in Western attitudes toward death. The 1870s witnessed the advent of a religious upheaval in America and Western Europe. Traditional arguments about immortality lacked the weight they carried only a few decades earlier, especially among the middle and upper classes. Accounts of death during this period no longer expressed the piety and spiritual fervor of earlier times.
No wonder, then, that the effort to tame and beautify death in daguerreotypes collapsed in the late 19th century. In its place, a confusion of approaches appeared. Some postmortem photos still portrayed peaceful, domestic images of the dead. But the faces in those images are mostly Latin American, Eastern European, and working class. It was a sign, perhaps, that these groups possessed a deeper faith in God—or in photography.
Meanwhile, members of the white middle-class began to procure photos of themselves in mourning, no corpse in sight. Many of these subjects are women, attired in black crepe. They weep into handkerchiefs, or turn their backs to the camera. The photograph’s earlier stoicism gives way to the performance of grief, as if melodrama were supplanting faith. Other mourning photographs foreground the act of remembrance. Bereaved ones stand or sit next to portraits of the dead, recalling the anthropologist Nigel Clark’s comment that in an age of disbelief, death has nowhere to go but memory.











Beginning in the 1890s, postmortem photography turned toward burial. No pretense at life here: just death, flat and absolute, marked by coffins and cemeteries and a community that carries on. Reproduced on postcards, these images traveled to distant friends and relatives. They became vulnerable to the postman’s stamp and other desecrations. The postmortem photograph had devolved from a near-sacred object to a formality, a social obligation. By the mid-1920s, it disappeared from public view, defeated by Kodak and its happy promotion of snapshot photography. Underneath photography’s new lively glee, however, the fear of death quietly smoldered. Photographic reminders of it began to be judged as obscene.
Every so often, postmortem photography experiences a brief resurgence. The organization Now I Lay Me Down to Sleep, for example, recruits volunteer photographers to take images of stillborn or dead infants for grieving parents. And a few years ago, it was a trend among teenagers and 20-somethings to take selfies at funerals. “Caskies,” they coined them. These trends hardly became mainstream, earning more reproach than approval.
* * *
The dead help the living face what lies ahead. In exchange, the living must translate the lives of the dead into history. They can find myriad ways to do so, from visiting gravesites to writing someone’s biography. But photography has become so commonplace that images of death have lost most of their original meaning.
Many postmortem photographs are hard to look at. They’re too graphic or too desperate in their attempts to simulate life. But others provide an almost visceral connection to the past. Visiting the Thanatos Archive, I linger over the faces of the bereaved, remembering what it feels like to lose someone you love. I learn the names of the dead before me: Odie, Sulisse, Viola. I discover the strange ways people die (brain fever, an accidental swallowing of coyote poison) and the all-too-familiar ways they do (cancer, an accidental gunshot). And I surrender to my own fears of dying. I see, as if in palimpsest, my demise in these portraits of strangers, and I recognize that mortality connects us all.
This article appears courtesy of Object Lessons.


Marion Tinsley—math professor, minister, and the best checkers player in the world—sat across a game board from a computer, dying.
Tinsley had been the world’s best for 40 years, a time during which he'd lost a handful of games to humans, but never a match. It's possible no single person had ever dominated a competitive pursuit the way Tinsley dominated checkers. But this was a different sort of competition, the Man-Machine World Championship.
His opponent was Chinook, a checkers-playing program programmed by Jonathan Schaeffer, a round, frizzy-haired professor from the University of Alberta, who operated the machine. Through obsessive work, Chinook had become very good. It hadn't lost a game in its last 125—and since they’d come close to defeating Tinsley in 1992, Schaeffer’s team had spent thousands of hours perfecting his machine.
The night before the match, Tinsley dreamt that God spoke to him and said, “I like Jonathan, too,” which had led him to believe that he might have lost exclusive divine backing.
So, they sat in the now-defunct Computer Museum in Boston. The room was large, but the crowd numbered in the teens. The two men were slated to play 30 matches over the next two weeks. The year was 1994, before Garry Kasparov and Deep Blue or Lee Sedol and AlphaGo.
Contemporary accounts played the story as a Man vs. Machine battle, the quick wits of a human versus the brute computing power of a supercomputer. But Tinsley and Schaeffer both agreed: This was a battle between two men, each having prepared and tuned a unique instrument to defeat the other. Having been so dominant against humans for so long, Tinsley seemed to thrill at finally having some entity that could give him a real game. He had volunteered to play friendly matches against the computer in the run-up to their two world championship matches. And Schaeffer, though he was a bull-headed young man, had become the most effective promoter of Tinsley’s prowess and legacy.
But there, in that hall, a quirk of human development was troubling Tinsley. His stomach hurt. The pain was keeping him up all night. After six games—all draws—he needed to see a doctor. Schaeffer took him to the hospital. He left with Maalox. But the next day, an X-ray revealed there was a lump on his pancreas. Tinsley understood his fate.
He withdrew. Chinook became the first computer program in history to win a human world championship. But Schaeffer was crushed. He’d devoted years of his life to creating a program that could beat the best checkers player ever, and just as he was about to realize this dream, Tinsley quit. Seven months later, Tinsley died, never having truly lost a match to Chinook.
And that would lead Schaeffer to undertake a 13-year computational odyssey to exorcise the man’s ghost. With Tinsley gone, the only way to prove that Chinook could have beaten the man was to beat the game itself. The results would be published July 19, 2007, in Science with the headline: Checkers Is Solved.
“From the end of the Tinsley saga in ’94–’95 until 2007, I worked obsessively on building a perfect checkers program,” Schaeffer told me. “The reason was simple: I wanted to get rid of the ghost of Marion Tinsley. People said to me, ‘You could never have beaten Tinsley because he was perfect.’ Well, yes, we would have beaten Tinsley because he was only almost perfect. But my computer program is perfect.”
* * *
Jonathan Schaeffer did not begin his career intending to solve checkers. He was a chess player, first. Good, not great. But he also loved computers—and had done a Ph.D. in computer science, so he decided to build a chess-playing program. He called it Phoenix and it was one of the better chess programs of many that were created in the 1980s. In 1989, he “crashed and burned” at the World Computer Chess Championships, however. At the same time, the team that would form DeepBlue, the chess software that would eventually defeat Kasparov, was coming together. Schaefer realized he would never build the world computer champion.
A colleague suggested that perhaps he should try checkers, and thrillingly, with just a few months of work, his software was good enough to bring to the Computer Olympiad in London to compete against other checkers-playing bots. And it was there that he began to hear about Marion Tinsley, the great.
At the highest levels, checkers is a game of mental attrition. Most games are draws. In serious matches, players don’t begin with the standard initial starting position. Instead, a three-move opening is drawn from a stack of approved beginnings, which give some tiny advantage to one or the other player. They play that out, then switch colors. The primary way to lose is to make a mistake that your opponent can jump on.
This would seem to make checkers a game amenable to computer play. That was certainly the idea back in the mid-1950s, when an IBM research scientist named Arthur Samuel began to experiment with getting a checkers-playing program to run on an IBM 704. He worked on the problem for the next 15 years or so, publishing several important papers on what he called—and what we all would now call—“machine learning.”
Machine learning is the underlying concept for the current wave of artificial intelligence. The descendants of that early work now promise to revolutionize whole industries and labor markets. But Samuel’s programs never had much success against actual humans. In May 1958, several members of the Endicott Johnson Corporation Chess and Checkers Club trounced the computer, much to the delight of the Binghamton Press and Sun-Bulletin.
“The human brain, sometimes lost sight of in an age of satellites, frozen foods, and electronic data processing machines, returned to former glories early today,” the paper said. “The 704, Dr. Samuel explained, does not think. What it does, he said, is to search its ‘memory,’ stored on tape, of checkerboard situations it has encountered previously. Then it rejects choices which have turned out badly in the past and makes moves which turned out well.”
This still would work pretty well as a description of what’s known as “reinforcement learning,” one of the basket of machine-learning techniques that has revitalized the field of artificial intelligence in recent years. One of the men who wrote the book Reinforcement Learning, Rich Sutton, called Samuel’s research the “earliest” work that’s “now viewed as directly relevant” to the current AI enterprise. And Sutton also happens to be one of Schaeffer’s colleagues at the University of Alberta, where Google’s DeepMind AI shop recently announced it will set up its first international research office.
While his techniques were groundbreaking, 10 years later Samuel had made, in his own words, “limited progress” on the problem, though he’d kept working at it at IBM, while visiting at the Massachusetts Institute of Technology, and then with a Department of Defense grant at Stanford University. As with so many of the currently hot AI techniques, he just did not have the computational horsepower or datasets that he needed to make the beautiful ideas work.
So when Schaeffer began building his own software, he went straight back to Samuel, and found that he could not exactly follow Samuel’s path. He’d have to build a new system. At first they called it The Beast, but eventually they named it Chinook after the warm winds that sometimes blow through Alberta.*
The work consumed Schaeffer. As he described it in his 1997 book, “Sometimes, when I had difficulty getting to sleep, I would fantasize about the exhilaration that I would experience when Chinook finally defeated the Terrible Tinsley.” His wife would interrupt his reveries asking, “You’re thinking about him again, aren’t you?”
As the software developed through the years, it retained two core components. The first is easy to understand: It’s a “book” of complete computations of every possible checkers position with a certain small number of pieces on the board. So, if there were six pieces left—and as time went on, seven, then eight—Schaeffer’s software knew every possible combination. Back in the early ’90s, it took vast amounts of computer time to be able to run all those calculations.
But as Schaeffer and his team did more and more computation, Chinook got better and better. It started to be able to beat people. But they knew that they couldn’t calculate every possible position.
The rules of checkers are simple, but the number of potential moves is massive—there are 5 x 1020 possibilities. Schaeffer has an analogy to help people understand how ridiculous that is: Imagine draining the Pacific Ocean and then having to fill it back up with a tiny cup, one at a time. The sea is the number of checkers possibilities. The cup is each calculation.
The second component of the system is a little trickier to grasp. Chinook needed to search through possible moves beginning with the start of a match. Like many similar systems, Chinook would look ahead many possible moves and then try to score each permutation’s desirability. At first, Chinook could only look ahead 14 to 15 moves out, but as computers and the software improved, it could look further and further. “As with chess, deeper was always better,” Schaeffer told me.
In late 1990, the American Checker Federation allowed Chinook to play in the U.S. Championships. The software went undefeated and played Tinsley to a draw six times. That earned the software the right to challenge Tinsley for the world championship.
And after Chinook’s 1990 performance, Tinsley called up Jonathan Schaeffer and asked if he might like to play a few friendly matches.
* * *
From 1950 to 1990, Tinsley had been the world champion of checkers whenever he wanted to be. He’d occasionally retire to work on mathematics or devote himself to religious study, but he’d eventually return, beat everyone and become champion again. In that 40-year span, he lost five total games and never once dropped a match.
Derek Oldbury, probably the second best player of all time, wrote an encyclopedia of checkers. It was effusive in its praise for the master: “Marion Tinsley is to checkers what Leonardo da Vinci was to science, what Michelangelo was to art and what Beethoven was to music.”
It’s hard to know what to make of Marion Tinsley from the perspective of the 21st century. He seems otherworldly or, at the very least, othertimely. His life was composed of checkers, mathematics, and his abiding faith. He was kind, by literally every account, and yet his style of play was relentless and aggressive.
It is not uncommon to say that someone lived a “life of the mind,” but in Tinsley’s case both the life and the mind were unusual. For years, as an undergraduate and then graduate student at the Ohio State University, he spent eight hours a day on checkers. He never married. “I haven’t seen a checker marriage that worked out,” he told a reporter. “It is a very rare woman who can be married to a real student of checkers.” His mother lived with him well into the 1980s. A 1993 profile in The Philadelphia Inquirer found Tinsley in a big, blue sweater over a shirt and tie. His lunch was “a jar of milk, half an apple, and a peanut butter sandwich.”
His relationship with the racial dynamics of the South is also fascinating. “I had thought of going to Africa as a self-supporting missionary," he told Sports Illustrated in 1992, "until a sharp-tongued sister pointed out to me that most people who wanted to help blacks in Africa wouldn't even talk to blacks in America.”
Instead, he became a lay minister at a predominantly black church and left Florida State University’s math department to teach at the historically black Florida Agricultural and Mechanical University. He spent 26 years there. A yearbook from towards the end of his time there showed a deep and lively campus life in which Tinsley might have been the only white person over the age of 40. No contemporary accounts ask any of the black students what they made of their checkers-champion professor, but a colleague was able to describe Tinsley’s role in a local obituary: “At his retirement dinner, literally everybody; young, old, black, white, students, faculty members ... gave testimonies about the impact he had had on their lives.” For a man of his time and upbringing in Kentucky, his path seems almost miraculous.
One thing is for sure: Tinsley was a genius. His genius had been refined and whittled into a strange and wonderful shape. He was the very best at this one thing and pretty ordinary in all else. One part of him had become almost like an artificial intelligence—narrow but extraordinarily capable—while the rest of him lived this simple human life.
When a reporter visited him in Tallahassee, Florida, in 1993, lavender azaleas lined the driveway leading up to his two-story brick house. His home had bare white walls. A room upstairs contained his checkerboard and heavily used checkers books. Tinsley liked to sit in a velour La-Z-Boy. He could never quite explain what checkers meant to him—why he’d pored over sequences of moves for almost his entire life, why he’d kept a magnetic checkerboard by his bedside to work out new combinations. It was something close to godly, though.
“Checkers is a deep, simple, elegant game,” he once said. Playing another human great was “like two artists collaborating on a work of art,” Tinsley said another time.
And then there is his most quotable line: “Chess is like looking out over a vast open ocean; checkers is like looking into a bottomless well.”
It was as if the checker moves were Scriptures that he could endlessly meditate on and understand in new ways. “Out of the clear blue sky an improvement of a published play will just come to mind, as if the subconscious has been working to come to light,” he told the Chicago Tribune in 1985. “A lot of my discoveries come that way, out of the clear blue sky. Some of my insights into the Scriptures come the same way.”
* * *
When Tinsley came to Edmonton in 1991 to play the friendly matches against Chinook, Schaeffer was also blown away that the world champion would agree to play this computer for fun.
The two men sat in his office and began the matches, Schaeffer moving for Chinook and entering changes in the game into the system. The first nine games were all draws. In the tenth game, Chinook was cruising along, searching 16 to 17 moves deep into the future. And it made a move where it thought it had a small advantage. “Tinsley immediately said, ‘You’re gonna regret that.’” Schaeffer said. “And at the time, I was thinking, what the heck does he know, what could possibly go wrong?” But, in fact, from that point forward, Tinsley began to pull ahead.
“In his notes to the game, he later wrote that he had seen all the way to the end of the game and he knew he was going to win,” Schaeffer said.
The computer scientist became fixated on that moment. After the match, he ran simulations to examine what had gone wrong. And he discovered that, in fact, from that move to the end of the game, if both sides played perfectly, he would lose every time. But what he discovered next blew his mind. To see that, a computer or a human would have to look 64 moves ahead.
“I was absolutely stunned,” Schaeffer told me. “How do you compete with somebody whose understanding of the game is so deep that he immediately knows through experience or knowledge or doing some amazing search that he was gonna win that position?”
Schaeffer still struggles to make sense of Tinsley’s incredible skill. When he wrote his book about the saga, One Jump Ahead, Schaeffer received a letter from Tinsley’s academic supervisor. It read, Schaeffer told me, “that he was an exceptionally talented person and he was capable of doing one thing brilliantly. If it wasn’t checkers, he probably would have been a brilliant mathematician.”
When someone’s motivations are not fame or money, we seem to require a higher level of explanation, some emotional engine the rest of us do not possess. The closest Tinsley ever came to describing his motivations came in the 1993 Philadelphia Inquirer feature. He was an introvert who “felt unloved” by his parents, who he thought doted on his sister. To gain their approval, he competed at math and spelling bees. “And as a twig is bent, it grows: As I grew up, I still kept feeling that way.”
* * *
The hunger to excel propelled Tinsley to college at 15, where he really discovered the passion that would dominate his life. He won his first world title in 1955.
And in 1992, he agreed to put his title on the line in the first Man-Machine World Championship against Chinook. The match was sponsored by the computer manufacturer Silicon Graphics and held in London. “I can win,” Tinsley told The Independent. “I have a better programmer than Chinook. His was Jonathan, mine was the Lord.”
For the two weeks leading up to the event, another world-class player, Don Lafferty, trained with him down in Tallahassee, going over their matches and reviewing positions deep into the night.
The 1992 games were held at the Park Lane Hotel, which had hosted world chess championships as well as the Computer Olympiad that had been Chinook’s public debut two years earlier. The room was large and two-storied, with a balcony that overlooked the players and the refrigerator-sized computer that was running Chinook.
Schaeffer and Tinsley sat across from each other, and a large screen rendered the movement of the pieces. Tinsley drew first blood, besting Chinook in game five. But then in game eight, Chinook delivered a stunning win; it was Tinsley’s sixth loss in 40 years.
Despite the years of toil and dreams of success, Schaeffer felt sadness in that moment. “We’re still members of the human race,” he wrote in his book, “and Chinook defeating Tinsley in a single game means that it will only be a matter of time before computers will be supreme in checkers, and eventually in other games like chess.” Schaeffer might have won, but the humans have lost.
After a series of draws, Chinook won again in game 16. No living player had ever defeated Tinsley more than once. Incredibly, almost unbelievably, the software was on top. They were on the verge of making computing history.
Then, in an episode that Schaeffer still finds too painful to describe, Chinook had some sort of error, which forced them to resign the game, tying the match up. “Tinsley viewed it as God helping him out,” Schaeffer said. “It was a religious experience for Tinsley and one of the most devastating experiences of my life.”
Schaeffer and Chinook were never able to get back on top.Tinsley came from behind to win the match, retaining his title.
“I think, if I can keep my health, I don’t believe there will ever be a computer that can beat me,” Tinsley told CNN after the match.
* * *
All of which set the table for the 1994 matchup in Boston.
In the run-up, Chinook had played Derek Oldbury and trounced him. Shortly thereafter, Oldbury died. “Chinook plays Oldbury. Chinook beats Oldbury. Oldbury dies,” Tinsley joked with Schaeffer. “He must have died of Chinookitis!”
Schaeffer was not amused. He was a young man programming computers to play an old man’s game. The best players were dying off. And it was, in the eyes of some in the checkers world, a bit untoward for this guy with his fancy machines and code to come around beating up on the frail masters.
As he’s gotten older, Schaeffer has come to wish that he’d appreciated Tinsley more during their entanglement. “I also look back at my time with Marion Tinsley, not realizing how fleeting it would be and how much I appreciated it. I didn’t take advantage of that and reap the benefits of a warm and deep friendship that I had with him,” he told me. “I look back at it and with hindsight; I can say that I am disappointed in myself. I wish I would have done things differently, but at the time, when you’re obsessed, you can only see in one direction.”
Even in his narration of taking Tinsley to the hospital, Schaeffer cannot get over the idea that the older man’s illness might prevent him from having the chance to beat him. Tinsley says, “I’m ready to go,” and Schaeffer acts befuddled. The old man knew he was dying, and the younger didn’t get it.
After Tinsley resigned, Don Lafferty took his spot and dueled with the computer, but Schaeffer’s sights became set on the larger prize of solving checkers. Human checkers players came to despise Schaeffer, by his own telling. They sent him letters trash talking him and his software. He released One Jump Ahead in 1997, and still checkers players kept coming for him.
From 1997 to 2001, he suspended his project to solve the game, which meant creating a computer program that always knew the right move. It would be unbeatable. It would be perfect.
When he returned, his team expanded the endgame database—Chinook’s perfect knowledge—to any situation where there were 10 pieces or less on the board. That’s 39 trillion positions.
They continued improving Chinook’s “best-first” search strategy, which allowed them to do a fraction of the calculations necessary to compute every single possible play in the game. It took Schaeffer harnessing computers all over the world, drawing on and expanding his expertise in parallel computing. He conscripted machines everywhere from Switzerland to Lawrence Livermore National Laboratory, a major Department of Energy facility that often deals with nuclear weapons.
“There was somebody else there [running a program] called BOMB and I was running checkers programs,” Schaeffer told me. “It was a very strange situation. Security should have been concerned.” And they were. They paid him a visit after discovering gigabytes of data flowing out of a national lab to Edmonton, Alberta.
Through the years, the computers cranked away. As Samuel’s machines had before him. As future DeepMind machines would. This was a narrow kind of intelligence to develop, but it was part of the long arc of the development of artificial intelligence, which is expected to revolutionize the world within this generation.
Schaeffer saw what he was doing as the completion of Samuel’s initial dream. After his team’s success at the 1990 U.S. championships, he reached out to the old IBM man to share the news. The unhappy word came back: Samuel had just died, one of the oldest computer programmers still working. Born in Emporia, Kansas, in 1901, his last login to Stanford’s computer network was recorded in February of 1990. He’d seen the diffusion of electricity, the release of the Model T, two world wars, humans on the moon, and the very first glimmers of machine intelligence.
Finally, in 2007, Schaeffer was able to announce in Science that after working on checkers for 19 years, he had solved the game. The forward search had met the endgame database somewhere in the middle, like an AI transcontinental railroad, with the high-profile publication as Schaeffer’s golden spike. His team had figured out the sequences for 19 of the 300 tournament openings, but as it turned out, those 19 were all that were needed to prove that the game, played perfectly, amounts to a draw.
* * *
The game of checkers grinds on. The sport has been officially conquered by computers, but the wave of artificial intelligence has long moved on to more difficult challenges, leaving the humans to beat up on each other in places like the Honeysuckle Inn and Conference Center in Branson, Missouri. There are still tournaments and champions and even some prize money. There’s a group of young Italians who are challenging for global dominance.
Schaeffer thinks he’s laid checkers to rest himself. He’s working on a book about the history of computer chess that’s coming out later this year. He’s a dean at University of Alberta. “I’ve tried to move on and exorcise some of the ghosts of the story, but it keeps coming back,” he tells me. “I guess it will be part of me until the day I die.”
Before he does, though, he has one more checkers item on his to-do list. “I’d like to make a pilgrimage to Ohio and visit his grave,” Schaeffer said.
The grave is marked by a simple headstone: Marion F. Tinsley. In the upper right corner, there’s a checkerboard. In the upper left, a Scriptural reference, Hebrews 13:1: “Let brotherly love continue.”
Followed by: “Be not forgetful to entertain strangers: for thereby some have entertained angels unawares.” Imagine Schaeffer, once one of those strangers, completing his pilgrimage and looking down at this grave, as if down a well. For Tinsley, the spiritualist, the metaphor of checkers as a well without end was both poetic and true. But Schaeffer, the engineer, knew that no well is bottomless. And humans will always sound the depth.
* This article originally stated that the Chinook winds blow through Edmonton. We regret the error.


In Ralph Waldo Emerson's famous essay on self-reliance, the 19th-century writer and naturalist sang the praises of spiritual isolation and the evils of distraction, bemoaning the forces that conspired to direct his attention to "emphatic trifles." He would not be cowed, he said, but would stand resolute in the face of such bad influences: "The power men possess to annoy me, I give them by a weak curiosity ... If we cannot at once rise to the sanctities of obedience and faith, let us at least resist our temptations."
Don't tell Ralph about Twitter.
I joined Twitter in 2009 at the urging of my husband, who works in technology. "What am I going to do, tell the internet what I ate for breakfast?" I asked him. Eight years later, I'm the one checking Twitter over my morning toast while he gets ready for work. Twitter has become the place where I get my news, where I check in on my friends, where I go to make jokes and read good essays. As a lifelong sufferer of anxiety, it is where I go to talk about what I’m feeling when I’m anxious, and maybe find some camaraderie. And as a lifelong sufferer of anxiety, using Twitter is also making my anxiety worse. The like-minded community I’ve built on Twitter has made confessing anxiety easier than ever, but the comparison Twitter enables has made the experience of anxiety worse. And when it comes to Twitter, you have to take the good with the bad.
Psychologists typically distinguish between two types of anxiety: trait anxiety, a persistent and lasting tendency to experience fear and worry; and state anxiety, a temporary response of fear to a threatening situation. Many forms of social media can agitate both trait and state anxiety, and perhaps none more so than Twitter, which reminds the perpetually anxious that we always have something to be anxious about and instills a sense of anxiety in even the most laid-back user. Twitter’s constant flow of new information and the fact that users tend to follow people who are more accomplished and successful than they are creates an especially potent cocktail of comparison for anxious people. "Twitter really inflames my professional anxiety," says Caitlin Cruz, a freelance journalist based in New York. "But it's also given me a lot of professional success." Cruz deleted the Twitter app from her phone a few weeks ago, which she says has made her life more bearable.
Twitter users have to contend with competing voices that yell at you as soon as you log on. You haven't written a best-selling novel yet? Here's a “30 Under 30” list of best-selling novelists! You're over 30? Here's an article about how you're a bad parent! You haven't had children yet? This bestselling author has three, and she's under 30! Twitter is a megaphone for achievements and a magnifying glass for insecurities, and when you start comparing your insecurities with another person's achievements, it's a recipe for anxiety.
"Generally speaking, the comparisons that we make on social media are more likely to be 'upward' comparisons," says Azadeh Aalai, a professor of psychology at Montgomery College in Maryland. "We're comparing ourselves to the individuals who appear to be higher status and are achieving more" than we are, which can lead to feelings of envy, discontent, and anxiety. It's also not the whole story. When I was young, my mom used to warn me against "comparing my insides to other people's outsides." Using Twitter, I am constantly comparing my insides—my anxieties, fears, and insecurities—with other people's outward selves: their accomplishments, polished selfies, and edited articles. There will always be someone who’s doing better than I am in any aspect of my life. And because I, like many people, tend to follow people I admire or who are already famous, I am constantly aware of just how much better other people are. Twitter also gives me a quick and handy way to quantify my worth: this many likes, this many retweets. I'd like to think I'm more than the sum of my followers, but there are plenty of days when I don't feel that way.
Anxiety functions by constantly reminding you to pay attention to it. And so does Twitter. Twitter draws users back for more and more and more. Smartphones are designed to provide instant gratification, and many of Twitter's features depend on our biological fear of scarcity, says Pamela Rutledge, the director of the Media Psychology Research Center. The push notifications, the little number next to our mentions, the bar that tells us how many tweets have been sent since we last refreshed the page—all of these details are designed to keep users coming back, afraid that we might have missed something vital. "Social media doesn't really promote moderation," Aalai says (in what could perhaps be the understatement of the year).
The desire to know what is going on at every moment is quenched when met with the firehose of information that is Twitter. But my anxiety skyrockets when I’m met with the seemingly endless amount of bad news about tragic events going on around the world—ISIS bombings, systemic racism, refugees in crisis, the threat of war, political upheaval. Many Twitter users I surveyed cited feeling powerless in the face of overwhelming fear as one of the biggest causes of their anxiety. Even if it does offer the occasional practical solution—donating to the International Rescue Committee, calling a congressperson, sharing a GoFundMe—Twitter remains dominantly focused on the world's ills in a way that can decimate a person's sense of efficacy and replace it with profound despair.
If Twitter is full of bad news and anxiety-inducing fodder for comparison, why are we there in the first place? Some people, like the writer Lindy West, have left Twitter altogether due to harassment and trolling, while the New York Times columnist Bret Stephens just announced he would leave Twitter because it had become "pornified politics," although he isn't really leaving—"I'll keep my Twitter handle, and hopefully my followers," he wrote, and an editorial assistant will update the profile for him. But the rest of users are there, presumably, because they find some value amid the constant updates and jokes and hot takes. Twitter provides a sense of camaraderie.
Twitter provides a platform for neurotic people to share their fears. And for those of us who work from home or on the road, Twitter becomes an office space and the people we interact with become our coworkers. A recent Harvard University study found that "the act of disclosing information about oneself activates the same part of the brain that is associated with the sensation of pleasure"—the same pleasure center that is activated by food, money, and sex. Confessing my anxiety on social media, then, is an attempt not to feel so alone. Anxiety isolates the people who suffer from it, convincing them that they are the only ones who think in this distorted way. Bringing this kind of myopic thinking into the light and examining it can help combat it, and Twitter can actually be a useful place for doing just that. "You're anxious? Me, too!" is the kind of rallying cry that unites anxious people. But even as we find our tribe of fellow worriers, the question remains: What are we using Twitter for?
Rutledge encourages Twitter users to think about why they're online. "If you're checking Twitter a hundred times a day, what are you avoiding doing?" she asks. "That's where you need cognitive override," or the ability to step out of the moment at hand and evaluate how realistic your feelings are given your use of this technology. "When we're anxious, we feel compelled to be continually scanning the environment," Rutledge says. "That's how we make ourselves feel safe." It's what our ancestors did to anticipate attacks from enemies or saber-toothed tigers, but the advantage now isn’t quite as clear. Assuming we live in a world that is connected enough that we won't completely miss important news, there isn’t a real need to be constantly scanning the feed, looking for threats.
The cycle of anxiety on Twitter use can be especially bad for women, non-binary and queer people, and people of color. "Vulnerable populations in face-to-face interactions are similarly going to be vulnerable in virtual interactions," says Aalai. These are often people who benefit greatly from Twitter because they can speak directly to the friendly audience who follows them, cutting out the potential for harassment they might receive in other places. But trolls follow, too: A 2014 Pew study shows that 25 percent of women ages 18–24 have been sexually harassed online (as opposed to 13 percent of young men), and 23 percent have been physically threatened. Fifty-one percent of African-American and 54 percent of Hispanic internet users had experienced some form of harassment online, as opposed to 34 percent of white internet users.
"You have to make a conscious decision about whether Twitter is still adding value," Rutledge says. The difficult part is that "value" is entirely subjective, and it's hard to make (good) decisions when our brain isn't working at full capacity. A recent study from the University of Chicago found that "the mere presence of one’s own smartphone reduces available cognitive capacity." And a recent New Republic article asked journalists whether they could live without Twitter. The answer was uniformly "no," although many people acknowledged that life without Twitter would be "better." It reminds me of the apostle Paul's words about sin in the letter to the Romans: "I do not understand my own actions. For I do not do what I want, but I do the very thing I hate."
In 1855, the poet Walt Whitman sent Ralph Waldo Emerson a copy of his newly published collection of poetry, Leaves of Grass. "I find it the most extraordinary piece of wit and wisdom that America has yet contributed," Emerson wrote to him. "I greet you at the beginning of a great career, which yet must have had a long foreground somewhere, for such a start." Emerson saw in Whitman’s moving poetry the long and careful career of devoted practice that had gone before.
Some years later, Twitter CEO Jack Dorsey said Leaves of Grass was one of the most influential books in his career, comparing its “efficiency” to great programming. It didn’t seem to strike Dorsey as ironic that Whitman took years to craft the efficiency of language that Dorsey praised. Dorsey called Whitman a “total entrepreneur,” looking, as many of us do, for the presence of his own values in the person he admired. And that is one of the reasons people are drawn to Twitter—it gives them access to the inner lives of people they would otherwise never interact with. But in so doing, they may also start to fear that they will never become the person they want to be—never be as smart or prolific or original or beautiful as the composite of people they follow. That gap is where anxiety thrives.
In the meantime, I'm itching to know what's going on in the world. Who knows what's happened since I started writing? Is there some new political scandal? Has someone tweeted something outrageous? How am I adding up to the people I follow? I know I could wait. I could go for a walk, or read a book, or take a bath. But I think I'll check. Just one more time.


Every year, billions of dollars change hands in needlessly clumsy ways. Parents realize they’re short on cash and go out of their way to stop at an ATM so they can pay their babysitter; grandparents mail checks as birthday gifts, which take days to arrive and days to clear. Even as more and more of life is lived through a screen, paper is still how the vast majority of Americans give each other money.
How Apple Pay Gets People to Part With More of Their Money
In the past few years, a handful of tech companies have recognized these inefficiencies, introducing apps—such as Circle Pay, Square Cash, and Venmo—that let users transfer money to one another’s bank accounts using their phones, relatively frictionlessly. Among other things, they let users enter their bank-account information and then transfer money to others who have done the same. With Venmo, one of the more popular of these services, there is an additional wrinkle: Once money is transferred, the exchange shows up in the app’s social feed, a running record of who went out for drinks with whom, or whose roommate pays the electricity bill each month. (Users can elect to make a transfer private, but most don’t.) The app has among many—mostly young, city-dwelling people—attained a level of linguistic uptake reserved for the likes of Google and Uber: “Just Venmo me,” they say, after picking up a dinner bill.
The feature that sets Venmo apart is the social feed, which brings transparency to a class of transactions that used to be entirely private. The feed—an emoji-laden stream of often-indecipherable payment descriptions and inside jokes—seems frivolous; it is not a social-media destination in the way that Facebook or Twitter is. But as a public record, it is quite revealing of social dynamics—who’s hanging out with whom, and perhaps where. A friend of mine told me that Venmo proved invaluable in trying to determine if her ex and his new girlfriend were still dating.
In other words, pointless and goofy as it seems, people do pay attention to what they see in Venmo’s feed. And there’s actually a way this parade of public transactions might give the app a significant advantage over its many competitors.
The reason, says Richard Crone, who runs a payments-focused firm called Crone Consulting, has to do with how Venmo makes money—or, more precisely, how it will make money. Currently, Venmo doesn’t directly generate all that much revenue for the company that owns it, PayPal. (Contrary to what some of its users may have guessed, the app doesn’t make money “on the float”—that is, by investing whatever funds users keep as a positive balance in their Venmo accounts.)
Things could look different not too long from now. Venmo’s plan, which it has already initiated and will expand in the coming year, is to facilitate more transactions between businesses and their customers. Last summer, Venmo introduced partnerships with about a dozen apps (including the food-delivery service Munchery and the fast-food chain White Castle) that now let users pay straight from their Venmo accounts. The idea, Crone explains, is that Venmo would take a cut—its standard rate is 2.9 percent plus a small flat fee, which is at the higher end of what merchants pay for a typical credit-card transaction—of not just in-app purchases like these, but of in-person transactions at physical checkout counters, where customers spend trillions of dollars a year.
This is where the social feed comes in. “You walk into any retailer, any restaurant, any service provider—what do they want you to do? Like them on Facebook, follow them on Twitter,” Crone says. Working with retailers would give Venmo a business model similar to credit-card issuers and processors—“but with much more upside,” he says, “because the retailers spend far more trying to get you to like them on Facebook and follow them on Twitter and all these other things that they could just get as a byproduct of the payment.” That is, if someone paid for a taco using Venmo, their friends might see where they ate lunch.
One limit on this strategy’s effectiveness is that consumers might not be eager to publicize every one of their purchases. But Venmo is aware of this: In the year or so since it started trying its service out with a few businesses, the default setting has been for payments not to be shared in the social feed. Still, Bill Ready, PayPal’s chief operating officer, recently told Barron’s that when the initiative is expanded, “social aspects will be not only present, but also be what’s most attractive to our users.” And Fast Company has reported that a job-application prompt for prospective Venmo employees last year mentioned research finding that “Venmo users are more open to purchasing at new businesses … that they learn about from friends on Venmo.”
But the other, even more lucrative aspect of becoming merchants’ preferred means of payment is access to information about where customers are spending their money. “The real value is in the data, and the ability to render customized ads and offers, and generate a revenue stream from that,” Crone says. “We estimate that the value of mobile payments per enrolled active account is worth more than $400 per year in revenue, to whoever does it—Venmo, Apple Pay, Android Pay, Samsung Pay, a bank, Visa, or Mastercard."
If Venmo or another service were to gain access to this payment data, the typical recipients of it would start missing out. Even though digital-payments apps are built on top of banks’ infrastructure, banks wouldn’t see the details of consumers’ spending, but instead just requests from an app to add or withdraw money from an account. Referring to the value of owning the platform that consumers directly interact with, Crone says, “The one who enrolls is the one who controls”—a phrase he went on to repeat five times in one conversation I had with him.
Crone thinks that banks are worried about Venmo’s ambitions. Banks have rightly recognized that convenience, affordability, and ease of use are not characteristics that appeal uniquely to 20-somethings, and so they have in recent years collaborated on a payment platform that does more or less what Venmo does. The product of that collaboration, called Zelle, began showing up last month on the screens of tens of millions of Americans who use mobile-banking apps on their phones.
Zelle differs from Venmo in three important ways. The first is that Zelle appears within users’ banking apps, as opposed to being an app all its own. The second is a consequence of the first: Because Zelle was developed by banks and appears in their apps, transfers will register in users’ bank accounts in minutes, whereas with Venmo, that currently takes days. (Zelle’s association with banks is also a selling point when it comes to security, which Venmo has in the past gotten some bad publicity for, but has since seemed to have gotten under control.) Third, Zelle lacks Venmo’s performative social component.
And, technically speaking, Zelle is not new—the name and the bright purple buttons reading “Send,” “Request,” and “Split” are just the consistent branding given to a payments platform called clearXchange that banks have been using, under other names, for years. In fact, last year, clearXchange processed about $175 million in payments per day, while Venmo’s daily rate was only $54 million. Still, the pre-Zelle clearXchange, which wasn’t given a consistent name or look across banks, left room for more-intuitive apps like Venmo to claim a chunk of the market. They had plenty of time to do that, given how long it took for more than 30 banks, normally in competition with each other, to cooperate and release a cohesive product.
Looking to the future, Crone notes that Venmo’s social feed is something that banks would by their nature have trouble reproducing. “You expect your bank to keep things private,” he says. “With Venmo, you expect them to publicly post.” (When I talked to Melissa Lowry, the vice president of marketing and branding at Early Warning, the industry group that helped develop Zelle, she said that Zelle could add a social component down the line, if banks decided that was something they wanted.)
Of course, because of its reputation for security and privacy, Zelle is arguably much better positioned than Venmo to handle business-to-consumer disbursements, such as when, instead of sending a check, an insurance company transfers a customer money for making car repairs, or when a market-research firm compensates people for participating in a focus group. ClearXchange has already been handling such payments, and it’s not hard to imagine people using Venmo at a fancy restaurant to broadcast their good taste and Zelle for payments that say less about their social status than they do about mundane financial chores.
And at the moment, Venmo’s ability to capitalize on its social dimension is far from certain. When I asked Talie Baker, a senior researcher at Aite Group, about what Crone observed, she wrote in an email, “I think it’s a good point he makes that companies won’t mind the transaction fees with the built-in advertising BUT, consumers LOVE their credit cards.” (That said, PayPal appears ready to work with credit-card companies in order to gain a foothold at physical checkouts.)
An additional wrinkle, of course, is Venmo depends on banks’ payment systems in order to function, so it might be hesitant about encroaching too much on their revenue streams. But if Venmo started getting a larger market share than the banks are comfortable with, would they try to do something about it? “That'd be a pretty dangerous and bold move,” Crone says. He adds that in some ways the banks actually don’t mind that services like Venmo have gotten bigger, because they have the banks processing payments that they wouldn’t have processed otherwise. “If they are not able to position themselves in the social sphere of payments,” he says, “then they'll still participate in the back-end plumbing."
After the announcement last month of Zelle’s impending rollout, a number of media outlets presented it as a direct competitor to Venmo. Reuters called it banks’ “answer” to Venmo. Engadget said that Zelle is “tak[ing] on” Venmo. Bloomberg went so far as to wonder if the app was a “Venmo killer.”
In truth, the dynamic between the two payment platforms will not be quite so oppositional; paper money and checks are still so prevalent that Zelle and Venmo could both gain immense popularity and still not run up against each other for many years. Crone thinks the most likely outcome is that the various companies involved with payments will end up cooperating as much as they compete. “There won’t be one mobile payment to rule them all,” he says. As much competition as there currently seems to be, the future that both Crone and Baker foresee for digital payments leaves plenty of room for emoji—and more ☮️  than ⚔, at that.


Imagine a group of volunteers, their chests rigged with biophysical sensors, preparing for a mission in a military office building outfitted with cameras and microphones to capture everything they do. “We want to set up a living laboratory where we can actually pervasively sense people, continuously, for a long period of time. The goal is to do our best to quantify the person, the environment, and how the person is behaving in the environment,” Justin Brooks, a scientist at the Army Research Laboratory, or ARL, told me last year.
ARL was launching the Human Variability Project, essentially a military version of the reality-TV show Big Brother without the drama. The Project seeks to turn a wide variety of human biophysical signals into machine-readable data by outfitting humans and their environment with interactive sensors.
The Army is not alone. The Air Force, Marine Corps, Navy, and their special operations forces are also funding research to collect biophysical data from soldiers, sailors, Marines, and pilots. The goal is to improve troops’ performance by understanding what’s happening inside their bodies, down to how their experiences affect them on a genetic level. It’s not exactly genetically engineering soldiers into superhero Captain Americas; the U.S. military insists they have no intention of using biometric data science for anything like the genetic engineering of superior traits. But it’s close. The military is after the next best thing.
If today’s Pentagon leaders get their way, the next generation of fighter jets, body armor, computer systems, and weapons will understand more about the pilots, soldiers, and analysts using them than those operators understand about the machines they are using. The very experience of flying the plane, analyzing satellite images, even firing a gun could change depending on what the weapon, vehicle, or software detects about the person to whom the weapon is bound. To make this dream real, Pentagon-backed researchers are designing an entirely new generation of wearable health monitors that make Silicon Valley’s best consumer fitness gear look quaint. They’re discovering how to detect incredibly slight changes in focus, alertness, health, and stress—and to convey those signals to machines. Design the boots well enough and the super soldier will arrive to fill them.
Army Research Laboratory researchers already monitor individual subjects from six months to two years. Brooks wants to expand that to other military training environments, such as the U.S. Military Academy at West Point, New York, and then to more than a dozen universities. He hopes the data will reveal how people of varied size, weight, height, health, level of alertness, etc., differ in terms of the signals they send out—hence the name “human variability.” That, in turn, will help researchers gather much more precise information on how different people interact with their environment. The ultimate goal is sensors that can tell the Pentagon how each human soldier performs, or could perform, to their best ability, from battlefield to home front.
“It’s not just while they’re at work, but also when they go on leave,” says Brooks. “This is continuous, with the highest practical resolution that we can obtain for a long period of time. Hopefully, we would see information going into many programs” to build future gear. “A greater understanding of natural human variability would then feed pretty much any system that adapts to the person.”
It’s an ambitious undertaking, considering the current limitations of body-worn sensors. Over the past two years, the military bought more than $2 million worth of Fitbits and other biomedical tracking devices. But it turns out that off-the-shelf consumer devices aren’t good enough for the military’s biotracking ambitions. So researchers are creating a new class of wearables, based on new research into embedding electronic components into fabric. If the electrodes are too small, the signal is worthless; too big, and they feel like an artificial electric shell separating the wearer from the real world. The connection between the environment and the human must remain seamless.
One application for such sensors is helmets that record brain activity while their wearers do their jobs. An ARL team is preparing for continuous electroencephalography, or EEG, by using 3-D printing to create helmets that fit perfectly to each individual soldier’s head. But the military is not eager to embed wires and metal into gear that’s meant to protect a soldier during a massive blast. So the lab is constantly looking at new materials, solutions, and tradeoffs, inching toward sensors that collect information without getting in the way of soldiering. Lab technicians showed me one experimental electrode that they were making that was so small and soft to the touch it seemed to have no metal in it at all (they are in fact constructed of nanofibers that conduct electricity, encased in silicon).
* * *
The Air Force, as well, needs a next generation of wearables to help tomorrow’s combat aircraft understand their pilots. Modern fighter jets expose human bodies to physical forces that are still not entirely understood. In 2010, multiple F-22 pilots reported in-flight episodes of confusion, shortness of breath, and skin-color changes—all symptoms of hypoxia, or decreased oxygen in the blood. The reason was speed.
“I pull a G in the airplane, blood has a tendency to collect in some of those dependent areas of the body, like the arms and legs and that,” said Lloyd Tripp, a program manager for aerospace physiology and toxicology at the Air Force Research Laboratory’s 711th Human Performance Wing. Two years later, the Air Force began to affix sensors inside the helmets of F-22 pilots to read the blood-oxygen level of their temporal artery.
Around the same time, the Russian military was also seeing confusion and skin-color changes among their pilots who pulled high G-forces, Tripp said. Lacking the same sensor technology, Russian commanders began to give pilots blood transfusions before their flights. It didn’t work. Russian pilots flying at supersonic speeds suffered hypoxia at greater rates. “They didn’t actually admit that for quite a few years,” he said. Correct diagnoses enabled the U.S. Air Force to read the problem and improve performance.
Beyond helmets, Air Force researchers are working on what they call a comprehensive cognitive monitoring system. This means exploring what sensor technologies work well for what purposes, and what signals can be detected without interfering with or disturbing the pilot—who is, after all, supposed to be flying a combat mission. Depending on what you seek to measure, they found, you may no longer need a physical sensor on the body. You can now collect incredibly intimate and important internal health data with cameras.
Take cerebral oxygenation, the amount of oxygen in the tissue of specific portions of a pilot’s brain. You can measure this key biophysical signal by shining infrared light on the forehead because the blood in front of the skull is about as oxygenated as the brain tissue behind the skull wall. “If I’m shining that infrared light through the skin, I can see the amount of oxygen within the blood in that tissue. As I increase G-force, I’m decreasing the amount of oxygen that I have here and that decrease in oxygen is directly correlated back to decreases in cognitive function,” said James Christensen, a portfolio manager with the 711th Human Performance Wing.
Another research project configured simple laptop-camera lenses to detect whether a person’s hemoglobin is oxygenated, which makes blood shows up slightly redder, or de-oxygenated, which is slightly bluer. Essentially, this lets you read a person’s heart rate from a distance.
Even your breath says something about your physical state. “The ratio between oxygen and carbon dioxide will change as I become more and more fatigued. That’s important because as I’m fatigued, it takes about 24 hours for me to actually recover 100 percent,” Christensen said. “That fatigue is important because my muscles can’t strain to push the blood back to my head and so the probability of me losing consciousness increases significantly.”
Good sensors can even detect changes in metabolism that indicate weariness and stress before the person notices. When you’re stressed, you exhale fat—or rather, water-soluble molecules called ketones that your liver produces from fat. Stress is detectable by the molecular content of your breath.
“We’re working with some folks over at our materials lab and they have a couple of companies that are looking at sensors that are going to be placed in the [pilot’s oxygen] mask that’ll look at those types of fatigue-related volatile organic compounds,” says Christensen.
Your eyes, too, give you away. “Imagine eye-tracking cameras,” Christensen said. “If those can collect not just the motion data and the eye-motion data, but those are also getting heart rate and respiration, then we can have no hardware on you at all and still get all the same physiological metrics ... A certain amount of cognitive workload tends to correlate pretty highly with stress generically. You can combine heart rate with several other measures to get at workload stress; vigilance, even.”
“We are comparing it, just for reference, with wet medical electrodes on the chest. Under most conditions, you can do about as well as wet electrodes,” he said. The lab is “testing the limits of how far away can you get and still get a reliable signal. It turns out, it’s mostly an optics problem.” That means cameras and lenses alone can detect those subtle changes in stress and attention. It’s just a matter of figuring out which ones.
There are privacy ramifications to collecting so much information. A simple camera can gather enough biometric data on an individual to understand how small changes in heart rate can be a sign of stress. For a fighter pilot, an analyst, or a soldier, this might help warn of decreased cognitive ability. But among the general population, stress can also be a signal of deception, depending on the context in which that stress expresses itself, such as an interview at a checkpoint. Today’s military-funded biophysical research shows that it’s possible to detect that stress response from 100 meters away, and perhaps even at longer distances. In theory, if you could create a lens that could capture infrared data at sufficient resolution (currently, only a theoretical possibility), you could measure brain tissue oxygenation from low-earth orbit. You could see stress from space.
When performed without a subject’s awareness or permission, biophysical monitoring can be a violation of privacy. But conducted as part of an experiment with knowing volunteers, like elite soldiers eager to understand their bodies and improve their own performance, it becomes a powerful tool. One former special operations training psychologist, who currently works for a major league baseball team, said the elite soldiers he had served with were eager to improve their performance through data. In the Air Force, pilots want to improve how they fly, complete their missions, interact with their equipment, etc.
Bit by bit, this science is making its way into actual gear and weapons. In the year 2020, Navy SEAL teams and Army Rangers could take down high-value targets while wearing an exoskeleton that’s earned the nickname ‘Iron Man.’ Biophysical sensors will play a big role in the way the suit functions.
In February and March, the Air Force successfully tested a new helmet with “physiological monitoring capabilities,” as Tripp put it. Its heads-up display shows different information based on how the pilot is feeling and other factors. The goal is to give every pilot a slightly different experience based on their unique physical and mental strengths and weaknesses, as well as their physical condition at the moment. Lab researchers and contractors anticipate it will guide the design of the next U.S. fighter jet, to be launched between 2025 and 2030.
“I may do a really, really good job on a spatial cognitive task where I’m looking at a radar warning display, and maybe James doesn’t,” Tripp said. “The thought, down the road, is to quantify my performance in these decreased physiological conditions from a cognitive perspective, and then use the changes in physiology to make the airplane smart about what kind of help I need.”
Kaleb McDowell, lead of ARL’s Center for Adaptive Soldier Technology, said there will be a fundamental give-and-take when designing the weapons of the future. People perform better when their tools are crafted specifically for them. But it’s hard to design for individuals quickly and at the scale of hundreds of thousands of troops. That’s why the design of weapons software today flows toward averages—and mediocrity. “You’re designing it to be simple for everyone,” McDowell said. “A guy that’s great spatially doesn’t use the spatial capabilities on any system that you see today. A woman that has a great math capability isn’t using that in today’s systems because no one’s conceiving of a system that actually relies on that capability. You just design it for everyone to use.”
So McDowell wants to build weapons that adapt to their users. “I want my system to be able to rely on, say a great memory, poor math capability, and a great spatial capability. I want the system to be able to say, ‘This person’s really creative. How do I tap into that imagination when doing this dull task?’”
But that also affords the military far greater insight into what job or mission they are giving to what soldier. Researchers say that that is a key benefit of the new data-collection programs. “The basic goal here is: We want to get greater precision and accuracy in predicting which people will succeed in particular job areas or missions,” Air Force research psychologist Glenn Gunzelmann said at a National Defense Industrial Association event in March.
* * *
What if the Air Force could use an airman’s personal history to predict how he would perform in his surroundings—even in battle? The military already keeps massive records on troops’ lives that, if structured properly, might furnish a treasure trove of mineable health data.
Kirk Phillips, associate chief for bioenvironmental engineering at the U.S. Air Force, and his colleague Richard Hartman are pioneering a program called Total Exposure Health. The goal is simple: collect and analyze as much data as possible about what happens to soldiers beyond the battlefield, right down to the kinds of molecules to which they are exposed. And in the military, a lot is recorded.
“In the Air Force, for instance, if you want your house treated for an infestation, that gets recorded,” Hartman said. “We have more opportunity to interact with [people in the military] in that total environment. Where they live and where they work. It’s something that is better known to us. They receive health care from us. We can measure their exposure at work so we can offer to measure their exposure at home. We can know what exposures are in the environment because nobody is saying, ‘Why are you measuring the amount of chemicals in the soil?’”
If you could take that information and convert it into structured data, algorithms could produce all sorts of new insights about how individuals are interacting with their environment, in real time and in incredible detail. Phillips believes that exposure science has enormous applications in the emerging field of epigenetics research.
Here’s where Phillips’s vision becomes both revolutionary and controversial.
Epigenetics is what your genes do with the change that you experience. It’s based not on your immutable DNA, but rather on your micro-RNA, the tiny molecules that turn on or off in response to stimuli. Think of a stress hormone that your body creates in response to an event. When your stress level goes down, new micro-RNA are formed and that controls gene expression in everything from your metabolism to how well you recover from disease. But it’s incredibly difficult to understand these interactions, precisely because everyone’s genetic makeup is different. Phillips hopes Total Exposure Health will yield a fuller picture of how specific sets of experiences affect specific sets of micro-RNA inside a specific soldier.
“Let’s say that external stress happens to be a chemical exposure you may never encounter, or there may not even be a micro-RNA that turns that part of your gene off that it activated. You may have the gene that activates under that exposure, and I may not. You may be very susceptible to a chemical that I have very little susceptibility to,” Phillips said.
Phillips thinks that if he can detect these kinds of things for the military, Total Exposure Health could revolutionize civilian healthcare as well. It offers high specificity on individual health on a scale of billions of people.
“You’ve probably read in the newspaper that they did a big study and they looked at red wine. They tried to see whether there was a health benefit to drinking red wine. Another study says: Maybe. Another study says: Not really. That’s because it’s population-health-based,” he said. “They’re just trying to pick a random population to see a population level change. If you have a gene that’s not very prevalent in a population, then you won’t get a population result of that exposure. Precision health and medicine says, ‘I should understand your gene in a way that I can understand whether your gene is activated by red wine and whether that activation is a health benefit, or health detractor.’”
“Right now, you might go, ‘I’ve read studies about red wine and they seem to be all over the place. I don’t know whether I should do it.’ Health care of the future would look like this: Your physician would say, ‘You know what? We looked at your genome. We know that red wine activates in the genome in a way that provides the health benefit. You don’t have the gene, so only drink red wine to the level that you find it pleasurable in social situations.’”
If Phillips is right, Total Exposure Health may ultimately give millions of people an incredibly detailed understanding of how their health choices affect their future. Not just, for example, how much alcohol is unhealthy for an average person of their age, weight, etc. to consume, but how much red meat, caffeine, sleep, etc. is good for them specifically.
It is becoming possible to know the health outcome of any action with an accuracy that would have seemed supernatural just a few years ago. The ability to comprehend the probabilities that form the future is the ability to influence it. The interplay of our genes and our experiences, of nature and nurture, moves from the mysterious to the knowable, or at least toward the more knowable.
* * *
For the military, this opens up new choices that are pulled directly from dystopian science fiction: anticipating what soldier is best suited for what assignment or mission.
In 150 BC, the Greek writer Polybius observed that Roman military units were doing something that no known army had done before: keeping careful and consistent records. The Romans could ration grain and wine across soldier classes and types because they had a uniform system of record-keeping for just that purpose. The reduction of unpredictability was proving a great battlefield advantage.
Imagine a military doing the same thing today but on a level both grander and more granular, where the substance to be rationed is a particular type of soldier personality, or even a specific kind of neurotransmitter.
Again: U.S. military officials are adamant that they are not genetically engineering military personnel and have no plans to do so. But they do not expect potential adversaries to share the same constraint, especially if it offers advantage over the military might of the United States. (Remember the movie Rocky IV? Just consider the Russian government’s recent systemic and secretive use of performance-enhancing drugs to win at the last Winter Olympics. Now imagine a battlefield of soldiers.) It’s a future to either embrace or learn to defend against.
If you were to use biometrics to genetically design a superior military, how would you do it? The outlines are visible today.
Individuals disposed toward risk-taking are probably better suited for particularly dangerous deployments and missions. But those same individuals are poorly suited for other aspects of military life, or less exciting military vocations, according to a landmark 2000 study by U.S. Army Maj. Michael Russell. He proposed that there were two primary military personalities: soldiers who exhibited a need for action and unpredictability (high stimulus-seeking) and people who were attracted to the military because its life offers a high degree of structure and discipline. A military needs both types to perform at peak but therein lies a fundamental contradiction. Military life is incredibly structured. War is unstructured. The stronger your attraction to one set of stimuli on the spectrum, the greater your aversion to the other.
“It has long been recognized that a peacetime army differs in many ways from that of an army at war. This is intuitively obvious: Destruction of personnel and equipment, even enemy equipment and personnel, is somewhat antisocial,” Russell wrote, achieving a new plateau in euphemism by calling blowing up the enemy “somewhat antisocial.”
“To plan the ultimate defeat of an entire army or nation on the battlefield requires at least a dose of narcissism. Therefore, those personality attributes that make for a war hero are primarily from cluster B. These people do not function as well in garrison. Such individuals thrive on challenge and require constant stimulation,” he wrote.
Merle Parmak, a military psychologist and a former Estonian Army captain, discovered that individuals who perform better in a highly structured, less exciting environment can also have great military careers, but perhaps not on the front lines. To a certain extent, you can train risk-taking soldiers to better accept the rigid boredom of military life away from the action, just as training can help structure-minded military personnel to better cope with the unpredictability of combat. But sticking the wrong person in the wrong job has costs.
Now consider the role that dopamine plays in risk-taking, according to an established and rapidly growing body of research. Dopamine levels are at least partially controlled by the monoamine oxidase A gene, or MAOA. A specific variant of MAOA called VNTR 2 was correlated with violent antisocial behavior, but only in the context of a stressful life event in adolescence.
If the connection between genetic factors, life experience, and risk-taking can be better observed, can they also be controlled? This is the question that will loom over military leaders in the decades ahead.
The Pentagon’s projections for future conflict are these: highly confusing and stressful urban warfare engagements. Population demographics pushing people into megacities means more door-to-door fighting, and more rules to protect civilians against adversaries who don’t have the same commitments to internal law or norms. War in the future ... sucks.
Depending on the intensity level of different conflicts in which the United States is engaged, the level of violence, the effectiveness or the simple ruthlessness of the enemy, the military may feel pressure to keep up with an adversary short on reservation. Should the United States find itself in such a conflict, Pentagon leadership may feel very differently about genetic engineering to secure better soldier performance, especially since doing so might degrade U.S. military advantage at less cost.
Should some future leader—of any country—make the decision to abandon the ethical frameworks we live by today, the tools will be there for him or her to make a swift transition.
But even genetically engineered humans might lose the battle in the end. The pace of war exceeds the speed at which humans can observe what’s happening, conceptualize a strategy, and deliver commands to pull off complicated counter-maneuvers. This is sometimes called the observe, orient, decide, and act, or OODA, loop, and it’s moving from a thing that humans do on the battlefield to a thing machines do. If you listen to the Pentagon’s top strategists when they talk about the future, this concern rises repeatedly. “When you think about the day-trading world of stock markets, where it’s really machines that are doing it, what happens when that goes to warfare?” William Roper, the head of the Pentagon’s Strategic Capabilities Office, asked at last year’s Defense One Tech Summit. “It’s a whole level of conflict that hasn’t existed. It’s ... scary to think about what other countries might do that don’t have the same level of scruples as the U.S.”
Given a choice between losing a major conflict and taking advantage of next-generation science to create a new advantage, it’s not hard to predict what any military will choose.
This article appears courtesy of Defense One.


The president of the United States wants a see-through wall at the U.S.-Mexico border, in part because he’s worried about people getting hit in the head by any bags of drugs that might be hurled over the divide.
“As horrible as it sounds, when they throw the large sacks of drugs over, and if you have people on the other side of the wall, you don’t see them—they hit you on the head with 60 pounds of stuff? It’s over,” Trump told reporters on Thursday. “As crazy as that sounds, you need transparency through that wall.”
To which at least one border security expert responds: “Over a 2,000 mile border, I think you’d have a higher chance of getting hit by a meteorite than a bag of drugs.” That’s Sanho Tree, a fellow at the Institute for Policy Studies who focuses on drug policies and the border. When I talked to Tree this morning, he was amused and astonished by the president’s remarks.
“The most important thing is, Donald Trump just admitted that he’s worried about people throwing drugs over a wall that’s designed to stop drugs!” Tree said. “Wile E. Coyote needs to take this wall back to Acme, because the road runner just owned him.”
Well, Trump did acknowledge he sounded crazy, didn’t he?
The thing is, the suggestion of a transparent wall isn’t Trump’s alone. Top officials from the Department of Homeland Security and U.S. Customs and Border Protection have advocated for a design that allows agents to see through portions of the divide.
“Not just for what the president describes as bags being thrown over, but just as important is being able to get a view of the threat that is actually staging on the other side of the fence and is preparing to take on some kind of breaching effort,” says David Aguilar, the former deputy commissioner of Customs and Border Protection who now works as a consultant for Global Security and Innovative Strategies. “The reality of the situation is that a see-through capability is critically important—but what is even more vital is situational awareness.”
And situational awareness doesn’t just come from being able to peek through slats in a wall, he says, though he emphasizes that “the president is right, the transparency is important.” Modern border security also requires surveillance towers, tethered drones, cameras, sensors, and other technology. “It is a combination of all of those things,” Aguilar told me. “Technology cannot be everywhere all the time.”
Trump may have gotten a bit confused about the threat of soaring bags of drugs. (“It’s not easy to describe,” Aguilar says.) What the president may have been thinking of, Aguilar suggests, is the problem with people throwing rocks or concrete blocks over the wall—sometimes a tactic used by spotters to divert attention, he told me. “Chucking things that can hurt our agents,” Aguilar says. “There have been very, very, very serious injuries with cement blocks being thrown.” Such injuries are rare now that there are more protections in place at the border, he says, but assaults against agents have climbed in recent months. CBP could not immediately provide data about how many agents have been struck or injured by objects thrown over the border.
From a drug-policy perspective, Tree says, the president’s ideas are dangerously misguided. “The idea that this wall will stop a few migrants carrying a couple of kilos compared with the tons that come through the tunnels?” he said. “Stopping a couple of kilos is really ridiculous. The vast majority of illegal drugs come through legal checkpoints. The wall will do nothing to stop that.”
Tree also worries that the president’s approach will inspire drug traffickers to cut their heroin supplies with even more fentanyl, a synthetic version of the drug that’s much stronger than heroin. Anticipating a potential dip in the heroin supply, he says, they’re more likely to reach for fentanyl as a way to stretch profits.
“People will drop like flies in this country,” Tree says. “It illustrates how little thought the president has actually given to any of these problems. He won’t take any advice. He won’t talk to anyone who won’t agree with him. That’s very dangerous.”
“Well, look,” says Aguilar, the former CBP leader, “There’s no doubt that the president’s approach to the border has brought attention to it. Now there’s attention, and then there’s attention. From an enforcement perspective, the attention that has been brought is an absolutely a positive thing. Now the question needs to be asked: What do you need?”


There is an old debate (at least, counting in internet years) that tends to crop up after major cybersecurity breaches such as the widespread WannaCry ransomware attack in May. In the aftermath of such incidents, some decry the sorry state of cybersecurity and insist that if only tech firms, with their wealth of resources and technical expertise, were allowed to go after the perpetrators of these attacks, they would do a much better job of stopping the damage and deterring other perpetrators than the slow, plodding, over-worked, under-resourced, jurisdiction-bound law-enforcement agencies.
Which raises a question: Beyond the standard set of protective tools—encryption, firewalls, anti-virus software, intrusion-detection systems, two-factor authentication—should companies be allowed to go outside the boundaries of their own networks and crash the servers that are attacking them, or delete data that has been stolen from them off their adversaries’ machines? The answer of most companies and cybersecurity experts is no. But that doesn’t stop a vocal minority—usually researchers at libertarian think tanks and lawyers concerned by how restrictive anti-hacking regulations have become—from suggesting otherwise.
The New Economics of Cybercrime
The notion that companies should be given more leeway to engage in “active defense”—the cybersecurity euphemism for offense—has been quelled for years by the Computer Fraud and Abuse Act (CFAA) in the United States and its counterparts in other countries, which effectively make it illegal for people to access computer systems that don’t belong to them without permission from the owners. But some lawmakers feel the need to carve out an exception to that blanket ban for companies that infiltrate external networks in the name of self-defense. In March, Georgia Representative Tom Graves proposed the Active Cyber Defense Certainty Act (ACDC), which would change the CFAA so that it would not apply to victims of cyberattacks who accessed attackers’ networks to “gather information in order to establish attribution of criminal activity to share with law enforcement” or to “disrupt continued unauthorized activity against the victim’s own network.”
Stewart Baker, a former homeland security assistant secretary under George W. Bush and a current partner at the D.C. law firm Steptoe & Johnson, would also like to see companies be allowed to hack back. A sharp critic of the current state of law enforcement’s cybersecurity efforts, Baker has for years been trying to make it easier for private firms to be able to pursue their adversaries in cyberspace. “For a company to go to the FBI and say ‘I’ve been hacked, can you find the hacker,’ it’s like going to a university town’s police force and saying, ‘Somebody stole my bike’—you’re lucky if they don’t laugh at you,” Baker says. “The government is completely consumed just trying to take care of its own data and tracking its own attackers. It doesn’t have the resources to help firms and probably never will.”
While it may seem irrelevant to liken denial-of-service attacks and data breaches to bicycle theft, most discussions of cybersecurity laws and policies, for better or for worse, happen by analogy to the physical world. And to Baker, there are two other important ways of thinking about the importance of legislation like the ACDC. One is the widely accepted principle of an organization’s right to defend its own interests. The other is the idea that there are many tiers of people with different responsibilities in between ordinary civilians and actual law enforcement. “In the physical world,” Baker explains, “there are all kinds of people in the middle, between innocent civilians and full-on military and law-enforcement protective personnel, who have intermediary authorities—bounty hunters, private investigators, mall cops—all people who have some additional authority and who ought to be able to use that additional authority because it was deemed necessary not to rely exclusively on the police. If that’s where we end up in the physical world, why would we not welcome the idea of having intermediate authorities between ordinary civilians hunkering down behind their firewalls and the police?”
Baker’s advocacy is driven not by industry interests so much as his own deeply held belief that government officials and law enforcement agencies are incapable of addressing online threats themselves. “It’s like the NRA saying, ‘When seconds count, the police are only minutes away,’ except the police are days away when you’re talking about cybercrime,” Baker says.
Baker and Representative Graves, though, are in the minority. At least among most people willing to speak on the record, legalizing proactive responses to cybercrime is a wildly unpopular idea. Its critics range from law enforcement officials who worry it will lead to confusion in investigating cyberattacks, to lawyers who caution that such activity might well violate foreign laws even if permitted by the U.S., to security advocates who fear it will merely serve as a vehicle for more attacks and greater chaos, particularly if victims incorrectly identify who is attacking them, or even invent or stage fake attacks from adversaries as an excuse for hacking back.
And if big tech firms are clamoring for the opportunity to go after their attackers more aggressively, they are certainly not doing so publicly. “I haven’t heard from particular companies that they want to have that activity authorized,” says Greg Nojeim, the director of the Freedom, Security and Technology Project at the Center for Democracy and Technology, a think tank. At least a couple companies have actively gone after adversaries in the past—Google reportedly breached a computer in Taiwan in 2010 while investigating attacks on its customers, and in 2014 the FBI examined whether some banks had hired hackers to crash servers being used by Iran—but known examples are few and, on the whole, relatively tame.
“I think a lot of companies would be hesitant to take the position,” Nojeim continued, “that it’s okay to engage in active-defense measures on somebody else’s network out of fear that their own networks would then become targets.” He, like many critics of broad hacking-back legalization, makes certain distinctions for defensive activities he views as less problematic. For instance, he is comfortable with “beaconing,” the practice of attaching code to sensitive files that will report back to their owners the IP address of machines they are copied onto when stolen.
Others argue that the crucial limits relate to who is permitted to hack back rather than what they are permitted to do. For instance, Jeremy Rabkin, a law professor at George Mason University, has advocated for putting together a list of cybersecurity firms vetted by the U.S. government, so that companies could hire an approved hack-back vendor to go after its online adversaries. “A lot of things can go wrong when people start mucking around in your files and your systems,” Rabkin told me. “You have to trust these people; you have to be sure that they’re not going to steal stuff or tip off other people.” In his estimation, there are only a handful of firms—highly regarded security companies and contractors that have longstanding relationships with the U.S. government and ex-military personnel, mostly—that can be trusted to pull this off.
Michael Chertoff, the former secretary of homeland security under George W. Bush who now runs his own consulting firm, argues that any private firm’s activities should be not only government-approved but also closely coordinated with U.S. officials. “If it’s not done at the direction of the government, then you get into something which is not terribly different from what the Russians do,” he says, referring to the Russian government’s reliance on intelligence gathered by criminals, allowing it to benefit from crimes without accepting responsibility for them. Ultimately, Chertoff doesn’t think that countering cyberattacks is something the government needs help with or for which it is interested in outsourcing its responsibilities.
Patrick Lin, a professor of philosophy at California Polytechnic State University, has finer-grained logistical concerns about any legislation that opens up the possibility of hacking back, regardless of what one makes of whether it is justified or not. “It is much too premature to allow for hacking back, even if the practice isn’t immoral,” Lin says. “At minimum, there needs to be a clear process to authorize or post-hoc review cyber counterattacks to ensure they’re justified, including penalties for irresponsible attacks. That oversight infrastructure hasn’t even been sketched out.” (There’s little discussion of such oversight in the current discussion draft of the ACDC, though under the most recent draft, released in May, companies would be required to report their activities to the FBI.)
At a moment when most people are concerned with trying to reduce online attacks, proposals to legalize hacking back and encourage more cyber conflict are a bit of an oddity. They rely on the implicit assumption that offense is the best defense, even though offense and defense have, in general, looked entirely different from each other online: The tools for defending computers like encryption and network monitoring bear almost no resemblance to the tools used to attack computers, such as botnets and phishing. Legalizing hacking back would conflate those two domains and, in doing so, likely make it that much harder to distinguish between the good guys and the bad guys online.


Half the world’s population already lives in cities, and that number is expected to jump to 70 percent by the end of the century. To accommodate the new urban dwellers, cities will have to build higher—and that will mean doubling down on ways to transport residents from the ground up into the sky.
The medieval town of Rottweil, in rural South Germany, may seem like an odd place to contemplate the high-tech future. (The locale’s claim to fame is breeding the Rottweiler dog.) But ThyssenKrupp, an industrial company based out of Essen, managed to do so last month, at a flashy event promising to change how we design, build, and occupy tall buildings.
“For 150 years, elevators have been dominated by ropes,” says Andreas Schierenbeck, CEO of ThyssenKrupp Elevator. It’s technology that by now, most of the world knows well: cables hoist a car up and down the elevator shaft, making stops along the way.

But with this promise of increasing urbanization, ThyssenKrupp sought to fill an opportunity to make tall buildings more efficient. Their new technology, known as MULTI, throws out the traditional elevator configuration in favor of a ropeless system that can move both horizontally and vertically. The conventional steel rope most elevators run on adds considerable weight to a building, and becomes more strained the taller you build, ultimately restricting a tower's overall height. By eliminating the cables—and the height restrictions that come with them—ThyssenKrupp executives brag it’s a technology that could send “an elevator up to the moon.” Indeed, it’s the stuff of Star Trek and Willy Wonka—but it could eventually make its way to a city near you.
The company unveiled a functioning MULTI system at ThyssenKrupp’s 807-foot-tall concrete test tower, which has been a proving ground for the system over the past two-and-a-half years. The result is an elevator utilizing the same magnetic technology that moves Japan’s bullet trains. In this model, elevator cars—not unlike train cars—move along magnetic tracks, uninhibited by traditional cables. Linear motors and a multiple-level brake system replace cables. Cabs are able to change direction from vertical to horizontal thanks to a rotating “exchanger.”
“We’ve been waiting for these developments for a while,” says Roger Soto, a design principal with the global architecture firm HOK. Soto led the design of the Capital Market Authority Tower in Riyadh, Saudi Arabia, which includes another ThyssenKrupp elevator innovation. When the 1,260-foot tower opens in 2018, it’ll utilize the company’s TWIN elevator system, in which two elevator cabs travel independently—one above the other—in the same shaft.
The elevator is “pretty critical” to skyscraper design, Soto explains, as it makes up the building’s core. But the current cable system takes up more space the higher you go. For the CMA Tower, the TWIN allowed HOK to build taller on a smaller floor plate: “The TWIN system allowed us to actually pack the elevators into the core in a way that made the tower more efficient and economical,” Soto explains.
The horizontal movement “is something I’m still trying to get my head around,” he says. “But I think the elevator can free us from certain constraints we have right now, and allow us to innovate in the way we conceive of towers.”

At the MULTI unveiling event, Antony Wood, executive director of the Council on Tall Buildings and Urban Habitat, spoke to the shifting trends already happening in skyscraper design. For one, most of the innovation has moved out of North American cities like Chicago and New York, and is happening across Asia and the Middle East. (The world’s tallest tower, Burj Khalifa, opened in 2010 and extends 2,717 feet in Dubai.)
We’re also using these towers differently. Instead of office towers built to symbolize a single company—the Chrysler and Sears towers, for example—they are often operating as mixed-use “mini cities” with a combination of residential, office, hotel, and public space. As Soto put it, “We’re thinking more about creating social connections in a vertical setting.”
Wood called the MULTI “the holy grail of elevators” to address such shifts. For one, the system allows multiple elevator cabins to operate on a loop, moving more people in a continuous flow. Eliminating the space traditionally reserved for elevator shafts, it also frees up square footage for more apartments or office space. Schierenbeck estimates the system can achieve up to a 50 percent higher transport capacity, while increase a building's usable area by as much as 25 percent.
But at what point does an elevator become a totally different mode of transit? “This is technology that’s not really distinguishable as an elevator anymore,” says Daniel Safarik, the China Office Director at Council on Tall Buildings. “They’re more like transportation vehicles of some kind. If you can get an elevator to go sideways or diagonally, then what’s the difference between the car, the subway, and the elevator? They start to have a lot of the same properties.”
Safarik thinks the real innovation of the MULTI is the possibility to send elevators underground, where they could move horizontally to connect buildings and transit hubs. It’s been proposed before. Last year, London architecture firm Weston Williamson & Partners proposed a MULTI system that traveled underneath buildings vertically before descending down to Tube platforms, creating connections between stations. Innovative as it may sound, Safarik sees the “jurisdiction between what’s a building and what’s infrastructure” posing the biggest challenge to MULTI transforming cities in this way.
The stalwarts of the elevator industry have their own concerns. “Tradition still has a strong role here,” says Rick Sayah, vice president of the New Jersey elevator consulting firm Van Deusen & Associates. “The reproducibility of work is how we’re able to maintain safety, training and maintenance.” His questions, too, revolve around what the technology exactly is, and who should regulate it: “Is this an elevator? Is it a vertical extension of the transit system? Will it require a new paradigm of building code?” He believes it’ll be a challenge to build new safety codes around a technology that’s so unlike the traditional elevator.
Schierenbeck says, “In the last five years, ThyssenKrupp developed comprehensive functional safety concepts using a multi-step braking system capable to handle all possible scenarios of operation.” The company, which has yet to obtain a safety certificate for the technology, expects cars can begin testing with people inside in the next few months.
The East Side Tower in Berlin, designed by Netherlands-based OVG Real Estate, will be the first with a MULTI system. The company has released few details on the project, besides an anticipated opening in 2019.
Safarik, of the Council on Tall Buildings, believes the technology is likely to come to the United States as part of a hospital, campus, or government complex, where horizontal elevators can shuttle people from one building to the next. “It’s not particularly freaky to imagine,” he says. “It’s a logical thing, even if it has always been in the realm of science fiction.”
This post appears courtesy of CityLab.


I am the Danube River.
My spirit is sparkling and swift. I yearn for new experiences and deep connections with people. I’m adaptable, but to a fault; I rarely see danger ahead. I’m capable of infidelity without much remorse. I’m also great at ceramics.
So says Meet Yourself As You Really Are, the oldest, longest, and WTF-est personality quiz I’ve taken. Published in 1936, Meet Yourself is a 336-page home-psychoanalysis test that promises to “‘X-ray’ the reader’s fundamental character.” It does so with an interminable line of questions both probing and random. Are your parents dead? Have you ever had the sensation of standing outside your own body? Do Mickey Mouse cartoons freak you out? What do you think of unskimmed milk?
As you tally “yes” and “no” answers, the book directs you to new sections based on your responses. Somewhere in the middle, you’re categorized as one of 15 rivers—the Nile, Seine, Thames, Missouri, and so on—and eventually you’re offered long-winded personality breakdowns. “As you travel across the network of questions and data by your private track, your story unfolds and your character is explained,” the introduction teases. The book has been described as a Freudian Choose Your Own Adventure, which is accurate enough: It’s like Give Yourself Goosebumps, but instead of escaping the Carnival of Horrors at the end, you learn that you have commitment problems.
Clearly, personality quizzes have some sort of perennial appeal. Facebook newsfeeds are filled with BuzzFeed quizzes and other oddball questionnaires that tell you which city you should actually live in, which ousted Arab Spring ruler you are, and which Hogwarts house you belong in. But these new online quizzes have a dark edge that their analog predecessors didn’t. In the wake of the U.S. election, a secretive data firm hired by Donald Trump’s campaign boasted that it has been using quizzes for years to gather personal information about millions of voters. Its goal: the creation of digital profiles that can predict—and possibly exploit—Americans’ values, anxieties, and political leanings.
Whether this firm, Cambridge Analytica, has actually used predictive profiles to influence people isn’t certain; reports suggest it hasn’t, at least not directly. But the company’s methods nonetheless expose the growing scale of personality analysis online—and the dangers that come with it. On the internet, anything you do is like taking a personality quiz: Everywhere you click reveals something about you. And you’re not the only one who sees the results.
On Meet Yourself’s spine, a silver-painted mirror depicts the personality quiz’s allure: See yourself as you really are. What happens when the mirror is two-way?
* * *
No one seems to know when the personality quiz first gained a foothold in popular culture. The journalist Sarah Laskow has traced its origin in America at least as far back as the late 19th century, “when ladies’ magazines started gaining traction and the yellow press would try anything to sell papers.” But the quiz has persisted with remarkable consistency since, with spikes in popularity during a quick magazine boom immediately post-WWII, the Cosmopolitan quizzes of the 1960s and ’70s, and today’s ubiquitous BuzzFeed quizzes.
This stretch neatly overlaps with the history of the popular quiz’s buttoned-up, high-achieving sibling: the personality test. In-depth psychological assessments like the Myers-Briggs Type Indicator began popping up in the first half of the 20th century for the purpose of scanning and sorting employees in industrial workplaces. While many of these tests, including the Myers-Briggs, have since been dismissed by the scientific community as unreliable—if not dangerously discriminatory—they, too, have persisted, perhaps in part because they at least provide a framework for otherwise-difficult office conversations. Somewhere around 10,000 companies, 2,500 colleges and universities, and 200 government agencies still use the Myers-Briggs in the U.S. today, including the majority of Fortune 500 companies.
Affiliation with these more legitimate-seeming forms of personality analysis has always given the personality quiz a vague air of authority. Indeed, if there’s any one way to characterize quizzes’ mystique, it’s probably that, through all their many iterations, they have somehow managed to tightrope-walk the line between entertainment and science, or at least something approaching science. “BuzzFeed quizzes are crafted to create the illusion of truth, or potential truth,” writes the journalist Caroline O’Donovan, in explaining the fad. She quotes Summer Anne Burton, one of BuzzFeed’s editors: “You sort of write them like horoscopes, with tidbits people can relate to.”
Meet Yourself discovered this strategy long before today’s click-miners. The book’s co-authors, the British novelist William Gerhardie and the Spanish aristocrat Prince Leopold Loewenstein—whose son, incidentally, went on to be the financial manager of the Rolling Stones—flip freely between promises of profound insights and guarantees of fun.
If the limited information out there about the book’s critical reception is any indication, people have never taken Meet Yourself too seriously. A short review in a March 1937 issue of Ohio’s Piqua Daily Call deems it “a fairly amusing way of filling in an odd hour or so,” and includes this sick 1930s burn: “If you sit down prayerfully with it and answer all of its impertinent questions, you will never again be phased by any little thing like an income tax or civil service examination blank.” Decades later, a columnist for The Independent ran into the book on a trip with her boyfriend to her parents’ house, and cracked up when it announced to the family that the boyfriend was “a conqueror of women.”
As BuzzFeed’s quizzes really started gaining steam a few years ago, a deluge of think-pieces attempted to make sense of why people just can’t get enough of them, even when they clearly have little to do with reality. Reasons included narcissism, existential searching, and boredom. Laskow made the case that people simply like talking about themselves. These probably are all true, to some extent. But they overlook something deeper about the nature of personality itself.
* * *
Simine Vazire believes that a good personality test rarely tells you anything you don’t already know. As director of the Personality and Self-Knowledge Lab at the University of California, Davis, she studies how people come to understand who they are. “We know a lot just by being in our bodies, by being ourselves,” she says. Tests promising to unveil hidden truths about their takers—tests known as projective in psychology—are mostly bogus.
What tests offer instead, Vazire suggests, is reflection. “When you have someone summarize to you what you just told them, it gives you a sense of, ‘Oh, yeah, that’s what I was trying to say.’ I think this is just a version of that,” she says. “Because you’re talking about yourself, and you’re answering a bunch of questions about yourself, a test can summarize this information for you. It can give you a precise or better language for summarizing yourself, even if it’s based on what you told it.”
This reflection isn’t the quasi-mystical type of self-knowledge Meet Yourself claims to be after. It doesn’t show you as you really are, but rather helps you articulate who you know yourself to be.
That distinction might sound trivial, but it actually makes a critical point about how personality functions. In its textbook definition, personality is all about patterns: “individual differences in characteristic patterns of thinking, feeling, and behaving,” as the American Psychological Association puts it. Personality, in other words, is not some set thing. It’s the result of a messy web of tendencies and habits, all informed by some incalculable mix of biology, disposition, and learned behavior.
That makes personality slippery. While psychologists debate the granular details of its definition, what it amounts to is that personality is something of a paradox. People tend to have a sense of their own character, but this sense is never complete. We know ourselves, but we don’t.
This is why people love personality quizzes. Beyond vanity and narcissism and harmless fun, taking a personality quiz helps me get out of my own head, to see whether my experience of myself matches up with how others experience me. This is the same reason I sometimes catch myself staring into the mirror even after I’ve double-checked that my fly is zipped and fretted over my oh-so-slightly thinning hair. There’s an element of affirmation, even awe and wonder, to the reflection. That is me in the world.
In this light, personality quizzes actually appear pretty beneficial, or at least innocuous. And maybe they would be, if they left me to my own musings and no one else ever saw the results. But a psychological need for self-reflection gets complicated when the mirror also snatches up information for other people to use.
* * *
In 2008, Michal Kosinski joined a research project that helped revolutionize how data about people is collected. As a graduate student in Cambridge University’s Psychometrics Centre, a department that studies online psychological assessment, he and his classmate David Stillwell distributed a short personality quiz on Facebook that told people how they rated among psychology’s “big five” personality traits: openness, conscientiousness, extroversion, agreeableness, and neuroticism (OCEAN, for short). Upon receiving their results, quiz-takers had the option of sharing their Facebook profiles with Kosinski and Stillwell.
It didn’t take long for the two researchers to amass the largest-ever collection of psychometric scores paired with Facebook profiles. “It was surprisingly easy to get people engaged,” says Kosinski, who’s now a professor of organizational behavior at Stanford’s graduate school of business. “[Stillwell] just sent it to his friends on Facebook. Those friends took the questionnaire, then shared it on their profiles, and suddenly you had thousands of people taking it every day.” Over a few years, Kosinski and Stillwell gathered info from millions of Facebook users.
The two researchers used this data to build a new system for predicting people’s personalities. With access to their subjects’ OCEAN traits and Facebook information side by side, Kosinski was able to correlate what people were like with the personal details available about them online—Facebook likes, gender, age, and so on. Soon, he had an algorithm that, based on analyzing Facebook likes alone, could guess how people think, feel, and act—that messy web of tendencies and habits—with startling accuracy. With 70 likes, the model could predict someone’s personality, as measured by a 100-question personality test, better than that person’s friends could. With 300 likes, it could outperform a husband or wife.
This predictive approach pioneered by Kosinski is what stirred up so much controversy in the U.S. election. Kosinski himself had nothing to do with the data firm hired by Trump’s campaign. Motherboard and Das Magazin have reported that his research appears to have been brought over to Cambridge Analytica by a young colleague of his with ties to the firm’s parent company, the London-based Strategic Communication Laboratories Group, or SCL. (Cambridge Analytica denies that the company or its methodologies have any connection to Kosinski.) After the election, the firm first declared that its brand of “psychographic” profiling played a major role in Trump’s victory, then conceded that it never actually used the approach to influence voters.
Whatever the impact so far, predictive methods like Kosinski’s and Cambridge Analytica’s are pushing open the door to a new—and unsettling—stage of personality analysis. It’s no revelation that personal data can be collected online, and there are already plenty of ways this data can be used to influence people: In politics, for instance, Barack Obama targeted individual voters with psychometrics well before Cambridge Analytica was even formed. But Kosinski saw just how much information can be gained. With personality quizzes, he found what might be the most direct route into people’s hearts and minds yet.
Kosinski never requires Facebook users to give up their profiles to take his quizzes. But the worrying implication of his kind of approach is that online personality analysis can easily blur the line between opting in and opting out. When algorithms can be trained to accurately infer your personality based on anything you do, the internet is a personality quiz—or, at least, it can be, so long as each page visit, web search, and “like” can be gathered and correlated.
Online, before you even click on a quiz, you’re already filling something out.
* * *
There are a lot of good reasons to worry about this technology. Imagine an advertising company that knows you’re self-conscious about your weight, so tries to sell you diet pills. Or—in a hypothetical that often comes up in this kind of discussion—imagine a political campaign that knows you’re prone to anxiety, so targets you with ads about the dangers of the Islamic State. “Big data companies already know your age, income, favorite cereal and when you last voted,” notes a New York Times report on Cambridge Analytica. “But the company that can perfect psychological targeting could offer far more potent tools: the ability to manipulate behavior by understanding how someone thinks and what he or she fears.”
While Kosinski thinks people’s rapidly diminishing privacy online is indeed dangerous, he’s quick to point out that there are potential benefits to personality profiling, too. Targeted ad campaigns could get kids to quit smoking, he suggests. Personalized political messages could inform voters, not pull their strings.
Companies like Cambridge Analytica have a commercial stake in exaggerating their techniques’ reach, as well. “What they’re selling is not exactly snake oil, though it can work as a placebo for panicky candidates who are down in the polls with weeks to go before Election Day,” the journalist Leonid Bershidsky has argued. “But just like artificial intelligence or, say, the blockchain, [data science has yet to produce] killer apps that can ensure a political victory or business success.”
I’m not convinced the nerdy podcasts and obscure track-and-field clubs I like on Facebook will hand the reins of my life to some shadowy corporation anytime soon. But I do think the threat is real—real enough, at least, that I wouldn’t give away my profile information for a personality assessment.
Something Kosinski told me gave me an uneasy feeling I haven’t been able to shake, too. There’s research that has been done on people’s trust in algorithms. A subject talks to an expert on a topic, and the expert offers some sort of insight on that topic, backed by one of two possible justifications: either a) the expert has thought about this for a long time, or b) the expert’s computer calculated the solution. The results show that people are more likely to trust the computer. “We’re being trained by algorithms that they’re always right,” Kosinski says.
Surely, such trust isn’t always misplaced. Vazire, the UC Davis psychologist, admits that she’d probably trust an algorithm over an expert—if she knew the algorithm to be accurate. But what if it’s not? What if, say, it’s built upon data collected by researchers who are prone to error and bias? Or what if it’s intentionally incorrect—sneakily incorrect? Conceivably, an algorithm could know so much about you that it could say exactly what would make you think, act, or feel a certain way.
That’s where the impulse to take a personality quiz keeps me up at night. I’m wired to seek out ways to reflect on who I am, but who I am is slippery—and that makes me open to suggestions. If people’s faith in algorithms continues to grow, it might not be long before I trust a computer to tell me about my personality more than I trust friends or family—or more than I trust myself.
That’s a strange future to imagine. But, hey, I am the Danube River. I’m adaptable. I’m sure I’ll adjust.


Okay, what the hell happened to the New York subway?
It wasn’t so long ago that New Yorkers could smugly look to cities like Boston (Aww, what a darling toy train!) and Washington (Six whole lines? Adorable!) and appreciate the scope and reliability of their underground transit system.
But between fare hikes, overcrowding, frequent breakdowns, mechanical failures, signal gaps, janky cars, and rickety tracks, New York’s Metropolitan Transportation Authority is facing millions of angry riders and a multibillion-dollar repair job that is likely to span decades.
One of the weirdest aspects of all this is that the problems seemed to come all at once, out of nowhere. The subway was mostly fine! But, then, it was most definitely not.
Subway delays have more than doubled over a five-year period. Track fires increased. “The current state of the subway system is unacceptable,” the MTA’s newly reinstated chairman, Joseph Lhota, said in a statement late last month as he outlined an emergency improvement plan. “We must rebuild confidence in the authority with a complete overhaul of the system.”
It’s not yet clear exactly what that overhaul will look like—or, crucially, how much it will cost. Initially, the MTA said it would need as much as $20 billion for systemwide maintenance and repairs. (Lhota didn’t reply to my request for an interview.) In the meantime, New Yorkers are left wondering how things got so awful.
Where to begin? For one thing, there’s the old—so very, very old—infrastructure. “In fact it’s so old that the MTA can no longer buy replacement parts from the manufacturer,” James Somers wrote in a 2015 essay for The Atlantic. “It has to refurbish them itself.”
The 1960s-era Brightliners, those stainless-steel C-train cars, break down constantly—every 33,000 miles on average, The New York Times recently reported. That’s compared with the average subway car, which breaks down every  400,000 miles, and the newest cars, which break down every 750,000 miles, according to the newspaper. Then there’s the signaling system that Somers wrote about. On top of being ancient and unreliable, signals are inspected far less frequently than they were a decade ago. They’re languishing despite sorely needed upgrades that could otherwise improve efficiency to accommodate the growing throngs of riders.
Why New York Subway Lines Are Missing Countdown Clocks
Oh yeah, the people. Here’s where we get into tipping-point territory that explains how things seemed to get so bad so quickly. There are more passengers now than there have been since the 1940s. The all-time ridership record was set in 1946, the year 2 billion passengers rode the subway. Ridership exceeded 1.7 billion last year, and broke records set in 1948. These days, overcrowding is the reason for about one-third of the system’s delays any given month, the MTA says. (Then there’s Penn Station, which is facing its own chronic failures after “decades of neglect,” as New York Governor Andrew Cuomo put it in May.)
“The other factor is there’s no political capital in doing preventative maintenance,” says Andrew Natsios, the former chairman of the Massachusetts Turnpike Authority who helped save Boston’s notorious Big Dig highway project. (He’s now the director of the Scowcroft Institute of International Affairs at Texas A&M University.) “The consequence is that systems deteriorate much faster. No one gets elected for voting for preventative maintenance. It’s very sad.”
Even when state officials have set aside the necessary money for upgrades, funds haven’t been managed well, and repairs are rarely made on schedule. Cost overruns are a big issue on any major infrastructure project, but especially when you’re dealing with an old urban area. At least New York officials now seem to be taking the subway’s many problems seriously. Lhota’s return to the authority and a recently announced modernization competition are reasons for cautious optimism. Then again, “there are always problems you don’t expect,” Natsios told me.
He gives an example from his time in Boston, when Big Dig construction disturbed the city’s many rats—and the rats disturbed the city’s many humans. “The rat population under Boston goes back to the Colonial period,” he said. “You suddenly had tens of thousands of rats on Boston streets. They had to spend tens of millions of dollars on rat extermination.”
At least in New York’s subways, the rats are already expected. But the rodents are a growing problem of their own. On Wednesday, the New York mayor, Bill de Blasio, declared war (again) on the city’s rat population, pledging $32 million to kill as many as possible. (“We want more rat corpses,” the mayor said.) That’s a paltry sum compared with the $20 billion estimate for a complete overhaul of the subway system. For now, the rats of New York have this in common with the city’s subway cars: They aren’t going anywhere.

* This article originally featured a caption that mischaracterized this illustration as an early sketch of the existing subway system. We regret the error.


Nothing like a presidential scandal to turn something utterly mundane into a national obsession. But here we are, once again, talking about emails.
The emails. Oh, the emails!
During the 2016 presidential campaign, email became a symbol of Hillary Clinton’s alleged corruption. Clinton’s aides wiped about 33,000 emails from a private server she used during her time as secretary of state, and Donald Trump made the deletions a cornerstone of his campaign against her.
“How can anyone vote for Hillary when she careless with emails that jeopardize our security,” he tweeted in September 2015. “She is not to be trusted.”
Then, in June 2016: “How can Hillary run the economy when she can't even send emails without putting entire nation at risk?” And also: “One of the reasons Hillary hid her emails was so the public wouldn't see how she got rich- selling out America.”
A year later, in July 2016: “If Russia or any other country or person has Hillary Clinton's 33,000 illegally deleted emails, perhaps they should share them with the FBI!”
That last one still raises an eyebrow. Especially today. What the American people didn’t know at the time, and what we know now, is that Trump’s son, Donald Trump Jr., had been doing some scandalous emailing of his own related to Russia.
On June 3, 2016, Trump Jr. received an email from Rob Goldstone, a Trump associate, former tabloid reporter, and entertainment publicist. Goldstone promised information that “would incriminate Hillary and her dealings with Russia and would be very useful to your father.” The New York Times on Tuesday first reported the contents of the email exchange, which Trump Jr. also released on Twitter. “This is obviously very high level and sensitive information but is part of Russia and its government’s support for Mr. Trump,” Goldstone had written. Trump Jr. replied: “If it’s what you say I love it especially later in the summer.” (You can read a transcript of the entire exchange here.)
The Triumph of Email
In an earlier era, this sort of exchange might have taken place over the phone, or via telegram, or, you know, face to face. So it makes sense that the scandal of the moment is tied to a ubiquitous form of communication. Especially when that form of communication is so easily forwarded, or copied, or hacked. Given that it was the utter carelessness that Donald Trump seemed to find most offensive about Clinton’s use of a private server, one can only imagine the conversation he’s having with his son today. This scandal isn’t really about email, after all, it’s about how Trump Jr. used it.
Clinton’s supporters have long cried, “but her emails!” as a way to minimize the Clinton email scandal and maximize Trump’s hypocrisies. Yes, Clinton should have known better than to use an outside server to conduct government business. But Trump Jr. is now learning a similarly rudimentary lesson. Email is not a secure way to communicate, but lots of people are sloppy with it anyway. The common-sense advice is this: Don’t write anything in an email that you wouldn’t want to see on the front page of the newspaper tomorrow. (This is a thought exercise that no longer requires imagination for Trump Jr.)
“Part of the reason Donald Trump Jr. is finding himself in hot water right now is he forwarded an entire email chain,” said Amy Webb, the writer and founder of the Future Today Institute. “I can’t state that enough. Email is penetrable. Unless you are using a lockdown system and encryption, anybody can access it.”
The fact that an email-centered scandal has boomeranged back to the Trump White House isn’t the only irony, Webb says, noting reports that the White House has gutted its Office of Science and Technology Policy. (The White House disputes this characterization.)
“These are some fundamental misunderstandings by the people who are now deciding technology policy,” she said, “in the absence of real scientists and technologists who know what they’re doing.”
For many people, it’s not at all practical to avoid email entirely. And it’s arguably lucky for the American people that those in positions of power are sometimes careless. A paper trail, even a digital one, can reveal important information that politicians would otherwise keep secret. But, as Trump Sr. once suggested, carelessness with something so simple as email is a red flag. A person who’d jeopardize security that way, he said, is “not to be trusted.”


The president was seething.
His problem was with the press, yes, but also with the technology they used. Electronic media had changed everything. People were glued to their screens. “I have never heard or seen such outrageous, vicious, distorted reporting,” he said in a news conference.
The age of television news, Richard Nixon told reporters gathered that day in October 1973, was shaking the confidence of the American people. He didn’t yet know his presidency would reach a calamitous end. When Nixon announced he would resign, in August 1974, he spoke directly into a television camera. The recording remains stunning half-a-century later—mostly because of the historic nature of the moment, but also because of the power of broadcast.
Even in an informational era transformed by the web, video is a gripping format. In the chaos of real-time news, especially, there’s an advantage to being able to see something with your own eyes.
Or, there used to be.

At a time when distrust in journalistic institutions is swelling, technology that further muddies the ability to discern what’s real is rapidly advancing. Convincing Photoshop-esque techniques for video have arrived, and the result is simultaneously terrifying and remarkable.
Computer scientists can now make realistic lip-synched videos—ostensibly putting anyone’s words into another person’s mouth.
The animated gif that you see above? That’s not actually Barack Obama speaking. It’s a synthesized video of Obama, made to appear as though he’s speaking words that were actually inputted from an audio file.
The clip comes from researchers at the University of Washington, who developed an algorithm to take audio of someone talking and turn that into a realistic video of someone speaking those words. In the video below, you can see a side-by-side comparison of the original audio—which came from actual Obama remarks—and the generated video.

Obama was a natural subject for this kind of experiment because there are so many readily available, high-quality video clips of him speaking. In order to make a photo-realistic mouth texture, researchers had to input many, many examples of Obama speaking—layering that data atop a more basic mouth shape. The researchers used what’s called a recurrent neural network to synthesize the mouth shape from the audio. (This kind of system, modeled on the human brain, can take in huge piles of data and find patterns. Recurrent neural networks are also used for facial recognition and speech recognition.) They trained their system using millions of existing video frames. Finally, they smoothed out the footage using compositing techniques applied to real footage of Obama’s head and torso.
The researchers wrote a paper about their technique, and they plan to present their findings at a computer graphics and interactive techniques conference next month.
“The idea is to use the technology for better communication between people,” says Ira Kemelmacher-Shlizerman, a co-author of the paper and an assistant professor in the department of computer science and engineering at the University of Washington. She thinks this technology could be useful for video conferencing—one could generate a realistic video from audio, even when a system’s bandwidth is too low to support video transmission, for example. Eventually, the technique could be used as a form of teleportation in virtual reality and augmented reality, making a convincing avatar of a person appear to be in the same room as a real person, across any distance in space and time.
What Makes Tom Hanks Look Like Tom Hanks
“We’re not learning just how to give a talking face to Siri, or to use Obama as your GPS navigation, but we’re learning how to capture human personas,” says Supasorn Suwajanakorn, a co-author of the paper. Not surprisingly, several major technology companies have taken notice: Samsung, Google, Facebook, and Intel all chipped in funding for this research. Their interest likely spans the realms of artificial intelligence, augmented reality, and robotics. “I hope we can study and transfer these human qualities to robots and make them more like a person,” Suwajanakorn told me.
Quite clearly, though, the technique could be used to deceive. People are already fooled by doctored photos, impostor accounts on social media, and other sorts of digital mimicry all the time.
Imagine the confusion that might surround a convincing video of the president being made to say something he never actually said. “I do worry,” Kemelmacher-Shlizerman acknowledged. But the good outweighs the bad, she insists. “I believe it’s a breakthrough.”
There are ways for experts to determine whether a video has been faked using this technique. Since researchers still rely on legitimate footage to produce portions of a lip-synched video, like the speaker’s head, it’s possible to identify the original video that was used to create the made-up one.
“So, by creating a database of internet videos, we can detect fake videos by searching through the database and see whether there exists a video with the same head and background,” Suwajanakorn told me. “Another artifact that can be an indication is the blurry mouth [and] teeth region. This may be not noticeable by human eyes, but a program that compares the blurriness of the mouth region to the rest of the video can easily be developed and will work quite reliably.”
It also helps if you have two or more recordings of a person from different views, Suwajanakorn said. That’s much harder to fake. These are useful safeguards, but the technology will still pose challenges as people realize its potential. Not everyone will know how to seek out the databases and programs that allow for careful vetting—or even think to question a realistic-looking video in the first place. And those who share misinformation unintentionally will likely exacerbate the increasing distrust in experts who can help make sense of things
“My thought is that people will not believe videos, just like how we do not believe photos once we’re aware that tools like Photoshop exist,” Suwajanakorn told me. “This could be both good and bad, and we have to move on to a more reliable source of evidence.”
But what does reliability mean when you cannot believe your own eyes? With enough convincing distortions to reality, it becomes very difficult to know what’s real.


A consortium of newspaper publishers are preparing to take the unusual step of begging the nation’s legislature for the right to collectively negotiate with Facebook and Google.
(Generally, antitrust laws forbid this kind of collective bargaining because it reduces economic competition, except in specifically legislated cases such as labor unions.)
It’s easy to see why publishers want to team up and bargain as one: Facebook and Google continue to take what they want from the news media—content to churn through the News Feed and ever-more-recent search results—while soaking up all the advertising profits available. This is a game where the distributor keeps almost the whole pie, and only so many news organizations can survive on the scraps the Internet platforms allow to fall to the floor.
So, how’d we get here?
We can radically boil it down to three movements.
1. Starting at the beginning: Ask most newspaper people and they’d tell you the “original sin” was that news gatherers decided to give away their product for free when they went online. This is a ridiculous frame (for reasons I’ll get into) but this would be most people in the news business’ common understanding.
It was the ethos of the time that stories should flow around the Internet, unburdened by payment systems. The key underpinning of the whole web was the link and anybody had to be able to view a link for it to be of maximum value. That was just an axiom of the early web.
So, (most) newspapers went along. Foreshadowing. But everything had changed: They were in an entirely different business and didn’t realize it.
2. Google took advantage of all the high-quality pages, and the links between them, being freely placed on the World Wide Web. They took all those links and turned them into the data that powered their search engine.
Google became the best place to find out about the world. One could know what was going on without a newspaper or a TV or a radio.
As Google gained users, newspapers gained greater reach and readership than they’d had before. Sure, most of them were visitors from outside the metro areas that their advertisers wanted to reach, but the business would fill itself in, everyone hoped.
Google, meanwhile, was figuring out its own business model, which turned out to be sponsored links running next to search results. They created an easy way to buy advertising and a backend that could tell you precisely how well ads were performing. (Other people were doing similar things, but only one of them has become one of the most valuable companies in the world.)
Turns out that many small, local advertisers preferred the Google way of doing ads to the newspaper way of doing ads. Combine that with the hit from Craigslist taking mainstay classified-ad business, and newspaper print advertising was in trouble.
That would have happened with or without the so-called “original sin.” The disruption came to the papers: their local/regional business practices got stomped by the efficiency of the technology business.
In the new advertising world, a reader wasn’t worth very much. Maybe a few cents a visit. But there was always a hope: more scale. More visitors meant more pageviews, which meant more ads, which meant maybe the news organizations could pay their journalists with enough pennies from web ads.
Thus, people are always searching out scale, traffic, visitors, pageviews, ad inventory.
3. And who has the scale? Facebook.
For a long time, Facebook was happy to send readers to news articles, but it was incidental to their larger concerns. And then came, roughly, August 2013, when Facebook turned on the taps for news organizations. Charlie Warzel noted it in Buzzfeed at the time: “traffic from Facebook referrals to partner network sites are up 69% from August to October of this year.” But he was just spelling out what many in digital media knew from their own traffic reports. Traffic was through the roof and it was all coming from Facebook.
“Facebook has sent unprecedented levels of traffic to publishers across the internet in recent months, a dramatic and unexpected increase affecting a large range of sites serving a wide variety of content,” Warzel wrote.
Facebook enacted this monster change in the information flow of the world in two ways. One, it seemed to start privileging news content from the best, say, quartile of publishers. Stories from these publishers were more likely to show up in your newsfeed relative to stories from other publishers (or people).
More significantly, for the long-run, Facebook also began—essentially—running house ads for different media brands. For several months, they suggested that users should follow media brand pages. This was basically an in-kind gift from Facebook to a variety of publishers. And it gave them much larger pages from which to promote their stories.
The tactics worked. Facebook began to generate huge scale for publishers. Now many sites have “audiences” of 15-20-25-30 million unique visitors, many times the size of even the largest print publications. (USA Today’s average circulation is down substantially to something like 3 million.)
But that also put publishers at the mercy of Facebook’s News Feed algorithm and corporate practices.
In the midst of the Facebook traffic surge, David Carr wrote a column about Facebook’s new dominance. “The traffic they send is astounding and it’s been great that they have made an effort to reach out and boost quality content,” a publishing exec told him.  “But all any of us are talking about is when the other shoe might drop.”
The other shoe, we presumed at the time, was that Facebook would close the taps for one reason or another. And they partially did when they began emphasizing video, which is one reason so many media companies are “pivoting to video.” They’re merely running where Facebook’s corporate finger is pointing.
But worse, the scale that Facebook rapidly induced also caused inflation of a sort. The currency of digital advertising was traffic and suddenly there was a lot more of it. So what do you think happened? All that scale put downward pressure on how much advertisers were willing to pay publishers. Which meant that you needed Facebook’s scale just to maintain your business.
Facebook’s control of the attention of 2 billion people and the media has done a lot more strange things to “the news.” Just a couple examples: Everything’s spikier now—meaning fewer and fewer stories drive more and more traffic. More publications are clustering around fewer themes and news events because most of the available Facebook traffic will come around a single news item, whether it’s a John Oliver rant or a Trump associate’s meeting with someone or a doctor dragged off a plane or covfefe.
These are some of the complex effects that what I’ve long called Facebookworld has brought to journalism. And so I can see why newspapers would like to sit down with Facebook to tell them about their problems. Tech has wound its way around journalism like a vine around an old tree.
We’ve just seen the consequences of the strange information ecosystem Facebook and Google have created. In the crucial months leading up to the election, hoaxes, lies, and fakery proliferated right alongside (or on top of) serious coverage from serious reporters. Since the election, a revitalized press has taken to covering the many problems and scandals of the Trump administration with intensity.
In other words, in the last year, we’ve been shown the value of a trained press corps supported by journalistic institutions that know what they’re doing.
And at the same time, papers and other journalistic outfits continue to struggle with creating digital businesses that can support enough journalists to make a difference.
The press has never been more needed nor more endangered.
The newspapers probably won’t get the help they need from Congress. And even if they do, they are in a weak position vis-a-vis the two most powerful information gatekeepers the world has ever known.
The good news is that Facebook and Google, both the individuals inside of them and the corporate structures, may finally want to help the business of journalism. The bad news is that, at this point, these two companies may not be able to fix what they’ve broken.


Updated: Sunday, July 9 at 9:09 p.m. ET
Nothing connected to the internet is safe from hackers. And I mean nothing.
Modern cybersecurity is a constant cycle of breaches and patches. Systems are compromised, security experts play catch up, and eventually hackers find a new way in. Each side tries to outwit the other. But at any given moment, one of them is always a step ahead.
President Donald Trump doesn’t seem to understand that. “Putin & I discussed forming an impenetrable Cyber Security unit so that election hacking, & many other negative things, will be guarded,” he tweeted on Sunday.
Yes, Russia. Yes, really.
Setting aside the question of what “many other negative things” Trump and Putin plan to guard, and how; and setting aside the absurdity of the idea that the United States would partner with Russia, of all countries, on a cybersecurity initiative, there is a basic question to answer: Is “impenetrable cybersecurity” even possible?
No, it is not. (Trump later acknowledged as much, but more on that in a minute.)
“Anything connected to the internet is by definition vulnerable,” says Robert Cattanach,   a partner with the law firm Dorsey and Whitney who specializes in cybersecurity. “The clients I work for who are serious about protecting their ‘crown jewels’ keep that information in an isolated, unconnected server, locked in a limited-access room with no connectivity to the outsider world.”
“Protection only can go so far,” he added. “After that you’re relying on detection processes and response protocols.”
Trump’s Incoherent Ideas About ‘the Cyber’
Give hackers enough time and money to break into a system, and they’ll often find a way to do it.  And the thing is, it’s not always easy—or even possible—to detect a breach. That’s especially the case when you get to the highest levels of hacking, with state actors fighting against each other across complex networked systems.
“If you’re at war with a foreign power and they drop a bomb on you and that’s why your power doesn’t work, you know who did it,” John Kelly, the founder of network analytics firm Graphika told me in a conversation before the Putin-Trump meeting. “One of the problems with cyber attacks is you may not know who did it. And even if your intelligence services know, you can’t prove it to the world.”
No wonder, then, that Putin is telling the United States to prove it; and asking for evidence of Russia’s alleged interference in the 2016 presidential election, according to the Secretary of State, Rex Tillerson. When you couple actual hacking with what Kelly calls “hacking of the mind,” like attempts to sway public opinion, things get thornier still. Several American intelligence agencies confirmed in a Director of National Intelligence report earlier this year that they have “high confidence” that Putin ordered an influence campaign in 2016 aimed at undermining “public faith in the US democratic process.” Social publishers like Facebook have suggested they’ve detected similar activity across their networks.
Yet without hard evidence, and at a time when Americans’ faith in democratic institutions is nosediving, Russia can deny, deny, deny. Apparently that’s enough for Trump, who says now that Putin has “vehemently denied” meddling in the election, he wants to “move forward in working constructively with Russia!”
“If I had to choose, I’d let the power grid fail, and let our democracy be strong,” Kelly says. “If you completely erode the underpinnings of democratic society, everything else can be working, but you’re still broken. That’s a lot harder to fix.”
[Update:] On Sunday night, about 30 minutes after this article was originally published, Trump seemed to walk back his earlier comments on cybersecurity. “The fact that President Putin and I discussed a Cyber Security unit doesn’t mean I think it can happen,” he tweeted. “It can’t.”


Two dozen mountains drape themselves diagonally across the middle of Acadia National Park on Mount Desert Island in Maine. The peaks start near Bar Harbor in the northeast, tumble down across Jordan Pond, Somes Sound, and Echo Lake, and end with Mansell and Bernard Mountains in the southwest. Every one of these mountains, like the state of Maine, all of New England, and much of the continent of North America, has been scoured by glaciers during multiple ice ages of the past. The most recent glacier began its retreat about 17,000 years ago.
In the mid-19th century, the Swiss-born geologist Louis Agassiz visited Maine on a vacation from Harvard University. The glacier-scoured bedrock formations on Mount Desert Island gave him plenty of evidence to support his theory of the Ice Age: that enormous glaciers, rather than a flood, had once covered much of Europe and North America. A few years after his visit, in 1867, Agassiz published two essays on “Glacial Phenomena in Maine” in The Atlantic (part I, part II). He used a French term to name what he saw, calling the glacially shaped formations roches moutonnées. The meaning of this term has caused confusion in geology ever since.
* * *
To this day, many geologists follow Agassiz in considering all the rounded peaks and outcrops on Mount Desert Island to be roches moutonnées. Other geologists, including Duane and Ruth Braun, the co-authors of Guide to the Geology of Mount Desert Island and Acadia National Park, reserve this curious French term for the most distinctly asymmetrical of the mountains, ones with a long gradual slope along the northern “up-ice” side (the direction from which the glacier advanced) and cliffs on the lee or “down-ice” side. A mountain known as the Beehive, whose steep, plucked, south-facing cliffs terrify hikers with vertigo, illustrates the accepted American definition of roche moutonnée.
Indeed, the “Glossary of Glacial Terms” published in 2013 by the U.S. Geological Survey (USGS), defines roche moutonnée in just this way, as “an elongated, rounded, asymmetrical, bedrock knob produced by glacier erosion. It has a gentle slope on its up-glacier side and a steep to vertical face on the down-glacier side.” Interestingly, the USGS definition of the landform makes no attempt to translate the French roche moutonnée—and perhaps for good reason. It’s tricky, it turns out, to turn the expression into sensible English.
Roche means “rock.” Moutonnée is an antiquated word. Derived from the French word for sheep, mouton, it has no exact English equivalent. Some dictionaries have no listing for it; others say that the adjective means “sheep-like” or “fluffy.” In the 17th and 18th centuries, the word was used to describe hair that was curled and styled to resemble lamb’s wool.
As a geological term, roche moutonnée has been used for more than two centuries in a widespread, if not consistent way. The translation often given for the expression on websites and in textbooks is “sheepback,” a reasonable compromise. In a beautiful mid-19th century painting by Rosa Bonheur, for example, it’s possible to see asymmetrical “mountains” in the contours of her sheep’s hindquarters.
Stephen Marshak, an author of popular college geology textbooks, notes the resemblance to a sheep’s hindquarters in his definition of roche moutonnée. When one looks at the “granite and metamorphic rock hills of Acadia National Park,” he explains in Earth: Portrait of a Planet, “the hill’s profile resembles that of a sheep lying in a meadow.” Similarly, Julia A. Jackson’s comprehensive Glossary of Geology gives as synonyms for roche moutonnée both “sheepbackrock” and “sheeprock,” suggesting that if in doubt about a glacial landform, one should simply imagine a sheep’s contour.
The similarity between glacially scoured rocks and sheep is even more intriguing if one interprets the expression roches moutonnées more broadly, as did the 19th-century French geologist Albert de Lapparent. In his book Les Anciens Glaciers (“Ancient Glaciers”), Lapparent explained the origin of the term this way:
All the unevenness of the rock appears rounded on the upstream side, and [. . .] when one turns one’s eyes downstream, one has the impression of a cluster of sheep backs, whence comes the name “roches moutonnées,” used to designate these landforms so very characteristic of glacial action.
For Lapparent in 1893, the expression makes sense in the plural: A cluster of glacially sculpted, asymmetrical rocks look like a “suite” of sheep, whose backs are similarly curved and plunging. Lapparent assumes that the term came into being because of this resemblance, and his authority on the matter influenced scientists throughout the following century.
Some geology websites, however, understand a different sort of similarity between sheep and rocks. They translate the French expression not as “sheepback” but as “fleecy rock,” shifting attention from contour to texture. The rounded “backs” of glacially polished rock hardly look fleecy or “frizzy” (another translation). “Moutonnée” does mean something closer to “fleecy” than it does to “sheepback”—but one has to strain to see these sculpted landforms as woolly. They look smooth and slick instead.
* * *
It takes a bit more detective work to figure out what is going on. Many geology textbooks and glossaries, including the current entry in Wikipedia, note that the term roche moutonnée originated with Horace-Bénédict de Saussure, an 18th-century Swiss geologist who traveled extensively through the Alps. He made his observations about 50 years before the advent of modern glacial theory, but he noticed a pattern that would become important for future glaciologists. According to Saussure, in some spots in the Swiss Alps, one could find smoothed rocks with a peculiar appearance: They looked to him like wigs. The “rounded, contiguous, repeated rocks (rochers) resemble,” wrote Saussure in 1786, “a thick mane of hair, or the wigs we also call moutonnées.”
It’s not entirely clear which wigs Saussure has in mind, as it’s hard to find a wig actually called a “moutonnée.” One mid-18th century hairstyle worn by French women was called the tête de mouton; it involved a smooth back and rows of curls in the front, in what one might imagine to be the same sort of asymmetrical arrangement as the roche moutonnée.
Most geologists, though, have assumed that Saussure had in mind a more dramatic mane of hair. They assert that he was referring to wigs shaped and attached with mutton grease pomade. In that case, the slick, hairy waviness explains the name’s origin—not a resemblance to the contour of a sheep’s back. This second theory is the one that the British geologists Douglas Benn and David Evans promote in their 1998 study Glaciers and Glaciations. Saussure named these landforms, the authors explain, “based on a fancied resemblance to the wavy wigs of [the late-18th century], which were called moutonnées after the mutton fat used to hold them in place.” Authoritative indeed, though Benn and Evans give no source.
Since its first coinage in 1786, Saussure’s name for glacially sculpted rocks and mountains has taken on a life of its own. In the 19th century, when the modern theory of glaciers was introduced, glaciologists plucked two terms—rochers and moutonnées—from Saussure’s famous studies of the Alps and brought them together in the expression roches moutonnées, losing sight of the essence (and essential wit) of the analogy he had used. Saussure’s happy observation that certain landforms looked like big hair or mutton wigs was put aside; wigs had gone out of fashion by the 19th century, and it must have seemed obvious to geologists that Saussure meant for us to see the contour of ovine bodies in roches moutonnées, not their fleece. At one point in the late 19th century, a scientist named Grenville Cole reread the original passage in Saussure and, alarmed, publicized the mistaken translation. But his attempt to reinstate the wig analogy was only partly successful, and through the decades the two not quite compatible explanations have coexisted in strange and wonderful ways.
Even now, geology websites make heroic attempts to reconcile the wig and the sheep-hindquarters theories. A Candian earth sciences website gives my favorite example. Describing Old Fort Point hill in Alberta, the website tells us that Saussure, who was “the first to clearly define the term geology,” also coined roches moutonnées to name landforms that “reminded him of the mutton wigs worn by the French aristocracy.” But then it continues:  
Smooth and rounded on the back, combed upward in the front, such a wig looked rather like a sheep lying down. Which is pretty much the shape of any bedrock hill carved by moving glacial ice. Old Fort Point is a classic roche moutonnée.
As a crowning touch, the website displays an image of a fluffy lamb’s wool wig, whose resemblance to a resting sheep and to the asymmetrical Old Fort Point hill pictured next to it can be seen—with some effort, anyway. Recognizing the humor in the fanciful, multi-layered analogy, the site goes on to note that “a sheep-back hill” is also the home of bighorn sheep. Thus, the two readings of Saussure are wedded together, making a reader unlikely to forget this piece of geological lore.
* * *
The Old Fort Point website has it mostly right: The glacially sculpted boulders and hills known today as roches moutonnées look like 18th-century wigs, which apparently got their name from mutton fat, and which also happen to look like the backs of sheep whose fat was used to slick them down. But there’s one last twist. The rounded up-ice surface of a roche moutonnée has on its back parallel scratched lines running roughly north-south, in the direction of the advancing ice sheet. Rocks lodged under the heavy ice scored the bedrock underneath, and from a distance, these parallel scratches (called striations) could be said to resemble combed hair.
The explanation of the roche moutonnée is surely more complicated than Saussure meant it to be. It has adapted itself nicely, though, to the needs of geologists who try to make sense of the formations they study. Saussure himself, I like to think, would have admired the bald peaks of Mount Desert Island, whose name, too, comes from the French. In 1607 the explorer Samuel de Champlain noted the place on his map of the Maine coast, calling it l’île des monts déserts, the island of bare mountains. Had Saussure been with him on that voyage, the mountains that looked so stark might have sprung to life—as a suite of grazing sheep or a well-oiled wig.
This article appears courtesy of Object Lessons.


Yet another announcement came yesterday: Volvo, the Swedish slash Chinese car company, announced that it will only offer electric or hybrid vehicles by 2019. It was widely hailed as a bold move.
Previously, the company had committed to selling 1 million hybrid and fully electric vehicles by 2025.
Right now, hybrids have 2 percent of the passenger car market in the U.S., and completely electric vehicles are still a rarity. Even in EV-friendly California, battery-powered cars only hold 2.7 percent of the market.
But in a car industry roiled by self-driving vehicles and self-promoting Tesla, which is now valued as highly as General Motors and far more highly than Ford, automakers have to sell more than cars to be seen as exciting by Wall Street. They’ve got to be technology companies, not manufacturers. And that means developing autonomous systems, rethinking the motive power under the hood, and figuring out the art of bold pronouncements.
So, I compiled all the grand promises that the world’s traditional carmakers have made in the past two years or so, and one thing is clear: Either the automotive world is going to undergo a radical transformation around 2020, or these companies have seriously erred in their planning.
Volkswagen
Volkswagen corporate is engaged in a major initiative they’ve dubbed “Together-Strategy 2025,” which ties together the electrification and smartening of cars. As part of that, they’ve promised to “bring highly automated driving functions to market as a core competency from 2021.” Recently, they introduced an on-demand self-driving car-like thing, which sort of looks like a character in Thomas the Tank Engine: Future Edition. They call it Sedric.
Audi, which is a part of the Volkswagen Group, has been more aggressive. They announced they’re rolling out “Level 3” automation—which means a car that can drive itself some of the time—in the A8 model this year with promises to bring fully autonomous vehicles to market in 2020. Audi has also promised that 30 percent of sales will be partially or fully electrified vehicles by 2025.
On the electric side, the company has promised a sporty little electric vehicle called the I.D. by 2020. VW had bet heavily on what it termed “clean diesel” technology as its powertrain of the future. But as last year's major scandal revealed, their engines weren't that clean under normal road conditions. Instead, the company's engineers had built them to run artificially well under testing conditions (and only under testing conditions). That's left the company with a reputation to repair and a newfound interest in the benefits of electric vehicles.
General Motors
GM’s big self-driving play so far has been its Super Cruise system for semi-autonomous highway driving. After a delay, Super Cruise will debut this fall on the Cadillac CT6. Drivers will only be allowed to switch on the autopilot when they’re on the highway, and the company has built sensors that track drivers' head movements and prod them to keep focused on the road, even when the computer is driving.
Of course, the company has plans to deliver fully self-driving cars in the early 2020s. They acquired self-driving startup Cruise Automation (for $1 billion) and have plans to hire 1,100 more technology-focused workers in California, centered around San Francisco.
GM has already delivered on a major promise in electrification with the introduction of the Chevy Bolt EV earlier this year. It’s rated at 238 miles per charge — and costs about $36,000 before tax incentives, which in California could bring the price down to $25,000.
The company has also promised that it will have 10 electric models in the Chinese market by 2020, and expects sales of 500,000 EVs there by 2025.
Hyundai-Kia
Hyundai showed off an electric, autonomous concept car at CES in 2017 called the Ioniq, but it has been circumspect about its plans.
But the Hyundai-Kia automotive group’s parts supplier Hyundai-Mobis has already laid out its roadmap for the next decade. The company will offer some components for autonomous systems by 2020, with mass production coming in 2022. They described themselves as in “fast follower” rather than “leader” mode. And they’re trying to make self-driving work without the laser systems that add to the price and complexity of systems like Waymo’s.
Ford
In August 2016, Ford promised a fully self-driving car—sans steering wheel—by 2021. The company reaffirmed that commitment this spring. Ford’s plan is to have self-driving cars available in some sort of ride-sharing fleet.  The company is also aggressively investing in and snatching up startup companies working on the foundational technologies for self-driving cars like sensors and maps.
As for electrification, Ford has promised it will add an EV SUV with a range of 300 miles by 2020.
Nissan-Renault
Last month, Nissan and Renault announced a partnership to create a self-driving car ride-sharing service. The companies said the service wouldn’t launch by 2020, but “certainly within 10 years,” or by 2027.
Nissan CEO Carlos Ghosn had made several big promises about self-driving cars. Back in 2013, he said that the company would have autonomous vehicles for sale at reasonable prices by 2020. He reaffirmed that in 2015 (with some caveats). And in 2016, the company said it would have 10 car models with some autonomy baked in by 2020.
The 2018 Nissan Leaf will have enough self-driving features to allow for autonomous single-lane highway driving.
Electrification-wise, Nissan has made the Leaf the second-best-selling electric vehicle (after Tesla, of course).
Daimler
Like many other car companies, Daimler—the maker of Mercedes—sees an opportunity in moving their business toward ride-hailing with self-driving cars at the core of the operation. Daimler cut a deal with Uber in January to partner on self-driving cars. And in April, they announced another one with Bosch to deliver autonomous taxis to the road in 2021.
Also in April, the company announced that it was bringing up its electric vehicle rollout date from 2025 to 2022. That’s when Daimler expects to have 10 EVs in its lineup.
BMW
BMW has been something of a frontrunner in autonomous vehicle development. In 2016, the company committed to introducing a fully autonomous vehicle by 2021. In May of this year, BMW struck a deal with Intel and major automotive supplier Delphi to create an alliance to meet that goal, possibly with other car companies who might want to join in.
The company expects to sell 100,000 electric vehicles in 2017.
Toyota
Toyota, the world’s largest automaker, has played it cool for the most part. In 2013, the company emphasized that humans must remain in control of vehicles. But in October 2015, Toyota promised that vehicles “with an autopilot switch” would be on the roads of Tokyo by 2020. (They’re working with chipmaker Nvidia.)
The company has had mainstream success with hybrid vehicles, but has not gone all-in on electrification like some of its competitors. In part, that’s because Toyota continues to push fuel-cell vehicles.
Honda
Honda has promised to have fully autonomous cars on the road by 2025. Along the way, the company plans to have lower level self-driving features such as supercharged cruise control in cars by 2020. That’s several years behind its competitors. But hey: It’s no sure thing that the first-generation of autonomous vehicles are going to be a success.
Mazda
Mazda might be the only true contrarian among the traditional automakers: They still want to make cars that you buy and drive. They are only interested in developing driver-assistance technology, their CEO said in April.
Subaru
Japan’s smallest carmaker does not currently have a promise around self-driving cars — and is planning its first plug-in hybrid electric car next year.
Tata
Tata Motors, India’s largest carmaker, has asked Indian authorities to test an autonomous vehicle, but many people are skeptical that self-driving cars will work well on India’s chaotic roads.
SAIC
We don’t know much about China’s largest car maker’s self-driving car plans, aside from a partnership with U.S. sensor maker Savari. The company maintains a research center in Silicon Valley and recently received a permit to test its vehicles on California roads.


Volvo, the Chinese-owned Swedish automaker, announced Tuesday that starting in 2019 it will only make fully electric or hybrid cars.
“This announcement marks the end of the solely combustion engine-powered car,” Håkan Samuelsson, Volvo’s president and chief executive, said in a statement.
The move is a significant bet by the carmaker that the age of the internal-combustion engine is quickly coming to an end—though the Gothenburg, Sweden-based automaker is lone among the world’s major automakers to move so aggressively into electric or hybrid cars. Volvo sold around half a million cars last year, significantly less than the world’s largest car companies such as Toyota, Volkswagen, and GM, but far greater than the 76,000 sold by Tesla, the all-electric carmaker.
Volvo said Wednesday that starting in two years, its cars will be electric, plug-in hybrids or mild hybrids. Between 2019 and 2021, Volvo will launch three fully electric cars and two high-performance electric cars from Polestar, which will be spun off into a separate premium car company. Volvo said these five cars “will be supplemented by a range of petrol and diesel plug in hybrid and mild hybrid 48 volt options on all models.”
“This means that there will in future be no Volvo cars without an electric motor, as pure ICE cars are gradually phased out and replaced by ICE cars that are enhanced with electrified options,” the company’s statement said.
Ever since it was bought in 2010 by Geely, the Chinese automaker, from Ford, Volvo has invested heavily in a range of new vehicles, and seen a steady growth in its share of the market. The New York Times adds:
But by focusing on electrification, Volvo can concentrate its limited research and development resources on new technologies rather than continuing to invest in fuel-powered motors that may become obsolete. ...
Volvo … will also be able to draw on technology developed by its parent company. Geely sells electric cars in China, one of the fastest growing markets for battery-powered vehicles.
    
Samuelsson, the Volvo executive, said that Wednesday’s announcement is in line with the company’s plan to have sold 1 million electrified cars by 2025.
“When we said it we meant it,” he said. “This is how we are going to do it.”


The rules are simple, okay? No threats of violence. No targeted abuse or harassment. No inciting anybody else to engage in targeted abuse or harassment. No hateful conduct.
Now think about Donald Trump’s tweeting habits. Is he breaking those rules, which come from Twitter’s terms of service?
Trump has long been criticized for his impulsiveness, but less than six months into his presidency, alarm over his Twitter conduct has hit fever pitch.
On Sunday morning, Trump tweeted a short video clip showing him pummeling another person outside of a wrestling ring—with the other person’s face blocked out by the CNN logo. If that’s not a direct threat of violence against the American citizens who work for CNN, it’s certainly a celebration of violence.
#FraudNewsCNN #FNN pic.twitter.com/WYUnHjjUjg
The president is not only aware of the firestorm he’s ignited, he appears to be relishing it. “My use of social media is not Presidential,” Trump tweeted on Saturday. “it’s MODERN DAY PRESIDENTIAL.”
These latest messages came came on the heels of a bizarre barrage of tweets—odd even by the president’s standards—that set off a new round of scrutiny of his use of social media. Beginning on June 29, Trump began tweeting repeated insults at Joe Scarborough and Mika Brzezinski, the hosts of the MSNBC talk show, Morning Joe. Trump’s treatment of Brzezinski was particularly strange. In addition to calling her “dumb,” “crazy,” and “low I.Q.” in three separate tweets, he claimed that she and Scarborough traveled to Mar-a-Lago for New Year’s Eve and insisted on seeing Trump while Brzezinski was “bleeding badly from a face-lift.” (Brzezinski and Scarborough published a rebuke in The Washington Post, calling the president’s claim “a lie.”)
In true Trump fashion, the president doubled down, calling Scarborough “crazy” and Brzezinski “dumb as a rock.”
Does that constitute targeted harassment? And given Trump’s huge following—more than 33.1 million Twitter followers on his primary account—does a string of attacks against the same two individuals constitute inciting harassment? “We don’t comment on individual accounts, for privacy and security reasons,” a Twitter spokesperson told me on Saturday. Twitter also declined to tell me whether, when considering the question of a user inciting harassment, it takes into consideration that person’s number of followers or public status—a movie star or politician, for example.
Twitter’s website does offer some clarification on how it assesses abusive behavior. The company says it assesses whether the primary purpose of an account is to harass or send abusive messages; and it looks at whether the reported behavior is “one-sided.”Setting aside Twitter’s notoriously bad track record for actually enforcing its own standards on harassment, the question of one-sidedness poses an interesting problem here.
When one of the people involved in a Twitter fight isn’t just a public official but also the president of the United States, is it fair to consider anyone he’s attacking an equal player in a fight?
We know what Trump would say. This is a man whose 2007 book Never Give Up has multiple chapters dedicated to the subject of fighting with people. There’s Chapter 5 (“I Love a Good Fight”) and Chapter 29 (“You Will Be Attacked For Trying to Change Anything”) and Chapter 38 (“When You’re Attacked, Bite Back”). If Trump doesn’t like what a person says about him, he attacks them. Period.
But Trump’s Twitter conduct also raises a question about what Twitter is, and what it should be. Often, the service is treated as a new kind of public square, a place for the unfiltered exchange of ideas (and, clearly, hurling of insults). Silicon Valley has rarely stepped in to correct the persistent cultural conflation between the actual right to free speech—that is, the constitutionally protected right that says the government cannot make a law that inhibits people’s freedom of expression—and the idea that people should get to say whatever they want wherever they want to without consequence. (Complicating things further, Twitter must answer to its shareholders, and having the president use its service so routinely—and so bombastically—certainly keeps the service relevant.)
In reality, though, Twitter is a media company. Just like CNN and The New York Times are media companies. Except, unlike in a traditional model where publishers and readers are distinct groups, everyone can be both on Twitter. So what’s a company like Twitter to do when one of its users—who is also the president of the United States, by the way—incessantly publishes attacks against individuals? Nothing, apparently. At least nothing yet. The thornier question is: What should it do? Only rarely would any news organization turn down the opportunity to exclusively print or broadcast a message from the president. (U.S. senators and presidential candidates, however, are another story.) Though it’s not like the president doesn’t have plenty of opportunities for his voice to be amplified. He has said he likes Twitter because it’s a direct channel to the American people, but he has his own website where he could be live-streaming or blogging, for instance. He is also a constant subject of media attention; his press conferences—when the White House permits it—are broadcast over cable and network television.
Presidents have historically made good use of new media platforms. Franklin Roosevelt’s fireside chats may seem quaint to us now, but they were a revolutionary experiment with a nascent media platform when they began in the 1930s. But, as with all things Trump-related, looking to norms and historic conventions can only get you so far. Imagine if Roosevelt had used his radio access to relentlessly criticize individual Americans by name. Trump knows that his critics are disgusted by the way he represents the country on Twitter, and he trusts that his supporters delight in their disgust.
“It never stops, and I wouldn’t have it any other way,” he wrote in The Art of the Deal. “I try to learn from the past, but I plan for the future by focusing exclusively on the present. That’s where the fun is. And if it can’t be fun, what’s the point?”
If Twitter were to suspend or even outright ban Trump, his most fervent left-wing critics would surely rejoice. His supporters would likely boycott Twitter. Their outrage could help him keep their support. And in Trump’s worldview, this may well look like a win-win.


The Golden Record was never meant for this planet. Yet it has remained an object of curiosity on Earth, even after decades of hurtling through the void of outer space.
In fact, the Golden Record has had something of a revival lately. For years, there’s been talk of making a modern, internet-crowdsourced follow-up to the original 1977 version. The original record plays a prominent role in the new young-adult novel, See You in the Cosmos, by Jack Cheng. And a recent Kickstarter campaign to reissue the record on vinyl raised nearly $1.4 million, seven times more than its fundraising goal. Last fall, around the time that Kickstarter campaign launched, I found myself revisiting the record’s tracks.
In doing so, I stumbled upon a mystery.
* * *
In the late summer of 1977, NASA launched twin spacecrafts—Voyager 1 and Voyager 2—as part of a mission to better understand Jupiter, Saturn, and the outer solar system. As a bonus: Each probe carried a gold-plated copper phonograph that contained sounds and images from Earth. The idea was to send something into the universe that demonstrated humanity’s wish to join a “community of galactic civilizations,” as President Jimmy Carter put it at the time, and to express good will to intelligent life elsewhere. It was also meant as a cosmic postcard, of sorts, a way of sharing the experience of living on Earth with intelligent life elsewhere.
The record, curated by a team led by the astrophysicist Carl Sagan, featured the music of Beethoven, Chuck Berry, Kesarbai Kerkar, and Blind Willie Johnson, and various folk music from around the world. Images, placed electronically on the phonograph, included photographs of a mother nursing her baby; a woman with a microscope; an astronaut in space; highway traffic in Ithaca, New York; the pages of an open book; a violin with sheet music; men laying bricks to build a house in Africa; a woman eating grapes at a supermarket; and a number of diagrams and illustrations of concepts like continental drift and vertebrate evolution. There were also audio clips depicting scenes of life on Earth—the sounds of rushing wind and the roar of ocean tides, whale songs, elephants trumpeting, human footsteps and human laughter.
It occurred to me last fall that I’d never actually heard the laughter track—and that I wanted to. What sort of laugh did the record’s producers select as a depiction of our species? And whose laugh was it? It could have been a chuckle, a snort, a guffaw, a snicker. It could have been anything from the irresistible staccato of a baby giggling to a the deep-throated mwahaha of a Hollywood villain. But my idle curiosity led me to more and more questions, and those questions turned into a months-long investigation into the origins of the Golden Record.
The contents of the record have been debated for decades—among its creators, certainly, but also among Golden Record enthusiasts. The project posed an impossible cultural and technological challenge to begin with: How do you carve an entire planet into the grooves of a single record—a record that’s also lightweight enough to piggyback  onto a 1970s space probe—all while capturing the richness of the human experience? Oh, and by the way, please make the whole thing potentially comprehensible to the alien civilization that might discover it 100 million years from now.
The team that conceived of the project first envisioned it as a record to be played at the conventional 33 and one-third revolutions per minute, with music on one side and non-musical information like photographs on the other. Eventually, they settled on 16 and two-thirds revolutions per minute—which meant a slightly lower quality sound, but not terribly so. The slower speed also meant they’d have 90 minutes available for music rather than the original 27 minutes.
The 12-minute audio essay that included the sounds of waves, elephants, and laughter was meant to capture the auditory experience of life on our planet. Those sounds were organized as a montage echoing the evolution of life on Earth—beginning with a “giddy whirl of tones reflecting the motions of the Sun’s planets in their orbits,” as the record’s creative director, Ann Druyan, put it in her 1978 essay about the project. Those ethereal notes gave way to sounds of an earthquake, thunder, mud pots, wind and rain, crickets and frogs, hyenas, birds, chimpanzees, and eventually humans. The first appearance of a human in this montage is the sound of footsteps, then laughter. That laugh is the first human utterance in this representation of the evolution of our species. This is fitting.
“Laughter is ancient,” says Robert Provine, the author of Laughter: A Scientific Investigation. “Laughing, like crying, is a human instinct. It’s not under conscious control. Whereas crying is a solicitation of caregiving, laughter is the signal of play. It is the sound of play, literally.”
Laughter is one of humanity’s most joyful peculiarities. Infants typically laugh long before they can speak. Laughter transcends differences in language entirely, yet remains a deeply important element of cultural and social interaction. Humans aren’t the only creatures that laugh—a chimpanzee’s laugh sounds like a dog panting; a rat laughs in ultrasonic chirps—but the rhythm and cadence of human laughter is unique to us. Humans also know, without really realizing, exactly what laughter sounds like. It’s a signal of play that we know immediately when we hear it, characterized by short bursts of sounds that last about one-fifteenth of a second and repeat in intervals each fifth of a second, Provine told me.
Selecting the right laughter for the Golden Record would have been less fraught, presumably, than selecting which language to feature on the record. Indeed, the language question created all sorts of difficulties. Sagan had suggested a day or two of recording at the United Nations headquarters in New York City, where delegates from each member nation might record a “hello” in their native language. “I had hoped that something like half of the voices could be male and half female, in order to reflect the distribution of sexes on the planet Earth,” he wrote in Murmurs of Earth, a book about the project published in 1978. “I was told that this was quite difficult on entirely other grounds. Virtually all the chiefs of delegations were male, and it was unlikely that they would delegate the privilege of saying ‘hello’ to the stars to anyone else.”
And so laughter, for all its complexities on Earth, was simpler to represent than language on the Golden Record. Except, when you listen to the “Footsteps, Heartbeat, Laughter” track of the “Sounds of Earth” audio that’s on the Voyager website—or to the version uploaded by NASA to Soundcloud—you won't actually hear any laughter. What happened to it? And whose voice is supposed to be there? I started to ask around.
“You are right that on the Voyager website there is a 30-second clip of a heartbeat and footsteps but not laughter,” a staffer at NASA’s Jet Propulsion Laboratory told me in an email when I asked what had happened. “Indeed, the NASA Soundcloud also has the same clip.” But there was a version of the recording where you could clearly hear the laughter, the staffer told me, on a website hosted by the Massachusetts Institute of Technology. NASA couldn’t verify the authenticity of that recording, but it was a start.
I listened, and there it was. The laughter is secondary to the heartbeat and footsteps. It starts low then tapers off into a light cackle. It sounds genuine, like the person is truly delighted by something. It only lasts a few seconds.
The laughter was there—but was this version of the Golden Record legitimate? My attempts to contact the website’s maker yielded little. The audio on the site originally came from the Library of Congress, I was told, but Library of Congress staffers couldn’t tell me for sure whether it was the same as the copy they had. (Nor did the Library of Congress have any additional information on the identity of the laugher.) Eventually, an archivist at the JPL was able to send me a sound file containing all 21 tracks from “The Sounds of Earth.” This was the official recording, but much better quality than what NASA had put online. It sounded just like the version on the MIT site—the crunch of footsteps, a steady heartbeat, and then the welling up of laughter. (Start around the 6:10 mark to skip right to the laughter in the audio-player embedded below.)

But that still didn’t solve my original question of whose laughter was on the record, or the question of why it had disappeared from the other official recordings. I couldn’t find anybody who could answer either question.
Not NASA, not the Library of Congress, not the Carl Sagan Institute at Cornell, where Sagan was a professor, and not NASA’s Jet Propulsion Laboratory. “JPL-Caltech was not involved in the creation of the audio recordings on the Voyager Golden Record,” a spokesperson there told me, “and has no knowledge of the identities of the persons recorded or those who did the recording.”
The official book about the Golden Record, Murmurs of Earth, contains a trove of fascinating background about the making of the record—including the painstaking process of gathering audio for the Sounds of Earth section—but there’s nothing in it about whose laugh is featured. I tried to arrange a time to speak with Ann Druyan, the producer and filmmaker who spearheaded audio collection for that portion of the record, but I couldn’t reach her. (Druyan and Sagan worked closely on the project, and later married.)
So I started contacting people listed in the “acknowledgements” section of Murmurs to see if anyone could remember. Nothing.
“I wish I could help you... but I don’t know,” wrote Lise Menn, a linguist and professor at the University of  Colorado who is listed in the book as having provided audio for the mother-and-child track. “I’m not even sure whose voice is on the mother-and-baby segment, although it may have been from [the linguist Margaret] Bullowa’s files—I was her assistant. If Dr. Druyan doesn’t know, I think we’re at a dead end.”
Fall turned to winter, winter to spring, spring to summer. It seemed that my mystery would remain unsolved. Meanwhile, the Golden Record was gliding ever farther into space, receding from our planet at 35,000 miles per hour. The two Voyager probes are now nearly 14 billion miles and 11 billion miles away from Earth, respectively. “Out there, our concepts of velocity become provincial,” Timothy Ferris, one of the record’s producers, wrote 10 years ago, anticipating the moment at which the space probe Voyager would tip across the threshold to interstellar space. That finally happened in 2014. At the time, Ferris described Voyager as a toy boat on a dark sea of stars swirling in “gigantic orbits around the center of the Milky Way galaxy.” He isn’t the first to evoke maritime comparisons. In 1986, the astronomer William Gutsch described Voyager as “a bottle cast by its creators, adrift on a cosmic ocean.” The Golden Record was a message in that bottle.
The people who made the record believed that, in the vacuum of space, it could remain in working condition for a billion years. The capsules protecting the records were configured in such a way, Sagan once wrote, that “all of the pictorial information, human and cetacean greetings, and ‘The Sounds of Earth’... will survive essentially forever.”
“If I had to guess, I’d say it’s as fresh and new as the day it was placed aboard the spacecraft,” David Doody, an engineer on the Voyager mission at NASA’s Jet Propulsion Laboratory told me a few years ago. “It’s been stored in a vacuum more perfect than any attainable on Earth, and protected from dust and cosmic rays by an aluminum metal case.”
Out there in space, the record swims onward through the stars. Back here on Earth, despite people’s enduring fondness for the project, little bits of context are vanishing. The answers to my questions about the laughter on the Golden Record seemed destined to be lost to time.
Then, two days ago, I received an unexpected message from someone at the Jet Propulsion Laboratory. “I met Ann Druyan today,” the lab’s Elizabeth Landau told me, “and she told me that the laughter on the Golden Record is Carl Sagan’s!”
I made half a dozen phone calls and sent out a flurry of emails—to the Golden Record’s producers, to NASA, to Druyan, to Sagan and Druyan’s children—all in an attempt to corroborate the account I’d received.
Was the laughter indeed Sagan’s? It certainly seemed plausible, but I had to be sure. Finally, I reached Sasha Sagan, the daughter of Carl Sagan and Ann Druyan.
“I just double checked with my mom to be absolutely sure, and yes, it is indeed my dad’s laugh!” Sagan wrote. “She said that laugh was the very first impression she ever had of my dad when she heard it upon entering Nora Ephron’s apartment [where they first met] in 1974, and included it in the Voyager sound essay because she wanted it to live on forever.”
The Golden Record has always been a love letter to humanity, and a love letter to the cosmos, but it’s also quite clearly a love letter between Druyan and Sagan, too. Druyan has said as much in past interviews when she’s described the afternoon she spent meditating at Bellevue Hospital. She went there to have electrodes attached to her body so a machine could register the pattern of electrical impulses in her brain and nervous system. The data from that session was etched onto the Golden Record, the idea being that some future, alien civilization might be able to convert EEG data back into comprehensible thoughts.
“This was two days after Carl and I declared our love for each other,” Druyan said in a radio interview years ago, “and so part of what I was thinking in this meditation was about the wonder of love, and of being in love. And to know it’s on those two spacecrafts... Now, whenever I’m down, I’m thinking, still they move, 35,000 miles an hour, leaving our solar system for the great wide open sea of interstellar space.”
It’s a cosmically romantic story. And if it sounds too good to be true, the Golden Record producer Timothy Ferris told me, that’s because he suspects it might be. “This is news to me,” he told me. He remembers Sagan’s laugh clearly, he said. “It doesn’t sound like what’s on that track. He had a great laugh. He had a good sense of humor.”
I wrote back to Druyan’s daughter to see what she thought. Was there even a chance Druyan was mistaken? Human memory is fallible, Druyan acknowledged in response via Sasha Sagan. It’s been 40 years since they made the Golden Record, and more than 20 years since Sagan died in 1996. But Druyan is quite certain about Sagan’s laughter and its place in the universe. “It’s highly unusual, virtually unique,” she said in an email via her daughter. “I chose it because of its exceptional lack of inhibition and because it was Carl’s.”
This morning, I went back and listened to the NASA file on SoundCloud and realized something else: It turns out the laughter is there, only it’s barely perceptible. Without hearing the better-quality version of the recording first, it’s next to impossible to register. “I've heard how bad the online versions are,” Ferris told me. It’s not clear why the version on NASA’s SoundCloud is so low-quality compared with the pristine audio in its archives.
The laughter is so faint. It’s mostly lost in the static. Billions of miles away, though, the original Golden Record is out there, still in mint condition. Which means Sagan’s laughter—if it is indeed his—may yet be heard in some faraway galaxy, by some species we cannot imagine. But that’s a mystery for another time.


Robot panic seems to move in cycles, as new innovations in technology drive fear about machines that will take over our jobs, our lives, and our society—only to collapse as it becomes clear just how far away such omnipotent robots are. Today’s robots can barely walk effectively, much less conquer civilization.
But that doesn’t mean there aren’t good reasons to be nervous. The more pressing problem today is not what robots can do our bodies and livelihoods, but what they will do to our brains.
“The problem is not that if we teach robots to kick they’ll kick our ass,” Kate Darling, an MIT robot ethicist, said Thursday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic. “We have to figure out what happens to us if we kick the robots.”
That’s not just a metaphor. Two years ago, Boston Dynamics released a video showing employees kicking a dog-like robot named Spot. The idea was to show that the machine could regain its balance when knocked askew. But that wasn’t the message many viewers took. Instead, they were horrified by what resembled animal cruelty. PETA even weighed in, saying that “PETA deals with actual animal abuse every day, so we won’t lose sleep over this incident,” but adding that “most reasonable people find even the idea of such violence inappropriate.”
The Spot incident, along with the outpouring of grief for the “Hitchbot”—a friendly android that asked people to carry it around the world, but met an untimely demise in Philadelphia—show the strange ways humans seem to associate with robots. Darling reeled off a series of other ways: People name their Roombas, and feel pity for it when it gets stuck under furniture. They are reluctant to touch the “private areas” of robots, even only vaguely humanoid ones. Robots have been shown to be more effective in helping weight loss than traditional methods, because of the social interaction involved.
People are more forgiving of robots’ flaws when they are given human names, and a Japanese manufacturer has its robots “stretch” with human workers to encourage the employees to think of the machines as colleagues rather than tools. Even when robots don’t have human features, people develop affection toward them. This phenomenon has manifested in soldiers bonding with bomb-dismantling robots that are neither anthropomorphic nor autonomous: The soldiers take care of them and repair them as though they were pets.
“We treat them as though they’re alive even though we know perfectly well they’re machines,” Darling said.
That can be good news—whether it’s as weight-loss coaches or therapy aides for autistic children—but it also opens up unexplored ethical territory. Human empathy is a volatile, unpredictable force, and if it can be manipulated for good, it can be manipulated for bad as well. Might people share sensitive personal information or data more readily with a robot they perceive as partly human than they would ever be willing to share with a “mere” computer?
Social scientists (and anxious parents) have wondered for years about the effect of violent video games on children and adults alike. Even as those questions remain unresolved, an increasing number of interactions with robots will create their own version of that debate. Could kicking a robot like Spot desensitize people, and make them more likely to kick their (real) dogs at home? Or, could the opportunity to visit violence on robots provide an outlet to divert dangerous behaviors? (Call it the Westworld hypothesis.)
An even more pungent version of that dilemma could revolve around child-size sex robots. Would such a thing provide a useful outlet for sex offenders, or would it simply make pedophilia seem more acceptable? Making the dilemma more challenging, it’s extremely difficult to research that question.
The sway that even rudimentary robots can hold over humans was clear near the end of Darling’s talk. A short robot whirred out on stage to alert her that she had five minutes left to speak. The audience, which had just listened to a thoughtful, in-depth litany of the ethical challenges of human-robot interactions, cooed involuntarily at the cute little machine. And Darling, who had just delivered the litany, knelt down to pat its head.


Earlier this week, I asked parents to share their approach to protecting the privacy of their children as they begin to use devices with Internet access and social networks. The inquiry was inspired by an Aspen Ideas Festival talk where Julia Angwin and Manoush Zomorodi revealed how their reporting on privacy changed their parenting.
The parents who’ve replied so far are in agreement that the task is difficult.
Our first correspondent is a married woman in her mid-40's with a 12-year-old child. She lives in Irvine, California. She recently created a technology contract with her child.
She writes:
My 12-year-old doesn't yet have a Facebook account, and doesn't remember how to use her Instagram account. I've showed her Snapchat, but her friends don't use it, and she hasn't pushed for it. (She was understanding when I told her I'd deleted it because the filters were so racist.) I expect her upcoming 7th-grade year to involve a lot of change in what has up to now been very limited use of social media. She just this year got a smartphone, several years after many in her upper-middle-class-neighborhood public school did. She has a cheap tablet that she uses to watch Cartoon Network and Youtube. She watches hours of Youtube with little supervision, mostly young adults who are passionate about animation or crafting, and she texts with friends. So my level of awareness about what she does in digital spaces is fairly low - I have no idea who most of the you-tubers are who she watches. We do talk about them, and she knows we can access her browser
history, for now.
We did have a conversation this year after one of my husband's periodic browser-history checks turned up some moderately adult content (YouTube animations illustrating funny-in-retrospect sexual experiences, like getting caught by a parent), but I'm okay with her using the internet to look around a bit. We have talked for a while now about how the internet can get intimacy wrong, and these conversations are an important part of the general ongoing conversations about intimacy, sex, contraception, and consent.
My job as a librarian does involve a slightly higher than average level of involvement with online privacy issues, but that hasn't translated to at-home chats about higher-level information security. We're still more focused on issues around communication, and how the online setting can make it even harder to make mature, empathetic decisions that it is normally, especially for teens whose brains are still developing. So her concept of personal information is evolving, and I think it's going to be challenging to navigate that at the very same time she's navigating the complicated personal-growth time of the early teen years.
I would characterize my attitude towards dealing with the online world in parenting as resentful but resigned. I'm not scared of the internet, like I don't worry about rando sex offenders. Instead, I feel like the companies and governments that are collecting dossiers of information about individuals can't be trusted to get things right, and I feel like I need to teach my kid to deal with that, and that's a drag. We're trading so much for cute cat videos and easy access to coffee filters and hand cream.
She really likes those cute cat videos, though.
* * *
James has worked for more than 20 years in the IT industry and has nine children ranging in age from eighteen to three years old. “I'm probably in the minority in that I'm definitely not ‘out-teched’ by our kids, despite years of trying to get them interested in what goes on under the hood,” he writes. “My wife and I are one of the last cohorts to remember, fully, the time prior to the Internet and so it's much easier for us to end-run the whole thing by simply not participating. Suggesting the same to our kids or their peers is akin to asking them to give up one eye and both feet.”
Their advice distills down to a few first principles:
1. If you are not paying for the app, website, or service, you are not the customer. You are the product, and the way that they're making all their money.
2. There is no privacy online. Doesn't matter how clever you think you are with a nickname or how careful you are with your pictures and comments. Say the wrong thing at the wrong time and you will be unmasked, publicly, for the world to gawk at.
3. Don't say anything online you wouldn't own in person, or want read back to you by us over dinner.
As to our awareness - one rule we have for the few that have taken some steps into social media (Facebook and Instagram for the most part) - is that they stay connected with us so we can see what's going on. We're aware of at least one finsta; the child (who is 16) is also aware that we're aware.
Using tools borne from the professional experience I mentioned above, internet traffic is monitored regularly (and filtered). I pull up the dashboard, explain how it all works, show what I'm able to see - and not see. So far, these lessons seem to have take hold, especially as the kids get older and see the consequences in the news, whether it's doxing, texting scandals, or cautionary tales of students (or professionals) getting called out for their online activities and losing scholarships (or jobs) as a result of them.
* * *
Our next correspondent is a mother of two boys, eight and ten, and the Data Privacy Consultant for the California Department of Education. In her work, she has seen “both the power of data to tell compelling stories and power of data to wreak havoc.”
She explains:
When used for good, data can ensure personalized instruction / interventions for kids who are struggling, food for kids who are hungry, and more. When used for ill, data can be inconvenient (e.g., useless to answer important questions), annoying (e.g., telemarketers), and terrifying (e.g., identity theft). In constantly-connected, perpetually-hacked digital spaces, collection of any data comes with risks. As such, one must constantly be asking: (1) Is collecting/sharing these data legal and necessary to answer important questions? (2) Do the expected benefits outweigh the inherent risks? and (3) Is every conceivable, plausible measure being taken to minimize data collection, protect data assets, manage/utilize data to maximize benefit?
But awareness of those perils does not cause her to keep her kids offline:
My approach to screen time mirrors my approach to life in general. I don’t believe in living in a bubble, avoiding uncomfortable truths, pretending that I can control things that I cannot. But I do believe in being reasonable and thoughtful, staying informed and sharing information with others, striving each day to learn and do better. I believe in being cautious without being alarmist…in managing and minimizing risks without suffocating benefits. When it comes to my kids and any topic—including but certainly not limited to screen time—I hope I can imbue them with both the confidence to explore and the knowledge/skills to ask questions/seek help when they find they’ve wandered too far. In both the physical and the digital world, parenting is a perpetual lesson in letting go, a daily affirmation that control is an illusion.
I recently heard an analogy that I think makes a lot of sense. The analogy is this: We don’t teach our kids to be water safe by showing them a video, pretending water doesn’t exist, or refusing to let them get in the pool. We teach them to be water safe by suiting up, jumping in the pool with them, and helping them learn the skills that will minimize their risk of drowning. Carrying this analogy into the online space, my personal stance is that—whether we like it—our kids are in the pool. Technology is everywhere. As such, it’s up to us to get in there with them and guide them through mistakes and dangers so that someday…even when we’re not around to pull them out of the water…they can save themselves with smart decisions and well-honed skills. I’m a big fan of Web sites like Common Sense Media, FERPA Sherpa, On Guard Online, US Department of Education’s Protecting Student Privacy, and Soul Behind That Screen.
Here are the rules in her house:
(1) No screen time is permitted before my hubs and I wake up in the morning. If the boys rise before us, they need to find other activities (e.g., reading, Legos) to fill their time.
(2) My hubs and I are the holders of the passwords. If the boys want online access, we’re the gatekeepers. The boys understand that at any moment, Mom and Dad can and will check their history to see what they’ve been up to.
(3) Any abuse of screen time privileges will result in immediate revocation of said privileges. We have a very small house and Mama’s got very good ears. If a Minecraft or basketball YouTube video veers into inappropriate language or content, the boys are responsible to shut it down…IMMEDIATELY. If Mom or Dad have to come in to shut it down, the screen is going OFF for a good, long time.
(4)  When playing games with potential to connect with others, they are required to:
a.       Never connect with anyone they don’t know
b. Never share personal information
c.       Only use online IDs that are nonsensical and won’t reveal anything about who they are, how old they are, where they live, etc.
d. Only connect with friends whose identities have been verified by me or their dad (e.g., through a text to other parents verifying that the user name my kiddos want to connect with is affiliated with the kiddo we think it is)
e.       Tell my hubs or I immediately if anyone is pressuring them to share information
(5) Screen time ends at least 30 minutes before bed.
(6) We watch together. Especially when watching shows that aren’t necessarily targeted to their demographic, either my hubs and I watch ahead of time to make sure everything is copacetic before giving approval and/or we watch together so that we can shut things down or answer questions as appropriate. One example is Gilmore Girls. This is one of my favorite shows and I’ve been binge watching it with the boys. It’s sparked a lot of important conversations about coming of age. It has been a great tool for us to bond and talk about important topics. If they ask to watch or play something that I don’t think they’re ready for, I’ll give them the respect of watching at least a portion/researching so that I can cite specific reasons (language, sex, violence, etc.) that I think they should wait or avoid consuming the content altogether. Especially as they grow older, I won’t be around all the time to hover. I want them to be analytic, critical thinkers who make thoughtful decisions. As such, I try to let them experience what it’s like to ask, debate, research, consider the input of others, and draw conclusions. I try to avoid too many “No! Because I said so’s.”
(7) Family time, exercise, and chores are greater than screen time. Those who live in the house connect in the house, help in the house, behave in healthy ways in the house. The second the screen gets in the way of responsibilities to one another and ourselves, it’s time to go cold turkey for a bit and remember that we are NOT addicts fiending for a drug but humans who have the capacity to enjoy things in moderation.
* * *
Veronica is the parent of a 3-year-old, and while she has thought deeply about the ways she will try to protect her digital privacy she feels that regulatory changes are what’s ultimately needed:
The production and collection of my child's digital traces is really beyond my control. For instance, although I asked her pre-school not to share photos of her on Facebook, other parents do, and this leads to the fact that the Facebook's DeepFace technology has already stored her facial recognition data. Sometimes I ask parents to remove the pictures, but social media content is deeply interconnected with highly emotional and personal relationships, with the need to share experiences and give meaning to them, and sometimes - as a parent - it is simply not possible to 'opt out'.
Yet social media are just one dimension of the datafication of children, and the impossibility to opt out. Most of my daugthers' data is collected and stored by a plurality of agents, from her preschool digital records to her health records (both stored on outsourced platforms), from social media to cloud systems. I have little control of how her data is collected, stored, shared and exploited.
As a parent I will of course talk to her about digital privacy, and what should and should not be posted on social media. I will probably use the technique used by one parent in London: everytime the daugther wanted to download an app on her phone she would need to study the terms and conditions… However, I believe that the issue is far more complex than simply developing ways to teach our children how to protect their privacy. As parents,  we should not only be talking about digital privacy and how we can instruct our children to protect it, but rather about 'data justice'. What we need is to campaign for more regulation and transparency.
* * *
Alex writes:
This was a topic my wife and I discussed at length during her pregnancy.  Our daughter is now 8months old. From the second she was born she was old enough for us to be concerned about her presence on the internet.  We haven't decides on rules for when she is older, but from the start we agreed and had a strict policy required for us, family, and friends: her face would NOT be used in any social media posts. Our concerns ranged from privacy policies on instagram and facebook, to who controls the rights to those photos once posted. We impose pretty harsh penalties for not abiding by our rules, mostly a length of time where you do not get access to our child or their photos as distributed by us.
Being 31 and growing up in the ancient times (Before Social Media), I know that had my parents plastered my image on the internet for all to see, I would have held it against them well into my adulthood. This is a simple matter of respect and trust for your child. I want her to make the decisions about her online footprint and understand the consequences of what happens if you are not thoughtful about your internet presence in our current world.
I won’t make that choice for her.
If you’ve taken a different approach than these correspondents write conor@theatlantic.com to share your approach.


The former head of DARPA, Arati Prabhakar, has a dream. It’s a civilian utopian neuroscience dream that’s kind of the inverse of the scenarios that the far-out research wing of the military normally develops.
In a presentation at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic, she’d just shown a video from University of Utah research in which a soldier who’d lost his arms “felt” a virtual door through neural stimulation.
She cautioned that the research was very new and “not yet a robust capability, but even at this stage, we can start to see that there will be some mind-bending questions about how we use these technologies in the future.” Prabhakar noted that from a technological point of view, “there’s not much distance from restoration to enhancement.”
She asked the audience: “Do you think the future we’re going to live in a society where neuro-enhancements will be a privilege? Will they be a right? Might they be a mandate? Or maybe the whole idea is gonna creep us out so much that we won’t want anything to do with it.”
Mandated neuroenhancements certainly seem creepy to me!
“But imagine if you could learn a new language as fast as a 6-year-old,” she continued. “Or imagine if you could experience a whole new palette of colors or a fourth physical dimension in space.”
I’m not even sure what that means, but I would most definitely want to experience a fourth physical dimension in space.
But then she delivered the true utopian dream:
“Imagine if we could connect among ourselves in new and deeper ways and imagine if those connections happened in a way that gave us so much empathy and understanding of each other that we could put our minds together, literally, to take on some of the world’s hardest problems.”
She did not expound on the image, but one imagines she’s thinking about a kind of direct brain-to-brain interface.
DARPA has, after all, invested a lot in direct electronic brain interfaces. In one research program, they’re working on “intuitive” neural interfaces for controlling prosthetic limbs. In another, they’re creating “an implantable neural interface able to provide advanced signal resolution and data-transfer bandwidth between the brain and electronics.” The goal there is to create a translator between “the electrochemical language used by neurons in the brain and the ones and zeroes that constitute the language of information technology.”
And once you’ve got intuitive neural controls and a translator that lets you send brain signals into computers and back again, it does not seem an incredible leap to hook two (or … a million?) humans up together.
“If we could get to that future, we would look back at today’s reality and it would look like black-and-white,” Prabhakar said. “It would look like flatland.”
It’s a pretty fascinating speculative fiction. Compelling, perhaps, but also repulsive. And, if we can use recent history as any guide, wiring humans together in ever tighter communication loops doesn’t always turn out so well.


It is hard to imagine more fitting names for code-gone-bad than WannaCry and Eternal Blue. Those are just some of the computer coding vulnerabilities pilfered from the National Security Agency’s super-secret stockpile that have been used in two separate global cyber attacks in recent weeks. An attack on Tuesday featuring Eternal Blue was the second of these to use stolen NSA cyber tools—disrupting everything from radiation monitoring at Chernobyl to shipping operations in India. Fort Meade’s trove of coding weaknesses is designed to give the NSA an edge. Instead, it’s giving the NSA heartburn. And it’s not going away any time soon.
As with most intelligence headlines, the story is complicated, filled with good intentions and unintended consequences. Home to the nation’s codebreakers and cyber spies, the NSA is paid to intercept communications of foreign adversaries. One way is by hunting for hidden vulnerabilities in the computer code powering Microsoft Windows and and all sorts of other products and services that connect us to the digital world. It’s a rich hunting ground. The rule of thumb is that one vulnerability can be found in about every 2,500 lines of code. Given that an Android phone uses 12 million lines of code, we’re talking a lot of vulnerabilities. Some are easy to find. Others are really hard. Companies are so worried about vulnerabilities that many—including Facebook and Microsoft—pay “bug bounties” to anyone who finds one and tells the company about it before alerting the world. Bug bounties can stretch into the hundreds of thousands of dollars.
Writing the Rules of Cyberwar
The NSA, which employs more mathematicians than any organization on Earth, has been collecting these vulnerabilities. The agency often shares the weaknesses they find with American manufacturers so they can be patched. But not always. As NSA Director Mike Rogers told a Stanford audience in 2014,“the default setting is if we become aware of a vulnerability, we share it,” but then added, “There are some instances where we are not going to do that.” Critics contend that’s tantamount to saying, “In most cases we administer our special snake bite anti-venom that saves the patient. But not always.”
In this case, a shadowy group called the Shadow Brokers (really, you can’t make these names up) posted part of the NSA’s collection online, and now it’s O.K. Corral time in cyberspace. Tuesday’s attacks are just the beginning. Once bad code is “in the wild,” it never really goes away. Generally speaking, the best approach is patching. But most of us are terrible about clicking on those updates, which means there are always victims—lots of them—for cyber bad guys to shoot at.
WannaCry and Eternal Blue must be how folks inside the NSA are feeling these days. America’s secret-keepers are struggling to keep their secrets. For the National Security Agency, this new reality must hit especially hard. For years, the agency was so cloaked in secrecy, officials refused to acknowledge its existence. People inside the Beltway joked that NSA stood for “No Such Agency.” When I visited NSA headquarters shortly after the Snowden revelations, one public-affairs officer said the job used to entail watching the phones ring and not commenting to reporters.
Now, the NSA finds itself confronting two wicked problems—one technical, the other human. The technical problem boils down to this: Is it ever possible to design technologies to be secure against everyone who wants to breach them except the good guys? Many government officials say yes, or at least “no, but…” In this view, weakening security just a smidge to give law-enforcement and intelligence officials an edge is worth it. That’s the basic idea behind the NSA’s vulnerability collection: “If we found a vulnerability, and we alone can use it, we get the advantage.” Sounds good, except for the part about “we alone can use it,” which turns out to be, well, dead wrong.
That’s essentially what the FBI argued when it tried to force Apple to design a new way to breach its own products so that special agents could access the iPhone of Syed Rizwan Farook, the terrorist who, along with his wife, killed 14 people in San Bernardino. Law-enforcement and intelligence agencies always want an edge, and there is a public interest in letting them have it.
As former FBI Director James Comey put it, “There will come a day—and it comes every day in this business—where it will matter a great deal to innocent people that we in law enforcement can’t access certain types of data or information, even with legal authorization.”
Many leading cryptographers (the geniuses who design secure communications systems) and some senior intelligence officials say that a technical backdoor for one is a backdoor for all. If there’s a weakness in the security of a device or system, anyone can eventually exploit it. It may be hard, it may take time, it may take a team of crack hackers, but the math doesn’t lie. It’s nice to imagine that the FBI and NSA are the only ones who can exploit coding vulnerabilities for the good of the nation. It’s also nice to imagine that I’m the only person my teenage kids listen to. Nice isn’t the same thing as true. Former NSA Director Mike Hayden publicly broke with many of his former colleagues last year. “I disagree with Jim Comey,” Hayden said. “I know encryption represents a particular challenge for the FBI. ... But on balance, I actually think it creates greater security for the American nation than the alternative: a backdoor.”
Hayden and others argue that digital security is good for everyone. If people don’t trust their devices and systems, they just won’t use them. And for all the talk that security improvements will lock out U.S. intelligence agencies, that hasn’t happened in the 40 years of this raging debate. That’s right. 40 years. Back in 1976, during the first “crypto war,” one of my Stanford colleagues, Martin Hellman, nearly went to jail over this dispute. His crime: publishing his academic research that became the foundational technology used to protect electronic communications. Back then, some NSA officials feared that securing communications would make it harder for them to penetrate adversaries’ systems. They were right, of course—it did get harder. But instead of “going dark,” U.S. intelligence officials have been “going smart,” finding new ways to gather information about the capabilities and intentions of bad guys through electronic means.
The NSA’s second wicked problem is humans. All the best security clearance procedures in the world cannot eliminate the risk of an “insider threat.” The digital era has supersized the damage that one person can inflict. Pre-internet, traitors had to sneak into files, snap pictures with hidden mini-cameras, and smuggle documents out of secure buildings in their pant legs or a tissue box. Edward Snowden could download millions of pages onto a thumb drive with some clicks and clever social engineering, all from the comfort of his own desktop.  
There are no easy solutions to either the technical or human challenge the NSA now faces. Tuesday’s global cyber attack is a sneak preview of the movie known as our lives forever after.
Talk about WannaCry.


There’s a paradox in technology. For something new to become widespread, familiar, and mass-market, it must create enough novelty and curiosity to draw people’s attention. But novelty alone is not enough to reach saturation. To permeate life, a technology must elicit more than novelty and curiosity in its users. It must become ordinary. It must recede into the background, where it continues to run but ceases to be noticed by the humans  who made it pervasive.
This is the story of all successful technologies. The locomotive, airplane, and automobile. The electric light, the telephone, the washing machine, the personal computer. So humdrum are these once-revolutionary machines that no one gives them a second thought, unless they break down.
Ten years after its introduction, the iPhone—and the smartphone category it created—is starting to recede into the background. Apple has sold a billion of the things alone. Android devices account for a billion and a half smartphone users. Glass-and-metal rectangles fill hands, lounge on tables, tuck in pockets, illuminate faces. Like toasters and gas stations, like bus ads and Starbucks, anywhere you look you’ll see an iPhone.
Now that the iPhone is everywhere, it can finally disappear.
* * *
As it quickly replaced its predecessors, the iPhone challenged people to make sense of it, and to integrate it into their lives. At first, it was a gadget. I remember my first encounter with it, in 2007: A friend had bought one early, and he eagerly brushed and pinched it to show me how the multi-touch screen worked. Those were new once, and merely operating one was exciting on its own. It helped that the iPhone didn’t quite work right yet. It was slow, it stuttered, it froze. Almost like it was getting used to its owners as much as they were getting used to it.
Within a year or two, it became a compulsion. First for work on the run, as emails and texts and reminders, previously available only to executives and bureaucrats on BlackBerries and Palm Treos, became a staple of the everyperson’s information diet. Then for play, when games, apps, and social media lured people back to the device just to see if anything new had happened while they were away from it.
Several years in, that compulsion became a ritual. All the bad, deleterious urges to warm one’s face in front of the iPhone transformed into a mode of living. A way of being. This is just what life was like: Scrolling vigorously while standing in line for coffee. Retreating into the device during lulls in dinner conversation. Operating it unwisely, and knowingly so, while driving. Smartphones began replacing computers and laptops for some, and televisions and movie theaters for many. People began acquiring devices for their children, and even for their toddlers, and even for their babies.
For me, each of these phases suggested its own analogy to make sense of the novelty. For the gadget period, the iPhone was like a toy dog—a thing to hold and carry and stroke and dote on. A device that could be cared for, and conspicuously so.
For the compulsive era, the iPhone was like a cigarette. A nervous tic, facilitated by a handheld apparatus that releases relief when operated. Along with it comes dependence, and an awareness of that dependence. A shame in it, even, and an attendant vow to stop, if only stopping was possible. And yet, as with the cigarette, the iPhone also conferred an air of cool. It gave people something to do with their hands. Though compulsive, it offered a compulsion everyone shared.
For the ritual phase, the iPhone was like a rosary. In the secular age, industry in general and the computer in particular has taken the place of the church. Instead of God, technology has become the ultimate means of understanding and changing the world. Algorithms became mystical, delivering truths. What can be thought and what should be done is equal to what the computer allows and makes possible. Its toy-dog quirks having been tamed, its compulsive nature having been accepted, the iPhone became the magic wand by which all worldly actions could be performed, all possible information acquired. There’s a reason an ancestor of the iPhone was code-named “Pocket Crystal” inside Apple.
What the iPhone isn’t, anymore, is a phone, in the traditional sense. A smartphone, yes, although that’s a product category for industrialists and business writers. And yet, people still call it a “phone” colloquially: I can’t find my phone! or Let me just check my phone or Sorry, I was on my phone. The last one condenses much of the reality of iPhone use. Where once one might have been “on the phone”—speaking into the singular telephony device in a home—now everyone is engaged with, and transmitting information by means of, their own private device. All the time.
The metaphor for this phase of the iPhone’s life has long eluded me. But recently I fell upon it, thanks to the writer Claire Donato. “From the rectangle, she downloads instructions for preparing a slick coat for the green beans and potatoes,” I heard Donato read from an in-progress novel late this spring.
The rectangle. Abstract, as a shape. Flat, as a surface. But suggestive of so much. A table for community. A door for entry, or for exit. A window for looking out of, or a picture for looking into. A movie screen for distraction, or a cradle for comfort, or a bed for seduction. A hole of infinite depth, yet also a veil whose blackness covers up that chasm.
The rectangle cuts to the truth of the iPhone as a widely adopted, mass-market device. When Steve Jobs first introduced the iPhone, he described it as “a widescreen iPod with touch controls, a revolutionary mobile phone, and a breakthrough internet communications device.”
“Three things,” he called it, before lingering on the reveal. “These are not separate devices. This is one device.”
This reality seems so obvious today that it’s quaint to recall when things could have been otherwise. Convergence didn’t collapse all media into a single format, as some had predicted. Instead it channeled both old and new ways of working, living, and playing through a common opening. It flattened it. “A rectangle is a flat shape,” Donato tells me when I ask how she arrived at the metaphor. “I can’t stop thinking about how the internet is such a flat surface. We’re compressed here.”
There’s obvious sorrow in Donato’s words. It doesn’t always feel good, this rectangle flatland. The MIT professor and psychologist Sherry Turkle has lamented it from the vantage point of a social critic. To her, the era of the rectangle is one of lost conversation, of the masses “alone together.” But it has also become the basis for ordinary life. Perhaps some can withdraw from it into Charles Riverside salons or art-gala afterparties, but for most, life is now conducted by rectangle. It is technological populism.
And in that respect, the abstract, flat image of the rectangle disarms the curiosity and novelty of the iPhone. As its first decade ends, the rectangle can finally become a technology of ordinary life. Drop the “ordinary,” even. Just life, without the novelty and curiosity interrupting it with new diversions—even as the device itself persists in delivering constant distraction.
* * *
The irony of the iPhone’s ubiquity is that nobody seems content with it. Voice-activated artificial intelligences, virtual reality, augmented reality, robots, and more futurist interlopers all press at the gates, eager to unleash their novelties and disrupt the iPhone. Apple’s investors are impatient that the company hasn’t yet done so itself, dissatisfied with a single company selling a billion rectangles to turn the round Earth into its private flatland.
It’s a bittersweet victory when a technology achieves ubiquity. On the one hand, it makes universal experience possible. Huge swaths of people share a common means for socializing, learning, working, and playing. But on the other hand, pervasiveness domesticates technology. What was once wild and exciting becomes ordinary and routine. Its wings clipped, its paws declawed, the rectangle poses no threat—and thereby appears to offer no further promise, either.
When faced with technology’s paradox, I often think of Star Trek. The crew of the Enterprise doesn’t think much of the computer. It doesn’t have a name, like Siri or Alexa or Cortana, even though it also speaks in the disembodied voice of a woman. It’s just “Computer.” Despite playing a central role in the operation of the starship, nobody thinks much about the computer. It’s just there, like always. No big deal.
It’s a science-fictional future too few acknowledge, let alone pine for. One in which technology becomes advanced by becoming ordinary. When all the cheers and wails cease, they leave behind a changed world, but one its human residents must nevertheless occupy. That’s us, now: Billions of people, their hands clutched to rectangles both dark and emissive, learning to live with it.


The Harvard psychologist Joshua Greene is an expert in “trolleyology,” the self-effacing way he describes his research into the manifold variations on the “trolley problem.” The basic form of this problem is simple: There’s a trolley barreling towards five people, who will die if they’re hit. But you could switch the trolley onto another track on which only a single person stands. Should you do it?
From this simple test of moral intuition, researchers like Greene have created an endless set of variations. By varying the conditions ever so slightly, the trolley problem can serve as an empirical probe of human minds and communities (though not everyone agrees).
For example, consider the footbridge variation: You’re standing on a footbridge above the trolley tracks with a very large person, who, if you push him or her, can stop the trolley from killing the five people.  Though the number of lives saved is the same, it turns out that far more people would throw the switch than push the person.  
But this is not quite a universal result. During a session Wednesday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic, Greene joked that only two populations were likely to say that it was okay to push the person on the tracks: psychopaths and economists.
Later in his talk, he returned to this, however, through the work of Xin Xiang, an undergraduate researcher who wrote a prize-winning thesis in his lab titled “Would the Buddha Push the Man of the Footbridge? Systematic Variations in the Moral Judgment and Punishment Tendencies of the Han Chinese, Tibetans, and Americans.”
Xiang administered the footbridge variation to practicing Buddhist monks near the city of Lhasa and compared their answers to Han Chinese and American populations.  “The [monks] were overwhelmingly more likely to say it was okay to push the guy off the footbridge,” Greene said.
He noted that their results were similar to psychopaths—clinically defined— and people with damage to a specific part of the brain called the ventral medial prefrontal cortex.
“But I think the Buddhist monks were doing something very different,” Greene said. “When they gave that response, they said, ‘Of course, killing somebody is a terrible thing to do, but if your intention is pure and you are really doing it for the greater good, and you’re not doing it for yourself or your family, then that could be justified.’”
For Greene, the common intuition that it’s okay to use the switch but not to push the person is a kind of “bug” in our biologically evolved moral systems.
“So you might look at the footbridge trolley case and say, okay, pushing the guy off the bridge, that’s clearly wrong. That violates someone’s rights. You’re using them as a trolley stopper, et cetera. But the switch case that’s fine,” he said. “And then I come along and tell you, look, a large part of what you’re responding to is pushing with your hands versus hitting a switch. Do you think that’s morally important?”
He waited a beat, then continued.
“If a friend was on a footbridge and called you and said, ‘Hey, there’s a trolley coming. I might be able to save five lives but I’m going to end up killing somebody! What should I do?’ Would you say, ‘Well, that depends. Will you be pushing with your hands or using  a switch?’”
What people should strive for, in Greene’s estimation, is moral consistency that doesn’t flop around based on particulars that shouldn’t determine whether people live or die.
Greene tied his work about moral intuitions to the current crop of artificial-intelligence software. Even if they don’t or won’t encounter problems as simplified as the trolley and footbridge examples, AI systems must embed some kind of ethical framework. Even if they don’t lay out specific rules for when to take certain behaviors, they must be trained with some kind of ethical sense.
And, in fact, Greene said that he’s witnessed a surge in people talking about trolleyology because of the imminent appearance of self-driving cars on human-made roads. Autonomous vehicles do seem like they will be faced with some variations on the trolley problem, though Greene said the most likely would be whether the cars should ever sacrifice their occupants to save more lives on the road.
Again, in that instance, people don’t hold consistent views. They say, in general, that cars should be utilitarian and save the most lives. But when it comes to their specific car, their feelings flip.
All these toy problems add up to a (still incomplete) portrait of human moral intuitions, which are being forced into explicit shapes by the necessity of training robots. Which is totally bizarre.
And the big question Greene wants us to ask ourselves before building these systems is: Do we know which parts of our moral intuition are features and which are bugs?


A man sits in a chair in front of a small documentary camera crew. He’s trim, dressed in all black. A red notebook sits on his lap. “Here’s what I wrote in 1989,” he says. “This is a very personal object. It must be beautiful. It must offer the kind of personal satisfaction that a fine piece of jewelry brings. It will have a perceived value even when it’s not being used. It should offer the comfort of a touchstone, the tactile satisfaction of a seashell, the enchantment of a crystal.”
Then comes the reveal. He picks up the notebook. We see a sketch: a rectangular slab of glass, all display, except for bezel at the top and bottom. From his pocket, he pulls an iPhone and holds it above the drawing. The similarities are startling.
“We really had it,” he says with a thin laugh. “We definitely had it.”
This is a scene from the forthcoming documentary General Magic, named for the company that attempted to manufacture the device from the notebook. The man is Marc Porat, CEO of the company. He’d recruited two Apple employees, Bill Atkinson and Andy Hertzfeld, who had created the Macintosh. In its earliest iteration, inside Apple, the project had been called Pocket Crystal.
After the project was spun out and years of frenzied development, Wired profiled the company in their April 1994 issue. There were 13 million internet users in 1994. There was roughly one cell phone per 100 people on Earth, none of them equipped to do much more than make calls. The first SMS text message had been sent just two years before.
Yet they were convinced that they were making the most important device ever.
“It’s like a lot of different areas are converging on an electronic box that’s in your pocket, that’s with you all the time, that supports you in some way,”  Atkinson told Wired. “That’s going to happen with or without General Magic.”
He was right.
The iPhone launched 10 years ago. The device—and its many, many descendants—is core to how we live. After only a decade, smartphones easily outnumber PCs, despite personal computing’s quarter-century head start. There are 2.5 billion Apple iOS and Android smartphones in use out there, with that number, as analyst Ben Evans puts it, “heading for 5 billion plus users.” PCs never even cracked 2 billion users and are now drifting downwards.
The iPhone is the single-most successful product of all time. One billion iPhones have been sold. They underpin the most valuable company in history, and have catalyzed a whole new technology industry that’s an order of magnitude larger than the one built around PCs. This came with a major assist from Android, the mobile operating system that Google acquired, and then rebuilt after the iPhone came out. But the iPhone pioneered the market, the user interface, the working form factor, and the app store. And iPhone users drove network upgrades and buildouts by the major wireless carriers across the world because people with the Apple devices consumed so much data relative to other cellphone users.
In short: the iPhone is the Pocket Crystal, and we are all enchanted.
But staring at the 1989 sketch and down at one’s phone, it is hard not to ask: How could the form, appeal, and importance of the device have been apparent 18 years before its appearance?
Was the iPhone, in some way, inevitable?
* * *
If you want to understand the long sweep of tech history that culminated in the iPhone, it’s worth paying a visit to Bill Buxton’s gadget museum. A Microsoft user-interface designer, he’s collected dozens and dozens of interactive devices, and documented them for all to see. Strange keyboards, handheld devices, electronic gloves, touch screens, touch pads, phones, and e-readers.
The General Magic Data Rover 840, a 1998 release, is in the collection. It looks nothing like a Pocket Crystal. Like all the other devices designed to work with General Magic’s software—e.g. the Sony PIC series and Motorola Envoy—the housing is grey and bulky. There’s a stylus, of course, and a grayscale backlit screen. The device is heavily skeumorphic, drawing on real-world analogs for everything. To add a new contact, one had to first go to the “Office,” one of the software’s “rooms,” before pulling up the address book functionality. The settings were located in the “Hallway,” like a thermostat.
This is not to fault General Magic for creating devices with the technology of the era. Buxton’s collection contains other key precursors to the modern smartphone—and all of them have that teenage awkwardness to them.
There’s the Newton, Apple’s own personal digital assistant, which was released in 1993. The so-called MessagePad looks more like Porat’s sketch, but it relied on shaky handwriting recognition and inadequate battery technology. While Newton improved through the ’90s, it was eventually canceled, and history records it mostly as a flop.
Then there are the various devices that Palm powered. The Palm Pilot, introduced in 1996, became the standard bearer for PDAs, as they were known through the end of the ’90s. They were useful and improved steadily, but never became much more than glorified address books and calendars.
“No computer product category has been more ridiculed than the PDA,” wrote Home Office Computing magazine in 1995. “Originally conceived as a tiny digital factotum that would call home, receive faxes, store documents, and send email, the first PDAs from AT&T, Apple, Casio, and Tandy fell far short of expectations.”
That’s how a review of the most intriguing early smartphone, the IBM/BellSouth Simon, begins. It was a straight-up smartphone with a touchscreen—in the mid-’90s. The battery lasted eight hours in standby mode or a single hour in use. It weighed more than a pound. And it cost $899. But it worked better than the rest of the devices out there.
The Home Office Computing review ends promisingly, or ominously, as it were. “It may be that we're still asking too much of PDAs,” it says. “For example, how can you possibly fit an acceptably large touch screen on an object that’s supposed to fit in your pocket?”
While PDAs floundered through the 1990s, cell phones soared. Nokia became the world’s dominant smartphone maker with rugged, simple devices. It’s easy to forget that Nokia was the cell phone game for many years. In the year the iPhone came out (2007), Nokia sold 437 million phones and had near half of the cell phone market. And yet they never released anything that looked like the Pocket Crystal.
But that’s not to say that they didn’t think about it. In a funereal piece in The Wall Street Journal, former head designer Frank Nuovo rued Nokia’s mistakes.
“More than seven years before Apple Inc. rolled out the iPhone, the Nokia team showed a phone with a color touchscreen set above a single button. The device was shown locating a restaurant, playing a racing game and ordering lipstick,” the Journal narrated. “In the late 1990s, Nokia secretly developed another alluring product: a tablet computer with a wireless connection and touch screen—all features today of the hot-selling Apple iPad.”
Nuovo, clicking through his old slides like General Magic’s Porat paging through his old sketches, echoed the General Magic CEO’s lament. “Oh my God,” he said. “We had it completely nailed.”
So many people had it—and with the backing of the world’s most powerful electronics’ companies—and yet none of them made it.
When Buxton launched his virtual museum six years ago, he told me that it takes two decades for something genuinely new to become a billion-dollar business.
“If what I said is credible, then it is equally credible that anything that is going to become a billion dollar industry in the next 10 years is already 10 years old,” Buxton said. “That completely changes how we should approach innovation. There is no invention out of the blue, but prospecting, mining, refining and then goldsmithing to create something that's worth more than its weight in gold.”
There is no wizard, no singular genius, who comes up with the Next Big Thing, but something like an evolutionary process. Apple’s iPhone business hit a billion dollars in sales in 2008. By 1998, most of the conceptual work thinking through an iPhone-like device had been done.
*  * *
“The iPhone is a deeply, almost incomprehensibly, collective achievement,” Brian Merchant declares in his new biography of the iPhone, The One Device.
“Thomas Edison did not invent the lightbulb, but his research team found the filament that produced the beautiful, long-lasting glow necessary to turn it into a hit product,” Merchant writes. “Likewise, Steve Jobs did not invent the smartphone, though his team did make it universally desirable. Yet the lone-inventor concept lives on.”
In The One Device, Merchant works through the technical achievements, distributing acclaim in and outside Apple. The glass—Gorilla Glass—was a Corning achievement, which had its roots in a half-century-old research project. The multi-touch screen that allowed the entire surface of the glass to become the user-interface has its origins in the European physics organization, CERN. Merchant quotes Buxton saying his lab at the University of Toronto was working on multi-touch in the early 1980s, and that he’d seen an earlier working system at Bell Labs. The winding multitouch trail continues through the University of Delaware, where an electrical engineer named Wayne Westerman created a multi-touch system for typing to ease his own repetitive stress injuries. Apple eventually bought the company and filed patents on the technology with Westerman’s name on them.
One last example, the lithium-ion battery. Merchant provides a pithy genealogy: “The lithium-ion battery—conceived in an Exxon lab, built into a game-changer by an industry lifer, turned into a mainstream commercial product by a Japanese camera maker, and manufactured with ingredients dredged up in the driest, hottest place on Earth—is the unheralded engine driving our future machines.”
Apple’s supply chain for tin, tantalum, tungsten, gold, and cobalt includes no less than 256 refiners and smelters. Look just at the cobalt in lithium-ion batteries and you find a crazy trail that’d leads back primarily to the copper mines of the Congo, and on to smelters in China. China is the biggest consumer of cobalt and 80 percent of it goes to battery production.
And that’s just the stuff inside the phone. There is also the nearly unbelievable story of how much data capacity the various cell phone providers have added, which requires tower after tower of equipment. From 2007 to 2010, when iPhones were only available with an AT&T wireless connection, data traffic on AT&T’s network went up 8,000 percent. And the growth kept going. A Cisco research study found that from 2011 to 2016, when smartphones became far more prevalent, mobile data traffic grew 18-fold. Now, the country hosts an enormous electronic forest: more than 118 thousand towers are now in operation, according to an industry publication.
Underpinning all of these systems are the incredible leaps in computing power (Moore’s law) and energy efficiency (Koomey’s law) that have been hallmarks of the computing revolution. The chip work, alone, represents hundreds of billions of dollars of R&D, not to mention the work on modems and wireless technology by places like Qualcomm.
Merchant follows computer historian Chris Garcia in calling the iPhone a confluence technology: “There are so many highly evolved and mature technologies packed into our slim rectangles, blending apparently seamlessly together, that they have converged into a product that may resemble magic.”
A general-purpose kind of magic, you might say.
* * *
General Magic will probably be written out of the history books as time goes on. The company itself never amounted to much. But look at a list of the people who worked there, and two names jump off the (very distinguished) list: Tony Fadell and Andy Rubin. Fadell led the hardware team that created the iPhone. Rubin led the team that created Android. While Android dominates by market share (87 percent worldwide), the iPhone dominates the profits made from smartphone sales. In any case, together, the two operating systems we can trace back to General Magic have 99 percent share of the smartphone market.
It’s a perfect narrative. A few people in the Apple orbit have the perfect idea. That seed incubates for 15 years until the technology stack catches up, and then two alums of General Magic finally create the object from that original inspired vision.
The only problem is that Fadell has said the iPhone team tried out all kinds of things. They put a scrollwheel on one proto-iPhone. The team had a months-long battle over whether to include a hardware keyboard before Steve Jobs made the decision to go keyboard-less. And Rubin’s team only ditched its hardware keyboard plans after the iPhone came out. If General Magic did have a map of the future, the legend must have been lost somewhere along the way.
The iPhone happened, and we can mark the world as before-and-after. It unlocked a new era of human-computer interaction and human-human interaction. The iPhone is the ur object of our time. A version of it is attached to the vast majority of adults. We sleep with them. We spend more time with them than our children. The success of other technology companies, media empires, romantic relationships, and political campaigns depends on reaching people through them.
Happy 10th Birthday iPhone. Happy 10th Birthday World That the iPhone Made.


As MIT professor and psychologist Sherry Turkle sees it, students are obsessed with perfection and invulnerability. That’s why they will email her their questions instead of coming to office hours.
“As I get famouser and famouser, I post more office hours, and the numbers [of students attending them] come down,” said Turkle, who researches and writes on peoples’ relationship to technology, during a panel at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic. “What they say is basically, ‘I’ll tell you what’s wrong with conversation, it takes place in real time, and you can’t control what you’re going to say.’” These students are trying to hide their vulnerabilities and imperfections behind screens, she said, and they have a “fantasy that at 2 in the morning I’m going to write them the perfect answer to the perfect question.”
It’s all a sign that we’ve become too dependent on our devices to get us through life, as Turkle sees it. She inveighed not only against email-loving Millennials but also against new moms who would rather sit at home on their phones than go meet each other at the playground.
She points to studies that show that having your phone on the table during mealtimes, even if it’s off, leads to reduced feelings of empathy. To truly turn the tide, Turkle said, there ought to be some no-phone times and places.
They are, uh, most of the times of the day and most places, including when you’re in:
Strict? Yes. This is the price you pay for empathy. This “is not an anti-tech position,” she said. “It’s a pro-conversation position.”
With that, she was interrupted by someone’s iPhone alarm going off.


The function of advertising, wrote Robert E. Lane in The Loss of Happiness in Market Democracies, “is to increase people’s dissatisfaction with any current state of affairs, to create wants, and to exploit the dissatisfactions of the present. Advertising must use dissatisfaction to achieve its purpose.”
The web browser is a dissatisfaction-seeking machine. Every search query we input reflects a desire—to have, to know, to find. Ordinarily, that fact may escape notice. But there are moments when the machine reveals its inhumanity.
Speaking on a panel at the Aspen Ideas Festival, which is cohosted by the Aspen Institute and The Atlantic, Manoush Zomorodi, host of WNYC’s Note to Self, shared a story of a message she received from a listener who’d been following her series on digital privacy. “She was concerned that she might have a drinking problem, and so she went on Google and asked one of those questions, ‘How do you know if you have a drinking problem?’ Two hours later, she goes on Facebook, and she gets an ad for her local liquor store.
“And she left me a voicemail crying, ’cause she was like, ‘You know, it would be one thing if it were even sending me, like, clinics maybe where I could get help. But the fact that that’s how it was targeting me ...’ She felt so betrayed by Facebook, this company with whom she had a very intimate relationship.”
Only 9 percent of adults in the United States say “they feel they have ‘a lot’ of control over how much information is collected about them and how it is used,” according to the Pew Research Center. For most of us, unless we’ve expended the effort to limit the information we share, a vast network of automated snoops constantly monitors our behavior online, and tries to match ads to the fears and desires implicit in our searches and messages.
“You hear these little betrayals of privacy that actually are extremely powerful on a daily basis,” Zomorodi said.
Zomorodi’s co-panelist, the investigative journalist Julia Angwin, spoke about seeing middle-school students plagued by body-image insecurities. “Online, all they get is ads on how to lose weight,” Angwin said. “It preys on their fears. It’s just awful, right? And that is—I don’t know that it’s necessarily targeted advertising, because actually the entire internet is weight-loss ads, as far as I can tell.”
While Google effectively publicizes its aggregate search data—the annual compendium of the year’s queries is always a draw—the service’s value to advertisers comes from precisely the opposite type of data: the personal, strange, incredibly revealing things that each of us is looking for. In a recent episode of the Freakonomics podcast, Seth Stevens-Davidowitz, who wrote his dissertation on what people reveal in Google searches, spoke about how people expose a version of themselves to the search engine that they rarely present in surveys, or even in conversations with friends. “There are lots of bizarre questions—not just questions but statements—that people make on Google,” said Stephens-Davidowitz. “‘I’m sad’ or ‘I’m drunk’ or ‘I love my girlfriend’s boobs.’ Why are you telling Google that? It feels like a confessional window where people just type statements with no reason[able impression] that Google would be able to help.”
If our ad networks have become our confessors, what sort of penance will they extract? What latent or secret desires will they exploit? What could they prod us to do?


The future of privacy in the United States will be shaped by the next generation of citizens and consumers, a rising generation that has never known a pre-Internet world.  
The broadcast journalist Manoush Zomorodi created a segment called The Privacy Paradox on the WYNC show “Note to Self.” Its premise: “ You love the convenience of living online. But you want more control over where your personal information goes.” (The shows dubbed “The 5 Day Plan” are informative. I learned about an additional way that my iPhone was tracking me. And I pay attention to this stuff.)
Zomorodi’s interactions with listeners caused her to think more deeply about the attitudes toward privacy and digital best practices that she ought to pass along as a parent. At a panel Tuesday at the Aspen Ideas Festival, co-hosted by The Aspen Institute and The Atlantic, she expressed chagrin at having chosen Yahoo when creating her child’s first email account––and pride at the child’s subsequent decision to sign up for an account with an overseas email provider that offers strong encryption.
8 Overly Confident, Mostly Pessimistic Predictions About Tech in 2018
Many parents don’t offer any guidance to their children on digital privacy, if only because their children seem so much tech savvy than they are. But Zomorodi’s reflections got me wondering what parents who do think about these matters tell their kids as they begin to use the Internet, or smart phones, or get their first social media account. As Julia Angwin has observed, “if I don’t do anything to help my children learn to protect themselves, all their data will be swept up into giant databases, and their identity will be forever shaped by that information.”
How do you acculturate your children into the digital world?
If you’re a parent who is willing to share, I’d be eager to hear about your approach in your own words. How old is your child? What rules do you lay down? What guidance do you offer, if any? What do you leave up to your child? What do you think of the way they conceive of personal information, digital privacy, and the trail of data they are creating? How would you rate your level of awareness of what they do in digital spaces? What are your biggest worries, challenges, and dilemmas? Email conor@theatlantic.com if you’re willing to share answers to these questions, or any related thoughts.
I expect many parents will benefit from hearing one another’s experiences.


What news do people see? What do they believe to be true about the world around them? What do they do with that information as citizens—as voters?
Facebook, Google, and other giant technology companies have significant control over the answers to those questions. It’s no exaggeration to say that their decisions shape how billions see the world and, in the long run, will contribute to, or detract from, the health of governing institutions around the world.
That’s a hefty responsibility, but one that many tech companies say they want to uphold. For example, in an open letter in February, Facebook’s founder and CEO Mark Zuckerberg wrote that the company’s next focus would be “developing the social infrastructure for community—for supporting us, for keeping us safe, for informing us, for civic engagement, and for inclusion of all.”
The trouble is not a lack of good intentions on Zuckerberg’s part, but the system he is working within, the Stanford professor Rob Reich argued on Monday at the Aspen Ideas Festival, which is co-hosted by the Aspen Institute and The Atlantic.
Reich said that Zuckerberg’s effort to position Facebook as committed to a civic purpose is “in deep and obvious tension with the for-profit business model of a technology company.” The company’s shareholders are bound to be focused on increasing revenue, which in Facebook’s case comes from user engagement. And, as Reich put it, “it’s not the case that responsible civic engagement will always coincide with maximizing engagement on the platform.”
For example, Facebook’s news feed may elicit more user engagement when the content provokes some sort of emotional response, as is the case with cute babies and conspiracy theories. Cute babies are well and good for democracy, but those conspiracy theories aren’t. Tamping down on them may lead to less user engagement, and Facebook will find that its commitment to civic engagement is at odds with its need to increase profits.
The idea that a company’s sole obligation is to its shareholders comes from a 1970 article in The New York Times Magazine by the economist Milton Friedman called “The Social Responsibility of Business Is to Increase Its Profits.” In it, Friedman argued that if corporate executives try to pursue any sort of “social responsibility” (and Friedman always put that in quotes), the executive was in a sense betraying the shareholders who had hired him. Instead, he must solely pursue profits, and leave social commitments out of it. Reich says that these ideas have contributed to a libertarian “background ethos” in Silicon Valley, where people believe that “you can have your social responsibility as a philanthropist, and in the meantime make sure you are responding to your shareholders by maximizing profit.”
Reich believes that some sort of oversight is necessary to ensure that big tech companies make decisions that are in the public’s interest, even when it’s at odds with increasing revenue. Relying on CEOs and boards of directors to choose to do good doesn’t cut it, he said: “I think we need to think structurally about how to create a system of checks and balances or an incentive arrangement so that whether you get a good person or a bad person or a good board or a bad board, it’s just much more difficult for any particular company or any particular sector to do a whole bunch of things that threaten nothing less than the integrity of our democratic institutions.”
Reich said that one model for corporations might be creating something like ethics committees that hospitals have. When hospitals run into complicated medical questions, they can refer the question to the ethics committee whose members—doctors, patients, community members, executives, and so on—represent a variety of interests. That group dives deeply into the question and comes up with a course of action that takes into account various values they prize. It’s a complicated, thoughtful process—“not an algorithm where you spit out the correct moral answer at the end of the day,” Reich said.


When I saw that Google had introduced a “Classic Papers” section of Google Scholar, its search tool for academic journals, I couldn’t help but stroke my chin professorially. What would make a paper a classic, especially for the search giant? In a blog post introducing the feature, Google software engineer Sean Henderson explains the company’s rationale. While some articles gain temporary attention for a new and surprising finding or discovery, others “have stood the test of time,” as Henderson puts it.
How to measure that longevity? Classic Papers selects papers published in 2006, in a wide range of disciplines, which had earned the most citations as of this year. To become a classic, according to Google, is just to have been the most popular over the decade during which Google itself rose to prominence.
It might seem like an unimportant, pedantic gripe to people outside of academia. But Scholar’s classic papers offers a window into how Google conceives of knowledge—and the effect that theory has on the ideas people find with its services.
* * *
Google’s original mission is to “organize the world’s information and make it universally accessible.” It sounds simple enough, if challenging given the quantity of information in the world and the number of people that might access it. But that mission obscures certain questions. What counts as information? By what means is it accessible, and on whose terms?
The universals quickly decay into contingencies. Computers are required, for one. Information that lives offline, in libraries or in people’s heads, must be digitized or recorded to become “universally” accessible. Then users must pay for the broadband or mobile data services necessary to access it.
At a lower level, ordinary searches reveal Google’s selectiveness. A query for “Zelda,” for example, yields six pages of links related to The Legend of Zelda series of Nintendo video games. On the seventh page, a reference to Zelda Fitzgerald appears. By the eighth, a pizzeria called Zelda in Chicago gets acknowledgement, along with Zelda’s café in Newport, Rhode Island. Adding a term to the query, like “novelist” or “pizza,” produces different results—as does searching from a physical location in Chicago or Newport. But the company’s default results for simple searches offers a reminder that organization and accessibility mean something very particular for Google.
That hidden truth starts with PageRank, Google’s first and most important product. Named after Google founder Larry Page, it is the method by which Google vanquished almost all its predecessors in web search. It did so by measuring the reputation of web sites, and using that reputation to improve or diminish its likelihood of appearing earlier in search results.
When I started using the web in 1994, there were 2,738 unique hostnames (e.g., TheAtlantic.com) online, according to Internet Live Stats. That’s few enough that it still made sense to catalog the web in a directory, like a phone book. Which is exactly what the big web business founded that year did. It was called Yahoo!
But by the time Page and Sergey Brin started Google in 1998, the web was already very large, comprising over 2.4 million unique hosts. A directory that large made no sense. Text searches had already been commercialized by Excite in 1993, and both Infoseek and AltaVista appeared in 1995, along with Hotbot in 1996. These and other early search engines used a combination of paid placement and text-matching of query terms against the contents of web pages to produce results.
Those factors proved easy to game. If queries match the words and phrases on web pages, operators can just obscure misleading terms in order to rise in the rankings. Page and Brin proposed an addition. Along with analysis of the content of a page, their software would use its status to make it rise or fall in the results. The PageRank algorithm is complex, but the idea behind it is simple: It treats a link to a webpage as a recommendation for that page. The more recommendations a page has, the more important it becomes to Google. And the more important the pages that link to a page are, the more valuable its recommendations become. Eventually, that calculated importance ranks a page higher or lower in search results.
Although numerical at heart, Google made search affective instead. The results just felt right—especially compared to other early search tools. That ability to respond as if it knew what its users were thinking needed laid the foundation for Google’s success. As the media scholar Siva Vaidhyanathan puts it in his book The Googlization of Everything, relevance became akin to value. But that value was always “relative and contingent,” in Vaidhyanathan’s words. That is, the actual relevance of a web page—whether or not it might best solve the problem or provide the information the user initially sought—became subordinated to the sense of initial delight and subsequent trust in Google’s ability to deliver the “right” results. And those results are derived mostly from a series of recurrent popularity contests PageRank runs behind the scenes.
* * *
Google Scholar’s idea of what makes a paper a classic turns out to be a lot like Google’s idea of makes a website relevant. Scholarly papers cite other papers. Like a link, a citation is a recommendation. With enough citations, a paper becomes “classic” by having been cited many times. What else would “classic” mean, to Google?
As it turns out, scholars have long used citation count as a measure of the impact of papers and the scholars who write them. But some saw problems with this metric as a measure of scholarly success. For one, a single, killer paper can skew a scholar’s citation count. For another, it’s relatively easy to game citation counts, either through self-citation or via a cabal of related scholars who systematically cite one another.
In 2005, shortly after Google went public, a University of California physicist named Jorge Hirsch tried to solve some of these problems with a new method. Instead of counting total citations, Hirsch’s index (or h-index, as it’s known) measures a scholar’s impact by finding the largest number of papers (call that number h) that have been cited at least h times. A scholar with an h-index of 12, for example, has 12 papers each of which is cited at least 12 times by other papers. H-index downgrades the impact of a few massively successful papers on a scholar’s professional standing, rewarding consistency and longevity in scholarly output instead. Hirsch’s method also somewhat dampens the effect of self- and group-citation by downplaying raw citation counts.
H-index has become immensely influential in scholarly life, especially in science and engineering. It is not uncommon to hear scholars ask after a researcher’s h-index as a measure of success, or to express pride or anxiety over their own h-indexes. H-index is regularly used to evaluate (and especially to cull) candidates for academic jobs, too. It also has its downsides. It’s hard to compare h-indexes across fields, the measure obscures an individual’s contribution in co-authored papers, and it abstracts scholarly success from its intellectual merit—the actual content of the articles in question.
That makes h-index eminently compatible with life in the Google era. For one, Google Scholar has been a boon to its influence, because it automates the process of counting citations. But for another, Google has helped normalize reference-counting as a general means of measuring relevance and value for information of all kinds, making the process seem less arbitrary and clinical when used by scholars. The geeks brought obsessive numerism to the masses.
Instead of measuring researchers’ success, Google Scholar’s Classic Papers directory defines canon by distance in time. 2006 is about ten years ago—long enough to be hard to remember in full for those who lived through it, but recent enough that Google had found its legs tracking scholarly research (the Scholar service launched in 2004). Classic papers, in other words, are classic to Google more than they are classic to humanity writ large.
In the academy today, scholars maintain professional standing by virtue of the quantity and regulatory of their productivity—thus Hirsch’s sneer at brilliant one-offs. Often, that means scholarly work gets produced not because of social, industrial, or even cosmic need, but because the wheels of academic productivity must appear to turn. Pressing toward novel methods or discoveries is still valued, but it’s hard and risky work. Instead, scholars who respond to a specific, present conditions in the context of their fields tend to perform best when measured on the calendar of performance reviews.
Looking at papers cited the most in 2006, as Google Scholar’s Classic Papers does, mostly reveals how scholars have succeeded at this gambit, whether intentionally or not. For example, the most-cited paper in film is “Narrative complexity in contemporary American television,” by the Middlebury College television-studies scholar Jason Mittell. Mittell was one of the first critics to explain the rise of television as high culture, particularly via social-realist serials with complex narratives, like The Sopranos. Mittell’s take was both well-reasoned and well-timed, as shows like Deadwood, Big Love, and The Wire were enjoying their runs when he wrote the paper. That trend has continued uninterrupted for the decade since, making Mittell’s article a popular citation.
Likewise, the most cited 2006 paper in history is “Can history be open source? Wikipedia and the future of the past,” by Roy Rosenzweig. The article offers a history and explanation of Wikipedia, along with an assessment of the website’s quality and accuracy as an historical record (good and bad, it turns out). As with complex TV, the popularity of Rosenzweig’s paper relates largely to the accidents of its origin. Wikipedia was started in 2001, and by 2005 it had begun to exert significant impact on teaching and research. History has a unique relationship to encyclopedic knowledge, giving the field an obvious role in benchmarking the site. Rosenzweig’s paper even discusses the role of Google’s indexing methods in helping to boost Wikipedia’s appearance in search results, and the resulting temptation among students to use Wikipedia as a first source. Just as in Mittell’s case, these circumstances have only amplified in the ten years since the paper’s publication, steadying its influence.
This pattern continues in technical fields. In computer vision, for example, a method of identifying the subject of images is the top cited paper. Image recognition and classification was becoming increasingly important in 2006, and the technique the paper describes, called spatial pyramid matching, remains important as a method for image matching. Once more, Google itself remains an obvious beneficiary of computer vision methods.
To claim that these papers “stand the test of time,” as Henderson does, is suspect. Instead, they show that the most popular scholarship is the kind that happened to find purchase on a current or emerging trend, just at the time that it was becoming a concern for a large group of people in a field, and for whom that interest amplified rather than dissipated. A decade hence, the papers haven’t stood the test of time so much as proved, in retrospect, to have taken the right bet at the right moment—where that moment also corresponds directly with the era of Google’s ascendance and dominance.
* * *
PageRank and Classic Papers reveal Google’s theory of knowledge: What is worth knowing is what best relates to what is already known to be worth knowing. Given a system that construes value by something’s visibility, be it academic paper or web page, the valuable resources are always the ones closest to those that already proved their value.
Google enjoys the benefits of this reasoning as much as anyone. When Google tells people that it has found the most lasting scholarly articles on a subject, for example, the public is likely believe that story because they also believe Google tends to find the right answers.
But on further reflection, a lot of Google searches do not produce satisfactory answers, products, businesses, or ideas. Instead, they tend to point to other venues with high reputations, like Wikipedia and Amazon, with which the public has also developed an unexamined relationship of trust. When the information, products, and resources Google lists don’t provide a solution to the problem the seeker sought, the user has two options. Either continue searching with more and more precise terms and conditions in the hopes of being led to more relevant answers, or shrug and click the links provided, resolved to take what was given. Most choose the latter.
This way of consuming information and ideas has spread everywhere else, too. The goods worth buying are the ones that ship via Amazon Prime. The Facebook posts worth seeing are the ones that show up in the newsfeed. The news worth reading is the stuff that shows up to be tapped on. And as services like Facebook, Twitter, and Instagram incorporate algorithmic methods of sorting information, as Google did for search, all those likes and clicks and searches and hashtags and the rest become votes—recommendations that combine with one another to produce output that’s right by virtue of having been sufficiently right before.
It’s as if Google, the company that promised to organize and make accessible the world’s information, has done the opposite. Almost anything can be posted, published, or sold online today, but most of it cannot be seen. Instead, information remains hidden, penalized for having failed to be sufficiently connected to other, more popular information. But to think differently is so uncommon, the idea of doing so might not even arise—for shoppers and citizens as much as for scholars. All information is universally accessible, but some information is more universally accessible than others.


The European Commission has fined Google a record $2.7 billion for the way it promotes its own shopping service over those of its rivals, and ordered the tech giant to change the way it shows the results or face further fines.
“What Google has done is illegal under EU antitrust rules,” Margrethe Vestager, the European Union’s Competition Commissioner, said in a statement. “It has denied other companies the chance to compete on their merits and to innovate, and most importantly it has denied European consumers the benefits of competition, genuine choice and innovation.”
Google in a statement said it “respectfully disagree[s]” with the ruling and will review it “as we consider an appeal.”
The EC said it was up to Google to decide how it would change its search results related to shopping. But if the company fails to comply, it will be ordered to pay 5 percent of Alphabet’s daily worldwide earnings—an amount equivalent to about $14 million each day. Alphabet is Google’s parent company.  
The ruling is the latest run-in U.S. tech companies have had with the EU’s regulators, who regularly target them for antitrust and tax-related issues.  In August 2016, Vestager demanded that Apple repay $14.5 billion in back taxes, calling the incentives the company received in Ireland “illegal tax benefits.”  Apple CEO Tim Cook called that ruling “maddening.” Vestager is also investigating Amazon’s tax practices in Europe and has fined Facebook over its acquisition of WhatsApp. But it’s Google that has felt the brunt of the rulings: Last year the EC announced it was investigating Google mobile-operating system Android on antitrust charges. It also being scrutinized for its advertising, which the bloc says violates its rules.     
The EC’s moves have prompted criticism that European regulators are deliberately targeting U.S. tech companies. The bloc’s regulators reject the accusation. The companies, too, have denied any wrongdoing.






James Berri traveled three hours to Sacramento earlier this month for his first Pride parade, one of hundreds of annual LGBTQ celebrations across America. Berri also talked about the experience on Facebook, reading and reacting to other people’s posts with thumbs-up likes and Facebook’s new rainbow “Pride” emoji. Throughout June, the platform is offering a rainbow flag alongside likes, hearts, and angry faces that people can click on to react to others’ posts and comments. Yet Berri, a 21-year-old transgender artist, is conflicted over the fact that not everyone can use this new rainbow button.
Back in Fresno, Berri wondered how Facebook decides who’s eligible. “Why don’t they have it, too?” he asked, referring to friends sitting with him in a salon in the larger, less-prominent California city. “It makes me confused for my friends.”
One friend disagreed: “Maybe I don’t want my family to actively know that I’m in all of these things because they’re just gonna—they’re not gonna like it.”
As a rare commodity, the Pride reaction has attracted a rainbow hunt among Facebook users. This June, Facebook announced that the feature would be available in “major markets with Pride celebrations” and for people who follow the company’s LGBTQ page. They also announced that the rainbow would “not be available everywhere.” For example, Facebook limits access in countries where LGBTQ rights are politically risky. Yet many Americans, like Berri’s Fresno friends, also missed out.
Is Facebook’s rollout of rainbow flags a case of algorithmic hypocrisy, user protection, or something else? Using their ability to detect people’s location and interests, the company's algorithms are choosing which people get the rainbow flag while hiding it from others. At first glance, this approach looks like it could contribute to the creation of political bubbles, as a feature promoted in progressive cities and less available in the rest of America. If real, these discriminatory political bubbles could constitute a secret kind of “digital gerrymandering,” according to Harvard Law professor Jonathan Zittrain.
Algorithmic political bubbles are hard to detect because they show something different to each person. Only by comparing notes can people map the boundaries of what a platform chooses to show its users. Doing so, when legal, allows independent researchers to detect discrimination and hold platforms accountable for their actions. To find out if Facebook's rainbow Pride reaction was a case of digital gerrymandering, our three-person team—a data scientist, a survey researcher, and an ethnographer of youth social-media practices—conducted an algorithmic audit, asking hundreds of Facebook users in 30 cities to report if they could access the Pride reaction.
Our audit asked two questions. First, are there U.S. cities where everyone is allowed to give a rainbow reaction? Second, do Facebook’s own LGBTQ-interest algorithms predict who has access elsewhere?
By using Facebook’s algorithms, we based our audit on the way that Facebook’s software sees the world. When advertisers publish an ad with Facebook, the company asks them to define the regions, interests, and demographics of the people they want to reach. While the platform’s gender targeting does not allow grouping by LGBTQ identities, their algorithms do infer LGBTQ-interest based on what people like, share, and write about. People can be categorized for their interests in, for instance, “Gay Pride,” “LGBT Culture,” “Pride Parade,” “Rainbow Flag (LGBT),” and “LGBT Social Movements.” Since Facebook allows advertisers to include or exclude people from those categories, we could survey people to discover if LGBTQ-interested people have a different experience on the platform from people that Facebook categorizes as not LGBTQ-interested.
Across 15 states that are home to the largest U.S. cities, we chose a large city per state and paired it with a smaller city elsewhere in the state. Within each city, we used Facebook’s ad targeting to recruit people who the platform’s algorithms think are interested in LGBTQ issues, compared to people who aren’t. We then tested the correlation between LGBTQ interest and access to the Pride reaction.
Among the cities we investigated, an overwhelming percentage of people without LGBTQ interests reported having the pride emoji in New York City, San Francisco, Chicago, Seattle, and Boston. Yet many other cities among the largest 25 in the U.S. were excluded from city-wide access, including Philadelphia, Detroit, Phoenix, and Nashville.
In places without city-wide access, Facebook’s LGBTQ advertising groups correlated strongly with people's ability to use the rainbow reaction. On average, people with LGBTQ interests who responded to our ads were 46 percent more likely than the non-LGBTQ interest group to report having access to the rainbow reaction. It's possible that people in the LGBTQ interest groups received the rainbow because they chose to “like” the LGBTQ@Facebook group, which the company says will unlock the rainbow reaction.
Kristina Boerger, a 52-year-old musician and human-rights organizer from Greencastle, Indiana, was surprised that other people could use the reaction but not her. “It certainly wouldn’t be because Facebook doesn’t know that I am queer,” she said. “That would be one of the first things they know about me.”
Why would Facebook selectively release the pride reaction? When we reached out for comment, a company spokesperson replied with a quote from an early June press release, explaining that Facebook limited access to test the feature, even though 22.7 million people have presumably unlocked the rainbow by liking the LGBTQ page. The platform may also be trying to protect users in parts of the U.S. where they could face harassment. When Facebook encouraged people in 2015 to choose a rainbow profile picture, some administrators of Facebook groups banned any member who made the change.
Limited access to the Pride reaction might also allow Facebook to gain PR benefits from supporting gay rights in some U.S. cities while avoiding scandal elsewhere. Could regional geo-fencing help the company manage public expectations in a polarized political environment? Betsy Willmore, an organizer of PrideFest in Springfield, Illinois, thinks the company is carefully managing its image. “Their intention is not to piss people off,” she said. “And they are legitimizing those that are getting pissed off by it.”
Many Americans could be unaware of Facebook’s public support for LGBTQ rights. After facing election-year pressures, the company might benefit from selective public understanding of its positions. Facebook and its PAC, like many corporations, routinely fund both Democrat and Republican candidates and events. Yet we failed to find a correlation between 2016 presidential election patterns and access to the Pride rainbow. Large cities that supported Trump in 2016 didn’t receive the pride reaction, but neither did many Clinton-supporting cities. If there’s a political pattern to Facebook's decision, we couldn't detect it.
Overall, our audit found that Facebook is doing what it says. The platform has avoided offering city-wide pride reactions in large metropolitan areas that supported Trump in the last election, but LGBTQ-interested people are still able to access the feature on average.
This month, millions of Americans have celebrated Pride with large urban events, in small towns, and across their digital-connected communities. For Berri and his friends in a Fresno salon, the choice to fly a flag online was as consequential as any march. During the conversation, one friend, a queer 19-year-old from Clovis whose name has been omitted to protect them from harassment, decided to “like” Facebook’s LGBTQ page for access to the rainbow reaction. Speaking of disapproving family, they said, “If they’re gonna be pissed off about it, whatever.”


To understand what the world will be like in ten years, it isn’t enough to look back at how different things were a decade ago and presume the differences will be comparable. The pace of technological change is speeding up so quickly, says Astro Teller, who leads the arm of Google that aims at “moonshots,” that one must look back 30 years to experience the same amount of discontinuity we’ll feel ten years hence.
A decade out, he continued, half of all cars on the road will be self-driving (and there would be more but for the fact that today’s cars are too expensive an asset to junk immediately).
The remarks took place Sunday at the Aspen Ideas Festival, which the Aspen Institute co-hosts with The Atlantic. And it prompted a question from moderator Andrew Ross Sorkin.
Trying to imagine a rapid shift toward self-driving cars, Sorkin wondered if the public would be ready as quickly as the technology. “Today there are 35,000 fatalities on the road using cars that we all drive just in the United States,” he said. “What number does that have to go down to that it becomes politically palatable, to the public, that they get in the car, and there may very well be a fatality as the result of a computer?”
In Teller’s view, we’re nearly there already.
“Almost every single person in this room already made that choice, because you got on a plane,” he told the Aspen crowd. “Planes fly roughly 99 percent of the miles that they fly by computer. It's now to the place that it is not safe for humans to fly in a lot of conditions. It's mandated that the computer fly because the computer can do it better.”
He posed this question to skeptics:
If you could have a robotic surgeon that makes one mistake in 10,000, or a human that made one mistake in 1,000, are you really going to go under the knife with the human? Really? We are already at that stage. I think self-driving cars are not in some weird other bucket. We make this decision all the time.
I suspect he is right, if only because more than half of young people already say in surveys that they look forward to self-driving cars, and the ubiquity of ride-sharing services with human drivers is already conditioning car passengers to give over more control. As a counterpoint, however, there are lots of Americans who choose to drive rather than fly, fearing the latter more despite knowing that it is statistically much safer.
With that in mind, I pose the question to readers who shudder at the thought of getting in a self-driving car, even after they are well tested and statistically safer than a car piloted by a human. Are you able to articulate what it is about the self-driving car that scares you? I fear sharks, despite the long odds against one biting me, because they are prehistoric sea monsters who rise up to unexpectedly bite people with razor sharp teeth. Dying by a combination of being eaten alive and drowning seems unusually scary. Why is getting in a self-driving car scarier than getting in a taxi?
The entire opening session of the Aspen Ideas Festival is below, with the Astro Teller interview starting at the 36-minute mark:



Let’s first acknowledge that Gchat was never officially called Gchat. Launched in February 2006, Google named it Google Talk, refusing to refer to it by its colloquial name. For anyone mourning its demise, which the company announced in a March blog post, those names sound awkward, like they’re describing something else. To me, and to many other users, it’s Gchat, and always will be.
The brilliance of Gchat was that it allowed you to instant message any Gmail user within a web browser, instead of using a separate application. This attribute was a lifeline for those of us who, a decade ago, were online all day at our entry-level jobs in open offices, every move tracked on computers that required admin access to download new software, with supervisors who could appear behind you at any time. You could open a separate browser window or a single tab, keeping Gchat running in the background as you ostensibly worked on projects aside from the dramas of your personal life.
Before Gchat, IMing was cloaked in anonymity. On AIM, I dialed up as “thalia587”—inspired by the Greek muse of comedy—after finishing my homework every night in high school. I shed that identity in college, when I’d log onto iChat on my blue iMac as “beulahtengo,” a mash-up of Beulah and Yo La Tengo, two of my favorite bands at the time. My friends knew it was me, but if I’d been a more rebellious youngster, I could have used those handles to IM anyone anonymously.
On Gchat, I was myself. When my invitation from Gmail—which at that point was still invitation-only—arrived right before my college graduation, I jumped on a username that was a variation of my real name, something I could print on a resume.
My college friends all did the same. When we scattered across continents after graduation, just a few months after abandoning Friendster for a new site called Facebook that, as far as we could tell, was most useful for determining who on campus was In A Relationship, Gmail and later, Gchat, helped us stay in touch, filling in the gaps between LiveJournal entries.
* * *
Gchat became another sort of lifeline during my time as a stay-at-home parent. I no longer had an employer standing over my shoulder or restricting what I downloaded. But some of my friends still used Gchat. So once my son whittled his naps down to one a day, guaranteeing a solid chunk of time for me to turn off Raffi and seek adult conversation, I’d crack open my MacBook and launch Gmail, around the time my friends were eating leftovers at their desks, their idle yellow status icons turning green again.
In the middle of my days of unpaid labor, Gchat was my remaining connection to the world of paid work. While I scanned the latest tweets in my feed, I kept a tab open to run Gchat in the background, ready in case someone wanted to talk during the one time of the day I was free.
* * *
Other people used Gchat for its ability to talk “off the record” without saving a transcript of the messages exchanged. As a digital packrat who saves folders of downloads and screen shots in case I need them someday, I never opted to do so. I wasn’t trading secrets or conducting an illicit affair. On the contrary—I loved being able to, for the first time, preserve transcripts of chats with my quick-witted friends, that, short of hiring a stenographer to follow me around, I’d never be fast enough to record in real life.
Only after a decade of trying to capture the ephemeral did I realize my mistake. Now, whenever I use Gmail’s search feature, essential for a service that urges you to keep everything while making it tedious to organize anything, driftwood from some years-old chat floats to the surface. Searching for, say, “Sleater-Kinney” in an effort to retrieve purchased concert tickets bubbles up ancient conversations with a variety of people with whom I’ve discussed the band over the years, only some of whom I’m still friends with.
Reading email exchanges from past relationships that soured is awkward enough. But it’s the old Gchats, conducted in close to real time, that transport me to the past, revealing thoughts I don’t remember having in conversations with people I no longer speak to, people who at the time I could never imagine not knowing. There they are, in stark black sans-serif: my overabundant exclamation points, my unsuccessful attempts at sarcasm, my bad jokes, or worse, responding “lol” to misogynistic ones. All preserved in digital amber, like the insect from Jurassic Park. And just like in the movie, when the past is within such close reach, I can’t leave it alone.
I understand why Google abandoned what it calls Talk. Like Google Reader, the now-defunct RSS feed aggregator that was the first Google product I mourned, Gchat’s limited features are a relic from a simpler time. When Gchat launched, you were either online or offline, with your status indicating your availability. The cultural tide has shifted in the opposite direction—now we’re always on, all messages are instant, and people have embraced the impermanence of digital scraps that briefly remain “on the record” before disappearing forever—think Snapchat, and Instagram Stories.
Unlike with Reader, which Google killed outright, the company has in mind a replacement for Gchat—Google Hangouts, which was stealthily integrated into Gmail in 2013. The company says Hangouts offers “advanced improvements” to Gchat’s “simple chat experience,” and that the vast majority of users who’ve switched over report few differences in functionality. Any tweaks are minor, like the discontinuation of idle and busy status icons in favor of “Last Seen” indicators and a mute feature.
I don’t know that I need Gmail to offer group video calling, photo messages or location sharing. I miss the time when green, yellow, and red bubbles of availability sufficed. We’re already flush with ways to convey the intricate mundanity of our lives, though each new one requires someone younger and younger to explain it to me. Inevitably, something new will change the game again. As an individual user, caught up in the whims of corporations competing for eyeballs and profit, it’s best not to get too attached to any one particular method of communicating.
Hangouts ushers Gchat into the mobile era, allowing asynchronous communication between two or more Gmail users, none of whom need to be sitting behind a computer to send a message. I downloaded the Hangouts app on my phone, but as I examine it in the lineup of other options, its relevance to my own life seems questionable. Rather than using it to contact the people I’m used to Gchatting, I imagine I’ll reach them with another app we’re both already on.
* * *
When I first signed up for Gmail, the mobile world as it exists today was unimaginable. I’d just upgraded from a 30-minute-a-month phone plan, reserved for emergencies, to my first two-year contract. T-Mobile shipped a small box to my first apartment with a shiny black flip phone wrapped in clear plastic. Texting was difficult and expensive. Each one cost about a dime, and if you wanted to type a C, you had to hit the “2” button three times. My mom had a similar phone; until she got the hang of it, she’d type my aunt Marcia’s name as “Mapaga.” The nickname stuck, even as the technology improved.
As new phones and data plans made texting easier and cheaper, and smartphones popularized multimedia messages, like videos, GIFs and emojis, our phones became our go-to sources for instant connection. Now I can send a minute-long video of my son’s first haircut to a group message of out-of-state family members, or show my friends a screenshot of an acquaintance I just saw on TV, and receive an instant response, before the show’s credits roll.
This impulse to share is what Google is trying to leverage through Hangouts, but with a corporate-friendly spin. “We’ve been working hard to streamline the classic Hangouts product for enterprise users,” reads the blog post announcing Gchat's demise. Another post on a different Google blog goes further, highlighting the company’s efforts to “[double] down on our enterprise focus for Hangouts and our commitment to building communication tools focused on the way teams work.” Clearly, people using Gmail for work, not just during work, are increasingly critical as Google competes with Microsoft and Slack for corporate users.
* * *
After Google announced the future of its messaging tools, I could only think about the past. When Google Reader vanished, the accompanying data disappeared forever, so I worried that the formal end of Gchat might mean the loss of those conversations. I searched Gmail’s help section for steps to download an archive of my chats, which number in the thousands, but there’s no easy way to do so. My pulse quickened at the thought of losing all those transcripts I hadn’t read in years, but that I might someday want to read again.
Like the one in which I coached my younger sister, who now has a masters degree and just bought a house with her fiancée, on her college application essay. “I suck at the ‘how did you first learn of Smith College’ question,” she’d lamented. “I was lurking colleges in Princeton Review … and I saw that Smith had ‘dorms like palaces’?”
Or the wistful ones from a friend in the throes of new motherhood, including one in which she contemplated a long car drive with her infant. “What’s the worst that can happen? She cries for three hours? That just sounds like…yesterday.”
Even ones that make me cringe, like one in which a guy who knew I pined for him told me “Serious Talk is a Poor Idea right now” because he was drunk on cheap wine and watching Predator 2 on a Saturday afternoon. “I mean,” he’d typed, “this movie has Bill Paxton in it.”
As with most 21st-century dilemmas requiring an immediate solution, I consulted—what else?—Google search. I discovered a step-by-step method to export all archived chats that looked legit. I followed the instructions and a file started downloading to my desktop with the extension .mbox, something the Mail application could read.
Once complete, I scrolled through the new Mail folder, relieved to see my fleeting correspondence from the previous decade. But as I looked closer, it became clear that the file had only imported the last line of each one of the thousands of chat threads in my Gmail history. Most of them were simple salutations or responses to something unknown—ttyl, haha, brb, lol, you too—stripped of all context through this technological hiccup. But some friends had a habit of never formally ending Gchat conversations, so scrolling through some lines revealed more about what we’d been discussing when one of us had signed off.
            al qaeda clearly has the wrong target
            did you bring the hobo gloves?
            not really wastednot really wasted
            plus i have to find some meat to eat
            she wants help with her Ikea bookshelf
            but I’m Mom Terrible, which is much better than regular terrible
            life is continually amusing
Fortunately, my paranoia was unwarranted. Google’s communications team assured me the company will archive all on-the-record chats, even those predating Hangouts. I’m relieved I can still peek at that time in my life to see how much has changed in a decade, but it’s unsettling to realize that ultimately, it’s not up to me. To keep enjoying the perks of any communication platform, some control over the content must be ceded. Not a comfortable thought, this powerlessness, but technology unspools in one direction only, offering no way to rewind.  


In 2012, the Curiosity rover began its slow trek across the surface of Mars, listening for commands from Earth about where to go, what to photograph, which rocks to inspect. Then last year, something interesting happened: Curiosity started making decisions on its own.
In May last year, engineers back at NASA installed artificial-intelligence software on the rover’s main flight computer that allowed it to recognize inspection-worthy features on the Martian surface and correct the aim of its rock-zapping lasers. The humans behind the Curiosity mission are still calling the shots in most of the rover’s activities. But the software allows the rover to actively contribute to scientific observations without much human input, making the leap from automation to autonomy.
In other words, the software—just about 20,000 lines of code out of the 3.8 million that make Curiosity tick—has turned a car-sized, six-wheeled, nuclear-powered robot into a field scientist.
And it’s good, too. The software, known as Autonomous Exploration for Gathering Increased Science, or AEGIS, selected inspection-worthy rocks and soil targets with 93 percent accuracy between last May and this April, according to a study from its developers published this week in the journal Science Robotics.
AEGIS works with an instrument on Curiosity called the ChemCam, short for chemistry and camera. The ChemCam, a one-eyed, brick-shaped device that sits atop the rover’s spindly robotic neck, emits laser beams at rocks and soil as far as 23 feet away. It then uses the light coming from the impacts to analyze and detect the geochemical composition of the vaporized material. Before AEGIS, when Curiosity arrived at a new spot, ready to explore, it fired the laser at whatever rock or soil fell into the field of view of its navigation cameras. This method certainly collected new data, but it wasn’t the most discerning way of doing it.
With AEGIS, Curiosity can search and pick targets in a much more sophisticated fashion. AEGIS is guided by a computer program that developers, using images of the Martian surface, taught to recognize the kind of rock and soil features that mission scientists want to study. AEGIS examines the images and finds targets that resemble set parameters, ranking them by how closely they match what the scientists asked for. (It’s not perfect; AEGIS can sometimes include a rock’s shadow as part of the object.)
Here’s how Curiosity’s cameras see the Martian landscape with AEGIS. The targets outlined in blue were rejected, the red are potential candidates. The best targets are filled in with green, and the second-best with orange:
When AEGIS settles on a preferred target, ChemCam zaps it.
AEGIS also helps ChemCam with its aim. Let’s say operators back on Earth want the instrument to target a specific geological feature they saw in a particular image. And let’s say that feature is a narrow mineral vein carved into bedrock. If the operators’ commands are off by a pixel or two, ChemCam could miss it. They may not get a second chance to try if Curiosity’s schedule calls for it so start driving again. AEGIS corrects ChemCam’s aim in human-requested observations and its own search.
These autonomous activities have allowed Curiosity to do science when Earth isn’t in the loop, says Raymond Francis, the lead system engineer for AEGIS at NASA’s Jet Propulsion Laboratory in California. Before AEGIS, scientists and engineers would examine images from Curiosity, determine further observations, and then send instructions back to Mars. But while Curiosity is capable of transmitting large amounts of data back to Earth, it can only do so under certain conditions. The rover can only directly transmit data to Earth for a few hours of day because it saps power. It can also transmit data to orbiters circling Mars, which will then kick it over to Earth, but the spacecraft only have eyes on the rover for about two-thirds of the Martian day.
“If you drive the rover into a new place, often that happens in the middle of the day, and then you’ve got several hours of daylight after that when you could make scientific measurements. But no one on Earth has seen the images, no one on Earth knows where the rover is yet,” Francis says. “We can make measurements right after the drives and send them to Earth, so when the team comes in the next day, sometimes they already have geochemical measurements of the place the rover’s in.”
Francis said there was at first some hesitation on the science side of the mission when AEGIS was installed. “I think there’s some people who imagine that the reason we’re doing this is so that we can give scientists a view of Mars, and so we shouldn’t be letting computers make these decisions, that the wisdom of the human being is what matters here,” he said. But “AEGIS is running during periods when humans can’t do this job at all.”
AEGIS is like cruise control for rovers, Francis said. “Micromanaging the speed of a car to the closest kilometer an hour is something that a computer does really well, but choosing where to drive, that’s something you leave to the human,” he said.
There were some safety concerns in designing AEGIS. Each pulse from ChemCam’s laser delivers more than 1 million watts of power. What if the software somehow directed ChemCam to zap the rover itself? To protect against that disastrous scenario, AEGIS engineers made sure the software was capable of recognizing the exact position of the rover during its observations.  “When I give talks about this, I say we have a rule that says, don’t shoot the rover,” Francis says. AEGIS is also programmed to keep ChemCam’s eye from pointing at the sun, which could damage the instrument.
In many ways, it’s not surprising that humanity has a fairly autonomous robot roaming another planet, zapping away at rocks like a nerdy Wall-E. Robots complete far more impressive tasks on Earth. But Curiosity is operating in an environment no human can control. “In a factory, you can program a robot to move in a very exact way over to a place where it picks up a part and then moves again in a very exact way and places it onto a new car that’s being built,” Francis says. “You can be assured that it will work every time. But when you’re in a space exploration context, literally every time AEGIS runs it’s in a place no one has ever seen before. You don’t always know what you’re going to find.”
Francis says the NASA’s next Mars rover, scheduled to launch in 2020, will leave Earth with AEGIS already installed. Future robotic missions to the surfaces or even oceans of other worlds will need it, too. The farther humans send spacecraft, the longer it will take to communicate with them. The rovers and submarines of the future will spend hours out of Earth’s reach, waiting for instructions.
Why not give them something to do?


Stephanie Woodward just wanted to meet her friends for a drink. It was a bar she’d never visited, and she was excited. But going anywhere new for Woodward requires a vetting process. She uses a wheelchair, so building access is always a worry. Research on Google Street View proved promising in this case: A ramp led up into the entryway. That evening, Woodward entered the front door without trouble. But once inside, a single step stood between her and the bar.
It was one step, but for Woodward it may as well have been a wall. “I’m in the front lobby, but to get any sort of service, to even be seen, I had to call the staff,” she says. “I can’t visit this business independently. I’m a strong wheelchair user, but hopping steps is not an easy task.”
Thanks to decades of disability activism culminating in the passage of the Americans with Disability Act (ADA) in 1990, the ramp has become both a tool for accessibility and opportunity for architectural innovation. In the modern built environment, the ramp services people bound to wheelchairs or strollers—making those bodies newly visible in the process. Yet, despite their apparent success, ramps remain contested sites for equal access.
* * *
The ramp is believed to have moved the materials that built the Egyptian pyramids and Stonehenge. The ancient Greeks constructed a paved ramp known as the Diolkos to drag ships across the Isthmus of Corinth. In 1600, Galileo hailed the inclined plane as one of the six simple machines in his work Le Meccaniche.
The ramp’s ability to move objects shouldn’t overshadow its astounding ability to move people. The ramp was retooled as a highly effective “people mover” 300 years after Galileo, in the design of New York’s Grand Central Terminal. The Vanderbilt family, who operated the rail lines the terminal would service, promised New Yorkers an innovative train hub to accommodate newly electrified tracks. They hired the Minnesota-based architecture firm Reed & Stem to get the job done. “Its innovative scheme featured pedestrian ramps inside, and a ramp-like roadway outside that wrapped around the building to connect the northern and southern halves of Park Avenue,” explains the New York Transit Museum.
As design moved forward, engineers built mock-ups at various slopes and, according to the New-York Tribune, studied “the gait and gasping limit of lean men … fat men … women with babies… and all other types of travelers” to determine the ideal grade. It wasn’t a pointless exercise: When the terminal opened in 1913, it was billed as the first great “stairless” station, in the words of Grand Central historian Sam Roberts. The flow of passengers with luggage, strollers and wheelchairs was swift; the “Red Cap” attendees could move their wheeled carriers with ease. The system remains one of the most celebrated in American transit terminals; modern travelers move as easily up and down the ramps, just with less fanfare.
One frequent passenger to Grand Central Terminal was Presidential Franklin D. Roosevelt, who utilized a “secret platform” and elevator to ascend from the lower-level tracks directly up to the Presidential Suite at the Waldorf-Astoria Hotel. At the time, he was hiding his disability and wheelchair from the American public; Grand Central’s ramps were of no use to him. “The first president with a disability was a great advocate for the rehabilitation of people with disabilities,” explains the Anti-Defamation League. “But [he] still operated under the notion that a disability was an abnormal, shameful condition, and should be medically cured or fixed.”
* * *
This sentiment began to change in the 1940s and 1950s. Many World War II veterans returned home with mobility-related injuries. There was little accommodation for wheelchair users at the time, particularly within public spaces. According to a study by the historian Julie Peterson, disabled veterans attending the University of Illinois often hitched rides on service trucks to avoid sidewalks without accessible ramps.
Returning vets planted the seeds for the disability-rights movement, and activism grew alongside the other social movements of the 1960s. Protesters took to the streets, smashing curbs to create their own accessible ramps. In the 1970s, founders of the Independent Living Movement in Berkeley, California, established a wheelchair route through the University of California campus and its vicinity. According to Peterson, they even rolled their own curb ramps, “covertly laying asphalt in the middle of the night.”
Disability activists lobbied Congress and marched on Washington to include their rights in a major affirmative-action bill that would prohibit employment discrimination by the federal government. The so-called Rehabilitation Act was passed in 1973, and for the first time in history, the civil rights of people with disabilities were protected by law. In ensuing years, activists sought to consolidate various pieces of legislation into a single civil-rights statute, much like the 1964 Civil Rights Acts had done for race. But it wasn’t until 1990 that the government passed the Americans with Disability Act, making way for the contemporary, ramped environment. While the law protected the civil rights of disabled Americans, it also required businesses provide accommodations to people with disabilities, and ensured public spaces would receive modifications to become wheelchair accessible.
Architectural, design, and planning practices had to adapt after the ADA. It wasn’t—and still isn’t—an easy shift. Annie Boivin, a designer (and wheelchair user) with the architecture firm Perkins+Will, tells me that the Swiss architect Le Corbusier is partly to blame. In the early 20th century, Le Corbusier created the fictitious character Le Modulor—an able-bodied man, of average height and dimension, around whom Le Corbusier believed standardized design should revolve. Whole cities were designed by the able-bodied men on which Le Modulor was modeled. It was a period with no distinction between what are now known as the two models of disability: medical and social. The medical model views disabled bodies as impaired, the social model points out the environment was never built for them in the first place.
ADA standardization has attempted to remedy the situation. Architects rely on tools like elevators, lifts, and automatic doors. The ramp, the most visible architectural element of the post-ADA period, is also the most important to wheelchair users. Woodward compares the ramp to a “dependable boyfriend who will never leave us.”
Reliable though it might be, the ADA is hardly a cure-all. All buildings constructed or renovated after the law’s passage must follow standards for accessible design, but many older structures still have relics of inaccessibility—like the single-step entrance that kept Woodward from entering the bar. Disability activists I spoke with say that it’s common for building owners to ignore ADA requirements, and pressing them to follow the rules can be difficult. Just this year, a bill was introduced in Congress that would make it harder to sue building owners who fail to remove so-called “architectural barriers.”
* * *
The problem of the single-step entryway inspired the design researcher Sara Hendren to build her own ramp, called Slope Intercept. It can nest, stack, and move on casters. Hendren bemoans an enduring “compliance culture” within architecture, in which ramps are tacked onto buildings with little imagination. Mia Ives-Rublee, who led the disability caucus for the Washington, D.C., Women’s March, says that ramps are often hard to find, placed in the back of buildings, difficult to navigate, or lead to locked doors. She adds that searching for ramps and finding them along the back of buildings “makes you feel like a second-class citizen.”
Those persistent difficulties impact the visibility of disabled bodies in public spaces. “It can become so tiring,” Ives-Rublee tells me. “A lot of people with disabilities won’t go to new places.”
Hendren, eager to show the creative potential that stemmed from one of Galileo’s simple machines, partnered with the dancer Alice Sheppard to design a ramp Sheppard could use onstage with her wheelchair. Sheppard came to the table no stranger to the failure of ramp design. “Why the hell are these eyesores?” she asked. “Compliance-oriented design tends to miss the aesthetic and physical experience of going down a ramp. It should be beautiful, it should participate.”
Attitudes might be starting to change. In 2001, the architect William Leddy was asked to design the Ed Roberts Campus in Berkeley, California. The campus, which opened in 2010, is named after the founder of Berkeley’s Center for Independent Living—the group that installed their own ramps, in the dead of night, throughout the city in the 1970s. It needed to serve as “a symbol of universal design to the community,” according to Leddy. Universal design, he explains, is a design philosophy that strives to create buildings and products usable by all people, to the greatest extent possible, without any need for adaptation.
At the Ed Roberts Campus, the firm designed a helical, bright-red ramp, a dramatic focal point emerging from the middle of the first-floor lobby. At a width of seven feet, there is space for a row of friends or colleagues to traverse it together. Leddy once stumbled upon a wedding ceremony on the ramp, and he vividly recalled a conversation with a wheelchair user who said this was the first building he could move through seamlessly, without asking for any help. The campus, inspired by the design, integrated the ramp into its logo.
Stephanie Woodward is doing her part, too, as Director of Advocacy for New York’s Center for Disability Rights. Upon encountering a non-compliant business—like the bar she couldn’t access—the group writes a letter offering to assist in improving accessibility. If they get no response, they organize protests around the business. “A lawsuit can take seven years to get one ramp in front of a building, one protest could result in a ramp there next week,” she says.
The organization has only started one lawsuit against a non-compliant business, but it’s not how Woodward wants to win her battles. “We shouldn't have to start a lawsuit to have the same access and everyone else,” she says. “We don’t want to sue, we just want to get in.”
This article appears courtesy of Object Lessons.


Few were surprised this morning to learn of the resignation of Travis Kalanick from being the CEO of Uber. The company has endured scandal after scandal, many of which trace back to Kalanick in one way or the other, whether directly as a result of his behavior or his business choices, or less directly as a result of the allegedly toxic and discriminatory culture he helped to create as Uber’s founder. It was easy to see why Kalanick had to go. By removing him, investors and the board are undoubtedly hoping to curtail the onslaught of negative attention and return the company to grow and raise money in peace. But at this point, rebuilding and rebranding Uber will take more than pushing out its leader.
For Uber’s investors and directors, a leadership change is a way of showing that Uber is serious about taking a new direction, and protecting the company’s reported $70 billion valuation in the process. “Uber’s clearly in a situation where small changes, simple policy adjustments, those sorts of things, weren’t going to satisfy the investor community, the customer base, and the employee base,” says Brian Kropp, the head of the human-resources practice at Gartner, a research and consulting firm.
But though Uber’s troubles tended to trace back to Kalanick in some way, they also went beyond him: Last week, at the same meeting that it was announced that Kalanick would be going on a (then-temporary) leave, a different board member made a sexist comment that resulted in his own resignation soon after. “Uber has demonstrated that its problem is not only about a single figure—a reputational cancer that could have been cut away—but that the cancer has infected the rest of the body,” says Audra Diers-Lawson, a professor of public relations strategy  at Leeds Beckett University. “Because the bad behaviors have extended beyond just the CEO, a new negative expectation is probably being formed and this is fundamentally damaging to the company.”
According to Diers-Lawson, Kalanick’s ouster was absolutely the right decision for the company, but it would have been better if it had done so when problems were nascent. “In 2015, the company had the opportunity to genuinely mitigate the damage of his influence on the corporate culture and the company’s reputation as the first wave of this crisis hit the public eye,” she said.  
Uber certainly isn’t the only, or first, tech startup with the problem of a young, brash CEO who creates a unique and disruptive product, but cannot seem to make the leap to successful management. A New York Times article from April dubbed this phenomenon the “bro CEO,” citing examples such as Quirky, a gadget-pedaling platform that raised $185 million before being undone by the questionable behavior of its 20-something CEO and founder, and an HR startup called Zenefits, which was once valued at $4.5 billion but ousted its young male CEO amid both criticisms about the company’s frat-like culture and allegations that the company had engaged in cheating on licensing courses. While the company still exists, it is severely diminished and only a fraction of its former size.
But though CEO problems are somewhat common, Uber is a special case. The company, though it’s never actually turned a profit, is flush with investor cash and wildly popular. But beyond that, the timing of Uber’s drama hits right when the public and investors are more engaged than ever in a conversation about the role of corporate culture in the health of a company and the economy more widely. According to Kropp, in 2010, fewer than 40 percent of company earnings calls took the time to discuss issues such as talent or corporate culture; now that figure has climbed to more than 60 percent. That’s because more people today believe that culture is a critical factor in whether a company can attract the right employees and turn a profit.
Now that Kalanick’s gone, there are still some significant structural challenges for the company to overcome. First, there’s the question of what happens to the upper echelons of Uber’s management. The company has long been without a chief operating officer, a vacancy that many experts, and the Holder report, have suggested desperately needs to be filled. As Quartz reported, some tech startups have filled this role with someone who is all the things that the CEO is not. Facebook’s hiring of Sheryl Sandberg in 2008, for example, is widely seen as a brilliant and effective hire that complemented the company’s CEO Mark Zuckerberg, with Sandberg’s experience and corporate diplomacy tempering Zuckerberg’s relative inexperience and sometimes tough management style. But hiring for the COO role, particularly one who will work as a part of a management team, might be difficult without a CEO.
Replacing any CEO, especially one who is pushed out amidst controversy, is a significant task. Whoever Uber hires or promotes to fill the role could drastically alter operations, or continue to proliferate the same problems. Kropp says that replacing a founder-CEO is often an especially tricky task. In cases where the company is doing well and the CEO is well-loved, it makes sense to promote internally, someone who could potentially continue the current path. For established companies with CEO problems, it can serve to change tacks completely, bringing in an outsider. But for a startup such as Uber, a fairly young company with a CEO who left under very public and difficult circumstances, neither might be quite right. An internal hire may be seen as having accepted and contributed to the existing problems. An outsider may have a difficult time acclimating and understanding which factors make Uber special and unique, and are worth retaining. An outsider may also want to make their mark by completely changing the brand, and that can create corporate and cultural destruction in a different way. The sweet spot, Kropp says, would be someone who has worked at the company before, but then left and was successful elsewhere. And that’s not easy to come by.
In order to create real and lasting change, Uber will need to spend money, Kropp says, not just try to implement one-time changes. “A lot of companies try to talk themselves out of these sorts of cultural challenges. They’ll write memos, send notes, make presentations, saying things need to change. But at the end of the day, if you’re not spending money to try to change the problem, they likelihood that you’re actually able to change the culture is incredibly low.” Forcing the CEO out is certainly a bold step toward change, but Kropp says that alone won’t be enough. Instead, salvaging Uber will require constant investment and training for initiatives that will constantly reinforce the company’s new values, accepted behaviors, and expectations. They’ll need to hire people who align with the new values, and create new roles, such as the one Frances Frei, the new senior vice president of leadership and strategy, inhabits. They’ll also need to expand their budgets to help the people in those new roles build teams and implement big changes that can influence the culture. And they’ll have to implement ongoing methods of measuring progress and sussing out new problems. Without those continuing efforts, eventually muscle memory will kick in and everyone will go back to their same old behavior, new CEO or not.


In a leafy Detroit suburb last March, federal authorities raided a one-story brick house. Their target: Rudy Carcamo-Carranza, a 23-year-old restaurant worker from El Salvador with two deportation orders, a DUI, and a hit-and-run.
The incident would have seemed like a standard deportation case, except for a key detail unearthed by The Detroit News: The feds didn’t find Carcamo-Carranza through traditional detective work. They found him using a cell-site simulator, a powerful surveillance device developed for the global war on terror.
Five days after his election, Donald Trump announced his plan to quickly deport up to 3 million undocumented immigrants—“people that are criminal,” “gang members,” “drug dealers.” How would he do it? How would he deport more people, more quickly, than any of his recent predecessors? The Carcamo-Carranza case suggests an answer: After 9/11, America spent untold sums to build tools to find enemy soldiers and terrorists. Those tools are now being used to find immigrants. And it won’t just be “bad hombres.”
There’s a lot to Trump’s tactics that are very old. Trump seeks to ban Muslim immigrants, spy on mosques, and subject Muslims to extreme border interrogations. In the Chinese exclusion of the late 19th and early 20th centuries, the U.S. government banned most Chinese immigrants, sent investigators to spy on their businesses, and subjected them to extreme border interrogations. In 2017, Trump allies defend the Muslim ban by saying it’s not a Muslim ban, but a geographic ban on people from certain “areas of the world.” In 1917, Congress banned Indian immigrants not by name, but by drawing a box around the region and calling it an “Asiatic Barred Zone.”
Still, there are key aspects to immigration enforcement under Trump that are frighteningly new, albeit some time in the making.
In 2000, when George W. Bush was elected, drones, face recognition, mobile fingerprint scanners, and cell-site simulators—which mimic cellphone towers to intercept phone data—were novel or non-existent. Under the Immigration and Naturalization Service and its successor, Immigration and Customs Enforcement, or ICE, immigration enforcement was a low-tech affair, mostly known for large worksite raids.
Under Barack Obama, ICE went high-tech. At the heart of that shift were biometrics: precise, digitized measurements of immigrants’ bodies. Obama ramped up a Bush-era program, Secure Communities, which sent booking fingerprints from local jails to the Department of Homeland Security, shunting hundreds of thousands of undocumented and legal immigrants, many arrested for minor offenses, into federal deportations.
Previously, federal use of biometrics in the field had focused on Iraq and Afghanistan; with a fingerprint or iris scan, soldiers could tell militants from civilians. In his final years, Obama hit the brakes on Secure Communities—but mobile biometrics trickled down anyways. ICE agents began to stop people in the street to scan their fingerprints. Authorities requested face-recognition searches of Vermont driver’s license photos, looking for visa overstays. Customs and Border Protection sought proposals for face-recognition enhanced drones that, mid-flight, would scan and identify people’s faces.
For all of these technical advances, however, Obama never unleashed his full surveillance powers on immigration enforcement inside the U.S.; most of Obama’s removals took place at the border. Under his Priority Enforcement Program, actions inside the country were primarily targeted against people with criminal records.
Donald Trump brings two fundamental changes. The first is animus. When Trump calls Mexican immigrants drug traffickers and rapists, when he says a judge cannot do his job because of his Mexican heritage, when he implies that Muslim immigrants are party to a vast, Islamist conspiracy (we have to “figure out what’s going on”), it could send a signal to rank and file immigration enforcement.
Second, Trump is starting to use his surveillance arsenal to its utmost legal and technical capacity—within the U.S. Shortly after Carcamo-Carranza’s arrest using a cell-site simulator, a DHS spokesperson clarified that the new “border” drones would not be limited to the border. Instead, the drones would be used wherever there is a “mission need,” a wink at DHS’s claim that the Border Patrol can conduct searches up to 100 miles from the actual border. Simon Sandoval-Moshenberg, a prominent immigration attorney in Virginia, reports that since Trump’s inauguration, every one of his clients arrested by ICE has had their fingerprints scanned before being taken into custody.
Trump’s aggressive use of surveillance is not just about devices. It’s about data. On his fifth day in office, the president issued an executive order on immigration enforcement inside the U.S. Many focused on the fact that he was restoring Secure Communities, the fingerprint-sharing program of the Bush and Obama eras. Fewer noticed the short section, a few lines down, that revoked Privacy Act protections for non-citizens, making it easier for many federal agencies to share with ICE troves of data on legal and undocumented immigrants.
In the era of late-20th-century surveillance—beginning, loosely, with the final years of J. Edgar Hoover and ending with 9/11—there were limits, informal and formal, that focused America’s most powerful surveillance techniques on investigations of the most serious offenses. They were far from perfect, but they were real. The first was cost: It was expensive to “tail” people and track their movements. The second was legal. In 1968, Congress passed the Wiretap Act, which had at its core a simple idea: Wiretaps should be used to catch serious criminals, not petty offenders. You can’t wiretap a jaywalker; you can wiretap a bank robber.
Modern surveillance tools bypass these restraints. They bring the cost of surveillance down to a fraction of the original expense. They outpace federal lawmakers. State legislatures have passed dozens of laws restricting geolocation tracking, cell-site simulators, drones, and other technologies; Congress has passed zero such laws for criminal law enforcement, let alone ICE.
Most people caught in this dragnet will not be like Rudy Carcamo-Carranza. There are not, and never have been, 3 million undocumented criminals. Like his predecessors, most of the people Trump deports will be like Maribel Trujillo Diaz, Arino Massie, or Mario Hernandez-Delacruz: People innocent of any crime. And as Wade Henderson, a dean of the civil rights community, warned, Trump will have, at his disposal, “the greatest surveillance apparatus that this nation, and arguably the world, has ever known.”
In the public eye, Trump’s policies on health care, climate change, and foreign affairs have eclipsed his agenda on immigration. Perhaps people think it only affects immigrants. This is a mistake: Surveillance of immigrants has long paved the way for surveillance of everyone.
Biometrics are no exception. For years, the State Department let the FBI use face recognition to compare suspected criminals’ faces to those of visa applicants. In 2015, State and the FBI announced a pilot program to run these searches against the faces of Americans in passport photos. For years, Congress pressed DHS to use biometrics to track foreign nationals leaving the country. This year, DHS launched face scans through Delta and JetBlue—and both systems scan the faces of foreign nationals and citizens alike.


This year, Netflix will spend something in the realm of $6 billon on original programming, more than any media company apart from ESPN. Amazon is expected to spend $4.5 billion. Even Google, the owners of YouTube, are looking to spend hundreds of millions making TV shows this year. Streaming TV is no longer a fad—it’s a booming industry, one that’s competitive with cable and network television, and supremely attractive to artists who want to make their work with the least interference possible. Now, just as things have gotten crowded, another tech giant is looking to muscle in to the original-TV content world: Apple.
Though Apple, of course, has plenty of money to throw at scripted programming, it’s always seemed cautious about committing to the kind of onslaught that Netflix, Amazon, Hulu and others have engaged in over the past few years. Netflix is now basically offering an entire new season of a television series every week, on top of its original films and slew of comedy specials. Amazon, which provides shows like Transparent to all of its Prime subscribers, has a more democratic process in which it posts pilot episodes online and invites subscribers to watch and review them before ordering them to series.
Netflix Believes in the Power of Thumbs
It’s still unclear what Apple’s strategy is going to be—but the company has hired two of the biggest names in television production to oversee new positions in video programming. Jamie Erlicht and Zack Van Amburg, the longtime presidents of Sony Pictures Television, are joining Apple this summer to begin work on something “exciting,” according to a statement from Apple’s senior vice president Eddy Cue. “There is much more to come,” he teased, providing no other information on their new responsibilities.
It’s pretty easy to guess what comes next. Sony Pictures Television is one of the most respected production companies in the industry, one that’s worked in all genres and mediums. Among the eclectic shows stewarded by Erlicht and Van Amburg since they took the Sony helm in 2005 are Damages, Breaking Bad, Better Call Saul, Drop Dead Diva, Community, Justified, Happy Endings, Hannibal, Masters of Sex, and Underground.
Even before then, they were part of an initial movement toward offering challenging series on basic cable. They worked at Sony (below executive Steve Mosko) when it sold the shows The Shield and Rescue Me to FX, two of the earliest basic-cable programs to attract attention from critics and Emmy voters. That expanded the “Golden Age” of TV beyond premium-channel offerings like The Sopranos and Sex and the City, eventually spurring the rise of streaming networks. In general, the pair have a proven record of teaming up with interesting creators and shepherding projects with the kind of individual touch that stands out—exactly what is needed in the packed world of Peak TV.
The streaming boom is, first and foremost, auteur-driven: Netflix, Amazon, and Hulu attract well-known creators by offering them more artistic freedom than the world of network television. Shows like House of Cards, Transparent, and Orange Is the New Black are sold as distinctive items: not for everyone, of course, but appealing enough to draw in new subscribers eager to watch one particular show. Critical acclaim is only so important; Netflix CEO Reed Hastings long bragged about how the much derided Hemlock Grove attracted more subscribers than House of Cards, at first.
Erlicht and Van Amburg will now be tasked with defining Apple’s new original-TV brand. A statement from the pair said that Apple was looking to bring in programming of “unparalleled quality,” which of course doesn’t mean much; their hiring does seem to indicate that Apple will try to function as more of a traditional TV studio. Some rumors had indicated the company wanted to buy another production company, like Ron Howard and Brian Grazer’s Imagine Entertainment, outright, but instead Erlicht and Van Amburg will build something from the ground up.
Other questions remain: How will Apple present its new shows? Will you need an Apple TV device to watch them? Will the company introduce a subscription service mimicking Netflix, Amazon, and Hulu, and if so, will it buy up the rights to various existing shows and movies to fill out its library? It could also go the route of networks like CBS, offering new shows like The Good Fight and Star Trek Discovery for a smaller monthly fee, or try something else entirely. Other details will come to light soon, but for now, Apple’s big hires suggest Peak TV’s rapid expansion won’t slow down anytime soon.


It came down to money, in the end. Investors backing Uber decided it wasn’t enough that Travis Kalanick announced last week he would take an indefinite leave from his position at the helm of the scandal-plagued company.
He had to go. Now.
This was an “outright rebellion” by shareholders, says Mike Isaac, The New York Times reporter who first reported Kalanick’s surprise ouster overnight. On one hand, it all seemed to have happened rather quickly: Investors delivered a letter to Kalanick while he was on business in Chicago on Tuesday, insisting he step down. Kalanick then spoke with investors and at least one Uber board member, the Times reported, and agreed to resign. (Uber didn’t immediately respond to The Atlantic’s request for comment early Wednesday.)
Viewed another way, Kalanick’s departure was a long, long time coming. Uber has been beset by scandals for most of the year, including a boycott campaign from users, explosive allegations of sexual harassment by a former Uber engineer, a leaked video showing Kalanick arguing with an Uber driver, a federal lawsuit alleging Uber stole a competitors’s design secrets—and those aren’t even all of the big ones. More than once, one unfavorable story about Uber was still prominently in the news when the next PR nightmare materialized.
To onlookers without any stake in the company, Uber’s troubles have been so pronounced as to seem, at times, darkly funny. (“Getting Out Ahead Of This One: Uber Has Apologized In Advance If Anyone Finds Out About Something Called ‘Project Judas,’” said a joke-headline from the satirical website Clickhole, a sister site to The Onion.) In recent weeks, so many of Uber’s senior leaders had either resigned or been fired that, as one mock-suggested on Twitter, a company focused on self-driving cars had become driverless itself. Susan Fowler, the engineer who wrote the explosive blog post about Uber’s toxic culture in February, joked about the possibility of a Hollywood adaptation of the mess: “I would just like to say, just for the record, that I would like to be played by Jennifer Lawrence.”
But the serious questions always came back to Kalanick. It began to seem there was no breaking point. How long could one man remain in charge of a company that seemed to be so badly flailing? And, crucially, what was the public-relations fire-swamp doing to Uber’s $70 billion valuation?
Kalanick’s ouster—and the paradox of how it seems both sudden and drawn out—is a reflection of the forces that rule Silicon Valley. Namely, money, money, and more money. (“Cash flows before bros,” as the tech news site Pando put it last week.)
It was ultimately concerns over the bottom line—not merely the toxic culture, or Kalanick’s trademark hubris, or explosive allegations of sexual harassment, or revelations about Uber’s secret software to evade of law enforcement—that forced Kalanick out. Well, out of his job as CEO, that is. He’ll still be on Uber’s board of directors, and he will retain his control of a majority of Uber’s voting shares.
Which means that, even without Kalanick at the helm, Uber is still the Uber Kalanick built—barring other changes that the company has promised to make. In the meantime, you can be sure Uber employees are watching to see who will succeed their old boss, and what that hire might reveal about the seriousness with which Uber takes its employees’ complaints and its commitment to improving diversity. That remains an open question: The results of Uber’s recent internal investigation yielded superficial and outright bizarre attempts to change the company’s culture—renaming the “War Room” the “Peace Room,” for example, and a request for everyone who attended a company meeting to hug. (Seriously.)
All this calls to mind the old business joke about a CEO who attends a conference on the importance of corporate culture, then barks at the head of HR, “get me one of those things.” The difficulty of shedding a company’s culture—even after shaking up top leadership—was on full display last week. Shortly after Uber published a spate of initiatives it said would help make the company move past its hostile reputation, leaked audio emerged of a board member making a sexist remark at a meeting intended to help with a smooth transition during Kalanick’s then-leave. (Within hours, that board member had resigned.)
Now that Kalanick’s indefinite leave has become definite, Uber finds itself at a crossroads. An Uber without its founding CEO is an Uber untethered to the principles that the company has associated with its rapid growth since it launched in 2009, for better and perhaps for worse. Uber has recently tried to distance itself from some of what it long described as core competencies—qualities like “super pumpedness,” “always be hustling,” and “toe-stepping.” It even announced this week it will allow tips to drivers in a longstanding reversal of a controversial policy.
Neither Kalanick’s departure nor small hints at changes to come are guarantees that Uber’s troubles are over. One of the biggest tests ahead is Uber’s legal battle with Waymo, the driverless-car company that spun out from Google, which claims Uber stole its design secrets.
Eventually, it was investors who answered the question of whether Uber could thrive with Kalanick as CEO. They decided it could not. Next, they will find out if the company can survive without him.


Uber CEO Travis Kalanick has resigned reportedly following a shareholder revolt, capping a tumultuous few months of PR disasters of its own making.
“I love Uber more than anything in the world and at this difficult moment in my personal life I have accepted the investors request to step aside so that Uber can go back to building rather than be distracted with another fight,” Kalanick said in a statement, cited by The New York Times and others. Bloomberg said he’d remain on the company’s board.
Last week Kalanick said he would take an indefinite leave of absence from the company to both work on himself amid a series of controversies as well as to mourn his late mother.
Here’s more from the Times on his resignation:
Mr. Kalanick’s exit [Tuesday] came under pressure after hours of drama involving Uber’s investors, according to two people with knowledge of the situation, who asked to remain anonymous because the details were confidential.
Earlier on Tuesday, five of Uber’s major investors demanded that the chief executive resign immediately. The investors included one of Uber’s biggest shareholders, the venture capital firm Benchmark, which has one of its partners, Bill Gurley, on Uber’s board. The investors made their demand for Mr. Kalanick to step down in a letter delivered to the chief executive while he was in Chicago, said the people with knowledge of the situation.
Tuesday’s move by the controversial CEO is the culmination of months of controversy that began when Kalanick agreed last December to serve on President Trump’s advisory council. But in February, following the president’s executive order on immigration—and public criticism of how Uber reacted to protests against the order—Kalanick resigned from the group.
Controversy followed: There were allegations of a culture of widespread sexism at Uber; a federal lawsuit by Waymo, Google’s self-driving car company, accused the company of stealing its designs, leading ultimately to Uber’s firing of Anthony Levandowski, the central figure in the allegations; and the Department of Justice opened an investigation into a software Uber used to sidestep authorities.
Amid this Kalanick’s own PR troubled mounted: He was filmed berating an Uber driver; it emerged he directed his engineers to camouflage the Uber app so Apple’s engineers wouldn’t see it, allowing the app to secretly track iPhones even after it was deleted; and at least one high-profile departure from the company said “the beliefs and approach to leadership that have guided my career are inconsistent with what I saw and experienced at Uber.”
Ultimately the very attributes that made Kalanick and Uber a darling of Silicon Valley’s investors brought about his downfall. The company has been valued at about $70 billion, and investors feared that any initial-public offering would be imperiled by Uber’s temperamental CEO. As the Times noted:
Taking a start-up chief executive to task so publicly is relatively unusual in Silicon Valley, where investors often praise entrepreneurs and their aggressiveness, especially if their companies are growing fast. It is only when those start-ups are in a precarious position or are declining that shareholders move to protect their investment.
In the case of Uber — one of the most highly valued private companies in the world — investors could lose billions of dollars if the company were to be marked down in valuation.
The result: Kalanick’s resignation.








Weather always makes good news, but the role of climate change in altering weather, especially extreme weather, has made the subject a lightning rod for unease.
A case in point this week: A heat wave is triggering record temperatures in the Southwest. American Airlines reported having canceled up to 50 flights at Phoenix’s Sky Harbor airport, where the temperature has neared 120 degrees in recent days.
Flight cancellations are a perfect foundation for climate-change panic. Commercial air travel is an aspect of ordinary life that touches everyone: Travelers can’t help but worry that their mobility will be impacted by near- and long-term effects of climate change. Much of the coverage tracking the American Airlines cancellations pegs climate change as a direct or indirect cause of the disruption.
That account isn’t wrong. But it doesn’t tell the full story, either.
When I asked, American Airlines cited a 118 degree “maximum operating temperature” for the flights in question, and confirmed that “the heat has impacted some of our regional flights.” But airplanes don’t exactly have such neat and tidy maximum temperatures. Temperature limits might affect avionic systems—the electronics that run communication, navigation, and so forth—but temperatures interact with airplane performance more than they allow or prohibit flight itself. Density altitude, which can change in part based on temperature, affects aerodynamic performance of specific aircraft, but that performance also interacts with other factors, including weight.
“Aircraft engine performance is a function of many things including air temperature,” Glenn Lightsey, an aerospace engineer and colleague of mine at Georgia Tech said. “Hotter days require longer runways and more gradual ascent paths to lift the same weight.” Flight is complex, and it cannot be boiled down to a single number.
The specific aircraft matters, too. American Airlines canceled flights using Canadair Regional Jet (CRJ) equipment. These are the business jets that cover routes between hubs and smaller markets. Larger passenger jets are rated to tolerate higher temperatures, well above those currently being experienced in the American Southwest—after all, planes also fly from Dubai, Riyadh, and Cairo.
The CRJ’s history might play a role in its airworthiness under extreme heat. CRJs are currently made by Bombardier, a multinational transportation manufacturer. Bombardier bought the CRJ line from Canadair, a Canadian state aerospace company. These jets were originally designed for business use, and only later developed to serve the commercial regional jet market. They were not necessarily intended for use in all conditions and markets, nor to be packed full of passengers like they are today. (Bombardier did not immediately respond to a request for comment.)
That circumstance is a consequence of deregulation and consolidation in the American airline market. When regulation demanded that airlines serve all markets, larger jets serviced smaller airports. But as those requirements lifted, and as more airlines merged, even once-thriving hubs like Cincinnati, St. Louis, and Memphis have become minor markets. Airlines began relying on equipment like the CRJ, because they can transport a smaller number of people at a lower cost. Were the affected flights on Boeing large jets instead, there would be no question about their ability to fly.
Speaking of cost, it’s not clear if American itself has issued the CRJ-based flight cancellations, or if they came from the regional partners that actually operate those flights under American-Airlines livery. The business relationships between major carriers and their regional partners are complex. Some are wholly owned subsidiaries, while others—including Mesa and SkyWest, which serve Phoenix on behalf of American—are contracted carriers.
Regional carriers tend to endure financial pressures from their major-carrier partners, some of which might make the effects of high-temperature operation a financial or operational burden. For example, it’s possible that the planes could fly safely above a certain ground temperature, but that the performance data to facilitate that flight is not already available or easily determined. Airlines have to buy the performance charts used to operate flights, and they might determine that it is not worth purchasing them for unlikely or uncommon scenarios.
American Airlines didn’t comment when I asked who had made the determination to cancel flights, or if available performance data had any impact on the decision. At least one other airline, Delta, also canceled a flight operated by SkyWest on CRJ metal scheduled at the peak of Tuesday’s heat, although it isn’t clear if temperature played a role in that decision, or which airline made the call to cancel it.
Grounding flights due to heat in Phoenix clearly is a matter that interacts with climate change. But it’s not solely explained by climate change. Industrial history, public policy, market economics, and other factors exert pressure on the situation, too.
And that applies to more than flight. Climate change is a wicked problem because it interacts with so many other aspects of the lived and built environment. It does the subject a disservice to pretend that it can be summarized by the reading on a thermometer.


What is the best way to describe Rupert Murdoch having a foam pie thrown at his face? This wasn’t much of a problem for the world’s press, who were content to run articles depicting the incident during the media mogul’s testimony at a 2011 parliamentary committee hearing as everything from high drama to low comedy. It was another matter for the hearing’s official transcriptionist. Typically, a transcriptionist’s job only involves typing out the words as they were actually said. After the pie attack—either by choice or hemmed in by the conventions of house style—the transcriptionist decided to go the simplest route by marking it as an “[interruption].”  
Across professional fields, a whole multitude of conversations—meetings, interviews, and conference calls—need to be transcribed and recorded for future reference. This can be a daily, onerous task, but for those willing to pay, the job can be outsourced to a professional transcription service. The service, in turn, will employ staff to transcribe audio files remotely or, as in my own couple of months in the profession, attend meetings to type out what is said in real time.
Despite the recent emergence of browser-based transcription aids, transcription’s an area of drudgery in the modern Western economy where machines can’t quite squeeze human beings out of the equation. That is until last year, when Microsoft built one that could.
Automatic speech recognition, or ASR, is an area that has gripped the firm’s chief speech scientist, Xuedong Huang, since he entered a doctoral program at Scotland’s Edinburgh University. “I’d just left China,” he says, remembering the difficulty he had in using his undergraduate knowledge of the American English to parse the Scottish brogue of his lecturers. “I wished every lecturer and every professor, when they talked in the classroom, could have subtitles.”
In order to reach that kind of real-time service, Huang and his team would first have to create a program capable of retrospective transcription. Advances in artificial intelligence allowed them to employ a technique called deep learning, wherein a program is trained to recognize patterns from vast amounts of data. Huang and his colleagues used their software to transcribe the NIST 2000 CTS test set, a bundle of recorded conversations that’s served as the benchmark for speech recognition work for more than 20 years. The error rates of professional transcriptionists in reproducing two different portions of the test are 5.9 and 11.3 percent. The system built by the team at Microsoft edged past both.
“It wasn’t a real-time system,” acknowledges Huang. “It was very much like we wanted to see, with all the horsepower we have, what is the limit. But the real-time system is not that far off.”
Indeed, the promise of ASR programs capable of accurately transcribing interviews or meetings as they happen no longer seems so outlandish. At Microsoft’s Build conference last month, the company’s vice-president, Harry Shum, demonstrated a PowerPoint transcription service that would allow the spoken words of the presentation to be tied to individual slides. The firm is also in a close race with the likes of Apple and Google to perfect the transcripts produced by its real-time mobile translation app.
Huang believes the point at which transcription software will overtake human capabilities is open to interpretation. “The definition of a perfect result would be controversial,” he says, citing the error rates among human transcriptionists. “How ‘perfect’ this is depends on the scenario and the application.”
An ASR system tasked with transcribing speech in real time is only deemed successful if every word is interpreted correctly, something that largely has been achieved with mobile assistants like Cortana and Siri, but has yet to be mastered in real-time translation apps.  However, a growing number of computer scientists are realizing that standards do not need to be as high when it comes to the automatic transcription of recorded audio, where any mistakes in the text can be amended after the fact.
Two companies—Trint, a start-up in London, and Baidu, the Chinese internet giant with an application called SwiftScribe—have begun to offer browser-based tools that can convert recordings of up to an hour into text with a word-error rate of 5 percent or less.* On the page, their output looks very similar to the raw documents I typed out in real-time during the many meetings I attended as a freelance transcriptionist: at best, a Joycean stream-of-consciousness marvel, and at worst, gobbledygook. But by turning the user from a scribe into an editor, both programs can shave hours off an onerous and distracting task.
The amount of time saved, of course, is contingent on the quality of the audio. Trint and SwiftScribe tend to make short work of face-to-face interviews with the bare minimum of ambient noise, but struggle to transcribe recordings of crowded rooms, telephone interviews with bad reception, or anyone who speaks with an accent that isn’t American or British English. My attempt to run a recording of a German-accented speaker through Trint, for example, saw the engine interpret “it was rather cold, but the atmosphere was great” as “That heart is also all barf. Yes. His first face.”
“We don’t claim that this turnaround in a couple of minutes of an interview like this is perfect,” says Jeff Kofman, Trint’s CEO. “But, with good audio, it can be close to perfect. You can search it, you can hear it, you [can] find the errors, and you know within seconds what was actually said.”
According to Kofman, most of the people using Trint are journalists, followed by academics doing qualitative research and clients in business and healthcare—in other words, professions expected to transcribe a large volume of audio on tight deadlines. That’s in keeping with the anonymized data on user behavior being collected by the developer Ryan Prenger and his colleagues at SwiftScribe. While there is a long tail of users who Prenger speculates are simply AI enthusiasts eager to test out SwiftScribe’s capabilities, he’s also spotted several “power users” that are running audio through the program on almost a daily basis. It’s left him optimistic about the range of people the tool could attract as ASR technology continues to improve.
“That’s the thing with transcription technology in general,” says Prenger. “Once the accuracy gets above a certain bar, everyone will probably start doing their transcriptions that way, at least for the first several rounds.” He predicts that, ultimately, automated transcription tools will increase both the supply of and the demand for transcripts. “There could be a virtuous circle where more people expect more of their audio that they produce to be transcribed, because it’s now cheaper and easier to get things transcribed quickly. And so, it becomes the standard to transcribe everything.”
It’s a future that Trint is consciously maneuvering itself to exploit. The company just raised $3.1 million in seed money to fund its next round of expansion. Kofman and his team plan to demonstrate its capabilities later this month at the Global Editors Network in Vienna. Their aim is to have the transcription of the event’s keynote address up on the Washington Post’s website within the hour.
It’s difficult to predict precisely what this new order could look like, although casualties are expected. The stenographer would likely join the ranks of the costermonger and the iceman in the list of forgotten professions. Journalists could spend more time reporting and writing, aided by a plethora of assistive writing tools, while detectives could analyze the contradictions in suspect testimony earlier. Captioning on YouTube videos could be standard, while radio shows and podcasts could become accessible to the hard of hearing on a mass scale. Calls to acquaintances, friends, and old flames could be archived and searched in the same way that social-media messages and emails are, or intercepted and hoarded by law-enforcement agencies.
For Huang, transcription is just one of a whole range of changes ASR is set to provide that will fundamentally change society itself, one that can already be glimpsed in voice assistants like Cortana, Siri, and Amazon’s Alexa. “The next wave, clearly, is beyond the devices that you have to touch,” he says, envisioning computing technology discreetly woven into a range of working environments. “UI technology that can free people from being tethered to the device will be in the front and center.”
For the moment, however, the engineers behind automated transcribers will have to content themselves with more germane users: the journalist sweating a deadline, or the transcriptionist working out the right way to describe a man being pied in a parliamentary select committee.
* This article originally stated that SwiftScribe is a subsidiary of Baidu. We regret the error.


Something unexpected happened recently at the Facebook Artificial Intelligence Research lab. Researchers who had been training bots to negotiate with one another realized that the bots, left to their own devices, started communicating in a non-human language.
In order to actually follow what the bots were saying, the researchers had to tweak their model, limiting the machines to a conversation humans could understand. (They want bots to stick to human languages because eventually they want those bots to be able to converse with human Facebook users.) When I wrote about all this last week, lots of people reacted with some degree of trepidatious wonder. Machines making up their own language is really cool, sure, but isn’t it actually terrifying?
And also: What does this language actually look like? Here’s an example of one of the bot negotiations that Facebook observed:
Bob: i can i i everything else . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me toBob: you i everything else . . . . . . . . . . . . . .Alice: balls have a ball to me to me to me to me to me to me to meBob: i i can i i i everything else . . . . . . . . . . . . . .Alice: balls have a ball to me to me to me to me to me to me to meBob: i . . . . . . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me toBob: you i i i i i everything else . . . . . . . . . . . . . .Alice: balls have 0 to me to me to me to me to me to me to me to me toBob: you i i i everything else . . . . . . . . . . . . . .Alice: balls have zero to me to me to me to me to me to me to me to me to
Not only does this appear to be nonsense, but the bots don’t really seem to be getting anywhere in the negotiation. Alice isn’t budging from her original position, anyway. The weird thing is, Facebook’s data shows that conversations like this sometimes still led to successful negotiations between the bots in the end, a spokesperson from the AI lab told me. (In other cases, researchers adjusted their model and the bots would develop bad strategies for negotiating—even if their conversation remained interpretable by human standards.)
One way to think about all this is to consider cryptophasia, the name for the phenomenon when twins make up their own secret language, understandable only to them. Perhaps you recall the 2011 YouTube video of two exuberant toddlers chattering back and forth in what sounds like a lively, if inscrutable, dialogue.

There’s some debate over whether this sort of twin speak is actually language or merely a joyful, babbling imitation of  language. The YouTube babies are socializing, but probably not saying anything with specific meaning, many linguists say.
In the case of Facebook’s bots, however, there seems to be something more language-like occurring, Facebook’s researchers say. Other AI researchers, too, say they’ve observed machines that can develop their own languages, including languages with a coherent structure, and defined vocabulary and syntax—though not always actual meaningful, by human standards.
A Computer Tried (and Failed) to Write This Article
In one preprint paper added earlier this year to the research repository arXiv, a pair of computer scientists from the non-profit AI research firm OpenAI wrote about how bots learned to communicate in an abstract language—and how those bots turned to non-verbal communication, the equivalent of human gesturing or pointing, when language communication was unavailable. (Bots don’t need to have corporeal form to engage in non-verbal communication; they just engage with what’s called a visual sensory modality.) Another recent preprint paper, from researchers at the Georgia Institute of Technology, Carnegie Mellon, and Virginia Tech, describes an experiment in which two bots invent their own communication protocol by discussing and assigning values to colors and shapes—in other words, the researchers write, they witnessed the “automatic emergence of grounded language and communication ... no human supervision!”
The implications of this kind of work are dizzying. Not only are researchers beginning to see how bots could communicate with one another, they may be scratching the surface of how syntax and compositional structure emerged among humans in the first place.
But let’s take a step back for a minute. Is what any of these bots are doing really language? “We have to start by admitting that it’s not up to linguists to decide how the word ‘language’ can be used, though linguists certainly have opinions and arguments about the nature of human languages, and the boundaries of that natural class,” said Mark Liberman, a professor of linguistics at the University of Pennsylvania.
So the question of whether Facebook’s bots really made up their own language depends on what we mean when we say “language.” For example, linguists tend to agree that sign languages and vernacular languages really are “capital-L languages,” as Liberman puts it—and not mere approximations of actual language, whatever that is. They also tend to agree that “body language” and computer languages like Python and JavaScript aren’t really languages, even though we call them that.
So here’s the question Liberman poses instead: Could Facebook’s bot language—Facebotlish, he calls it—signal a new and lasting kind of language?
“Probably not, though there’s not enough information available to tell,” he said. “In the first place, it’s entirely text-based, while human languages are all basically spoken or gestured, with text being an artificial overlay.”
The larger point, he says, is that Facebook’s bots are not anywhere near intelligent in the way we think about human intelligence. (That’s part of the reason the term AI can be so misleading.)
“The ‘expert systems’ style of AI programs of the 1970s are at best a historical curiosity now, like the clockwork automata of the 17th century,” Liberman said. “We can be pretty sure that in a few decades, today’s machine-learning AI will seem equally quaint.”
It’s already easy to set up artificial worlds populated by mysterious algorithmic entities with communications procedures that “evolve through a combination of random drift, social convergence, and optimizing selection,” Liberman said. “Just as it’s easy to build a clockwork figurine that plays the clavier.”


When Amazon announced last week that it intended to acquire the upscale grocery chain Whole Foods, it sent shockwaves through the grocery industry. Other grocers’ share prices plummeted. Analysts predicted Amazon would become a “top five” grocer within a few years. Synergies were imagined.
Within all the business chatter, however, a few policy wonks and at least one ally in Congress began to raise the antitrust alarm. They think Amazon is too powerful and might engage in anti-competitive practices.
When Does Amazon Become a Monopoly?
On its face, and judged on the scale of recent jurisprudence, it’s not the most obvious antitrust situation. Amazon has a tiny slice of the grocery market. Whole Foods, large though it may loom in affluent cities, only has 1.2 percent market share. And while Amazon has a dominant position in e-commerce, e-commerce sales remain less than 10 percent of total retail receipts.
But freshman Congressman Ro Khanna, who represents the South Bay, including a big chunk of Silicon Valley, said that the Amazon-Whole Foods deal shows why the government should think differently about mergers. “This as a case study for how we think about antitrust policy,” he said. “It’s the particulars here.”
Khanna said that recent antitrust cases have turned on the question of whether a merger would, in point of fact, immediately raise prices for consumers. Drawing on the work of Matt Stoller and Lina Khan at the New America Foundation, he traced that very narrow test to Robert Bork’s The Antitrust Paradox, which was a move away from decades of more expansive thinking about industry concentration.
In this interview, Khanna calls for a “reorientation” of antitrust decision making to look at a much broader set of concerns, including the effect that a merger could have on jobs, wages, innovation, and small businesses. Whether he can get traction for this idea might be a bellwether for how well the populist wave in U.S. politics can translate into policy reprioritization.
This interview has been lightly edited and condensed.
Alexis Madrigal: Over the last few days, you’ve said that you’re “deeply worried” about the Amazon-Whole Foods deal. What’s drawn your attention to it?
Ro Khanna: I’m very concerned about it, especially the impact that it’s going to have on local grocers. The Walmarts and Targets already are putting pressure on grocers. And that is something in my district: For example, you have Felipe’s Produce in Sunnyvale and Cupertino. These local groceries have already faced so much pressure, and that’s gonna aggravate that situation. As you know, for many immigrant families, grocers are the route into the middle class and the path to wealth creation.
The second challenge to the merger is wages. Whole Foods has a record of paying people really well. One of their founders had a rule that the CEO shouldn’t be paid more than 20 times the average worker. Amazon has not had the same record. You could have downward pressure on wages. And Amazon is a large conglomerate and can leverage suppliers to lower prices, which creates downward pressure on suppliers’ workers wages, too.
If the only metric is “Is this gonna lower prices?”—if that’s the only criteria, that’s debatable. But we also need to consider the impact on local communities and the impact on innovation.
If you look across the economy, if you have multiple players in an industry, you have more customization, more innovation, greater choice for consumers. The more you have consolidation, the less likely you are to invest in innovation. It becomes all about driving down cost and mass production. And that’s not good for innovation in an industry.
Madrigal: The obvious counterargument that people have been making is that Whole Foods controls a teensy tiny fraction of the overall grocery market—1.2 percent,  according to research firm GlobalData.
Khanna: Well, the question is more, what is the potential for it to become? If you look at the past history in Amazon, they were willing to have losses for years to grow their position with the industry. The concern is there could be predatory pricing where they are able to absorb huge losses, which threatens other grocers.
And this has to be viewed not just in its implications for the grocery vertical, but is this amplifying Amazon’s online dominance into the physical retail space? It shouldn’t just be viewed as limited to groceries, but should be viewed in the broader context and Amazon playing into brick and mortar retail.
What I’ve said is that all of that has to be reviewed by the Department of Justice and the Federal Trade Commission to see what is the impact of such a merger given the market share that Amazon does have in many industries.
Madrigal: Who are you thinking through all these issues with? It seems as if there is a group of people in and around D.C. who are rethinking antitrust policy.
Khanna: I think there is a group. There is [Minnesota Congressman] Rick Nolan, who is interested in starting a monopoly caucus in Congress. He’s very concerned about the concentration in industries and the concentration of economic power and what that means for jobs
I’ve talked with [Massachusetts] Senator Elizabeth Warren in the context of defense contractors’ monopolization of the defense industry and what that means for prices.
Then [New America’s] Matt Stoller’s and Lina Khan’s work. Their work has gotten the attention of some of us in Congress that we need to reorient antitrust policy from the Robert Bork days, who made the whole thing a litmus test just about consumer prices, so that if something helps consumer prices, it can’t be an antitrust violation.
The problem is that there wasn’t a consideration of long-term price. Even if short-term consumers benefitted, long-term, there have been cases—airlines, ISP providers—where prices hurt consumers. And it didn’t consider the impact on wages and on local jobs and small businesses, who create most of the jobs. It didn’t take into account the impact on communities. I know what Felipe’s means to the family who created it and the community that it’s in.
Madrigal: Are there specific cases that show the way you think antitrust jurisprudence should be handled?
Khanna: There’s a 1966 Supreme Court case called United States v. Von's Grocery Co. The court blocked a merger between two grocery stores in Los Angeles to prevent a trend towards concentration. And the court said that the dominant theme in Congress was what was considered to be a rising tide of concentration in the American economy. It’s a Supreme Court case. Still good law. So, the courts have looked at economic concentration, particularly in grocery, and that’s a strain of jurisprudence that should be amplified. [From the decision: “The courts must be alert to protect competition against increasing concentration through mergers especially where concentration is gaining momentum in the market.”]
Madrigal: The argument that you’re making seems as if it could be extended to many other technology businesses. The online ad market, for example, is dominated by two companies, Google and Facebook. Are you pushing for tougher antitrust measures across the board?
Khanna: I think we need to have stronger antitrust enforcement. The biggest challenge in the internet space is the ISPs—AT&T, Comcast, Charter—and the fact that we’re paying fives times for access to the internet compared to Europe. There’s only five companies and not much choice because of the extraordinary infrastructure cost. And there is the airline industry. So, in general, we need stronger antitrust enforcement.
What makes the Amazon-Whole Foods deal so problematic is that they are going into an industry with large infrastructure, brick-and-mortar cost, and seeking to build consolidation where we already suffer from consolidation. It’s not like Walmarts and Targets have been good for wages or local grocery stores or niche producers. You already have a problem of concentration and this will just aggravate that.
Madrigal: But you’d like to see the antitrust decision-making overhauled.
Khanna: The big question that some of us in Congress are interested in is how do we reorient antitrust policy to consider all the factors of economic concentration. And consumer price and price discrimination is one factor. But there are also the loss of jobs, the impact on wages, the impact on local small businesses, and the impact on innovation within an industry.
And my point is that especially in a time with declining unionization, if you look at industries where they have numerous competitors and not a few big actors with high market concentration, there’s greater leverage for employees and wages, greater investment in innovation, greater leverage for suppliers, so less downward pressure on wages in supply chains. This is not universally true and there may be exceptions to that, but the FTC and DOJ need to consider all of these factors and make a holistic determination: Is a merger on balance helping wages, jobs, investment for innovation, and prices? Or is it, on balance, not?
And the problem of the current antitrust legislation is that it’s just a litmus test on prices and doesn’t consider all these other equally important factors. And that’s the really the philosophical debate between Brandeis and the consensus all the way from Theodore Roosevelt versus the shift to free-market absolutism that Robert Bork enabled.
Madrigal: Do have any hope that this kind of antitrust transformation will happen during this administration?
Khanna: I hope so. I hope the president is consistent with his campaign promises. He said he’d look at antitrust issues very seriously. Working families, or as he puts it, forgotten Americans, are being shafted by large banks and large corporations. And he campaigned as a populist on antitrust. No one is saying he should arbitrarily make a decision on antitrust, but he should put resources behind the DOJ and FTC to review these things. I have great confidence in the career civil servants at the DOJ and the FTC.
It’s my hope and I’m optimistic that there will be a review.


Updated on June 17, 2017 at 7:51 p.m. ET
The catastrophe wasn’t what it seemed. It was an inside job, people whispered. Rome didn’t have to burn to the ground.
Nearly 2,000 years ago, after the Great Fire of Rome leveled most of the city, Romans questioned whether the emperor Nero had ordered his guards to start the inferno so he could rebuild Rome the way he wanted. They said the emperor had watched the blaze from the the summit of Palatine Hill, the centermost of the seven hills of Rome, plucking his lyre in celebration as countless people died. There’s no evidence of this maniacal lyre-playing, but historians today still debate whether Nero orchestrated the disaster.
What we do know is this: Conspiracy theories flourish when people feel vulnerable. They thrive on paranoia. It has always been this way.
So it’s understandable that, at this chaotic moment in global politics, conspiracy theories seem to have seeped out from the edges of society and flooded into mainstream political discourse. They’re everywhere.
That’s partly because of the richness of today’s informational environment. In Nero’s day, conspiracy theories were local. Today, they’re global. The web has made it easier than ever for people to watch events unfold in real time. Any person with a web connection can participate in news coverage, follow contradicting reports, sift through blurry photos, and pick out (or publish) bad information. The democratization of internet publishing and the ceaseless news cycle work together to provide a never-ending deluge of raw material that feeds conspiracy theories of all stripes.
From all over the world, likeminded people congregate around the same comforting lies, explanations that validate their ideas. “Things seem a whole lot simpler in the world according to conspiracy theories,” writes Rob Brotherton, in his book, Suspicious Minds: Why We Believe Conspiracy Theories. “The prototypical conspiracy theory is an unanswered question; it assumes nothing is as it seems; it portrays the conspirators as preternaturally competent; and as unusually evil.”
But there’s a difference between people talking about outlandish theories and actually believing them to be true. “Those are two very different things,” says Joseph Uscinski, a political science professor at the University of Miami and the co-author of the book American Conspiracy Theories. “There’s a lot of elite discussion of conspiracy theories, but that doesn’t mean that anyone’s believing them any more than they did in the past. People understand what conspiracy theories are. They can understand these theories as political signals when they don’t in fact believe them.”
And most people don’t, Uscinski says. His data shows that belief in partisan conspiracy theories maxes out at 25 percent—and rarely reach that point. Imagine a quadrant, he says, with Republicans on the right and Democrats on the left. The top half of the quadrant is the people of either party who are more likely to believe in conspiracy theories. The bottom half is the people least likely to believe them. Any partisan conspiracy theory will only resonate with people in one of the two top-half squares—because to be believable, it must affirm the political worldview of a person who is already predisposed to believe in conspiracy theories.
“You aren’t going to believe in theories that denigrate your own side, and you have to have a previous position of buying into conspiracy logic,” Uscinski says.
Since conspiracy theories are often concerned with the most visible concentration of power, the president of the United States is a frequent target. “So when a Republican is president, the accusations are about Republicans, the wealthy, and big business; and when a Democrat is president, the accusations focus on Democrats, communists, and socialists.”
“Right now,” he added, “Things are little different. Because of Donald Trump.”
As it turns out, the most famous conspiracy theorist in the world is the president of the United States. Donald Trump spent years spreading birtherism, a movement founded on the idea that his predecessor was born outside the country and therefore ineligible for the nation’s highest office. (Even when Trump finally admitted in September that he knew Barack Obama was born in the United States, he attempted to spark a new conspiracy.)
Now, Trump’s presidency is the focus of a range of conspiracies and cover-ups—from the very real investigation he’s under to the crackpot ideas about him constantly being floated by some of his detractors on the left. Like the implication that Paul Ryan and Mitch McConnell are involved in a money laundering scheme with the Russians, plus countless more theories about who’s funneling Russian money where and to whom.
“The left has lost its fucking mind, and you can quote me on that,” Uscinski said. “They spent the last eight years chastising Republicans about being a bunch of conspiracy kooks, and they have become exactly what they swore they were not. The hypocrisy is thick and it’s disgusting.”
Trump’s strategy in the face of all this drama has been to treat real and fake information interchangeably and discredit any report that’s unflattering to him. It’s why he refers to reputable news organizations as “fake news,” and why he brags about “going around” journalists by tweeting directly to the people. He wants to shorten the distance between the loony theories on the left and legitimate allegations of wrongdoing against him, making them indistinguishable.
Pushing conspiracy theories helped win Trump the presidency, and he’s now banking on the idea that they’ll help him as president. He’s casting himself as the victim of a new conspiracy—a “witch hunt” perpetrated by the forces that want to see him fail.
“Donald Trump communicates through conspiracy theories,” Uscinski says. “You can win the presidency on conspiracy theories, but it’s very difficult to govern on them. Because conspiracy theories are for losers, and now he’s a winner.”
What he means is, conspiracy theories are often a way of expressing an imbalance of power by those who perceive themselves to be the underdog. “But if you control the Supreme Court, the Senate, the House, and the White House, you can’t pull that,” Uscinski says. “Just like how Hillary Clinton can’t, in 1998, say her husband’s troubles are due to a vast right-wing conspiracy.”
Donald Trump may be the most famous conspiracy theorist in America, but a close second is the Infowars talk-radio personality Alex Jones, who has made a name for himself spewing reprehensible theories. He claimed the Sandy Hook Elementary School massacre was a hoax. He says 9/11 and the Boston Marathon bombings were carried out by the U.S. government. Jones has an online store where he peddles products like iodine to people prepping for the apocalypse.
Jones has long been a controversial figure, but not enormously well known. That’s changing. Jones was a vocal supporter of Trump, who has in turn praised Jones. “Your reputation is amazing,” Trump told him in an Infowars appearance in 2015. “I will not let you down.” Jones has claimed he is opening a Washington Bureau and considering applying for White House press credentials.
The latest Jones drama is a three-parter (so far): First, the NBC News anchor Megyn Kelly announced she had interviewed Jones, and that NBC would air the segment on Sunday, June 18. Next came the backlash: People disgusted by Jones blasted Kelly and NBC, saying a man whose lies had tortured the families of murdered children should never be given such a prominent platform. Even Jones joined the fracas, saying he’d been treated unfairly in the interview. Finally, on Thursday night, Jones claimed he had secretly recorded the interview, and would release it in full. (So far, he has released what seems to be audio from a phone conversation with Kelly that took place before the interview.)
Kelly has defended her decision to do the interview in the first place by describing Jones’s popularity: “How does Jones, who traffics in these outrageous conspiracy theories, have the respect of the president of the United States and an audience of millions?” The public interest in questioning a person like Jones, she argues, eclipses any worries about normalizing his outlandish views. The questions are arguably more valuable than the answers.
Many journalists agree with Kelly’s reasoning. But it’s also true, scholars say, that giving a platform to conspiracy theorists has measurable harmful effects on society.  In 1995, a group of Stanford University psychologists interviewed people either right before or right after they’d viewed Oliver Stone’s 1991 film JFK, which was full of conspiracy theories. Brotherton, who describes the findings in Suspicious Minds, says people leaving the movie described themselves as less likely to vote in an upcoming election and less likely to volunteer or donate to a political campaign, compared with those walking in. “Merely watching the movie eroded, at least temporarily, a little of the viewer’s sense of civic engagement,” Brotherton writes.
There are other examples of real-world consequences of giving platforms to conspiracy theorists, too. The conspiracy theory known as Pizzagate, which rose to prominence across websites like 4chan and niche conservative blogs, resulted in a man firing a weapon in a Washington, D.C., pizza parlor.  
The debate over Kelly’s interview comes on the heels of another high-profile conspiracy theory that sent shockwaves through conservative media circles. At the center of that scandal was the TV host Sean Hannity pushing a conspiracy theory about the unsolved murder of a Democratic National Committee staff member and an explosive Fox News report about the murder that was eventually retracted.
* * *
There’s a popular science-fiction podcast, Welcome to Night Vale, developed around the idea of life in a desert town where all conspiracy theories are true. It was first released in June 2012, the summer before a U.S. presidential election, at a moment when Trump was test-driving a new anti-Obama conspiracy. “I wonder when we will be able to see @BarackObama’s college and law school applications and transcripts,” he tweeted the day Night Vale launched. “Why the long wait?”
Joseph Fink, who co-created the podcast, says conspiracy theories today are continuing to function the way they always have. Conspiracy theories are easy ways to tell difficult stories. They provide a storyline that makes a harsh or random world seem ordered. “Especially if it’s ordered against you,” he says. “Since, then, none of it is your fault, which is even more comforting.”
“That said, more extreme conspiracy theories are becoming more mainstream, which is obviously dangerous,” Fink adds. “Conspiracy theories act in a similar way as religious stories: they give you an explanation and structure for why things are the way they are. We are in a Great Awakening of conspiracy theories, and like any massive religious movement, the same power it has to move people also is easily turned into a power to move people against other people.”
Look for the last awakening of this sort in the United States, and you’ll find a sea of similarities—of course, as conspiracy theories tell us, it’s easy to find connections when you go looking for them. Several scholars—people who focus on real conspiracies and people who study conspiracy theories—say the paranoia surrounding the Trump presidency evokes the tumult surrounding the Vietnam War. It’s not that conspiracy theories weren’t, at times, rampant before that. In the 1940s and 1950s, McCarthyism and the trial of Alger Hiss brought with them a surreal spate of hoaxes and misinformation. But it was the assassination of President John F. Kennedy that set off a “general sense of suspicion” that would permeate the culture for some time, says Josiah Thompson, the author of Six Seconds in Dallas: A Micro-Study of the Kennedy Assassination.
“Part of that was, what occurred almost immediately after the assassination, in the years afterward, was Vietnam,” Thompson said, “And over time, a complete loss of confidence in what ever the government was saying about Vietnam. That was not just from the presidency, that was from the government itself.”
This was also a period in which some of the most dramatic ideas that had been disparaged as conspiracy theories turned out to be true. “I am not a crook,” Nixon had insisted. Less than a year later, he resigned. Nixon and Trump are compared not infrequently. Not all presidents are so thin-skinned and antagonistic to the press. Jennifer Senior, reviewing a recent Nixon biography, wrote that “the similarities between Nixon and Trump leap off the page like crickets.” Nixon may have been increasingly paranoid in the final months of his presidency, but he didn't have access to the technology that Trump uses to showcase his conspiracy mindedness.
“With real conspiracy theorists, there’s a kind of—how to put it—almost a dialectic operative,” Thompson says. “Like Trump. You have to keep making wilder and wilder pronouncements over time to hold your audience.”
I tell Thompson the idea Uscinski had shared, about how a person can win the presidency on conspiracy theories, but how they don’t work so well once you’re president. He seems to agree. “In a campaign, what you’re trying to do is affect people’s opinions that will be harvested on one day,” he said. “But governing doesn’t have to do with people’s opinions. It has to do with facts. That’s the real difference.”
When the facts are disputed, of course, you do the best you can with the evidence you can find. Josiah Thompson, the author of Six Seconds in Dallas: A Micro-Study of the Kennedy Assassination, has spent years thinking about all this. When I bring up the enormity of unknown unknowns in people’s understanding of history, Thompson quotes the writer Geoffrey O’Brien:  “‘History unfolds as always in the midst of distraction, misunderstanding, and partially obscured sight-lines,’” Thompson says, reading a line from O’Brien’s 2016 review of the novel Black Deutschland by Darryl Pinckney.*
“And that’s the trouble,” Thompson says. “What may appear as conspiracy theory at one point turns out to be truth at another.”
I ask Thompson how sure he is about the official explanation of the JFK assassination, that there was one gunman who fired on the president’s motorcade from the Texas School Book Depository.
Thompson believes, based on controversial acoustic evidence, that on November 22, 1963, a shot was fired from the grassy knoll at Dealey Plaza—not just from the depository. “The acoustics give us a kind of template for how the event occurred—these two flurries of shots, separated by about six seconds.” (Thompson later clarified that he believes the flurries of shots were 4.6 seconds apart.) He says it was two shots in the second flurry that killed Kennedy.**
Thompson pauses.
“Does that make me a conspiracy theorist?”
He laughs.
“After all these years? What do you think?”
* This article originally quoted Josiah Thompson as having said, “history unfolds, as always, in the midst of distraction, misunderstanding, and partially obscured sight-lines.” After publication, Thompson clarified that he had been quoting the New York Review of Books writer Geoffrey O’Brien, who first wrote the line in his review of the Darryl Pinckney novel Black Deutschland.
** Thompson clarified after publication that he believes the flurries of shots in the Kennedy assassination were 4.6 seconds apart, not six seconds apart. He believes Kennedy was killed by two shots in the second flurry, not by the two flurries of shots.


On Friday morning, Amazon announced it was buying Whole Foods Market for more than $13 billion. About an hour later, Amazon’s stock had risen by about 3 percent, adding $14 billion to its value.
Amazon basically bought the country’s sixth-largest grocery store for free.
As the financial reporter Ben Walsh pointed out on Twitter, this is the opposite of what’s supposed to happen—normally, the acquiring company’s share price falls after a major purchase—and it suggests that investors now believe something odd is going on with Amazon. What could it be?
From a straightforward standpoint, the Whole Foods acquisition means that Amazon will now participate in the $700 billion grocery-store business. Jeff Bezos, the company’s president and CEO, has made grabs at that market for several years—launching Amazon Fresh, a food home-delivery service, and opening several Amazon-branded bodegas in Seattle. Now he owns one of the industry’s best-known brand names.
But Amazon paid a premium to buy Whole Foods, so its new full entry into another industry doesn’t quite explain the rise. Instead, the boost in share price suggests something more ominous: An incredible amount of economic power is now concentrated in Amazon, Inc., and investors now believe it is stifling competition in the retail sector and the broader American economy.
As the country’s biggest online retailer of cleaning supplies and home goods, Amazon competes with Walmart, Target, and Bed, Bath & Beyond. As a clothing and shoe retailer, it competes with DSW, Foot Locker, and Gap. As a distributor of music, books, and television, it competes with Apple, Netflix, and HBO. In the past decade, Amazon has also purchased the web’s biggest independent online shoe store, its biggest independent online diaper store, and its biggest independent online comics store.
And it is successful on nearly all of those fronts. Last year, Amazon sold six times as much online as Walmart, Target, Best Buy, Nordstrom, Home Depot, Macy’s, Kohl’s, and Costco did combined. Amazon also generated 30 percent of all U.S. retail sales growth, online or offline.
Yet Amazon’s dominance extends far beyond retail. It also lends credit, publishes books, designs clothing, and manufactures hardware. Three years ago, it bought Twitch.com, a central player in the $1-billion business of e-sports. And on top of all this, it operates Amazon Web Services, a $12-billion business that rents servers, bandwidth, and computing power to other companies. Slack, Netflix, Dropbox, Tumblr, Pinterest, and the federal government all use Amazon Web Services.
It is, in short, an Everything Store: not only selling goods but also producing them, not only distributing media from its servers but also renting them out to others. And it’s left many experts and analysts wondering: When does Amazon become a a monopoly?
“I think of Amazon as serving almost as the essential infrastructure for the American economy at this point, when it comes to commerce. And that affords Amazon a lot of power and control,” says Lina Khan, a fellow on the Open Markets team at New America, a center-left think tank.
In January, Khan called for Amazon to receive more antitrust scrutiny in an article in The Yale Law Journal.
Historically, many of Amazon’s critics have focused on their Marketplace feature, which allows small businesses to sell their goods through Amazon’s website. Some merchants have accused Amazon of secretly using Marketplace as a laboratory: After collecting data on which products do best, it introduces low-price competitors available through its flagship service.
The Institute for Local Self-Reliance, a nonpartisan advocacy group, has also criticized Amazon for this alleged anticompetitive behavior. “By controlling this critical infrastructure, Amazon both competes with other companies and sets the terms by which these same rivals can reach the market. Locally owned retailers and independent manufacturers have been among the hardest hit,” said a recent report from the group.
But as Amazon has grown across the economy, concern has grown about its strength and power more broadly. “Amazon introduced itself to consumers as a middle man for books,” Khan told me. “But it expanded into becoming a middle man for all sorts of other things—and, for some time now, it has expanded well beyond that middle-man role. As it distributes more content and produces more goods, it’s running into more and more conflicts of interest.”
In short, people have begun to wonder if Amazon is just too big: a company that already controls too much of online retail and that has started to exert its dominance downward into the rest of the supply chain.
Amazon has historically declined to comment about antitrust issues. It recently began searching for a professional economist to consult on competition law concerns. Before November, one of the loudest critics of its market dominance was Donald Trump, who implied on the campaign trail that Jeff Bezos faces “antitrust problems.”
Trump has not yet appointed someone to chair the Federal Trade Commission. The commission must review the acquisition before its completion.
When the United States began to enforce for fairer competition between businesses in the early 20th century, it focused on two kinds of monopolistic organizations: horizontal monopolies and vertical monopolies. In the steel business, for instance, a horizontal monopoly buys up a lot of steel mills, such that other competitors would be boxed out. A vertical monopoly buys up and down the supply chain—acquiring barges and trains and coal mines—essentially barring other companies from competing with it.
Through the middle of the century, regulators focused on business arrangements that could use their control of markets to inflate prices for consumers—cracking down on cartels and more informal price-fixing or market-controlling arrangements—and also on trusts and firms that exercised monopolistic control over their industries.
Starting in the late 1970s, though, legal scholars began arguing that monopolistic behavior could only be measured if it raised prices for consumers. Regulators and judges took notice and opted to pay less attention to overall market structure. And inspired by the corporate raiding and hostile takeovers of the early 1980s, many experts came to believe that bigness in the market would always fix itself.
That consensus has come under attack in the past decade, partly thanks to companies like Amazon. During its first 10 years in operation, Amazon rarely returned profits, and investors allowed Bezos to continue invest in infrastructure and market share. The end result is today’s Amazon: a behemoth company that returns a meager profit, with a stock worth nearly 200 times as much as it earns.
Khan and others have called for the focus to be less on Amazon’s prices and more on its economic power. “Nobody would quibble that Amazon in its current form today is great for consumers. The question is what do things look like going forward?” she asked.
“Americans love to think about their economy as open and competitive,” she said. “But when a growing share of the economy is contained by Amazon, it’s a form of centralization. Owning your own business used to be a way for Americans to build assets and pass on wealth inter-generationally. But if you look at any sector where Amazon is a dominant player—you’d be somewhat crazy to enter there.”
This effect has been true even of large startups. Jet.com opened early last year as a Sam’s Club-style competitor to Amazon, attracting millions in venture capital and plenty of press coverage. And while it grew quickly, it did not last long as an independent company. Walmart bought Jet.com for $3 billion last August.
In the near term, the Whole Foods purchase worries some analysts most because it immediately gives Amazon another infrastructural advantage: more than 400 small warehouses, spread out across some of the most affluent (and Amazon-using) neighborhoods in the United States. They fret that Amazon’s logistical advantages—its network of warehouses, delivery routes, and cargo jets across North America—have given it an unbeatable advantage over other firms. And they argue that advantage was spawned not by technological innovation but by an unending stream of money from Wall Street.
These critics are calling for Amazon to receive a kind of scrutiny now rare in the United States. First, they say, the Securities and Exchange Commission should think hard before approving its purchase of Whole Foods. Second, politicians and regulators should look harder at its structure. They should ask themselves whether its integration is worth its cost—and then either restrict its integrate, essentially breaking Amazon up; or regulating and neutralizing its consolidation.
I asked Khan if she was really thinking about breaking up Amazon. “People have been timid and think that’s an extreme response,” she said. “But I think it’s worth noting that Amazon is expanding in an unprecedented way across our economy.”
She called back to its spiky share price this morning. “Investors know it’s monopolistic,” she told me. “That’s why it’s stock price has been so untethered from profits. The market can register a reality that our laws cannot.”


In 1991, most Americans had not yet heard of the internet. But all of France was online, buying, selling, gaming, and chatting, thanks to a ubiquitous little box that connected to the telephone. It was called Minitel.
Minitel was a computer terminal. It housed a screen, a keyboard, and a modem—but not a microprocessor. Instead of computing on its own, Minitel connected to remote services via uplink, like a 1960s mainframe or a modern Google Chromebook. Terminals were given out, for free, to every French telephone subscriber by the state (which also ran the phone company).
Minitel was a huge success. With free terminals at home or work, people in France could connect to more than 25,000 online services long before the world wide web had even been invented. Many services of the dotcom-and-app eras had precursors in 1980s France. With a Minitel, one could read the news, engage in multi-player interactive gaming, grocery shop for same-day delivery, submit natural language requests like “reserve theater tickets in Paris,” purchase said tickets using a credit card, remotely control thermostats and other home appliances, manage a bank account, chat, and date.
Minitel was decommissioned in 2012 after 30 years of distinguished service. The terminals still functioned, but they could not handle advances in graphics technology, their modems were outdated, and the French had long since moved on to the internet.
But Minitel’s lessons live on, and with new relevance. In the U.S., the Federal Communications Commission’s Open Internet Order made network neutrality law in 2015. But this year, it has come under attack by both cable internet operators and the current FCC chairman. The American implementation of a network derived from Minitel was done by private industry alone. It failed in part because its usage was not regulated by the government. For this reason, it offers a view from the past on why the FCC’s move today might be misguided. It turns out that regulated networks might offer better market opportunities.
* * *
In Silicon Valley, Minitel is often derided as a “backwards system,” the epitome of state centralization and bureaucracy. As the enemy of creative agility. But Minitel was the only online service to reach mass-market penetration to reach mass-market penetration before the late 1990s. Similar systems in the U.S., such as The Source, DowJones, Compuserve, and AOL, were only accessible to the wealthy, geeky few. These American systems were also centralized networks, gated communities where all content was curated by the service provider. They were the computer versions of a cable-television bundle.
By contrast, Minitel didn’t operate as a closed network. Unlike AOL or Facebook, the French state made Minitel an open and neutral platform, which allowed users to connect to privately run services. The state telecom built and operated the underlying infrastructure for the network, and it then allowed anyone to provide services atop it, so long as they registered to do so. Minitel merged state intervention (build and maintain the marketplace) with market-neutrality (anyone can sell legal products and services). That combination catalyzed the boom of Minitel services.
In 1991, France Telecom tried to reproduce their domestic Minitel success in the U.S. through a San Francisco-based venture called 101 Online. It seemed like a match made in heaven. What was then the world’s most successful public computer network was set to meet the world’s hippest tech crowd. For an extra cool factor, France Telecom hired John Coate, the guy who had turned San Francisco’s online bulletin board system, The WELL, into the world’s most influential online community at the time.
As community manager, Coate distributed the little Minitel box to technology and culture leaders such as Alan Lundell of Byte magazine and Mondo 2000. He also took terminals to rave parties such as Oakland’s 1992 Woopy Ball, where hip crowds chatted in digital chill rooms, all in an effort to build a new digital community. The ravers loved it.
But curiosity alone wasn’t enough to spur American adoption of Minitel. It needed a community with intrinsic value. And communities arise when people can meet and exchange goods, services, and ideas freely.
Consider a farmers market. If a city builds and runs one, it must let all types of legal goods to be sold there for the infrastructure to provide maximum value. If citizens can only buy tomatoes and oranges, but not kale nor lettuce, then the value of the market is limited. The same is true of computer networks: If an internet service provider does not let content providers freely access the infrastructure that the user has rented (through a cable or cellphone subscription), the value of the internet as a whole becomes depleted. That’s why the American Minitel failed—and why people should be concerned about ISPs being able to restrict the traffic on broadband and wireless networks.
* * *
On paper, 101 Online understood the distinction between an open, online platform and a cable TV bundle. In a press release, it outlined its mission: to provide Bay Area residents “with a powerful and efficient new way to communicate with each other.” The 101 Online forum, their rebranding of the Minitel network, was said to be “an electronic ‘meeting place’… the first widely available and cheap electronic medium that allows society to talk directly with itself without TV, radio and newspapers acting as a go-between.”
But in practice, 101 Online acted as a go-between for online content. Instead of letting content providers manage their own services, as France Telecom had done, it replicated the dominant model for U.S. online networks of the era: curating all the content itself. Individuals and companies couldn’t plug into the network and sell their content, goods, and services like their French counterparts had done, and as dotcom startups would soon do on the web. Instead, they had to travel to 101 Online’s office in downtown San Francisco, hand a floppy to an operator, and wait for its content to be converted to 101 Online’s proprietary format and uploaded to the company’s server.
As 101’s head of marketing would later admit, “we did not create an ecosystem enabling anyone but us to make money.” It wasn’t anything new for online systems available to Americans at the time. In a 1983, for example, the online version of the World Book Encyclopedia was removed from the CompuServe online platform and replaced with the Grolier electronic encyclopedia—probably the result of some behind-the-scenes licensing deal. The same year, The Source announced a new policy for curating the content on its platform: “new products are receiving close scrutiny based on likely long-term usage rates, as opposed to ‘attention getter qualities.’” It shouldn't come as a surprise that The Source chose to act as a curator, since it was the online arm of Reader’s Digest, itself a master curator. No more than it should startle anyone that AT&T, Comcast, or Verizon—all network providers who also own content companies—might want to do the same with the internet.
What might surprise a proponent of private enterprise over state-run services, however, is that it was private-sector operators who curtailed these early online platforms—whereas in Minitel’s case, the state had remained agnostic. 101 Online used exactly the same technology that the French had implemented across the Atlantic. But when the private sector was fully in charge of administering the platform, it chose to limit rather than facilitate the marketplace.
* * *
When Comcast slows down Netflix, when AT&T forces Apple to block VOIP services, or when Verizon blocks its customers from using tethering apps on their phones, they do nothing different from what CompuServe, The Source, and 101 Online did in the 1980s and 1990s. Acting as private curators, the companies that own the infrastructure through which users seek content can control that content.
By contrast, when an operator treats its infrastructure as neutral, as Minitel had done, its marketplace invites a greater diversity of content and services. That diversity creates more value for users and businesses alike. In Minitel’s case, openness and neutrality were guaranteed by the state, an agent bound by a duty to act in the public interest.
Today, cable internet lobbies have claimed that further regulation of internet services inevitably leads to an internet doomsday that will “increase consumer costs, slow investment and innovation and cause years of uncertainty.” Senator Ted Cruz has even called net neutrality “Obamacare for the internet,” urging against online services that “operate at the speed of government.”
But Minitel offers an unusual historical endorsement for state intervention in commercial computer networks. Government involvement can benefit both public and private enterprise, whereas unbridled reliance on the private sector can restrict innovation, as it did for 101 Online.


I bring tidings from the frontier of airplane Wi-Fi. I experienced faster internet with my own two thumbs aboard a 757! I did a series of speed tests and received between 17 and 27 megabits per second while also flying through the sky. This is streaming-Netflix-while-streaming-Spotify territory—and a far cry from the measly speeds one can attain on most current flights.
Airplane Wi-Fi is technically measured on a scale from makes-you-want-to-throw-your-computer-out-of-the-plane to makes-you-want-to-throw-yourself-out-of-the-plane. That’s because it is hard to direct data to a plane 30,000 feet in the air. You need an antenna pointed down at ground towers or up at a small number of bandwidth-limited satellites. And yet some internet is better than none. And so despite the speed, Wi-Fi aboard planes remains indispensable for work travelers, who might even pay $40 for the privilege of sipping bits through a tiny kinked straw for four or five hours.
As you might expect, the airlines and the people who provide tech components to them are trying to make that experience better. Our sighs have made their way to research team’s ears. Our muttering has led to and starred in PowerPoints.
Here’s the good news: Yes, it will get better. Even though it’s hard, no fewer than three companies are willing to challenge physics to bring you your streaming video.
And now for the bad news: It’s gonna take a while before you get to use that better Wi-Fi.
* * *
My adventure to the future of airplane Wi-Fi began at a part of the San Francisco airport that I’d never seen: Signature Flight Support, the name bold and rich, the planes private. I’d been summoned to the place by the noted thermostat-maker Honeywell, which has been a major supplier of airplane components for decades. They had invited a group of journalists for a ride aboard their hardware-testing plane, and by the time I arrived, the presenters and audience were gathered into a small conference room at the terminal.
Before we flew, Kristin Slyker, Honeywell Aerospace’s VP of “Connected Aircraft” wanted to fill us in on the current aviation industry. The pitch was familiar but compelling: They don’t make the thing you know (the airplane), but they make the thing you know better.
Imagine any Boeing or Airbus: Each one is a conglomeration of parts created by all kinds of different suppliers. Honeywell, for example, does a good business with auxiliary power units, a small engine that lets the pilots turn on the air conditioning, even when the main jet engines aren’t running. Not something you probably think about everyday, but Honeywell has 36,000 of these units flying around on airplanes right now.
They also supply wheels and electrical power infrastructure, cabin pressure control systems and LED lighting, radios, sensors, gyroscopes, accelerometers, and a bunch of other things.
And this is where the Wi-Fi comes in. What is Wi-Fi, from the plane’s perspective?
It’s a system with a few pieces:
And those kinds of things—antennas and radios and aircraft systems—are what Honeywell makes. If airplane Wi-Fi is gonna get better, it’s gonna be through improvements in these parts and the systems that animate them.
So, I boarded the Honeywell plane with the other journalists. On the outside, it looks like any other plane—tube-y, white. But inside, the wall and ceiling panels have been torn out, so all the plane’s cabling is visible. There are bundles of cords spanning the length of the craft, and most of them would be present in any commercial plane, not just this specially kitted out one.
It’s a striking visual: A plane is a flying data-generating machine. Honeywell wants to tie all of these data streams together. They want to sell airlines components, the service for assessing the data that the sensors on those parts generate, apps for pilots, and a host of other services they have rolled under Slyker in a business they call the “Connected Aircraft.” And the charismatic avatar of their capabilities and the coming change is their next-gen internet technology, the Wi-Fi, which they have branded Jetwave.
Honeywell believes that Jetwave is faster and more reliable than the current products from competitors like GoGo, the major incumbent in airplane Wi-Fi, and ViaSat, a general satellite internet company that's moved strongly into the airplane space.
Those companies are also working on next-generation products, like Gogo’s 2ku and ViaSat’s Gen-2 set of equipment. Both of those companies have a stronger current presence in the commercial airplane Wi-Fi market, especially in North America, so it could be a while before you use this particular technology on your flight from Birmingham to Dallas.
Nonetheless, all of the next-generation systems make use of satellite technology, so what Honeywell showed me is a good indicator of how you’ll eventually stream Netflix from a window seat.
* * *
On board, I couldn’t help but follow the cords towards the back of the airplane until I ran into Stéphane Klander, who is an engineer working on Jetwave. He wore a red Honeywell polo shirt and straight leg khakis. He’s blunt, funny, less media-trained than his executive colleagues.
Slyker introduced him as their “communications specialist.” So, eyes twinkling, Klander announced with a light and unplaceable accent, “I know everything about communication. What do you want to know?”
This was the man who could tell me what I wanted to know about how the internet works aboard airplanes, and why it could get better. He invited me back to his workstation on the plane and I strapped into the three-point harness that would keep me in the leather seat next to Klander.
He pulled out his reading glasses and began to run through his standard checks. It was automatic; these guys spend 400 hours a year flying around in this thing.
Honeywell’s new system works with Inmarsat’s satellite system. Built to provide maritime communications, Inmarsat is one of the big, established satcom companies. Honeywell works with three newish Inmarsat-5 satellites in geostationary orbit. That means—with small corrections—the satellites orbit in time with the Earth’s rotation, which to an observer on the ground (or in a plane) means that they appear fixed in the same place in the sky.
The satellites can only cover a slice of the Earth, so when a plane flies out of the range of one, it has to hand-off to the next satellite.
On our flight out of San Francisco, Klander is excited to gather data on how their system performs on the Pacific Ocean Region (POR) satellite, which they’ve tested less than the Atlantic and Indian regions.
Inmarsat calls this particular satellite array Global Xpress. Each satellite uses dozens of beams that provide up to 50 megabytes per second of bandwidth to each commercial plane flying down on Earth.
A key feature of the satellite system is that it operates in the Ka part of the spectrum—that’s roughly 30 gigahertz. That’s extremely high frequency, technically! Consider that cell phones use frequencies in the low thousands of megahertz. FM radio uses spectrum around 85 to 110 megahertz. And AM radio is measured in kilohertz.
Even for satellite communication norms, the Ka band is substantially higher frequency than the Ku band used by many satellites today. And there is one of those massive and acrimonious debates that you’ve never heard of going on between Ka and Ku proponents pushing their technology for higher-throughput satellite communications.
Ku is tried and true. Ka is newer, less congested, and allows for smaller, more powerful data beams.
Suffice to say that the Inmarsat/Honeywell solution is the only global Ka band option. Gogo’s next-generation tech uses the Ku band and Viasat’s second-generation tech uses Ka, but with less blanket coverage.
Honeywell, obviously, stands with the Ka-band proponents for one reason: bandwidth.
“These high frequencies allow us to pass a lot of traffic,” Lander said.
But there’s a tradeoff. Higher frequency means shorter wavelengths, and that tends to make it more difficult for the radio waves to travel long distances. (Longer waves tend to travel farther in the atmosphere: see AM versus FM radio).
So, to use this Ka high-frequency spectrum, the Honeywell radio transmitter applies 25 watts of power. Your phone’s transmitter might have 1 or 2 watts of power.
“This would fry your brain if it were used in a cell phone,” he said. “There is a safety distance we have to respect. No one can be on top of the airplane while we are transmitting. It would harm you physically if you were standing in front of it.”
In fact, the Jetwave amplifier’s 25 watts of power is roughly comparable to the transmission power of a ground-based cell tower.
* * *
Lander pulls up a photograph of the antenna that's sitting on top of the plane under a little dome. Installing this specialized hardware and getting it approved to use with each class of aircraft is one reason why Wi-Fi took so long to roll out and why it will probably be a few years before you regularly encounter the next generation technology.
The antenna itself is beautiful. It can rotate to pick up signals close to the horizon (zero degrees) and up to straight above it (90 degrees). If your hand was the antenna, a karate chop would pick up horizon signals, rotate your hand to an open palm, and now you’d be picking up signals from directly above you.
The antenna needs that flexibility because it needs to be able to lock onto the data beams that the Inmarsat spacecraft are projecting onto the Earth no matter where the plane is relative to the satellite.
Klander showed me the detailed map of how the beams fall on the Earth. And it’s complicated! “Right under the satellite, they are very circular, but as we go along the world,” he said, “they are more elongated because there is more room for the RF [radio frequency] to use the space.”
He didn’t want me to take a photograph of the map he’d pulled up on one of his three screens, but it looked like a zoomed-in version of this map from an Inmarsat presentation:
We were in region 82 of the POR. But if we’d flown into region 81, the internet traffic would be seamless. That's because the plane has two receivers, one of which would lock onto 81, the new adjacent beam, so there’d be no interruption of service when we left 82.
While Honeywell makes the antenna controller, the antenna, the radio, and the modem, Inmarsat’s network software actually makes the decisions on when and how to switch beams and satellites. Inmarsat’s constantly trying to load balance between satellites and beams, as well as effecting seamless transfers from region to region.
They also have to manage the ground operations, called satellite anchor stations. The only one in the United States is located in Lino Lakes, Minnesota (pop. 20,948), and serves as one of two redundant spots for the Atlantic region. (For completists, Inmarsat’s other ground stations are in Canada, New Zealand, Greece, and Italy.)
“The antennas there need to be massive with a lot of gain, so they can receive fine information and push a lot of data,” Lander said.
* * *
And all this, the whole system, is what will make your airplane Wi-Fi experience a little bit better. New satellites up in space. New antenna designs down here. Huge satellite dishes in little towns in Minnesota. Eight hundred hours of troubleshooting and testing by a guy named Stéphane. Raging blog debates about frequency bands. Deals with various regulators and aircraft maker and airlines.
It takes a lot of time to build a new system like this or Gogo’s or ViaSat’s. But man, are we getting close.
And indeed, after we lift off, I open up a speed test app at 9,000 feet and get 17 megabytes per second, despite the bevy of aviation and travel journalists who are doing the exact same thing at that moment. Soon, actual commercial travelers on some Lufthansa flights will carry out this same action, and probably be equally delighted.
As we circled around California, everyone happily Instagramming and tweeting and speedtesting, I couldn’t help but think about that famous of indictment of our era: Louis C.K. talking about how everything is amazing and yet no one appreciates it, using the example of airplane Wi-Fi.
But what is most amazing about our era is not the fact of the whining about modern inconveniences, but rather that the whining is read as demand by ever-more-nimble companies. Wi-Fi grumbling initiates global, technological change enacted by some of the most potent corporate forces on the planet. Our expletives, muttered over peanut bits and stray pretzel salt, have caused satellites to be launched.


It would be hard to fashion a more exquisite snare for a man like Donald Trump than the modern, institutional presidency. Just five months into his term, he appears trapped by its constraints—and the harder he tries to escape them, the more thoroughly entangled he becomes.
On Thursday morning, President Trump again lashed out at the “bad and conflicted people” investigating him for obstructing justice. “They made up a phony collusion with the Russians story, found zero proof, so now they go for obstruction of justice on the phony story,” he tweeted. But to take Trump’s charge at face value is to read it as an indictment of his own blunders. Trump is claiming that there was no underlying wrongdoing, but his decision to fire his FBI director sparked the appointment of a special counsel who’s now exploring whether it was a criminal act. This, he says, is a purely self-inflicted wound—or, as a senior administration official told The Daily Beast, “The president did this to himself.”
Trump is, in many ways, a man out of his time. He ran his business as he is attempting to run the presidency, as a 19th-century style entity, built around its proprietor. But the federal government has long-since evolved into a modern bureaucracy, an institution Trump appears to have neither the experience nor the patience to successfully operate.
Trump’s business empire sprawls into hundreds of LLCs and licensing agreements, but at its core, it takes a familiar form: the proprietary firm. Built around its founder, generally branded with his name, its reputation intertwined with his, and its affairs directly under his management—this was the dominant form of business in the United States until the final decade of the 19th century.  
Who Can Tell the Emperor When He Has No Clothes?
There are real advantages to that arrangement. It confers flexibility, allowing leaders to react to shifting circumstances with speed, and to take risks without fear of being second-guessed. It mitigates the agency problem—the danger that professional managers will prioritize their own interests. And to the proprietors, it offers the satisfactions of independence; they control their own destinies.
It is a form almost perfectly adapted to play to Trump’s strengths, and cover his weaknesses. As a CEO hired by an independent board, he might not have survived the bankruptcies of some of his businesses, a string of failed ventures, constant lawsuits, or the other setbacks of his career. But the Trump Organization is his to do with as he pleases, and if not all the risks he chooses to take, the loopholes he exploits, the deals he strikes, or the ventures he launches have succeeded, enough have paid off to preserve and expand the fortune he inherited.
But America is now a century or more past its managerial revolution—the heyday of the proprietary firm is gone, displaced by the corporate bureaucracy. It swept through industry in the Great Merger Wave at the turn of the 20th century, and through the federal government in the decades that followed. Bureaucracies offer a solution to the challenge of scale; they create rules and procedures, and the corps of professionals who populate bureaucracies abide by them, allowing business to be performed in a predictable fashion, even between actors with no personal relationship. And they bring with them their own set of costs and benefits, requiring the surrender of a degree of autonomy and flexibility in exchange for stability and scale, and putting systems ahead of individual initiative.
The presidency itself underwent a similar transition. In the 19th century, as the historian Brian Balogh has argued, it was already tremendously powerful—but operated indirectly, through third-party entities like state and local governments. It was a great deal like the Trump Organization—a relatively small staff, multiplying its influence by striking deals with larger entities that had the personnel to put its plans into action. As late as 1900, William McKinley had just 13 staffers working directly for him in the White House. Today, the Executive Office of the President claims more than two thousand personnel; the federal government, more than two million.
It is a world against which Trump seems to rebel at every turn. He refuses to empower his chief of staff to create a rule-bound White House, preferring instead to pit advisers against each other in a more freewheeling style. He insists on reaching directly down to subordinates, instead of moving through the hierarchy—calling the acting head of the National Park Service to complain about a photo, tweeting his defense of his travel ban instead of issuing statements through his press office, and meeting with the FBI director instead of the attorney general.
And each time, he has only worsened the trouble he sought to address, or created new problems for himself—producing mockery of his exaggerated crowd-size claims, court injunctions against his executive orders, and now an investigation for obstruction of justice. His repeated defeats seem only to deepen his anger as he strains against bureaucratic rules, the thousands of Lilliputian strings that not even presidential giants can snap.
There is no better example of this than the memo, that humblest—most spare and restrained—of literary genres. It was born in early modern Europe, but came of age during America’s managerial revolution, a key technology of the administrative bureaucracy. They institutionalized memory, making decisions legible to those not personally present, and creating records of conversations that could be referenced later by other bureaucrats.
In the process, they became the key tool of bureaucratic warfare. As MIT’s JoAnne Yates wrote, in her history of the memo as a managerial genre, “This extension of documentary communication also reflected more specific political motivations. As companies grew, allegiances to and rivalries between departments created friction, and each side of each dispute wanted to document its view of the issue.”
When Trump breached protocol and met alone with Comey, the FBI director went back to his office, and wrote a memo. When Trump called NSA Director Mike Rogers to criticize the intelligence community’s conclusions on Russian interference and to pressure him to publicly disavow the possibility of collusion, The Wall Street Journal reports, the NSA’s deputy director dutifully recorded it in a memo. There’s no indication the president ordered his own staff to document his view of these conversations. His preferred form of written communication to subordinates is the personal message scrawled with a Sharpie, not the memorandum dictated to file.
Memo to the president: You’re losing this game because you don’t understand its rules.
There’ve been no shortage of op-eds and talking heads telling the president that he disregards the institutional constraints at his considerable political and legal peril. Many White House advisers have reportedly told him the same, even as others have egged him on. But Trump obtained his clearest warning of all from the man from whom he was least inclined to receive it.
A week after Trump’s election, President Obama held a news conference. He was asked, among other things, about his handling of detainees in Guantanamo Bay. “Keep in mind,” he replied, “that it's not just a matter of what I'm willing to do.” Obama was no minimalist when it came to executive authority; he had pushed the powers of his office further than any of his recent predecessors, often in ways that federal courts would later strike down for exceeding his authority, or failing to follow prescribed bureaucratic procedures. And along the way, he had been humbled to discover that there are limits to what a president can achieve through sheer force of will and disregard of precedents.  
“One of the things you discover about being president is that there are all these rules and norms and laws and you've got to pay attention to them,” he continued. “And the people who work for you are also subject to those rules and norms. And that's a piece of advice that I gave to the incoming president.”
Maybe it’s a lesson each president needs to learn for himself.


A buried line in a new Facebook report about chatbots’ conversations with one another offers a remarkable glimpse at the future of language.
In the report, researchers at the Facebook Artificial Intelligence Research lab describe using machine learning to train their “dialog agents” to negotiate. (And it turns out bots are actually quite good at dealmaking.) At one point, the researchers write, they had to tweak one of their models because otherwise the bot-to-bot conversation “led to divergence from human language as the agents developed their own language for negotiating.” They had to use what’s called a fixed supervised model instead.
In other words, the model that allowed two bots to have a conversation—and use machine learning to constantly iterate strategies for that conversation along the way—led to those bots communicating in their own non-human language. If this doesn’t fill you with a sense of wonder and awe about the future of machines and humanity then, I don’t know, go watch Blade Runner or something.
What an AI's Non-Human Language Actually Looks Like
The larger point of the report is that bots can be pretty decent negotiators—they even use strategies like feigning interest in something valueless, so that it can later appear to “compromise” by conceding it. But the detail about language is, as one tech entrepreneur put it, a mind-boggling “sign of what’s to come.”
To be clear, Facebook’s chatty bots aren’t evidence of the singularity’s arrival. Not even close. But they do demonstrate how machines are redefining people’s understanding of so many realms once believed to be exclusively human—like language.
Already, there’s a good deal of guesswork involved in machine learning research, which often involves feeding a neural net a huge pile of data then examining the output to try to understand how the machine thinks. But the fact that machines will make up their own non-human ways of conversing is an astonishing reminder of just how little we know, even when people are the ones designing these systems.
“There remains much potential for future work,” Facebook’s researchers wrote in their  paper, “particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.”


Amazon is rumored to be mulling a purchase of Slack, the fast-growing corporate chat platform. A deal could give Slack a valuation of $9 billion, according to a report from Bloomberg.
It’s no surprise that tech giants have taken interest in Slack, with its elegant, user-friendly interface that keeps employees ever-connected to work via their smartphones. The startup has enjoyed extraordinary growth since its 2013 debut. It now has about 5 million daily users, including more than 1 million paying users. As of last year, Slack claimed 77 Fortune 100 companies among its clients. It’s quite popular in American newsrooms—including at The Washington Post, which the Amazon CEO Jeff Bezos owns. (The Atlantic has used the platform since 2014.) Last year, Microsoft was considering scooping up Slack for itself. Instead, it launched a competing collaborative group-chat service called Teams in November. Even for Silicon Valley’s most formidable companies, a multibillion-dollar acquisition isn’t taken lightly.
So in the broader sense, the idea that Amazon is flirting with a Slack takeover is just another chapter in the Office Wars of Silicon Valley. Remember, Amazon is not just an online retailer; Amazon Web Services is already a major force in the corporate world. But Amazon’s possible interest also signals some clear ways of thinking about how the company wants to position itself in the future. (Neither Amazon nor Slack returned requests for comment early Thursday morning).
For one thing, it’s easy to see why Amazon would want to add a popular corporate communications tool to its suite of offerings to Amazon Web Services customers, Amazon’s widely used cloud-computing platform. “Widely used” may be an understatement. AWS, with its global server farms, is the backbone of the commercial web. It reported an eye-popping $12.2 billion in sales last year, and more than $3 billion in profit.
But the more intriguing explanation of Amazon’s interest has to do with one of the company’s even bolder visions of the future. Amazon is one of the major players in the fight for dominance in the realm of voice-activated artificial intelligence. And it seems to be doing pretty well so far. As of January, Amazon had sold more than 11 million of its Echo home device, according to a report by the investment banker Morgan Stanley. (You may know the Echo as “Alexa,” which is the word users must say to get the device’s attention.)
Silicon Valley is, at the moment, obsessed with this technology. The consensus is that voice—the commonly used shorthand for voice-activated devices and other conversational machines—promises to be the most transformative technology since the smartphone.  
Lex, the conversation interface that powers the Echo, already has a Slack integration. But many of Google’s apps integrate with Slack, too, and Google is one of Amazon’s major competitors—in voice and in general. So at a pivotal point in the fight to rule potentially world-changing technology, why wouldn’t Amazon leap to acquire a communications platform that its top competitors could be eyeing for themselves? Besides, where Amazon’s peers—Microsoft, Google, and Facebook—all already have robust communications platforms among their key offerings, Amazon has none. Acquiring Slack could change that—and could position Amazon for shaping the way workers use voice-activated technology at a time when Slack is already considered a possible email slayer.
Just think of what bringing all that work data to the Echo’s capabilities would mean for the worker—and the further blurring of any remaining line between work and home. A person could be driving to work, or cooking dinner, and dip back into work through a conversation with the Echo:
“Alexa, read me the conversation in The Atlantic’s technology Slack channel since I last checked it.”
“Alexa, dial-in to the 10 o’clock conference call for me.”
“Alexa, download and play that podcast that my colleague recommended the other day.”
“Alexa, let me know if my boss Slacks me.”
We’re only in the very earliest stages of imagining how voice will transform the workplace, but it makes sense that Amazon—which aims to lead the way—would look to Slack as it vies for superiority.


When Major Spencer Williams was ordered to “shut down shop and move out” of Afghanistan in 2005, he closed his final message from the field as he always did—quoting a long-dead historian. “Plant yourself not in Europe but in Iraq; it will become evident that half of the roads of the Old World lead to Aleppo, and half to Bagram.”
Williams made up one-third of the U.S. Army’s historical field staff in Afghanistan—a team directed to cover the breadth of the country, vacuuming up media, documents, and oral histories so that some future soldier or academic could better understand the course of the war and how one might respond to circumstances should they arise again. The war offered more than enough material to keep Williams and the others busy, but they weren’t able to communicate the importance of that work to those leading the mission in the country. Following a command from the highest-ranking officer in Afghanistan, the historians were on their way out of Kabul.
It would be almost two years until another team came back into the country. In that time, units cycled in and out of the war zone, each adding a small drop to the bucket of the longest U.S. military engagement in history. Whenever a unit prepared for the return trip home, its soldiers collected their gear, prepared the site for the following unit, wiped local servers, and allowed the details of the prior months to fade.
The gap in record-keeping created by the absence of Williams and his team—and the difficulties they faced in demonstrating this record’s value while overseas—illustrate a common headache of 21st century historians. Though technology has made more sources than ever available to color, verify, and explore history, determining those sources’ value remains the task of a trained human eye. And in the case of the Army, support for that eye has declined as its necessity increased.
* * *
Williams and his team were sent to Afghanistan in order to collect material for inclusion in the Army’s Tan Books, histories of the wars in Iraq and Afghanistan written by the Center of Military History. CMH’s official histories, which have existed in various forms since the years following World War I, trace narratives of wars using material only a massive enterprise like the Army can synthesize. They offer maps of documents flying through the Pentagon and overseas bases everyday, and examine points in which the arc of history bends ever so slightly. The sprawling nature of the histories also means that precise connections between events can be found and applied.
Jill S. Russell, a visiting professor of national security and strategy at the U.S. Army War College, recalls digging through niche volumes of CMH’s  The United States Army in World War II, also known as the Green Books. “There were things that guys coming back from the Philippines were writing that are being repeated by guys coming back from Afghanistan almost literally word-for-word,” she says. Russell cited modifications made by soldiers in World War II to improve the ease of use of heating elements in mountainous terrain. Upon deploying to similar terrain in Afghanistan, some soldiers rediscovered the modifications made by their predecessors.
Modern wars have upended some of the most basic factors CMH relied upon in order to write histories in a consistent way. “Since 1991, the operations we’ve gotten into—Somalia, Haiti, Bosnia, Kosovo—were more complicated scenarios,” says Shane Story, a retired Army helicopter pilot who’s overseeing the creation of the 30-some eventual volumes of the Tan Books. “Even more since 9/11, both Afghanistan and Iraq, they’re not clean, neat narratives. ... [In] the best work that you write, you know what the last word is before you start. We don’t know what the last word is.”
Story and Jerry Brooks, who’s responsible for collections in the field, speak candidly of the problems CMH historians face as they work through the Tan Books creation, a process that will take decades to complete. Once, there were scores of clerks responsible for maintaining detailed records of units. With the rise of computers and software, the Army believed that “everybody would be their own records manager,” and broadened this responsibility to average soldiers, Brooks says. “Well, they failed to take into consideration that people are lazy.”
In theory, these individual record-keepers hand off documents and other sources of information to field historians when requested. However, in addition to the two years after Spencer Williams and his team left Afghanistan a decade ago, the country hasn’t had a field historian from the Army since 2014. Historians dealing with Iraq have avoided these same gaps, but still suffer from fewer personnel in the field than in past wars. Brooks cites Vietnam, where U.S. headquarters in Saigon alone maintained a staff of more than 20 historians. Nowadays, as a result of caps on the number of troops deployed, historians oftentimes find themselves on the first flights back home.
“We did a disservice to the American public, because we did not put enough historians downrange to collect the documents. And now we're reaping what we’ve sowed,” Brooks says.
By his estimate, more records from the wars in Iraq and Afghanistan have been wiped or otherwise lost than remain in existence. But he admits the exact figure is difficult to determine due to the few historians in the field. A concept document that lays out the plan for the Tan Books supports this estimate, listing the amount of data lost in the wars as “incalculable.”
* * *
Both Brooks and the Tan Books concept document exclude the multitude of informal records created by soldiers during the rise of the digital age—emails, Facebook posts, and blogs, to only name a few. These materials are both an opportunity and a source of unanswered methodological questions, according to Michael Gisick, a Ph.D. candidate at Australian National University who studies the use of social media by U.S. service members during the war on terror. “I suspect there are great quantities of images, emails, and other digital narratives tucked away on the hard drives of America’s veterans,” he says, explaining that these sources of information can “jog the memories of participants, spur questions, and illustrate events,” as well as counter popular narratives that might have arisen in political circles and at higher levels of the military.
Even for historical organizations that have been quick to adopt online technology, the role these sources can play is still being tested and explored, says Russell Riley, who co-chairs the Presidential Oral History Program at the University of Virginia’s Miller Center. The program he helps direct, whose texts are of a similar scale to the Army’s official histories, is examining how to use social media as an additional springboard for the Obama administration histories, and he expects they’ll play an even larger role once they begin considering the Trump presidency. He believes that as officials of all stripes have become more reluctant to put thoughts down in writing, social media can offer a less filtered means of communication.
CMH notes that more than 150 terabytes of data have been collected for cataloging and eventual use in the Tan Books. The value of that data remains to be seen. Detachments sent into the field by CMH, though trained to recognize historically significant documents and conduct interviews with people of interest, act as “vacuum cleaners,” as Brooks has called them, rather than analysts. When Brooks briefed a group of Army historians, he included a photo of a Burger King fry holder tagged as coming from a restaurant set up after the invasion. “Technically Correct but Useless” reads the title—a classification that might only increase by including the Facebook posts of young, deployed soldiers.
Despite these limitations, Story remains convinced that the “glass is half-full” for the Tan Books. Like the famed Civil War historian and author Shelby Foote, he believes a good historian “can build the story around just one document,” if necessary. But finding those capable historians is still a challenge. Right now, Story can dedicate one individual to work on an actual text of the Tan Books at any one time. He’s pleased with the work already released, a 70-page pamphlet on the 2007 to 2008 surge of American troops in Iraq, but notes that a complete text stemming from the work is still six or seven years down the line from release.
“In certain respects, I think of ourselves as having a role comparable to the government accounting office,” Story says, arguing that even first drafts of the texts play an important role. Russell and Brooks agree, saying that these early efforts, which few expect to be complete or even immutable, will drive further research and questions as more becomes known and unclassified about the wars.
Even the best histories don’t provide a means of knowing everything ahead of time, Story says. But “when circumstances arise, done well, at least, I think it can give you a means of judging them with a little more acuity.”


In America, terrible acts of violence are often met with handmade signs, bunches of flowers, and teddy bears. Makeshift memorials to shooting victims are flooded with these sorts of objects, and they spring up reliably wherever a horrifying event has occurred.
After the elementary school shooting in Newtown, Connecticut, in 2012, town officials had to rent a warehouse to house all of the gifts and donations that people sent.
But in the era of social media, there’s another place where people gather to react to acts of violence: on the Facebook page of the suspected perpetrator.*
Within minutes of news reports identifying the man who shot at members of Congress while they played baseball this morning, people had found what appeared to be the suspect’s Facebook profile. And then they flooded it with comments. In the span of about an hour, there were hundreds of messages. And then thousands. Things like:
“Rot in Hell buddy!!!”
“Have fun ‘fighting the man’ in prison loser!”
“Another Bernie demorat. You suck and deserve to die in prison”
“I hope you survive bro. I hope you get waterboarded for weeks, then tossed in solitary for life.”
“Dang he's getting his page blown up lmao good”
“You will not win this war”
“Enjoy prison or death”
And, like so many areas where people congregate online, the string of comments devolved quickly into vitriol and political arguments—prompted in part by the large photo of Bernie Sanders atop the man’s profile page. Then there are the memes, so many of them: silly images of Donald Trump, cartoon frogs, disparaging images of Barack Obama and Hillary Clinton, plenty of objectionable language—pretty much what you find on every other social platform.
Among the comments, there is the occasional attempt at unity: “Sad, Sad situation,” one person wrote. “We are a very divided Country. Some way, some how we need to attempt to find middle ground and move forward.”
“There is an unbelievable amount of hate in these comments,” another person wrote. “It's really sad to see that this is what society has boiled down to.”
Such comments were, for the most part, either mocked outright or ignored. And when President Donald Trump announced in remarks from the Rose Garden that the suspect had died, dozens of people cheered the news in real-time on his Facebook page.
Watching reaction unfold in this space is a surreal and disturbing activity—but it’s not exactly surprising. On platforms where anyone can publish, people congregate around what interests them. Just as people use hashtags to find people with similar interests on platforms like Twitter and Facebook, the suspect’s very identity became a temporary newsfeed of sorts. The barrier to entry for participation is practically nonexistent. People might not show up at a shooting suspect’s house to register their dismay; but it’s effortless to swing by his Facebook page and press “post.”
As of this writing, the comments were coming in too quickly to keep up.
* After this story was published, Facebook removed the shooting suspect’s page. A spokesperson for Facebook sent me this statement: “We are shocked and saddened by the incident that took place this morning. We have identified and removed the suspect’s profiles for violating our community standards.”


It didn’t take long for Uber to jeopardize whatever goodwill the company had earned by announcing a series of new initiatives aimed at increasing the hiring and retention of women and minorities. On Tuesday afternoon, the company published a series of initiatives it would be implementing to make the company’s culture more welcoming. By early Tuesday evening, audio had leaked of a board member, David Bonderman, making a sexist remark. By Tuesday night, Bonderman had submitted his resignation from the board.
The exchange in question took place at a meeting to discuss the new policies and the absence of the company’s embattled CEO, Travis Kalanick. According to a recording obtained by Yahoo, Arianna Huffington, a board member, says, “There’s a lot of data that shows when there’s one woman on the board, it’s much more likely that there will be a second woman on the board.” Bonderman replies, “Actually what it shows is it’s much likely to be more talking,” to which Huffington says, “Oh come on, David.”
Bonderman’s statement wasn’t just in poor taste, it was also wrong, according to research on gender dynamics during business meetings. As The New York Times reports:
Tali Mendelberg, professor of politics at Princeton University, and Christopher Karpowitz, an associate professor of political science at Brigham Young University, conducted a study in 2012 concluding that men talked far more than women did at meetings. The professors convened 94 groups of five people and varied the number of men and women in the groups. Their study is in line with multiple others drawing similar conclusions — men talk more than women, and men interrupt more than women.
In a statement, Bonderman, who is a partner in the private equity firm, TPG, an investor in Uber, wrote that his comments “came across in a way that was the opposite of what I intended, but I understand the destructive effect it had, and I take full responsibility for that.”
The events of Tuesday cap another iteration of what is now a very familiar pattern for the company: There was a revelation of problematic behavior or actions, an apology and a pledge to change, and then a revelation of new problematic behavior or actions. Uber has been in a seemingly never-ending public-relations spiral that started with a bombshell blog post by a former employee, Susan Fowler, alleging that she faced overwhelming sexism at the company. In response, Uber hired the former attorney general Eric Holder and his law firm to investigate the company and submit what has been called the Holder report, which would tell it how to fix things. But as Holder and his team were doing their investigation, a series of other troubling revelations came to light, including footage of Kalanick berating an Uber driver (for which he later apologized in a memo), and questionable business practices such as a program known as Greyball, which some said made it possible to discriminate against riders.
Given Uber’s history, Bonderman’s comments are not especially surprising, but their timing and delivery are especially cringeworthy: A powerful male board member makes an offensive comment about women in response to a female board member speaking at a meeting meant to introduce policies to address allegations of sexism. What makes matters worse is that Uber framed the release of the Holder report as a turning point for the company. But now, even with Bonderman's resignation, the narrative for Uber is once again of a company whose culture is hostile to many of the goals the company espouses.


Ernest Moniz is the antithesis of Donald Trump. As the head of the Department of Energy throughout much of President Obama’s second term, he was responsible for championing the Paris Climate Accord, negotiating the Iran nuclear deal with former Secretary of State John Kerry, and diligently pursuing a broad-based energy strategy often called the “all of the above” option. He is as comfortable testifying before the Senate as he is leading rigorous research efforts into the future of energy systems. There are few theoretical physicists who seem to enjoy the grind-it-out politics of Congressional appropriations, but Moniz is one of them.
Perhaps it is not a surprise that Moniz’s accomplishments have come under attack by Trump’s administration. The biggest move was the president’s announcement that the United States would pull out of the Paris treaty, much to the dismay of the international community and American business leaders.
When I spoke to him recently, Moniz spelled out his legacy, as he sees it, and the various ways that the current administration is undermining the very programs that, as he put it, “have clear track records of tremendous success.”
For Moniz, that includes the Loan Programs Office, which made capital available for the deployment of energy technology. After—and despite—the well-publicized Solyndra debacle, that program has experienced few losses and generated substantial returns for taxpayers. Also on Trump’s chopping block are ARPA-E, a research program modeled on the famous Defense Advance Research Projects Agency, and a new structure Moniz created at the Department of Energy called the Energy Policy and Systems Analysis Office, which he says was designed to and received bipartisan support.
But you’d be hard-pressed to find anyone who understands the politics, policy, science, and technology of energy as well as Ernie Moniz, so the discussion covers a lot more: the growth of solar, government support for drilling technology, the continued sluggishness of innovation in nuclear power, and the current Department of Energy “review” of base-load power, which has renewable-energy advocates worried.
Perhaps most intriguingly, Moniz calls attention to the fact that the job losses in the coal industry have been primarily driven by technological change in fossil-fuel extraction, namely the development of fracking and other drilling technologies. That allowed for a massive ramp in the production of natural gas, which made it less expensive than coal. And he is founding a new organization that he calls The Roosevelt Project to try to address, on a community-scale, the dislocations that workers will face in a deeply decarbonized world, underpinned by new energy technologies.
A lightly edited and condensed transcript of our conversation follows.
Alexis Madrigal: How did you think about your overall remit at the DOE?
Ernest Moniz: I always looked at the overall portfolio of the department as addressing three different time scales. So, the loan program is addressing the decadal time scale. The idea was to provide relatively low-cost capital to get these technologies out there: go down the cost curve, go up the experience curve.
Then, you have something like ARPA-E, which addresses maybe the mid-term. These are still pre-commercial, taking a lot of risk, but something that could hit a few triples and a couple of home runs, a bunch of doubles, and in that 10 to 20 year time frame, hopefully we’re going to see a number of those technologies scaling.
A number of researchers have formed companies. There has been $1.8 billion in follow on private capital [funding those companies].
Then for the long-term, going to mid-century and beyond, when I think the industrialized countries, obviously including the United States, will need to be into a deep decarbonziation mode. The Paris targets are only a first step: 25, 27 percent in 10 to 15 years. We’re going to need 80 percent reductions, and that’s where much of the early-stage basic science comes in.
For example, at DOE, the Energy Frontier Research Centers. They’re fantastic. They’re run out of the Office of Science and they are addressing a whole suite of very fundamental challenges that could enable the big breakthrough technologies for the longer term.
So, it’s a portfolio and there is often a knee-jerk mantra that, “Oh, the government should only invest in the earliest stage science.” Well, okay, that’s a possible point of view. I don’t think it’s a point of view that’s going to get us to the kinds of solutions that we want and need and that’s going to give us the economic edge we need in a multi-trillion dollar marketplace.
Madrigal: You were tasked with executing on an “all of the above” energy research plan while you were the head of the DOE. Do you think you were able to support all of the energy technologies adequately? Or do you, looking back, feel any got the short stick?
Moniz: I think, overall, we did pretty well. Certainly efficiency, renewables. Fossil, we put quite a bit of money into the carbon capture. That was the focus of the administration. On the nuclear side, I would have liked to see a bit more, to be honest. We did some good things, but we could have done more.
I think a big area that still needs a lot more is on the system side, especially the electric grid, but also the integration of energy infrastructures—electricity and gas. Obviously now the cyber component has come up big. I think a lot more could have been done on resilience of energy infrastructure. We went in the right direction but not quite as fast as I would have liked.
Madrigal: What would you have liked to have seen done with nuclear power?
Moniz: I think there could have been a lot more done—though we were a little bit hamstrung—on the back end, on spent fuel-disposal approaches.  I would have liked to have seen more done on various small modular reactor concepts, for example.
Madrigal: If I compare my own expectations from five years ago, I would say I expected more reactor concepts and things to be farther along than they are. There seem to have been a lot of ideas that never got off the drawing board.
Moniz: In nuclear energy, it’s somehow very slow moving. Part of it is that—for obvious reasons—the regulatory process, the safety process, is so complex.
And a lot of people who are very interested in alternatives to light-water reactors feel they are boxed in because the regulatory process may not have the expertise it needs yet. And of course, that’s difficult for NRC [the Nuclear Regulatory Commission] to do because most of their funding comes from fees. And by definition, if you don’t have an industry of, say, fast reactors, well, where is the fee coming from?
I would have liked to have seen not only DOE but NRC to have more appropriated funding to allow for more alternative approaches.
Madrigal: Carbon capture and storage has encountered a lot of difficulty in scaling up and becoming a practical, usable part of the energy system. What’s the argument to keep spending research dollars there?
Moniz: We’ve seen the cost of lots of stuff coming way down—like wind—after deployment. Carbon capture is at a very early stage. After all, to a very good approximation, it’s always cheaper to release the CO2 than to capture it. In the absence of a carbon charge or a policy to cap CO2, it’s got to be government funding to move forward. Ironically, it is the carbon capture which I could argue is the only part of the energy R&D portfolio that really has only climate as its motivation, as opposed to other possible benefits.
I would also note that carbon capture is much less expensive for many industrial facilities. And if nothing else, this may be an important use of carbon capture and sequestration. Frankly, to be perfectly honest, in electricity production, we know we have multiple options with low carbon, CCS would be one option. In industry, we may be hard pressed to find alternatives to carbon capture for large facilities that produce large amounts of CO2. And often, the cost is much much less because in their chemical-process stream, they have to separate CO2 anyway. And a big cost in the power plant is the separation of the CO2.
Madrigal: Looking back over your tenure, what do you think your biggest accomplishments were?
Moniz: On the big scale, obviously, the role the department played in Paris and, most specifically, the Mission Innovation commitment in Paris, for countries to double R&D investments over 5 years. And then, of course, the Iran Deal, in a different space. Those were the major advances in the Obama administration.
Madrigal: What about things you were able to do inside the institution that maybe didn’t get as much attention as the climate accord and the Iran deal?
Moniz: There were a lot of other things that are very important for the department going forward. For example, I think the relationship with the 17 national laboratories of the Department of Energy and the way they became much more a part of the department’s strategic planning. I think that was important and I hope that will be built upon.
Another thing, if you go really inside baseball, there are some of the reorganizations we made were quite important and helped the department function more effectively.
Madrigal: Like what?
Moniz: For example, I rearranged the responsibilities for the undersecretaries. There were three. What I think was very important was combining the science and energy portfolios in one secretary. The first undersecretary in that role, Franklin Orr, was a professor at Stanford.
Another one: The department always had some energy-policy function, but when I arrived, it was combined with the international affairs office. I never understood the logic of that. The policy got second shrift but it’s a core function for the department. So we split that off. But in splitting it off, it became named the Energy Policy and Systems Analysis Office. The “systems analysis” words were very important. That was the signal that this was going to be seriously analytical. And the hope was that in doing so, it would provide the foundation for a serious bipartisan response in Congress.
That played out dramatically.
That office, EPSA, was the tip of the spear on the Quadrennial Energy Reviews [a duo of reports examining energy infrastructures in the U.S.]. The first one came out in April 2015 and, as you know, these were not years of great political comity, and yet that first report installment had 21 policy recommendations fully or partly put into law. It received very, very strong bipartisan support.
The person who headed that was Melanie Kenderdine, my long-time colleague. She did a fabulous job and Congress even dramatically increased its budget from when we formed it because the response was so positive. That’s the kind of thing that should continue.
And yet the budget proposal from the administration proposes to end it. As they do ARPA-E. As they do the Loan program. It’s almost selectively going after programs that have clear track records of tremendous success. The budget proposal undercuts, in many ways, the entire energy and Paris agenda.
Madrigal: Okay, let’s talk about the U.S. pullback from the Paris Accord. You were intimately involved in the Accord negotiations. How big of a setback is the U.S. pullout for the global effort to prevent the most dangerous levels of warming of the earth?
Moniz: The Paris Accord is extremely important as a great first step towards significant decarbonization. The administration’s decision to announce the intent to withdraw—I think as you know there are some steps that are required and that will go until November of 2020—that obviously was very bad news on many grounds.
For one thing, it certainly flies in the face of the science that clearly calls for some robust response. But beyond that, I’m going to respond to both the announced withdrawal from the Paris Accord and the administration’s budget proposal to Congress. They kind of go hand in hand.
The latter would also, if followed by the Congress, undercut the innovation agenda that both provides solutions to climate risks and, frankly, positions those who innovate to be able to capture a good part of a multi-trillion dollar clean-energy economy. What it does do for sure, at least, for the near term, is abdicate U.S. leadership in this issue of such great global concern. Leadership that was essential for reaching Paris. I would argue, as many would, that the leadership of President Obama in producing the joint announcement with President Xi of China in November 2014  was really the turning point for the road to success in Paris.
And finally this announcement by the current president comes in a string of statements that together really have shaken the confidence of our allies and friends in the reliability of the United States, in terms of the commitments it makes and the leadership it offers. So, it’s a pretty broad-based set of negatives there.
Madrigal: Are there ways to make progress on climate during this administration?
Moniz: I am strongly convinced that the world is going to low-carbon. I think there’s no doubt about it. That’s why that big market is forming and will form even more strongly. So, in the end, I believe the administration’s actions will not deflect the United States from being part of that low-carbon future, but it will slow things down. It will create some bumps in the road and ultimately that will just come back to haunt us by making it more costly to resume the path to low carbon.
In terms of maintaining momentum, clearly, mayors and governors and business leaders have all made it pretty clear that they have the intent of continuing on the pathway because they know that’s where the future is going. Certainly, as far as the business community goes, they are not going to make long-term large capital allocations based on anything other than a low-carbon future.
I don’t want to minimize the impact of the president’s announcement in terms of getting to the climate solutions as efficiently as we can. But I think there is also the near-term damage in terms of American leadership and reliability in the eyes of the world and in the eyes of our allies.
Madrigal: If you were advising young climate researchers, would you tell them to take the offer by Prime Minister Macron of France to go work there on climate? Would you have taken him up on it?
Moniz: I did in a certain sense, take him up on it, in that I was a post-doc in France. It wasn’t Macron, but one of his predecessors. I certainly would do that again and in fact that’s where I met and married my wife.
But look, climate science is obviously an international collaborative effort. I wouldn’t leave the United States just for the reasons you are implying, but I would say there are great opportunities in France and Germany and the U.K. in terms of advancing climate science.
Madrigal: While we’re on the topic of the new administration, there’s currently a review of “base-load power” under Department of Energy Secretary Perry’s direction. [Base-load power is steady, nearly-always-on power currently provided by large fossil fuel, nuclear, and hydroelectric plants.] What do you think of it?
Moniz: It is an important issue today as we are looking at the uncertainty of operations of existing nuclear plants, which are essentially carbon-free. What is the value of base-load power in the mix as part of the overall system design and system reliability? With some colleagues, we’re starting up a small non-profit in the energy space and this was also a question that we intended to look at.
However, a review of this type also needs to look at the emerging technologies. For example, the utility in Tucson recently announced a long-term, a 20-year purchase-power agreement for solar energy plus storage at a pretty attractive—stunning, actually in my view—price. They quoted less than 4.5 cents per kilowatt-hour, including the storage.
Madrigal: Wow. [In Arizona, the average cost of electricity in March 2017 was 9.7 cents per kilowatt-hour. Electricity prices vary around the nation, but the U.S. average was 10.3 cents per kilowatt-hour in March 2017.]
Moniz: That’s with a subsidy, of course. So, without subsidy, maybe it’s twice that. But still, it’s amazing how rapidly these costs are coming down. The extent to which something like storage coming in big time at large scale affects the equation in terms of baseload power. Obviously, that can address a great deal of the non-dispatchabilty of solar or wind.
The critical issue is that these reviews have to be done in an objective, analytical fashion with transparency about assumptions—for example, assumptions on the trajectory of storage costs and deployment would be important in terms of the role of inherently intermittent sources in the grid, as well as nuclear, natural gas combined cycle plants, and the like.
Madrigal: From what you know about the review, as it is, do you think it is being approached in an objective, analytical way?
Moniz: I just don’t know. I certainly haven’t seen a lot of transparency. If the outcomes are not transparent, I don’t see how it is going to have much influence. My impression is that there has not been a wide set of inputs being solicited to the study.
Madrigal: In the past, you’ve said that restricting climate emissions will have a negative impact on some sectors, so in crafting policy, the emphasis should be on “workers.” What do you mean by that?
Moniz: As in any significant transformation, including those in many sectors through technology advances, the economy is dynamic and some areas will directly benefit and some areas will be directly challenged.
The obvious example is the decrease in coal, although I want to emphasize that up til now, that’s had very little to do with climate. That’s been natural gas prices being low.
The job losses in the coal sector have been there for decades. This is not a recent thing. A small impact to date from things like the mercury rule and a very substantial impact from technology development.
Now clearly, if one is going to a very deeply decarbonized economy, then you need to invest in two things, it seems to me. And I would say that the Obama Administration, we did. One is help for communities in transition. That could be retraining. There are lots of approaches. And two: the innovation investments that, for the example we are discussing, can actually permit coal to be a player in a very low carbon world, specifically carbon capture and storage or carbon capture and utilization and storage.
But there are other communities that are affected in very, very different ways. There are all kinds of possibilities, but what we’ve done is think there is a one-size-fits-all solution. Rather than having the headwinds from some of the real or asserted job dislocations, why don’t we start with the workers and the jobs and the communities and build up solutions for their regional development that are consistent with a long-term low-carbon economy?
I will say that is a project that I am intending to carry out. We’re calling it The Roosevelt Project. That will be in concert with the Emerson Collective, which is out in California.
Madrigal: Can you tell me more about that?
Moniz: The idea is to do studies over several years of exactly the type that I’ve described. The reason it is being called The Roosevelt Project, or the reason I called it that, is because Teddy Roosevelt is identified with environmental stewardship, Franklin Roosevelt with the New Deal and jobs, and Eleanor Roosevelt with social justice. And so I think it all comes together as the Roosevelt Project.
Madrigal: One of the things that I want to pick up on is that coal-job loss is actually due to technological development, not regulation. And drawing out your implicit assertion—that’s because drilling technology allowed for access to new reserves, which drove down the cost of natural gas, which then put coal operators in a tough position. And it’s my understanding that these drilling technologies were developed with government assistance at multiple levels. How did the DOE support the project of reaching unconventional natural-gas reserves?
Moniz: When you go back to the late ’70s and early ’80s, the DOE supported a variety of programs involving the drilling technologies but also involving the characterization of unconventional reservoirs. Then in the ’80s to ’90s, there was what I would call a public-private partnership that carried on. This was run by GRI, the Gas Research Institute—
Madrigal: Which was created by that distribution tax?
Moniz: Technically it wasn’t a tax. But you’re on the right track. FERC [Federal Energy Regulatory Commission] allowed a small fee on interstate gas transmission, and that went to provide half the funding because it had to be matched by companies. That research fund, if I recall, grew to something over $200 million a year, and the Gas Research Institute was established as a non-profit organization to manage it. And George Mitchell, the famous George Mitchell from Texas kinda viewed as the father of shale gas, he was on the board of GRI, and helped advance that. That was in effect a public private partnership. They did a lot of demonstrations, demonstration wells and the like. That built up to the coal-bed methane, to tight sands, and to shale as unconventional resources.
At the same time, very importantly as well, there was also finite-time tax credit. So you had the DOE early-stage stuff, you had the GRI developing the technology, and you had the tax credit for the initial deployment, and when the tax credit went away, the production kept going up, up, up, because by that time, the cost had been brought down to a level where it obviously competed on its own.
Madrigal: Another area where the government appears to have jumpstarted a major movement is so-called utility-scale solar—big photovoltaic farms put in by energy companies. And the mechanism was the Loan Program at the DOE, which was made kind of infamous by the collapse of Solyndra. How did the Loan Program work in this case?
Moniz: When the loan program made its first commitments, there were zero [utility-scale solar PV projects]. Partly it was the time period: 2009 to 2010, debt financing was a little bit tricky in general. But the reality also is that banks and investors, unless they are specialized, have a hard time going into new areas, so I think the DOE program and the tremendous diligence process that they developed over several years really gave a lot of confidence to investors.
So, in this case, there were none. The first five got loan guarantees. And that was the end of DOE supporting PV farms. And now there are 10 times as many with private money.
Madrigal: How has the loan portfolio performed generally?
Moniz: Certainly, these [solar-farm] loans, they’ve had no losses. The projects all had power-purchase agreements to cover the cash flow requirements. And certainly there were other successes including the loan to Tesla for their factory in Fremont, which they then paid back nine years early, but it got them going.
If you look at the portfolio overall, clearly there have been losses and Solyndra is the famous one. I might note, I don’t know the exact numbers today, but I believe the total losses in the entire portfolio are in the $800 million range. Over $500 million was Solyndra alone. It is the biggest loss item on the book of the portfolio.
However, the portfolio as a whole has generated I think it is now $2.5 billion of interest payments to the Treasury. And second, when Congress formed the program, they created a $10 billion reserve anticipated for losses. You might argue we’re way behind in the losses. And that may come to the risk level that’s being tolerated.
Madrigal: Let’s turn to your other big accomplishment, the Iran nuclear deal and nuclear non-proliferation. Do you think you’ve stopped Iran from getting a nuclear weapon or more kicked the can down the road? At this point, thinking about Iran’s behavior since the deal and all the geopolitical factors, how do you see the Iran deal?
Moniz: Let me emphasize what I consider to be the most important part of the deal, which is verification. Certainly the bar to any covert weapons program has been raised a lot, and will be quite high.
Now, what I want to say is, first of all, you may know that I now have another job as CEO of the Nuclear Threat Initiative, so proliferation in general is something I’m also working hard on.
Our hope, as was President Obama’s hope, was to minimize the role of nuclear weapons over time and certainly to prevent the further spread of nuclear weapons. With Iran, they have made a very strong commitment to never having a nuclear weapon. So, it is my expectation that Iran will never have a nuclear weapon. If they attempt to go there, it will be known and actions will be taken to, shall we say, discourage that.
The Iran agreement stands on its own: If 15 years from now—or whatever is left of 15 years, 13.5 years—we chalk up the score, and say, yup, the [deal] was followed to the letter and now Iran is free to develop its peaceful nuclear program as it chooses, the deal was a success.
However, I have to say that those of us who were engaged in it so heavily, along with many others, are hopeful that over this time period, 10, 15 years, that there will be other more fundamental changes in that whole regional dynamic. Because obviously we have a lot of problems with Iran in the regional context. If this can help push that society in the right direction, that would be great, but the deal stands on its own as well in terms of nuclear activity.
Madrigal: Looking out 50 years, say, do you expect a greater or smaller number of nations to have nuclear weapons?
Moniz: In 100 years, I would hope that the answer is zero. Let me add an important word. It’s a word I just used in the Iran context. When we talk about a world without nuclear weapons, what I always talk about is a world verifiably without nuclear weapons. The word verification is critical.
What is the success path that is practical? I strongly believe in the goal of a world without nuclear weapons, but that doesn’t help if we don’t have something that is pragmatic that can survive in the political realities of all the countries that have to play in this.
It’s a very, very tough problem. I think we are decades away from reaching that kind of a goal, but we’ll be even more decades away if we don’t start thinking about it now.
Madrigal: Sounds like it’s time for some game theory.
Moniz: Yup. Tom Schelling [the eminent strategist and an important contributor to game theory].
Madrigal: I read in The Boston Globe profile of you that you’ve literally never taken a vacation? Did you finally take one after you left the DOE?
Moniz: Yeah, I took … My wife termed it the anti-Genesis strategy, I slept for six days and worked on the seventh.
Madrigal: Really?
Moniz: Yes.


On Tuesday, Uber announced a host of changes that it hopes will stem the ongoing public-relations crisis the company has found itself in for the past several months. The embattled CEO, Travis Kalanick, will step away from the company for an unspecified period of time. But that won’t likely change the day-to-day lives of the more than 5,000 Uber employees as much as the the changes the company is committing to make to their recruiting, retention, and workplace-culture policies, detailed in a report known as the Holder Report.
For many, a cultural change at Uber seems overdue. At the start of 2017, a former Uber employee, Susan Fowler, published a damning blog post alleging her experiences with sexism and discrimination at the company, and the feeble response she received when she complained. Immediately after Fowler’s post, the company retained the former U.S. attorney general Eric Holder and his firm Covington Burling to investigate the company’s culture and structure and to provide recommendations to help it improve. While the firm worked on its investigation, more disturbing details about Kalanick’s behavior and the culture of the company came to light, leading many to wonder if Uber’s survival was on the line.
The 13-page report by Holder and his team attempts to address a wide variety of problems, some of which are Uber-specific, but many of which are common to companies across the country, such as a lack of diversity, particularly at the higher levels of the corporate structure. Many of the changes Uber plans to implement, such as flexible work schedules and less aggressive mottos, are quite basic, but if the company can do so successfully it might encourage companies struggling with similar problems to follow suit. Some of the other initiatives, such as hiring a chief operating officer and instituting mandatory management and interview trainings for HR and other leadership, are the sorts of things that are extremely common at small and medium-sized companies, to say nothing of a big, international company like Uber.
This report isn’t the company’s first foray into the discussion about diversity. In late 2016, Uber shared data about its dismal diversity numbers and its commitment to improving them. The report encourages the company to continue to publish its diversity statistics, as well as to assemble an employee diversity board, and to expand the role and reach of Bernard Coleman, the current head of diversity, elevating the position to report directly to the CEO and COO. Holder and his team also suggest that employees undergo implicit-bias training (a contested method of increasing awareness and diversity) and implement blind resume review to decrease opportunities for bias based on perceived race or gender—a proven issue in hiring.
“Implementing these recommendations will improve our culture, promote fairness and accountability, and establish processes and systems to ensure the mistakes of the past will not be repeated. While change does not happen overnight, we’re committed to rebuilding trust with our employees, riders and drivers,” Liane Hornsey, Uber’s chief human-resources officer said in a statement.
Beyond these basic plans, additional changes coming down the pike may strike some as a bit more symbolic than substantive. According to Bloomberg, the “War Room” will be renamed the “Peace Room,” and Mike Isaac of The New York Times reports that Uber’s head of Human Resources requested that everyone in attendance at a company meeting to give each other a hug. Those type of feel-good exercises are undoubtedly an attempt to bring about a new, kinder Uber. But they’ll be meaningless without real shifts in the company’s policies and corporate structure—the kinds of changes that are much harder to make.


Travis Kalanick, Uber’s embattled CEO, is taking a leave of absence. “It’s hard to put a timeline on this,” he told his employees in an emotional email on Tuesday. “It may be shorter or longer than we might expect.”
An extraordinary string of scandals have consumed Uber for the better part of the year, including an instance when a leaked dashboard-video showed Kalanick cursing at a driver. Amid all this, Kalanick’s mother died in a boating accident last month.
“For the last eight years my life has always been about Uber,” Kalanick wrote. “Recent events have brought home for me that people are more important than work, and that I need to take some time off of the day-to-day to grieve my mother, whom I buried on Friday, to reflect, to work on myself, and to focus on building out a world-class leadership team.”
“But if we are going to work on Uber 2.0,” he added, “I also need to work on Travis 2.0 to become the leader that this company needs and that you deserve.”
Also on Tuesday, Uber made public the recommendations put forth by a special committee hired to investigate the company’s culture. In a marathon meeting on Sunday, the Uber board unanimously voted to adopt all recommendations in the Holder Report, a representative for Uber told The Atlantic, referring to the report that came from the investigation.
Uber asked Eric Holder, the former U.S. attorney general, to lead the probe after an extraordinary string of scandals at the ridesharing company. The past six months have been enough of a public-relations nightmare for Uber to have raised the question of whether the company can survive—or at least whether its $70 billion valuation will take a hit.
Uber’s recent problems have included a high-profile public boycott, an explosive first-hand account of sexism at the company, revelations of how Uber used secret software to evade law enforcement, and the leaked video of Kalanick’s argument with a driver.
Kalanick’s leave will begin just three months after he pledged to his employees that he would “grow up” and get the “leadership help” he needed, Kalanick may take a leave of absence from the embattled ridesharing company, according to multiple reports. Though the board was expected to discuss Kalanick’s future on Sunday, it’s not clear whether a leave of absence was one of the items formally recommended in the Holder report.
The Holder report chronicles “a frequently chaotic and ‘hostile work environment’ without adequate systems in place to ensure that violations such as sexual harassment and retaliatory behavior were dealt with professionally,” according to Recode. Emil Michael, one of Kalanick’s closest confidants and the second in command at Uber, stepped down on Monday, according to an email to Uber employees obtained by The New York Times. It wasn’t clear from the email whether he resigned or was forced out.
Uber has suffered several high-profile resignations and terminations in recent months. In May, Uber fired Anthony Levandowski, a star engineer and the central figure in a high-profile legal battle over self-driving cars.
His termination was tied to a federal lawsuit in which Waymo—the self-driving car company that spun out of Google—is accusing Uber and Levandowski of stealing its design secrets. An Uber spokesperson told The Atlantic that Uber had been pressing Levandowski to help with its internal investigation of the matter for months, but that he did not meet a deadline set for him. Levandowski could not be reached for comment. He has exercised his Fifth Amendment right against self-incrimination, which played a role in a federal judge’s decision to deny arbitration in the Waymo case.
Uber’s request for arbitration would have allowed the ride-sharing giant to defend itself behind closed doors. Instead, Uber will have to go to trial to answer Waymo’s accusations in what is sure to be an ugly showdown.
Uber Did What!?
The case is poised to be among the biggest legal fights over intellectual property so far this century. The technology at stake is at the center of the emerging and potentially highly lucrative self-driving car industry, and the key players are two of the world’s top tech companies. (Google rebranded its self-driving car project as Waymo in 2016.)
Not only will there be a trial, but Judge William Alsup also referred the case to the U.S. Attorney for investigation of possible theft of trade secrets, a move that suggests the Uber engineer at the center of the case could face criminal charges.
Waymo is accusing the former Google engineer Levandowski of secretly stealing a trove of files from the company before he quit to start his own self-driving truck startup. Uber acquired that company, Otto, for $680 million shortly after it launched last year.
Self-driving cars are arguably the great technological promise of the 21st century. They are in that rare class of technology that might actually change the world. And not just in the way everyone in Silicon Valley talks about changing the world—but really, fundamentally change it. So much so that their dramatic life-saving potential is almost secondary to the other economic, cultural, and aesthetic transformations they would cause.
Those who aren’t able to drive themselves today—people who are blind, for example—would be granted a new level of transportation freedom. Mass adoption of self-driving cars would create and destroy entire industries, alter the way people work and move through cities, and change the way those cities are designed and connected.
To build the technology that prompts all this change is to be in an enormous position of power.
That’s why the race to bring self-driving cars to the masses is so intense. It’s also what makes this particular competition echo other transformative moments in technological history—going all the way back to the Railroad Wars, at least. (Incidentally, there was a different kind of driverless car back then.) “The Wright brothers jump into my brain immediately,” John Leonard, an engineering professor at M.I.T., told me in 2015. “But maybe it’s kind of like a decentralized space race. Like Sputnik, but between the traditional car companies and their suppliers versus tech companies and their startups.”
There’s a lot of money at stake. A lot a lot. We’re talking billions of dollars per year in potential profits, maybe more. All of the major players know this. For some companies, it is a fight to the death. Each one intends to come out on top.
* * *
Waymo (formerly Google)
When Google (now Alphabet) launched its self-driving car program in 2009, it had no competition to speak of. Culturally, the idea of a self-driving car was novel. Even the flying cars in 20th-century science fiction tended to have human drivers. So when Google began to go public with information about the project, in 2010, its level of seriousness about the effort wasn’t yet clear. “Some of these things will turn out to be wildly successful, and others will just fade away,” one investor told The Los Angeles Times at the time, referring to Google’s suite of unusual projects.
Wild success still isn’t a guarantee, but it’s now obvious that Google—which has since spun off its self-driving-car unit into a company called Waymo—is deeply invested in the work it’s doing. Its test fleet is now on public roads in four states: California (since 2009), Texas (2015), Arizona (2016), and Washington state (2016). “We’ve self-driven more than 2 million miles mostly on city streets,” Waymo says on its website. “That’s the equivalent of over 300 years of human driving experience, considering the hours we’ve spent on the road. This builds on the 1 billion miles we’ve driven in simulation in just 2016.”
All that driving and a near-perfect safety record—a reputation that has undoubtedly helped buoy the public’s perception of self-driving vehicles.
Uber
Uber catapulted itself into the self-driving car space in truly Uberesque fashion: With a scandal. In 2015, the ride-sharing giant hired an entire department away from Carnegie Mellon—some 40 robotics experts and engineers, including several top experts in autonomous-driving systems.
Since then, Uber’s commitment to the future of self-driving cars has only intensified. (Consider the business incentive of eliminating human drivers, who get a cut of every ride they give.) In 2016, Uber began testing its self-driving vehicles on public roads in Pittsburgh, it doubled down its own proprietary street-mapping system—ostensibly to reduce reliance on competitors like Google and Apple—and poached Google’s top mapping expert to do so. (He later resigned.) Uber also acquired a fledgling self-driving truck company, Otto, for $680 million in 2016—but more on that in a minute.
Given the talents of its employees and how much venture capital the company has on hand, Uber has emerged as a formidable player in the emerging self-driving car industry. Yet Uber continues to be plagued by controversies.
After a dustup over Uber’s refusal to seek permits for its self-driving cars in California in late 2016, the company changed course and applied for a state testing permit. In February 2017, Waymo filed a federal lawsuit claiming a former Google engineer had stolen self-driving car secrets before leaving the company to found Otto. Waymo says that when Uber acquired Otto, the former Google engineer used the information he allegedly stole to help build a circuit board for Uber’s self-driving car systems. The legal battle is poised to be the first major intellectual-property fight of the driverless car era.
Apple
Apple remains one of the more mysterious and intriguing players in the self-driving car game. On one hand, Apple can’t afford not to pursue this emerging technology if many of its major competitors are. On the other, Apple? A car company? To be fair, though, that’s what people said of Google in 2010. And not all self-driving car companies will manufacturer vehicles themselves; some will just lease out the self-driving software for auto manufacturers.
In April 2017, Apple secured permits to test self-driving vehicles on California roadways. Two weeks later, Bloomberg News published photos of a sensor-equipped Lexus RX450h SUV that emerged from an Apple facility, according to a person who provided the photos to Bloomberg.
For years it was rumored that Apple had a secret self-driving car project in the works. But there have been persistent reports that the project— which according to The Wall Street Journal had hundreds of dedicated employees as of 2016—was plagued by organizational and managerial problems. It wasn’t until December of 2016 that Apple officially made it known that it is working in some capacity on self-driving cars, via a letter to the National Highway Traffic Safety Administration.
“The company is investing heavily in the study of machine learning and automation,” wrote Steve Kenner, Apple’s director of product integrity, “and is excited about the potential of automated systems in many areas, including transportation.”
Other than that, however, Apple has remained characteristically secretive about its work.
Tesla
Tesla wants to bring driverless cars to the market, but it has a markedly different approach than Waymo, which may be its biggest competition. While Google wants to build fully self-driving systems from the ground-up, its critics say this will take too long. In the interest of making everybody safer sooner, Tesla is adding increasingly autonomous systems, bit by bit, to its existing high-end vehicles. But there’s a big debate over which method—fully autonomous versus incrementally autonomous—is actually better for public safety.
Tesla’s CEO, Elon Musk, has said it’s “morally reprehensible” to wait until the technology is advanced enough for complete autonomy. Yet critics of the Tesla approach say that here-and-there semi-autonomous features may present too much of a gray area for today’s drivers to safely navigate. The marketing around Autopilot, the current Tesla system, has arguably left people with the impression that Tesla’s cars are more autonomous than they really are. The very name, Autopilot, certainly suggests it might be okay for human drivers to stop paying attention.
This concern came up again in the spring of 2016, when a Tesla driver who was using the Autopilot feature died in a car accident. At the time, Tesla’s Autopilot feature was in beta mode, meaning the drivers who tested it on public roads were required to acknowledge any risks involved. Federal investigators eventually concluded Autopilot was not to blame in the fatal crash.
Tesla already claims on its website that all of its vehicles “have the hardware needed for full self-driving capability at a safety level substantially greater than that of a human driver.” This is, at best, slightly misleading. Tesla’s hardware may eventually allow for a “full self-driving” system, but it’s definitely not there yet.
Whichever approach to building a truly autonomous car is the right one, Tesla’s sense of urgency is helping to quicken the pace of competition in the driverless-car space.
Legacy automakers
Like Tesla, several legacy automakers are announcing their entry into the driverless-car space with incremental assisted-driving systems. This approach makes sense for them: After all, they already manufacture cars that people can go and buy—something that isn’t true of Apple, Google, or Uber—which means one of the best hopes for legacy carmakers to stay in business is to evolve now rather than attempting to play catchup later (which they may still have to do).
But some legacy companies have gone farther than others. While nearly every major automaker pays lip service to the importance of developing autonomous vehicles, only some have backed up their talk with action. Volvo stands out among the more committed, for instance. In a project Volvo is calling Drive Me, the automaker will put a fleet of 100 driverless cars on the highways in Sweden. (As with tests by Google and Uber on public roads in the United States, humans will sit behind the wheel, ready to take control of the vehicles if needed.) In March 2017, Toyota unveiled its first self-driving car  prototype. The car came out of Toyota’s artificial intelligence research institute, which it launched with a $1 billion investment in 2015.
Ford Motor Company has also made its efforts increasingly visible. Ford announced in August 2016 that it plans to be “mass producing vehicles capable of driving fully autonomously” by 2021. Six months later, the automaker announced a $1 billion investment in the software company Argo AI, a software startup specializing in self-driving cars. (“Ford is the majority stakeholder in Argo AI, but we are structured to operate with substantial independence,” Argo AI says.) “This work is easily the most challenging of my career, and it may be the most important, as well,” wrote Chris Brewer, the head of engineering for Ford’s Autonomous Vehicle Development department, in a blog post in March 2016. “Come to think about it, who better to develop a self-driving car than a company that’s been making cars for more than 100 years?”
As with Ford and Argo AI, several other tech firms and automakers are forging partnerships. Chrysler and Google announced in May 2016 that they would team up to make a driverless minivan, while Volvo and Uber announced a partnership in August 2016.
Newcomers
We should expect to see more startups in the self-driving car space in the years to come. One example is Drive.ai, which launched in August 2016 and is creating deep-learning software for driverless cars.
There will be others. Chris Urmson, the longtime head of Google’s driverless car initiative, left the company in August 2016, at a time when the project seemed to be shedding several key players. In December 2016, the technology-focused news website Recode reported that Urmson is starting his own self-driving-cars venture.
Technology history tells us that the first company to build a technology is not always the company that ends up making a windfall. That may well be the case in the realm of autonomous vehicles.
There are many uncertainties in all this, but one thing is clear: The cultural space occupied by the automobile is undergoing rapid, radical transformation. There are sure to be big winners and losers along the way.


A decade ago, for the most part, phones were phones. Computers were computers. Cameras were cameras. Portable music players were portable music players. The idea that the future of the computer would be a phone, or vice versa, wasn’t merely absurd. It just wasn’t how people thought about consumer technology. At all.
So when the first iPhone was unveiled in 2007, plenty of people assumed it wouldn’t change the world. (“Touch-screen buttons? BAD idea. This thing will never work,” as one naysayer put it at the time.)
To those who had been watching Apple since the 1980s, however, shrinking computers and videophones seemed to be always just tantalizingly out of reach, emblems of a future that would, fingers crossed, eventually arrive.
But when? By 1995, even though Apple’s laptops had dipped to a svelte six pounds, and the transformative power of the internet was becoming apparent, the next great iteration of the web was barely imaginable. Today’s mobile web, the one that would be ushered in by smartphones, was still out of reach. But there were hints of what was to come.
Apple has always been fond of dreaming up hardware and software from a not-too-distant future, and there are glimmers of the iPhone in Apple’s history since long before the rumors about the device were taken seriously in the early 2000s. More than a decade before the smartphone was unveiled, Apple shared with the computing magazine Macworld a semi-outlandish design for a videophone-PDA that could exchange data. (Smartphones eventually made the PDA, or personal digital assistant, obsolete.)
The prototype for the device, published in the May 1995 issue of the magazine, is something of a missing link between the Newton and the iPhone—though still more parts the former than the latter. The Newton was Apple’s lackluster PDA, first released in 1987, 20 years pre-iPhone. The Newton may have been ahead of its time in some ways; but it also failed because it was pricey and didn’t work particularly well. (In 1993, one pithy New York Times writer memorialized his attempts to write on the device this way: “This is being writings a worth it takes a while before the handed tiny red floor is footprint. Signed, Bite (poof!) Beers (poof!) been (poof!) I sits.”)
The Macworld prototype combined a PDA and a videophone, complete with handset, and visualized a future in which the devices would be able to exchange data. Naturally, because this was 1995, the concept also included a CD drive and a stylus.
The design was made public as part of a collection of several made-up Apple products, all published in that same 1995 issue of Macworld. The spread is charming in retrospect, but also revealing for how it signals a shift in the way Apple was changing the way people thought about the intersection of design and technology. Flipping through the old issue of Macworld this week made me think of a conversation I had last year with  Robert Brunner, the industrial designer who worked for many years at Apple and now runs his own design studio.
“When I started out in my career, design was seen as a necessary evil, especially in relation to technology,” Brunner told me at the time. “It moved into this phase where all of the sudden people saw design as a corporate identity thing, like ‘all of our products need to look alike.’ In the early 1990s, it moved into innovation for innovation’s sake. And then there started being this shift, driven somewhat by Apple, where people began to understand that design was what made them want your technology to be part of their lives.”
Design isn’t just the aesthetic quality that makes a device beautiful or identifiable by brand, in other words. It is a core part of how the technology works. Brunner attributes that cultural change largely to Jony Ive, Apple’s chief design officer, and his team’s work over the last 10 years. “Jony and his team have changed the way people see design,” Brunner told me.
You can see the stirrings for this change in attitude across the pages of the 1995 Macworld spread. Computers are compared to “stylish furniture,” and to “strong personal statement piece[s]” of art, namely Richard Sapper’s minimalist, counterweighted Tizio lamp.
In the caption for one imagined computer of the future—a curved and dynamic prototype that was designed to swivel on a four-footed pedestal “so you can get at the floppy disk drive on one side and the CD-ROM drive on the other side”—Macworld described the change that was taking place in the design world explicitly, because at the time it still needed to be said: “The emphasis of this radical approach is how you interact with the Mac, not on the Mac itself.”
A new concept for the Newton made an appearance in Macworld, with Apple adding splashes of color that would eventually reach the market with the iPhone’s colorful, plastic 5c models. “For a personal device, the black Newton MessagePad sure lacks personality,” Macworld wrote back in 1995. “But these Newton designs have plenty. The yellow Sports Newton borrows Sony’s Sports Walkman idea—a ruggedized high-visibility version for people on the go. If you carried a purple Game Freak, people would have no doubt that you’re a serious video-game player. … The MessagePad could be given a custom look for a particular company or application.”
But at the time, most of these computer-of-the-future designs were seen as impractical—too confusing, too far outside the realm of what was technologically possible (or even desirable) for consumers at the time.
Looking back now, two decades since the Macworld feature and one decade since the iPhone reached the market, it’s clear that Apple’s smartphone has forever altered industry standards for electronics design. Losing the keyboard and prioritizing software over hardware was crucial to the iPhone’s success—as was playing up the phone in iPhone to distance the device from the failed Newton that preceded it. “I don’t want people to think of this as a computer,” Steve Jobs, the former Apple CEO, told John Markoff, the veteran technology writer for The New York Times, when Jobs introduced the iPhone in January 2007. “I think of it as reinventing the phone.”
Where Apple’s past failures had always hummed with untapped potential, as one newspaper columnist described the Newton in 1993, the iPhone elegantly and boldly realized it. The device would go on to dramatically reconfigure social norms and behaviors. It changed how people socialize, how people work, how people shop, how people seek information, and how designers think about technology. Gorgeous design is now mainstream. “With the escalation of average design—average is now pretty good, right? So you have to even look harder for what’s really good,” Brunner told me.
“I think for us we constantly have to put more and more pressure on ourselves to be original and meaningful and not just derivative,” he added. “But there’s something unique about American design culture—and, in particular, Silicon Valley design culture—that really drives that originality. I think there’s something in the water here that drives people to always push to do something different, beyond the status quo.”
Even in 1995, Apple’s futuristic concepts offered a glimmer of what might come to pass, Macworld wrote at the time. “Although these prototypes won’t become real products, you can expect many elements to show up in real Apple products of the future,” the magazine said.
Macworld was right. But as we now know, the real products of the future were far better than even Apple’s wildest dreams just 22 years ago.


Updated on June 12 at 11:30 a.m. ET
One of the best predictors of whether people install solar panels on their house isn’t their age, their race, their level of income, or their political affiliation.
It’s whether their neighbors did it first.
This finding has been shown repeatedly across space and time, including in California, Connecticut, Germany, Switzerland, and the United Kingdom. “It happens at the street level, it happens within zip codes, it happens within states. It seems to be a common feature of human decision-making that crosses many boundaries,” says Kenneth Gillingham, a professor of economics at Yale University whose study helped establish the finding.
On Monday, Google will put the finding into practice with Project Sunroof, its free online tool that aims to make it easier for people to obtain and use home solar panels. Project Sunroof will now not only inform users how much sun hits their roof, or how much solar panels would save them per month, but also which of their neighbors have taken the plunge first.  
Project Sunroof was launched in 2015 by Carl Elkin, an engineer at Google who had worked on local solar-installation campaigns in Massachusetts. It now provides data for 60 million homes across the United States that it has already assessed with its algorithms.
For the past two years, Project Sunroof has walked people through all the information-gathering steps of installing solar panels: After you tell it where you live, its algorithms estimate how much solar energy falls on your roof, calculate how much solar panels would reduce your electricity bill, and deliver estimates from local installation firms like Solar City.
It can also walk you through similar steps if you’re interested in leasing or borrowing panels. “It highlights that, for many people, solar is often free. In many cases, including for my house, solar is better than free,” Elkin told me last week.
Now—in a nod to the powerful peer effects of solar power—it will also show you which of your neighbors have already installed panels. In its map view, Project Sunroof will show a red dot over any home or structure that appears to have rooftop solar.
“People want to know: ‘What if there’s some hidden gotcha in the contract?’—and usually there isn’t. ‘Does this work for other people like me? Is solar really viable in my neighborhood?’” Elkin says. “You can zoom around through your town and understand how common solar is in your neighborhood. And many people have found: Wow, there is a lot more solar in my neighborhood than I’d realized.”
Google created the data for this feature in-house, training a machine-learning algorithm on the common appearance of rooftop solar panels and then letting it loose on the cities and towns that Project Sunroof already covers. Right now, the company has analyzed installations on about 60 million buildings in the United States; it hopes to get to the remaining 40 million buildings in the next few years. The methodology doesn’t seem to be perfect yet—I noticed some rooftop solar installations in my own neighborhood that the algorithms missed—but it seems to identify most of them.
“I think the idea is a really great one,” says Gillingham, who has previously talked with Google about his research but did not know they were working on this feature.
His current research has found that people are even more likely to install solar panels if they can see their neighbors’ installations from the street—suggesting that day-to-day visibility, and not, say, word-of-mouth or local marketing efforts, is what nudges people to look into solar power. The new Project Sunroof feature “tells you that people nearby have installed solar panels even if you can’t see them from the road,” he says.
Gillingham did share some privacy concerns about the feature. State solar registries often hide address-level data, and they only share it with academic researchers confidentially. “However, if you go on Google Maps or Google Earth, you can pretty easily see the installation in pretty much all cases,” he says.
When I asked about privacy, Elkin also pointed out that most of this is already in satellite imagery. He also said that Google only stores its installation data at the latitude-longitude level—in other words, it does not resolve it into addresses—and it has no plans to share it with the public or other companies.
“We’re not looking at individuals, we’re looking at buildings,” he told me. “And we’re not publishing a list of these buildings.”
The company has also created a larger map that shows the popularity of solar panels by census tract. Like other studies into the social adoption of solar panels, it has not found a strong correlation between income and installation.  
And privacy concerns aside, Gillingham praised the feature for increasing the visibility of renewable energy. “It creates a social norm around solar panels,” he says. “When many people have solar panels around you, it’s a normal thing to do. You’re not going out on a limb by having a company come out and look at your rooftop.”
Elkin hoped for the same goal. Speaking to me, he almost seemed to conjure an image of Americana Solara: a two-story house, a white-picket fence, a tree and tire swing, and rooftop solar panels. “We want people to realize solar is absolutely part of the fabric of American life,” he says. ‘Out of these many houses, each saving money, comes one solution to the environmental problem. Out of many, one. That’s a very American idea.”


For nearly three hours on Thursday, many Americans turned their attention to the engrossing, absorbing spectacle that was James Comey, the former FBI director, giving his first public remarks since President Donald Trump fired him one month ago. One of the nation’s top law-enforcement officials was poised to divulge damaging information about a president accused of trying to influence a federal investigation into his campaign’s ties to a foreign government—and the people weren’t going to miss it.
The room hosting the congressional hearing could seat less than 100 people. Live-streams, broadcast from cable-news channels, regular television networks, and numerous social-media sites and other places online, accommodated many more, turning computer screens into front-row seats to the action. They also provided the public with something that, in the quiet moments before the opening gavel struck, felt almost unusual in the noisy, cluttered, and deeply divided environment that has come to be the norm in political news.
News consumers would be watching alongside reporters, rather than waiting for a handful of national news organizations to publish bombshell scoops. A live-stream offered the public an unedited, unfiltered, and uninterrupted space, a drop of transparency in a sea of tweets and “fake news.”
Not accounting for any spin coming from the people inside the room, what you saw was what you got.
Viewers brought their own biases to the live-streams, of course, their minds perhaps made up long ago about who’s to blame for the current state of affairs. The same live-stream won’t look the same to Fox News viewers and MSNBC fans. But people’s understanding of newsworthy events, as the country has learned over and over in the last year, can become influenced by the smallest forces of news dissemination. Research has shown readers’ perceptions of news stories can be distorted after reading the comments, home to the kind of caustic discourse and debate that has increasingly moved into other parts of media. On cable networks, anchors and commentators chime in, showing viewers real-time action through their own, sometimes narrow, lens. On Twitter, users fire off commentary atop clips and GIFs. A small chyron, the ticker of text at the bottom of a screen, can make a huge difference:
two worlds pic.twitter.com/b5EVMg6Cbr
Those two chyrons refer to a line of questioning from Jim Risch, a Republican senator from Idaho. Risch asked about a conversation, described in memos written by the former FBI director and leaked to the press, in which Comey said Trump asked him to shut down the FBI’s investigation of the president’s former national security adviser, Michael Flynn, and his alleged ties to Russian officials. “I hope you can let this go,” Trump said, according to the memo.
“Now, those are his exact words, is that correct?” Risch asked Comey.
“Correct,” Comey replied.
“He did not direct you to let it go?” Risch followed up.
“Not in his words, no,” Comey said. Risch pressed him further.
“I mean, this is a president of the United States with me alone saying I hope this,” Comey said. “I took it as, this is what he wants me to do. I didn’t obey that, but that’s the way I took it.
“You may have taken it as a direction but that’s not what he said,” Risch said.
Comey: “Correct.”
Comey appears to suggest that Trump wanted him to end the inquiry into Flynn, but didn’t explicitly direct him to do so. The live-stream told that story, confusing as it might be, but the networks, in their chyrons, cherry-picked what parts to highlight; in the process, they seemed to offer their respective viewers two completely different interpretations. The Washington Post scraped all the chyrons shown during the hearing by three networks—MSNBC, CNN, and Fox—and published them alongside each other, revealing the alternate realities among them.  
Perhaps the quietest place to watch the Comey hearing was on C-SPAN, the no-frills network that has spent decades solely streaming government proceedings, and especially congressional hearings. C-SPAN has its own set of anchors and guests on its morning show, but they get out of the way when the action begins on Capitol Hill, at the White House, and elsewhere. C-SPAN is indeed a provider of news, but even Trump could hardly call it “dishonest.” A Washington Post article from 1989 marking C-SPAN’s 10th anniversary describes the network as “America’s town hall,” a nod to the nature of live-streaming that puts some of the power of interpretation into the viewers’ hands first.
Live-streaming technology has been around since the 1990s, but the explosion the of internet and social media has propagated it to all corners of the world, bringing people closer to a broad spectrum of experiences. It has galvanized a stronger sense of immediacy not found with regular-old television, sometimes with distressing and confusing implications. The technology can capture such precious moments like the birth of a giraffe in New York, or a wedding that guests can’t attend. It can also capture some of the most harrowing aspects of humanity, like violent police shootings, a murder of an innocent bystander, or a teenager’s suicide. Live-streams have one thing in common; as they happen, there’s usually no one weighing in. It’s what comes after—the context, the outrage, the official statements, the attempts to understand or mislead—that can alter viewers’ perception of the footage.
After the Comey hearing was over, the live-streams went black. Journalists returned to pumping out analysis of the testimony to decipher and dissect for their viewers and readers, in their sometimes separate realities, the meaning of what they witnessed. C-SPAN, naturally, went back to streaming less interesting hearings. The other streams, the noisier ones tinged with filters and commentary, flowed on.


Binky is an app that does everything an app is expected to do. It’s got posts. It’s got likes. It’s got comments. It’s got the infinitely scrolling timeline found in all social apps, from Facebook to Twitter, Instagram to Snapchat.
I open it and start scrolling. Images of people, foods, and objects appear on and then vanish off the screen. Solar cooker. B.F. Skinner. Shoes. Marmalade. Sports Bra. Michael Jackson. Ganesha. Aurora Borealis. These are “binks,” the name for posts on Binky.
I can “like” a bink by tapping a star, which unleashes an affirming explosion. I can “re-bink” binks, too. I can swipe left to judge them unsavory, Tinder-style, and I can swipe right to signal approval. I am a binker, and I am binking.
There’s just one catch: None of it is real. Binky is a ruse, a Potemkin-Village social network with no people, where the content is fake and feedback disappears into the void. And it might be exactly the thing that smartphone users want—and even need.
* * *
It’s strange to think of content as optional. When Bill Gates declared that “Content is King” in 1996, he meant that digital content creators would make more money online than computer manufacturers. Gates cited television as a precursor: It was an invention that created many industries, but broadcasters—the content creators—were the long-term winners on TV.
Gates was right and wrong. Content, from e-commerce to social media, did drive  huge profits in the two decades since. But equipment also produced enormous wealth—just look at Apple. With the rise of Facebook, Google, Uber, Microsoft, Amazon, and others, content stopped being a name for ideas alone and started signifying a confluence of machines, services, media, and ideas. This is the phenomenon some nickname #content (as a hashtag), implying that the purpose of ideas is to fill every moment with computational engagement. Technology’s effect on ordinary life is always more important than the ideas its content carries.
Marshall McLuhan was the best theorist of media as mechanisms for behavior rather than channels for ideas. His famous quip “the medium is the message” was meant to deemphasize content in favor of the media forms that make it possible. For McLuhan, the meaning of individual books, television programs, newspaper articles, movies, and software programs is just a distraction. More important: how those media change the way people think and behave in aggregate. The book, for example, creates a society for which knowledge is singular, certain, and authoritative thanks to the uniformity of print.
The smartphone’s effects have evolved and changed. When I wrote about the iPhone shortly after its launch, I called it the geek’s Chihuahua: a glass-and-metal companion that people could hold, stroke, and pet—a toy dog for the tech set. Some years later, after games, apps, and social media made smartphone use compulsive, I dubbed the device the cigarette of this century: a source of obsessive attention that, like smoking, brings people together in a shared dependency whose indulgence also produces the calming relief of new data.
It doesn’t make sense to talk about the meaning of cigarettes or Chihuahuas. Their meaning is the pattern of their use. That’s the thing about content: Its form and meaning matters less than how it changes people’s behavior. And when it comes to smartphones, seeing and touching them is far more important than processing the meaning they deliver.
* * *
Binky eviscerates meaning by design. Every bink on Binky is a labeled image, chosen randomly and generated endlessly. Liking a bink does nothing. Swiping or re-binking sends binks nowhere. The comments are my favorite: A keyboard appears on which to type them, but each key-tap reveals a whole word in a pre-generated comment. Words, tags, or emoji continue appending until I stop typing. “This looks amazing! #harlemshake #wordsToLiveBy #rofl,” or “I dunno, I like this but it’s problematic  😹😜😁👾😱🎃😣😡.”
Binky is a social network app with no network and no socializing. And yet, Binky is not just as satisfying as “real” social apps like Twitter or Instagram, but even more satisfying than those services. Its posts are innocuous: competent but aesthetically unambitious photos of ordinary things and people. Should binkers feel the urge to express disgust at Linus Paulding or Lederhosen, they can swipe left, and Binky accommodates without consequence. And the app doesn’t court obsession by counting followers or likes or re-binks.
Dan Kurtz, the game developer and improv actor who created Binky, tells me that the idea for the app arose partly from his own feelings after reading through the current updates on Facebook or Twitter while waiting for a train. “I don’t even want that level of cognitive engagement with anything,” he explains, “but I feel like I ought to be looking at my phone, like it’s my default state of being.” Kurtz wondered what it would look like to boil down those services into their purest, most content-free form. This is what people really want from their smartphones. Not content in the sense of quips, photos, and videos, but content as the repetitive action of touching and tapping a glass rectangle with purpose and seeing it nod in response.
Binky also offers a new take on the smartphone’s effects, McLuhan-style. Some of the toy-dog aspects of mobile computing remain, along with the compulsive ones, too. But the novelty of touching the smartphone has long since ended, and the angst of its compulsive use is universally acknowledged. Those habits are here to stay, like it or not.
Standard smartphone fare inspires users to create content whose publication accrues value for the tech titans that operate walled-garden services. Those businesses transform that aggregated attention into revenue and stock value in turn. Meanwhile, the pleasure and benefit of those services dwindles by the day, as conflict and exhaustion suffocate delight and utility.
Binky offers a way to see and tolerate that new normalcy. What if the problem with smartphones isn’t the compulsion to keep up with the new ideas they deliver, but believing that the meaning of those ideas matters in the first place? Binky offers all the pleasure of tapping, scrolling, liking, and commenting without any of the burden of meaning.
The app frames its intervention with humor and mockery. Its name is a trademark for baby pacifiers, an image that also adorns the app’s icon. Calling it “Binky” implies a global infancy among apps, but also a legitimate comfort thanks to Binky’s succor. And Kurtz initially conceived of the app in a Comedy Hack Day mini-hackathon held by Cultivated Wit, a firm that produces, well, content—videos and events and software and the like. Forged from games and comedy, Binky might look like an ironic joke to some.
“Is a baby pacifier just a parody?” Kurtz retorts when I press him on the matter. It’s a good point; something that replaces another isn’t always a joke. He reminds me of my own ironic app, which, to my delight, he cites as an inspiration: a game called Cow Clicker that boiled down Facebook games to their purest form like Binky does social apps. In both cases, irony offers an in-road for some but burns out fast. Deliberate use always wins.
On that front, Kurtz makes his faith in the app’s earnest utility clear. “Look, all we want from our apps is to see new stuff scroll up from the bottom of the screen,” the Binky website reads. “It doesn’t matter what the stuff is.” That’s no gag; it’s an incisive elucidation of why people want to handle their smartphones so often.  By sparing the mental and emotional effort of taking in content and spitting back approval and commentary, Binky makes it possible to experience the smartphone as such, as a pure medium for its behavior rather than a delivery channel for social-media content.
That’s also where apps start, it turns out. Kurtz wanted to learn iOS programming, and he reasoned that the best approach would be to incorporate all the standard interface widgets. Binky was the result. What’s an app without content? Pure, unadulterated tapping and scrolling through the hollowed-out interfaces that all apps now share.
* * *
There’s a use of cigarettes beyond their chemical effects. Smoking gives people something to hold and something to do with their hands. McLuhan called it poise. And smartphones offer something similar. At the bus stop, in the elevator, in front of the television, on the toilet, the smartphone offers purpose to idle fingers. To use one is more like knitting or doodling than it is like work or play. It is an activity whose ends are irrelevant. One that is conducted solely to extract nervous attention and to process it into exhaust.
There have been attempts to cure the ills of smartphone compulsion. Fidget cubes and spinners offer a recent example, doodads that offer mechanical intrigue that might, some users hope, distract them from the draw of the smartphone. But these devices fail to cop to the smartphone’s victory in standardizing the mechanics of idle effort. The tapping, the scrolling, the liking, the #content, even. Those must be preserved. Binky offers an unexpected salve: a way to use a smartphone without using one to do anything in particular. Isn’t that all anyone really wants?


Perhaps the aphorism should be changed to “In Google, veritas.” Where do people go with their most intimate worries, thoughts, and fears? Not the nearest water cooler or humblebrag app. More likely, they’ll seek comfort in the relative privacy of a search box.
Seth Stephens-Davidowitz, a former data scientist at Google, used his data-analysis skills to learn what was really on Americans’ minds. The result, a new book called Everybody Lies, shows how the terms and questions people type into search engines don’t at all match what they claim on surveys.
“So for example,” he told me recently, “there have historically been more searches for porn than for weather.” But just 25 percent of men and 8 percent of women will admit to survey researchers that they watch porn.
In addition to Google, some of his research comes from tape-recorder (rather than self-report) studies, which can provide a similar truth-serum effect.
I recently spoke with Stephens-Davidowitz about some of the most surprising findings from his book, which spans data on gender norms, prejudice, and romance. We focused on the search data about sex and relationships, because who are we kidding. An edited version of our conversation follows:
Olga Khazan: Speaking of porn, I was wondering if you could talk about pornography featuring violence against women. What's surprising about who looks for that, and what might that tell us?
Seth Stephens-Davidowitz: It’s a big theme of pornography, but I think the somewhat surprising thing is that it’s far more popular among women than men. It’s one of the most popular genres of pornography for women. Just about every search that is looking for violent porn is roughly twice as common among women than men.
Of course the danger is that somehow people will hear this and they’ll think that somehow this makes rape a less horrific crime, which it doesn’t. It’s just a fantasy, of course it doesn’t mean that they want that in real life.
Khazan: To me that suggested that there’s a really big distinction between fantasy life and real life, as far as people's sexual desires.
Stephens-Davidowitz: Well it’s kind of similar also to horror movies, which are also [popular] among women. I don't think women want to be kidnapped in real life, but many women enjoy watching movies featuring kidnappings.
Women also search for a lot of lesbian porn, even women who do not consider themselves lesbians.
Khazan: So let’s say you stop watching porn and actually go on a date with someone. How can a man tell if a woman is interested in him, and vice-versa?
Stephens-Davidowitz: This is a study where researchers gave tape recorders to men and women, heterosexual men and women, who are on speed dates. Then they measured whether the men and women wanted to go on a second date. Then they said: What words do men and women use on first dates that suggest that they want to go on a second date, or that can improve the chances that a partner wants to go on a second date?
For the women, a woman frequently signals interest by talking about herself using the word “I” a lot. A man signals interest by talking in a deep monotone voice. A woman signals disinterest by using hedge words, such as “sort of,” “kind of,” or “probably.” A man can increase the odds of a woman wanting a second date by laughing at her jokes or showing support, such as saying “that must have been difficult” or “that sounds tough.”
Of course that's not rocket science, but I think a lot of men probably still need to read it. A woman can increase the odds of a man wanting a second date by talking about herself a lot, by using the word “I.” That kind of goes against conventional wisdom. I think a lot of women think that they shouldn't talk too much about themselves. But, men seem to like when a woman opens up on a first date.
Khazan: Alright, and once they’ve been dating a while … what's the number one search complaint about boyfriends?
Stephens-Davidowitz: That my boyfriend won't have sex with me.
Khazan: You said that's more common than “my girlfriend won't have sex with me,” right?
Stephens-Davidowitz: Yeah, it's about twice as common. That doesn't mean that twice as many boyfriends are refusing sex, relative to girlfriends. It may be that when a boyfriend doesn't want sex, women are more likely to turn to Google, because it's more surprising. Because men in popular culture are supposed to want sex all the time. But, I think this data does show that men avoiding sex is probably more common than is traditionally thought.
Khazan: Why are they so reluctant to have sex? What are men's biggest insecurities about their bodies?
Stephens-Davidowitz: Men tend to be insecure about the size of their penises. It definitely wasn't too surprising. It was surprising the degree of it. I estimate that men ask more questions about their penis than any other body part. Men's top concern about the aging process is not their blood pressure, cholesterol, or potential memory problems. It's whether their penis is getting smaller.
Women don't usually search about their partner's penis. When they do, they're about as likely to complain that it's too big and hurts as that it's too small.
Khazan: Do men start worrying about actual health issues as they get older, or is that pretty consistent?
Stephens-Davidowitz: We don't know exactly. You don't know the age of a searcher for sure.
Khazan: Okay, what about women? What are they concerned about?
Stephens-Davidowitz: I think the main insecurity, and this did surprise me, I didn’t know about it at all, was vaginal odor. That takes up a good percentage of women's questions about their genitals. I think there's a lot of value in knowing this information because this isn't really talked about in most sex ed classes, but there clearly is a fairly widespread paranoia among many women, particularly younger women, around odors.
So it clearly is something that should be talked about. What's normal and what’s maybe a cause for concern? It's a big issue that we didn't know about, because it's a little hush hush because it's embarrassing to a lot of people. But because people tell Google everything, now we know how widespread this insecurity is.
Khazan: And do men search for, "I don't like the way my girlfriend's vagina smells"?
Stephens-Davidowitz: Yeah, they do. This is kind of humorous, they're concerned that it smells like condoms or another man’s semen. Because [that, in their minds, means] she may be cheating on them.
Khazan: Despite all this insecurity and worries about smells, how often do people actually have sex? What's the disparity between how much they say they have sex and how much they actually do?
Stephens-Davidowitz: They have a lot less sex than they say they do. The way I studied this is I looked at condom data. The General Social Survey asks people how frequently they have sex, whether it's heterosexual or homosexual sex, and whether they use a condom. You do the math. Heterosexual women say they use 1.1 billion condoms every year in heterosexual sex. Men say they use 1.6 billion condoms in heterosexual sex, but you know that someone's lying. So who’s lying?
Only 600 million condoms are sold every year in the United States. Some of them [are used by] gay men and some of them thrown out. They're exaggerating how frequently they use a condom. This doesn't mean that they are lying about how frequently they have sex. They may just be lying about how frequently they use protection when they do have sex, but if you look at how frequently American women of fertility age say they have sex without using any contraception, if they really were having that much unprotected sex, there would be more pregnancies every year in the United States. I think everybody in surveys exaggerates how frequently they have sex, because in today's culture there is a lot of pressure to have a lot of sex and to not admit if you're having not that much sex. For both men and women, there is a pressure to exaggerate.
Khazan: Another thing that I thought was interesting was that “Is my husband gay?” is a more popular search term than “Is my husband cheating?” Why is that?
Stephens-Davidowitz: “Is my husband gay?” is most common in states where it's hard to be gay, states like South Carolina and Mississippi and Tennessee. I think some of the husbands are gay in those states. Also, the percentage of porn searches that are for gay porn is much higher in these states than the percentage of men who say they’re gay.
So I think it is true that in [places like] Mississippi, South Carolina, and Tennessee, there is a risk of men being gay. That said, I think that women are probably a little too concerned that their husband may be gay. I think there are 10 times more searches for “Is my husband gay?” than “Is my husband depressed?” But, there are a lot more depressed men married to women than gay men married to women.
I think it goes back to how there's not that much sex happening in the United States and there are a lot of sexless marriages. It may be that many women in a sexless marriage, their first thought is, “Oh he must be gay.” Which probably isn't usually the case. There are lots of other reasons a man might not want to have sex.
Khazan: It’s a little conceited of us. “Oh, he must be gay.”
Stephens-Davidowitz: Yeah, well, I probably do the same thing. Anytime a woman rejects me, I'm just like, “She's a lesbian.” Which is not really true probably, but I think it's a little bit of a defense mechanism.
It's kind of a weird contrast. On the one hand you see this enormous insecurity online—an almost needless insecurity. But then you have the “Is my husband gay?” as soon as he doesn't want sex. Which is a defense mechanism.
Khazan: Did you have any takeaways or big insights about Americans’ personal lives that struck you when you were done researching this?
Stephens-Davidowitz: I think there's two. One is depressing and kind of horrifying. The book is called Everybody Lies, and I start the book with racism and how people were saying to surveys that they didn't care that Barack Obama was black. But at the same time they were making horrible racist searches, and very clearly the data shows that many Americans were not voting for Obama precisely because he was black.
I started the book with that, because that is the ultimate lie. You might be saying that you don't care that [someone is black or a woman], but that really is driving your behavior. People can say one thing and do something totally different. You see the darkness that is often hidden from polite society. That made me feel kind of worse about the world a little bit. It was a little bit frightening and horrifying.
But, I think the second thing that you see is a widespread insecurity, and that made me feel a little bit better. I think people put on a front, whether it's to friends or on social media, of having things together and being sure of themselves and confident and polished. But we're all anxious. We’re all neurotic.
That made me feel less alone, and it also made me more compassionate to people. I now assume that people are going through some sort of struggle, even if you wouldn't know that from their Facebook posts.


President Donald Trump wants to cut a budget the Bureau of Land Management uses to care for wild horses. Instead of paying to feed them, he has proposed lifting restrictions preventing the sale of American mustangs to horse meat dealers who supply Canadian and Mexican slaughterhouses.
Horse meat, or chevaline, as its supporters have rebranded it, looks like beef, but darker, with coarser grain and yellow fat. It seems healthy enough, boasting almost as much omega-3 fatty acids as farmed salmon and twice as much iron as steak. But horse meat has always lurked in the shadow of beef in the United States. Its supply and demand are irregular, and its regulation is minimal. Horse meat’s cheapness and resemblance to beef make it easy to sneak into sausages and ground meat. Horse lovers are committed and formidable opponents of the industry, too.
The management of wild horse herds is a complex issue, which might create difficulty for Trump. Horse meat has a long history of causing problems for American politicians.
* * *
Horses originated in North America. They departed for Eurasia when the climate cooled in the Pleistocene, only to return thousands of years later with the conquistadors. Horses became a taboo meat in the ancient Middle East, possibly because they were associated with companionship, royalty, and war. The Book of Leviticus rules out eating horse, and in 732 Pope Gregory III instructed his subjects to stop eating horse because it was an “impure and detestable” pagan meat. As butchers formed guilds, they too strengthened the distinction between their work and that of the knacker, who broke down old horses into unclean meat and parts. By the 16th century, hippophagy—the practice of eating horse meat—had become a capital offense in France.
However, a combination of Enlightenment rationalism, the Napoleonic Wars, and a rising population of urban working horses led European nations to experiment with horse meat in the 19th century. Gradually, the taboo fell. Horses were killed in specialist abattoirs, and their meat was sold in separate butcher shops, where it remained marginalized. Britain alone rejected hippophagy, perhaps because it could source adequate red meat from its empire.
America also needed no horse meat. For one part, the Pilgrims had brought the European prohibition on eating horse flesh, inherited from the pre-Christian tradition. But for another, by the 1700s the New World was a place of carnivorous abundance. Even the Civil War caused beef prices to fall, thanks to a wartime surplus and new access to Western cattle ranges. Innovations in meat production, from transport by rail to packing plants and refrigeration, further increased the sense of plenty. Periodic rises in the price of beef were never enough to put horse on the American plate.
Besides, horse meat was considered un-American. Nineteenth-century newspapers abound with ghoulish accounts of the rise of hippophagy in the Old World. In these narratives, horse meat is the food of poverty, war, social breakdown, and revolution—everything new migrants had left behind. Nihilists share horse carcasses in Russia; wretched Frenchmen gnaw on cab horses in besieged Paris; poor Berliners slurp on horse soup.
But in the 1890s, a new American horse meat industry arose, if awkwardly. With the appearance of the electric street car and the battery-powered automobile, the era of the horse as a transportation technology was ending. American entrepreneurs proposed canning unwanted horses for sale in the Old World, paying hefty bonds to guarantee they wouldn’t sell their goods at home. But Europe had higher standards and didn’t like the intrusion of American meat onto its home market. U.S. aversion to regulation had led to food scares and poisonings. When French and German consuls visited a Chicago abattoir suspected of selling diseased horse to Europe, opponents tried to smear the U.S. Agriculture secretary, who had previously intervened. By 1896, the fledgling industry was faltering: Belgium barred U.S. horse meat, Chicagoans were rumored to be eating chevaline unwittingly, and the price of horses had fallen so drastically that their flesh was being fed to chickens because it was cheaper than corn.
In 1899, horse meat was dragged into one of the highest-profile food scandals of the century: the notorious Beef Court investigating how American soldiers fighting in the Spanish-American War ended up poisoned by their own corned meat. Many speculated wrongly that the contaminated beef was in fact horse meat. The first decade of America’s horse meat industry had been an unprofitable, ill-regulated disaster for the country’s reputation. The new regulations put in place in the 1906 Pure Food Act could not reverse this overnight.
* * *
When beef prices rose as canners shipped it abroad during World War I, Americans finally discovered horse steak. By 1919, Congress was persuaded to authorize the Department of Agriculture to provide official inspections and stamps for American horse meat, although as soon as beef returned after the war, most citizens abandoned chevaline.
The end of the war meant another drop in demand for range-bred horses no longer needed on the Western Front. A dealer, Philip Chappel, found a new use for them: Ken-L-Ration, the first commercial canned dog food. His success attracted perhaps the first direct action in the name of animal liberation: A miner named Frank Litts twice attempted to dynamite his Rockford, Illinois packing plant.
During World War II food shortages, horse meat once again found its way to American tables, but the post-war backlash was rapid. “Horse meat” became a political insult. “You don’t want your administration to be known as a horse meat administration, do you?” the former New York Mayor Fiorello La Guardia demanded of his successor William O’Dwyer. President Truman was nicknamed “Horse meat Harry” by Republicans during food shortages in the run up to the 1948 “Beefsteak Election.” In 1951, reporters asked if there would be a “Horse meat Congress,” one “that put the old gray mare on the family dinner table.” When Adlai Stevenson ran for president in 1952, he was also taunted as “Horse meat Adlai” thanks to a Mafia scam uncovered in Illinois when he was governor.
Although work horses vanished by the 1970s and mustangs were finally under federal protection, the growing number of leisure horses led to another surge in horse slaughter. The 1973 oil crisis pushed up the price of beef and, inevitably, domestic horse meat sales rose. Protestors picketed stores on horseback, and Pennsylvania Senator Paul S. Schweiker floated a bill banning the sale of horse meat for human consumption.
But once again the bubble burst. Competition sent beef prices into freefall. Even poor Americans didn’t need to buy the “poor man’s beef,” so U.S. manufacturers continued to export horse meat to Europe and Asia. Politicians began to apply pressure. In the early 1980s, Montana and Texas senators shamed the Navy into removing horse meat from commissary stores. The few remaining horse-packing plants dwindled during a market squeeze that also drove down welfare standards. Sick, injured, or distressed horses were driven long distances to slaughter under poor conditions.
In 1997, the Los Angeles Times broke the news that 90 percent of the mustangs removed from the range by the Bureau of Land Management had been sold on for meat by their supposed adopters. An Oregon horse abattoir called Cavel West was named in the report. It burned down that July, in an attack claimed by the Animal Liberation Front on behalf of the mustangs. The members of the ALF cell responsible were tried for terrorism, but Cavel West was never rebuilt. Nonviolent activists also applied pressure to the horse meat business, with California banning the transport and sale of horses for meat.
Activists and politicians worked to shut down the remaining abattoirs in the years that followed. In early September 2006, the Horse Slaughter Prevention Act passed the U.S. House, with Republican John Sweeney calling the horse meat business “one of the most inhumane, brutal and shady practices going on in the United States today.” Horse slaughter was not outlawed, but both federal and commercial funding for inspections was canceled, effectively shutting down the business.
Meanwhile, the town of Kaufman, Texas, mobilized against the Belgian-owned abattoir on their outskirts that paid little tax but spilled blood into the sewage system. The plant, along with another in Fort Worth, were closed. In DeKalb, Illinois, the only remaining American horse meat plant burned down in unexplained circumstances. The owners were prevented from rebuilding, as Illinois once more passed a law to stop the horse meat business. Horse slaughter ceased on U.S. soil, at least for domestic use as food. Even so, American horses were still being transported long distance to Mexican and Canadian abattoirs.
* * *
The 2009 financial crisis dealt the equestrian industry a heavy blow. The pro-slaughter lobby, backed by a 2011 GAO study, suggested that American horses had suffered, as owners no longer receiving meat money would not pay to dispose of them. Groups like United Horsemen coopted Tea Party rhetoric to compare animal-welfare campaigners to the Nazis. Opponents pointed out that poor paperwork meant many slaughter-bound horses had been treated by drugs that should have ruled them out of the food chain. Across America, both sides clashed when Obama signed a new law lifting the ban on funding for inspections. New abattoirs were proposed, but town after town blocked the measures. The 2014 Obama budget once more ruled out a revival. Meanwhile, the horses continued to be shipped to Mexico and Canada.
Today, all the familiar contradictions of the American horse meat business are playing out again, as Trump looks toward horse meat as a cost-cutting measure. Ranges are overflowing with mustangs. Animal-welfare information has disappeared from government websites, and the administration is rumored to have called on the GAO to launch another study into the benefits of building domestic abattoirs.
And yet, without adequate funding for proper inspections in a reborn U.S. horse meat industry, the market might languish. Europe is already skeptical of Mexican and Canadian exports sourced from the United States, making horse meat less profitable anyway.
Forever marginal, always unsteady, the business of packing and selling the poor man’s beef could boom and crash again in America. If it does, Trump might find himself sporting a new political epithet: Horse-Meat Donny.
This article appears courtesy of Object Lessons.


Imagine someone told you to draw a pig and a truck. Maybe you’d sketch this:
​
Easy enough. But then, imagine you were asked to draw a pig truck. You, a human, would intuitively understand how to mix the salient features of the two objects, and maybe you’d come up with something like this:
​
Note the little squiggly pig tail, the slight rounding of the window in the cab, which recalls an eye. The wheels have turned hoof-like, or alternatively, the pig legs have turned wheel-like. If you’d drawn it, I, a fellow human, would subjectively rate this a creative interpretation of the prompt “pig truck.”
Until recently, only human beings could have pulled off this sort of conceptual twist, but no more.  This pig truck is actually the output of a fascinating artificial intelligence system called SketchRNN, a part of a new effort at Google to see if AI can make art. It’s called Project Magenta, and it’s led by Doug Eck.
Last week, I visited Eck at Google Brain team’s offices in Mountain View, where Magenta is housed. Eck is clever, casual, and self-effacing. He received his Ph.D. in computer science from the University of Indiana in 2000, and has spent the intervening years working on music and machine learning, first as a professor at the University of Montreal (a hotbed for artificial intelligence) and then at Google, where he worked at Google Music before heading to Google Brain to work on Magenta.
Eck’s drive to create AI tools for making art began as a rant, “but after a few cycles of thinking,” he said, “it became, ‘Of course we need to do this, this is really important.’”
The point of SketchRNN, as he and Google collaborator David Ha have written, is not only to learn how to draw pictures, but to “generalize abstract concepts in a manner similar to humans.” They don’t want to create a machine that can sketch pigs. They want to create a machine that can recognize and output “pigness,” even if it is fed prompts, like a truck, which don’t belong in the barnyard.
The implicit argument is that when humans draw, they make abstractions of the world. They sketch the generalized concept of “pig,” not any particular animal. That is to say, there is a connection between how our brains store “pigness” and how we draw pigs. Learn how to draw pigs and maybe you learn something about the human ability to synthesize pigness.
Here’s how the software works. Google built a game called, “Quick, Draw!” which, as people played, generated a large database of human drawings of all kinds of stuff: pigs and rain, firetrucks and yoga poses, gardens and owls.
When we sketch, we compress the rich, colorful, noisy world into just a few movements of a (digital) pen. It is these simple strokes that are the underlying dataset for SketchRNN. Each class of drawing—cat, yoga, rain—can be used to train a particular kind of neural network using Google’s open-source TensorFlow software library. This is distinct from the kind of photograph-based work that’s inspired so many news stories, like when a machine can render a photograph in the style of Van Gogh or the original DeepDream, or drawing any shape and having it fill in with “catness.”
These projects all feel, subjectively, to humans, uncanny. They are interesting because they make images that are sort of like, but not exactly like, human perception of the real world.
The outputs of SketchRNN, however, don’t feel uncanny at all. “They feel so right,” Eck told me. “I don’t want to say ‘so human,’ but they feel so right in a way that these pixel-generation things don’t.”
This is a core insight of the Magenta team. “Humans … do not understand the world as a grid of pixels, but rather develop abstract concepts to represent what we see,” Eck and Ha argue in their paper describing the work. “From a young age, we develop the ability to communicate what we see by drawing on paper with a pencil or crayon.”
And if humans can do it, Google would like machines to be able to do it. Last year, Google CEO Sundar Pichai declared the company “artificial intelligence-first.” AI, for Google, is a natural extension of its original mission “to organize the world's information and make it universally accessible and useful.” What’s changed is that now the information is being organized for artificial intelligences, which then make it accessible and useful for people. Magenta is one of Google’s wilder attempts to organize and understand a particular human domain.
Machine learning is the broadest term for the tools Google has adopted. ML, as it is often abbreviated, is a way of programming computers to teach themselves how to do various tasks, usually by feeding them labeled data to “train” on. One popular way of doing machine learning is with neural networks that are very loosely modeled on the brain’s system of connections. Various nodes (artificial neurons) are connected to each other with different weightings that respond to some inputs, but not others.
In recent years, neural networks with multiple layers have proven very successful in solving tough problems, especially in translation and image recognition/manipulation. Google has rebuilt many of its core services on these new architectures. Mimicking the known functioning of our own brains, these networks have interconnected layers that recognize different patterns in an input (say, an image). A low-level layer might contain neurons that respond to simple pixel level patterns of light and dark. A high-level layer might respond to dog faces or cars or butterflies.
Building networks with these kinds of architectures and mechanics can be unreasonably effective. Computing problems that were remarkably difficult become a matter of tuning the training of a model and then leaving some graphics-processing units to compute for a while. As Gideon Lewis-Kraus described in The New York Times, Google Translate had been a complex system built over 10 years. Then the company rebuilt it with a deep-learning system in 9 months. “The A.I. system had demonstrated overnight improvements roughly equal to the total gains the old one had accrued over its entire lifetime,” Lewis-Kraus wrote.
Because of this, there have been an explosion of uses and types of neural networks. For SketchRNN, they used a recurrent neural network, which deals with sequences of inputs. They trained the network on the progression of pen strokes people made to draw different things.
The easiest way to describe training is as a type of encoding. Data (sketches) are fed in, and the network tries to come up with the general rules for what it is processing. Those generalizations are a model of the data, which is stored in the mathematics describing the propensities of the neurons in the network.
That configuration is evocatively called the latent space or Z (zed) and it is where the pigness or truckness or yoganess is held. Sample it, as the AI people say, by asking the system to draw what it has been trained on, and SketchRNN spits out a pig or a truck or a yoga pose. What it draws is what it has learned.
What can SketchRNN learn? Below is a network trained on firetrucks generating new fire trucks. Inside the model, there is a variable called “temperature,” which allows the researchers to crank the randomness of the output up or down. In the following images, bluer images have the temperature turned down, redder ones are “hotter.”
​
Or maybe you’d prefer to see owls:
​
And the best example of all, yoga poses:
​
Now, these are like human drawings, but they are not themselves drawn by any human. They are reconstructions of how a human might sketch such a thing. Some of them are quite good and others are less so, but they would all pretty much make sense if you were playing Pictionary with an AI.
SketchRNN is also built to accept input in the form of human drawings. You send something in and it tries to make sense of that. Working with a model trained on cat data, what would happen if you lobbed in a three-eyed cat drawing?
​
You see that? In the various outputs from the model to the right (again showing different “temperatures”), it strips out the third eye! Why? Because the model has learned that cats have triangular ears, two whiskers, a roundish face, and only two eyes.
Of course, the model does not have any idea what an ear actually is or if cats’ whiskers move or even what a face is or that our eyes can transmit images to our brains because photons change the shape of the protein rhodopsin in specialized cells in the retina. It knows nothing of the world to which these sketches refer.
But it does know something about how humans represent cats or pigs or yoga or sailboats.
“When we start generating a drawing of a sailboat, the model will fill in with hundreds of other models of sailboats that could come from that drawing,” Google’s Eck told me. “And they all kind of make sense to us because the model has pulled out from all this training data the platonic sailboat—you’ll kill me for saying this—but the ur sailboat. It’s not a question of specific sailboats, but sailboatness.”
As soon as he said it, he seemed to regret his momentary loftiness. “I’m gonna have the philosophers come crush me for that,” he said. “But as a handwavey thing, it makes sense.” (The Atlantic’s resident philosopher Ian Bogost told me, “Philosophically speaking, this is pure immanent materialism.”)
The excitement of being a part of the artificial intelligence movement, the most exciting technological project ever conceived, at least to those within it, and to a lot of other people, too—well, it can get the better of even a Doug Eck.
I mean, train a network on drawings of rain. Then input a sketch of a fluffy cloud, and, well, it does this:
​
Rain falls out of the cloud you’ve sent into the model. That’s because many people draw rain by first drawing a cloud and then drops coming out of it. So if the neural network sees a cloud, it makes rain fall out of the bottom of that shape. (Interestingly, the data is a succession of strokes, though, so if you start with the rain, the model will not produce a cloud.)
It’s delightful work, but in the long project to reverse engineer how humans think, is this a clever side project or a major piece of the puzzle?  
What Eck finds fascinating about sketches is that they contain so much with so little information. “You draw a smiley face and it’s just a few strokes,” he said, strokes that look nothing like the pixel-by-pixel photographic representation of  a face. And yet any 3-year-old could tell you a face was a face, and if it was happy or sad. Eck sees it as a kind of compression, an encoding that SketchRNN decodes and then can re-encode at will.
It’s not unlike Scott McCloud’s famous (among a certain kind of nerd) case for the power of cartooning.  

“I’m very supportive of the SketchRNN work and it's really cool,” said Andrej Karpathy, a researcher at OpenAI, who has become a central node in AI research dissemination. But he also noted that they have made some very strong assumptions about the importance of strokes into their model, which means they are less useful to the overall enterprise of developing artificial intelligence.
“The generative models we develop usually try to be as agnostic as possible to the details of the dataset, and should work no matter what data you throw at them: images, audio, text, or whatever else,” he said. “Except for images, none of these are made up of strokes.”
“I'm also perfectly okay with people making strong assumptions, encoding them in the models, and getting more impressive results in the respective specific domains,” he added.
Eck and Ha are building something closer to a chess-playing AI than an AI that could figure out and play the rules any game. To Karpathy, the scope of their current work seems limited.
But there are some reasons to think that line drawings are fundamental to the way humans think. The Googlers are not the only researchers who have become intrigued by the power of sketches. In 2012, Georgia Tech’s James Hays teamed up with Technische Universität Berlin’s Mathias Eitz and Marc Alexa to create a dataset of sketches as well as a machine learning system for identifying them.
For them, sketches represent a form of “universal communication,” something all humans with standard cognitive functioning can do and have done. “Since prehistoric times, people have rendered the visual world in sketch-like petroglyphs or cave paintings,” they write. “Such pictographs predate the appearance of language by tens of thousands of years and today the ability to draw and recognize sketched objects is ubiquitous.”
They point to a paper in the Proceedings of the National Academy of Sciences by neuroscientist Dirk Walther at the University of Toronto that “suggests that simple, abstracted sketches activate our brain in similar ways to real stimuli.” Walther and his co-authors hypothesize that line drawings “capture the essence of our natural world,” even if on a pixel-by-pixel basis, a line-drawing of a cat looks nothing like a picture of a cat.
If the neurons in our brains work within the layered hierarchies that neural networks mimic (slash caricature), sketches may be one way to grasp at the layer that stores our stripped-down concepts of objects—“the essence” as Walther put it. That is to say: they may tell us something important about the fresh way that humans began to think when our ancestors rounded into modern form some time in the last 100,000 years. Sketches, on cave walls or on the backs of napkins, may be the literal depiction of the jump from horse to horseness, from everyday experience to abstract, symbolic thought, and with it, the modern human.
Most of modern life flows from that transition: language, money, mathematics, and eventually computing itself. So, it would be fitting if sketches ended up playing an important role in the creation of a significant artificial intelligence.
But of course, for humans, a sketch is a depiction of a real thing. We can easily understand the relationship between the abstract four-line representation and the thing itself. The concept means something to us.  For SketchRNN, a sketch is a sequence of pen strokes, a shape being formed through time. The task for the machine is to take the essences of things depicted in our drawings and try to use them to understand the world as it is.  
The SketchRNN team is exploring in many different directions. They might build a system that tries to get better at drawing through human feedback. They could train models on more than one kind of sketch. Maybe they'll find a way to see if their model, trained to recognize pigness in sketches, say, can generalize to photorealistic images. I'd love to see their model plugged into others that, for example, have been trained on traditional photographs of cats. This would let them skin cat drawings, coloring in the sketches with what a UC Berkeley-created neural network knows about the texture of cats.  
But they themselves admit that SketchRNN is a “first step” and that there is so much to learn. The arc of human life that these sketch-decoding machines find themselves a part of is long. The human history of art has occurred on roughly the opposite of technological time.
In covering cave paintings in Europe for The New Yorker, Judith Thurman wrote that Paleolithic art remained mostly unchanged for “for 25 millennia with almost no innovation or revolt.” She notes that’s “four times as long as recorded history.”
The art must have been deeply satisfying and its broader culture stable, a scholar tells Thurman.  
Computers, and especially the new artificial intelligence techniques, are destabilizing long-held notions of what humans are good at. Humans fell to machines in checkers in the ’90s. Then chess. Most recently Go.  
But the power of recent work in AI is not due to the speed at which the state of the art is advancing (though it is moving very fast). For Eck, it’s more that they are striving after the very bedrock of how humans think, and by extension, who we are. “A really core part of art is its basic humanity, that we’re communicating with each other,” Eck told me.  
Taking in the whole enterprise of deep learning, all the different people working on the underlying mechanisms of human life—how we see, how we move, how we talk, how we recognize faces, how we structure words into stories, how we play music— and it looks a little like an outline not of any particular human, but humanness.
Right now, it’s low-resolution, a caricature, a stick figure of real thought, but it is not hard to recognize the gathering intelligence from the sketch.


Across the computer security world yesterday, heads were shaking.
The FBI filed a criminal complaint against Reality Winner, an NSA contractor, who the agency alleges stole classified documents and shared them with an “online news outlet” believed to be The Intercept. Because the documents in question appear to have been printed, some security experts have been wondering if a mysterious code used by some printers is to blame for Winner’s capture. That code is an almost-invisible grid of dots that some color printers ink into every document they print.
The complaint also details how agents say they tracked the leak back to Winner. The news org contacted the National Security Agency and said they were “in possession of what they believed to be a classified document.” The news organization then sent that document to the NSA, presumably for verification. “The U.S. Government Agency examined the document shared by the News Outlet and determined the pages of the intelligence reporting appeared to be folded and/or creased, suggesting they had been printed and hand-carried out of a secured space,” the complaint continues.
From there, the agents say that they simply looked to see who had printed the document—six people had—and then discovered that one of them, Winner, had been in contact with the media company in question from her work computer (although on an unrelated topic).
When FBI agents showed up at her house, they say she confessed to “removing the classified intelligence reporting from her office space, retaining it, and mailing it from Augusta, Georgia, to the News Outlet.” She faces up to 10 years in prison.
Given what is in the public record from the FBI complaint, Winner was almost certain to get caught, and some have argued that The Intercept could not have prevented that. Obviously, the NSA monitors and records who prints what documents. There’s an audit trail there, which one imagines an NSA contractor would know.
That’s why many in the computer security have deemed the way the leak was made and handled to be a terrible example of “operational security,” or as you’ll see it relentlessly abbreviated, “opsec.”
The Intercept released a statement today, however, reminding people not to take the FBI’s complaint as fact.
“While the FBI’s allegations against Winner have been made public through the release of an affidavit and search warrant, which were unsealed at the government’s request, it is important to keep in mind that these documents contain unproven assertions and speculation designed to serve the government’s agenda and as such warrant skepticism,” they wrote. “Winner faces allegations that have not been proven. The same is true of the FBI’s claims about how it came to arrest Winner.”
If Winner wasn’t found the way the complaint claims, the mysterious dot code is one other way the FBI could have found her, as the research blog Errata Security spelled out in detail.
In fact, the document that The Intercept published contains these dots, and the code spells out a date—May 9—that matches the FBI affidavit’s account of Winner’s printing. It also notes a serial number, which the NSA could obviously match back up to a machine in their offices.
Let’s invert the colors.
And then crank up the brightness of the dots.
As you can probably see now, the dots are printed in a repeating rectangular pattern. They’re a code. And the Electronic Frontier Foundation cracked it some years ago, at least for Xerox printers.
Let’s look at the rectangular code up close.
Errata Security says that the pattern was flipped upside down, so let’s flip it.
Then they used the EFF decoder, which I’ve combined with the original image here, so you can see the pattern more easily.
Run through EFF’s parser, Errata Security found that it output a time of 6:20, a date of May 9, and a couple of possible serial numbers. As Errata Security notes, “The NSA almost certainly has a record of who used the printer at that time.” If they didn’t already have Winner in some other way, this would probably have sealed her capture.
As one might expect, the EFF has some strong opinions about these dots, and their inclusion in the firmware that runs color laser printers. They’ve been tracking the use of these dots for a decade.
“In a purported effort to identify counterfeiters the U.S. government has succeeded in persuading some color laser printer manufacturers to encode each page with identifying information,” they write.
The idea was that color laser printers might be (or become) so good that anyone could just print money. So, manufacturers were persuaded to add this countermeasure into the software that’s baked into some printers. When someone prints in color on one of these machines, the printer does everything normally, but also adds this nearly invisible identifying stamp. (If they print in black and white, though, there are no dots.)
According to a research team at the Illinois Institute of Technology that worked on ways to counteract the dot-code, some printer manufacturers don’t even acknowledge that they exist or provide any information about how they work. “It’s a trivial way of tracking,” said Louis McHugh, one of the IIT researchers. “There’s nothing high-tech about it.”
But the dots are tracking documents, and the people who print them, nonetheless.
Did these dots play a role in Reality Winner’s investigation? We don’t know, but if you’re planning on leaking some classified documents by printing them through a color printer, you may want to rethink your plan.


On a recent Thursday, I waited for an email that was supposed to contain every personal detail the internet knows about me. The message would be from an online data broker—a company that collects and sells information that many people would hope is private. This includes browsing history, online purchases, and any information about you that’s publicly available: property records, court cases, marital status, social-media connections, and more. Facebook collaborates with data brokers for targeting advertisements. In some states, the Department of Motor Vehicles, among other agencies, sells information to brokers. Brick-and-mortar stores do, too.
As I refreshed my inbox, I listened to garbage trucks outside my window begin to make their evening rounds through Taipei’s Beitou District. I work remotely, so I’ve opted to travel from city to city for the past 11 months, exploring the Canary Islands, Bulgaria, Serbia, Hungary, Israel, Vietnam, South Korea, and Taiwan, all while editing books and answering emails. The internet has been my most permanent residence, both my office and the nexus of my social life. I generate data all day as I make money, stay in touch with friends, and order e-books to read in my spare time.
In the time I’ve been away, Congress has repealed protections that would have blocked internet providers like AT&T, Comcast, and Verizon from sharing browsing data with other companies. It’s unlikely that these legislative changes will have a significant effect on the relationship between data brokers and internet-service providers, but they do bring concerns about privacy to the fore, even more than usual. And when the news first broke, it caused a lot of confusion about just how much data companies will one day be able to collect and just how far people need go to protect themselves.   
It all sounded scary enough that I wondered what would come up if I bought data about myself from a data broker. With a quick Google search, I found a company that promised to detail net worth, age, zip code, and education, among other personal information. All I had to do was upload a text file of the email addresses of people whose info I wanted (in this case, just my own) and pay a $50 fee. The whole endeavor gave me pause. It seemed like I was about to do something that violated the company’s lengthy terms of service. Then there was the queasiness about the data itself: Did I really want to know?
The report arrived in my inbox a matter of hours later with an accompanied missive trumpeting, “Wow! That was easy.” Yes. I never had to talk to a customer service representative nor identify myself. It was just like any other transaction. My misgivings gave way to glee. A strong Christmas morning vibe overtook me. Would I find something I didn’t know? There was a part of me that genuinely believed the internet knew me best: Maybe I’d discover a pattern in my life that could point toward the future—a palm reading constructed from metadata.
In the zip file, I found a PDF, a spreadsheet, and a .txt file. I chose the spreadsheet first, and this was the first of many letdowns. It was merely a summary of how many of the email addresses had provided “matches” for the various information categories. I tried again with the charts, which aren’t visually interesting when they each feature one piece of data about a single subject. The pie chart, for example, was just an uninterrupted blue circle labeled “Female 100.0%.” I got a sense I had wasted my money. Finally, I opened the.txt file, and as though I had time traveled back to the advent of personal computing, a document I was reading in Notepad was the most useful of the three: It included each data point, organized email address by email address. But much of the data was flat-out wrong.
If you like percentages, nearly 50 percent of the data in the report about me was incorrect. Even the zip code listed does not match that of my permanent address in the U.S.; it shows instead the zip code of an apartment where I lived several years ago. Many data points were so out of date as to be useless for marketing—or nefarious—purposes: My occupation is listed as “student”; my net worth does not take into account my really rather impressive student loan debt. And the information that is accurate, including my age and aforementioned net worth (when adjusted for the student debt), is presented in wide ranges.
Historically, data brokers don’t do nuance. Companies care about demographics: If they can get information that is in the right ballpark, it’s likely to suit their needs just fine. I thought opening my data would be like looking in a mirror, maybe a dressing room mirror under lighting that makes you think you should start taking many vitamins. Instead, it was like seeing an expressionist painting of myself. I caught glimpses of something I recognized, but everything was hazy and a little off.
The sight was a relief. Conversations and debates about privacy tend to take for granted that the technology invading privacy finds information that is correct. But while our data is collected aggressively these days, clearly companies still aren’t infallible. Maybe the death of privacy isn’t quite so near.
So I did something I probably should have done much earlier in the day. I got up from my computer. I went either to get lo mein or to buy dumplings from the cart outside our apartment—I don’t remember which. Some things aren’t worth tracking. Most facts couldn’t tell you who I really am, anyway.


Without once saying the words “artificial intelligence,” a stream of Apple executives described a vision of the near future in which Siri, the company’s AI avatar, stitches together the company’s many hardware products.  
And they introduced a new—and widely anticipated—entry into their lineup: a $349 cylindrical voice-controlled speaker they call HomePod.
After a strangely dystopian video in which Apple’s apps go away and the world plunges into post-apocalyptic violence, Apple CEO Tim Cook led off the company’s keynote at its big gathering for coders, the Worldwide Developers Conference, in San Jose.
The WWDC keynote tends to be a place where Apple showcases all the little incremental “refinements” they are making to their software and hardware. This year, however, there was a thread that ran through many presentations: Siri.
Through the demonstrations and talks, Apple’s vision for Siri became clearer: It is an all-purpose stand-in for predictive, helpful intelligence across all Apple devices. “Siri isn’t just a voice assistant,” said Craig Federighi, Apple’s senior VP of software engineering. “With Siri intelligence, it understands context. It understands your interests. It understands how you use your device. It understands what you want next.”
For example, Federighi said, imagine you’re planning a trip to Iceland. Siri might suggest stories about Iceland within the news app or even suggest the spelling for a difficult Icelandic place name. (Perhaps she’ll suggest some Björk for your HomePod, even.)
Even the Apple Watch has a new (and decidedly Google Now-like) face that guesses what information you might want to see on that tiny screen at any given time.
Siri suffuses all the Apple products now. It’s less a voice-UI gimmick than an organizational structure for how Apple thinks about proactive and reactive user assistance. Or, to put it slightly less generously, “Siri is turning into Watson, a generic brand for anything using simple machine learning,” tweeted Ben Cunningham, a former Facebook engineer.
The Apple presenters probably said machine learning two dozen times as they described their plans for iOS, the software that runs iPhones and iPads, watchOS, and the next version of macOS. They planned to roll out a new series of machine-learning tools for developers, which will allow app makers to access Apple’s computer vision and natural-language processing tools.
That kind of easy AI access was a theme of Google’s developer conference, too. But unlike Google and Amazon, Apple emphasized  the privacy features of their devices. For example, Amazon’s Echo speakers transmit some data to Amazon as they wait to hear the word “Alexa” and spring into action. But Apple’s HomePod will do that processing locally inside the speaker. “Until you say [‘Hey Siri’], nothing is being sent to Apple,” said Phil Schiller, Apple’s VP of marketing. And then, what is sent to the company’s servers is an “anonymous Siri ID.”
Amazon emailed me to clarify that they don't send data while the Echo is idling, waiting to be called upon. "When you use the wake word," a company spokesperson explained, "the audio stream includes a fraction of a second of audio before the wake word, and closes once your question or request has been processed."
It is true that Apple’s business model is far less dependent on amassing data about individual people than Google, Facebook, or Amazon. They sell stuff to people.
Or as analyst Horace Dediu summed up the Apple pitch: “Siri knows you. Apple doesn’t.”
Taken together with Google, Microsoft, Amazon, and Facebook’s pushes into this space, and it would seem that we’ll soon have a wide variety of systems that build and rebuild their models of your desires every moment, hoping they can provide just the right suggestion. It’ll be nudges all the way down.


It seemed like an odd pairing from the start: Elon Musk, the brilliant South African immigrant who runs companies that build electric cars and lease solar panels to homeowners, and Donald Trump, the television-obsessed politician who repeatedly describes climate change as a hoax.
Musk joined the president’s advisory councils a month after the election last year, along with a band of high-profile tech executives. Trump’s election seemed like it could hurt those in the renewable-energy business, and Musk seemed to hop on board in part to get the president’s ear. It also helped that Trump supported partnerships between NASA and private spaceflight company’s like Musk’s SpaceX. The partnership seemed shaky from the beginning, with Musk criticizing Trump’s executive order banning immigration from seven predominantly Muslim nations.
On Wednesday, as rumors of the Trump administration’s decision to withdraw the United States from the Paris climate accord swirled, Musk said he was leaving the advisory councils over the move. On Thursday, after Trump announced the nation would indeed exit the agreement, Musk followed through on his promise:
Am departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world.

He also tweeted a pointed message referencing China, Trump’s favorite culprit for many global ailments, particularly rising temperatures:
Under Paris deal, China committed to produce as much clean electricity by 2030 as the US does from all sources today https://t.co/F8Ppr2o7Rl

Others in the technology sector have also voiced their disappointment.
Brad Smith, Microsoft’s president and chief legal officer, tweeted Thursday during Trump’s speech:*
We’re disappointed with the decision to exit the Paris Agreement. Microsoft remains committed to doing our part to achieve its goals.


Elon Musk Is Betting Big on Donald Trump
Oil gas giant ExxonMobil supported the U.S. remaining in the pact, as did Rex Tillerson, the company’s former CEO and Trump’s secretary of state. Earlier this month, the CEOs of 30 companies with operations in the U.S. published an open letter to Trump about the Paris agreement. “Our business interests are best served by a stable and practical framework facilitating an effective and balanced response to reducing global [greenhouse gas] emissions,” they wrote. “The Paris agreement gives us that flexible framework to manage climate change while providing a smooth transition for business.”
Business leaders have spent months trying to change Trump’s mind on the Paris agreement, but in the end they proved unpersuasive. As for Musk, he can now try his hand at criticizing the administration from the outside, leveraging his considerable popularity with voters from both parties. He may not find success with this strategy, but at least he won’t lose face with the renewable energy community, in which he is widely perceived as a leader, moral or otherwise.
*This article originally misstated that Smith is the CEO of Microsoft. We regret the error.


Updated on June 1, 2017
Hillary Clinton came to Recode’s Code Conference with her gloves off. In an interview with the journalists Kara Swisher and Walt Mossberg, she delivered a fully baked articulation of the ways technology was “weaponized” against her campaign to aid Donald Trump.
“I take responsibility for every decision I made,” Clinton said, “but that is not why I lost.”
In previous elections, the internet was primarily used to identify likely donors and voters and then get them to give money and turn out to cast their ballots, she said. That was definitely the story of the 2008 and 2012 Obama campaigns, as I encountered them as a journalist.  
But that changed in 2016, Clinton said.
“What we thought we were doing was going to be Obama 3.0: better targeting, better messaging, and the ability turn out our voters as we identified them, and to communicate more broadly with voters,” she explained. “Here’s what the other side was doing, and they were in a different arena. Through content farms, through an enormous investment in falsehoods, fake news, call it what you will—”
“Lies,” Mossberg interjected.
“Lies, that’s a good word, too,” Clinton continued. “The other side was using content that was just flat-out false, and delivering it in a very personalized way, both above the radar screen and below. And, look, I’m not a tech expert by any stretch of the imagination, [but] that really influenced the information that people were relying on.”
She called out fake news stories on Facebook, which she said were spread by 1,000 Russian agents, as well as bots running on social media to amplify the disinformation. “It was such a new experience. I understand why people on their Facebook pages, [said] ‘Oh, Hillary Clinton did that. I did not know that! Well, that’s gonna effect my opinion about her,’” Clinton said. “And we did not engage in false content.”
Mossberg asked her why the Democrats were not better at combatting that false information. “There’s a way to weaponize tech that doesn’t involve lying or having Russians help you,” he said. “It is a political weapon. It’s a fact of life. But how do you do it?”
At that point, Clinton claimed that the data candidate Trump received from the Republican National Committee was much better than what she received from the Democratic National Committee.
“I get the nomination. I’m now the nominee of the Democratic party. I inherit nothing from the Democratic party,” she said. “I mean, it was bankrupt. It was on the verge of insolvency. Its data was mediocre to poor, non-existent, wrong. I had to inject money into it.”
(Michael Tyler, the DNC’s spokesman, notes that DNC chair Tom Perez has admitted that the committee was not “firing on all cylinders,” and says that the DNC “is now undergoing an organizational restructuring that will include a new Chief Technology Officer, who will do an in depth analysis and maintain the party’s analytics infrastructure needs.”)
Meanwhile, Clinton continued, the Republican National Committee spent $100 million on its data infrastructure between 2012 and 2016, which it handed over to Donald Trump’s campaign.
“Then you’ve got Cambridge Analytica,” Clinton said, referencing the political analytics start-up backed by the wealthy and right-wing Mercer family.
In the run up to and aftermath of the election, Cambridge Analytica got tons of press for its targeting abilities. “We are thrilled that our revolutionary approach to data-driven communication has played such an integral part in President-elect Trump’s extraordinary win," the company’s CEO said in a press release.  Later reports tended to be more circumspect about the firm’s importance.
“You can believe the hype on how great they were or how not great they were, but the fact is they added something. And I think we better understand that,” she said. “The Mercers did not invest all that money just for their own amusement.”
She described a deal that Cambridge Analytica cut with the Trump campaign that put Steve Bannon, who had been running Breitbart, into the center of Trump’s world.
“They marry content with delivery and data. And it was a potent combination,” Clinton said. “The question is where and how did the Russians get into this.”
Then, like a prosecutor walking through her argument, she talked about the 17-agency report from the intelligence community about Russian interference into the presidential election.
“[The report] concluded with high confidence that the Russians ran an extensive information war campaign against my campaign to influence voters in the election,” Clinton said. “They did it through paid advertising, we think. They did it through false news sites. They did it through these 1,000 agents. They did it through machine learning, which kept spewing out this stuff over and over again, the algorithms they developed.”
Then she asked, not-quite-rhetorically, “Who were they coordinating with or colluding with?”
Unlike previous Russian cyberattacks inside the U.S., “This was different. They went public,” she said. “The Russians, in my opinion—and based on the intel and counterintel people I’ve talked to—they could not have known how best to weaponize that information unless they had been guided.”
“Guided by Americans?” Mossberg asked.
“Guided by Americans,” Clinton answered. “And guided by people who had polling data and information.”
After a brief tour of James Comey’s behavior during the election, Kara Swisher asked Clinton who she thought was guiding the Russians. “ I hope that we’ll get enough information to be able to answer that question,” Clinton responded at first.
Swisher prompted, “But you’re leaning Trump.”
“I am leaning Trump,” Clinton said.
“We’re going to, I hope, connect up a lot of the dots,” she said. “And it’s really important because when Comey did testify before being fired this last couple of weeks, he was asked, ‘Are the Russians still involved?’ And he goes, ‘Yes. They are.’ Why wouldn’t they be? It worked for them. It is important for Americans, particularly people in tech and business, to understand, Putin wants to bring us down and he is an old KGB agent.”
Having made her case, Clinton then tried to put herself in Mark Zuckerberg’s shoes. “With respect to the platforms, I am, again, not exactly sure what conclusions we should draw,” she said. “But here’s what I believe. I believe that what was happening to me was unprecedented and we were scrambling.”
But she did have some advice for Facebook and Twitter. “They’ve gotta get back to trying to curate more effectively,” she said. “Put me out of the equation. They’ve got to help prevent fake news from creating a new reality that does influence how people think of themselves, see the world, the decisions that they make.”



In the early days, when motion pictures were still new, filming the ocean was a radical idea.
A surface-level shot of the waves was certainly feasible, but capturing footage of swaying undersea fauna, swimming fish, and marbled sunlight dancing on the seafloor? The consensus was: It couldn’t be done.
In fact, it could be. A century ago, the brothers John Ernest and George Williamson, the sons of a sea captain and inventor, would prove it. To do so, the Williamsons turned to a piece of technology their father had designed for divers in undersea repair and salvage jobs. The device was a series of flexible concentric tubes, “interlocking iron rings that stretched like an accordion,” as the Library of Congress puts it, made to suspend from a specially outfitted ship so that a diver could descend into a watertight chamber below. At one end of the tube was the boat on the surface of the water; at the other, the submersible room.
John Ernest and George were enchanted by their father’s machine. From the glass portals along the tube, they observed red snappers, yellowtails, fat groupers, and other shimmery creatures weaving through the coral reef of the Bahamas. And they had the idea of bringing a camera with them next time. Later, when they shared their still photos with newspapers—images included a blurry oblong shark and shadowy seaweed—it created a sensation.
Eventually, the Williamsons’ tubes—outfitted with a new, specialized spherical observation chamber that had a large funnel-shaped window—would be used to film the underwater scenes in the 1916 film, 20,000 Leagues Under the Sea. “I call this my magic window,” one of the silent film’s intertitle cards reads, before revealing a gray, clouded ocean view. “We gaze on scenes which you might think God never intended us to see.”
At the time, the footage was extraordinary. The film was a smashing success.
Filmmakers have been using technology to push the limits of how the ocean is portrayed ever since—and not just in live-action films. Most recently, Disney dazzled audiences with the animated film, Moana, which tells the story of a girl from the Pacific Islands who sets out on a voyage to rediscover her ancestors’ wayfinding heritage.
Moana’s directors, Ron Clements and John Musker, have been obsessed with stories about the ocean for decades. They made The Little Mermaid in 1989. The 1940 film Pinocchio, with its famous whale sequence, is what first inspired Clements to pursue a career in animation. But depictions of the ocean in those stories are nothing like what audiences see in Moana, which is as groundbreaking for its portrayal of water in 2017 as 20,000 Leagues Under the Sea was in 1916.
“Water is always hard,” said Marlon West, the co-head of effects animation for Moana. “Usually, in animation, we have a dozen water shots. They’re hard to do.” In Moana, however, the majority of the movie involves water. And the ocean isn’t just a presence; the action takes place on the water, introducing yet another layer of complexity. On top of that, Moana’s Pacific ocean is occasionally anthropomorphic, like a distant cousin to the water in James Cameron’s The Abyss.
Disney’s software team came up with a program it calls Splash—a companion to Matterhorn, which was used to create the snow in Frozen—to help automate the way the water would behave in various shots. Splash is a “fluid solver” that plugs into Houdini, third-party 3-D animation software. To use the solver, effects specialists would define the area they wanted to simulate—say, a section of water around an animated boat—then use a setting to determine what the ocean condition should be like to begin with. From there, they’d run the simulation on the pre-determined ocean surface, to animate how that area of water responds to the boat. The output from that simulation—“millions of particles,” essentially millions of new points of animation data—would then be smoothed into the final rendering of the film.
Splash also involved a series of algorithms that could simulate splashes, eddies, and wakes. The program’s buoyancy algorithm made it so the huge navigating canoes in the film bounced in and out of waves realistically. (In several shots, many of these canoes appeared in the water together, in close proximity, creating an additional animation challenge.)
“As effects artists, working with fluids, you can’t always predict what you’re going to get from your water simulation,” said Erin Ramos, the film’s effects lead. “And the hard thing with water is, if it doesn’t look right, you can really tell. Even if it’s in the background.”
Disney effects specialists told me they were able to successfully automate key ocean-movement details about 80 percent of the time—meaning you could have a production assistant simply run a script to generate the animated boat’s wake. “Running these scripts to generate these animations leaves room for the artists to focus on the artistry of the shot,” Ramos told me, “so they have time to create these sweeping shots, and the ability to have the ocean acting as a character.”
The ocean in Moana is an anthropomorphic force that occasionally nudges Moana along the way. Except the ocean character doesn’t have a face. And it doesn’t talk. (In this way, it was a bit like the animated magic carpet in Disney’s Aladdin, Osnat Shurer, the Moana producer told me.) So Disney’s effects specialists and animators were constantly navigating the tension between wanting the water to look and act like actual water—but to be magical at the same time.
“That was a big challenge,” West said. “They would animate the ocean as almost a sock puppet, and we would take that and fill it full of bubbles and liquid or we’d do a simulation over it and make it full of water to make it look more watery.”
There were many debates among animators and effects specialists over how to convert the ocean from its ordinary state into a character with agency and back again. “What made the ocean character look like water to one person, looked too agitated and aggressive to another,” West said. “You have the ocean often as a chartacter looming over toddler Moana and looming over Moana’s grandmother, and at no point do you ever want to be afraid for them. You want to be in awe.”
In the end, calibrating between those two expectations—a realistic-looking ocean that could also convey subtle warmth and encouragement as a character—meant keeping a portion of the water unnaturally smooth and rounded when it surfaced as a character. Also, there were moments when obeying the laws of physics were discarded in favor of keeping the audience focused on the characters. “Because it’s storytelling,” West told me. “It’s a stylized world. And we’re trying to create water that exists in your heart and your mind’s eye.”
Then there was the question of representing the ocean in an authentic way—not physically realistic, but culturally true. To do so, Disney formed an Oceanic Story Trust, a group of cultural practitioners from around the Pacific who acted as consultants on the film. Members of the trust weighed in on everything from haka chants to tattoo design to the demigod Maui’s hair. (He was drawn as bald at first. He shouldn’t have been, members of the trust said. Disney gave him luxurious hair.)
On their first research trip, to Fiji, the filmmakers met with Jiujiua “Angel” Bera, a skilled wayfinder. “He spoke about the ocean in such a personal way,” Osnat Shurer, the Moana producer told me. “He would stroke it really gently, and told us you had to speak gently to the ocean. ‘The ocean knows,’ he said. He goes out to greet the ocean in the morning like he greets his family. This left a very deep impression on us.”
Shurer and her colleagues were also struck by a larger theme of connectedness from the Pacific Islanders they met—and the way many island cultures see the land and sea as indistinct. (In ancient Hawaii, for instance, this idea was encapsulated in the concept of ahupuaʻa, divisions of land that run from the mountain down to the ocean.) And also the extent to which some cultures view the ocean itself as a connective force. “In the Pacific, we don’t consider the water a barrier to each other,” Dionne Fonoti, an anthropologist and a member of the Oceanic Story Trust said in an interview with Disney that’s in the Moana bonus features. “It’s not just the cultures of the people and the islands that connect us, it’s also the ocean that connects us.”
When the film project began, however, the Disney team had no idea how they could portray all this complexity—even just from a technological standpoint. Not to mention the separate but related challenge of animating an anthropomorphic volcanic island.
“I’m pretty pumped up about what we did,” West told me. “There’s nothing, when I look at the final film, that I cringe at. And there usually is.”
The project also changed the way that animators and effects specialists think about the actual ocean. Ramos, the effects lead, told me she spent more than a year and a half working on getting the shoreline animation just right. Even now, she says, she can’t go to the beach without noticing things she’d never considered before her work on Moana.
“You know it’s hard for me to go to the beach nowadays,” she said. “When I’m there, I’m looking at how foam dissipates, at how the water recedes back into the ocean, the cadence and the rhythm of the little breaks. I’m looking at how the beach itself is modeled to create the reef breaks, how the light affects the water, the clarity of the water itself, the colors. There’s just a million things going through my head.”
“I don’t think it’s a bad thing,” she added. “I think it’s gorgeous.”


Last month, the technology developer Gnosis sold $12.5 million worth of “GNO,” its in-house digital currency, in 12 minutes. The April 24 sale, intended to fund development of an advanced prediction market, got admiring coverage from Forbes and The Wall Street Journal. On the same day, in an exurb of Mumbai, a company called OneCoin was in the midst of a sales pitch for its own digital currency when financial enforcement officers raided the meeting, jailing 18 OneCoin representatives and ultimately seizing more than $2 million in investor funds. Multiple national authorities have now described OneCoin, which pitched itself as the next Bitcoin, as a Ponzi scheme; by the time of the Mumbai bust, it had already moved at least $350 million in allegedly scammed funds through a payment processor in Germany.
These two projects—one trumpeted as an innovative success, the other targeted as a criminal conspiracy—claimed to be doing essentially the same thing. In the last two months alone, more than two dozen companies building on the “blockchain” technology pioneered by Bitcoin have launched what are known as Initial Coin Offerings to raise operating capital. The hype around blockchain technology is turning ICOs into the next digital gold rush: According to the research firm Smith and Crown, ICOs raised $27.6 million in the first two weeks of May alone.
Unlike IPOs, however, ICOs are catnip for scammers. They are not formally regulated by any financial authority, and exist in an ecosystem with few checks and balances. OneCoin loudly trumpeted its use of blockchain technology, but holes in that claim were visible long before international law enforcement took notice. Whereas Gnosis had experienced engineers, endorsements from known experts, and an operational version of their software, OneCoin was led and promoted by known fraudsters waving fake credentials. According to a respected blockchain engineer who was offered a position as OneCoin’s Chief Technology Officer, OneCoin’s “blockchain” consisted of little more than a glorified Excel spreadsheet and a fugazi portal that displayed demonstrably fake transactions.
And yet, OneCoin attracted hundreds of millions of dollars more than Gnosis. The company seems to have targeted a global category of aspirational investors who noticed the breathless coverage and booming valuations of cryptocurrencies and blockchain companies, but weren’t savvy enough to understand the difference between the real thing and a sham. Left unchecked, this growing crypto-mania could be hugely destructive to one of the most promising technologies of the 21st century.
* * *
This danger exists in large part because grasping even the basics of blockchain technology remains daunting for non-specialists. In a nutshell, blockchains link together a global swarm of servers that hosts thousands of copies of the system’s transaction records. Server operators constantly monitor one another’s records, meaning that to steal money or otherwise alter the ledger, a hacker would have to compromise many machines across a vast network in one fell swoop. Even as the global banking system faces relentless cyberattacks, the more than $30 billion in value on Bitcoin’s blockchain has proven essentially immune to hacking.
That level of security has potential uses far beyond digital money. Introduced in July of 2015, a platform called Ethereum pioneered the idea of more complex and interactive applications backed by blockchain tech. Because these systems can’t be altered without the agreement of everyone involved, and maintain incorruptible records of every change, blockchains could eventually streamline sensitive, high-value networks ranging from health records to interbank transfers to remote file storage. Some have called the blockchain “Cloud Computing 3.0.”
Using most of these blockchain applications will require owning the digital currencies linked to them—the same digital currencies being sold in all these ICOs. So, for example, to upload your vacation photos to the blockchain cloud-storage service Storj will cost a few Storj tokens. In the long term, demand for services will set the price of each blockchain project’s token.
While a traditional stock is a legal claim backed up by regulators and governments, then, the tokens sold in an ICO are deeply embedded in the blockchain software their sale helps create. Knowledgeable tech investors are excited by this because, along with the open-source nature of much of the software, it means that ICO-funded projects can, like Bitcoin itself, outlast any single founder or legal entity. In a 2016 blog post, Joel Monegro, of the venture capital fund Union Square Ventures, compared owning a blockchain-based asset to owning a piece of digital infrastructure as fundamental as the internet’s TCP/IP protocol.
Almost all groups launching ICOs reiterate some version of this idea to potential buyers, in part as a kind of incantation to ward off financial regulators. The thinking is that, if they are selling part of a platform, rather than stakes in any company, they’re not subject to oversight by bodies like the U.S. Securities and Exchange Commission. But in practice, ICOs are constantly traded across a variety of online marketplaces as buyers breathlessly track their fluctuating prices. In this light, they look an awful lot like speculative investments.
Buyer expectations may matter more to regulators than technical hair-splitting. Todd Kornfeld, a securities specialist at the law firm Pepper Hamilton, finds precedent in the landmark 1946 case SEC v. W.J. Howey Co. Howey, a Florida orange-growing operation, was selling grove plots and accompanying “service contracts” that paid faraway landowners based on the orange harvest’s success. When the SEC closed in, Howey argued they were selling real estate and services, not a security. But the Supreme Court ultimately disagreed, establishing what’s known as the Howey test: In essence, if you give someone else money in the hope that their activities will generate a profit on your behalf, you’ve just bought a security, no matter what the seller calls it.
Knowledgeable observers tend to agree that some form of regulation is inevitable, and that the term ICO itself—so intentionally close to IPO—is a reckless red flag waved in the SEC’s face. The SEC declined to comment on any prospective moves to regulate ICOs, but the Ontario Securities Commission has issued an advisory that “assets that are tracked and traded as part of a distributed ledger may be securities, even if they do not represent shares of a company or ownership of an entity.”
According to Kornfeld, even those who believe they are conducting ICOs in complete good faith could face serious repercussions when regulators do act, especially if prosecutors think they’ve made misleading statements. “If [prosecutors] think that you’re really bad,” he says. “They can say, hey, you deserve 20 years in jail.”
* * *
While it’s easy to see the lie in OneCoin’s fictional blockchain, entirely sincere claims about such a nascent sector still can strain the limits of mere optimism. Many experts, for instance, believe that Gnosis’s use of the blockchain to aggregate data could become a widespread backbone technology for managing complex systems from traffic to financial markets. But the $12.5 million worth of GNO sold in the Gnosis ICO represented only 5 percent of the tokens created for the project, implying a total market value of nearly $300 million. Most tech startups at similar stages are valued at under $5 million.
That astronomical early valuation alone could become bait for an aggressive regulator. Many founders of legitimate blockchain projects have chosen to remain anonymous because of this fear, in turn creating more opportunities for scams.
Much of the money flowing into these offerings is smart, both in that it comes from knowledgeable insiders, and in a more literal sense: Buying into ICOs almost always requires using either Bitcoin or Ethereum tokens (OneCoin, tellingly, accepted payment in standard currency). Jeff Garzik, a longtime Bitcoin developer who now helps organize ICOs through his company Bloq, thinks their momentum is largely driven by recently minted Bitcoin millionaires looking to diversify their gains. Many of these investors are able to do their own due diligence—evaluating a project’s team, examining demo versions of their software, or scrutinizing their blockchain after launch.
But as cryptocurrency becomes more mainstream, ICOs will present greater risks to larger numbers of people. There are few barriers to participation aside from knowing how to conduct a Bitcoin transaction, and the space mostly lacks the robust independent analysis performed by underwriters in the IPO market, which can help tamp down overoptimism. The risk isn’t just to individual investors; many argue that the mania of the late-1990s internet bubble ultimately slowed the entire sector down by making investors skittish for years afterwards. Imagine how much worse things might have been if the whole thing had been entirely unregulated.
Careful regulation, then, could protect blockchain projects from a hugely damaging bust. And the model is genuinely utopian enough to deserve nurturing. Cryptographic tokens effectively make all of a platform’s users part-owners. Anyone selling goods for Bitcoin, for example, has had a chance to benefit from its huge price boost over the past year, while Facebook and Google users have not shared in those companies’ growth.
The Gnosis team is taking this very long view. Their token sale was halted after that furious 12 minutes by an Ethereum-based bot that knew exactly what the fundraising goal was. It even returned more than $1 million to eager buyers who missed the cutoff. Gnosis’s co-founder Martin Koppelman says the company wants to use its remaining tokens not to enrich its creators, but to attract developers and users. That’s similar to the way that Uber has used cash subsidies to recruit riders and drivers, except that once those new recruits hold Gnosis tokens, they will have a serious stake in the platform’s future.


All over town, the parking meters are disappearing. Drivers now pay at a central machine, or with an app. It’s so convenient I sometimes forget to pay entirely—and then suffer the much higher price of a parking ticket. The last time that happened, I wondered: Why can’t my car pay for its own parking automatically?
Listen to the audio version of this article:Feature stories, read aloud: download the Audm app for your iPhone.
It’s technically possible. Both my car and my smartphone know my location via GPS. My phone already couples to my car via Bluetooth. An app could prompt me to pay for parking upon arrival.
Or imagine this: My car, which is already mostly a computer, enters an agreement to lease time from a parking lot, which is managed by another computer. It “signs” this contract just by entering the lot and occupying a parking space. In exchange, the car transfers a small amount of Bitcoin, the currency of choice for computers, into the parking lot’s wallet.
With computers handling the entire process, I’d never even be able to forget to pay for parking. The only way to fail would be for my car to run out of Bitcoin, in which case the parking lot has easy recourse: Because my car’s ignition is managed by a computer, the parking lot could just shut my vehicle down.
Scenarios like this are possible when blockchain—the digital transaction record originally invented to validate Bitcoin transactions—gets used for purposes beyond payment. In certain circles, the technology has been hailed for its potential to usher in a new era of services that are less reliant on intermediaries like businesses and nation-states. But its boosters often overlook that the opposite is equally possible: Blockchain could further consolidate the centralized power of corporations and governments instead.
* * *
In his book Radical Technologies, the urban designer Adam Greenfield calls cryptocurrency and blockchain the first technology that’s “just fundamentally difficult for otherwise intelligent and highly capable people to understand.” I was relieved when I read this, because I have been pretending to understand cryptocurrencies—digital money based in code-breaking—for years. Bitcoin is hard to grasp because it’s almost like a technology from an alien civilization. It’s not just another platform or app. Making sense of it first requires deciphering the political assumptions that inspire it.
Bitcoin is an expression of extreme technological libertarianism. This school of thought goes by many names: anarcho-capitalism (or ancap for short), libertarian anarchy, market anarchism. Central to the philosophy is a distrust of states in favor of individuals. Its adherents believe society best facilitates individual will in a free-market economy driven by individual property owners—not governments or corporations—engaging in free trade of that private property.
Anarcho-capitalism is far more extreme than Silicon Valley’s usual brand of technological individualism. For one, the tech sector’s libertarianism is corporatist in its bent, and amenable to government, if in a strongly reduced capacity. And Silicon Valley takes a broader approach to the liberating capacity of technology: Facebook hopes to connect people, Google to make information more accessible, Uber to improve transit, and so on.
The ancap worldview only supports sovereign individuals engaging in free-market exchange. Neither states nor corporations are acceptable intermediaries. That leaves a sparsely set table. At it: individuals, the property they own, the contracts into which they enter to exchange that property, and a market to facilitate that exchange. All that’s missing is a means to process exchanges in that market.
Ordinarily, money would be sufficient. But currency troubles market anarchists. The central banks that control the money supply are entities of the state. Financial payment networks like Visa are corporations, which aren’t much better. That’s where Bitcoin and other cryptocurrencies enter the picture. They attempt to provide a technological alternative to currency and banking that would avoid tainting the pure individualism of the ancap ideal.
This makes Bitcoin’s design different from other technology-facilitated payment systems, like PayPal or Apple Pay. Those services just provide a more convenient computer interface to bank accounts and payment cards. For anarcho-capitalism to work in earnest, it would need to divorce transactions entirely from the traditional monetary system and the organizations that run it. Central banks and corporations could interfere with transactions. And yet, if individuals alone maintained currency records, money could be used fraudulently, or fabricated from thin air.
To solve these problems, Bitcoin is backed by mathematics instead of state governments. The Bitcoin “blockchain” is a shared, digital record of all the transactions (or “blocks”) that have ever been exchanged. Every transaction contains a cryptographic record of the previous succession (the “chain”) of exchanges. Each one can thus be mathematically verified to be valid. The community of Bitcoin users does the work of verification. To incentivize the onerous work of cryptographically verifying each transaction in the chain that precedes it, the protocol awards a bounty—in Bitcoin of course—to the first user to validate a new transaction on the network. This is the process known as “mining”—a confusing and aspirational name for what amounts to computational accounting.
There’s a lot more detail that I am omitting. But the key to Bitcoin is that the network distributes copies of one common record of all Bitcoin transactions, against which individuals verify new exchanges. This record is the blockchain, which is sometimes also called the “distributed ledger”—a much more elucidating name. This is the missing element that’s supposed to allow the hypothetical anarcho-capitalist techno-utopia to flourish.
* * *
At least, that’s the theory. In practice, Bitcoin and other cryptocurrencies don’t really meet the ancap ideal. Perhaps it’s an impossible goal; imagining the end of both nation-states and corporations is even harder than imagining the end of capitalism itself. Greenfield speculates in his book that Bitcoin was never meant to be a store of value, like state-backed currency, but only a medium for exchange “between parties who would presumably continue to hold the bulk of their assets in some other currency.”
Anarcho-capitalism might seem fringe and unfamiliar to most people, but at least it helps explain the rationale behind cryptocurrency and blockchain. Unfortunately, those topics become even more confusing when Bitcoin and its kin get used in ways incompatible with their original inspiration—which turns out to be most of the time.
As a medium for exchange, Bitcoin is relatively limited. Some retailers, many tech-oriented, accept the currency for purchases, but it remains best known as a means to buy black-market goods on darknet exchanges like Silk Road. (The fact that such uses were illicit in the first place, the anarcho-capitalist would point out, is precisely the reason individual freedom-fighters should demand a decentralized market unbeholden to governments.)
But Bitcoin’s success has accidentally undermined its viability. Each Bitcoin transaction adds more encrypted data to the blockchain, requiring increasingly more computer power to verify (and to earn the associated commission). More computing power means more energy cost to run and cool the machines, which requires more capital and physical infrastructure to support. Those rising costs inspire centralization. Adam Greenfield tells me that two Chinese giants can control over half of the global Bitcoin mining operations. If they collaborate, a majority-control of the blockchain could allow them to manipulate it. That’s precisely the risk a decentralized currency was meant to avoid.
More often, Bitcoin has been used as a financial instrument instead of a currency. From tulips to tech start-ups, market capitalism is flexible enough to turn anything into a tradable security or futures commodity. Bitcoin hype has made it appealing for speculators certain to transfer their gains back into more stable state currencies, although its volatility makes it a difficult case either as a store of value or a medium of exchange.
The same hype driving cryptocurrency speculation has also attracted banks, governments, and corporations—exactly the authorities it was designed to circumvent. Financial services firms have taken an interest in cryptocurrency. Federal Reserve chair Janet Yellen has called for the Fed to leverage blockchain. Canada has been experimenting with a blockchain-backed version of its national currency, called CAD-Coin. Future cryptocurrencies operated by banks or governments might enjoy more productive use than Bitcoin.
But those futures also undermine cryptocurrency’s ancap aspirations. Corporations and governments re-centralize control, for one. But also, they undermine the discretion and anonymity that accompanies free trade in the ancap fantasy. When the local or central bank manages the cryptocurrency platform, it also gets a record of every transaction that takes place in that economy. One doesn’t need to be an anarchist to surmise potential downsides of that situation. Picture China mandating state cryptocurrency, tying the country’s proposed social credit system to that ledger. Or imagine if the North Carolina State legislature decided to issue all food stamp vouchers in crypto form to better manage their future use.
* * *
Even if Bitcoin’s utility and value might decline, the distributed ledger offers potential uses beyond simple currency exchange. In theory, any internet-connected device could participate in verified, distributed transactions.
Greenfield offers a simple example: the German startup Slock.it, which “gives connected objects an identity, the ability to receive payments, enter into complex agreements and transact without intermediary.” The simplest Slock.it device is a physical padlock that is connected to the internet. Networked locks are nothing new, thanks to the internet of things. But a blockchain-backed connected lock offers some additional capabilities. A distributed-ledger lock could enter into a “smart contract,” an agreement whose terms are implemented directly in code. If attached to an AirBnB rental, such a lock could be programmed to automatically release when a smartphone belonging to a pre-paid renter approaches. Likewise, it could be programmed to cease to unlatch after that tenant’s contract had terminated—or perhaps it could cut off the power or internet service if a sensor inside the property determined that its occupants were cavorting too loudly, or rifling through unauthorized cabinets.
Kik, a startup that makes a messaging app popular among teens, offers a more recent example of distributed-ledger tech in action. The company recently announced plans to introduce its own cryptocurrency, called Kin. Kik will automatically dole out Kin as rewards for developers who build apps on its platform, like stickers or chat bots. Kik’s CEO, Ted Livingston, presented the move as nothing short of emancipation from the oppression of ad-driven content platforms like Facebook and YouTube: “a cryptocurrency for an open future.”
Kin is built atop a platform called Ethereum, which is based on the same distributed ledger as Bitcoin. But Ethereum uses that technology to express a different aspect of the ancap model: contracts. For libertarians, contracts exist to facilitate market exchange, so smart contracts are always backed by currency (Ether, in Ethereum’s case). If Bitcoin is digital money for people, Ether is digital money for computers. It decides how to spend itself via software automation.
Why tout a private, distributed-ledger currency as an agent of liberation when it amounts to a complicated, software-backed, company-town store? One answer: It could give the workers a stake in the company store. In the world of cryptocurrency, this is known as an ICO or Initial Coin Offering. ICOs incentivize the use of an unproven platform, like Kik’s, by distributing an initial batch of cryptocurrency to early adopters. In theory, that value will increase if the platform becomes popular, creating a valuable base investment for its initial users.
In the extremist libertarian aspiration, smart contracts would allow anonymous actors to trade anything whatsoever in an untraceable way, via unregulatable markets. Instead, actual smart contracts, ICOs, and distributed ledger-backed devices mostly offer new ways to interface with the private technology industry. For example, in Brooklyn, a solar microgrid startup called Transactive sells clean energy to a community via Ethereum. And Toyota just announced a partnership with MIT to develop distributed ledger-based infrastructure for future autonomous vehicle services.
On that front, the anarcho-liberatarians share something in common with the plain-vanilla technolibertarians: a belief in the wisdom and righteousness of a fully computational universe. My hypothetical smart-contract parking meter, Toyota’s future blockchain-backed rideshare system, Slock.it’s blockchain lock, Kik’s Kin, Transactive’s solar grid—all are just technology companies enjoying the capitalization and publicity spoils of the latest hot trend. They might become more than that, of course. But in order to do so, something terrifying has to happen first.
* * *
Consider an off-the-cuff example of smart contracts from an Ethereum advocate:
An individual wants to purchase a home from another person. Traditionally there are multiple third parties involved in the exchange including lawyers and escrow agents which makes the process unnecessarily slow and expensive. With Ethereum, a piece of code could automatically transfer the home ownership to the buyer and the funds to the seller after a deal is agreed upon without needing a third party to execute on their behalf.
It sounds so easy. Who needs real-estate agents, closing attorneys, assessors, mortgage brokers, title insurers, municipal tax authorities, and all the rest? Just transfer some Ether after the computers shake hands.
But absent a global ancap revolution, those intermediaries are unlikely to disappear. Consider what would be required for distributed-ledger scenarios like this one become reality. Smart contracts require computational intermediation everywhere. Non-computational devices like parking lots and door locks and property deeds must become connected to computers. People would have to become willing to use machines that enter into decentralized contracts with other machines absent intermediary protection of government, law, banking, and other legacy infrastructures.
The problems with those old institutions are many. In a widely shared tale of voter suppression in the 2016 election, Eddie Lee Holloway Jr., a 58-year-old Wisconsin man, couldn’t vote because the state’s new voter-ID law demanded that he show proper identification. But an error on his birth certificate prevented him from getting a new ID. In a future run by the distributed ledger, a single copy of Holloway’s identification would be securely stored on the blockchain, easily verifiable when needed. For the tech evangelist, it offers a rational solution that would solve social ills by means of impartial technology. (On that note, blockchain-based digital IDs have also been proposed for refugees.)
It sure sounds good. But the scenario only works if the entire system of contemporary life becomes sufficiently interconnected to make it possible. All the departments of public health and the DMVs and the voter registration venues—not to mention the parking spaces and the automobiles and the power grids and all the rest—would have to cohere around a common understanding, so that the machines could execute smart contracts on their behalf. This would require a complete reinvention of public and private life.
A different reinvention is more likely. Instead of defanging governments and big corporations, the distributed ledger offers those domains enormous incentive to consolidate their power and influence. For people like Eddie Lee Holloway, Jr, who’s African American, that might mean even greater exclusion, as the very institutions that locked him out of the voting booth might suppress his transformation into a digital-ledger citizen in the first place.
Or if not, other traumas might yet face citizens like Holloway in a society run by blockchain. A mandated DNA-test could accompany citizens’ blockchainification, allowing their ethnic origins and medical predispositions to become attached to an identity record. Financial assets would also be connected, thanks to an underlying cryptocurrency account through which they make debits and credits. Not to mention all the personal insights already consolidated by services like Facebook.
Businesses might subscribe to this data. Thanks to distributed ledger, it could be used to prevent their automated doors from opening for people whom a smart-contract risk-assessment service rates below a threshold of desirability. Left outside, privately-contracted security robots might deploy ledger-backed ID scanners to sweep loiterers from private property. Once delivered and booked into jails, smart courts could automate sentences based on an automated assessment of future crime potential.
And that’s just America. Imagine how a mature authoritarian state would fare under the rule of blockchain. Is this starting to feel like a Black Mirror episode yet? For Adam Greenfield, the anti-authoritarian left has profoundly misunderstood the corner into which such an ambitious aspiration paints society. “I believe distributed ledger enables the kind of central control they’ve never in their worst nightmares contemplated,” he tells me. The irony would be tragic if it weren’t also so frightening. The invitation to transform distributed-ledger systems into the ultimate tool of corporate and authoritarian control might be too great a temptation for human nature to forgo.
* * *
If this sounds familiar, it’s because contemporary culture has been here before. The existing, comparatively modest surveillance and control technologies in use by Google, Facebook, and their ilk—whose impact on governance we now know all too well—proliferated on the assumption that technology could make life better and more efficient. Nobody chose this life, exactly. People adopted technology in sufficient numbers to allow industry, and the culture that follows it, to conclude that the market had decided what was best.
Likewise, Bitcoin’s triumph hinges mostly on the financial success of speculators who never had any intention of using it as currency, and who appear to have strip-mined it into oblivion in the process. Similarly, blockchain’s future seems tied to the short-term vision of investors and entrepreneurs willing to speculate on a hypothetical, distributed utopia without hedging against the consolidated autocracy it seems equally likely to realize. “This is what happens,” Greenfield says, “when very bright people outsmart themselves.”


When sprinters take their marks, they place their hands on the ground and position their feet onto angled blocks. Before the start signal, the runners rest their knees on the ground, then transfer their weight squarely on the blocks. As soon as the sound waves of the signal reach their ears, their feet catapult off the blocks. If the sprinter pushes off too soon, it means disqualification from the race. But sprints last 400 meters or less, so every millisecond spent on the blocks after the race has begun is a millisecond wasted.
The start of competitive foot races like those of the Olympics have only recently evolved into the test of nerves and alertness they currently are. The stadion race of 600 podes (locally determined “feet”) began the tradition of sprints at Olympia. It spread to the other Panhellenic games throughout the Mediterranean. As the Greeks designed and redesigned the games, they invented ways to make racers take the field in a uniform manner. First, they gave the runners a place to put their feet at the beginning of the race. Later, they designed a gate called a hysplex that released runners at the same time. When the modern Olympic sprints commenced in 1896, runners improvised their start, choosing to crouch or start in the ancient style as they wished. Then came the starting blocks.
The starting blocks helped runners take the crouching position developed in the late 19th century by American and Australian sprinters. They required all runners to direct their energy horizontally—instead of vertically—at the race’s start. But today, they do more than just help runners get off the line. They also detect false starts, putting the race behind the feet of the sprinter as much as in front of them. As a technology of fairness, the starting blocks helped turn foot racing into an ideal for egalitarian citizenship.
* * *
Ancient Greek stadiums show continual upgrades to the starting line of the race. The Greeks ran from a stone starting gate along a central rectangular track. Ropes stretched in front of the runners fell in unison onto the track, releasing them to begin the race. The precursor to the starting block, called a balbis, was redesigned at several stadiums to accommodate a starting gate, even when it meant reducing the number of runners or the width of the track to provide room for the starting gate. The perpetual reworking of the stone balbis is one indication of how seriously the Greeks took these athletic events.
Most contemporary knowledge of the hysplex comes from four tracks at Epidauros, Isthmia, Nemea, and Corinth. At both ends of the track, engineers placed stone support bases for a starting gate. These bases were filled in with wood, which was removed, oiled, and stored when games were not being held. Since the wood and ropes of the starting gate have not been preserved in the archaeological record, historians make guesses based on textual and artistic representations.
The art on one jar from Athens shows the start of the hoplite race. Two horizontal cords act as barriers. One grazes the knees, the other the waist. The cords stretch from one end of the stone starting block to the other. The cords connect to two posts, which were held up under tension by rope. Officials kept the posts upright while the athletes positioned themselves. At the start of the race, they released both posts simultaneously. The weight of the gate and the tension of the ropes brought both crashing to the floor, providing a visual and auditory signal that the race was afoot.   
Starting blocks built into the field make foot races a permanent part of the city. That gave blocks symbolic meaning as much as competitive utility. In Aristophanes’s Lysistrata, first performed in 411 BC, the starting gate serves as a metaphor for releasing women onto the political field, where they run an antiwar campaign. War might have helped inspire technological improvements to the device, too. The ropes and wooden base of the starting gate correspond with the development of the catapult, first mentioned in Athens in 355 BC. Some stadia constructed in this period seem to have been caught up with the interest in ropes and pulleys demonstrated by engineers of Philip II of Macedon.
Evidence for starting gates have been found at several sites of major Panhellenic games, including Isthmia, Olympia, and Nemea. Some suggest that the Greeks also engineered against false starts in other ways. Toe holds were carved into the blocks, forcing runners to adopt a wide stance. It provided balance, which reduced the likelihood of false starts. Runners had a fixed starting location, so they were more likely to stay in their lane, which was marked by cords or colored chalk. As is the case today, sprinters were not supposed to block or run into another athlete in order to win.
* * *
Starting blocks appeared in modern games thanks to an innovation that was made simultaneously on opposite sides of the world. Whereas Ancient Greek runners are depicted with one arm forward, using their undulating hands to build momentum, modern sprinters rely more on their legs. In 1887, the Aboriginal sprinter Bobby McDonald began from a crouching position in Sydney. Across the Atlantic, the Yale athlete C.H. Sherrill was photographed in the same pose in 1888. The crouching start became ubiquitous among college runners by 1890. Today, both the Government of Australia and Yale claim the first crouching start without mentioning the other.
The crouching position that modern starting blocks impose facilitates a horizontal surge of energy. This burst propels runners to higher speeds more quickly. Formerly, track stars dug holes for their toes in the dirt of the track to plant their feet for the start. George Bresnahan, of the University of Iowa, filed a patent for a foot support in 1927, changing the definition of a starting block from the ancient, permanent stone base to the modern, portable metal device with offset foot rests—the design competitors and spectators know today.
The blocks were not immediately used, as races would have been different enough to upset records. However, the benefits of starting blocks soon became apparent. In one case, film footage showed that Ralph Metcalfe dug his holes significantly back from his true start in the 200 meter event at the 1932 Olympics, but he refused a rerun of his third place finish in deference to an impressive and potentially unrepeatable American sweep of the podium. It took until the 1948 London games for starting blocks to appear at the Olympics. Early models were both heavy and susceptible to slipping from the force of the athlete’s takeoff.
In contrast to the Greek starting gate, contemporary athletes must take off from the starting blocks without the visual aid—and physical barrier—of a starting gate. This increases the possibility of a false start. In modern games, false starts are determined by an athlete’s reaction to the starting sound. Because sound waves take time to reach human ears, there is a lag between the starting sound and the athlete’s movement. Athletes take at least 0.110 of a second to respond. Given the small margins of victory in short sprints, even this delay can make a difference. In the 100 meter final at the 2016 Olympics in Rio, Usain Bolt, the world-record holder, reacted in 0.155 seconds. Since Bolt is a competitor whose tall frame lends to slower starts, the first fractions of a second are one place in the sprint where Bolt’s rivals can seek to gain an advantage. To ensure the fairest start possible, race officials began placing speakers behind each athlete to make the start equally audible for all competitors.
The starting blocks can also disqualify athletes who begin the race before the starting sound reaches their ears. Sensors within the blocks track the pressure the athlete’s body applies to the block. The latest Omega blocks take measurements 4,000 times per second, enough to tell judges if athletes moved their bodies while still on the starting blocks, or if they have reacted so fast that they must have begun the start before the signal. The technology has also inspired tactical use among sprinters. A 2003 rule change led some to believe that runners were deliberately committing a penalty-free first false start to encourage the unfortunate runner who ran the second false start to be disqualified. In 2010, international rules instituted a zero-tolerance policy for false starts.
* * *
For the ancient Greeks, starting blocks and gates mattered enough to set in stone and wood.  This was not simply because races formed a circuit that took athletes back to the same cities and stadia. It was also because the moment that everyone took their place on the starting line was an important one. At the start of the race, all competitors were equal. They took the field in unison. Athletes ran on a perfectly flat surface, one that bore no resemblance to the grassy fields and rocky hills of their homes or battlefields. The falling ropes of the starting gate showed that these sprints were signs of civilization.
In the modern Olympics, athletes did not originally have starting blocks or a starting gate. With the adoption of the crouching start, starting blocks have replicated the ancient concern for a coordinated movement in the modern games. The standardization of blocks has meant that athletes no longer face the race variables of the condition of the track and their adroitness at digging holes. In their ideal form, starting blocks and gates are a technology of democracy. Every man or woman can run a race, but the blocks are designed to turn that race into a contest of equals.
This article appears courtesy of Object Lessons.


I work at an ethics center in Silicon Valley.
I know, I know, “ethics” is not the first word that comes to mind when most people think of Silicon Valley or the tech industry. It’s probably not even in the top 10. But given the outsized role that tech companies now play, it’s time to focus on the ethical responsibilities of the technologists who help shape our lives.
In a recent talk, technologist Maciej Ceglowski argued that “[t]his year especially there’s an uncomfortable feeling in the tech industry that we did something wrong, that in following our credo of ‘move fast and break things,’ some of what we knocked down were the load-bearing walls of our democracy.”
This was not unforeseeable—or even unforeseen. In 2014, for example, in an article titled “How Facebook Is Shaping Who Will Win the Next Election,” Tarun Wadhwa cited a study published in 2012: “A 61-Milion-Person Experiment in Social Influence and Political Mobilization.” The study’s authors reported on  “a randomized controlled trial of political mobilization messages delivered to 61 million Facebook users during the 2010 US congressional elections. The results show that the messages directly influenced political self-expression, information seeking and real-world voting behaviour of millions of people.”
So we already knew that tools for “maximizing engagement” can shape the political sphere.  In 2014, Wadhwa concluded, “Whether it wants this responsibility or not, Facebook has now become an integral part of the democratic process globally.”
We also know that technology can be harmful to our democracy. Privacy invasions and algorithmic manipulation, for example, can limit the ability to research and formulate opinions, and then in turn affect how people express views—even via voting. When companies implement practices that are good for targeted advertising but bad for individuals’ democratic engagement (like, for example, the practices involved in the use of “dark posts” on Facebook, tied to the creation of psychological profiles for hundreds of millions of Facebook users in the U.S.), the benefits-versus-harms balance tilts pretty sharply.
Who minds that balance?
You often hear the adage that law can’t keep up with technology. What about ethics? Ethics, too, is deliberative, and new norms take some time to develop; but an initial ethical analysis of a new development or practice can happen fairly quickly. Many technologists, however, are not encouraged to conduct that analysis, even superficially. They are not even taught to spot an ethical issue—and some (though certainly not all) seem surprised when backlash ensues against some of their creations. (See, for example, the critical coverage of the now-defunct Google Buzz, or more recent reaction to, say, “Hello Barbie.”)
A growing chorus has argued that we need a code of ethics for technologists. That’s a start, but we need more than that. If technology can mold us, and technologists are the ones who shape that technology, we should demand some level of ethics training for technologists. And that training should not be limited to the university context; an ethics training component should also be included in the curriculum of any developer “bootcamp,” and maybe in the onboarding process when tech companies hire new employees.  
Such training would not inoculate technologists against making unethical decisions—nothing can do that, and in some situations we may well reach no consensus on what the ethical action is. Such training, however, would prepare them to make more thoughtful decisions when confronted, say, with ethical dilemmas that involve conflicts between competing goods. It would help them make choices that better reflect their own values.
Sometimes, we need consumers and regulators to push back against Big Tech. But in his talk titled “Build a Better Monster: Morality, Machine Learning, and Mass Surveillance,” Maciej Ceglowski argues that “[t]he one effective lever we have against tech companies is employee pressure. Software engineers are difficult to hire, expensive to train, and take a long time to replace.” If he is right, then tech employees might have even more power than people realized—or at least an additional kind of power they can wield. All the more reason why we should demand that technologists get at least some ethics training and recognize their role in defending democracy.
I work in an applied ethics center, and we do believe that technology can help democracy (we offer a free ethical-decision-making app, for example; we even offer a MOOC—a free online course—on ethical campaigning!). For it to do that, though, we need people who are ready to tackle the ethical questions—both within and outside of tech companies.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


It’s time to fix the voting process.
American voting systems have improved in recent years, but they collectively remain a giant mess. Voting is controlled by states, and typically administered by counties and local governments. Voting laws differ depending on where you are. Voting machines vary, too; there’s no standard system for the nation.
Accountability is a crapshoot. In some jurisdictions, voters use machines that create electronic tallies with no “paper trail”—that is, no tangible evidence whatsoever that the voter’s choices were honored. A “recount” in such places means asking the machine whether it was right the first time.
We need to fix all of this. But state and local governments are perpetually cash-starved, and politicians refuse to spend the money that would be required to do it.
Among many other needed measures promoted by nonprofit and nonpartisan Verified Voting, Congress should require standardized voting systems around the nation. It should insist on rock-solid security, augmented by frequent audits of hardware and software. Recounts should be performed routinely and randomly to ensure that verified-voting systems work as designed. The paper ballot generated by the machine should be the official ballot.
What Congress should emphatically not do is allow or encourage online voting. The sorry state of cybersecurity in general makes clear how foolhardy it would be to go anywhere near widespread “Internet voting” in the foreseeable future.
There’s one benefit to note in our massively decentralized voting systems: It would be harder to steal a national election. But flipping just a few precincts in some key districts and states could have outsized impact. There’s every incentive for malicious actors to try, though I don’t know if Russia or anyone else had a direct-hacking impact in 2016. But why do we keep taking these kinds of risks?
For reasons that remain unclear, Congress has been largely uninterested in doing what’s needed to make voting safe, secure, and verifiable (perhaps because the existing system is how members got elected). In President Obama’s final weeks in office, the Department of Homeland Security added voting machines to its “critical infrastructure” list, but there’s no sign that Congress will back that statement with money to get things done, and under Trump all bets are off.
Barring a national commitment to getting this right, maybe the answer is to change direction entirely. Maybe we should abandon electronic voting systems and do everything on paper, and count by hand. We’d wait longer for results, a lot longer. If it ensured accurate results, though, I’d call that a reasonable trade.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


When I told my mother my work focused on improving public trust in the news, she thought the idea was hilarious. “Trust? News?” I was a bit insulted. Her daughter—me, that is—has been a journalist for years. But she had a point.
Journalism has been struggling to stay afloat in an era when people expect information to come both fast and free. Now, competition by principle-free enterprises further undermines its very role and purpose as an engine for democracy.
The digital world has muddied formerly clear divisions between factual news, sales pitches, hoaxes, and hyper-partisan propaganda designed to incite. On social media and in online search, misinformation spreads with lightning speed. Lurid headlines loom far larger than the logos of news brands, which at least offer a clue to the origin of information and the care behind it.
Last fall, barely one third of Americans polled by Gallup expressed at least a fair amount of trust in the mass media “to report the news fully, accurately and fairly.”
To make collective decisions about our communities, our shared resources, and our government, we have to agree on basic facts. Today those facts can take on the shape of a funhouse mirror where we each see a distorted image tailored to our own expectations.
The Trust Project, which is a collaboration of news organizations around the world, aims to sharpen the picture by using technology to encourage accurate, ethically produced news and make it easy to find. Think along the lines of a nutrition label on a package of food, or a lab report that conveys your health status when you go in for a checkup.
Based at the Markkula Center for Applied Ethics in Santa Clara, California, the project has brought together more than 70 news organizations to work on a transparency system that would show how a story came to be and who stands behind it. Technology companies, which have become powerful distributors of news, are also contributing their expertise and a strong willingness to apply the results.
Imagine you encountered a piece of text or video in your social media newsfeed or while searching for news on your phone or computer. It would be marked clearly as news, opinion, or sponsored content designed to sell you something. If you clicked on the byline, you’d see the author’s background and other published work. Did that person have local expertise? Experience and knowledge covering the topic? Another click would take you to information about the news site itself. What commitments has it made to ethics, diversity, correcting mistakes? Who owns or funds the site, who is the leadership, when was it created?
For investigative or more controversial stories, you could see citations backing up the information presented and a short description of the reporting involved. If you liked, you could learn details about the site’s track record for reaching across economic background, race, age, gender and other differences to get the full story.
You’d quickly find what you needed to make informed choices about your news. The Trust Indicators would also send machine-readable signals to Facebook, Google, Bing, Twitter and other technology platforms. We’re already working with these four companies, all of which have said they want to use our indicators to prioritize honest, well-reported news over fakery and falsehood.
While some news organizations show bits and pieces of these features, until now they have not been standardized and structured across sites for the public to quickly find or platform machines to read.
The journalists working on this project aren’t attempting to prescribe the perfect news diet for the public. That would be self-serving, pompous, and dull. No, we’re asking people to tell us what they want and need from the news. We have conducted dozens of one-on-one interviews with consumers across the United States and Europe. Our questions: What do you value in news? When do you trust it? When have you had your trust broken?
Pundits complain about a naive public that likes cat videos and bias-confirming memes. Our interviews turned up something different: Thoughtful online readers who genuinely want to be informed about their own communities and the world.  Skeptical citizens who work very hard to gather a more complete, nuanced picture of issues and events than they believe news organizations typically provide.
Some members of the public are frustrated with journalism that seems thin, uninformed, biased against their community and replete with argument, anger and violence. They complain about opinion presented exactly like news. Some people are so fed up that they have simply disengaged. They want more humility from journalists, more recognition that in spite of journalists’ best aspirations, we do sometimes get it wrong.
Most of the people we talked to, though, valued the news and the people who produce it. Our interviewees told us that they knew journalists aspire to be objective, but we all have a perspective based on our life experiences. They wanted news organizations to be clear about their funding sources and agenda. They wanted to know more about the reporter and where she got her information.
They wanted to hear from people like themselves and unlike themselves—certainly more than the usual high-powered leaders in business and government. Many talked about more engagement. A news story, we heard, should offer tools like annotation and forms that would allow readers to contest claims, suggest more sources and propose ideas for reporting further. Journalists, people urged, should be more collaborative with the public they aim to serve.
Journalism executives listen eagerly to these insights but don’t always like what they hear. All that transparency may stretch their technology systems and eat up precious time. So far, however, many are still game to try. The Washington Post, Mic, and The Globe and Mail are in the early stages of rolling out our indicators, while many other organizations in the U.S. and Europe are preparing to do so. Technology platforms are mulling the best ways to apply the signals in their own environments. I hope they’ll all recognize the opportunity to become more relevant and responsive. A vibrant, free press that the public can trust is vital to democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


The audio recording of a congressional candidate accused of body-slamming a reporter on the eve of a special election in Montana is extraordinary for several reasons.
If you haven’t listened to it yet, and you really should, it’s essential to at least read the short transcript:
Ben Jacobs, a reporter for The Guardian: ...the CBO score. Because, you know, you were waiting to make your decision about health care until you saw the bill and it just came out...
Greg Gianforte, the congressional candidate: Yeah, we’ll talk to you about that later.
Jacobs: Yeah, but there’s not going to be time. I’m just curious—
Gianforte: Okay, speak with Shane, please.
[loud scuffling noises, an even louder crash, repeated thumping]
Gianforte: [shouting] I’m sick and tired of you guys!
Jacobs: Jesus chri—!
Gianforte: The last guy that came in here, you did the same thing! Get the hell out of here!
Jacobs: Jesus!
Gianforte: Get the hell out of here! The last guy did the same thing! You with The Guardian?
Jacobs: Yes! And you just broke my glasses.
Gianforte: The last guy did the same damn thing.
Jacobs: You just body-slammed me and broke my glasses.
Gianforte: Get the hell out of here.
Jacobs: You’d like me to get the hell out of here, I’d also like to call the police. Can I get you guys’ names?
Unidentified third man: Hey, you gotta leave.
Jacobs: He just body-slammed me.
Unidentified third man: You gotta leave.
This is an astonishing encounter. First, there is the simple fact of it. A man who is perhaps hours away from being elected to the U.S. Congress—a person who is pledging to represent the interests of his fellow Americans—can be heard shouting and allegedly attacking a citizen who calmly asked him what he thought about his party’s health-care plan.
I have experienced many tense confrontations with elected officials in my life as a journalist. They have shouted at me and run away from me—into taxi cabs, state-house elevators, even (hilariously) the little slow-moving trolley cars under the U.S. Capitol. Once, the former mayor of Honolulu—at 6 feet 7 inches tall, towering over me—screamed at me after I asked him a question, about historic preservation of all things, on the steps of City Hall.
Reporters are accustomed to this treatment. What appears to have happened to Jacobs is something else entirely, and it should frighten any citizen who believes in democracy. (And not least of all because the Gianforte campaign’s description of what happened contradicts what anyone can hear on the tape.)
There’s something just as frightening in the fact that the audio—shocking as it is—is so ordinary these days. I don’t just mean the alleged assaults in recent weeks against journalists like Jacobs in Montana; or Dan Heyman, the reporter arrested for shouting a question at the Health and Human Services secretary, Tom Price, as Price toured the West Virginia Capitol two weeks ago; or John M. Donnelly the reporter who says he was pinned to the wall at the Federal Communications Commission after he tried to ask the commissioner Michael P. O’Rielly a question.
The ubiquity of recording and broadcasting technology has given Americans an unprecedented view of how frequently and grievously people in positions of power will intimidate, assault, and even kill ordinary citizens in the United States.
Why should anyone be surprised to hear that a congressional candidate may have grabbed a reporter by the throat, thrown him to the ground, and repeatedly punched him—that’s how a Fox News crew in the room described Gianforte’s actions—when we’ve seen the blood-soaked T-shirt of a man killed by police during a routine traffic stop, and the stoic face of the 4-year-old girl who watched it happen?
We have seen it all before and we will see it again. And it’s still not clear whether watching or listening, which is itself a kind of knowing, prevents future violence. Videos of police violence against black citizens are everywhere. Attacks against American citizens who question public officials in public spaces seem to be on the rise. All while corporate and government surveillance is rampant. All while the president of the United States calls information-gathering citizens his “enemies” and “the opposition” and “the lowest form of life.”
There are still strong institutional responses—from advocates for police-worn body cameras, for example, and from the three Montana newspapers that took the rare step of rescinding their endorsements of Gianforte last night. But such steps can only do so much. The public’s faith in myriad institutions is steeply declining.
Citizens have the power to bear witness, to document, to question—and to elect—our nation’s top legislators. It’s unclear how last night’s altercation, or the recording of it, will affect the Montana election, if at all. Many voters already submitted ballots in early voting, before the alleged assault occurred. Still, the outcome of the race will offer a glimpse of how Americans perceive the status of their own rights in democratic society. The rest of us will be watching.


How I tire of journalists asking whether and how technology is bad for society, forever starting with what could go wrong, hunting for blame.
Is technology hurting democracy? No. Can technology help save democracy? No.
These are the wrong questions. We, the people—and we, the media—are hurting democracy. It is in our hands to save it if we still can. Democracy’s enemies and saviors will use whatever tools and technologies are at hand.
The better question: What is wrong with our democracy? Until we know the problem, we cannot build the solution.
My diagnosis: I think we are allowing ourselves to be ruled—in every sense of the word—by emotion over rationality, fear over facts. That opens the door for the cynical and the ignorant, those greedy for power or money, to exploit our weakness, to play to some dark and empty hole in our souls, arguing that they alone are the solution.
The tool they use to degrade democracy is not Facebook or Twitter. Their weapon is the strange. To play to our fears, they must project the spectre of an enemy, an other who is to blame for our problems, who will take our jobs, addict our youth, bomb us in the mall, run us over on the street. That enemy is the stranger.
So we need to make the stranger less strange. We should meet the people we are told to fear, convening communities in conflict into informed and civil conversation. Facebook could help do that, for I think its ultimate potential as a platform is not just to connect us with the people we know but to introduce us to strangers, providing a safe space where people can tell their own stories and try to understand other worldviews.
News media can play a role here, too. Witness an experiment by the local Alabama news site AL.com and the startup Spaceship Media, which brought together Trump voters from the South and Clinton voters from the West on Facebook to meet, listen to each other, find where they disagreed, and—here’s the good part—call on journalists to help them combat misunderstanding with fact.
The lesson for  us in the press: We must learn to listen to the public conversation before we can hope to inform it. We must hear, understand, empathize with, and reflect communities’ concerns and needs to earn their trust. Then and only then will we have any hope of calling them to the rational dialogue and collective discernment informed by fact that define a functioning democracy.
We in media can no longer expect every citizen to come to us and our content as the destination, as if we alone are the solution. We must go to the people where they are having their conversations to listen and then bring value. We have many new ways to do that, which are—yes—created by technologists: Facebook, Twitter, YouTube, Meetup, Reddit, Google, Snapchat, Instagram are a few. How we use them is up to us.

This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


I’m a reporter first, and a writer second, which means I often find myself writing in odd places. Not just geographically unusual, though there’s that, too. I write everywhere, with whatever technology is at hand.
Most of the time, I’m typing away in a plain text editor on my laptop. But I still write first drafts in reporter’s notebooks, and in the Notes section of my iPhone, and on scraps of paper when necessary.
Now here’s a first for me: I’m writing a story for The Atlantic in MacWrite 4.5, the word processing program first released with the Apple Macintosh in 1984 and discontinued a decade later. So here I am, awash in 1980s computing nostalgia, clacking away in an emulated version of the original software, thanks to the Internet Archive.
The only problem is, how am I going to file this story into The Atlantic’s 2017 web based content management system? (Also, the hyphen key isn’t working.) But more on that in a minute.
First, let me get out of here and switch back to my regular text editor. The Internet Archive’s latest in-browser emulator lets anyone with internet access play and use dozens of games and programs originally released for the first Apple Macintosh computers in all of their black-and-white, low-resolution glory. (Ah, so nice to have that hyphen back.)
Along with MacWrite, the collection includes MacPaint, Dark Castle, The Oregon Trail, Space Invaders, Frogger, Shuffle Puck, Brickles, Prince of Persia, and dozens more. The emulator doesn’t just launch the software itself, but situates users in the old-school Mac operating environment, meaning you often find yourself looking at a 1984-style desktop, and opening the program yourself.
“The presentation represents some shift in philosophies, in terms of what we wanted to do,” says Jason Scott, an archivist at the Internet Archive. Whereas Scott went with a “shock and awe” approach to earlier software emulators—making hundreds of programs available all at once—he decided to go for a more methodical, curated strategy this time. One big reason for this is quality control. He’s still fielding tech-support requests for the MS-DOS emulator the archive released in 2014. (It includes thousands of titles.) But Scott also knew the early Mac programs that people would want to see at the outset.
“The main one was Dark Castle,” Scott told me. “Everyone remembers Dark Castle because it was a particularly well-made, good-looking game—but not even a fun one, I want to point out! People playing it on the Mac emulation are not happy. There are reviews.”
Reviews like: “I can't tell if the emulator is laggy, making my controls unresponsive? Or is this just a terrible game? Maybe a bit of both,” as one person commented on the site.
“They are like, ‘This runs too slow for it to be good,’” Scott told me, “when what they really mean is the game was originally so unfair.”
“But it looks beautiful, and the sound is beautiful, so I knew Dark Castle would be a big deal,” he added.  
For what it’s worth, I only vaguely remember Dark Castle from when I had an Apple IIc. When I tried playing it on the emulator this morning I was repeatedly killed by rabid bats, which I can confidently say is a reflection of my own rustiness and has nothing to do with the emulator quality. (It seemed to run pretty smoothly to me.)
But regardless of how well they run, the big question is why it’s worth the drudgery and the painstaking work of presenting ancient programs this way in the first place.
“The existential questions,” Scott said. “What is all this for? What do people need from the original Mac operating systems in the modern era?”
The Internet Archive focused on the Apple II era for a few reasons: It was a finite period of time, it represents a particularly rich moment in computing history, and people remain especially interested in the era. “Nostalgia, to be honest, is a huge chunk of it,” he added. “You’ve got people who come in, and look at the old thing, and they’re happy about the old thing, and then they move on.”
If all goes a planned, the next two emulators will be for the Commodore 64, which predated the early Macintosh; then Windows 98, which came after it. (“That’s only if it works,” Scott emphasized.)
Emulators can be quite buggy, given their complexity. A browser-based system involves the emulated machine running inside the browser's javascript environment, all within the computer running that browser. So, basically, “you’re running a computer within a simulated computer within another computer," Scott says. “It’s crazy.”
Scott’s also hoping to stretch the very idea of what people can do with emulators.
“The initial burst to emulation on the web was about removing the barrier to old software,” Scott told me. “The next realm will be that you can output the data that’s being generated and export it to your modern machine. That’s basically one developer away from happening right now. That’s the kind of thing people eventually will want and get.”
In the meantime, you can’t copy and paste text from the MacWrite emulator back to a contemporary word processor, for example—which is why I had to retype the opening to this story, letter by letter, just to get it into The Atlantic’s web-publishing program. This is still much easier than my predecessors had it, back when the Macintosh was brand new. It was around that time that my colleague James Fallows wrote a long piece for The Atlantic about his own adventure into computerdom. In 1982, he was using a Processor Technology SOL-20 that had 48KB of random access memory. This was miraculous to him then, as were the floppy disks it took, and the printer he hooked up to the machine—it spit out about one page per minute.
It wasn’t all peachy, even for an early adopter like him. There was the time his computer broke in dramatic fashion, sending him back to his old Smith-Corona typewriter for a full month. And also, Fallows wrote: “Computers cause another, more insidious problem, by forever distorting your sense of time.”
What he meant was that computers change people’s expectations about what we should be able to do, and how quickly we should be able to do it. But this observation, made back in 1982 about machines that were quite different from the ones we use today, also got me thinking about how technology collides with people’s perceptions of time as we look back at it years later. Once-miraculous systems seem impossibly slow. They make contemporary software—and the hardware like smartphones running that software—seem newly extraordinary. Watching a 35-year-old program do what it was designed to do is also an implicit reminder that the best tools we have today will, before too long, seem absurd in their limitations.
And we’re able to see all this because so many people, improbably, save objects like old floppy disks and computers.  “I actually still have the SOL-20, walnut case and all,” Fallows recently told me when I asked him what ever happened to it. Scott, from the Internet Archive, says he’s been flooded with requests from people who want to share the programs they’ve held onto all these decades.
“One person, he wasn’t comfortable mailing his floppies to us, so we had to mail him the equipment,” Scott said. “And now he is bringing up one of a kind—or, I should say, extremely rare—software.” His programs, which will be added to the emulator, include original games that are highly sought-after by collectors, and at least one piece of software that was never available commercially.
“This emulation is bringing back into the froth of contemporary culture the existence of all these old programs,” Scott said. “They’re no longer just words on a page.”
Or in my case, they are words on a page. Words rendered in Apple’s familiar old Chicago typeface, materializing on the screen just the way I remember it from so very long ago.


Over the last 20 years, the technology industry has become the most powerful industry in the world, boasting seven of the 20 most profitable companies. Last year, Apple literally doubled the profits ($53.4 billion) of the second-most profitable company, J.P. Morgan Chase ($24.4 billion). And when it comes to market value, tech companies sweep the top five: Apple, Google, Microsoft, Amazon, and Facebook. These companies are not only huge and profitable; they’re also growing.
By most measures, though not all, this power is concentrated in one specific region, the Pacific Coast, and even more tightly centralized in the San Francisco Bay Area. Incredibly, three of those five most valuable companies are located in three adjacent little towns in Silicon Valley. The total distance from Facebook in Menlo Park to Alphabet (née Google) in Mountain View to Apple in Cupertino is just 15 miles.
These companies—with apologies to Intel, Oracle, and Cisco—have become the Big Three of Silicon Valley.
Detroit had a Big Three for decades: General Motors, Ford, and Chrysler. They were also amazingly profitable, industry-leading, and birthed a global industry. In the late 1950s, these three companies had over 90 percent market share in the U.S. car market, which was also the world’s largest.
Now, companies from a similarly small region occupy a similarly dominant role in the economy, which has powered economic growth over the last several decades. But a comparison between Detroit’s Big Three and Silicon Valley’s shows how much the economy around any individual company or place has changed.
* * *
Investors now value tech as they once valued automotive (and oil) companies.
It was the IPO of the decade. Thousands of people flocked to brokers hoping to get their hands on some of the paper from one of the century’s most innovative and respected companies. Finally, finally, the common person could share in the wealth generated by the genius of … Ford.
The year was 1956, and Ford, privately held since its inception by the Ford family and (later) the Ford Foundation, was accessing the public markets. More than 10 million shares went on the market and were immediately snatched up by hundreds of thousands of investors at an opening price of $64.50. The Ford Foundation made $642.6 million in the sale.
It was the biggest IPO ever, as befit the automotive industry, which was the biggest in everything around the mid-century. Likewise, at the time Ford went public, the true behemoth of the American economy, General Motors, was the nation’s most valuable stock, running $263.27. And for good reason.
These companies make a ton of money.
In the second (1956) edition of the Fortune 500, Ford held the third slot in revenue and profit. That year, the company made $437 million dollars. General Motors took the top spot by becoming the first company to break $1 billion in profit that year ($1.19 to be exact). Only 16 companies even made $100 million in 1956. Chrysler was the least profitable of those companies, eking into the echelon with $100.1 million in profits.
The only rival the car industry had was the oil industry, which had the number-two company on Fortune’s list, Exxon Mobil, as well as seven others in the top 20 most profitable companies.
All this to say: making cars and fueling them dominated the American profit-making enterprise. Hell, even the two big tire manufacturers were among the top 35 profit-makers of 1956.
Cars were national. Tech is global.
But there are crucial differences between Detroit’s Big Three and Silicon Valley’s. One is that Silicon Valley’s companies are fully global enterprises.
Since 2015, the majority of Facebook’s ad revenue comes from overseas. Apple crossed that threshold in the first quarter of 2010, and now roughly two-thirds of the company’s revenue comes from abroad. Google, too, has long made a majority of its money outside the U.S., though its home country represents nearly half its revenue.
In fact, all the money that these companies are making overseas is one reason why they are valued so highly, Harvard Business School’s Shane Greenstein told me. “Since the election, the markets have factored in a presumed ‘tax holiday’ that allows firms to repatriate their foreign earnings without U.S. taxes,” Greenstein said. “That especially shapes the values of Apple and Google.”
Since the election, Facebook is up 11 percent, Google is up 21 percent, and Apple is up a gobsmacking 34 percent. Perhaps this is even more remarkable, given that tech company employees gave Hillary Clinton 60 times the money they gave to Donald Trump ($3 million to $50,000).
The tech labor force is a tiny fraction of the automotive industry’s.
The other crucial difference is that tech’s leading companies employee far fewer people than Detroit’s Big Three did. This point can be made in the single chart above,  but it’s worth unpacking in three ways.
One, even though the big industrial giants did employ a lot of people, by the 1950s they were already automating away some of the jobs that they’d just created by building huge factories. “Between 1948 and 1967—when the auto industry was at its economic peak—Detroit lost more than 130,000 manufacturing jobs,” the historian Thomas J. Sugrue has written. To me, that’s startling. This was the absolute golden age of manufacturing, yet in the seat of the most important industry, companies were shedding jobs.
Two, the car companies’ employees were far more concentrated in Detroit and the surrounding cities than the tech companies’. Apple only has 25,000 employees in the “Santa Clara Valley.” Google likely has around 20,000 at its home. And let’s call it at around 6,000 Facebook folks in Menlo Park.
Three, the tech companies have many, many, many subcontractors, from content moderators in the Philippines to manufacturing people at Foxconn in China to custodians on their own campuses to bus drivers dragging people up and down from San Francisco. The way modern companies work, they try to keep employees off their own balance sheets unless absolutely necessary, especially lower-wage workers.
The original Big Three were the motive power for a whole region’s economy. By employing so many people at decent wages, they created broad-based prosperity. In Silicon Valley, the wealth that the Big Three create goes to a much smaller slice of the population, building wealth for thousands instead of hundreds of thousands of workers. In 2016, Facebook generated $600,000 of net income per employee.
That is to say, the tech world, for all its disruptions, is a supercharged example of how the American economy as a whole works right now: The skilled and the already rich make huge amounts of money, and everyone else gets the leftovers.


Thinking about what technological innovation has done to journalism in the past two decades can be a dizzying experience. People have more data, better maps, prettier visualizations, more push notifications, faster fact-checking, and so on.
Yet there is a unifying feature behind all of these innovations, and it has to do with the role of media and the public in a democracy.
The news media, the argument goes, must provide the rationally-minded members of the public with enough information for them to see a clear and accurate picture of the world, and then become deliberative citizens. In that regard, technology could help news reports to be more accurate, data-driven, timely, fact-checked, with rich multimedia embellishment.
Technologically-enhanced journalism was supposed to become better at conveying the complexities of our reality to the public. Why, then, instead of an enlightened citizenry, did we then find ourselves facing a horde of hateful trolls, hysterical fake news outlets, a news agenda led by Russian hackers, and a never-ending spiral of conspiracy theories?
Maybe something was lost along the way. One of the fundamental problems with that vision of the role of media in democracy—that only imagines media as neutral transmitters of information on which the public then rationally deliberates—is that it might not be enough for the news media to hold a mirror that seek to reflect reality as accurately as possible.
A democratic public only emerges when its members feel concerned with something, and therefore become a public that cares. Here, the public is not an aggregate of rational individuals, but a community that realizes that it is affected by some issues. And to be affected, to be concerned, one has to have some kind of experience or sensation. Journalism, then, should also pay attention to what we could call “sensationalism.” Not in its derogatory meaning of exaggerating facts and events in an inaccurate way, but rather that our senses and perceptions, our sensations, inform knowledge in the most basic and important ways.
Among the technological innovations of the last decade, there’s a discrete yet enduring format that may fulfill such an alternative, “sensational” vision of the role of media in democracy: podcasts.
Of course, there is a wide variety of podcasts styles and tones, but with their conversational color and their immersion in sound and atmospheres, they have the potential to make you feel things. Podcasts bring you to places you’ve never been, they give you the impression of sharing an animated kitchen-table banter (or a loud bar argument) with a couple of friends. In that regard, podcasts are a “sensational” medium, a quality that may explain why millions of listeners tune in regularly and listen to long-form episodes that defy all common-sense knowledge about the shortness of our attention span.
One of my current favorites is Reply All, a Gimlet Media podcast that explores internet culture. Its hosts produce silly and fascinating episodes about, among many other curiosities, videos of rats eating a slice of pizza in the New York subway. But, springing from the same wide-eyed wonder about anything that pops-up from the weird corners of the internet, they also regularly bring about smart and honest reporting about phenomena that shine a vivid light on the current political landscape.
Even further from a traditional current affairs beat is the endless stream of podcasts about TV shows produced by Bald Moves. The two hosts, who are ex-Jehovah witnesses from the Midwest, record absurdly long podcasts where they just chat about the latest episode of Game of Thrones or Westworld. Their success (20 million downloads, and counting) may look like another embodiment of the futility of pop culture, until you realize that part of what they do for hours on is to meticulously debunk crazy fan theories—patiently drawing a line between the factual, the plausible, and the ludicrous. Which seems like a useful skill for a democratic public to have.
Freed from the stranglehold of objective or neutral reporting, podcasters act as storytellers rather than merely as journalists, allowing them to take their audiences around unexplored territories that listeners can experience, and maybe care about.
Sounds familiar? Maybe because that’s the recipe of talk radio, that has perfectly understood the power of someone just talking to an audience. What podcasting adds to the mix is a diversity of voices that were not heard before, and a capacity to reach audiences that were not in the habit of tuning in to the radio at the same time every day or every week.
That seamless integration of podcasts in people’s lives might be the key feature of what is otherwise a relatively low-tech medium that pretty much recycles the codes and craft of radio. Flexibility and chronicity—whenever and wherever you want, but you’ll hear from us again next week—allow podcasters to build a relationship with their audience, a relationship that is made of sensations, friendliness, and familiarity. Not a spectacular innovation, in terms of technology, but maybe just enough of a shift to realize what media theorist James Carey saw as the role of media, that is, to be the “conversation of our culture.”
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


There’re some really bad people who harass journalists. Women and minorities, especially, are the targets of extreme vitriol. Yet many newsrooms do little or nothing to attempt to protect their employees, or to think through how journalists and organizations should respond when harassment occurs.
Harassers and trolls have multiple motivations, often simple racism or misogyny, or in support of misinformation, or to suppress law enforcement or intelligence operations. Frequently, what appears to be multiple harassers are actually sock puppets, Twitter bots, or multiple accounts operated by single individuals.
Sustained harassment can do some serious psychological damage, and I speak from personal experience. Outright intimidation is a related problem, suppressing the delivery of trustworthy news—the kind of news reporting that is vital to democratic governance.
The usual solution is to ignore trolls and harassers, but they can be persistent, and they often game the system successfully. You can mute or block a harasser on Twitter or Facebook, but it's easy enough for them to create a new account in most systems.
If you're knowledgeable in Internet forensics, you can sometimes trace a harasser’s account, and “dox” them—that is, post personally identifiable information as a deterrent. However, that really needs to be done in a manner consistent with site terms and conditions, maybe working with their trust and safety team. (Seriously, this is a major ethical and legal issue.)
Or, if you have a thick skin, you can respond with “shock and awe,” that is, with a brutal response in turn. Or, you can reason with them, which has sometimes been known to work. Retaliation against professionals, however, often backfires. They’re usually well-funded, without conscience, and are often very smart.
One method to address rampant harassment would be for news organizations to work with their security departments to evaluate the worst abuse, and do risk assessment. Sometimes threats are only threats—but sometimes they’re serious. News organizations might share information regarding harassers, while respecting the rights of the accused and the terms and conditions of the organizations involved. There are also serious legal and ethical considerations here, to be considered.
Perhaps news orgs could enlist subscribers or other friends to bring harassment to light. Participants in such a system could simply tweet to the harasser an empty message, or with a designated hashtag, withdrawing approval while avoiding bringing attention to the actual harassment. The empty message might communicate a lot, in zero words.
I believe that the targets of harassment need help from platforms, and here’s the start of a way that could happen. I’m attempting to balance fairness with preventing harassers from gaming the system, so please consider this only a start.
Let’s use Twitter for this thought experiment, mostly because I understand it, and they’re genuinely trying to figure this out.
Suppose you’re a reporter who is a verified user, and you get a harassing tweet. You’d do a quote retweet to a specific account as a way to report the harassment. That specific account would be a bot which could begin to analyze the harassing tweet. The bot would enter the email and IP addresses of the tweet into a database.
Periodically, a process would run to see if there’s a pattern of harassment from that IP or email address, and if so, that account could be suspended and contacted.
While most journalists would find it easy to do such a retweet, perhaps this should be more open to all, which could involve a harassment report button or option in the menu on a particularly tweet. (There’s a button and other means within the Twitter UI to do some of this, and Twitter has signaled that more’s on the way.)
News orgs also need to step up to protect their own reporters.
They could enlist subscribers or other friends to bring harassment to light. Participants in such a system could simply send an automated tweet to the harasser that says “This account has been reported for harassment and is being monitored by the community.” This type of system publicly tells harassers “you are on notice” and the community is watching. Note that this might be easily gamed, unless from verified journalists or similar.
Since this is a significant job, social networks may want to test organizing a volunteer community—like the one Wikipedia has—to help monitor the reports and accounts. Social networks can take it a step further and have trained members of the community respond to some of the harassers (not the bots) to discuss why the tweets were reported for harassment. Teaching moments are important to address harassment. If the social media account user continues the harassment, they get permanently banned from the social network. Some online games have adapted a similar strategy and have had some success with this approach.
I realize these ideas are fairly half-baked; the devil’s in the details. I’m also omitting a lot of detail, since that deeply detailed info could help harassers game this or other systems. In any case, we need to start, somewhere. Harassment and intimidation of reporters is a real problem, with real consequences for democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


In 2013, a mysterious group of hackers that calls itself the Shadow Brokers stole a few disks full of National Security Agency secrets. Since last summer, they’ve been dumping these secrets on the internet. They have publicly embarrassed the NSA and damaged its intelligence-gathering capabilities, while at the same time have put sophisticated cyberweapons in the hands of anyone who wants them. They have exposed major vulnerabilities in Cisco routers, Microsoft Windows, and Linux mail servers, forcing those companies and their customers to scramble. And they gave the authors of the WannaCry ransomware the exploit they needed to infect hundreds of thousands of computer worldwide this month.
After the WannaCry outbreak, the Shadow Brokers threatened to release more NSA secrets every month, giving cybercriminals and other governments worldwide even more exploits and hacking tools.
Who are these guys? And how did they steal this information? The short answer is: We don’t know. But we can make some educated guesses based on the material they’ve published.
The Shadow Brokers suddenly appeared last August, when they published a series of hacking tools and computer exploits—vulnerabilities in common software—from the NSA. The material was from autumn 2013, and seems to have been collected from an external NSA staging server, a machine that is owned, leased, or otherwise controlled by the U.S., but with no connection to the agency. NSA hackers find obscure corners of the internet to hide the tools they need as they go about their work, and it seems the Shadow Brokers successfully hacked one of those caches.
In total, the group has published four sets of NSA material: a set of exploits and hacking tools against routers, the devices that direct data throughout computer networks; a similar collection against mail servers; another collection against Microsoft Windows; and a working directory of an NSA analyst breaking into the SWIFT banking network. Looking at the time stamps on the files and other material, they all come from around 2013. The Windows attack tools, published last month, might be a year or so older, based on which versions of Windows the tools support.
The releases are so different that they’re almost certainly from multiple sources at the NSA. The SWIFT files seem to come from an internal NSA computer, albeit one connected to the internet. The Microsoft files seem different, too; they don’t have the same identifying information that the router and mail server files do. The Shadow Brokers have released all the material unredacted, without the care journalists took with the Snowden documents or even the care WikiLeaks has taken with the CIA secrets it’s publishing. They also posted anonymous messages in bad English but with American cultural references.
Given all of this, I don’t think the agent responsible is a whistleblower. While possible, it seems like a whistleblower wouldn’t sit on attack tools for three years before publishing. They would act more like Edward Snowden or Chelsea Manning, collecting for a time and then publishing immediately—and publishing documents that discuss what the U.S. is doing to whom. That’s not what we’re seeing here; it’s simply a bunch of exploit code, which doesn’t have the political or ethical implications that a whistleblower would want to highlight. The SWIFT documents are records of an NSA operation, and the other posted files demonstrate that the NSA is hoarding vulnerabilities for attack rather than helping fix them and improve all of our security.
I also don’t think that it’s random hackers who stumbled on these tools and are just trying to harm the NSA or the U.S. Again, the three-year wait makes no sense. These documents and tools are cyber-Kryptonite; anyone who is secretly hoarding them is in danger from half the intelligence agencies in the world. Additionally, the publication schedule doesn’t make sense for the leakers to be cybercriminals. Criminals would use the hacking tools for themselves, incorporating the exploits into worms and viruses, and generally profiting from the theft.
That leaves a nation state. Whoever got this information years before and is leaking it now has to be both capable of hacking the NSA and willing to publish it all. Countries like Israel and France are capable, but would never publish, because they wouldn’t want to incur the wrath of the U.S. Country like North Korea or Iran probably aren’t capable. (Additionally, North Korea is suspected of being behind WannaCry, which was written after the Shadow Brokers released that vulnerability to the public.) As I’ve written previously, the obvious list of countries who fit my two criteria is small: Russia, China, and—I’m out of ideas. And China is currently trying to make nice with the U.S.
It was generally believed last August, when the first documents were released and before it became politically controversial to say so, that the Russians were behind the leak, and that it was a warning message to President Barack Obama not to retaliate for the Democratic National Committee hacks. Edward Snowden guessed Russia, too. But the problem with the Russia theory is, why? These leaked tools are much more valuable if kept secret. Russia could use the knowledge to detect NSA hacking in its own country and to attack other countries. By publishing the tools, the Shadow Brokers are signaling that they don’t care if the U.S. knows the tools were stolen.
Sure, there’s a chance the attackers knew that the U.S. knew that the attackers knew—and round and round we go. But the “we don’t give a damn” nature of the releases points to an attacker who isn’t thinking strategically: a lone hacker or hacking group, which clashes with the nation-state theory.
This is all speculation on my part, based on discussion with others who don’t have access to the classified forensic and intelligence analysis. Inside the NSA, they have a lot more information. Many of the files published include operational notes and identifying information. NSA researchers know exactly which servers were compromised, and through that know what other information the attackers would have access to. As with the Snowden documents, though, they only know what the attackers could have taken and not what they did take. But they did alert Microsoft about the Windows vulnerability the Shadow Brokers released months in advance. Did they have eavesdropping capability inside whoever stole the files, as they claimed to when the Russians attacked the State Department? We have no idea.
So, how did the Shadow Brokers do it? Did someone inside the NSA accidentally mount the wrong server on some external network? That’s possible, but seems very unlikely for the organization to make that kind of rookie mistake. Did someone hack the NSA itself? Could there be a mole inside the NSA?
If it is a mole, my guess is that the person was arrested before the Shadow Brokers released anything. No country would burn a mole working for it by publishing what that person delivered while he or she was still in danger. Intelligence agencies know that if they betray a source this severely, they’ll never get another one.
That points to two possibilities. The first is that the files came from Hal Martin. He’s the NSA contractor who was arrested in August for hoarding agency secrets in his house for two years. He can’t be the publisher, because the Shadow Brokers are in business even though he is in prison. But maybe the leaker got the documents from his stash, either because Martin gave the documents to them or because he himself was hacked. The dates line up, so it’s theoretically possible. There’s nothing in the public indictment against Martin that speaks to his selling secrets to a foreign power, but that’s just the sort of thing that would be left out. It’s not needed for a conviction.
If the source of the documents is Hal Martin, then we can speculate that a random hacker did in fact stumble on it—no need for nation-state cyberattack skills.
The other option is a mysterious second NSA leaker of cyberattack tools. Could this be the person who stole the NSA documents and passed them on to someone else? The only time I have ever heard about this was from a Washington Post story about Martin:
There was a second, previously undisclosed breach of cybertools, discovered in the summer of 2015, which was also carried out by a TAO employee [a worker in the Office of Tailored Access Operations], one official said. That individual also has been arrested, but his case has not been made public. The individual is not thought to have shared the material with another country, the official said.
Of course, “not thought to have” is not the same as not having done so.
It is interesting that there have been no public arrests of anyone in connection with these hacks. If the NSA knows where the files came from, it knows who had access to them—and it’s long since questioned everyone involved and should know if someone deliberately or accidentally lost control of them. I know that many people, both inside the government and out, think there is some sort of domestic involvement; things may be more complicated than I realize.
It’s also not over. Last week, the Shadow Brokers were back, with a rambling and taunting message announcing a “Data Dump of the Month” service. They’re offering to sell unreleased NSA attack tools—something they also tried last August—with the threat to publish them if no one pays. The group has made good on their previous boasts: In the coming months, we might see new exploits against web browsers, networking equipment, smartphones, and operating systems—Windows in particular. Even scarier, they’re threatening to release raw NSA intercepts: data from the SWIFT network and banks, and “compromised data from Russian, Chinese, Iranian, or North Korean nukes and missile programs.”
Whoever the Shadow Brokers are, however they stole these disks full of NSA secrets, and for whatever reason they’re releasing them, it’s going to be a long summer inside of Fort Meade—as it will be for the rest of us.


The Society of Women Engineers recently shared a trove of astonishing documents from the group’s archives. They’re letters, loads of them, all directed at women engineering students who had contacted various universities about their interest in connecting with other women studying engineering.
Lou Alta Melton and Hilda Counts, both students at the University of Colorado in 1919, were trying to start their own professional society. Their letters—and the many responses they received—are part of the Society of Women Engineers sprawling archives, which are housed at Wayne State University in Detroit.
“We have not now, have never had, and do not expect to have in the near future, any women students registered in our engineering department,” Thorndike Saville, and an associate professor at the University of North Carolina, wrote in his reply to Melton. He signed it, “Yours very truly.”
“We do not permit women to register in the Engineering School under present regulations,” wrote William Mott, the dean of the Carnegie Institute of Technology, which would later merge with the Mellon Institute to become Carnegie Mellon.
1919 was the year Congress passed the 19th amendment, granting women the right to vote. But, as so many of the letters in the collection demonstrate, many women wouldn’t be permitted to formally study the subjects that interested them until much later. Discrimination against women in engineering isn’t always so straightforward today, but the forces that push women out of the field (or prevent them from pursuing it in the first place) remain persistent and complex. Women account for some 20 percent of engineering graduates, according to Harvard Business Review, but a huge portion of them either quit or never enter the profession. Much has changed for women engineers in the past century, but perhaps not enough.
“I suspect the number of women who have undertaken general engineering courses is so few that you will hardly be able to form an organization,” William Raymond, the dean of the State University of Iowa wrote in 1919, adding, “However, I may be mistaken.”
Some schools seemed to encourage women to find loopholes so they could at least attend classes—but didn’t take the additional step of letting them pursue a degree. “While we cannot legally register women in the College,” wrote J.R. Benton, the dean of engineering at the University of Florida, in 1919, “there is nothing to prevent our admitting them as visitors to the classes, which permits them to get all the benefit of instruction altho without definite status as students.”
“Hitherto, there has been no demand for engineering courses here on the part of women,” he added, “except in one case, that of Leanora Semmes, who is now taking work in Mechanical Drawing.” A quick search of newspaper archives and digitized books provides no evidence that Semmes ever worked as an engineer—or at least no evidence that she was ever recognized for it.
Counts, one of the letter writers from the Society of Women Engineers archive, is remembered as a trailblazer—her electrical engineering degree was the first ever awarded to a woman in Colorado and she later took a job with the Rural Electrification Administration in Washington, D.C. Melton, the other letter writer, made headlines at least once, when in 1920 she took a job as a civil engineer in the U.S. Bureau of Public Roads.
“Leave it to a woman!” the Iowa City Press-Citizen wrote at the time. “That’s what the  United States Bureau of Public Roads in Denver did when an assistant bridge engineer’s job was open. Miss Lou Alta Melton is filling the place in fine shape.” The newspaper described Melton as the only “girl” graduate in her civil engineering class at the University of Colorado.
One response to Melton’s letter came from the secretary of the T-Square Society, a group of women engineers at the University of Michigan that had already formed. They were interested in a potential partnership, the secretary wrote. But these and other early organizing efforts eventually fell apart, as Margaret E. Layne described in her book, Women in Engineering: Pioneers and Trailblazers, “partly because they followed a logic of maintaining professional standards similar to that used by male national organizations. Hence they excluded engineering students and working women engineers without formal education.”
How Women Mentors Make a Difference in Engineering
In other words, the high standards for the hypothetical society were deemed necessary to combat sexism, but the sexism that kept women out of formal programs also thwarted efforts to find a critical mass of women engineers for a such a society. It would be decades before the Society of Women Engineers was founded—first as an informal group during World War II, then officially in 1950.
There are still small bright spots in the society’s collection of responses to Melton and Counts. At least one dean of engineering, W.N. Gladson, of the University of Arkansas, wished Melton well. It doesn’t sound like much, but it was more than many other deans were willing to do. “I am aware that in the Northern and Eastern Colleges, often girls register for engineering work and make very excellent students...” Gladson wrote. “Wishing for your organization the fullest measure of success, I am.”
Elsewhere, a professor of mechanical engineering at Georgia Tech seemed to signal that times were changing. (Though he didn’t bother responding to Melton by name.)
“Dear Lady,” wrote J.B. Boon, of Georgia Tech, “Up to the present, women students have not been admitted to [Georgia] Tech.” He added—perhaps optimistically?—that Atlanta officials had taken up the question of women’s suffrage, “so no knowing what may happen!”


Facebook’s 2 billion users post a steady stream of baby pictures, opinions about romantic comedies, reactions to the news—and disturbing depictions of violence, abuse, and self-harm. Over the last decade, the company has struggled to come to terms with moderating that last category. How do they parse a joke from a threat, art from pornography, a cry for help from a serious suicide attempt? And even if they can correctly categorize disturbing posts with thousands of human contractors sifting through user-flagged content, what should they do about it?
This weekend, The Guardian began publishing stories based on 100 documents leaked to them from the training process that these content moderators go through. They’re calling it The Facebook Files. Facebook neither confirmed nor denied the authenticity of the documents, but given The Guardian’s history of reporting from leaks, we proceed here with the assumption that the documents are real training materials used by at least one of Facebook’s content moderation contractors.
The Guardian has so far focused on specific types of cases that come up in content moderation: the abuse of children and animals, revenge porn, self-harm, and threats of violence.  
The moderator training guidelines are filled with examples. Some show moderators being trained to allow remarkably violent statements to stay on the site. This one, for example, is supposed to help content moderators see the difference between “credible” threats of violence and other statements invoking violence.
The slides suggest that Facebook has begun to come up with rules that cover literally anything distressing or horrible someone could post. But what do they say about the role Facebook sees itself playing in the world it's creating?
In explaining the company’s reasoning about violent posts, a training document says, “We aim to allow as much speech as possible but draw the line at content that could credibly cause real-world harm.”
In the U.S., there is obviously an entire body of legal cases dedicated to parsing the limits and protections of speech. Different places in the world have different rules and norms. But these cases occur in the context of a single national government and its relationship to “free speech.”
Here, we’re talking about a platform, not a government. Facebook is unconstrained by centuries of interpretations of constitutions and legal precedents. It could do whatever it wanted.
They could systematically aim for harm minimization not speech maximization. That change of assumptions would lead to a different set of individual guidelines on posts. The popular children's online world, Club Penguin, for example, offered multiple levels of language filtering as well as an "Ultimate Safe Chat" mode that only allowed pre-selected phrases to be chosen from a list. At one point, a thousand words were being added to the software's verboten list per day. But “allow[ing] as much speech as possible” has been part of the ideology of this generation of social media companies from the very beginning.
Getting people to post more, as opposed to less, is the core of Facebook’s mission as a company. It is no surprise that the companies built on sharing that have been the most successful come from the United States, which is the most pro-free speech country in the world.
From these documents and the company’s statements, the company has pragmatically chosen to limit areas where it has encountered problems. And those problems are primarily quantified through the flagging that users themselves do.
“As a trusted community of friends, family, coworkers, and classmates, Facebook is largely self-regulated,” one document reads. “People who use Facebook can and do report content that they find questionable or offensive.”
Facebook wants to stay out of it. So Facebook reacts, evolving content moderation guidelines to patch the holes where “self-regulation” fails. Given the number of territories and cultures into which Facebook has integrated itself, one can imagine Facebook’s leadership sees this both as the most reasonable and only practical approach. In cases where they have deployed top-down speech limits, they’ve gotten it wrong, too (as in the “Napalm Girl” controversy).
“We work hard to make Facebook as safe as possible while enabling free speech,” said Monika Bickert, Facebook’s Head of Global Policy Management. “This requires a lot of thought into detailed and often difficult questions, and getting it right is something we take very seriously.”
Let’s stipulate that these are difficult decisions on an individual basis. And let’s further stipulate that multiplying the problem by 2-billion users makes the task daunting, even for a company with $7 billion on hand. Facebook has committed to adding 3,000 more content moderators to the 4,500 working for the company today.
But is Facebook’s current approach to content moderation built on a firm foundation? The company’s approach to content moderation risks abdicating the responsibility that the world’s most popular platform needs to take on.
“When millions of people get together to share things that are important to them, sometimes these discussions and posts include controversial topics and content,” we read in the training document.  “We believe this online dialogue mirrors the exchange of ideas and opinions that happens throughout people’s lives offline, in conversations at home, at work, in cafes and in classrooms.”
In other words, Facebook holds that the posts on its platform reflect offline realities and are merely a reflection of what is, rather than a causal factor in making things come to be.
Social Media’s Silent Filter
Facebook must accept the reality that it has changed how people talk to each other. When we have conversations “at home, at work, in cafes, and in classrooms,” there is not an elaborate scoring methodology that determines whose voice will be the loudest. Russian trolls aren’t interjecting disinformation. My visibility to my family is not dependent on the quantifiable engagement that my statements generate. Every word that I utter or picture that I like is not being used to target advertisements (including many from media companies and political actors) at me.
The platform’s own dynamics are a huge part of what gets posted to the platform. They are less a “mirror” of social dynamics than an engine driving them to greater intensity, with unpredictable consequences.
Facebook’s Mark Zuckerberg seemed to acknowledge this in his epic manifesto about the kind of community that he wanted Facebook to build.
“For the past decade, Facebook has focused on connecting friends and families,” he wrote. “With that foundation, our next focus will be developing the social infrastructure for community—for supporting us, for keeping us safe, for informing us, for civic engagement, and for inclusion of all.”
To get this “social infrastructure for community” right, Facebook has to acknowledge that it has not merely “connected friends and families." It has changed their very nature.


The year was 1999, and the people were going online. AOL, Compuserve, mp3.com, and AltaVista loaded bit by bit after dial-up chirps, on screens across the world. Watching the internet extend its reach, a small group of scientists thought a more extensive digital leap was in order, one that encompassed the galaxy itself. And so it was that before the new millennium dawned, researchers at the University of California released a citizen-science program called SETI@Home.
The idea went like this: When internet-farers abandoned their computers long enough that a screen saver popped up, that saver wouldn’t be WordArt bouncing around, 3-D neon-metallic pipes installing themselves inch by inch, or a self-satisfied flying Windows logo. No. Their screens would be saved by displays of data analysis, showing which and how much data from elsewhere their CPUs were churning through during down-time. The data would come from observations of distant stars, conducted by astronomers searching for evidence of an extraterrestrial intelligence. Each participating computer would dig through SETI data for suspicious signals, possibly containing a “Hello, World” or two from aliens. Anyone with 28 kbps could be the person to discover another civilization.
When the researchers launched SETI@Home, in May of ’99, they thought maybe 1,000 people might sign up. That number—and the bleaker view from outsiders, who said perhaps no one would join the crew—informed a poor decision: to set up a single desktop to farm out the data and take back the analysis.
But the problem was, people really liked the idea of letting their computers find aliens while they did nothing except not touch the mouse. And for SETI@Home’s launch, a million people signed up.  Of course, the lone data-serving desktop staggered. SETI@Home fell down as soon as it started walking. Luckily, now-defunct Sun Microsystems donated computers to help the program get back on its feet. In the years since, more than 4 million people have tried SETI@Home. Together, they make up a collective computing power that exceeds 2008’s premier supercomputer.
But they have yet to find any aliens.
* * *
SETI is a middle-aged science, with 57 years under its sagging belt. It began in 1960, when an astronomer named Frank Drake used an 85-foot radio telescope in Green Bank, West Virginia, to scan two Sun-like stars for signs of intelligent life—radio emissions the systems couldn’t produce on their own, like the thin-frequency broadcasts of our radio stations, or blips that repeated in a purposeful-looking way. Since then, scientists and engineers have used radio and optical telescopes to search much more of the sky—for those “narrowband” broadcasts, for fast pings, for long drones, for patterns distinguishing themselves from the chaotic background static and natural signals from stars and supernovae.
But the hardest part about SETI is that scientists don’t know where ET may live, or how ET’s civilization might choose to communicate. And so they have to look for a rainbow of possible missives from other solar systems, all of which move and spin at their own special-snowflake speeds through the universe. There’s only one way to do that, says Dan Werthimer, the chief SETI scientist at Berkeley and a co-founder of SETI@Home: “We need a lot of computing power.”
In the 1970s, when Werthimer’s Berkeley colleagues launched a SETI project called SERENDIP, they sucked power from all the computers in their building, then the neighboring building. In a way, it was a SETI@Home prototype. In the decades that followed, they turned to supercomputers. And then, they came for your CPUs.
* * *
The idea for SETI@Home originated at a cocktail party in Seattle, when computer scientist David Gedye asked a friend what it might take to excite the public about science. Could computers somehow do something similar to what the Apollo program had done? Gedye dreamed up the idea of “volunteer computing,” in which people gave up their hard drives for the greater good when those drives were idle, much like people give up their idle cars, for periods of time, to Turo (if Turo didn’t make money and also served the greater good). What might people volunteer to help with? His mind wandered to The X-Files, UFOs, hit headlines fronting the National Enquirer. People were so interested in all that. “It’s a slightly misguided interest, but still,” says David Anderson, Gedye’s graduate-school advisor at Berkeley. Interest is interest is interest, misguided or guided perfectly.
But Gedye wasn’t a SETI guy—he was a computer guy—so he didn’t know if or how a citizen-computing project would work. He got in touch with astronomer Woody Sullivan, who worked at the University of Washington in Seattle. Sullivan turned him over to Werthimer. And Gedye looped in Anderson. They had a quorum, of sorts.
Anderson, who worked in industry at the time, dedicated evenings to writing software that could take data from the Arecibo radio telescope, mother-bird it into digestible bits, send it to your desktop, command it to hunt for aliens, and then send the results back to the Berkeley home base. No small task.
They raised some money—notably, $50,000 from the Planetary Society and $10,000 from a Paul Allen-backed company. But most of the work-hours, like the computer-hours they were soliciting, were volunteer labor. Out of necessity, they did hire a few people with operating-system expertise, to deal with the wonky screensaver behavior of both Windows and Macintosh. “It’s difficult trying to develop a program that’s intended to run on every computer in the world,” says Anderson.
And yet, by May 17, 1999, they were up, and soon after, they were running. And those million people in this world were looking for not-people on other worlds.
One morning, early in the new millennium, the team came into the office and surveyed the record of what those million had done so far. In the previous 24 hours, the volunteers had done what would have taken a single desktop one thousand years to do. “Suppose you’re a scientist, and you have some idea, and it’s going to take 1,000 years,” says Anderson. “You’re going to discard it. But we did it.”
After being noses-down to their keyboards since the start, it was their first feeling of triumph. “It was really a battle for survival,” says Anderson. “We didn’t really have time to look up and realize what an amazing thing we were doing.”
Then, when they looked up again, at the SETI@Home forums, they saw something else: “It was probably less than a year after we started that we started getting notices about the weddings of people who met through SETI@Home,” says Eric Korpela, a SETI@Home project scientist and astronomer at Berkeley.
* * *
The SETI astronomers began to collect more and different types of data, from the likes of the Arecibo radio telescope. Operating systems evolved. There were new signal types to search for, like pulses so rapid they would have seemed like notes held at pianissimo to previous processors. With all that change, they needed to update the software frequently. But they couldn’t put out a new version every few months and expect people to download it.
Anderson wanted to create a self-updating infrastructure that would solve that problem—and be flexible enough that other, non-SETI projects could bring their work onboard and benefit from distributed computing. And so BOINC—Berkeley Open Infrastructure for Network Computing—was born.
Today, you can use BOINC to serve up your computer’s free time to develop malaria drugs, cancer drugs, HIV drugs. You can fold proteins or help predict the climate. You can search for gravitational waves or run simulations of the heart’s electrical activity, or any of 30 projects. And you can now run BOINC on GPUs—graphical processing units, brought to you by gamers—and on Android smartphones Nearly half a million people use the infrastructure now, making the système totale a 19 petaflop supercomputer, the third-largest megacalculator on the planet.
Home computers have gotten about 100 times faster since 1999, thank God, and on the data distribution side, Berkeley has gotten about 10 times faster. They’re adding BOINC as a bandwidth-increasing option to the Texas Advanced Computing Center and nanoHUB, and also letting people sign up for volunteer computing, tell the system what they think are the most important scientific goals, and then have their computers be automatically matched to projects as those projects need time. It’s like OkCupid dating, for scientific research. BOINC, and SETI@Home can do more work than ever.
* * *
The thing is, though, they’ve already done a lot of work—so much work they can’t keep up with themselves. Sitting in a database are 7 billion possible alien signals that citizen scientists and their idle computers have already uncovered.
Most of these are probably human-made interference: short-circuiting electric fences, airport radar, XM satellite radio, or a microwave opened a second too soon. Others are likely random noise that added up to a masquerade of significance. As Anderson says, “Random noise has the property that whatever you’re looking for, it eventually occurs. If you generate random letters. You eventually get the complete works of Shakespeare.” Or the emissions are just miscategorized natural signals.
Anderson has been working on a program called Nebula that will trawl that billions-and-billions-strong database, reject the interference, and upvote the best candidates that might—just might—be actual alien signals. Four thousand computers at the Max Planck Institute for Gravitational Physics in Germany help him narrow down the digital location of that holiest of grails. Once something alien in appearance pops up—say from around the star Vega—the software automatically searches the rest of the data. It finds all the other times, in the 18 years of SETI@Home history, that Arecibo or the recently added telescopes from a $100 milion initiative called Breakthrough Listen have looked at Vega. Was the signal there then too? “We’re kind of hoping that the aliens are sending a constant beacon,” says Korpela, “and that every time a telescope passes over a point in the sky, we see it.”
If no old data exists—or if the old data is particularly promising—the researchers request new telescope time and ask SETI colleagues to verify the signal with their own telescopes, to see if they can intercept that beacon, that siren, that unequivocal statement of what SETI scientists and SETI@Home participants hope is true: That we are not alone.
So far, that’s a no-go. “We’ve never had a candidate so exciting that we call the director and say, ‘Throw everybody off the telescope,’” says Werthimer. “We’ve never had anything that resembles ET.”
And partly for that reason, the SETI@Homers are now working on detecting “wideband” signals—ones that come at a spread spectrum of frequencies, like the beam-downs from DIRECTV. Humans (and by extension, extraterrestrials) can embed more information more efficiently in these spread-spectrum emissions. If the goal is to disseminate information, rather than just graffiti “We’re here!” on the fabric of spacetime, wideband is the way to go. And SETI scientists’ thinking goes like this: We’ve been looking mostly for purposeful, obvious transmissions, ones wrapped neatly for us. But we haven’t found any—which might mean they just aren’t there. Extraterrestrial communications might be aimed at members of their own civilizations, in which case they’re more likely to go the DIRECTV route, and we’re likely to find only the “leakage” of those communication lines.
“If there really are these advanced civilizations, it’d be trivial to contact us,” says Werthimer. “They’d be landing on the White House—well, maybe not this White House. But they’d be shining a laser in Frank Drake’s eyes. I don’t see why they would make it so difficult that we would have to do all this hard stuff.”
And so humans, and our sleeping computers, may have to eavesdrop on messages not addressed to us—the ones the aliens send to their own (for lack of a better word) people, and then insert ourselves into the chatter. “I don’t mean to interrupt,” we might someday say, “but I couldn’t help overhearing...” And because of SETI@Home and  BOINC, it might be your laptop that gets that awkward conversation started.


American voting relies heavily on technology. Voting machines and ballot counters have sped up the formerly tedious process of counting votes. Yet long-standing research shows that these technologies are susceptible to errors and manipulation that could elect the wrong person. In the 2016 presidential election, those concerns made their way into public consciousness, worrying both sides of the political fence. The uncertainty led to a set of last-minute, expensive state recounts—most of which were incomplete or blocked by courts. But we could ensure that all elections are fair and accurate with one simple low-tech fix: risk-limiting audits.
Risk-limiting audits are specific to elections, but they are very similar to the audits that are routinely required of corporate America. Under them, a random sample of ballots is chosen and then hand-counted. That sample, plus a little applied math, can tell us whether the machines picked the right winner.
In nearly all cases, a risk-limiting audit can be performed by counting only a small fraction of ballots cast.  For example, the M.I.T. professor Ron Rivest calculates that Michigan could have checked just 11 percent of its ballots and achieved 95 percent confidence that their machine-counted result correctly named Donald Trump the winner of Michigan's electoral votes. Texas and Missouri, with their wider margins in the presidential race, could have counted a randomly chosen 700 ballots and 10 ballots, respectively, to achieve the same confidence level.  
Since risk-limiting audits verify elections while minimizing the number of audited ballots, they are both inexpensive and speedy. They largely eliminate the need for emergency recruitment of recount workers and can be conducted before the election must be certified by law. This also means that auditing can become a routine part of every election. Regular auditing will also allow state and county electioneers to become more skilled at spotting problems, from mundane system errors to deliberate hacking, something that is difficult for them to do today.
Colorado has been working on audits since 2011, and is ready to take the next step: Risk-limiting audits will be required in Colorado’s 2017 election. More states should follow Colorado’s bold lead.
Yet too many states still have electronic voting machines with no paper trail, meaning that no audit is possible at all. And all audits are not created equal. After the 2016 election, many Wisconsin counties simply ran ballots through their tabulating machines a second time and called it an “audit.” But if the machines were broken or compromised, the same inaccuracies they registered the first time would show up again the second time.
Technology is already deeply embedded in our voting systems. The next step isn’t to pile on more technology; it’s ensuring that the technology we rely on works properly and has not been hacked or undermined. The way to do that is clear: standard election procedure should include risk-limiting audits. If the Nevada Gaming Commission can establish detailed audit requirements for Keno, we can certainly do the same for our democracy.
This article is part of a collaboration with the Markkula Center for Applied Ethics at Santa Clara University.


Donald Trump doesn’t need a crystal ball, he has a mysterious glowing orb. No, wait. Scratch that. Donald Trump doesn’t need a crystal ball, he has a mysterious clairvoyant Twitter account.
There seems to be, Trump watchers have noticed, a weirdly prophetic tweet in Trump’s past for every new aspect of his presidency—from his weekends golfing at Mar-a-Lago to each new bombshell scoop about the embattled White House and its alleged ties to Russia.
This goes beyond using classic Trump tweets to insult him, though people are doing that, too—the prototypical example comes from June 2014, when Trump tweeted, “Are you allowed to impeach a president for gross incompetence?”
Trump’s critics are now delighting in the ability to criticize Trump by using his own targeted complaints about others. His past tweets underscore stupendous hypocrisy, they say, and perhaps a hint at an epic political downfall. Democrats have been agitating for Trump’s political demise since before he was the Republican nominee, but even the most apolitical observer would acknowledge how uncanny some of Trump’s past tweets have become.
When the Congressional Budget Office determined that Congressional Republicans’ Trump-supported plan to replace the Affordable Care Act would increase the number of uninsured people by 24 million in the next decade, the internet reached for a Trump tweet from 2014: “It’s Thursday. How many people have lost their health care today?” he’d written at the time.
When Trump ordered a missile strike against Syria in April, people shared this Trump tweet from 2013: “The President must get Congressional approval before attacking Syria-big mistake if he does not!”
This one has been making the rounds, too: “PresObama is not busy talking to Congress about Syria..he is playing golf ...go figure,” Trump tweeted in 2013. Fast forward to 2017 and Trump has already outpaced Obama’s presidential golfing rate. (Obama was a prolific golfer.*)
There’s more.
After reports that Trump is considering a massive troop surge in Afghanistan, this 2013 tweet reappeared: “Let’s get out of Afghanistan. Our troops are being killed by the Afghanis we train and we waste billions there. Nonsense!  Rebuild the USA.”
“Is there a name for the eerie way that Trump subtweeted his entire presidency?” Peter Daou, a former Hillary Clinton adviser, said recently. “There’s truly a tweet for every occasion.” Various observers have compared the phenomenon to everything from mass-produced greeting cards to the elegance of mathematics to science fiction.  
“Seems there’s a hypocritical Trump tweet for almost every occasion,” one Twitter user wrote. “They’re like Hallmark cards.”
And another: “His hypocrisy meter uses a Fibonacci number and it just keeps spinning into infinity through space and time...”
The appeal of reaching for Trump’s old tweets is understandable, and not just because people enjoy pointing out the hypocrisy of politicians they dislike. The medium is meaningful here, too. Rarely are schadenfreude and political commentary packaged together so neatly. Tweets are, by the platform’s very nature, succinct, atomized, and imminently shareable. Trump himself has employed the same tactic in an attempt to point out hypocrisy among his celebrity rivals.
Skipping through the linear order of events this way is also a reflection of warped time as a dominant theme in the Trump presidency—both among supporters who want to travel backward in time to Make America Great Again, and among critics who compare him to the time-traveling Back to the Future villain Biff Tannen (or worse.)
Using past tweets as present criticism isn’t just suited to Twitter’s platform, or political culture, or even outright partisanship. This approach also leverages Trump’s blustery style of attacking others as well as the richness of his particular Twitter archive, which goes back to 2009.
And in an irony that’s almost too delectable, there is the fact that so many of Trump’s past attacks against Hillary Clinton in last year’s presidential campaign were based on the premise that she was reckless with classified information—which is now the same criticism Trump faces in one of the biggest scandals of his fledgling presidency. “Crooked Hillary Clinton and her team ‘were extremely careless in their handling of very sensitive, highly classified information.’ Not fit!” he tweeted last July. (Trump’s ongoing refusal to share his tax returns is in similarly sharp contrast to this 2012 tweet: “All recent Presidents have released their transcripts. What is @BarackObama hiding?”)
Given the Russia probe, many of Trump’s old tweets seem to have startling new relevance. Like this one, from October, which people shared amid the news last week that the former FBI Director Robert Mueller had been appointed special counsel to investigate Russian interference in the 2016 presidential election. “If I win,” Trump had tweeted a month before election day, presumably directed at Clinton, “I am going to instruct my AG to get a special prosecutor to look into your situation bc there's never been anything like your lies.”
And this one, from February, which Democrats seized on when The Washington Post revealed Trump had shared highly classified information with Russian leaders in the Oval Office the day after he fired the FBI director James Comey: “The real scandal here is that classified information is illegally given out by ‘intelligence’ like candy. Very un-American!”
Last month, when Trump criticized the Obama administration for having done “nothing” to stop the Assad regime in Syria, people resurrected a string of Trump tweets from 2013: “We should stay the hell out of Syria,” he had tweeted in one case. And also: “Do NOT attack Syria,fix U.S.A.”And also: “Stay away and fix broken U.S.”
And just in case there was any doubt whatsoever: “What I am saying is stay out of Syria.”
This week, after Trump visited Saudi Arabia—where the first lady was photographed without a headscarf—this 2015 Trump tweet resurfaced: “Many people are saying it was wonderful that Mrs. Obama refused to wear a scarf in Saudi Arabia, but they were insulted. We have enuf enemies,” he tweeted in January of that year.
Other figures in the Trump inner circle have made cameos in this internet parlor game. After reports on Monday that Michael Flynn, Trump’s former national security adviser, would invoke his Fifth Amendment right against self-incrimination, a 2013 tweet from Sean Spicer, Trump’s press secretary, sprang back to life online:  “why do u take the 5th if you have done nothing wrong and have nothing to hide?” Spicer had tweeted at the time. It seems to have been a reference to an IRS official who invoked her right not to testify after disclosing the agency’s improper targeting of conservative groups. But untethered from context and time, Spicer’s past commentary seemed linked to Flynn today.
There are so many more examples that “a Trump tweet for everything” has long since crossed over into parody—meaning you should definitely remain skeptical about anything being shared as a past Trump tweet until you verify it for yourself. Consider this delightful but obviously fake mock-up, for example, and always cross reference against the legitimate Trump tweet archive.
someone's got egg on their face pic.twitter.com/1iIXwg98Xn
For the record, Trump’s pixelated paper trail shows no references to any orb other than the one in words like “Forbes,” “absorb,” and “forbid.” Even in the most surreal political scenarios, there’s only so much you can see coming.
Or, as Trump tweeted in 2013, “Just shows that you can have all the cards and lose if you don’t know what you’re doing.”
* Tracking any president’s time on the green is a longstanding, petty political pastime. Not surprisingly, then, pundits have gone full ouroboros on the Trump-versus-Obama golf question. Conservative commentators are now accusing fact-checking outlets of hypocrisy for tracking Trump’s golf-playing hypocrisy, arguing that fact checkers didn't follow Obama’s golfing schedule as closely. (Many national news organizations, including The New York Times, The Washington Post, and The Atlantic, wrote about Obama’s frequent golfing while he was president.)


