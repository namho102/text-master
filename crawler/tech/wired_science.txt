
Today, a teaspoon of spit and a hundred bucks is all you need to get a snapshot of your DNA. But getting the full picture—all 3 billion base pairs of your genome—requires a much more laborious process. One that, even with the aid of sophisticated statistics, scientists still struggle over. It’s exactly the kind of problem that makes sense to outsource to artificial intelligence.
On Monday, Google released a tool called DeepVariant that uses deep learning—the machine learning technique that now dominates AI—to assemble full human genomes. Modeled loosely on the networks of neurons in the human brain, these massive mathematical models have learned how to do things like identify faces posted to your Facebook news feed, transcribe your inane requests to Siri, and even fight internet trolls. And now, engineers at Google Brain and Verily (Alphabet’s life sciences spin-off) have taught one to take raw sequencing data and line up the billions of As, Ts, Cs, and Gs that make you you.
And oh yeah, it’s more accurate than all the existing methods out there. Last year, DeepVariant took first prize in an FDA contest promoting improvements in genetic sequencing. The open source version the Google Brain/Verily team introduced to the world Monday reduced the error rates even further—by more than 50 percent. Looks like grandmaster Ke Jie isn’t be the only one getting bested by Google’s AI neural networks this year.
DeepVariant arrives at a time when healthcare providers, pharma firms, and medical diagnostic manufacturers are all racing to capture as much genomic information as they can. To meet the need, Google rivals like IBM and Microsoft are all moving into the healthcare AI space, with speculation about whether Apple and Amazon will follow suit. While DeepVariant’s code comes at no cost, that isn’t true of the computing power required to run it. Scientists say that expense is going to prevent it from becoming the standard anytime soon, especially for large-scale projects.
But DeepVariant is just the front end of a much wider deployment; genomics is about to go deep learning. And once you go deep learning, you don’t go back.
It’s been nearly two decades since high-throughput sequencing escaped the labs and went commercial. Today, you can get your whole genome for just $1,000 (quite a steal compared to the $1.5 million it cost to sequence James Watson’s in 2008).
But the data produced by today’s machines still only produce incomplete, patchy, and glitch-riddled genomes. Errors can get introduced at each step of the process, and that makes it difficult for scientists to distinguish the natural mutations that make you you from random artifacts, especially in repetitive sections of a genome.
See, most modern sequencing technologies work by taking a sample of your DNA, chopping it up into millions of short snippets, and then using fluorescently-tagged nucleotides to produce reads—the list of As, Ts, Cs, and Gs that correspond to each snippet. Then those millions of reads have to be grouped into abutting sequences and aligned with a reference genome.
That’s the part that gives scientists so much trouble. Assembling those fragments into a usable approximation of the actual genome is still one of the biggest rate-limiting steps for genetics. A number of software programs exist to help put the jigsaw pieces together. FreeBayes, VarDict, Samtools, and the most well-used, GATK, depend on sophisticated statistical approaches to spot mutations and filter out errors. Each tool has strengths and weaknesses, and scientists often wind up having to use them in conjunction.
No one knows the limitations of the existing technology better than Mark DePristo and Ryan Poplin. They spent five years creating GATK from whole cloth. This was 2008: no tools, no bioinformatics formats, no standards. “We didn’t even know what we were trying to compute!” says DePristo. But they had a north star: an exciting paper that had just come out, written by a Silicon Valley celebrity named Jeff Dean. As one of Google’s earliest engineers, Dean had helped design and build the fundamental computing systems that underpin the tech titan’s vast online empire. DePristo and Poplin used some of those ideas to build GATK, which became the field’s gold standard.
But by 2013, the work had plateaued. “We tried almost every standard statistical approach under the sun, but we never found an effective way to move the needle,” says DePristo. “It was unclear after five years whether it was even possible to do better.” DePristo left to pursue a Google Ventures-backed start-up called SynapDx that was developing a blood test for autism. When that folded two years later, one of its board members, Andrew Conrad (of Google X, then Google Life Sciences, then Verily) convinced DePristo to join the Google/Alphabet fold. He was reunited with Poplin, who had joined up the month before.
And this time, Dean wasn’t just a citation; he was their boss.
As the head of Google Brain, Dean is the man behind the explosion of neural nets that now prop up all the ways you search and tweet and snap and shop. With his help, DePristo and Poplin wanted to see if they could teach one of these neural nets to piece together a genome more accurately than their baby, GATK.
The network wasted no time in making them feel obsolete. After training it on benchmark datasets of just seven human genomes, DeepVariant was able to accurately identify those single nucleotide swaps 99.9587 percent of the time. “It was shocking to see how fast the deep learning models outperformed our old tools,” says DePristo. Their team published the results on bioRxiv in December of 2016, and the next summer it went on to win a top performance award at the PrecisionFDA Truth Challenge.
DeepVariant works by transforming the task of variant calling—figuring out which base pairs actually belong to you and not to an error or other processing artifact—into an image classification problem. It takes layers of data and turns them into channels, like the colors on your television set. In the first working model they used three channels: The first was the actual bases, the second was a quality score defined by the sequencer the reads came off of, the third contained other metadata. By compressing all that data into an image file of sorts, and training the model on tens of millions of these multi-channel “images,” DeepVariant began to be able to figure out the likelihood that any given A or T or C or G either matched the reference genome completely, varied by one copy, or varied by both.
But they didn’t stop there. After the FDA contest they transitioned the model to TensorFlow, Google's artificial intelligence engine, and continued tweaking its parameters by changing the three compressed data channels into seven raw data channels. That allowed them to reduce the error rate by a further 50 percent. In an independent analysis conducted this week by genomics computing platform, DNAnexus, DeepVariant vastly outperformed GATK, Freebayes, and Samtools, sometimes reducing errors by as much as 10-fold.
“That shows that this technology really has an important future in the processing of bioinformatic data,” says DNAnexus CEO, Richard Daly. “But it’s only the opening chapter in a book that has 100 chapters.” Daly says he expects this kind of AI to one day actually find the mutations that cause disease. His company received a beta version of DeepVariant, and is now testing the current model with a limited number of its clients—including pharma firms, big health care providers, and medical diagnostic companies.
You Can Get Your Whole Genome Sequenced. But Should You?
Cheap DNA Sequencing Is Here. Writing DNA Is Next
Helix’s Bold Plan to Be Your One Stop Personal Genomics Shop
To run DeepVariant effectively for these customers, DNAnexus has had to invest in newer generation GPUs to support its platform. The same is true for Canadian competitor, DNAStack, which plans to offer two different versions of DeepVariant—one tuned for low cost and one tuned for speed. Google’s Cloud Platform already supports the tool, and the company is exploring using the TPUs (tensor processing units) that connect things like Google Search, Street View, and Translate to accelerate the genomics calculations as well.
DeepVariant’s code is open-source so anyone can run it, but to do so at scale will likely require paying for a cloud computing platform. And it’s this cost—computationally and in terms of actual dollars—that have researchers hedging on DeepVariant’s utility.
“It’s a promising first step, but it isn’t currently scalable to a very large number of samples because it’s just too computationally expensive,” says Daniel MacArthur, a Broad/Harvard human geneticist who has built one of the largest libraries of human DNA to date. For projects like his, which deal in tens of thousands of genomes, DeepVariant is just too costly. And, just like current statistical models, it can only work with the limited reads produced by today’s sequencers.
Still, he thinks deep learning is here to stay. “It’s just a matter of figuring out how to combine better quality data with better algorithms and eventually we’ll converge on something pretty close to perfect,” says MacArthur. But even then, it’ll still just be a list of letters. At least for the foreseeable future, we’ll still need talented humans to tell us what it all means.
Maybe you've heard of Crispr, the gene editing tool that could forever change life. So what is it and how does it work? Let us explain.


Your brain is one enigmatic hunk of meat—a wildly complex web of neurons numbering in the tens of billions. But years ago, when you were in the womb, it began as little more than a scattering of undifferentiated stem cells. A series of genetic signals transformed those blank slates into the wrinkly, three-pound mass between your ears. Scientists think the way your brain looks and functions can be traced back to those first molecular marching orders—but precisely when and where these genetic signals occur has been difficult to pin down.
Today, things are looking a little less mysterious. A team of researchers led by neuroscientists at UC San Francisco has spent the last five years compiling the first entries in what they hope will become an extensive atlas of gene expression in the developing human brain. The researchers describe the project in the latest issue of Science, and, with the help of researchers at UC Santa Cruz, they've made an interactive version of the atlas freely available online.
"The point of creating an atlas like this is to understand how we make a human brain," says study coauthor Aparna Bhaduri. To do that, she and her colleagues analyzed not only how gene expression varies from cell to cell, but where and at what stages of brain development those genes come into play.
Crucially, the researchers performed that analysis at the level of individual brain cells—a degree of specificity neuroscientists have struggled to achieve in the past. That's huge, in part because it gives researchers their clearest picture yet of where and in which cells certain genes are expressed in the fetal brain. But it also means scientists can begin to characterize early brain cells not according to things like their shape and location (two variables that neuroscientists have long used to classify cellular types and subtypes), but by the bits of DNA they turn on and off. As developmental neurobiologist Ed Lein, who was unaffiliated with the study, says: "This is not the first study in this area by any means, but the single cell technique is a game changer."
Lein would know. An investigator at the Allen Institute for Brain Science (a key institutional player in the mission to map the human brain, and the home of several ambitious brain atlas projects from the past decade), he and his colleagues performed a similar survey of gene expression in developing human brains in 2014. To build it, they sliced fetal brain tissue into tiny pieces and scanned them for gene expression. But even after dissecting them as finely as possible, Lein says the cell populations of the resulting brain bits were still extremely diverse. Even a microscopic speck of gray matter contains a menagerie of functionally distinct cells, from astrocytes to neurons to microglia (though, to be perfectly frank, neuroscientists aren't even sure how many cell types exist).
"When we measured the genes in our samples," says Lein, "what we actually saw was the average output of all the cells in that sample." When they were through, Lein and his colleagues had mapped the location and activity of some 20,000 genes in anatomical regions throughout the brain. But they still didn't know which individual cells those genes came from.
UCSF's new brain atlas doesn't span as many regions as the Allen Institute's (not yet, at least), but what anatomical areas it does covers it does with much greater specificity. "The difference between previous studies and ours is the difference between a smoothie and a fruit salad," says study coauthor Alex Pollen. "They have the same ingredients, but one mixes them together and the other looks at them individually."
The UCSF researchers focused on regions of the developing brain that eventually become the basal ganglia, which helps orchestrate things like voluntary motor control, and the cerebral cortex, the largest region of the mammalian brain and the seat of many human cognitive abilities. By examining the expression of individual cells from 48 brains at various stages of development, the researchers were able to trace a handful of genetic and developmental patterns to 11 broad categories of cell—and make a number of unexpected observations.
Scientists Create an Unprecedented Map of the Developing Human Brain
These Neurons are Alive and Firing. And You Can Watch Them In 3-D
A First Big Step Toward Mapping the Human Brain
"One big surprise is that region-specific neurons seem to form very early in the developmental process," says neurobiologist Tomasz Nowakowski, who led the study. That includes neurons in the prefrontal cortex, whose formation neuroscientists have long theorized to be influenced by sensory experience. But the new atlas suggests those areas begin to take shape before sensory experiences even have a chance to take place. That's the kind of finding that could fundamentally alter neuroscientists' understanding of the structure and function of adult brains.
The project's other takeaways are too numerous to list here. But that's the thing about brain atlases: They're great at generating questions. "These things are foundational," Lein says. "The reason these atlases are valuable is you can do a systematic analysis in one fell swoop and generate 10,000 hypotheses." Testing the hypotheses generated from this latest atlas will hinge on researchers' ability to access and add to it, which is why Nowakowski and his colleagues collaborated with UC Santa Cruz computer programmer Jim Kent to visualize their database in an interactive, online visualization.
Researchers will also want to cross-reference this atlas with similar projects. After all, there's more than one way to map a brain. You can classify its neurons by shape, location, or the genes they express. You can map the gene expression of old brains, young brains, and brains of different species. A recent project from the Allen Institute even classifies neurons according to their electrical activity. Brain atlases are like puzzle pieces that way: The more you have to work with, the easier it is to see the big picture—and how all the pieces fit.
The Connectome is a comprehensive diagram of all the neural connections existing in the brain. WIRED has challenged neuroscientist Bobby Kasthuri to explain this scientific concept to 5 different people; a 5 year-old, a 13 year-old, a college student, a neuroscience grad student and a connectome entrepreneur.


Humans dance. It's what they do. Everyone always wants to get some cool new dance move. First there was the electric slide. Yeah, that was cool—but then there was the moonwalk. That was really cool. And now we have the invisible box. OK, maybe it's not exactly a dance move, but more like a trick. The basic idea of this move is to make it seem like the dancer is stepping on a block—a block that's invisible. It's an impressive move, but how does it work?
https://twitter.com/arielo1220/status/936825731371241473
Let me start with an analysis of the motion. I will use a video from twitter along with video analysis techniques. Here is a plot of the vertical position of the girl's right foot (the one that steps on the "the box") during this move.
OK, she does a pretty nice job with that foot.  Notice that it has a fairly constant vertical position while she "steps" on the invisible block. In fact, the foot is stationary for almost 0.2 seconds. That might not seem like a long time, but it's long enough to appear awesome.
Since (SPOILER ALERT) there isn't really a box there, the girl is actually in the air for a little bit of time with only the gravitational force acting on her.  This means that she could be considered a type of projectile motion with a constant vertical acceleration of -9.8 m/s2.  But wait! If she is accelerating down, how does her foot stay stationary? The key here is that her center of mass accelerates down, but she can make a part of her (like her foot) stay stationary.
I could probably estimate her mass distribution to calculate her center of mass in each frame—but I will instead just plot the vertical motion of a few of her body parts (feet, arms, torso).  Here is the vertical position for all of these parts.
From this plot, you should notice a couple of things. First, the bottom foot actually doesn't move up or down that much compared to the rest of the body.  Second, the body seems to move in manner with a more constant acceleration—like you would expect for projectile motion. Do her arms have a significant impact on the move?  Maybe, it's difficult to tell.
But I'm not only here for the physics analysis. No, I'm here to help you pull this off. If you want to do this dance move, there are two things that you need to know—but remember, I'm a physicist and not a dancer. The higher you jump, the longer you can hold your foot in place. Since this jump is just like projectile motion, you want to jump as high as possible. However, the "hang time" for a jump is proportional to the square root of the height. For a half a second hang time, you would need to jump up 30 centimeters. That might not seem very high—but that is the height of your center of mass. It's tougher than it seems.
The other important thing to consider is the jumping force. In order to step on the invisible box, one of your legs doesn't really do anything except to "not move". This means that you have to jump over the invisible box using only one leg. That is not a very easy task. But I guess if it was easy, everyone would do it and it wouldn't be so cool.


The Thomas Fire spread through the hills above Ventura, in the northern greater Los Angeles megalopolis, with the speed of a hurricane. Driven by 50 mph Santa Ana winds—bone-dry katabatic air moving at freeway speeds out of the Mojave desert—the fire transformed overnight from a 5,000-acre burn in a charming chaparral-lined canyon to an inferno the size of Orlando, Florida, that only stopped spreading because it reached the Pacific. Tens of thousands of people evacuated their homes in Ventura; 150 buildings burned and thousands more along the hillside and into downtown are threatened.
That isn’t the only part of Southern California on fire. The hills above Valencia, where Interstate 5 drops down out of the hills into the city, are burning. Same for a hillside of the San Gabriel Mountains, overlooking the San Fernando Valley. And the same, too, near the Mount Wilson Observatory, and on a hillside overlooking Interstate 405—the flames in view of the Getty Center and destroying homes in the rich-people neighborhoods of Bel-Air and Holmby Hills.
And it’s all horribly normal.
Southern California’s transverse ranges—the mostly east-west mountains that slice up and define the greater Los Angeles region—were fire-prone long before there was a Los Angeles. They’re a broken fragment of tectonic plate, squeezed up out of the ground by the Pacific Plate on one side and the North American on the other, shaped into the San Gabriels, the Santa Monica Mountains, the San Bernardino Mountains. Even the Channel Islands off Ventura’s coast are the tippy-tops of a transverse range.
Santa Anas notwithstanding, the transverse ranges usually keep cool coastal air in and arid desert out. Famously, they’re part of why the great California writer Carey McWilliams called the region “an island on the land.” The hills provided hiding places for cowboy crooks, hiking for the naturalist John Muir, and passes both hidden and mapped for natives and explorers coming from the north and east.
With the growth and spread of Los Angeles, fire became even more part of Southern California life. “It’s almost textbook. It’s the end of the summer drought, there has not been a lot of rain this year, and we’ve got Santa Ana winds blowing,” says Alexandra Syphard, an ecologist at the Conservation Biology Institute. “Every single year, we have ideal conditions for the types of wildfires we’re experiencing. What we don’t have every single year is an ignition during a wind event. And we’ve had several.”
"The problem is not fire. The problem is people in the wrong places."
Alexandra Syphard, Conservation Biology Institute
Before humans, wildfires happened maybe once or twice a century, long enough for fire-adapted plant species like chapparal to build up a bank of seeds that could come back after a burn. Now, with fires more frequent, native plants can’t keep up. Exotic weeds take root. “A lot of Ventura County has burned way too frequently,” says Jon Keeley, a research ecologist with the US Geological Survey at the Sequoia and Kings Canyon Field Station. “We’ve lost a lot of our natural heritage.”
Fires don’t burn like this in Northern California. That’s one of the things that makes the island on the land an island. Most wildfires in the Sierra Nevadas and northern boreal forests are slower, smaller, and more easily put out, relative to the south. (The Napa and Sonoma fires this year were more like southern fires—wind-driven, outside the forests, and near or amid buildings.) Trees buffer the wind and burn less easily than undergrowth. Keeley says northern mountains and forests are “flammability-limited ecosystems,” where fires only get big if the climate allows it—higher temperatures and dryer conditions providing more fuel. Climate change makes fires there more frequent and more severe.
Southern California, on the other hand, is an “ignition-limited ecosystem.” It’s always a tinderbox. The canyons that cut through the transverse ranges align pretty well with the direction of the Santa Ana winds; they turn into funnels. “Whether or not you get a big fire event depends on whether humans ignite a fire,” he says.
And there are just a lot more humans in Southern California these days. In 1969 Ventura County’s population was 369,811. In 2016 it was 849,738—a faster gain than the state as a whole. In 1970 Los Angeles County had 7,032,000 people; in 2015 it was 9,827,000. “If you look historically at Southern California, the frequency of fire has risen along with population growth,” Keeley says. Though even that has a saturation point. The number of fires—though not necessarily their severity—started declining in the 1980s, maybe because of better fire fighting, and maybe because with more people and more buildings and roads and concrete, there’s less to burn.
As Syphard told me back at the beginning of this year’s fire season, “The problem is not fire. The problem is people in the wrong places.”
Like most fresh-faced young actors in Southern California, the idea of dense development is a relatively recent arrival. Most of the buildings on the island on the land are low, metastasizing in a stellate wave across the landscape, over the flats, up the canyons, and along the hillsides. In 1960 Santa Paula, where the Thomas Fire in Ventura started, was a little town where Santa Paula Canyon hit the Santa Clara River. Today it’s part of greater Ventura, stretching up the canyon, reaching past farms along the river toward Saticoy.
The Napa Fire Is a Perfectly Normal Apocalypse
In Cities, It's the Smoke, Not the Fire, That Will Get You
After the Napa Fires, a Disaster-in-Waiting: Toxic Ash
So the canyons are perfect places for fires. They’re at the Wildland-Urban Interface, developed but not too developed. Wall-to-wall hardscape leaves nothing to burn; no buildings at all means no people to provide an ignition source. But the hills of Ventura or Bel-Air? Firestarty.
As the transverse ranges defined Southern California before Los Angeles and during its spasmodic growth, today it’s defined by freeways. The mountains shape the roads—I-5 coming over the Grapevine through Tejon Pass in the Tehachapis, the 101 skirting the north side of the Santa Monica Mountains, and the 405 tucking through them via the Sepulveda Pass. The freeways, names spoken as a number with a "the" in front, frame time and space in SoCal. For an Angeleno like me, reports of fires closing the 101, the 210, and the 405 are code for the end of the world. Forget Carey McWilliams; that’s some Nathaniel West stuff right there—the burning of Los Angeles from Day of the Locust, the apocalypse that Hollywood always promises.
It won’t be the end end, of course. Southern California zoning and development are flirting, for now at least, with density, accommodating more people, dealing with the state’s broad crisis in housing, and incidentally minimizing the size of the wildland interface. No one can unbuild what makes the place an island on the land, but better building on the island might help stop the next fires before they can start.
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.


San Francisco, land of unrestrained tech wealth and the attendant hoodies and $29 loaves of bread, just said whoa whoa whoa to delivery robots.
The SF Board of Supervisors voted on Tuesday, December 5 to severely restrict the machines, which roll on sidewalks and autonomously dodge obstacles like dogs and buskers. Now startups will have to get permits to run their robots under strict guidelines in particular zones, typically industrial areas with low foot traffic. And even then, they may only do so for research purposes, not making actual deliveries. It’s perhaps the harshest crackdown on delivery robots in the United States—again, this in the city that gave the world an app that sends someone to your car to park it for you.
Actually, delivery robots are a bit like that, though far more advanced and less insufferable. Like self-driving cars, they see their world with a range of sensors, including lasers. Order food from a participating restaurant and a worker will load up your order into the robot and send it on its way. At the moment, a human handler will follow with a joystick, should something go awry. But these machines are actually pretty good at finding their way around. Once one gets to your place, you unlock it with a PIN, grab your food, and send the robot on its way.
Because an operator is following the robot at all times, you might consider the robot to be a fancied-up, slightly more autonomous version of a person pushing a shopping cart. “But that's not the business model that they're going after,” says San Francisco Supervisor Norman Yee, who spearheaded the legislation. “The business model is basically get as many robots out there to do deliveries and somebody in some office will monitor all these robots. So at that point you're inviting potential collisions with people.”
Unlike self-driving cars, or at least self-driving cars working properly, these bots roll on sidewalks, not streets. That gives them the advantage of not dealing with the high-speed chaos of roads, other than crossing intersections, but also means they have to deal with the cluttered chaos of sidewalks. Just think about how difficult it can be for you as a human to walk the city. Now imagine a very early technology trying to do it. (Requests for comment sent to three delivery robot companies—Dispatch, Marble, and Starship—were not immediately returned.)
San Francisco Tries to Ban Delivery Robots Before They Flatten Someone's Toes
The Crazy-Hard Technical Challenges of Robots Delivering Falafel
This Delivery Robot Isn’t Just Charming. It’s Stuffed With Pizza
What happened in that Board of Supervisors meeting was the manifestation of a new kind of anxiety toward the robots roaming among us. Just this last year has seen an explosion in robotics, as the machines escape the lab (thanks in part to cheaper, more powerful sensors) and begin rolling and walking in the real world. They've arrived quickly and with little warning.
And that’s made folks both curious and uneasy. Go to a mall and you may well find a security robot scooting around keeping an eye on things. Robot nurses roam the halls of hospitals. Autonomous drones fill the air. The question is: How are we supposed to interact with these machines? It’s a weird and fundamentally different kind of relationship than you’d form with a human, and not even experts in the field of human-robot interaction are sure how this is going to play out.
The big thing is safety. Machines are stronger than us and generally unfeeling (though that’s changing with robots that have a sense of touch), and can be very dangerous if not handled correctly. Which is what spooked Yee. San Francisco’s sidewalks are bustling with pedestrians and runners and homeless people and dogs and the occasional rat stacked on a cat stacked on a dog. How can the city make sure that roaming delivery robots and citizens get along?
For San Francisco, that means a crackdown. The legislation will require delivery robots to emit a warning noise for pedestrians and observe rights of way. They’ll also need headlights, and each permittee will need to furnish proof of insurance in the forms of general liability, automotive liability, and workers’ comp.
It’s sounds so very un-Silicon Valley. You know, move fast and break things, potentially literally in the case of the delivery robots. But states including Idaho and Virginia have actually welcomed delivery robots, working with one startup to legalize and regulate them early. Though really, San Francisco can better afford to put its foot down here—it’s not like it’s hurting for startups to come in and do business.
Might that seem like San Francisco isn’t as tech-friendly as it may seem? No, says Yee. “If you want to approach delivery, figure out how to do it and be as compatible with our values here,” he says. “Could robots do other things, for instance? Could it be that somebody's accompanying a robot that's picking up used needles in the street?”
If only Silicon Valley wasn't so busy developing parking apps.
Hungry? But you don't want to deal with a human? If you live in San Francisco's Mission district, you can get your food delivered by a robot named Marble.


This story originally appeared on Grist and is part of the Climate Desk collaboration.
If you’re like me, you’ve probably been ignoring the bitcoin phenomenon for years — because it seemed too complex, far-fetched, or maybe even too libertarian. But if you have any interest in a future where the world moves beyond fossil fuels, you and I should both start paying attention now.
Last week, the value of a single bitcoin broke the $10,000 barrier for the first time. Over the weekend, the price nearly hit $12,000. At the beginning of this year, it was less than $1,000.
If you had bought $100 in bitcoin back in 2011, your investment would be worth nearly $4 million today. All over the internet there are stories of people who treated their friends to lunch a few years ago and, as a novelty, paid with bitcoin. Those same people are now realizing that if they’d just paid in cash and held onto their digital currency, they’d now have enough money to buy a house.
That sort of precipitous rise is stunning, of course, but bitcoin wasn’t intended to be an investment instrument. Its creators envisioned it as a replacement for money itself—a decentralized, secure, anonymous method for transferring value between people.
But what they might not have accounted for is how much of an energy suck the computer network behind bitcoin could one day become. Simply put, bitcoin is slowing the effort to achieve a rapid transition away from fossil fuels. What’s more, this is just the beginning. Given its rapidly growing climate footprint, bitcoin is a malignant development, and it’s getting worse.
Cryptocurrencies like bitcoin provide a unique service: Financial transactions that don’t require governments to issue currency or banks to process payments. Writing in the Atlantic, Derek Thompson calls bitcoin an “ingenious and potentially transformative technology” that the entire economy could be built on — the currency equivalent of the internet. Some are even speculating that bitcoin could someday make the US dollar obsolete.
But the rise of bitcoin is also happening at a specific moment in history: Humanity is decades behind schedule on counteracting climate change, and every action in this era should be evaluated on its net impact on the climate. Increasingly, bitcoin is failing the test.
Digital financial transactions come with a real-world price: The tremendous growth of cryptocurrencies has created an exponential demand for computing power. As bitcoin grows, the math problems computers must solve to make more bitcoin (a process called “mining”) get more and more difficult—a wrinkle designed to control the currency’s supply.
Today, each bitcoin transaction requires the same amount of energy used to power nine homes in the US for one day. And miners are constantly installing more and faster computers. Already, the aggregate computing power of the bitcoin network is nearly 100,000 times larger than the world’s 500 fastest supercomputers combined.
The total energy use of this web of hardware is huge—an estimated 31 terawatt-hours per year. More than 150 individual countries in the world consume less energy annually. And that power-hungry network is currently increasing its energy use every day by about 450 gigawatt-hours, roughly the same amount of electricity the entire country of Haiti uses in a year.
‘I Forgot My PIN’: An Epic Tale of Losing $30,000 in Bitcoin
How to Keep Your Bitcoin Safe and Secure
Bitcoin Makes Even Smart People Feel Dumb
That sort of electricity use is pulling energy from grids all over the world, where it could be charging electric vehicles and powering homes, to bitcoin-mining farms. In Venezuela, where rampant hyperinflation and subsidized electricity has led to a boom in bitcoin mining, rogue operations are now occasionally causing blackouts across the country. The world’s largest bitcoin mines are in China, where they siphon energy from huge hydroelectric dams, some of the cheapest sources of carbon-free energy in the world. One enterprising Tesla owner even attempted to rig up a mining operation in his car, to make use of free electricity at a public charging station.
In just a few months from now, at bitcoin’s current growth rate, the electricity demanded by the cryptocurrency network will start to outstrip what’s available, requiring new energy-generating plants. And with the climate conscious racing to replace fossil fuel-base plants with renewable energy sources, new stress on the grid means more facilities using dirty technologies. By July 2019, the bitcoin network will require more electricity than the entire United States currently uses. By February 2020, it will use as much electricity as the entire world does today.
This is an unsustainable trajectory. It simply can’t continue.
There are already several efforts underway to reform how the bitcoin network processes transactions, with the hope that it’ll one day require less electricity to make new coins. But as with other technological advances like irrigation in agriculture and outdoor LED lighting, more efficient systems for mining bitcoin could have the effect of attracting thousands of new miners.
It’s certain that the increasing energy burden of bitcoin transactions will divert progress from electrifying the world and reducing global carbon emissions. In fact, I’d guess it probably already has. The only question at this point is: by how much?
The blockchain. Everyone's talking about it. But what is it, how does it work, and what's it for?


Go ahead, hit that BUY NOW button. Procure that sweater or TV or pillow that looks like a salmon fillet. Hit that button and fulfill the purpose of a hardworking warehouse robot.
Just know this: the more you rely on online shopping, the more online retailers rely on robots to deliver those products to you. Robots shuttle cabinets of goods around warehouses. Other robots scan barcodes to do inventory. And, increasingly, robotic arms do what once only humans could: Sort through a vast array of oddly-shaped objects to compile large orders, all to be shipped to you, dear consumer.
“To my mind, the big story in 2017 has been an inflection point in e-commerce,” says roboticist Ken Goldberg of UC Berkeley. “Companies like Amazon and others are now delivering products at an unprecedented rate, something like 500 packages per second. And that is only going to grow.”
And evolve. Working robots no longer just lift heavy objects or weld or do other large, brute-force tasks. The new breed of robot rolling through fulfillment centers like Amazon’s is more advanced, more nuanced—and more collaborative. And while automating parts of these processes makes order fulfillment cheaper for e-tailers (and, consequently, you), it’s also fueling a robotic renaissance that will have implications far beyond the warehouse.
When we think of factory robots, we think of the machines doing the exhausting bits—like rolling around fetching items—while the humans do what they do best: manipulation. This paradigm continues to exist. A human remains in charge of the crucial (and surprisingly complex) final step of actually filling boxes because nothing can beat the dexterity of the human hand. For now, at least. The machines are making rapid progress on that front.
That’s due in part to Amazon’s Picking Challenge, in which teams put their manipulative robots to work. This has helped bridge a divide between academia and industry. “Robotics for the longest time has been really just about research, and not about putting things in the real world because it was too hard,” says UC Berkeley roboticist Pieter Abbeel, whose new company Embodied Intelligence is on a quest to make industrial robots smarter. “And I think the Amazon Picking Challenge is kind of one of those things where people are saying, Wow, this is a real-world thing, a real need and we can do research on this.”
At a San Francisco startup called Kindred, for example, engineers are teaching robots to do that final step of fulfillment. Using a technique called imitation learning, engineers steer the robot to show how best to grasp a wide range of objects you’d find at a marketplace like Amazon. “Some are soft and squishy, some are hard, some are heavy, some are soft,” says George Babu, co-founder of Kindred. “And there's no way you can program that.”
Then a second technique, known as reinforcement learning, kicks in. The robot takes what it’s learned and through trial and error further hones it, both for speed and accuracy. Theoretically this would not only supercharge the fulfillment process, but make it more flexible. For instance if you’re a clothing retailer and winter rolls around, you’ll need to teach the robot to handle bulkier items like coats. (Kindred is running a pilot program at the Gap.) Why write out a bunch of complicated new code when you show the robot how to adapt?
But even in a relatively structured environment like a fulfillment center, the machines face plenty of obstacles. Some of them literal, like the humans they’re working with.
The need for increased collaboration between human and robot is forcing companies to look closely at how they integrate autonomous machines into the workforce. For Amazon and its 100,000 working robots, that has meant doing something very human: listening. Workers were a vocal part of the onboarding process. “Our associates actually got as granular as giving feedback on the fabric of the shelf and the color of the pods,” says Amazon spokesperson Nina Lindsey. “And that design has actually made it more efficient for our associates to find items.”
Which to a cynic might sound like workers willingly hastening the demise of their jobs. But in the near term, that’s not what’s going on here. Amazon has ramped up its hiring of humans right alongside its hiring of robots. And there is very much a place for humans as there is a place for robots. “Technology is extremely good at performing tasks that people do, but jobs are more than tasks,” says David Schatsky, managing director at Deloitte and co-author of a new report on robots in the workplace. “So jobs will change, but I don't see a wholesale elimination of lots and lots of job categories.”
Still, the automation of jobs is nothing new. Consider that at the end of the 1700s in America, 90 percent of workers toiled in agriculture. Fast forward to 2012, and that number is 1.5 percent. Warehouse work is fundamentally different, but it's not hard to see a time in the near future where increasingly sophisticated robots stop being collaborative and start replacing humans on the line. Whether that means those humans shift into more creative work, or they end up supervising the machines, will depend on the job.
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
So here we have the convergence of several factors that have kicked off a robotic renaissance. For one, the sensors that allow the robots to navigate a chaotic environment have gotten way cheaper at the same time as they’ve gotten way more powerful. Two, AI has vastly improved. And three, there’s money to be made—lots of it. E-commerce just keeps growing and growing, perhaps hitting $600 billion a year by 2020 in the US alone.
Which is not to say the e-commercers get to have all the fun. Expect the technologies developed for order fulfillment to spill out into the real world. The companion robots that have begun invading our homes will navigate better and better, taking a cue from their warehouse comrades. The machines will get all the smarter and easier for regular folk to teach, perhaps thanks to companies like Embodied Intelligence and Kindred. And that elusive dream of robotics, getting the machines to recognize and grip and manipulate a wide range of objects, could well come about because it’ll make someone in e-commerce a lot of money.
So go ahead, hit that BUY NOW button. The machines (and capitalists) thank you.
Robot co-workers and artificial intelligence assistants are becoming more common in the workplace. Could they edge human employees out? What then?


At HRL Laboratories in Malibu, California, materials scientist Hunter Martin and his team load a grey powder as fine as confectioner’s sugar into a machine. They’ve curated the powder recipe—mostly aluminum, blended with some other elements—down to the atom. The machine, a 3-D metal printer, lays the powder down a single dusting at time, while a laser overhead welds the layers together. Over several hours, the machine prints a small block the size of brownie.
HRL’s parent companies, Boeing and General Motors, want to 3-D print intricate metal parts in mass for their sleek new generation of cars and planes. Airbus has already installed the first-ever 3-D printed metal part on a commercial airplane, a bracket that attaches to its wings. But the tech is limited by the quality of today's metal powders, says Martin. Most useful alloys aren’t printable because the atoms in the powder grains don’t stack correctly—leading to a weak, brittle weld.
So Martin’s group, which largely works at Boeing and GM’s forward-thinking HRL’s Sensors and Materials Laboratory, figured out how to alter the recipe of a strong alloy so it was compatible with a 3-D printer. Their secret weapon: a machine learning software made by Bay Area-based company, Citrine Informatics. It turns out, algorithms can learn enough chemistry to figure out what materials Boeing should use in their next airplane body.
Martin's test block took more than 2 years of work. Scanning through the periodic table, his team came up with 10 million possible recipes for improving the powder. Then, they had to figure out which ones to try to make—using Citrine’s machine learning algorithms.
When companies upgrade their products—the next Prius, smartphone, or raincoat—they first consider how to upgrade the materials they’re made of. They could be improving quality, like making a harder glass for the iPhone, or figuring out how to make a cheaper battery. “Everything has to start with, what are we going to make it out of?” says materials scientist Liz Holm of Carnegie Mellon University, who has collaborated with Citrine in the past.
But historically, this process takes forever. If you were trying to make a more efficient LED, you’d use your years of materials science experience to pick an initial semiconductor recipe, and then you’d tweak it ad nauseum for years, until the material fit all your criteria. “You know the scientific method,” says Greg Mulholland, the CEO of Citrine. “You come up with a hypothesis; you test it; you conclude something. And you start over.”
So in 2013, when Mulholland was still in business school, he and Citrine co-founders Bryce Meredig and Kyle Michel thought they could speed up that process. A crucial step is to pick the first recipe in the right ballpark, which usually takes the touch of an experienced researcher who has worked with similar materials for years. But instead of relying on one scientist’s limited experience, why not ask an algorithm fed with decades of experimental data?
To create these algorithms, they had to trawl for the data from those decades of experiments. They wrote software to scan and convert the data printed in heavy reference books from another era. They fed their algorithms the results of supercomputer simulations of exotic crystals. They built a friendly user interface, where a researcher can select from drop-down menus and toggle buttons to describe the type of material they want. Other than HRL, the Citrine team has partnered with clients such as Panasonic, Darpa, and various national labs in the last four years.
But even still, materials science projects suffer from a lack of data. “We have to do some creative things to really make the most of the data available,” says Mulholland.  Unlike, say, the algorithms underpinning Google Translate, which are trained with millions of words, you might only have a thousand data points or fewer for a class of materials. Some companies want to work with materials only discovered a few years ago. To give the algorithms more to work with, Mulholland’s team teaches the algorithms general rules about physics and chemistry.
Sometimes they even have to resort to handwritten data. “There are times when we have to scan papers and notebooks from our customers, which is truly awful,” says Mulholland. “The norm is close to what my lab notebooks used to look like. It’s a series of hard-to-read notes, interspersed with chemicals dripped onto pages.”
A Freaky Anti-Rubber Is Still Weirding Scientists Out
Physicists Teach AI to Identify Exotic States of Matter
A Material From Shapeshifting Planes Could Heal Human Flesh
Luckily, they didn’t have to go that far with Martin’s group. Martin found out about Citrine when Meredig, Citrine’s chief science officer, gave a talk at his graduate school. They figured out that Citrine could predict what atoms to add to their alloy to improve weldability. For example, the algorithm could ballpark the optimal size of the atoms and and the type of chemical bonds they’d need to form. The software helped Martin’s team rule out most of the 10 million proposed recipes to a manageable 100. Conventionally, this process would have taken place in the lab over iterations of experiments. “What would’ve taken years, it narrowed it down to days,” Martin says.
Using those new powder formulations, they printed several prototype blocks and tested their strength. When they examined the blocks under microscopes and pulled them with thousands of pounds of force, they passed the test.
But as smart as the Citrine software is, it’s not going to replace human expertise, says William Paul King of the University of Illinois at Urbana-Champaign, who was not involved in the research. Martin’s team couldn’t just tell the software, “Fix this unweldable powder!” They had to tell the algorithm explicitly what chemical properties they were looking for. “It required significant expertise from them,” says King.
Instead, it makes it possible for materials scientists to use more of the institutional knowledge they’ve built for decades. “It shouldn’t take 100 years to have really advanced answers to a lot of these materials science questions,” says Mulholland. “It should take five to 10 years. Or shorter than that in some cases.” In answering Martin’s 3-D printing question—Citrine knocked that down to days.
It’s tough work for pairs of humans who install each of the 60,000 rivets that keep Boeing's 777 from falling apart in midair, so they’re getting some help from pairs of robots on the plane production line.


It took a while, but Russia finally got body-checked out of the Olympic Games. The road to ruin began in 2015, when two Russian track athletes-turned-whistleblowers raised suspicion about widespread state-sponsored doping at the 2012 London Games, followed by an independent report about problems at the 2014 Sochi Winter Olympics. Now, the International Olympic Committee has slammed the door on Russia's Olympic dreams, accusing the country of running a state-sponsored program involving more than a thousand athletes since 2011. The Russian team and all of its sports officials were banned from the upcoming winter games in PyeongChang, South Korea, in February, although individual Russian athletes who prove they are clean could compete under a neutral flag.
IOC president Thomas Bach announced the ban at a press conference in Lausanne, Switzerland Tuesday, citing a 17-month investigative report by Samuel Schmid, Switzerland’s former president. “The report clearly lays out an unprecedented attack on the integrity of the Olympic games and sport,” Bach said. “As an athlete myself, I’m feeling very sorry for all the clean athletes who have suffered from this manipulation.”
Despite the earlier warnings of Russian foul play, Bach said Tuesday the IOC didn’t have all the information needed back then to make its decision. The Schmid report detailed the structure of the Russian sports bureaucracy and how it is intertwined with the Russian government. It also gave interesting details about how Russian intelligence agents were able to unlock tamper-proof urine sample bottles—using a dental instrument and a lot of hard work.
Swiss forensic investigators spent two months to unlock the secrets of the supposedly impregnable BEREG-KIT bottles. These Swiss-made bottles are considered tight after five clicks of the sealing ring, with a maximum closure of 15 clicks. But by using a long, thin pointy metal instrument, the investigators were able to jimmy open the seal by carefully inserting it into the plastic ring and pushing it up. The process left tiny scratches on the inside of the bottle—though they were only visible under a microscope. That's how they were able to identify tampering in the Sochi samples.
More than a quarter of the Russian urine samples were likely tainted or swapped out with clean urine collected from the same athletes months earlier. The report also found that suspect Russian urine samples contained high levels of salt, several times higher than found in the human body, which was used to reconstitute the urine.
The Russians didn’t invent any new performance enhancing drugs for Sochi. “They just bought them from the pharmacy,” says Mark Johnson, a San Diego-based author who has written about doping in sports. “It shows that where when you take the resources of a government, both the scientific and financial resources and their research resources and apply it to a problem, they can find a solution,” Johnson says. “If it is one athlete trying to pry open a bottle, you can’t do it.”
The culture of doping in Russia and the Soviet Union goes back to the 1960s, when success in sports brought glory to the nation. Between 1968 and 2017, Russian athletes were stripped of 50 Olympic medals—including one third of their 33 won at the Sochi games.
The IOC’s Bach said that international athletes that finished behind the Russians who doped will have a special ceremony in South Korea. “We will do our best to reorganize ceremonies in PyeongChang in 2018 to try to made up for the moment they missed on the finish line or the podium,” Bach said. “The IOC will propose or will be taking measures for a more robust anti-doping system under [the World Anti-Doping Agency] so that something like this can not happen again.”
Olympic Drug Cops Will Scan for Genetically Modified Athletes
It’ll Be Really Hard to Test for Doping at the Rio Olympics
What Would Happen if the Olympics Banned Russia?
But Johnson and other critics are skeptical that the IOC’s Russia ban or a new testing system instituted will make a difference in future Olympic games. He notes that record-breaking athletic performance—regardless of whether it is the result of drugs, or perhaps soon through gene-editing techniques—is part of what draws TV viewers, advertisers, and national prestige.
“The objective of pro sports is to entertain and push the boundaries of performance, it’s not to be a moralistic teacher or imposer of values,” says Johnson. Of course, Olympic officials beg to differ. They point to the stated "Olympism" values of fair play, ethics. and hard work.
But sports and national prestige will always go together, and Russians have long since decided doping is worth the risk. Vitaly Murko, Russia’s former minister of sport, was implicated in having a direct role in the doping program by the Schmid report and an earlier 2015 investigation by WADA. On Tuesday, the IOC gave him a lifetime ban. But even though Murko can’t go to the Olympics, he won't be leaving sports behind. Now the country’s deputy prime minister, he is also president of the Russian football union—and next summer, he will be an official host of Russia’s World Cup soccer tournament.
This new technology is changing the Olympic Games.


This kid doesn’t know it, but he’s kind of a big deal. Sitting in his mother’s lap, he looks at a mohawked robotic head, which periodically turns left to look at a computer screen with its big blue eyes. And the infant takes the cue, glancing at the screen, where a human avatar signs a nursery rhyme.
This boy is doing something remarkable on two levels. For one, he’s practicing a pivotal skill for his development—language—with a clever new platform that blends robotics, fancy algorithms, and brain science. And he’s doing what few humans have done before: communicating with a robot using facial cues alone.
In an ideal world, every child would get enough face-to-face communication during early development to build solid language skills, be that by way of sign language or the spoken word. The reality is, not all parents have the time to sit down and read to their kids. And for deaf children, it may be that the parents themselves have to learn to sign.
What researchers at Gallaudet University—in collaboration with Yale, the University of Southern California, and Italy’s University of D’Annunzio—have developed isn’t a substitute for interpersonal communication between parents and infants, but an experimental supplement. It’s meant to simulate the natural interaction between baby and mother or father.
What’s interesting about the developing infant mind is that natural language, no matter if it’s spoken or signed, stimulates the same areas of the brain. “The same neural sensitivities, they are processed in the identical swatches of brain tissue,” says Gallaudet neuroscientist Laura-Ann Petitto. “The brain tissue that we used to think was only responsible for sound is not the unique bastion of sound processing. It's the unique bastion of human language processing.”
With this knowledge in hand, the team can strap little brain-scanning hats to deaf infants and watch for these areas to light up. Now they know when the child truly engages in natural language. (In the world outside Look Who’s Talking, a baby can’t tell you it’s interested in what you’re saying or signing.)
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
But the team’s robot-avatar system uses a more subtle method to read the infant. A thermal camera trained on the baby’s face watches for tiny changes in temperature, which are associated with heightened awareness. Combined with face-tracking software, this can determine not just when the robot is able to direct the kid’s gaze to the avatar, but when the kid is actually engaged. And infants seem to love it—even hearing children will try to sign back to the avatar.
But why go through all the trouble of face-tracking and algorithms and doing motion-capture to build the avatar? Because interaction, even if it’s with a robot masquerading as a human, is essential to language development.
Sure, you could plop a kid in front of Sesame Street, which tries its best to engage with children, but the medium inevitably comes up short. “It's not the tablet itself, it's not the computer itself or the TV itself, it's the way it's used,” says Diane Paul, director of clinical issues in speech-language pathology at the American Speech-Language-Hearing Association. “We actually want families, caregivers to be reading to their children, speaking to their children, signing, singing. We want that social interaction because it's within that context that you learn speech and language or signing skills.”
Without enough of that kind of interaction, a child’s brain doesn’t develop as it should. Now, a robot can’t replace a mother or father, but it isn’t meant to. One day it might work as a stand-in, grabbing a baby’s attention when the parents are otherwise occupied, to give the child that extra bit of language practice.
Beyond the implications for child development, the system is fascinating from a robotics perspective. Robots are notoriously bad at reading our emotions and making their own. The subtlety of human facial expressions is just too impenetrable, and robots (short of showing an animated face on a screen) have a hard time even smiling or frowning. Yet this robot, using its eyes, is able to grab the gaze of an infant and direct it to the avatar. Robot and human are communicating in a simple yet compelling way.
So sure, one day children may have sophisticated robot babysitters with sophisticated emotions and interactions. But for now, a little mohawked robot is catching the eye of a kid or two.
Mozzeria is a 100 percent deaf owned and operated pizzeria in a crowded San Francisco restaurant scene. So to stay afloat, it relies on a remarkable technology.


In the last five years, biology has undergone a seismic shift as researchers around the globe have embraced a revolutionary technology called gene editing. It involves the precise cutting and pasting of DNA by specialized proteins—inspired by nature, engineered by researchers. These proteins come in three varieties, all known by their somewhat clumsy acronyms: ZFNs, TALENs, and CRISPRs. But it’s Crispr, with its elegant design and simple cell delivery, that’s most captured the imagination of scientists. They’re now using it to treat genetic diseases, grow climate-resilient crops, and develop designer materials, foods, and drugs.
So how does it work?
When people refer to Crispr, they're probably talking about Crispr-Cas9, a complex of enzymes and genetic guides that together finds and edits DNA. But Crispr on its own just stands for Clustered Regularly Interspaced Palindromic Repeats—chunks of regularly recurring bits of DNA that arose as an ancient bacterial defense system against viral invasions.
Viruses work by taking over a cell, using its machinery to replicate until it bursts. So certain bacteria evolved a way to fight back. They deployed waves of DNA-cutting proteins to chop up any viral genes floating around. If the bacteria survived the attacks, they'd incorporate tiny snippets of virus DNA into their own genomes—like a mug shot of every foe they’d ever come across, so they could spot each one quicker in the future. To keep their genetic memory palace in order, they spaced out each bit of viral code (so-called “guide RNAs”) with those repetitive, palindromic sequences in between. It doesn't really matter that they read the same forward and backward; the important thing is that they helped file away genetic code from viral invaders past, far away from more essential genes.
And having them on file meant that the next time a virus returned, the bacteria could send out a more powerful weapon. They could equip Cas9—a lumpy, clam-shaped DNA-cutting protein—with a copy of that guide RNA, pulled straight out of storage. Like a molecular assassin, it would go out and snip anything that matched the genetic mug shot.
That’s what happens in the wild. But in the lab, scientists have harnessed this powerful Crispr system to do things other than fight off the flu. The first step is designing a guide RNA that can sniff out a particular block of code in any living cell—say, a genetic defect, or an undesirable plant trait. If that gene consists of a string of the bases A, A, T, G, C, scientists make a complementary strand of RNA: U, U, A, C, G. Then they inject this short sequence of RNA, along with Cas9, into the cell they’re trying to edit. The guide RNA forms a complex with Cas9; one end of the RNA forms a hairpin curve that keeps it stuck in the protein, while the other end—the business end—dangles out to interact with any DNA it comes across.
Scientists Crispr the First Human Embryos in the US (Maybe)
Crispr Is Getting Better. Now It's Time to Ask the Hard Ethical Questions
Scientists Upload a Galloping Horse GIF Into Bacteria With Crispr
Once in the cell's nucleus, the Crispr-Cas9 complex bumps along the genome, attaching every time it comes across a small sequence called PAM. This “protospacer adjacent motif” is just a few base pairs, but Cas9 needs it to grab onto the DNA. And by grabbing it, the protein is able to destabilize the adjacent sequence, unzipping just a little bit of the double helix. That allows the guide RNA to slip in and sniff around to see if it's a match. If not, they move on. But if every base pair lines up to the target sequence, the guide RNA triggers Cas9 to produce two pincer-like appendages, which cut the DNA in two.
The process can stop there, and simply take a gene out of commission. Or, scientists can add a bit of replacement DNA—to repair a gene instead of knocking it out.
And they don't have to limit themselves to just Cas9. There's a whole bunch of proteins that can use an RNA guide. There's Cas3, which gobbles up DNA Pac-Man style. Scientists are using it to develop targeted antibiotics that can wipe out a strain of C. diff, while leaving your gut microbiome intact. And there's an enzyme called Cas13 that works with a guide that gloms onto RNA, not DNA. Called Sherlock, the system is being used to develop sensitive tests for viral infections. Researchers are working hard to add more implements to the Crispr toolkit, but at least right now, Cas9 is still the most widely used.
Crispr isn’t perfect; sometimes the protein veers off course and makes cuts at unintended sites. So scientists are actively working on ways to minimize these off-target effects. And as it gets better, the ethical questions surrounding the technology are going to get a lot thornier. Hello, designer babies?! Figuring out where those lines get drawn is going to take more than science; it will require policymakers and the public coming to the table. Because pretty soon with Crispr, the question won’t be can we do it, but should we?
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.


During Sunday's NFL game between the Seattle Seahawks and the Philadelphia Eagles, Seahawks quarterback Russell Wilson makes a pretty nice move. After taking the ball and running with it, he makes a quick pitch off to the side while faking out the defender. Looks pretty cool, but it probably wasn't legal. In the NFL, once you cross the line of scrimmage you can only toss the ball backward. This should have been a penalty for an illegal forward pass, but it looks like he got away with it.
https://twitter.com/NFL/status/937533538013868032
But was it really a forward pass? Yes. It's not so easy to tell if you watch this in real time, but going frame by frame it's much clearer. Wilson tosses the ball when he is at the 46 and a half yard line. The running back (Mike Davis) catches it at 47.5 yard line. That's a pass that goes 1 yard forward—so a forward pass.
Just to further drive home the point, I can use my favorite video analysis techniques (using Tracker Video Analysis) to get the position of both Wilson and the ball as he runs down the field. Check this out.
In this plot, the red data shows the position of Russell Wilson in the direction of the field (x-direction) and the blue data is the position of the ball. Just for fun, I can also fit a linear function to the data (for straight parts) to get the velocity in that direction. During the toss, Wilson is running at 4.55 m/s (10.2 mph) while the ball has an x-velocity of 1.27 m/s. But since both the ball has a positive x-velocity, it is indeed a forward pass. Illegal.
It's OK if you thought it was a backward pass. It's also OK if Russell Wilson thought it was a backward pass—because in the reference frame of the quarterback, it is a backward pass. Let me give a simple analogy.
Suppose you are in a car driving 30 mph along a road.  While it's driving, you take a football and throw it backward from car with a speed of 10 mph. From your viewpoint (in the car moving 30 mph), it looks like the ball is traveling in the backward direction at 10 mph. However, if you were on the ground (and stationary) looking at the car, the ball would still be moving forward with a speed of 20 mph (30 mph – 10 mph = 20 mph).
This is exactly what's going on here.  We (as humans) tend to view the ball with respect to the moving thrower. On top of this, the camera pans with the motion of the runner. This also encourages us to view from the reference frame of the runner. So, in a sense we are tricked into thinking it's a backward pass.
Maybe this will help. Here is an animation with a stationary world view such is how it would appear if the camera didn't move.
From this view, it's much clearer that the ball moves forward. Velocity is always relative to your frame of reference. But of course, it's only a penalty if the referees decide to flag it.
A physics professor re-imagines the size and shape of an NFL football in an effort to optimize it for flight. It turns out a couple of small changes can add a whole lot of distance, but how would that change the way the game is played?


On July 1, 2013, Amos Joseph Wells III went to his pregnant girlfriend's home in Fort Worth, Texas and shot her multiple times in the head and stomach. He then killed her mother and her 10-year-old brother. Wells surrendered voluntarily within hours, and in a tearful jailhouse interview told reporters, "There's no explanation that I could give anyone, or anybody could give anyone, to try to make it seem right, or make it seem rational, to make everybody understand."
Heinous crimes tend to defy comprehension, but some researchers believe neuroscience and genetics could help explain why certain people commit such atrocities. Meanwhile, lawyers are introducing so-called neurobiological evidence into court more than ever.
Take Wells, for instance. His lawyers called on Pietro Pietrini—director of the IMT School for Advanced Studies in Lucca, Italy and an expert on the neurobiological correlates of antisocial behavior—to testify at their client's trial last year. “Wells had several abnormalities in the frontal regions of his brain, plus a very bad genetic profile," says Pietrini. Scans of the defendant's brain showed abnormally low neuronal activity in his frontal lobe, a condition associated with increased risk of reactive, aggressive, and violent behavior. In Pietrini's estimation, that "bad genetic profile" consisted of low MAOA gene activity—a trait long associated with aggression in people raised in abusive environments—and five other notable genetic variations. To differing degrees, they're linked with a susceptibility to violent behavior, impulsivity, risk-taking, and impaired decision-making.
"What we tried to sustain was that he had some evidence of a neurobiological impairment that would affect his brain function, decision making, and impulse control," Pietrini says. "And this, we hoped, would spare him from the death penalty."
It did not. On November 3, 2016, a Tarrant County jury found Wells guilty of capital murder. Two weeks later, the same jury deliberated Wells' fate for just four hours before sentencing him to die. The decision, as mandated by Texas law, was unanimous.
In front of a different judge or another jury, Wells might have avoided the death penalty. In 2010, lawyers used a brain-mapping technology called quantitative electroencephalography to try to convince a Dade City, Florida jury that defendant Grady Nelson was predisposed to impulsiveness and violence when he stabbed his wife 61 times before raping and stabbing her 11-year-old daughter. The evidence's sway over at least two jurors locked the jury in a 6-6 split over whether Nelson should be executed, resulting in a recommendation of life without parole.
Nelson's was one of nearly 1,600 court cases examined in a recent analysis of neurobiological evidence in the US criminal justice system. The study, by Duke University bioethicist Nita Farahany, found that the number of judicial opinions mentioning neuroscience or behavioral genetics more than doubled between 2005 and 2012, and that roughly 25 percent of death penalty trials employ neurobiological data in pursuit of a lighter sentence.
Farahany's findings also suggest defense attorneys are applying neuroscientific findings to more than capital murder cases; lawyers are increasingly introducing neuroscientific evidence in cases ranging from burglary and robbery to kidnapping and rape.
"Neuro cases without a doubt are increasing, and they're likely to continue increasing over time" says Farahany, who adds that people appear to be particularly enamored of brain-based explanations. "It’s a much simpler sell to jurors. They seem to believe that it’s much more individualized than population genetics. Also, they can see it, right? You can show somebody a brain scan and say: There. See that? That big thing, in this person’s brain? You don’t have that. I don’t have that. And it affects how this person behaves.”
And courts seem to be buying it. Farahany found that between 20 and 30 percent of defendants who invoke neuroscientific evidence get some kind of break on appeal—a higher success rate than one sees in criminal appeals, in general. (A 2010 analysis of nearly 70,000 US criminal appeals found that only about 12 percent of cases wound up being reversed, remanded, or modified.) At least in the instances Farahany investigated (a small sample, she notes, of criminal cases, 90 percent of which never go to trial), neurobiological evidence seemed to have a small but positive impact on defendants' outcomes.
The looming question—scientifically, legally, philosophically—is whether it should.
Many scientists and legal experts question whether neurobiological evidence belongs in court in the first place. "Most of the time, the science isn’t strong enough," says Stephen Morse, professor of law and psychiatry at the University of Pennsylvania.
Morse calls this the "clear cut" problem: Where the defendant's mental and behavioral state are obvious, you don’t need neurobiological evidence to support it. But in cases where the behavioral evidence is unclear, the brain data or genetic data aren't exact enough to serve as diagnostic markers. "So where we need the help most—where it’s a gray area case, and we’re simply not sure whether the behavioral impairment is sufficient—the scientific data can help us least," says Morse. "Maybe this will change over time, but that’s where we are now.”
You don't have to look hard to see his point. To date, no brain abnormality or genetic variation has been shown to have a deterministic effect on a person's behavior, and it's reasonable to assume that one never will. Medicine, after all, is not physics; your neurobiological state cannot predict that you will engage in violent, criminal, or otherwise antisocial activity, as any researcher will tell you.
But some scientific arguments appear to be more persuasive than others. Brain scans, for example, seem to hold greater sway over the legal system than behavioral genetic analyses. "Most of the evidence right now suggests that genetic evidence, alone, isn’t having much influence on judges and juries," says Columbia psychiatrist Paul Appelbaum, co-author of a recent review, published in Nature Human Behavior, that examines the use of such evidence in criminal court. Juries, he says, might not understand the technical intricacies of genetic evidence. Conversely, juries may simply believe genetic predispositions are irrelevant in determining someone's guilt or punishment.
Still another explanation could be what legal researchers call the double-edged sword phenomenon. "The genetic evidence might indicate a reduced degree of responsibility for my behavior, because I have a genetic variant that you don’t, but at the same time suggest that I'm more dangerous than you are. That if I really can't control my behavior, maybe I'm exactly the kind of person who should be locked up for a longer period of time," Appelbaum says. Whatever the reason for genetic evidence's weak impact, Appelbaum predicts its use in court—absent complementary neurological evidence—will decrease.
That's not necessarily a bad thing. There's considerable disagreement within the scientific community over the influence of so-called gene-environment interactions on human behavior, including ones believed to affect people like Amos Wells.
In their 2014 meta-analysis of the two most commonly studied genetic variants linked to aggression and antisocial behavior (both of which Wells possesses), Emory University psychologists Courtney Ficks and Irwin Waldman concluded that the variants appear to play a "modest" role in antisocial behavior. But they also identified numerous examples of studies bedeviled by methodological and interpretive flaws, susceptibility to error, loose standards for replication, and evidence of publication bias. "Notwithstanding the excitement that many researchers have felt at the prospect of [gene-environment] interactions in the development of complex traits, there is growing evidence that we must be wary of these findings," the researchers wrote.
So then. What should a jury consider in the case of someone like Amos Wells? In his expert report, Pietrini cited Ficks and Waldman's analysis—and more than 80 other papers—to emphasize the modest role of genetic variation in antisocial behavior. And in their cross examination, the prosecution went through several of Pietrini's citations line by line, calling for circumspection. They pointed to the Ficks paper, for instance. They also quoted excerpts that cast behavioral genetics findings in an uncertain light. Lines like this one, from a 2003 paper in Nature about the association of gene variants with anger-related traits: "Nevertheless, our findings warrant further replication to avoid any spurious associations for the example due to the ethnic stratification effects and sampling errors."
Pietrini chuckles when I recount the prosecution's criticisms. "You look at the discussion section of any medical study, and you'll find sentences like that: Needs more research. Needs a larger sample size. Needs to be replicated. Warrants caution. But it doesn't mean that what's been observed is wrong. It means that, as scientists, we're always cautious. Medical science is only ever proven true by history, but Amos Wells, from my point of view, had many genetic and neurological factors that impaired his mental ability. I say that not because I was a consultant to the defense, but in absolute terms."
To Protect Genetic Privacy, Encrypt Your DNA
Biohackers Encoded Malware in a Strand of DNA
Cheap DNA Sequencing Is Here. Writing DNA Is Next
Pietrini's point gets to the heart of a question still tackled by researchers and legal scholars: When do scientific findings become worthy of legal consideration?
The general assumption is that the same standards that guide the scientific community should guide the law, says Drexel University legal professor Adam Benforado, author of Unfair: The New Science of Criminal Injustice. "But I think that probably shouldn't be the case," he says. "I think when someone is facing the death penalty, they ought to have a right to present neuroscientific or genetic research findings that may not be entirely settled but are sound enough to be published in peer reviewed literature. Because at the end of the day, when someone's life is at stake, to wait for things to be absolutely settled is dangerous. The consequences of inaction are too grave."
That's basically the Supreme Court's stance, too. In the US, the bar for admissibility on mitigating evidence in death penalty proceedings is very low, owing to a Supreme Court ruling in the 1978 trial of Lockett against Ohio. "Essentially, the kitchen sink comes in. And in very few death penalty proceedings will the judge make a searching inquiry into relevance," says Morse, who begrudgingly agrees that neurobiological evidence should be admissible in capital cases, because so much is at stake. "I'd rather it wasn't, because I think it debases the legal process," he says, adding that most neuroscientific and genetic evidence introduced at capital proceedings has more rhetorical relevance than legal relevance.
"What they’re doing is making what I call the fundamental psycho-legal error. This is the belief that once you have found a partially causal explanation for a behavior, then the behavior must be excused altogether. All behavior has causes, including causes at the biological, psychological, and sociological level. But causation is not an excusing condition." If it were, Morse says, no one would be responsible for any behavior.
But that is not the world we live in. Today, in most cases, the law holds people responsible for their actions, not their predispositions. As Wells told his relatives in the courtroom after his sentence was handed down: "I did this. I'm an adult. Don't bear this burden. This burden is mine."
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.


Maybe you have watched Stranger Things but maybe you haven't. I've seen it, and I thought it was great—and not just because there's lots of science in it. Don't worry, I'm not going to talk about multiple universes or quantum tunneling. Instead I am going to talk about salt.
Small spoiler alert (but not really a spoiler): In season 1, the Stranger Things kids need to build a makeshift sensory deprivation tank. The essential component of this "tank" is a kiddie pool filled with water such that a person can easily float. Of course, normal water will make a human just barely float. To fix this problem, they add a bunch of salt to increase the liquid density to accommodate a floating human. According to Mr. Clark (their science teacher), they need 1,500 pounds of salt.
But was he right? Let's take a look at the science.
Why do things float? If an object is stationary at the surface of water (or any liquid), then the net force on that object is zero. Of course there is a gravitational force pulling down, so that must mean there is some other force (with an equal magnitude) pushing up. That force is the buoyancy force. But how does it work? Let's start with an example.
Here is a block of water floating in water. Yes, water floats.
In this diagram, the yellow arrows represent the rest of the water pushing on this floating block of water. The water pushes on the block in all directions and this force increases with depth. Notice that the forces from the water on the sides have to cancel (since they are balanced). However, the forces pushing up from the bottom are greater than the forces pushing down from the top. But since the block of water is floating, the net upward buoyancy force must be equal to the gravitational force pulling down.
Now replace the block of water with something else—it doesn't matter what it is made of as long as it is the exact same shape. If it's the exact same size, it must have the same buoyancy force on it. If the block is made of steel, the upward buoyancy force is going to be less than the downward gravitational force such that the steel will sink instead of float—but the buoyancy force is still there. Because a water block would float, the magnitude of this buoyancy force must be equal to the weight of the water the object displaces—this is Archimede's principle.
The weight of the water displaced depends on three things: the volume of the object, the density of the liquid (physicists like to use the Greek leter ρ for this) and the value of the gravitational field g. Putting this all together, the buoyancy can be written as:
But wait! What if an object is not completely submerged? What if the object is a block of wood or maybe a girl named Eleven? If the weight of the object is less than the weight of the water displaced then the buoyancy force will be greater and push the block up. It will keep moving up until part of the block is out of the water. The part of the block that is out of the water doesn't produce any buoyancy—so eventually the block will reach equilibrium with part of the object underwater and part above.
The fraction of the block that sticks above the water depends on two things: the density of the object and the density of the water. Let's do a quick example. Suppose I have a wood block with density ρb in water with density ρw.  Just for simplicity, it's a cubic block of length L. This is what it might look like.
Remember, the weight of the block has to be equal to the weight of the water displaced—so I will start off with the weight of the block. I know the density, so the mass (and thus the weight) can be found as ρb(L3)g. This should be equal to the weight of the water displaced with a value of ρw(L2 d)g where d is the depth of the block underwater. Notice that a lot of stuff cancels and I get:
So, the amount the block floats above the water depends on the ratio of the densities of the object and the liquid. Notice that if the object has a density equal to water, then it would float with nothing sticking out above the surface. If the density of the object was half of that of water, then have the object would stick out above the water.
This is idea is what Mr. Clark used to estimate the amount of salt to add to water. For sensory deprivation, you want to increase the density of the water such that it has a much higher density than the density of a human.
Water has a density of 1,000 kilograms per cubic meter. If you don't want to be cool, you could say the density is 1 gram per cubic centimeter but trust me—all the cool people use units of kg/m3. But what about the density of a human? It depends on the human, but it's normally a little bit less than 1,000 kg/m3 such that most humans float. Of course a human can float or sink depending on the lungs. If you take in a deep breath of air, your lungs get bigger and your density decreases. Blow all the air out of your lungs and you should sink.
How Netflix Made Stranger Things a Global Phenomenon
Stranger Things Is Based on a True Story. Kinda
Stranger Things' Creators on Season 2, Binge TV, and Foulmouthed Kids
Normal people breathe. This means that you might oscillate between floating and sinking. That would make it tough to focus on using your psionic powers to find other people (like Eleven does). You need a higher density liquid—like salt water. You might already know this, but you can more easily float in the ocean (salt water) than you can in a lake with fresh water.
So, adding salt to water will increase the density and hopefully the person can easily float.  But wait. If you add salt to water, doesn't that increase both the mass of the liquid and the volume? Actually, not really. Check this out: Here is 200 ml of water and 5 ml of salt.
What happens if I pour the salt into the water? This.
Yes, the volume of the mixture did increase a slight amount—but not by much. You can dissolve salt in water and the mass increases but not the volume. I know that seems crazy, but it's true. In fact, we like to think of water as this stuff that is continuous—but it's not. Liquid water is made of molecules of H2O and there are empty spaces between these molecules. Salt is made of sodium and chlorine atoms. When added to water, these salt crystals separate into sodium and chlorine ions that are much smaller than the water molecules so that they don't really increase the volume.
How about an analogy. Here I have two beakers. One has approximately 1,800 ml of ping pong balls and the other has about 600 ml of tiny cubes.
What happens if I mix these together?  It looks like this.
Notice that this cube-ball mixture is still about 1,800 ml. The cubes fit in the spaces left by the ping pong balls. Pretty cool, right?
So now that we know that adding salt just changes the mass (and not the volume) of water, we can change the density. Let's say that we want a human to float with 75 percent of the body underwater. What density of liquid do we need? Assuming a human density of 1,000 kg/m3, the liquid would have to be 1,333 kg/m3 (this is 1,000/0.75). In order to achieve this density, you would need to add 333 kilograms of salt for every cubic meter of water.
If I want to add salt to a kiddie pool, how much salt would that be? Let's say the pool has a diameter of 8 feet and a depth of 1.5 feet. Yes, I am using imperial units because Stranger Things takes place in the '80s—this is before they invented the metric units (just kidding). Using better units, this pool would hold 2.14 m3. That means 712 kilograms of salt. Converting to 1980s units, this is 1,569.69 pounds. Boom. Honestly, I can't believe my estimate was that close to the actual show. I guess they had a science advisor that essentially did my calculation—good job science advisor (or Mr. Clark).
'Stranger Things' stars Finn Wolfhard and Caleb McLaughlin show us the last things they did with their phones. What was the last emoji they used? The last text message sent? What was the last thing they searched?


Every year in December, a makeshift hangar at the NASA Ames Research Center pops up for one night, transforming the austere airfield into a glitzy, paparazzi’d, black velvet-roped Nerd Prom. At the Breakthrough Prizes—where on December 3, a total of $22 million was handed out to pioneers in math, physics, and the life sciences—researchers traded lab coats and latex gloves for floor-length gowns and bow-tied tuxedos. Outside it was all barbed wire and cold steel, but on the red carpet, stars and scientists alike sweated under the bright white lights and flash of cameras.
“My lab is going to be totally shocked to see me like this,” said Joanne Chory, a plant geneticist at the Salk Institute in San Diego and the only female awardee at Sunday’s prize ceremony, as she sparkled in a pink sequined shift with matching metal glasses frames. The winners, all 12 of them, had been under strict instructions not to tell any of their colleagues before hopping on planes and flying in for the event. But as the clock struck 4:30pm Pacific Time, and news began to get out, the emails started flooding in. David Spergel, a theoretical physicist at Princeton, was one of five members of the universe-cataloguing WMAP team to win the prize in physics. “There are five of us here being recognized, but 20 more on our team who just found out, they’re absolutely thrilled.”
The Nobels may be more prestigious than the Breakthroughs, but they come with a lot less money (about $2 million less, per prize). Alfred Nobel, whose fortune in the dynamite industry financed the namesake prize, hoped it might atone for his explosive contributions to science. But that isn’t the only thing that has embroiled the award in controversy from the very start. His will instructed that each prize could be awarded to only one person, only for discoveries made the preceding year, and oh, yeah, no mathematicians. While the committee tasked with carrying out his dying wishes has relaxed some of the rules over the years, the underlying framework still upholds the absurd notion that scientific advancement arrives on the back of lone geniuses.
The Breakthrough Prize was supposed to fix all that—with a spirit of inclusivity, optimism, and shiny Silicon Valley cash. Much of that prize money comes from Yuri Milner, the Russian billionaire tech investor who set up and financed the first award before convincing execs at Facebook, Google, 23andMe, and Alibaba to chip in more. (Since 2012, they’ve together awarded more than 70 $3 million prizes to research standouts.) But when the Paradise Papers were made public in early November, they revealed that behind Milner’s investments in Facebook and Twitter were hundreds of millions of dollars traced back to the Kremlin.
The Valley’s other oligarchs—Mark Zuckerberg, Sergey Brin, Sundar Pichai, Jack Dorsey—have also come under fire for their platforms’ complicity in spreading Russian misinformation during the 2016 presidential campaign. In October, the tech titans took a bipartisan beating on Capitol Hill, where members of Congress excoriated Facebook, Twitter, and Google for enabling Russian attempts to divide the American electorate and sow doubt in the democratic process.
The ensuing tech backlash is hitting hardest in Washington, where talk of regulation and anti-trust lawsuits have ticked up in recent months. In the nation’s capital, the corporate leviathans once seen as beacons of new American enterprise are increasingly portrayed as sinister centers of power, too big to be accountable. These revelations and transformations can’t help but change the perception of the wealth backing the various Breakthroughs. Perhaps anticipating this line of questioning, the event’s tech royalty were noticeably silent this year. Only Dick Costolo, previously of Twitter, braved the media corral, saying only that he was happy to be at an event that “puts scientists front and center.” Brin declined questions, as did Milner, who barely cracked a close-mouthed smile for the cameras. Mark Zuckerberg and Priscilla Chan were no-shows, though Zuck did send in a grey-hoodied video greeting that played during the award ceremony. Cori Bargmann was present, the lone, in-person representative from the couple’s philanthropic organization, CZI. This marked a striking comparison to last year, said one of the male reporters in the pool, who had found Milner an entertaining interview in 2016. “If I had known it was going to be like this, I don’t think I would have come,” he said.
With Silicon Valley’s luminaries sticking to the sidelines, it was perhaps Gavin Newsom, California’s Lieutenant Governor, who captured the moment best: “We are celebrating tonight everything that Trump’s Washington is not: facts, science, innovation, entrepreneurialism,” he said “It’s important that we show here in California that we are committed to investing in that.”
And at least for the winning scientists, the award has not yet been tainted by the backlash or the current political climate. Chory says she didn’t think twice about accepting. She’s planning to give most of the money to her kids, so that they can pay back student loan debts and buy houses. But at least a sizeable chunk will go toward turning her research into a global reality. Despite her daily battle with Parkinson’s, Chory has spent the last three decades in the lab genetically engineering crop plants like chickpeas and lentils that can pull 20 times the average amount of carbon dioxide from the atmosphere and store it as a cork-like polymer deep underground.
What’s next is scaling up to the planetary level. She’s calculated that converting just 5 percent of the world’s cropland to her plants could get rid of half of global CO2 emissions. But financing field trials and seed production and distribution and farmer outreach is beyond the scope of most basic research funding mechanisms. Which is why she’s hoping the prize money will give her initiative a jump-start to bring in other grants and investors. “I’ll do my best to milk it as best I can,” says Chory, who figures the total cost for launching the project hovers around what Milner paid for his $100 million mansion, located just up the hill from the Ames red carpet. She says she appreciates being recognized, and a reason to go shopping with her family. But while she was happy to attend the evening with her kids, she’s focused on doing something to make the world they’ll inherit a less dangerous place. “I’m trying to do something now for humankind, not just to please by brain or follow a scientific curiosity. I don’t want to leave a crappy planet as my legacy.”
Bargmann, who was on this year’s selection committee for the life sciences, said the prize was, to her, as much about the future, as about moments in the past that changed science forever. “We’re honoring people tonight who totally changed a field; it was one way before they came along, and something totally different afterward.”
For the chromosome theory of human genetic inheritance, i.e. how you got the genes you got, that was Kim Nasmyth, a biochemist from Oxford who figured out how chromosomes separate during mitosis. He thought about brushing off his old wool tuxedo for the event, but ultimately opted for something newer, and less warm. In his lapel he wore a gold pin with a white cross on a red shield—a gift from the city of Vienna, where he used to work. “It’s the only piece of jewelry I own,” he said. “I thought I might as well wear it.”
While he’s thrilled to receive the award, and pay some of the money forward to a foundation that will support the next generation of scientists, he says that recognition should never be the goal of a good researcher. “Ultimately, when you get out of bed in the morning, you just want to know, to understand,” he says. “I think what drives discoveries are the mysteries that can’t be explained.”
Life Sciences(Each of the five Life Science winners will receive a $3 million prize.)
Fundamental Physics(The five winners received a single, $3 million prize, which they will share with the entire WMAP science mission team.)
Charles L. Bennett, an astrophysicist at Johns Hopkins; Gary Hinshaw, an astrophysicist at the University of British of Columbia; Norman Jarosik, a physicist at Princeton; Lyman Page Jr., a physicist at Princeton; and, David N. Spergel, an astrophysicist at Princeton, for their work building detailed maps of the early universe that redefined the evolution of the cosmos and the fluctuations that seeded the formation of galaxies.
Mathematics(The two winners will equally share a $3 million prize.)
Christopher Hacon, a mathematician at the University of Utah, and James McKernan, a mathematician at the University of California, San Diego, for their transformational contributions to birational algebraic geometry, especially to the minimal model program in all dimensions.


Anurag Acharya’s problem was that the Google search bar is very smart, but also kind of dumb. As a Googler working on search 13 years ago, Acharya wanted to make search results encompass scholarly journal articles. A laudable goal, because unlike the open web, most of the raw output of scientific research was invisible—hidden behind paywalls. People might not even know it existed. “I grew up in India, and most of the time you didn’t even know if something existed. If you knew it existed, you could try to get it,” Acharya says. “‘How do I get access?’ is a second problem. If I don’t know about it, I won’t even try.”
Acharya and a colleague named Alex Verstak decided that their corner of search would break with Google tradition and look behind paywalls—showing citations and abstracts even if it couldn’t cough up an actual PDF. “It was useful even if you did not have university access. That was a deliberate decision we made,” Acharya says.
Then they hit that dumbness problem. The search bar doesn’t know what flavor of information you’re looking for. You type in “cancer;” do you want results that tell you your symptoms aren’t cancer (please), or do you want the Journal of the American Medical Association? The search bar doesn’t know.
Acharya and Verstak didn't try to teach it. Instead, they built a spinoff, a search bar separate from Google-prime that would only look for journal articles, case law, patents—hardcore primary sources. And it worked. “We showed it to Larry [Page] and he said, ‘why is this not already out?’ That’s always a positive sign,” Acharya says.
Today, even though you can’t access Scholar directly from the Google-prime page, it has become the internet’s default scientific search engine—even more than once-monopolistic Web of Science, the National Institutes of Health’s PubMed, and Scopus, owned by the giant scientific publisher Elsevier.
But most science is still paywalled. More than three quarters of published journal articles—114 million on the World Wide Web alone, by one (lowball) estimate—are only available if you are affiliated with an institution that can afford pricey subscriptions or you can swing $40-per-article fees. In the last several years, though, scientists have made strides to loosen the grip of giant science publishers. They skip over the lengthy peer review process mediated by the big journals and just … post. Review comes after. The paywall isn’t crumbling, but it might be eroding. The open science movement, with its free distribution of articles before their official publication, is a big reason.
Another reason, though, is stealthy improvement in scientific search engines like Google Scholar, Microsoft Academic, and Semantic Scholar—web tools increasingly able to see around paywalls or find articles that have jumped over. Scientific publishing ain’t like book publishing or journalism. In fact, it’s a little more like music, pre-iTunes, pre-Spotify. You know, right about when everyone started using Napster.
Before World War II most scientific journals were published by small professional societies. But capitalism’s gonna capitalism. By the early 1970s the top five scientific publishers—Reed-Elsevier, Wiley-Blackwell, Springer, and Taylor & Francis—published about 20 percent of all journal articles. In 1996, when the transition to digital was underway and the PDF became the format of choice for journals, that number went up to 30 percent. Ten years later it was 50 percent.
Those big-five publishers became the change they wanted to see in the publishing world—by buying it. Owning over 2,500 journals (including the powerhouse Cell) and 35,000 books and references (including Gray’s Anatomy) is big, right? Well, that’s Elsevier, the largest scientific publisher in the world, which also owns ScienceDirect, the online gateway to all those journals. It owns the (pre-Google Scholar) scientific search engine Scopus. It bought Mendeley, a reference manager with social and community functions. It even owns a company that monitors mentions of scientific work on social media. “Everywhere in the research ecosystem, from submission of papers to research evaluations made based on those papers and various acts associated with them online, Elsevier is present,” says Vincent Larivière, an information scientist at the University of Montreal and author of the paper with those stats about publishing I put one paragraph back.
The company says all that is actually in the service of wider dissemination. “We are firmly in the open science space. We have tools, services, and partnerships that help create a more inclusive, more collaborative, more transparent world of research,” says Gemma Hersh,1 Elsevier’s vice president for open science. “Our mission is around improving research performance and working with the research community to do that.” Indeed, in addition to traditional, for-profit journals it also owns SSRN, a preprint server—one of those places that hosts unpaywalled, pre-publication articles—and publishes thousands of articles at various levels of openness.
So Elsevier is science publishing’s version of Too Big to Fail. As such, it has faced various boycotts, slightly piratical workarounds, and general anger. (“The term ‘boycott’ comes up a lot, but I struggle with that. If I can be blunt, I think it’s a word that’s maybe misapplied,” Hersh says. “More researchers submit to us every year, and we publish more articles every year.”)
If you’re not someone with “.edu” in your email, this might make you a little nuts. Not just because you might want to actually see some cool science, but because you already paid for that research. Your taxes (or maybe some zillionaire’s grant money) paid the scientists and funded the studies. The experts who reviewed and critiqued the results and conclusions before publication were volunteers. Then the journal that published it charged a university or a library—again, probably funded at least in part by your taxes—to subscribe. And then you gotta buy the article? Or the researcher had to pony up $2,0002 to make it open access?
Now, publishers like Elsevier will say that the process of editing, peer-reviewing, copy editing, and distribution are a major, necessary value add. And look at the flip side: so-called predatory journals that charge authors to publish nominally open-access articles with no real editing or review (that, yes, show up in search results). Still, the scientific publishing business is a $10 billion-a-year game. In 2010, Elsevier reported profits of $1 billion and a 35 percent margin. So, yeah.
In that early-digital-music metaphor, the publishers are the record labels and the PDFs are MP3s. But you still need a Napster. That’s where open-science-powered search engines come in.
A couple years after Acharya and Verstak built Scholar, a team at Microsoft built their own version, called Academic. It was at the time a much, let’s say, leaner experience, with far fewer papers available. But then in 2015, Microsoft released a 2.0, and it’s a killer.
Microsoft’s communication team declined to make any of the people who run it available, but a paper from the team at Microsoft Research lays the specs out pretty well:  It figures out the bibliographic data of papers and combines that with results from Bing. (A real search engine that exists!) And you know what? It’s pretty great. It sees 83 million papers, not so far from estimations of the size of Google’s universe, and does the same kind of natural-language queries. Unlike Scholar, people can hook into Microsoft Academic’s API and see its citation graph, too.
Even as recently as 2015, scientific search engines weren’t much use to anyone outside universities and libraries. You could find a citation to a paper, sure—but good luck actually reading it. Even though more overt efforts to subvert copyright like Sci-Hub are falling to lawsuits from places like Elsevier and the American Chemical Society, the open science movement gaining is momentum. PDFs are falling off virtual trucks all over the internet—posted on university web sites or places like ResearchGate and Academia.edu, hosts for exactly this kind of thing—Scholar’s and Academic’s first sorties against the paywall have been joined by reinforcements. It’s starting to look like a siege.
For example the Chan Zuckerberg Initative, philanthropic arm of the founder of Facebook, is working on something aimed at increasing access. The founders of Mendeley have a new, venture-backed PDF finder called Kopernio. A browser extension called Unpaywall roots around the web for free PDFs of articles.
A particularly novel web crawler comes from the non-profit Allen Institute for Artificial Intelligence. Semantic Scholar pores over a corpus of 40 million citations in computer science and biomedicine, and extracts the tables and charts as well as using machine learning to infer meaningful cites as “highly influential citations,” a new metric. Almost a million people use it every month.
“We use AI techniques, particularly natural language processing and machine vision, to process the PDF and extract information that helps readers decide if the paper is of interest,” says Oren Etzioni, CEO of the Allen Institute for AI. “The net effect of all this is that more and more is open, and a number of publishers … have said making content discoverable via these search engines is not a bad thing.”
Even with all these increases in discoverability and access, the technical challenges of scientific search don’t stop with paywalls. When Acharya and Verstak started out, Google relied on PageRank, a way to model how important hyperlinks between two web pages were. That’s not how scientific citations work. “The linkage between articles is in text. There are references, and references are all approximate,” Acharya says. “In scholarship, all your citations are one way. Everybody cites older stuff, and papers never get modified.”
Plus, unlike a URL, the location or citation for a journal article is not the actual journal article. In fact, there might be multiple copies of the article at various locations. From a perspective as much philosophical and bibliographical, a PDF online is really just a picture of knowledge, in a way. So the search result showing a citation might also attach to multiple versions of the actual article.
That’s a special problem when researchers can post pre-print versions of their own work but might not have copyright to the publication of record, the peer-reviewed, copy-edited version in the journal. Sometimes the differences are small; sometimes they’re not.
Why don’t the search engines just use metadata to understand what version belongs where? Like when you download music, your app of choice automatically populates with things like an image, the artist’s name, the song titles...the data about the thing.
Biology's Roiling Debate Over Publishing Research Early
A Reboot of the Legendary Physics Site ArXiv Could Shape Open Science
Retracting Bad Science Doesn’t Make It Disappear
The answer: metadata LOL. It’s a big problem. “It varies by source,” Etzioni says. “A whole bunch of that information is not available as structured metadata.” Even when there is metadata, it’s in idiosyncratic formats from publisher to publisher and server to server. “In a surprising way, we’re kind of in the dark ages, and the problem just keeps getting worse,” he says. More papers get published; more are digital. Even specialists can’t keep up.
Which is why scientific search and open science are so intertwined and so critical. The reputation of a journal and the number of times a specific paper in that journal gets cited are metrics for determining who gets grants and who gets tenure, and by extension who gets to do bigger and bigger science. “Where the for-profit publishers and academic presses sort of have us by the balls is that we are addicted to prestige,” says Guy Geltner, a historian at the University of Amsterdam, open science advocate, and founder of a new user-owned social site for scientists called Scholarly Hub.
The thing is, as is typical for Google, Scholar is as opaque about how it works and what it finds. Acharya wouldn’t give me numbers of users or the number of papers it searches. (“It’s larger than the estimates that are out there,” he says, and “an order of magnitude bigger than when we started.) No one outside Google fully understands how the search engine applies its criteria for inclusion,3 and indeed Scholar hoovers up way more than just PDFs of published or pre-published articles. You get course syllabi, undergraduate coursework, PowerPoint presentations … actually, for a reporter, it’s kind of fun. But tricky.
That means the citation data is also obscure, which makes it hard to know what Scholar’s findings mean for science as a whole. Scholar may be a low-priority side-project (please don’t kill it like you killed Reader!) but maybe that data is going to be valuable someday. Elsevier obviously thinks it’s useful.
The scientific landscape is shifting. "If you took a group of academics right now and asked them to create a new system of publishing, nobody would suggest what we're currently doing," says David Barner, a psychologist at UC San Diego and open science advocate. But change, Barner says, is hard. The people who'd make those changes are already overworked, already volunteering their time.
Even Elsevier knows that change is coming. “Rather than scrabble around in one of the many programs you’ve mentioned, anyone can come to our Science and Society page, which details a host of programs and organizations we work with to cater through every scenario where somebody wants access,” Hersh says. And that’d be to the final, published, peer-reviewed version—the archived, permanent version of record.
Digital revolutions have a way of #disrupting no matter what. As journal articles get more open and more searchable, value will come from understanding what people search for—as Google long ago understood about the open web. “We’re a high quality publisher, but we’re also an information analytics company, evolving services that the research community can use,” Hersh says.
Because reputation and citation are core currencies to scientists, scientists have to be educated about the possibilities of open publication at the same time as prestigious, reputable venues have to exist. Preprints are great, and the researchers maintain copyright to them, but it’s also possible that the final citation-of-record could be different after it goes through review. There has to be a place where primary scientific work is available to the people who funded it, and a way for them to find it.
Because if there isn’t? “A huge part of research output is suffocating behind paywalls. Sixty-five of the 100 most cited articles in history are behind paywalls. That’s the opposite of what science is supposed to do,” Geltner says. “We’re not factories producing proprietary knowledge. We’re engaged in debates, and we want the public to learn from those debates.”
I'm sensitive to the irony of a WIRED writer talking about the social risks of a paywall, though I'd draw a distinction between paying a journalistic outlet for its journalism and paying a scientific publisher for someone else's science.
An even more critical difference, though, is that a science paywall does more than separate gown from town. When all the solid, good information is behind a paywall, what’s left outside in the wasteland will be crap—propaganda and marketing. Those are always free, because people with political agendas and financial interests underwrite them. Understanding that vaccines are critical to public health and human-driven carbon emissions are un-terraforming the planet cannot be the purview of the one percent. “Access to science is going to be a first-world privilege,” Geltner says. “That’s the opposite of what science is supposed to be about.”
1 UPDATE 12/3/17 11:55 AM Corrected the spelling of this name.
2 UPDATE 12/4/17 1:25 PM Removed the word "another;" researchers sometimes pay to make their own articles open-access.
3 UPDATE 12/4/17 1:25 PM Clarified to show that Google publishes inclusion criteria.
Senior Scientist at The Exploratorium, Paul Doherty, gives us an in depth look at technology used by the Mars Rover Curiosity.


This week’s space photos survey galaxies near and far—starting with the search for a missing arm. This pink, twinkling image is dwarf galaxy NGC 4625—and it should have at least two appendages, like most other spiral galaxies. Why? It turns out fellow nearby dwarf galaxy NGC 4618 must have interfered with the galaxy at one point, causing it to lose its outermost arm.
Next up, we’re going to stop and stare at the striking arm of our very own Milky Way galaxy, seen shooting out of the horizon of the Atacama desert in Chile. The bright bulge of light in the center is the core of the galaxy, where over half a million stars are burning, as well as a supermassive black hole. But don’t worry—our sun will have burned up long before we get sucked into oblivion.
Finally, we’re hopping to the galaxy next door, Andromeda. Astronomers were searching for a specific star in the Andromeda galaxy when they happened upon not one but two supermassive black holes. They estimate the mass of these two black holes to be 200 million times the mass of our sun.
Still want to hang out in deep space? Check out the full collection of cosmic photos here.


Glance at the night sky from a clear vantage point, and the thick band of the Milky Way will slash across the sky. But the stars and dust that paint our galaxy’s disk are an unwelcome sight to astronomers who study all the galaxies that lie beyond our own. It’s like a thick stripe of fog across a windshield, a blur that renders our knowledge of the greater universe incomplete. Astronomers call it the Zone of Avoidance.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Renée Kraan-Korteweg has spent her career trying to uncover what lies beyond the zone. She first caught a whiff of something spectacular in the background when, in the 1980s, she found hints of a potential cluster of objects on old photographic survey plates. Over the next few decades, the hints of a large-scale structure kept coming.
Late last year, Kraan-Korteweg and colleagues announced that they had discovered an enormous cosmic structure: a “supercluster” of thousands upon thousands of galaxies. The collection spans 300 million light years, stretching both above and below the galactic plane like an ogre hiding behind a lamppost. The astronomers call it the Vela Supercluster, for its approximate position around the constellation Vela.
The Milky Way, just like every galaxy in the cosmos, moves. While everything in the universe is constantly moving because the universe itself is expanding, since the 1970s astronomers have known of an additional motion, called peculiar velocity. This is a different sort of flow that we seem to be caught in. The Local Group of galaxies—a collection that includes the Milky Way, Andromeda and a few dozen smaller galactic companions—moves at about 600 kilometers per second with respect to the leftover radiation from the Big Bang.
Over the past few decades, astronomers have tallied up all the things that could be pulling and pushing on the Local Group — nearby galaxy clusters, superclusters, walls of clusters and cosmic voids that exert a non-negligible gravitational pull on our own neighborhood.
The biggest tugboat is the Shapley Supercluster, a behemoth of 50 million billion solar masses that resides about 500 million light years away from Earth (and not too far away in the sky from the Vela Supercluster). It accounts for between a quarter and half of the Local Group’s peculiar velocity.
The remaining motion can’t be accounted for by structures astronomers have already found. So astronomers keep looking farther out into the universe, tallying increasingly distant objects that contribute to the net gravitational pull on the Milky Way. Gravitational pull decreases with increasing distance, but the effect is partly offset by the increasing size of these structures. “As the maps have gone outward,” said Mike Hudson, a cosmologist at the University of Waterloo in Canada, “people continue to identify bigger and bigger things at the edge of the survey. We’re looking out farther, but there’s always a bigger mountain just out of sight.” So far astronomers have only been able to account for about 450 to 500 kilometers per second of the Local Group’s motion.
Astronomers still haven’t fully scoured the Zone of Avoidance to those same depths, however. And the Vela Supercluster discovery shows that something big can be out there, just out of reach.
In February 2014, Kraan-Korteweg and Michelle Cluver, an astronomer at the University of Western Cape in South Africa, set out to map the Vela Supercluster over a six-night observing run at the Anglo-Australian Telescope in Australia. Kraan-Korteweg, of the University of Cape Town, knew where the gas and dust in the Zone of Avoidance was thickest; she targeted individual spots where they had the best chance of seeing through the zone. The goal was to create a “skeleton,” as she calls it, of the structure. Cluver, who had prior experience with the instrument, would read off the distances to individual galaxies.
That project allowed them to conclude that the Vela Supercluster is real, and that it extends 20 by 25 degrees across the sky. But they still don’t understand what’s going on in the core of the supercluster. “We see walls crossing the Zone of Avoidance, but where they cross, we don’t have data at the moment because of the dust,” Kraan-Korteweg said. How are those walls interacting? Have they started to merge? Is there a denser core, hidden by the Milky Way’s glow?
And most important, what is the Vela’s Supercluster’s mass? After all, it is mass that governs the pull of gravity, the buildup of structure.
While the Zone’s dust and stars block out light in optical and infrared wavelengths, radio waves can pierce through the region. With that in mind, Kraan-Korteweg has a plan to use a type of cosmic radio beacon to map out everything behind the thickest parts of the Zone of Avoidance.
The plan hinges on hydrogen, the simplest and most abundant gas in the universe. Atomic hydrogen is made of a single proton and an electron. Both the proton and the electron have a quantum property called spin, which can be thought of as a little arrow attached to each particle. In hydrogen, these spins can line up parallel to each other, with both pointing in the same direction, or antiparallel, pointing in opposite directions. Occasionally a spin will flip—a parallel atom will switch to antiparallel. When this happens, the atom will release a photon of light with a particular wavelength.
The likelihood of one hydrogen atom’s emitting this radio wave is low, but gather a lot of neutral hydrogen gas together, and the chance of detecting it increases. Luckily for Kraan-Korteweg and her colleagues, many of Vela’s member galaxies have a lot of this gas.
During that 2014 observing session, she and Cluver saw indications that many of their identified galaxies host young stars. “And if you have young stars, it means they recently formed, it means there’s gas,” Kraan-Korteweg said, because gas is the raw material that makes stars.
The Milky Way has some of this hydrogen, too—another foreground haze to interfere with observations. But the expansion of the universe can be used to identify hydrogen coming from the Vela structure. As the universe expands, it pulls away galaxies that lie outside our Local Group and shifts the radio light toward the red end of the spectrum. “Those emission lines separate, so you can pick them out,” said Thomas Jarrett, an astronomer at the University of Cape Town and part of the Vela Supercluster discovery team.
This Dark Matter Theory Could Solve a Celestial Conundrum
Squishy or Solid? A Neutron Star’s Insides Open to Debate
A Mysterious Galactic Glow Hints at Hidden Pulsars
While Kraan-Korteweg’s work over her career has dug up some 5,000 galaxies in the Vela Supercluster, she is confident that a sensitive enough radio survey of this neutral hydrogen gas will triple that number and reveal structures that lie behind the densest part of the Milky Way’s disk.
That’s where the MeerKAT radio telescope enters the picture. Located near the small desert town of Carnarvon, South Africa, the instrument will be more sensitive than any radio telescope on Earth. Its 64th and final antenna dish was installed in October, although some dishes still need to be linked together and tested. A half array of 32 dishes should be operating by the end of this year, with the full array following early next year.
Kraan-Korteweg has been pushing over the past year for observing time in this half-array stage, but if she isn’t awarded her requested 200 hours, she’s hoping for 50 hours on the full array. Both options provide the same sensitivity, which she and her colleagues need to detect the radio signals of neutral hydrogen in thousands of individual galaxies hundreds of light years away. Armed with that data, they’ll be able to map what the full structure actually looks like.
Hélène Courtois, an astronomer at the University of Lyon, is taking a different approach to mapping Vela. She makes maps of the universe that she compares to watersheds, or basins. In certain areas of the sky, galaxies migrate toward a common point, just as all the rain in a watershed flows into a single lake or stream. She and her colleagues look for the boundaries, the tipping points of where matter flows toward one basin or another.
A few years ago, Courtois and colleagues used this method to attempt to define our local large-scale structure, which they call Laniakea. The emphasis on defining is important, Courtois explains, because while we have definitions of galaxies and galaxy clusters, there’s no commonly agreed-upon definition for larger-scale structures in the universe such as superclusters and walls.
Part of the problem is that there just aren’t enough superclusters to arrive at a statistically rigorous definition. We can list the ones we know about, but as aggregate structures filled with thousands of galaxies, superclusters show an unknown amount of variation.
Now Courtois and colleagues are turning their attention farther out. “Vela is the most intriguing,” Courtois said. “I want to try to measure the basin of attraction, the boundary, the frontier of Vela.” She is using her own data to find the flows that move toward Vela, and from that she can infer how much mass is pulling on those flows. By comparing those flow lines to Kraan-Korteweg’s map showing where the galaxies physically cluster together, they can try to address how dense of a supercluster Vela is and how far it extends. “The two methods are totally complementary,” Courtois added.
The two astronomers are now collaborating on a map of Vela. When it’s complete, the astronomers hope that they can use it to nail down Vela’s mass, and thus the puzzle of the remaining piece of the Local Group’s motion—“that discrepancy that has been haunting us for 25 years,” Kraan-Korteweg said. And even if the supercluster isn’t responsible for that remaining motion, collecting signals through the Zone of Avoidance from whatever is back there will help resolve our place in the universe.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
A cloud of gas and dust named G2 is headed toward the central supermassive black hole in our Milky Way. This simulation shows what the cloud could look like as it approaches and passes around the black hole.


This story originally appeared on Grist and is part of the Climate Desk collaboration.
It’s a sunny October day on the outskirts of the west German town of Bottrop. A quiet, two-lane road leads me through farm pasture to a cluster of anonymous, low-lying buildings set among the trees. The highway hums in the distance. Looming above everything else is a green A-frame structure with four great pulley wheels to carry men and equipment into a mine shaft. It’s the only visible sign that, almost three quarters of a mile below, Germany’s last hard coal lies beneath this spot.
Bottrop sits in the Ruhr Valley, a dense stretch of towns and suburbs home to 5.5 million people. Some 500,000 miners once worked in the region’s nearly 200 mines, producing as much as 124 million tons of coal every year.
Next year, that era will come to an end when this mine closes. The Ruhr Valley is in the midst of a remarkable transformation. Coal and steel plants have fallen quiet, one by one, over the course of the last half-century. Wind turbines have sprung up among old shaft towers and coking plants as Germany strives to hit its renewable energy goals.
But the path from dirty coal to clean energy isn’t an easy one. Bottrop’s Prosper-Haniel coal mine is a symbol of the challenges and opportunities facing Germany—and coal-producing states everywhere.
Around the world, as governments shift away from the coal that fueled two ages of industrial revolution, more and more mines are falling silent. If there’s an afterlife for retired coal mines, one that could put them to work for the next revolution in energy, it will have to come soon.
The elevator that carries Germany’s last coal miners on their daily commute down the mine shaft travels at about 40 feet a second, nearly 30 miles an hour. “Like a motorcycle in a city,” says Christof Beicke, the public affairs officer for the Ruhr mining consortium, as the door rattles shut. It’s not a comforting analogy.
The brakes release and, for a moment, we bob gently on the end of the mile-and-a-half long cable, like a boat in dock. Then we drop. After an initial flutter in my stomach, the long minutes of the ride are marked only by a strong breeze through the elevator grilles and the loud rush of the shaft going by.
When the elevator finally stops, on the seventh and deepest level of the mine, we file into a high-ceilinged room that looks like a subway platform. One of the men who built this tunnel, Hamazan Atli, leads our small group of visitors through the hall. Standing in the fluorescent light and crisp, engineered breeze, I have the uncanny sense of walking into an environment that humans have designed down to the last detail, like a space station or a submarine.
A monorail train takes us the rest of the way to the coal seam. After about half an hour, we clamber out of the cars and clip our headlamps into the brackets on our hard hats. It is noticeably warmer here. There is a sulfurous smell that grows stronger as we walk down the slight incline toward the deepest point of our day, more than 4,000 feet below the surface, and duck under the first of the hydraulic presses that keep the ceiling from collapsing on us.
Because this seam is only about five feet high, we have to hunch as we move through the tunnel of presses, stepping through deep pools of water that swallow our boots. The coal-cutting machine is stalled today, otherwise it would be chewing its way along the 310-yard-long seam, mouthparts clamped to the coal like a snail to aquarium glass. The coal would be sluiced away on a conveyor belt to the surface, and the hydraulic presses would inch forward, maintaining space for the miners to work.
Instead, the mine is eerily quiet. Two miners, their faces black, squeeze past us. As we sit, sweating and cramped under the hydraulic presses, the bare ceiling above the coal seam gives up an occasional gasp of rock, showering down dust and debris.
Later, in a brightly lit room back on the surface, Beicke from the mining consortium asks me what I thought of the mine. I tell him that it seems like an extreme environment for humans. “Yes,” he nods, “it is like an old world.”
A few days earlier, Beicke and I had trekked to the top of a hill outside the long-shuttered Ewald Mine in Herten, a half-hour drive from Bottrop. We climbed a set of stairs to a platform with a view over the whole region, the fenced-off or leased-out buildings of the old mine sitting below us.
The Ruhr Valley encompasses 53 cities of Germany’s once-formidable industrial heartland, including Essen, Bochum, and Oberhausen. The whole region was once low-lying riverland, but these days large hills rear above the landscape. These are the heaps of rock removed from the mines, tons of slag excavated with the coal and piled up. It’s a stark visual reminder of what’s been emptied out from underneath.
As the mines have closed down, most of these heaps have been covered with grass, and many have been crowned with a statue or other landmark. On one hill outside Essen, there’s a 50-foot steel slab by the sculptor Richard Serra; on another, atop other heaps, wind turbines stand like giant mechanical daisies.
Germany has been hailed as a leader in the global shift to clean energy, putting aside its industrial past for a renewable future faster than most of the industrialized world. The country has spent more than $200 billion on renewable energy subsidies since 2000 (compare that to the United States, which spends an estimated $20 billion to subsidize fossil fuel production every year).
In 2011, Chancellor Angela Merkel’s government announced the beginning of a policy of “energiewende” to wean Germany off fossil fuels and nuclear power. Last year, wind, solar, and other renewables supplied nearly 30 percent of the country’s electricity. The goal now is to hit 40 percent in the next 10 years, while slashing carbon emissions 40 percent below 1990 levels by 2020.
That transition has happened alongside attempts to restore the Ruhr Valley’s landscape. For every hill raised above ground level, there is an accompanying depression where the land subsided as coal seams were emptied out. The land here sank as the coal seams closest to the surface were emptied out. Overall, the region has sunk about 80 feet.
Streams that enter the Ruhr Valley are no longer able to flow out the other side, Beicke explains, and now water pools in places it never used to. The mining company is responsible for pumping that water away, as well as pumping groundwater across the region, to keep the water table below the level of the existing mines. Any contaminated water in the old mines must be removed and treated to keep it from polluting the groundwater.
These are just a few of the mining company’s “ewigkeitsaufgaben”—literally, eternity tasks.
“As long as 5 or 6 million people want to live in this area, we will have to do that,” Beicke tells me, of the expensive water management. “Maybe 2,000 years in the future that will change, but until that happens, well.” He shrugs.
The government gives the mining consortium 220 million euros a year in subsidies to deal with all the consequences of coal mining. Unlike in the United States, where aging coal companies often sell off their assets or declare bankruptcy to dodge clean up bills, here the mining company will be pumping and treating water long after it has stopped being a mining company at all.
Despite a national commitment to a broad energy transition, many now think that Germany will fall short of its renewable energy targets, thanks to a number of confounding economic and social factors, including the continued use of a coal alternative called lignite, also known as “brown coal.” Germans have the highest electricity costs in Europe, and the rise of the country’s extreme right-wing party in the last election has been pinned, in part, on those high bills.
If Germany does continue to progress toward its climate goals, much of the new energy is sure to come from wind power. Germany has more wind turbines than any other country in Europe, many of them installed in the last six or seven years. But wind doesn’t blow consistently, so this shift has been a challenge for the electrical grid. Even slight disruptions in the power supply can have wide-ranging consequences.
As more wind turbines are turned on, and more coal plants are retired, this problem will only get bigger, and the challenge of storing all that intermittent energy will be even more important. Here’s where the country’s retired coal mines might prove useful again — as giant batteries for clean energy.
To turn a coal mine into a battery, all you need is gravity.
OK, you also need a lot of money (more about that later), but the basic principle is gravitational. When you lift a heavy object, it stores the power used to lift it as potential energy until it’s released and falls to the ground.
Let’s say the heavy object you’re lifting is water. When you want to store energy, you just have to pump the water uphill, into a reservoir. When you want to use that energy, you let the water flow back down through a series of turbines that turn the gravitational rush into electricity.
This is the basic plan André Niemann and Ulrich Schreiber conceived when they were dreaming up new ways to use old mines. It seemed intuitive to the two professors at the University of Essen-Duisburg: The bigger the distance between your upper and lower reservoirs, the more energy you can store, and what’s deeper than a coal mine?
Schreiber, a geologist, realized it was theoretically possible to fit a pumped storage reservoir into a mine, but it had never been done before. Niemann, a hydraulic engineer, thought the proposal was worth pursuing. He drummed up some research money, then spent a few years conducting feasibility studies, looking for a likely site in the Ruhr Valley and running the numbers on costs and benefits.
After studying the region’s web of fault lines and stratigraphic layers, Niemann’s team settled on the closing Prosper-Haniel mine. Their underground reservoir would be built like a massive highway tunnel, a reinforced concrete ring nine miles around and nearly 100 feet high, with a few feet difference in height from one side of the ring to the other to allow the water to flow, Niemann explains.
At max storage, the turbines could run for four hours, providing 800 megawatt-hours of reserve energy, enough to power 200,000 homes.
The appeal of pumped storage is obvious for Germany. Wind and sun are fickle energy sources—“intermittent” by industry lingo—and energy storage can help smooth out the dramatic spikes. When the wind gusts, you can stash that extra power in a battery. When a cloud moves over the sun, you can pull power back out. It’s simple and, as the grid handles more and more renewable energy, increasingly needed.
The only problem: It’s expensive.
As wind turbine and solar technologies have become cheaper, energy storage costs have stayed high. Pumped hydro, especially, requires a big investment up front. Niemann estimates it would cost between 10,000 and 25,000 euros per meter of tunnel just to build the reservoir, and around 500 million euros for the whole thing. Right now, neither the government nor the energy companies in the Ruhr Valley are willing to make that kind of investment.
“It’s not a business, it’s a bet, to be honest,” Niemann says with a shrug.
In spite of the increasing unlikelihood of the proposal becoming reality, delegations from the United States, China, Poland, France, South Africa, and Slovakia, among others, have visited Niemann and Schreiber in Essen to learn about mine pumped-storage. Virginia’s Dominion Energy has been studying the idea with the support of a Republican state senator, and a group from Virginia Tech paid a visit the week after I did.
Here’s where any attempt to draw comparisons across the Atlantic gets complicated. In the United States, the federal government has been relatively hands-off in helping coal-dependent regions move on from the industries that fueled their way of life. In Germany, by contrast, there’s a broad agreement about the need to shift to renewable sources of energy. And yet, even with all that social, political, and economic foresight, important and necessary innovations remain stalled for lack of investment.
The Ruhr Valley is not Appalachia. And yet the two regions share key similarities that offer some important lessons about the a path to a cleaner, more sustainable future.
Dying industries take more than jobs with them. Towns built around a single industry, like coal mining, develop a shared identity. For many workers and their families, it’s not as simple as picking up and finding a new line of work when the mine closes. Mining is seen as a calling, an inheritance, and people want their way of life back.
That’s how residents of the Ruhr responded when coal jobs started to decline.
“For a long time, people thought the old times would come back, the old days would return,” says Kai van de Loo, an energy and economics expert for a German coal association in Essen. “But they can never come back.”
In the United States, of course, calls to bring back the old days often works wonders as a political sales pitch. Donald Trump campaigned for president on promises to stop the “war on coal” and revive the dying industry, and mining towns across the Rust Belt supported him.
In Pennsylvania’s Mon River Valley, home to a once-thriving underground mining complex bigger than Manhattan, mining continues to exert an oversized influence. Some 8,000 people work in coal in the state, a portion of the 50,000 coal jobs left in the United States. That’s a far cry from the 180,000 people who worked in the industry 30 years ago. worked in or around coal mines only 30 years ago.
And the legacy of coal mining on the landscape is hard to miss. Bare slag heaps rise above the trees, dwarfing the towns beside them. Maryann Kubacki, supervisor of East Bethlehem in Washington County, says that during rainy spells the township has to shovel the gritty, black runoff from their storm sewers.
A Crucial Climate Mystery Hides Just Beneath Your Feet
Tesla Is Turning Kauai Into a Renewable Energy Paradise
Silicon Valley’s Mission to Save California Ag From Dying of Thirst
But without the federal government leading the way with financial support as it has in Germany, getting these former coal towns on a new track is a daunting task. Veronica Coptis, director of the Center for Coalfield Justice in Pennsylvania, says that organizing people to put pressure on mining companies is a delicate matter. People don’t want to hear that coal is bad, or that its legacy is poisoned. “We want an end to mining,” she says, “but we know it can’t happen abruptly.”
Back in Germany, the mayor of Bottrop, Bernd Tischler, has been thinking about how to kick coal since at least the early 2000s, long before the federal government put an end date on the country’s mining. An urban planner by training, Tischler has a knack for long-range strategy. After he took office in 2009, Tischler thought Bottrop could reinvent itself as a hub of renewable energy and energy efficiency. He devised heating plants that run off methane collected from the coal mine, and made Bottrop the first town in the Ruhr with a planned zone for wind energy.
In 2010, Bottrop won the title of “Innovation City,” a model for what the Ruhr Valley cities could become. Bottrop now gets 40 percent of its energy from renewables, Tischler said, 10 percentage points above the national average.
Describing this transformation, Tischler makes it almost sound easy. I explain that the issue of coal seems to track larger divisions in the United States, and so discussions inevitably turn heated, emotional.
“In Bottrop, the people of course feared for the process of the end of the coal mining,” he said. But Tischler believes mining towns have an advantage that can help them adapt to change: They’re more cohesive. In the mines, people are used to working together and looking out for each other. Distrust is dangerous, even deadly.
The Ruhr cities absorbed waves of Polish, Italian, and Turkish laborers over the years. And they’ve managed to get along well, knitting a strong social fabric, Tischler said. In the past few years, Bottrop, a town of 117,000 people, has resettled thousands of Syrian refugees in new housing.
A strong social fabric isn’t enough to survive the loss of a major industry, of course. Some promising industry—technology and renewables in Bottrop’s case—has to be found to replace it.
“I think the responsibility of the mayors and the politicians is to change the fear into a new vision, a new way,” he says. “You can’t do it against your people; you have to convince your people. You have to work together with institutions and stakeholders that don’t normally work together, [so that] we are sitting in the same boat and we are rowing in the same direction.”
In our new series to kick off 2017, WIRED predicts the biggest trends for the year ahead. In this segment, Matt Simon explains how businesses are still going to go green, even in the face of renewed climate change denialism.


This holiday season, more people than ever before are giving the gift of spit. Well, what’s in your spit, to be precise. Want to know where your ancestors once walked or whether you’re at risk for a genetic disease? There’s a spit tube kit for that. And customers are buying them in record numbers.
Between Black Friday and Cyber Monday, leading personal genomics company AncestryDNA sold about 1.5 million testing kits designed to provide insights into your ethnicity and familial connections. That’s like 2,000 gallons of saliva—enough to fill a modest above-ground swimming pool with the genetic history of every person in the city of Philadelphia.
Ancestry says it’s equipped to deal with the impending deluge, but the flood of consumer interest has its executives eyeing the long-term prospects of their stretched supply chain. It also has some policymakers and public health officials concerned about the pace with which people are blindly giving away their genetic data to these types of companies, who can turn around and sell it to third parties.
At a press conference on Sunday, Senator Chuck Schumer (D–New York) called for increased federal scrutiny of the privacy practices of consumer DNA testing companies like Ancestry and its chief rival, 23andMe. The Food and Drug Administration regulates consumer DNA tests related to health, like the 23andMe panel it approved earlier this year. So what exactly does the congressman want? For the Federal Trade Commission to force the firms to extract all their buried fine print about how they might distribute your data, and broadcast it loud and clear. “I think if most people knew that this information could be sold to third parties they would think twice,” Schumer said. “The last gift any of us want to give away this holiday season is our most personal and sensitive information.”
While there’s no evidence that these companies have let anyone’s genetic data fall into the hands of hackers—or anything half that bad—their policies do grant them free rein to host, transfer, process, analyze, distribute, and communicate your genetic information. You still technically own your DNA, but they own the rights to what’s in it—after it’s been anonymized and de-identified, of course. Both companies say the primary way they use this genetic data is to improve their products and services. But both have research partnerships that involve exchanging data for money—23andMe with drug companies like Pfizer and Genentech, Ancestry with Alphabet longevity spinout Calico.
“This isn’t a videogame, it’s people’s genetic code and it’s a very valuable commodity,” says Peter Pitts, the president of the Center for Medicine in the Public Interest and former FDA associate commissioner. He’d like to see more transparency from Ancestry and 23andMe about how often they resell DNA data and how much they make from it. That’s the only way for people to know what it’s really worth. “To treat it like a toy and put it under the Christmas tree is incredibly irresponsible.”
But that’s exactly what millions of people are going to do. While Ancestry officials didn’t provide exact sales figures for this year’s Black Friday weekend, they did say they sold three times as many kits as the same time period in 2016, an amount they’d previously reported as 560,000. Going into the long weekend, the company had sold slightly more than 6 million tests since launching the product in 2012. 23andMe declined to give any financial details, but thanks in part to a big price cut, its health test was one of Amazon’s five best-selling items on Black Friday, behind the Amazon Echo Dot, two other Alexa add-ons, and a programmable pressure cooker.
Amazon has become an increasingly important sales channel for both Ancestry and 23andMe in the two years since they began selling in the "home tests" section of the two-click shopping platform. But it was particularly huge for Ancestry when the aforementioned pressure cooker sold out late in the day on Monday. “From that moment you could just see it take off like a hockey stick,” says Ancestry executive vice president and general manager Ken Chahine, still surprised.
A Single $249 Test Analyzes 30 Cancer Genes. But Do You Need It?
How 23andMe Won Back the Right to Foretell Your Diseases
Illumina, the Google of Genetic Testing, Has Plans for World Domination
But not as surprised, he says, as Amazon. “They didn’t expect us to sell that much, so they moved a bunch of inventory out of the distribution centers to cold storage, probably in the middle of nowhere, and then they had to go track it all down, and for a while nobody knew where it was,” he says. It’s since been sorted out. But if you ordered a kit and it hasn’t come yet, at least now you know why.
Both Ancestry and 23andMe have acknowledged the criticism that has come with more widespread use of their products. But the companies maintain that their customers understand the trade-offs and have the opportunity to opt out at any time. When I interviewed Ancestry’s chief scientific officer Catherine Ball at the Commonwealth Club in July, the majority questions from the audience focused on issues of privacy and third-party access. “We do not own or assert any ownership over your genetics,” she told the crowd of about 100. “We just see ourselves as stewards and only do that which our customers have consented us to do.”
On Sunday night, in response to Schumer’s remarks, Kate Black, 23andMe’s privacy officer and corporate counsel, told NBC News something similar: “We do not sell individual customer information, nor do we include any customer data in our research program without an individual’s voluntary and informed consent. 23andMe customers are in control of their data—customers can choose to consent, or not to, at any time.”
Critics like Pitts say that’s “true but not accurate,” if you dig into the fine print. Which, he fears, people will spend even less time doing if they get the tests from a friend or relative. “That comes with an implicit endorsement, so people are likely to pay even less attention to the potential risks,” he says. A genetic test won’t shoot your eye out, but it should be handled with care.
The ability to edit DNA is already here, but the effects of doing so are hardly understood. Genetics experts explain the potential pitfalls and alluring upside of using the technology on athletes.


Over the course of thousands of years, dogs have evolved alongside humans to be awesome. Unlike their wild ancestors, they don't gnaw on us (usually). They stick up for us. They go to the bathroom in designated areas. They're unrivaled companions. Looking at you disappointingly, cats.
Now, though, they have competition. The companion robots that science fiction has promised us for so long have finally hit the United States, and leading the way is an adorable little machine called Kuri. Made by Mayfield Robotics in Silicon Valley, Kuri—which starts shipping in December—will roll around your house and respond to your voice and recognize faces and take video of your dinner parties, if you're into that sort of thing.
Not exactly as useful as Rosie from The Jetsons, but hey, it's early days. And the design evolution of Kuri, from prototype to consumer product, provides a fascinating glimpse into a new era of interaction between humans and increasingly sophisticated machines. The question is: Do humans need or even want this kind of thing? And are we prepared to form a new kind of bond with what is essentially a new kind of being?
Kuri sprang from the minds of roboticists Kaijen Hsiao and Sarah Osentoski, who didn’t actually start out to make a friendly robot. What they originally conceived of was a security robot that would patrol the home. Not to taser intruders, but to keep an eye out. The problem, the pair eventually realized, is that you're better off detecting someone while they’re still outside. “By the time someone gets in the home, well it's kind of too late, isn't it?” Hsiao says.
A robot with fewer responsibilities seemed a more logical starting point. So Hsiao and Osentoski began building a bot for companionship, instead of protection. But this approach introduced a slew of subtle problems, chief among them: How do you get this new technology to work in the home, while also winning the affection of its owners?
First of all, Kuri has to be able to navigate the house without acting like an idiot. To avoid obstacles, it maps the world with lasers, just like a self-driving car. This is the sort of thing that’s really making machines like Kuri possible—sensors are getting cheaper at the same time that they’re getting more powerful. You don't have to drop $10K on lidar to get your robot to see its world.
Where the design gets subtler is in the look of Kuri. In these early days of personal robotics, it’s important for manufacturers to nonverbally telegraph what their machines are capable of—for safety, in some cases, but mostly to avoid disappointing the user. Do not, for instance, expect Kuri to drag you out of a burning building.
“We try, through her form, to really communicate exactly what Kuri's capable of,” says Osentoski. “She doesn't have arms because she's not going to be moving things around your home.”
Another consideration is how Kuri communicates. We humans tend to anthropomorphize anything that seems even remotely lifelike. Accordingly, Kuri’s designers decided that it shouldn’t speak human. “This is because when you have something that's talking to you that's driving around your house you start to expect a lot more,” says Osentoski. “You start to expect the intelligence of a 3-year-old or a 5-year-old.” Kuri just isn't there yet, mentally or physically, so users need to treat it as such.
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
At the same time, Mayfield Robotics wants Kuri to win your affection and become part of your family, and a lot of that comes down to the eyes. Watch any Pixar movie and you’ll notice how expressive the eyes are. Humans love eyes. So humans have to love Kuri’s peepers, which are actually mechanical. That limits their emotive potential compared to, say, just slapping a flatscreen on Kuri’s face, but they go a long way in telegraphing emotion for a robot that otherwise communicates with beeps and boops.
So, put it all together and you get an advanced, weirdly cute robot that’s quite remarkable to communicate with. It’s limited at the moment, sure, but as someone who’s interacted with Kuri, I can tell you that it conjures peculiar feelings. Rub its head and it looks up at you lovingly. I'm impressed by its apparent animalness, yet other times frustrated when it doesn't respond to my commands. I'm fully aware that it’s a machine meant to play with my emotions and expectations, but I don’t really care. In the end, I'm just not fully sure how to interact with it.
Whether humans are even ready for companion robots, though, remains to be seen. Think machines that strike up conversations with the elderly and even cuddle with them. “I don't believe in companion robots, I'm sorry,” says UC Berkeley roboticist Ken Goldberg. “I don't think that that's actually what people want. If I'm lonely, the last thing I want is a robot to come in and somehow be my friend. That's even more depressing.”
Whether Americans fall in love with it or not, Kuri is a technical milestone. And it will only get smarter from here. This is just the beginning of a new kind of relationship that humanity is beginning to form with robots, so prepare for a lot of awkward moments—and powerful ones, too. And bonus: Kuri will never chew up your slippers or assault your postal carrier.
Sophisticated companion robots have arrived. But we need to be very careful about how we interact with them.


This story originally appeared on Grist and is part of the Climate Desk collaboration.
In a remote region of Antarctica known as Pine Island Bay, 2,500 miles from the tip of South America, two glaciers hold human civilization hostage.
Stretching across a frozen plain more than 150 miles long, these glaciers, named Pine Island and Thwaites, have marched steadily for millennia toward the Amundsen Sea, part of the vast Southern Ocean. Further inland, the glaciers widen into a two-mile-thick reserve of ice covering an area the size of Texas.
There’s no doubt this ice will melt as the world warms. The vital question is when.
The glaciers of Pine Island Bay are two of the largest and fastest-melting in Antarctica. (A Rolling Stone feature earlier this year dubbed Thwaites “The Doomsday Glacier.”) Together, they act as a plug holding back enough ice to pour 11 feet of sea-level rise into the world’s oceans—an amount that would submerge every coastal city on the planet. For that reason, finding out how fast these glaciers will collapse is one of the most important scientific questions in the world today.
To figure that out, scientists have been looking back to the end of the last ice age, about 11,000 years ago, when global temperatures stood at roughly their current levels. The bad news? There’s growing evidence that the Pine Island Bay glaciers collapsed rapidly back then, flooding the world’s coastlines—partially the result of something called “marine ice-cliff instability.”
The ocean floor gets deeper toward the center of this part of Antarctica, so each new iceberg that breaks away exposes taller and taller cliffs. Ice gets so heavy that these taller cliffs can’t support their own weight. Once they start to crumble, the destruction would be unstoppable.
“Ice is only so strong, so it will collapse if these cliffs reach a certain height,” explains Kristin Poinar, a glaciologist at NASA’s Goddard Space Flight Center. “We need to know how fast it’s going to happen.”
In the past few years, scientists have identified marine ice-cliff instability as a feedback loop that could kickstart the disintegration of the entire West Antarctic ice sheet this century—much more quickly than previously thought.
Minute-by-minute, huge skyscraper-sized shards of ice cliffs would crumble into the sea, as tall as the Statue of Liberty and as deep underwater as the height of the Empire State Building. The result: a global catastrophe the likes of which we’ve never seen.
Ice comes in many forms, with different consequences when it melts. Floating ice, like the kind that covers the Arctic Ocean in wintertime and comprises ice shelves, doesn’t raise sea levels. (Think of a melting ice cube, which won’t cause a drink to spill over.)
Land-based ice, on the other hand, is much more troublesome. When it falls into the ocean, it adds to the overall volume of liquid in the seas. Thus, sea-level rise.
Antarctica is a giant landmass—about half the size of Africa—and the ice that covers it averages more than a mile thick. Before human burning of fossil fuels triggered global warming, the continent’s ice was in relative balance: The snows in the interior of the continent roughly matched the icebergs that broke away from glaciers at its edges.
Now, as carbon dioxide traps more heat in the atmosphere and warms the planet, the scales have tipped.
A wholesale collapse of Pine Island and Thwaites would set off a catastrophe. Giant icebergs would stream away from Antarctica like a parade of frozen soldiers. All over the world, high tides would creep higher, slowly burying every shoreline on the planet, flooding coastal cities and creating hundreds of millions of climate refugees.
All this could play out in a mere 20 to 50 years—much too quickly for humanity to adapt.
“With marine ice cliff instability, sea-level rise for the next century is potentially much larger than we thought it might be five or 10 years ago,” Poinar says.
A lot of this newfound concern is driven by the research of two climatologists: Rob DeConto at the University of Massachusetts-Amherst and David Pollard at Penn State University. A study they published last year was the first to incorporate the latest understanding of marine ice-cliff instability into a continent-scale model of Antarctica.
Their results drove estimates for how high the seas could rise this century sharply higher. “Antarctic model raises prospect of unstoppable ice collapse,” read the headline in the scientific journal Nature, a publication not known for hyperbole.
Instead of a three-foot increase in ocean levels by the end of the century, six feet was more likely, according to DeConto and Pollard’s findings. But if carbon emissions continue to track on something resembling a worst-case scenario, the full 11 feet of ice locked in West Antarctica might be freed up, their study showed.
Three feet of sea-level rise would be bad, leading to more frequent flooding of US cities such as New Orleans, Houston, New York, and Miami. Pacific Island nations, like the Marshall Islands, would lose most of their territory. Unfortunately, it now seems like three feet is possible only under the rosiest of scenarios.
At six feet, though, around 12 million people in the United States would be displaced, and the world’s most vulnerable megacities, like Shanghai, Mumbai, and Ho Chi Minh City, could be wiped off the map.
At 11 feet, land currently inhabited by hundreds of millions of people worldwide would wind up underwater. South Florida would be largely uninhabitable; floods on the scale of Hurricane Sandy would strike twice a month in New York and New Jersey, as the tug of the moon alone would be enough to send tidewaters into homes and buildings.
DeConto and Pollard’s breakthrough came from trying to match observations of ancient sea levels at shorelines around the world with current ice sheet behavior.
Around 3 million years ago, when global temperatures were about as warm as they’re expected to be later this century, oceans were dozens of feet higher than today.
Previous models suggested that it would take hundreds or thousands of years for sea-level rise of that magnitude to occur. But once they accounted for marine ice-cliff instability, DeConto and Pollard’s model pointed toward a catastrophe if the world maintains a “business as usual” path—meaning we don’t dramatically reduce carbon emissions.
Rapid cuts in greenhouse gases, however, showed Antarctica remaining almost completely intact for hundreds of years.
Pollard and DeConto are the first to admit that their model is still crude, but its results have pushed the entire scientific community into emergency mode.
“It could happen faster or slower, I don’t think we really know yet,” says Jeremy Bassis, a leading ice sheet scientist at the University of Michigan. “But it’s within the realm of possibility, and that’s kind of a scary thing.”
Scientists used to think that ice sheets could take millennia to respond to changing climates. These are, after all, mile-thick chunks of ice.
The new evidence, though, says that once a certain temperature threshold is reached, ice shelves of glaciers that extend into the sea, like those near Pine Island Bay, will begin to melt from both above and below, weakening their structure and hastening their demise, and paving the way for ice-cliff instability to kick in.
In a new study out last month in the journal Nature, a team of scientists from Cambridge and Sweden point to evidence from thousands of scratches left by ancient icebergs on the ocean floor, indicating that Pine Island’s glaciers shattered in a relatively short amount of time at the end of the last ice age.
The only place in the world where you can see ice-cliff instability in action today is at Jakobshavn glacier in Greenland, one of the fastest-collapsing glaciers in the world. DeConto says that to construct their model, they took the collapse rate of Jakobshavn, cut it in half to be extra conservative, then applied it to Thwaites and Pine Island.
But there’s reason to think Thwaites and Pine Island could go even faster than Jakobshavn.
For Scientists Predicting Sea Level Rise, Wind Is the Biggest Unknown
The Arctic Is Melting, and Fast. But Maybe Data Can Save It
Giant Antarctic Icebergs and Crushing Existential Dread
Right now, there’s a floating ice shelf protecting the two glaciers, helping to hold back the flow of ice into the sea. But recent examples from other regions, like the rapidly collapsing Larsen B ice shelf on the Antarctic Peninsula, show that once ice shelves break apart as a result of warming, their parent glaciers start to flow faster toward the sea, an effect that can weaken the stability of ice further inland, too.
“If you remove the ice shelf, there’s a potential that not just ice-cliff instabilities will start occurring, but a process called marine ice-sheet instabilities,” says Matthew Wise, a polar scientist at the University of Cambridge.
This signals the possible rapid destabilization of the entire West Antarctic ice sheet in this century. “Once the stresses exceed the strength of the ice,” Wise says, “it just falls off.”
And, it’s not just Pine Island Bay. On our current course, other glaciers around Antarctica will be similarly vulnerable. And then there’s Greenland, which could contribute as much as 20 feet of sea-level rise if it melts.
Next to a meteor strike, rapid sea-level rise from collapsing ice cliffs is one of the quickest ways our world can remake itself. This is about as fast as climate change gets.
Still, some scientists aren’t fully convinced the alarm is warranted. Ted Scambos, lead scientist at the National Snow and Ice Data Center in Colorado, says the new research by Wise and his colleagues, which identified ice-cliff instabilities in Pine Island Bay 11,000 years ago, is “tantalizing evidence.” But he says that research doesn’t establish how quickly it happened.
“There’s a whole lot more to understand if we’re going to use this mechanism to predict how far Thwaites glacier and the other glaciers are going to retreat,” he says. “The question boils down to, what are the brakes on this process?”
Scambos thinks it is unlikely that Thwaites or Pine Island would collapse all at once. For one thing, if rapid collapse did happen, it would produce a pile of icebergs that could act like a temporary ice shelf, slowing down the rate of retreat.
Despite the differences of opinion, however, there’s growing agreement within the scientific community that we need to do much more to determine the risk of rapid sea-level rise. In 2015, the US and UK governments began to plan a rare and urgent joint research program to study Thwaites glacier. Called “How much, how fast?,” the effort is set to begin early next year and run for five years.
Seeing the two governments pooling their resources is “really a sign of the importance of research like this,” NASA’s Poinar says.
Given what’s at stake, the research program at Thwaites isn’t enough, but it might be the most researchers can get. “Realistically, it’s probably all that can be done in the next five years in the current funding environment,” says Pollard.
He’s referring, of course, to the Trump administration’s disregard for science and adequate scientific funding; the White House’s 2018 budget proposal includes the first-ever cut to the National Science Foundation, which typically funds research in Antarctica.
“It would be sensible to put a huge effort into this, from my perspective,” Pollard says. Structural engineers need to study Antarctica’s key glaciers as though they were analyzing a building, he says, probing for weak spots and understanding how exactly they might fail. “If you vastly increase the research now, [the cost] would still be trivial compared to the losses that might happen.”
Bassis, the ice sheet scientist at the University of Michigan, first described the theoretical process of marine ice-cliff instability in research published only a few years ago.
He’s 40 years old, but his field has already changed enormously over the course of his career. In 2002, when Bassis was conducting his PhD research in a different region of Antarctica, he was shocked to return to his base camp and learn that the Larsen B ice shelf had vanished practically overnight.
“Every revision to our understanding has said that ice sheets can change faster than we thought,” he says. “We didn’t predict that Pine Island was going to retreat, we didn’t predict that Larsen B was going to disintegrate. We tend to look at these things after they’ve happened.”
There’s a recurring theme throughout these scientists’ findings in Antarctica: What we do now will determine how quickly Pine Island and Thwaites collapse. A fast transition away from fossil fuels in the next few decades could be enough to put off rapid sea-level rise for centuries. That’s a decision worth countless trillions of dollars and millions of lives.
“The range of outcomes,” Bassis says, “is really going to depend on choices that people make.”
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.


Hunter Williams used to be an English teacher. Then, three years into that job, he started reading the book The Moon Is a Harsh Mistress. The 1966 novel by Robert Heinlein takes place in the 2070s, on the moon, which, in this future, hosts a subterranean penal colony. Like all good sci-fi, the plot hinges on a rebellion and a computer that gains self-awareness. But more important to Williams were two basic fictional facts: First, people lived on the moon. Second, they mined the moon. “I thought, ‘This is it. This is what we really could be doing,” he says.
Today, that vision is closer than ever. And Williams is taking steps to make it reality. This year, he enrolled in a class called Space Resources Fundamentals, the pilot course for the first-ever academic program specializing in space mining. It's a good time for such an education, given that companies like Deep Space Industries and Planetary Resources are planning prospecting missions, NASA's OSIRIS-REx is on its way to get a sample of an asteroid and bring it back to Earth, and there's international and commercial talk of long-term living in space.
Williams had grown up with the space-farers on Star Trek, but he found Heinlein’s vision more credible: a colony that dug into and used the resources of their celestial body. That's the central tenet of the as-yet-unrealized space mining industry: You can't take everything with you, and, even if you can, it's a whole lot cheaper not to—to mine water to make fuel, for instance, rather than launching it on overburdened rockets. “I saw a future that wasn't a hundred or a thousand years away but could be happening now,” says Williams.
So in 2012, he adjusted trajectory and went to school for aerospace engineering. Then he worked at Cape Canaveral in Florida, doing ground support for Lockheed Martin. His building, on that cosmic coast, was right next to one of SpaceX's spots. “Every day when I came to work, I would see testaments to new technology,” he says. “It was inspiring.”
A few years later, he still hadn't let go of the idea that humans could work with what they found in space. Like in his book. So he started talking to Christopher Dreyer, a professor at the Colorado School of Mines’ Center for Space Resources, a research and technology development center that's existed within the school for more than a decade.
It was good timing. Because this summer, Mines announced its intention to found the world’s first graduate program in Space Resources—the science, technology, policy, and politics of prospecting, mining, and using those resources. The multidisciplinary program would offer Post-Baccalaureate certificates and Masters of Science degrees. Although it's still pending approval for a 2018 start date, the school is running its pilot course, taught by Dreyer, this semester.
Williams has committed fully: He left his Canaveral job this summer and moved to Colorado to do research for Dreyer, and hopefully start the grad program in 2018.
Williams wasn't the only one interested in the future of space mining. People from all over, non-traditional students, wanted to take Space Resources Fundamentals. And so Dreyer and Center for Space Resources director Angel Abbud-Madrid decided to run it remotely, ending up with about 15 enrollees who log in every Tuesday and Thursday night for the whole semester. Dreyer has a special setup in his office for his virtual lectures: a laptop stand, a wall of books behind him, a studio-type light that shines evenly.
In the minutes before Thanskgiving-week class started, students' heads popped up on Dreyer's screen as they logged in. Some are full-time students at Mines; some work in industry; some work for the government. There was the employee from the FAA’s Office of Commercial Space Transportation, an office tasked, in part, with making sure the US is obeying international treaties as they explore beyond the planet. Then there’s Justin Cyrus, the CEO of a startup called Lunar Outpost. Cyrus isn’t mining any moons yet, but Lunar Outpost has partnered with Denver’s Department of Environmental Health to deploy real-time air-quality sensors, of the kind it hopes to develop for moony use.
Cyrus was a Mines graduate, with a master’s in electrical and electronics engineering; he sought out Dreyer and Abbud-Madrid when he needed advice for his nascent company. When the professors announced the space resources program, Cyrus decided to get in on this pilot class. He, and the other attendees, seem to see the class not just as an educational opportunity but also as a networking one: Their classmates, they say, are the future leaders of this industry.
Asteroid Mining Sounds Hard, Right? You Don’t Know the Half of It
Luxembourg's Bid to Become the Silicon Valley of Space Mining
Space Mining Could Set Off a Star War
Cyrus, the FAA employee, and Williams all smiled from their screens in front of benign backgrounds. About a dozen other students—all men—joined in by the time class started. The day's lesson, about resources on the moon, came courtesy of scientist Paul Spudis, who live-broadcasted from a few states away. Spudis, a guest lecturer, showed charts and maps and data about resources the moon might harbor, and where, and their worth. He's bullish on the prospects of prospecting. Toward the end of his talk, he said, "I think we'll have commercial landings on the moon in the next year or so." Indeed, the company Moon Express is planning to land there in 2018, in a bid to win the Google Lunar X Prize.
Back during Halloween week, the class covered the Outer Space Treaty, a creation of the United Nations that governs outer-space actions and (in some people's interpretations) makes the legality of space mining dubious. The lecture was full of policy detail, but the students drove the ensuing Q&A toward the sociological. Space mining would disproportionately help already-wealthy countries, some thought, despite talk in the broader community about how space mining lowers the barrier to space entry.
In this realism, and this thoughtfulness, Dreyer's class is refreshing. The PR talk of big would-be space mining companies like Planetary Resources and Deep Space Industries can be slick, uncomplicated, and (sometimes) unrealistic. It often skips over the many steps between here and self-sustaining space societies—not to mention the companies' own long-term viability.
But in Space Resource Fundamentals, the students seem grounded. Student Nicholas Proctor, one of few with a non-engineering background, appreciates the  pragmatism. Proctor studied accounting as an undergrad and enrolled at Mines in mineral economics. After he received a NASA grant to study space-based solar power and its applications to the mining industry, Abbud-Madrid sent him an email telling him about the class. The professor thought it would be a good fit—and Proctor obviously agreed.
After Thanksgiving-week class was over, students logged off, waving one-handed goodbyes. Williams had been watching from the lab downstairs, in a high-tech warehouse-garage combo. There, he and other students work among experiments about how dust moves in space, and what asteroids are actually like. Of course, they're also interested in how to get stuff—resources—out of them. An old metal chamber dominates the room, looking like an unpeopled iron lung. "The big Apollo-era chamber is currently for asteroid mining," Williams explained, "breaking apart rocks with sunlight and extracting the water and even precious metals."
While Williams closed up class shop downstairs, Dreyer and Abbud-Madrid hung out in Dreyer's office for a few minutes. Dreyer, leaning back in his well-lit chair, talked bemusedly about some of the communications they receive. “We get interest from people to find out what they can mine and bring back to Earth and become a trillionaire,” he said.
That’s not really what the Space Resources program is about, in part because it’s not clear that’s possible—it’s expensive to bring the precious (to bring anything) back to Earth. The class focus—and, not coincidentally, the near-term harvest—is the H2O, which will stay in space, for space-use. “No matter how complex our society becomes, it always comes back to water,” said Abbud-Madrid. He laughed. “We’re going to the moon,” he continued. “For water.”
NASA announced the next step in the plan to retrieve an asteroid boulder from a near-Earth asteroid and redirect it into a stable orbit around the moon to carry out human exploration missions, all in support of advancing the nation's journey to Mars.


The modern human obsession with beachfront property is nothing new. For tens of thousands of years, our kind has been bonded to the coast and its bounty of food. Inland is alright, too, but nothing matches the productivity of the sea.
The problem with coastal living is that while the food supply is relatively stable, sea levels are not. They've always risen and fallen as the climate changes over the millennia—and thanks to the hyper-productivity of the Industrial Age, they're in the middle of a pretty significant uptick. In the coming decades, rising sea levels could jeopardize untold billions of dollars in real estate and infrastructure along the world’s coasts and displace millions of people.
While climate change imperils humanity’s future, it also imperils its past. A new study out in PLOS One quantifies that in alarming detail: Just in the southeastern United States, a sea level rise of one meter would inundate thousands of archaeological sites, from Native American settlements to early European colonies.
The researchers began their archaeological accounting with what is, quite frankly, a mess of data. States are federally mandated to keep records of archaeological sites, but they don’t all go about it in the same way. They might collect different kinds of information, ranging from the sorts of artifacts found to the era of settlement.
So the researchers worked with an ongoing project called the Digital Index of North American Archaeology, an umbrella database that compiles this mess of data into something more manageable. Then it was a matter of marrying location data for sites in nine southeastern states, including Florida, Georgia, and the Carolinas, with elevation data—plotting which sites will be in trouble. (None of this location data, by the way, is particularly exact. Security purposes.)
The results aren’t encouraging. With a sea level rise of one meter—which could happen before the end of this century—the states would lose over 13,000 total sites, 4,000 in Florida alone. With a rise of 5 meters, the sites lost in the region would top 32,000.
It’s all a bit demoralizing, I know. But this research is an attempt to bring some order to what is quickly becoming chaos. “What we are hoping to get started is a conversation in American archaeology, and world archaeology," says co-author Josh Wells of Indiana University South Bend. "What are the effects of climate change on the record as we understand it, and to what extent do we need to triage and focus our efforts on recovering what we can before it's gone?”
They'll need to work quickly, because the scourge has already begun. “This is ongoing now with sea levels slowly coming up, increased storm surges,” says study co-author David Anderson of the University of Tennessee. “We're seeing erosion of coastal archaeological sites.”
It’s the sheer scale here that’s so daunting. How do you save 13,000 individual sites? Well, you don’t. Archaeologists have to face the reality that if sea levels rise, they’re going to have to pick a select few sites to concentrate on, cataloging them as best they can—perhaps with slick new digital techniques—and doing whatever possible to ensure their survival.
You might build a sea wall around a particularly important site, for instance, but that could endanger other sites. “When you build these barriers you're typically taking soil,” says Anderson. “You're taking materials from locations that themselves contain archaeological and historical resources.”
Could San Francisco Get the Oil Industry to Pay for Climate Change?
What We’ve Learned About Climate Change Since Hurricane Sandy
Four Radical Plans to Save Civilization From Climate Change
Theoretically, you could move structures as well. The Egyptians did this with the Abu Simbel temples when a new dam threatened to submerge them in the 1960s. But what do you choose to save? Who chooses what to save? (Irony among ironies: Washington, DC and its many cultural sites may eventually be at risk, the study notes. The Lincoln Memorial sits at 10 meters above sea level, which seems like a lot until you factor in the threat of floods. So the government may need to move monuments one day, long after the man who shunned the Paris Agreement has left office.)
There’s also the human cost. Archaeological sites will be submerged, but so too will modern communities. This is also already happening. In 2016, the federal government announced it would move a band of the Biloxi-Chitimacha-Choctaw Native American tribe from coastal Louisiana, where rising waters were threatening to sweep the community away. Climate change forced them not just from their homes, but historically and culturally significant locations.
Relocations, though, can further complicate matters. As sea levels rise, more and more people will have to move inland. And development there could imperil the archaeological sites that would otherwise survive sea level rise.
Yeah, it's scary stuff. But this research is a big step toward making sea level rise not only more comprehensible, but manageable. After all, we've got a future to safeguard and a past to preserve.
Coral reefs are under threat, but measuring the loss has been difficult. To get a better and faster picture of coral health, researchers developed a new incredibly accurate 3D mapping system.
Music - "I Still Really Love You" By Abjo (http://abjo.bandcamp.com/)


It's almost always the last topic in the first semester of introductory physics—angular momentum. Best for last, or something? I've used this concept to describe everything from fidget spinners to standing double back flips to the movement of strange interstellar asteroids.
But really, what the heck is angular momentum?
Let me start with the following situation. Imagine that there are two balls in space connected by a spring. Why are there two balls in space? I don't know—just use your imagination.
Not only are these balls connected by a spring, but the red ball has a mass that is three times the mass of the yellow ball—just for fun.  Now the two balls are pushed such that they move around each other—just like this.
Yes, this is a numerical calculation. If you want to take a look at the code and play with it yourself (and you should), here it is. If you want all the details about how to make something like this, take a look at this post on the three body problem.
When we see stuff like these rotating spring-balls, we think about what is conserved—what doesn't change. Momentum is a good example of a conserved quantity. We can define momentum as:
Let me just make a plot of the total momentum as a function of time for this spring-ball system. Since momentum is a vector, I will have to plot one component of the momentum—just for fun, I will choose the x-coordinate. Here's what I get.
In that plot, the red curve is the x-momentum of the red (heavier) ball and the blue curve is for the yellow ball (yellow doesn't show up in the graph very well). The black line is the total momentum. Notice that as one object increases in momentum, the other object decreases. Momentum is conserved. You could do the same thing in the y-direction or the z-direction, but I think you get the idea.
What about energy?  I can calculate two types of energy for this system consisting of the balls and the spring. There is kinetic energy and there is a spring potential energy:
The kinetic energy depends on the mass (m) and velocity (v) of the objects where the potential energy is related to the stiffness of the spring (k) and the stretch (s). Now I can plot the total energy of this system. Note that energy is a scalar quantity, so I don't have to plot just one component of it.
The black curve is again the total energy. Notice that it is constant. Energy is also conserved.
But is there another conserved quantity that could be calculated? Is the angular velocity conserved? Clearly it is not. As the balls come closer together, they seem to spin faster. How about a quick check, using a plot of the angular velocity as a function of time.
Nope: Clearly, this is not conserved. I could plot the angular velocity of each ball—but they would just have the same value and not add up to a constant.
OK, but there is something else that can be calculated that will perhaps be conserved. You guessed it: It's called the angular momentum. The angular momentum of a single particle depends on both the momentum of that particle and its vector location from some point. The angular momentum can be calculated as:
Although this seems like a simple expression, there is much to go over.  First, the L vector represents the angular momentum—yes, it's a vector. Second, the r vector is a distance vector from some point to the object and finally the p vector represents the momentum (product of mass and velocity). But what about that "X"? That is the cross product operator. The cross product is an operation between two vectors that produces a vector result (because you can't use scalar multiplication between two vectors).
I don't want to go into a bunch of maths regarding the cross product, so instead I will just show it to you.  Here is a quick python program showing two vectors (A and B) as well as  A x B (you would say that as A cross B).
You can click and drag the yellow A vector around and see what happens to the resultant of A x B.  Also, don't forget that you can always look at the code by clicking the "pencil" icon and then click the "play" to run it. Notice that A X B is always perpendicular to both A and B—thus this is always a three-dimensional problem. Oh, you can also rotate the vectors by using the right-click or ctrl-click and drag.
But now I can calculate (and plot) the total angular momentum of this ball-spring system. Actually, I can't plot the angular momentum since that's a vector. Instead I will plot the z-component of the angular momentum. Also, I need to pick a point about which to calculate the angular momentum. I will use the center of mass for the ball-spring system.
There are some important things to notice in this plot. First, both the balls have constant z-component of angular momentum so of course the total angular momentum is also constant. Second, the z-component of angular momentum is negative. This means the angular momentum vector is pointing in a direction that would appear to be into the screen (from your view).
So it appears that this quantity called angular momentum is indeed conserved. If you want, you can check that the angular momentum is also conserved in the x and y-directions (but it is).
But wait! you say. Maybe angular momentum is only conserved because I am calculating it with respect to the center of mass for the ball-spring system. OK, fine. Let's move this point to somewhere else such that the momentum vectors will be the same, but now the r-vectors for the two balls will be something different. Here's what I get for the z-component of angular momentum.
Now you can see that the z-component for the two balls both individually change, but the total angular momentum is constant.  So angular momentum is still conserved.  In the end, angular momentum is something that is conserved for situations that have no external torque like these spring balls. But why do we even need angular momentum? In this case, we really don't need it. It is quite simple to model the motion of the objects just using the momentum principle and forces (which is how I made the python model you see).
But what about something else? Take a look at this quick experiment. There is a rotating platform with another disk attached to a motor.
What happens with the motor-disk starts to spin? Watch. (There's a YouTube version here.)
Again, angular momentum is conserved. As the motor disk starts to spin one way, the rest of the platform spins the other way such that the total angular momentum is constant (and zero in this case). For a situation like this, it would be pretty darn difficult to model this situation with just forces and momentum. Oh, you could indeed do it—but you would have to consider both the platform and the disk as many, many small masses each with different momentum vectors and position vectors. It would be pretty much impossible to explain with that method. However, by using angular momentum for these rigid objects, it's not such a bad physics problem.
In the end, angular momentum is yet another thing that we can calculate—and it turns out to be useful in quite a number of situations. If you can find some other quantity that is conserved in different situations, you will probably be famous. You can also name the quantity after yourself if that makes you happy.
Charlotte Drury, Maggie Nichols, and Aly Raisman talk to WIRED about the skill, precision, and control they employ when performing various Gymnastic moves and when training for the Olympics.


In 1891, a New York doctor named William B. Coley injected a mixture of beef broth and Streptococcus bacteria into the arm of a 40-year-old Italian man with an inoperable neck tumor. The patient got terribly sick—developing a fever, chills, and vomiting. But a month later, his cancer had shrunk drastically. Coley would go on to repeat the procedure in more than a thousand patients, with wildly varying degrees of success, before the US Food and Drug Administration shut him down.
Coley’s experiments were the first forays into a field of cancer research known today as immunotherapy. Since his first experiments, the oncology world has mostly moved on to radiation and chemo treatments. But for more than a century, immunotherapy—which encompasses a range of treatments designed to supercharge or reprogram a patient’s immune system to kill cancer cells—has persisted, mostly around the margins of medicine. In the last few years, though, an explosion of tantalizing clinical results have reinvigorated the field and plunged investors and pharma execs into a spending spree.
Though he didn’t have the molecular tools to understand why it worked, Coley’s forced infections put the body’s immune system into overdrive, allowing it to take out cancer cells along the way. While the FDA doesn’t have a formal definition for more modern immunotherapies, in the last few years it has approved at least eight drugs that fit the bill, unleashing a flood of money to finance new clinical trials. (Patients had better come with floods of money too—prices can now routinely top six figures.)
But while the drugs are dramatically improving the odds of survival for some patients, much of the basic science is still poorly understood. And a growing number of researchers worry that the sprint to the clinic offers cancer patients more hype than hope.
When immunotherapy works, it really works. But not for every kind of cancer, and not for every patient—not even, it turns out, for the majority of them. “The reality is immunotherapy is incredibly valuable for the people who can actually benefit from it, but there are far more people out there who don’t benefit at all,” says Vinay Prasad, an Oregon Health and Science University oncologist.
Prasad has come to be regarded as a professional cancer care critic, thanks to his bellicose Twitter style and John Arnold Foundation-backed crusade against medical practices he says are based on belief, not scientific evidence. Using national cancer statistics and FDA approval records, Prasad recently estimated the portion of all patients dying from all types of cancer in America this year who might actually benefit from immunotherapy. The results were disappointing: not even 10 percent.
Now, that’s probably a bit of an understatement. Prasad was only looking at the most widely used class of immunotherapy drugs in a field that is rapidly expanding. Called checkpoint inhibitors, they work by disrupting the immune system’s natural mechanism for reining in T cells, blood-borne sentinels that bind and kill diseased cells throughout the body. The immune cells are turned off most of the time, thanks to proteins that latch on to a handful of receptors on their surface. But scientists designed antibodies to bind to those same receptors, knocking out the regulatory protein and keeping the cells permanently switched to attack mode.
The first checkpoint inhibitors just turned T cells on. But some of the newer ones can work more selectively, using the same principle to jam a signal that tumors use to evade T cells. So far, checkpoint inhibitors have shown near-miraculous results for a few rare, previously incurable cancers like Hodgkin’s lymphoma, renal cell carcinoma, and non-small cell lung cancer. The drugs are only approved to treat those conditions, leaving about two-thirds of terminal cancer patients without an approved immunotherapy option.
But Prasad says that isn’t stopping physicians from prescribing the drugs anyway.
“Hype has encouraged rampant off-label use of checkpoint inhibitors as a last-ditch effort,” he says—even for patients with tumors that show no evidence they’ll respond to the drugs. The antibodies are available off the shelf, but at a list price near $150,000 per year, it’s an investment Prasad says doctors shouldn’t encourage lightly. Especially when there’s no reliable way of predicting who will respond and who won’t. “This thwarts one of the goals of cancer care," says Prasad. "When you run out of helpful responses, how do you help a patient navigate what it means to die well?”
Merck and Bristol-Myers Squibb have dominated this first wave of immunotherapy, selling almost $9 billion worth of checkpoint inhibitors since they went on sale in 2015. Roche, AstraZeneca, Novartis, Eli Lilly, Abbvie, and Regeneron have all since jumped in the game, spending billions on acquiring biotech startups and beefing up in-house pipelines. And 800 clinical trials involving a checkpoint inhibitor are currently underway in the US, compared with about 200 in 2015. “This is not sustainable,” Genentech VP of cancer immunology Ira Mellman told the audience at last year’s annual meeting of the Society for Immunotherapy of Cancer. With so many trials, he said, the industry was throwing every checkpoint inhibitor combination at the wall just to see what would stick.
After more than a decade stretching out the promise of checkpoint inhibitors, patients—and businesses—were ready for something new. And this year, they got it: CAR T cell therapy. The immunotherapy involves extracting a patient’s T cells and genetically rewiring them so they can more efficiently home in on tumors in the body—training a foot soldier as an assassin that can slip behind enemy lines.
In September, the FDA cleared the first CAR-T therapy—a treatment for children with advanced leukemia, developed by Novartis—which made history as the first-ever gene therapy approved for market. A month later the agency approved another live cell treatment, developed by Kite Pharma, for a form of adult lymphoma. In trials for the lymphoma drug, 50 percent of patients saw their cancer disappear completely, and stay gone.
Scientists Save a Kid By Growing a Whole New Skin For Him
Gene Therapy Emerges From Disgrace to Be the Next Big Thing, Again
How Crispr Could Snip Away Some of Humanity's Worst Diseases
Kite’s ascendance in particular is a stunning indicator of how much money CAR-T therapy has attracted, and how fast. The company staged a $128 million IPO in 2014—when it had only a single late-phase clinical trial to its name—and sold to Gilead Science in August for $11.9 billion. For some context, consider that when Pfizer bought cancer drugmaker Medivation for $14 billion last year—one of the biggest pharma deals of 2016—the company already had an FDA-approved blockbuster tumor-fighter on the market with $2 billion in annual sales, plus two late-stage candidates in the pipeline.
While Kite and Novartis were the only companies to actually launch products in 2017, more than 40 other pharma firms and startups are currently building pipelines. Chief rival Juno Therapeutics went public with a massive $265 million initial offering—the largest biotech IPO of 2014—before forming a $1 billion partnership with Celgene in 2015. In the last few years, at least half a dozen other companies have made similar up-front deals worth hundreds of millions.
These treatments will make up just a tiny slice of the $107 billion cancer drug market. Only about 600 people a year, for example, could benefit from Novartis’ flagship CAR-T therapy. But the company set the price for a full course of treatment at a whopping $475,000. So despite the small clientele, the potential payoff is huge—and the technology is attracting a lot of investor interest. “CAR-T venture financing is still a small piece of total venture funding in oncology, but given that these therapies are curative for a majority of patients that have received them in clinical trials, the investment would appear to be justified,” says Mandy Jackson, a managing editor for research firm Informa Pharma Intelligence.
CAR-T, with its combination of gene and cell therapies, may be the most radical anticancer treatment ever to arrive in clinics. But the bleeding edge of biology can be a dangerous place for patients.
Sometimes, the modified T cells go overboard, excreting huge quantities of molecules called cytokines that lead to severe fevers, low blood pressure, and difficulty breathing. In some patients it gets even worse. Sometimes the blood-brain barrier inexplicably breaks down—and the T cells and their cytokines get inside patients’ skulls. Last year, Juno pulled the plug on its lead clinical trial after five leukemia patients died from massive brain swelling. Other patients have died in CAR-T trials at the National Cancer Institute and the University of Pennsylvania.
Scientists don’t fully understand why some CAR-T patients experience cytokine storms and neurotoxicity and others come out cured. “It’s kind of like the equivalent of getting on a Wright Brother’s airplane as opposed to walking on a 747 today,” says Wendell Lim, a biophysical chemist and director of the UC San Francisco Center for Systems and Synthetic Biology. To go from bumping along at a few hundred feet to cruise control at Mach 0.85 will mean equipping T cells with cancer-sensing receptors that are more specific than the current offerings.
Take the two FDA-approved CAR-T cell therapies, he says. They both treat blood cancers in which immune responders called B cells become malignant and spread throughout the body. Doctors reprogram patients’ T cells to seek out a B cell receptor called CD-19. When they find it, they latch on and shoot it full of toxins. Thing is, the reprogrammed T cells can’t really tell the difference between cancerous B cells and normal ones. The therapy just takes them all out. Now, you can live without B cells if you receive antibody injections to compensate—so the treatment works out fine most of the time.
But solid tumors are trickier—they’re made up of a mix of cells with different genetic profiles. Scientists have to figure out which tumor cells matter to the growth of the cancer and which ones don’t. Then they have to design T cells with antigens that can target just those ones and nothing else. An ideal signature would involve two to three antigens that your assassin T cells can use to pinpoint the target with a bullet instead of a grenade.
Last year Lim launched a startup called Cell Design Labs to try to do just that, as well as creating a molecular on-off-switch to make treatments more controlled. Only if researchers can gain this type of precise command, says Lim, will CAR-T treatments become as safe and predictable as commercial airline flight.
The field has matured considerably since Coley first shot his dying patient full of a dangerous bacteria, crossed his fingers, and hoped for the best. Sure, the guy lived, even making a miraculous full recovery. But many after him didn’t. And that “fingers crossed” approach still lingers over immunotherapy today.
All these years later, the immune system remains a fickle ally in the war on cancer. Keeping the good guys from going double-agent is going to take a lot more science. But at least the revolution will be well-financed.
CRISPR is a new biomedical technique that enables powerful gene editing. WIRED challenged biologist Neville Sanjana to explain CRISPR to 5 different people; a child, a teen, a college student, a grad student, and a CRISPR expert.


For the first time, humans have detected an interstellar asteroid—a space rock they're calling 'Oumuamua, which is a Hawaiian word meaning "scout." It's the only object we've ever seen that entered the solar system from beyond our little collection of planets. That's a pretty big deal on its own. But on top of that, this asteroid has a really interesting shape: It's very long and skinny, with a width to length ratio of about 1 to 10.
Basically, it looks like a cigar—or at least that's what everyone is saying. The only images we have that show its shape in detail are artistic renderings. Because the asteroid is so relatively small and far away, you can't easily see it with a visible-light telescope.
But if you can't see it, how can you describe it? The answer to this (as in many situations in science) is to use indirect observations. The one thing that can be measured is the brightness of the object. Because this rock is also spinning, the light it reflects from the sun changes over time. By looking at the ratio of the brightest to weakest observations, you can get an estimate of largest to smallest size. If you estimate the albedo (a measure of reflectance), you can also estimate the total size.  Boom. There you have it—a cigar-shaped asteroid.
If you want to learn the answers to more "how do you know"-type questions about 'Oumuamua, check out this awesome NASA FAQ. But if you want to calculate some answers for yourself—well, just keep reading.
OK, everyone calm down. This isn't a page from Rendezvous with Rama—an Arthur C. Clarke novel that depicts an interstellar object that happens to be an alien starship. But what if it was a spaceship? Could its rotation make a type of artificial gravity?
From the data, we know the following two important things about 'Oumuamua: the length and the rotation rate. OK, we don't actually know the length, but it's somewhere between 200 and 400 meters. I'm going to use the bigger size. The rotation rate is known with more precision as it rotates once every 8.1 hours.
How would this make artificial gravity? Let me start with a diagram.
When this "spacecraft" is far away from a star or other massive objects, there is essentially no gravitational force acting on people inside. You can make fake gravity by spinning the spacecraft (as shown above). This spinning motion creates a force (labeled F above) that reproduces the effect of the floor pushing on you to counteract gravity. This force from the floor is what we feel on Earth. If you want a longer explanation, check out my older post about weight and weightlessness.
But why is there a force between the human (or maybe an alien) and the end of the rotating asteroid?  It's because the human is moving in a circle. In order to move in a circular motion, you need a force pointing towards the center of the circle. We often call this a centripetal force which means "center pointing force." The magnitude of this force depends on the mass of the object, the size of the circle and the rotation rate (ω).
Of course, different humans of different masses will have different forces—just like on Earth. So really it's just the acceleration that matters for artificial gravity; that's just the product of ω squared and the radius. If you get an acceleration of 9.8 m/s2, then you have successfully reproduced Earth-like gravity (but not really gravity—just apparent weight).
Now for the acceleration of the end of 'Oumuamua (it's sort of fun to say that out loud after you get used to it). This rock has a circular radius of 200 meters (I'm using the biggest value), but what about the angular velocity? Since it takes 8.1 hours for one rotation, ω would be 2 times π divided by 8.1 hours. Of course I really need this time in seconds, so that gives me:
Yes, that's a pretty slow rotation rate. Just for comparison, it takes the Earth about 24 hours to rotate. Now I can put this along with the radius to calculate the acceleration. This gives a centripetal acceleration of 9.3 x 10-6 m/s2 or an apparent weight that is just 9.48 x 10-7 of that on Earth—in other words, it's super tiny.
There, I just did the first homework question for you.  Now for some more. Note, some of these might require some complex calculations and estimations—that's what makes them fun.


"Nadie te quita lo bailado.” (No one can take from you what you’ve danced.)
For Federico Ardila, this Latin American expression epitomizes his approach to life and mathematics. It’s the driving force behind the parties he DJs in venues across the San Francisco Bay Area, where people dance till morning to the beats of his native Colombia. The dance floor is a place “where you have your freedom and you have your power, and nobody can take that away from you,” Ardila said.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
He taught the expression to his students at San Francisco State University, where he is a math professor, after giving them a punishingly hard exam. San Francisco State has a highly diverse student body, and Ardila, who just turned 40, is a prominent voice in the mathematics community about how to make students from underrepresented groups— such as women and people of color—feel that they belong. But on this occasion, as he looked around at his students’ demoralized faces, he knew he had missed the mark.
“Nadie te quita lo bailado,” Ardila told his students.
“I think that’s a very powerful message—that nobody can take away from you the joy that you’ve had doing mathematics,” he told Quanta Magazine in an interview last month. “And people can give you grades, but that’s not going to take away the freedom that you felt and the fulfillment that you felt.”
The expression also applies to Ardila’s research, though not always in ways he would have chosen. Four years ago in Portland, Oregon, a thief smashed his car window and made off with a backpack containing, as luck would have it, five years’ worth of work—all of Ardila’s notes from a sweeping new paper he was developing. Proofs, examples, counterexamples and conjectures were all gone.
But the thief couldn’t steal the mathematics Ardila had “danced” in his mind. Over the past few years, Ardila and his coauthor, Marcelo Aguiar of Cornell University, have painstakingly reconstructed their work unifying the geometric and algebraic sides of combinatorics—the study of discrete structures like a social network, a sudoku puzzle, or a phylogenetic tree. They finally posted their 113-page paper online in September, and in January Ardila will be presenting their work in an invited address at the Joint Mathematics Meetings, the biggest annual math conference in the United States.
Quanta spoke with Ardila at the Mathematical Sciences Research Institute in Berkeley, California, where he is visiting for the fall semester, about the mathematics he has danced and taught. The interview has been condensed and edited for clarity.
Your mathematical talent was identified quite early—in fourth grade, you got the highest score in your age group in a national math competition in Colombia.
It was actually my sister, Natalia, who first showed great promise in mathematics. I was just the little brother. She and my cousin Ana María, they both performed really, really well in this national math competition. And I think the organizers probably said, “OK, these two women are very good, and then here’s the little brother who’s coming along to the awards ceremony. Maybe he’s OK also.”
I feel like from a young age, they were paying attention to me. I never enjoyed mathematics in school very much, but my experience through the Math Olympics was much more creative and much more playful.
And it turned out that it was, as many of these spaces are, a very male-dominated space, and eventually both my sister and my cousin felt uncomfortable with this space. I mean, they’re doing amazing things now; my cousin is an engineer and my sister is a music pedagogy professor. But I do think it’s kind of interesting—that was a space where I felt very comfortable and that felt very nurturing to me, and it didn’t feel so to other people. It was a space that was very “othering” for them. I think that’s always served to remind me of the role of a mathematician, of an educator, in curating the culture of a place. That’s why that’s been such a theme in my work.
You’ve said that you were surprised to get into the Massachusetts Institute of Technology, where you did your undergraduate and doctoral studies. What’s the story there?
I had never heard of MIT. And it hadn’t crossed my mind to study abroad. I was already enrolled in the local university. But my classmate told me MIT had awesome financial aid and said the math there was really good. I wanted to learn more math, so I decided to play along and apply.
At that moment I was failing most of my classes in high school. It was not clear that I was going to graduate. I had a little bit of an attitude problem. I was very interested in a lot of things but I did not like being told, “Read this” or “Think this way.” I just kind of wanted to do things on my own terms.
I was failing, I think, six out of eight subjects. Had I known what MIT was, I should have known not to apply. There is no way I should have applied with that kind of transcript.
I like telling this story to my students because I think we often close doors to ourselves by thinking that we’re not eligible or that we’re not good enough. And especially if you’re somebody who feels “othered” in your discipline or who feels like you’re lacking confidence, it’s easy to close doors on yourself. There’s a lot of people in life who are ready to close doors for you, so you can’t do it for yourself.
When you came to the United States, as an undergraduate at MIT, it was your turn to feel like the “other.”
It’s not that anybody did anything to mistreat me or to doubt me or to explicitly make me feel unwelcome, but I definitely felt very different. I mean, my mathematical education was outstanding and I had fantastic access to professors and really interesting material, but I only realized in retrospect that I was extremely isolated.
There’s a system in place that makes certain people comfortable and others uncomfortable, I think just by the nature of who’s in the space. And I say that without wanting to point fingers, because I think you can be critical about the spaces that “other” you, but you also have to be critical about the ways in which you “other” other people.
I think because mathematics sees itself as very objective, we think we can just say, “Well, logically, this seems to make sense that we’re doing everything correctly.” I think sometimes we’re a little bit oblivious as to what is the culture of a place, or who feels welcome, or what are we doing to make them feel welcome?
So when I try to create mathematical spaces, I try to be very mindful of letting people be their full human selves. And I hope that will give people more access to tools and opportunities.
What are some of the ways you do that in your teaching?
In a classroom I’m the professor, and so in some sense I’m the culture keeper. And one thing that I try to do—and it’s a little bit scary and it’s not easy—is to really try to shift the power dynamic and make sure that students feel like equally powerful contributors to the place. I try to create spaces where we’re kind of together constructing a mathematical reality.
So, for example, I taught a combinatorics class, and in every single class every single student did something active and communicated their mathematical ideas to somebody else. The structure of the class was such that they couldn’t just sit there and be passive.
I believe in the power of music, and so I got each one of them to play a song for the rest for us at the beginning of each class. At the beginning it felt like this wild experiment where I didn’t know what was going to happen, but I was really moved by their responses.
Some of them would dedicate the song to their mom and talk about how whenever they’re studying math, they’re very aware that their mom worked incredibly hard to give them the opportunity to be the first ones in their family to go to college. Another student played this song in Arabic called “Freedom.” And she was talking about how in this day and age it’s very difficult for her to feel at home and welcome and free in this country, and how mathematics for her is a place where nobody can take her freedom away.
That classroom felt like no other classroom that I’ve ever taught in. It was a very human experience, and it was one of the richest math classrooms that I’ve had. I think one worries when you do that, “Are you covering enough mathematics?” But when students are engaged so actively and when you really listen to their ideas, then magic happens that you couldn’t have done by preparing a class and just delivering it.
Mathematics has this stereotype of being an emotionless subject, but you describe it in very emotional terms—for instance, in course curricula you promise your students a “joyful” experience.
I think doing mathematics is tremendously emotional, and I think that anybody who does mathematics knows this. I just don’t think that we have the emotional awareness or vocabulary to talk about this as a community. But you walk around this building and people are making these discoveries, and there are so many emotions going on—a lot of frustration and a lot of joy.
I think one thing that happens is we don’t acknowledge this as a culture—because mathematics is emotional in sometimes very difficult ways. It can really make you feel very bad about yourself sometimes. You can be pushing on something for six months and then have it collapse, and that hurts. I don’t think we talk about that hurt enough. And the joy of discovering something after six months of working on it is really deep.
Your own research is in combinatorics. And the paper you’ll be presenting at the Joint Mathematics Meetings connects two different ways of understanding combinatorial structures, through the lenses of geometry and algebra. How do those two approaches work?
When you look at the geometric side of things, suppose, for example, you want to study the permutations (the ways of rearranging a collection of objects). It’s pretty well known that if you have n objects, the number of ways of putting them in a row is n factorial (the product n(n-1)(n-2)…1). So it’s not a very interesting problem to count how many ways there are. But what is their inherent structure?
If you look at when two permutations are related to each other by just swapping two elements, then you start understanding not only how many there are but how are they related to each other. And then, when you say, “OK, let’s take all the permutations, and put an edge between two of them if they’re a swap away,” then you find that you get this beautiful shape that’s a polytope (a geometric object with flat sides). I think it’s completely surprising initially that the inherent relations between permutations are captured in this beautiful polytope called a permutahedron. So all of a sudden you have this geometric model, and you can use tools from polytope theory to try to say new things about permutations. And that polytope has existed for a long time and is very well understood.
And then you can also think of permutations algebraically—there’s a natural sort of “multiplication” on permutations, in which the product of two permutations is the permutation you get by doing one permutation after the other.
This is one of the most important objects in algebra, this group of permutations.
The Mathematician Who Will Make You Fall in Love With Numbers
Unknown Mathematician Proves Elusive Property of Prime Numbers
Meet the Guy Who Sorts All the World's Numbers in His Attic
There are these two traditions, to take combinatorial objects and either make them geometric or make them algebraic. This project with Marcelo Aguiar was about trying to bring together these two points of view, and in fact we discovered that polytopes like the permutahedra have an additional related algebraic structure. I think we found a really beautiful connection between the geometric and the algebraic structure of combinatorial objects. We got a dozen results just from building this architecture and then reaping the benefits.
For many, though by no means all, mathematicians, teaching takes a back seat to research. But for you, it seems that teaching and research are very much intertwined. You often give your students open-ended problems, and you’ve co-authored many papers with students.
I love working with students. And I love sharing the joy of discovery with them. Most of my students are master’s and undergrad students, because San Francisco State doesn’t have a PhD program. I think that has steered my research to things that are more immediately accessible. But still, I need the questions to be very deep.
I’m very excited about my research. I think I’m doing the most interesting research of my life right now. People tell you at 40 you start declining, and I feel like I’m getting good now.
Almost a decade ago you founded a DJ collective based in Oakland. How does that tie into your work as a mathematician and teacher?
When I DJ I’m really looking for joy, and I’m looking to create an atmosphere where people can build bridges and connect. My professor side comes out a little bit, because I play a lot of stuff that people don’t know, and I try to play music from many places that they haven’t thought about.
I also see music as a tool for social change. Some of the events I do are social benefits—it’s about music, but it’s also about feeding the soul and getting ready for the change that you want to make in the world. It feels very similar to the kind of atmosphere that I try to create in a classroom. I see all of these things as being connected.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Seventeen-year-old Catherine Ray just received a $100,000 Thiel Fellowship. Armed with a B.S. in computational physics, the mathematics whiz is planning to fix everything that’s wrong with wheelchairs. Get to know the teen technorati.


Imagine a world without grapes. Someday greenhouses like the one above may be our last defense against such a fate. Beneath the glow of high-voltage lamps, dozens of crop samples grow at the Agricultural Experiment Station in Geneva, New York. Here, Cornell University scientists crossbreed domesticated crops with their wild ancestors to propagate superhardy strains that better withstand droughts, heat waves, and freezes. The facility is one of more than 50 such USDA-funded research stations nationwide, where scientists are studying climate-­resilient produce. They use techniques such as genotyping (scanning plant genomes to identify specific, beneficial genes) and tissue culture analysis to capture the desirable traits of feral plants—like heat or cold tolerance—and introduce them into common crops. In his new book, Human Nature, photographer Lucas Foglia documents these agricultural experiments as a rumination on the intersection of nature and technology. “It’s amazing to me that the future of our food is being developed in these simple greenhouses,” Foglia says. But the station’s understated exterior belies the advanced science within. Blight-proof peppers, disease-repelling grapes, and rot-resistant raspberries ripen just behind the frosty glass.
A pineapple shoot grows in a test tube at the National Laboratory for Genetic Resource Preservation in Fort Collins, Colorado. Plant samples are frozen as backup clones in the event of a destructive climate event or disease. “It’s a pineapple in case of disaster,” Foglia says.
This article appears in the December issue. Subscribe now.
How do we feed a growing population in areas where land is scarce? Produced for Mouser Electronics by the WIRED Brand Lab


I can’t sit here and promise you that the robot apocalypse isn’t coming, that the machines won’t eventually rise up and overthrow their makers. But what I can promise you is that not all of them will be able to punch you out. Because robots are going soft. Like, literally soft, controlled with liquid or air instead of traditional motors. It’s called soft robotics, naturally, and it’s hot at the moment.
Problem, though: Without the rigidity and powerful motors of your typical robot, soft robots have been weak. That is, until now. Researchers at the MIT Computer Science and Artificial Intelligence Laboratory and Harvard’s Wyss Institute have developed a new kind of soft robotic muscle inspired by origami and awesomeness. It’s essentially a bag filled with air, inside of which is an origami structure that functions as a skeleton. By pumping air in and out, the researchers can get the muscle to lift 1,000 times its own weight. By comparison, a more traditional robot arm like the super popular UR10, which weighs in at 64 pounds, can lift a third of its weight.
“That's really amazing, if you think about it, you lift 1,000 times more than what you weigh,” says roboticist Daniela Rus of MIT CSAIL. “So we think that this will be really transformational for how we use soft robots in manufacturing, in homes, in everyday life.”
The secret is that skeleton. Without it, you could get a soft robotic muscle to contract, sure. Think Baymax from Big Hero 6, who can deflate and squeeze through windows. But in reality, a soft robot like that wouldn’t be very strong. The skeleton gives you both strength and the ability to start messing with how the muscle moves.
So for instance, if you want to do a straight-up lifting motion, an accordion-style skeleton might work best. But you might also 3-D print other origami shapes to execute other maneuvers. “Through those different patterns, we're able to actuate rotational motions or twisting motions,” Rus says.
You could then also combine these different muscles as modules to form a more complicated robot, like the one at the top of this story. The fingers bend inward to grasp the tire, while the stem contracts upwards to lift.
The beauty of this kind of system is its literal and figurative flexibility. Just as you could combine different muscles, you could work with different materials for the skin. So you'd use silicone, for instance, to resist heat, but you could also use materials that dissolve in water to make a disposable aquatic robot.
Soft robots would get along with humans better, which means robots will collaborate with us, not outright steal our jobs. Sure, they'll be extremely strong, but they'll also be extremely sensitive. Robotic arms like the UR10 I mentioned earlier already stop if they make contact with a human, and expect the softies to do the same. So the soft robots of the near future will combine brawn and gentleness, making them even safer than "hard" devices. Like André the Giant, only with fewer sideburns.
The next step? Making a soft robotic elephant. Scaled down, of course. “I like the elephant trunk because it's such a sophisticated manipulation mechanism,” Rus says.
Let's also get a robotic avatar of André the Giant while we're at it. Maybe André riding an elephant. Yeah, there we go.
The robotic snake has 16 windpipe-constricting actuators, but this serpent bot is more likely to save your life than suck it out of you.


In Tampa, the conference center’s roof leaked. In Austin, the airport flooded. In Reno, conference organizers had to wait until a motorcycle rally was over before they could do some setup.
During preparation for the SC Conference, a supercomputing meeting, there’s always something getting in the way of networking. But the conference, held annually in November, is perhaps more sensitive to water, delays, and herds of bikes than your average gathering. Because every year, a group of volunteers shows up weeks in advance to build, from the literal ground up, the world’s fastest temporary network. The conference's attendees and exhibitors—from scientific researchers to industry bigwigs—need superfast, reliable connections to stream in the results of their simulations and data analysis. Called SCinet, the network “takes one year to plan, three weeks to build, one week to operate, and less than 24 hours to tear down,” according to its catchphrase.
After all, what good is high-performance computing if its results can’t reach the wider world?
This year, in Denver, one difficulty was elevation—not of the city itself, but of the exhibit hall. The 188 volunteers built up the networks' 13 equipment racks on the floor below the big, main space, constructing the infrastructure that could eventually handle around 3.6 terabits per second of traffic. (For reference, that's probably around 400,000 times more powerful than your personal connection.) And then, after construction, they had to move those millions of dollars of delicate equipment—down a hall, into an elevator, up a floor, and across the exhibit hall.
On November 8, volunteers moved the equipment on customized racklifts. “Welcome to the crazy,” someone said, unprompted, as he rushed past. The SCinetters moved like tightrope walkers, servers in tow, toward the elevators.
One floor up, a guy wearing a Scooby Doo hat pulled up with a forklift, gingerly skewered one rack, and began to lift it to the central stage. As the rack approached the platform, other volunteers put their hands on it, like digital pallbearers. When they were done, eight racks sat on the stage—the beating, blinking heart of the network. Among other duties, it coordinates with the five other racks scattered strategically around the room, ready for the exhibitors that needed 100 gigabit connections, and those requiring mere 1 or 10 gigabit hookups.
https://twitter.com/Supercomputing/status/930123674471895045
The demonstrations started on November 13. NASA brought out a simulation of how shockwaves from meteorites affect the atmosphere—and then how their effects reach the ground, from impacts to tsunamis. Also on board: a simulation showing how person-transporting drones could work, and a global weather prediction model. The Department of Energy presented about particle accelerators, quantum computing in science, and cancer surveillance.
The company Nyriad Limited, meanwhile, has aligned its stars with the International Centre of Radio Astronomy Research, to develop a "science data processing" operating system for a telescope called the Murchison Widefield Array, which itself is a precursor to the Square Kilometer Array. The Square Kilometer Array will require more computing power than any previous one: Its data rate will exceed today's global internet traffic. Nyriad, at the conference, revealed its first commercial offering, spun out of its SKA work: a fast and low-power storage solution useful beyond the world of astronomy.
But their talks would have been all talk were it not for the homebuilt network that let them show and tell. In the weeks leading up to the actual conference, the SCinet volunteers laid 60 miles of fiber and crafted 280 WiFi access points for the nearly 13,000 attendees and their attendant devices. Oh, also, they had to have a network service provider crack up a road to illuminate a dark fiber connection.
The Astonishing Engineering Behind America's Latest, Greatest Supercomputer
Why You Should Put Your Supercomputer in Wyoming
China's New Supercomputer Puts the US Even Further Behind
SCinet requires lots of physical and mental labor, but people keep coming back because it's their brand of fun—and the kind of professional development they could never get at an individual institution. “They get to touch and play with equipment that they normally wouldn't get to touch and play with in their day jobs,” says Jackie Kern, former general chair of the whole conference and of SCinet. They learn new networking tricks, bring back big-kid versions of their knowledge base, and meet some of the world’s top network types. “It’s a Rolodex moment,” says Jeffrey Schwab, current SCinet chair.
Also, it’s summer camp for people who like to tape fiber to floors. “Everyone wants to be here,” says Schwab.
And the organization is trying to help make it more welcoming to more different kinds of people. Kate Petersen Mace helps run the Women in IT Networking at SC program, which has fully funded 19 women volunteers' attendance since 2015 (around 22 percent of the total number of volunteers, this year, were women). In the male-dominated networking network, that kind of professional opportunity can be rare. Mace says she has often been the only woman in a given professional space. “I got kind of used to it and didn’t think about it,” she says. But the differences and the deficits snap into relief once there are more women in the exhibit hall (real and proverbial), watching the blinking lights on a set of server racks together alongside their male colleagues. "You feel more empowered to speak up," says Mace.
A few hours after the first rack lift, Jim Stewart of the Utah Education and Telehealth Network, who co-chairs the architecture team, treks up to the exhibit hall. All of the equipment is on stage, and SCinet volunteers have installed mirrors behind it, so passersby can appreciate the effort in all dimensions. It won’t last long, though. Remember the catchphrase? “…less than 24 hours to tear down.”
Stewart surveys the hall, thinking, apparently, of creation and destruction. “We’re not even done turning it up, and we are talking about getting out,” he says.
Leigh Orf, an an atmospheric scientist, narrates a simulation of a superstorm tornado created by one of the world's most powerful supercomputers.


Intelligence is not a quality to attribute lightly to microbes. There is no reason to think that bacteria, slime molds and similar single-cell forms of life have awareness, understanding or other capacities implicit in real intellect. But particularly when these cells commune in great numbers, their startling collective talents for solving problems and controlling their environment emerge. Those behaviors may be genetically encoded into these cells by billions of years of evolution, but in that sense the cells are not so different from robots programmed to respond in sophisticated ways to their environment. If we can speak of artificial intelligence for the latter, perhaps it’s not too outrageous to refer to the underappreciated cellular intelligence of the former.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Under the microscope, the incredible exercise of the cells’ collective intelligence reveals itself with spectacular beauty. Since 1983, Roberto Kolter, a professor of microbiology and immunobiology at Harvard Medical School and co-director of the Microbial Sciences Initiative, has led a laboratory that has studied these phenomena. In more recent years, it has also developed techniques for visualizing them. In the photographic essay book Life at the Edge of Sight: A Photographic Exploration of the Microbial World (Harvard University Press), released in September, Kolter and his co-author, Scott Chimileski, a research fellow and imaging specialist in his lab, offer an appreciation of microorganisms that is both scientific and artistic, and that gives a glimpse of the cellular wonders that are literally underfoot. Imagery from the lab is also on display in the exhibition World in a Drop at the Harvard Museum of Natural History. That display will close in early January but will be followed by a broader exhibition, Microbial Life, scheduled to open in February.
The slime mold Physarum polycephalum sometimes barely qualifies as a microorganism at all: When it oozes across the leaf litter of a forest floor during the active, amoeboid stage of its life cycle, it can look like a puddle of yellowish goo between an inch and a meter across. Yet despite its size, Physarum is a huge single cell, with tens of thousands of nuclei floating in an uninterrupted mass of cytoplasm. In this form, Physarum is a superbly efficient hunter. When sensors on its cell membrane detect good sources of nutrients, contractile networks of proteins (closely related to the ones found in human muscle) start pumping streams of cytoplasm in that direction, advancing the slime mold toward what it needs.
But Physarum is not just reflexively surging toward food. As it moves in one direction, signals transmitted throughout the cell discourage it from pushing counterproductively along less promising routes. Moreover, slime molds have evolved a system for essentially mapping their terrain and memorizing where not to go: As they move, they leave a translucent chemical trail behind that tells them which areas are not worth revisiting.
When bacteria were first observed through a microscope, suspended in liquid on slides, in their simplicity they seemed like the archetypes of primitive, solitary cells. The truth, however, is that in the wild, most bacteria are highly gregarious. Some bacteria do swim through their environment as lonely individuals but most bacterial cells—and most species of bacteria—prefer to live in compact societies called biofilms anchored to surfaces. (The individual swimmers often represent offshoots of biofilms, seeking to colonize new locations.)
Moreover, biofilms are not just dense accumulations of bacterial cells. They have elaborate functional structures, inside and out, that serve the cells’ collective destiny, as can be seen in the images below of Pseudomonas aeruginosa. The biofilm is stained with Congo red dye, which bonds to the extracellular matrix proteins that the bacteria secrete as a scaffolding for their community. The deeply wrinkled surface of the biofilm maximizes the area through which the bacteria can absorb oxygen; it also probably helps them collect nutrients and release waste products efficiently.
Within the biofilm, the bacteria divide the labor of maintaining the colony and differentiate into forms specialized for their function. In this biofilm of the common soil bacterium Bacillus subtilis, for example, some cells secrete extracellular matrix and anchor in place, while some stay motile; cells at the edges of the biofilm may divide for growth, while others in the middle release spores for surviving tough conditions and colonizing new locations.
One might wonder why natural selection would have favored this collective behavior instead of more rampant individualism among the cells. Part of the answer might be what evolutionary theorists call inclusive fitness: In so far as the bacteria within a biofilm are related, individual sacrifices are offset by the increases in fitness to each cell’s millions of cousins. But it may also be that every role within the biofilm has its advantages: Cells at the edge are most exposed to dangers and must reproduce furiously to expand the biofilm, but they also have access to the most nutrients and oxygen. Cells on the inside depend on others for their vital rations but they may survive longer.
The surfaces that biofilms grow across are not always solid. These B. subtilis are forming a pellicle—a kind of floating biofilm at the interface between water and air. The genetic pathways involved in forming a pellicle are essentially the same as those used in growing across stones, though they may respond to the changes in their habitat by altering the precise mix of proteins in the extracellular matrix as needed.
Expansive growth is not the only way in which microbial communities can move. Below, B. subtilis is engaging in a behavior called dendritic swarming, in which cells rapidly push outward in branching columns that can efficiently pave a surface. Biofilms swarm when they detect that they are in environments rich in nutrients: Swarming helps a biofilm exploit this valuable territory before any competing communities can.
At least two important changes in the differentiation of the cells in a biofilm take place to enable swarming. First, motile cells on the periphery of the film develop extra flagellae, which enables them to swim more energetically. Second, some edge cells also begin to secrete surfactant, a slippery material that helps the motile cells slide more rapidly over the surface.
When biofilms grow in flat laboratory dishes, the dendritic columns of swarming biofilms remain neatly distinct: They extend and coil in and around one another but they do not cross. That seems to be in part because the surfactant piles up around the biofilm branches as a barrier. Similarly, some bacteria can swarm in more terraced structures under laboratory conditions. What the implications of that option are for bacteria in nature is still a mystery.
Another type of behavior demonstrated by biofilms growing under laboratory conditions is spiral migration, demonstrated in the time-lapse video below of Bacillus mycoides. These bacterial cells grow in long chains or filaments that curl either clockwise or counterclockwise. The specific advantages of this spiraling movement are still under investigation, according to Chimileski, but they must be considerable because B. mycoides excels at taking over available environments. “Bacillus mycoides is one of the easiest bacterial species to cultivate from the soil,” he explained. When scientists isolate microbes from soil and grow them on agar dishes, particularly at room temperature, “the mycoides will often spread across the entire plate and overtake all of the other organisms. For this reason, it is considered if anything a kind of ‘nuisance species’ for many microbiologists.”
Swirling Bacteria Illuminate the Strange Physics of Swarms
Microbes May Rig Their DNA to Speed Up Evolution
The Mysterious Machinery of Creatures That Glow in the Deep
What’s curious is that the direction of the spiraling migration—clockwise or counterclockwise—seems to be a hereditary trait: Different strains of bacteria, even within the same species, spiral in different directions. It is yet another example of how bacteria, obeying instructions in their individual DNA, can manifest problem-solving behaviors that are surprisingly complex and adaptive at the collective level of biofilms.
These geometric and presumably functional patterns that biofilms produce in culture are intriguingly beautiful. Yet Chimileski notes that there is much left to discover when it comes to translating behaviors seen in the lab to natural microbial communities.
Chimileski points out that “most natural biofilms are multi-species ecosystems and cells inside natural biofilms usually grow more slowly.” He continued, “I like to think of the way we grow bacteria in a petri plate, where a single species is by itself and has everything it needs to grow at optimal temperatures, as ‘turning up the volume’ on the biology of the organism.” Under laboratory conditions, researchers can study which genes are involved in complex multicellular behaviors and they can measure the benefits to the fitness of the bacterial species. But in natural environments, biofilms don’t usually get to form exactly the same patterns as in the lab because of limited nutrients or competition with other species. “So the same biology might be occurring on a particle of soil in your backyard at smaller size scales and over longer time periods,” he said, even if it is less easy to visualize.
Biofilm behaviors testify to the capacity and openness of bacterial to form collectives—but that openness has limits, as shown in this culture with several cohabiting biofilms. Here, adjacent biofilms that consist of the same bacteria or closely related strains comfortably merge. But the adjacent biofilms made up of more divergent bacteria keep themselves distinct and may even try to eliminate or control each other.
Biofilms are so intolerant of other strains and species because they invest considerably in the production of surfactant, extracellular matrix and other molecules that bacteriologists classify as public goods—ones that the bacteria secrete for other members of their community. The bacteria guard these jealously because unrelated freeloading cells could benefit strongly by using them first.
Biofilms rebuff such freeloaders in different ways. For example, the B. subtilis colonies in this image adopt a strategy of “kin discrimination,” in which they secrete antibiotic compounds that are toxic to other species but not to their own. Proteus mirabilis bacteria defend their interests in a different way based on “self-recognition”: The P. mirabilis biofilms examine encroaching cells, stab any from a different species with a spearlike structure and inject them with poisons that will kill almost all but closely related species.
The colors appearing in the biofilm culture of Streptomyces coelicolor in the video below reflect natural pigments that the bacteria produce. The value of the pigments for the biofilms is not entirely clear, but it is probably not tied to their color. Rather, these pigment molecules are often bioactive in various ways. “The blue pigment seen in this video is actinorhodin, which is technically an antibiotic,” Chimileski said, but added that the term is misleading in this context. “Killing or growth inhibition usually occurs only at very high concentrations relative to what is out in nature.” For that reason, he said, there is “an emerging view that killing is probably not the ecological function of many or most antibiotics. Rather, these bioactive molecules act as signals or developmental cues” to other cells.
That view is echoed in a note from Gleb Pishchany, another research fellow in Kolter’s laboratory who studies how diverse types of bacteria cohabit. “An intriguing possibility is that in natural ecosystems, Streptomyces use pigments and other bioactive molecules” at “lower concentrations as signals that are exchanged among multispecies microbial communities,” he wrote. The pigments may help cohabiting assortments of bacteria rein in one another’s less neighborly instincts, and thereby maintain a more cooperative and fruitful communal existence.
These striking photographs of microbe communities were captured by DSLR cameras. Chimileski collects his still images with macro lenses while working at the bench, while the videos are made in an incubator dedicated to time-lapse microscopy. He sets the camera to snap a picture every 10 minutes, although he increases the frequency to every minute or two for behaviors happening more quickly, such as the movements of slime molds. As a result, the movements of the microbes in these videos are typically accelerated between 5,000 and 50,000 times their actual speeds. Chimileski does not use false color to beautify the images: Aside from using dyes to stain the extracellular matrix in some cultures, he shows the natural coloration of the microorganisms.
Chimileski typically grows bacterial colonies at 30°C, a temperature at which he can collect images of slower growing species for several weeks. Although the heat and humidity suited to biofilm growth are less than ideal for cameras, he said the equipment is rated for more extreme conditions. The few cameras that have malfunctioned did so for a mechanical reason: The number of shots that he needs to document microbial behaviors is so large that the shutters on the cameras eventually break down after hundreds of thousands of clicks.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Scientists are looking to an unlikely source for new ways to fight bacteria. Could the skin of a Galapagos shark hold the key to warding off hospital-born bacteria and superbugs?


In 2009, Dan Hooper and his colleagues found a glow coming from the center of our galaxy that no one had ever noticed before. After analyzing publicly available data from the Fermi Gamma Ray Space Telescope, a satellite launched a year earlier, the team concluded that the center of the Milky Way was radiating more gamma rays than astrophysicists could account for.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
The finding was so unexpected that, at the time, few believed that it was real. It didn’t help that Hooper wasn’t a member of the Fermi collaboration, but rather an outsider picking over the data that the Fermi team made public. One of the scientists working on Fermi called his work “amateurish,” arguing that Hooper simply didn’t know how to properly interpret the data.
Yet as time wore on, astrophysicists began to realize that there’s a lot more high-energy radiation streaming through the galaxy than they could explain. Just a year before Hooper started analyzing Fermi data, a gamma-ray detector in New Mexico called Milagro had found an abundance of super-energetic gamma rays that appeared to come from all across the galactic plane. And in 2014, the Alpha Magnetic Spectrometer, an experiment on the International Space Station, found more antimatter streaming through the galaxy than could be accounted for, confirming earlier observations by satellite and balloon experiments.
These three anomalies—if real—showed that something was going on in the universe that we didn’t know about. A number of astrophysicists, including Hooper, began to argue that two of these mysterious signals were an astrophysical echo of dark matter, the profoundly mysterious substance thought to make up about a quarter of the universe.
This year, almost a decade after the launch of the Fermi telescope, researchers have nearly arrived at a consensus. First, pretty much all astrophysicists now agree that the center of our Milky Way produces much more gamma radiation than our models of known gamma-ray sources suggest, said Luigi Tibaldo, an astrophysicist at Stanford University and member of the Fermi collaboration, thus validating Hooper’s once-“amateurish” claims.
Second, all that extra radiation is probably not due to dark matter. A number of recent studies have convinced many researchers that pulsars—rapidly spinning neutron stars—can explain all three mysteries.
The only problem is that no one seems to be able to find them.
The center of the galaxy is a crowded place, dense with stars, dust and—presumably—dark matter. Astrophysicists have long believed that dark matter is probably made out of particles that don’t readily interact with ordinary matter—so-called “weakly interacting massive particles,” or WIMPs. Occasionally these WIMPs might collide with one another. When they do, they could produce gamma rays. Perhaps that’s just what’s going on in the galactic center, Hooper suggested back in 2009.
The theory dovetailed with another idea that Hooper had put forward just a year earlier. In 2008, he and three co-authors published a paper arguing that collisions of neutralinos—a type of WIMP—generated showers of exotic particles that then decayed into elementary particles. The process would explain the anomalously high levels of positrons (the antimatter counterpart of electrons) found earlier by a space-based experiment called Pamela.
In this case, Hooper was in good company. Since Pamela’s first results, “without exaggeration” around 1,000 papers have tried to explain the positron excess mystery, said Tim Linden, an astrophysicist at Ohio State University. The majority of these papers favored the dark-matter interpretation. In 2014, the Pamela results were buttressed by data coming from the AMS.
Yet other scientists quickly started to poke holes in both of these dark-matter–based explanations. In the case of the galactic center, WIMP collisions should create a smooth, hazy glow of gamma rays, like a floodlight seen through thick fog. When astrophysicists examined the gamma-ray glow in detail, however, they found a pointillist patchwork of light. It appeared as though the gamma rays were coming from many individual point sources.
And if WIMPs were producing all those positrons, they should also be creating a lot of gamma rays. Yet when astronomers look out at nearby dwarf galaxies—thought to be home to a huge amount of dark matter—the gamma rays don’t appear.
The tension in these dark-matter models has forced astrophysicists to consider some more astrophysically prosaic options.
Even though most scientists are fairly certain that dark matter exists (even if we cannot directly observe it), the models are still considered exotic. What’s much less exotic are astrophysical sources of radiation that we can actually detect with our telescopes. So as the data began to undermine the case for dark matter, many researchers, including Hooper, began to contemplate a much more mundane explanation: pulsars.
Pulsars are ultra-dense, rapidly rotating objects—neutron stars, the dead cores of massive stars that have gone supernova. They emit jets of radiation that spin around with the pulsar like the beam from a lighthouse. As this beam crosses Earth, our telescopes register a flash of energy.
In 2015, two groups—one led by Christoph Weniger, an astrophysicist at the University of Amsterdam, and the other by Tracy Slatyer, a theoretical physicist at the Massachusetts Institute of Technology—separately presented evidence that gave the pulsar theory a major boost. Each team used slightly different methods, but essentially they both divided the region of the sky covering the galactic center into numerous pixels. They then counted the number of fluctuations in each pixel—watching, essentially, for lighthouse beams to swing across the face of Earth. The researchers discovered big differences between pixels—hot and cold patches in the sky, which are much easier to explain if one assumes that the signal comes from different point sources. “This is what you would expect from pulsars, because there could be brighter pulsars, or more pulsars, at some sky locations compared to others,” said Linden.
Most astrophysicists now think that the strange abundance of positrons in the galaxy may also be due to pulsars. Pulsars generate huge magnetic fields that spin along with the rest of the object. A spinning magnetic field will generate an electric field, and this electric field pulls electrons from the surface of the pulsar and accelerates them rapidly. As the electrons curve through the magnetic fields, the electrons will emit high-energy gamma rays. Some of this radiation is energetic enough to spontaneously morph into pairs of electrons and positrons that then escape from the pulsar’s strong magnetic grasp.
There are a lot of steps in this process, and a lot of uncertainty. Specifically, researchers want to know how much of the pulsar’s energy goes into making these electron-positron pairs. Is it a fraction of a percentage point? Or a significant total, something like 20 or even 40 percent of the pulsar’s energy? If the latter, pulsars might be making enough positrons to explain the antimatter excess.
Researchers had to find a way to measure the number of electrons and positrons coming out of pulsars. Unfortunately, this is an extremely difficult task. Electrons and positrons, being charged particles, will loop and twist their way through the galaxy. If you detect one from Earth, it’s hard to know where it came from.
Gamma rays, on the other hand, stick to a straight path. With this in mind, researchers working with the High-Altitude Water Cherenkov Gamma-Ray Observatory in Mexico have recently made detailed studies of two relatively bright and relatively nearby pulsars, Geminga and Monogem. They examined not just the gamma rays coming from the pulsar itself, but also the super-energetic gamma rays (1,000 times more energetic than the excess streaming from the galactic center) that appeared as a relatively broad halo around the pulsars. Throughout this halo, high-energy electrons coming from the pulsar collided with low-energy photons from ambient starlight. The collisions transferred huge amounts of energy to the poky photons, like a sledgehammer smashing golf balls into orbit.
Earlier this year, a team that included Hooper and Linden published a study that compared the brightness of the pulsars with the brightness of their halos. They concluded that 8 to 27 percent of Geminga’s energy had to be converted to electrons and positrons, said Linden. For Monogem, it was twice as much. “This means that pulsars produce a tremendous population of electrons and positrons within our galaxy,” said Linden.
Slatyer said the research is “the first time we’ve really had any handle on the spectrum of high-energy positrons produced by pulsars, so this is a big step forward.”
The work also helps to explain the strange excess of very-high-energy gamma rays that were found a decade ago by the Milagro detector in New Mexico. The radiation could be coming from pulsar-generated electrons and positrons accelerating ambient starlight.
One hurdle remains: finding enough pulsars to account for all the mysterious emission. “We should see about 50 [bright] pulsars in the galactic center to produce the excess,” said Linden. “Instead we’ve only found a handful.” Similarly, we don’t yet know of enough pulsars in the rest of the galaxy to explain away the positron excess or the abundance of ultra-high-energy gamma rays found by Milagro and HAWC.
The issue doesn’t bother pulsar proponents that much, though. They hope that in the near future a new generation of radio telescopes — such as MeerKAT in South Africa and its planned successor, the Square Kilometer Array in South Africa and Australia — will find the so far invisible radio sources in our galaxy.
Meet the Physicist Searching for Dark Matter’s Hidden Light
Squishy or Solid? A Neutron Star’s Insides Open to Debate
Physicists Want to Rebuild Quantum Theory From Scratch
So is the dark matter-vs.-pulsars debate settled? For positrons, it appears to be so. While many more researchers used to favor the dark matter interpretation originally, most now lean towards pulsars.
And in the galactic center, pulsars are “the Occam’s razor candidate,” said Slatyer. “You could explain the data just as well with a dark-matter-annihilation scenario, but we knew pulsars were there and we don’t know if dark matter annihilates, so you could consider the pulsar scenario to be simpler.”
According to Slatyer, the dark-matter explanation for the galactic center could yet make a comeback, and there is indeed another way to test the dark-matter hypothesis. When cosmic rays interact with interstellar material, and—in theory—during dark-matter annihilations, they produce antiprotons, the antiparticle twin of a proton. Pulsars cannot produce antiprotons. If researchers were to find more antiprotons than could be accounted for by cosmic rays, the discovery would boost the dark-matter scenario. This is exactly what preliminary results from AMS have shown: a possible excess of antiprotons that may be consistent with annihilating dark-matter particles. AMS scientists aren’t making any conclusions about the source of the antiprotons, but two papers came out this year arguing that dark matter could be behind the antiproton excess.
For Linden, the pulsar confirmation would mean even more. For decades, he said, when we have thought about the energetics of cosmic rays in our universe, we’ve always thought about supernovas, producing protons that then generate all of the cosmic rays detected. “We have had this really pretty picture where supernovas produce everything,” said Linden. “Everything links together and looks perfect.”
But in setting up that model, the energetics from pulsars are generally neglected, he added—despite pulsars’ being among the highest-energy objects in space. “So if this new picture holds up, and pulsars produce these excesses, then it really changes our interpretation of the source of most of the very energetic radiation in galaxies, and maybe throughout the universe,” said Linden.
It might be a case of Pulsars: 3, Dark Matter: 0, at least for now. “But I would be lying if I said I didn’t want these signals to turn out to be dark matter,” said Linden. “That would be so, so much more exciting.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
In honor of NASA's Juno mission photos, here are some astounding images of our galaxy taken over the years.


The holidays are prime reading season. All that travel and post-meal lounging will give you plenty of time to plow through the books that have been stacking up on your bedside table for the past year. Just be sure to grab a good one. Your body will already be full of empty calories—you don't want your brain to be stuffed with junk, too.
Endurance by Scott Kelly
When Scott Kelly returned to Earth in March of 2016, he became the proud possessor of a major space record: He had completed the single-longest spaceflight—340 days—ever undertaken by an American. In his new book, Endurance, the veteran astronaut dishes on what it takes to prepare for and live in space for that long. The upshot: Space is weird. And don't you want to know the details? Like, oh, I don't know, the fact that the International Space Station smells like jail? Yes. Yes you do.
Soonish by Kelly & Zach Weinersmith
Predicting the future is thankless and hard and often ill-advised—but that didn't stop scientist Kelly Weinersmith and cartoonist Zach Weinersmith from trying, with this wild glimpse into a future that may or may not involve space elevators and brain-computer interfaces and programmable matter. In it, they sift through mountains of literature and pick the brains of the researchers at the forefront of things like bioprinting (like 3-D printing, only more bio) and augmented reality (like reality, only more augmented), turning a skeptical yet exuberant eye toward the technologies of tomorrow.
Making Contact: Jill Tarter and the Search for Extraterrestrial Intelligence by Sarah Scoles
If humans ever find aliens—or if aliens ever find us—it will likely be a consequence of the work of Jill Tarter.  A former director of the SETI Institute, a formidable radio astronomer, and the inspiration for the main character in Carl Sagan's Contact, Tarter has devoted her life and work to the search for life in space. In Making Contact,  WIRED contributor Sarah Scoles interweaves Tarter's story with the science, philosophy, and politics that underpin the ongoing quest to understand life in the universe and humanity's place with in it—whether we're alone or not.
Salt, Fat, Acid, Heat by Samin Nosrat
A science-y cookbook? Yes. A science-y cookbook. Samin Nosrat’s ode to its eponymous components is light on recipes and heavy on insightful exposition vis-à-vis salt, fat, acid, and heat—the fundamental ingredients to any dish. By understanding how they act and interact, she claims, you can learn to not just follow instructions but truly cook, in that elusive, improvisational way you know you fantasize about. Outstanding illustrations, graphs, and charts from Wendy MacNaughton up the informational ante, without weighing down the culinary knowledge Nosrat dishes out.
Why Time Flies: A Mostly Scientific Investigation by Alan Burdick
Is there a more well-worn subject than time? I submit that there is not. It routinely ranks as the most commonly used noun in English. Aristotle ruminated at length on its subjectivity. And physicists, philosophers, and neuroscientists quibble over its qualities to this day. But in the hands of New Yorker staff writer Alan Burdick, the topic of time is revivified through this compelling mix of personal experiences and expertly translated research findings.
Spineless: The Science of Jellyfish and the Art of Growing a Backbone by Juli Berwald
Before she wrote for publications like National Geographic, Nature, and WIRED, Juli Berwald was an ocean scientist. But jellyfish—those gelatinous, pulsating, invertebrate aquatic creatures—drew her back to the sea. Jellyfish, it so happens, have experienced a population explosion in recent years, even as many other components of the oceans' ecosystems have floundered. In Spineless, Berwald sets out to understand why jellies have flourished, and what their unexpected response to a changing planet could mean for the rest of us.
The River of Consciousness by Oliver Sacks
The second of Oliver Sacks' posthumously published works (the first, Gratitude, comprised four essays the neurologist wrote following the cancer diagnosis that would end his life), The River of Consciousness collects 10 essays on subjects ranging from the fallibility of memory to Sigmund Freud's early studies on fish. Fans of the good doctor will appreciate the chance to revisit a collection he curated shortly before his death: The writings are stitched through with Sacks' characteristic curiosity and verve, weaving esoteric research, incisive observations, and intimate anecdotes into lucid expositions on the natural world and those who seek to understand it.
Big Chicken by Maryn McKenna
To understand modern agriculture, one must consider the chicken. In this wide-ranging exposé, journalist Maryn McKenna details how growth-boosting antibiotics plumped a little backyard bird into the fat fowl we know today. By tracing poultry's rise, the book reveals how the most popular meat in America cleared the way for industrial farming practices and a rapidly proliferating slate of drug-resistant diseases. Equal parts informative, propulsive, and frightening, McKenna's work is the kind of public health journalism you'll actually want to read.
Former NASA astronaut Scott Kelly uses the power of Twitter to answer some common questions about astronauts. How fast is the international space station? Can you see the eclipse from space?
Scott Kelly's book 'Endurance' is now available.


This Thanksgiving, you have a plan: You've signed up for a race. Nothing crazy, just a few miles, but what better way to offset the culinary onslaught of Turkey-day dinner than by torching a few calories first thing in the morning? That fourth slice of pumpkin pie isn't going to eat itself, dammit, and you're going to need somewhere to put it. So you're going to run a turkey trot.
You and everyone else in the country. "Thanksgiving is the most popular running day in the US," says Rich Harshbarger, CEO of the trade association Running USA. That's according to race results from Athlinks, the largest results database in the world (if you've completed a race since the turn of the millennium, there's probably a record of it on Athlinks, along with your official time, how you placed, and even what the weather was like on race day). And that popularity is only increasing: Last Thanksgiving, 961,882 people crossed a finish line at 726 local races across America—6 percent more than the previous year, and 18 percent more than the year before that.
In fact, more people run on Thanksgiving than any other day of the year, whether they're racing or not. Users of Strava, the social network for athletes, recorded nearly twice as many runs on Thanksgiving 2016 than the daily average. Imagine that: America's most gluttonous day of the year is also its most active.
Which, of course, is fine and good and admirable. Exercise is great. Exercising with friends and family is even better. "The whole sport of running has become more and more social and less and less solitary in recent years—and that's especially true of Turkey trots," Harshbarger says. "I mean, when else are you gonna get a million people in the United States active at one time?"
Inside the Weird World of Social Media Marathon Cheating
Do Nike's New Marathon Shoes Actually Make You Run Faster?
Fitness Isn't a Lifestyle Anymore. Sometimes It's a Cult
But if you think running in a local footrace is going to do much of anything to make up for your day of feasting, you should know that the math just isn't in your favor. The average woman burns about 126 calories per mile while running; the average man about 147. For a 5K (3.1-mile) race, that translates to 390 or 458 calories, respectively.
The average American consumes upwards of 4,500 calories on Thanksgiving. Forget 5Ks—that's about 1,000 calories more than the average person burns running a marathon. I'm not trying to dissuade you or anything. Some exercise is better than no exercise. But calorically speaking, counterbalancing your holiday gluttony with a spot of prophylactic jogging is like bailing out a boat with a teaspoon.
It's not all bad news, though. People do gain weight during the holidays, but not as much as you probably think. A 2000 study published in the New England Journal of Medicine found that test subjects gained an average of just 0.82 pounds between mid-November and early- to mid-January—yet those same test subjects thought they had gained four times as much weight as they actually had.
So no, your Turkey trot won't make up for the culinary crimes you are about to commit against your body. But the repercussions for those crimes are probably less severe than you think. So just enjoy your race—and the company of one million other happy trotters.
One of the world's finest distance runners came so close to achieving the greatest feats of athleticism in history: a sub two-hour marathon. To do it, the Eliud Kipchoge should have maintained an average pace of at least 13.1 miles per hour. So, we timed how long WIRED staffers could run at that speed. Needless to say, we didn't last long. Here's why only a handful of people in the world could ever come close to a two-hour marathon.


As I understand it, the whole point of cooking a turkey is to take it at some temperature and then increase it to a higher temperature. Sure, maybe there's something about family togetherness in there, but really, Thanksgiving is all about thermal transfer. The USDA recommends a minimum internal temperature of 165°F (74°C). I guess this is the minimum temperature to kill all the bad stuff in there—or maybe it is the lowest temperature that it can be and still taste great.
Either way, if you want to increase the temperature of the turkey you need to add energy. Perhaps this energy comes from fire, or an oven or even from hot oil—but it needs energy. But be careful. There is a difference between energy and temperature. Let me give you an example.
Suppose you put some leftover pizza in the oven to heat it up. Since you don't want to make a mess, you just rip off a sheet of aluminum foil and put the pizza on that and then into the oven. The oven is set to 350 degrees Fahrenheit so that after 10 minutes, both the pizza and the foil are probably close to that temperature.  Now for the demonstration. You can easily grab the aluminum foil without burning yourself, but you can't do the same to the pizza. Even though these two objects have the same temperature, they have different amounts of thermal energy.
The thermal energy in an object depends on the object's mass, the object's material and the object's temperature. The change in thermal energy for an object then depends on the change in temperature.
In this expression, m is the mass of the object and the variable c is the specific heat capacity. The specific heat capacity is a quantity that tells you how much energy it takes to one gram of the object by 1 degree Celsius. The specific heat capacity of water is 4.18 Joules per gram per degree Celsius. For copper, the specific heat capacity is 0.385 J/g/°C (yes, water has a very high specific heat capacity).
But what about turkey? What is the energy needed to heat up 1 gram of turkey by 1°C? That is the question I want to answer. Oh sure, I could probably just do a quick search online for this answer, but that's no fun. Instead I want to calculate this myself.
Thanksgiving Hack: Cook Your Turkey Sous Vide
Why It's So Tough to Keep Antibiotics Out of Your Turkey
Here's the Real Reason Thanksgiving Makes You Sleepy
Here is the basic experimental setup. I am going to take a turkey breast (because I am too impatient to use the whole turkey) and put it in a known amount of hot water. I will then record the change in temperature of the water and the change in temperature of the turkey. Of course, this will have to be in an insulated container such that all of the energy that leaves the water will go into the turkey.
With the change in temperature of the water, I can calculate (based on the known specific heat capacity of water) the energy lost. Assuming all this energy goes into the turkey, I will then know the increase in energy of the turkey. With the mass and change in turkey temperature, I will have the specific heat capacity of a turkey.
Just to be clear, I can set the changes in energy to be opposite from each other and then solve for the specific heat capacity of the turkey. Like this.
OK, it's experiment time. I am going to start with 2,000 mL (2 kilograms) of hot water and add it to a foam box with my turkey breast. I will monitor both the temperature of the water and the turkey. Oh, the turkey has a mass of 1.1 kilograms. Here's what this looks like (without the box lid).
I collected data for quite a while and I assumed that the water and the turkey would reach an equilibrium temperature—but I was wrong. Apparently it takes quite a significant amount of time for this turkey to heat up. Still, the data should be good enough for a calculation.
Hopefully it's clear that the red curve is the hot water and the blue is for the turkey. From this plot, the water had a change in temperature of -21.7°C and the turkey had +27°C. Putting these values along with the mass of the water and turkey, I get a turkey specific heat capacity of 6.018 J/g/°C. That's a little bit higher than what I was expecting—but at least it is in the ballpark of the value for water. But overall, I'm pretty happy.
But what can you do with the specific heat capacity for a turkey? What if you want to do a type of sous-vide cooking in which the turkey is placed in a vacuum-sealed bag and then added water at a particular temperature? Normally, the temperature of the water is kept at some constant temperature. But what if you want to start with hot water and cold turkey and then end up with perfect temperature turkey? In order to do this, you could calculate the starting mass and temperature of water that would give you the best ending turkey temperature. I will let you do this as a homework assignment.
Of course there is another way to cook a turkey. You could drop it from some great height such that it heats up when it lands. Oh, wait—I already did this calculation.
You finish that thanksgiving feast and immediately all you want to do is sleep. Many people blame the turkey for their sudden comatose state, but that may not be 100% true.


You don’t want to be among the first human cyborgs. Because doctors won’t be replacing all your limbs with super-strong robotic ones, and they won’t be giving you cameras for eyes. More than likely, they’ll be saving your life by wrapping your heart in a robot.
Today in the journal Science Robotics, researchers introduced a new kind of device to keep a heart pumping: It cradles the organ and uses a probe to anchor to the wall that separates the heart’s lower chambers. The robot can precisely manipulate a particular chamber, and that could lead to devices that let doctors assist a heart in its normal function instead of relying on a transplant. (Another sort of robotic heart announced earlier this year envelops the organ like a sleeve, but this new robot can work on a single diseased chamber.)
These days, doctors keep a heart pumping blood with something called a ventricular assist device. This is a pump external to the body that helps ferry blood around when the heart just can’t manage on its own. Problem is, because blood is flowing through machinery, the patient has to take blood thinners to make sure the works don’t get gummed up. And doctors don’t like putting people on blood thinners if they can avoid it.
This new robot is incorporated right into the heart, and acts to encourage the organ’s normal function. The bit that rests on the heart is a soft robot made of polymers, meaning it’s, well, soft, so it better conforms to the organ and doesn’t irritate the flesh. But it’s also soft in its operation: Instead of using traditional motors that are complicated and bulky, it’s pneumatically activated, which is a gentler way to manipulate the heart.
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
The second bit of the robot is a rod that actually enters the heart and anchors to the wall that separates one ventricle from the other, known as a septum. A needle pierces the septum and a delivery shaft opens up an anchor on the other side of the wall like an umbrella. Then an operator places a disk on the other side to complete the anchor.
So in addition to the soft robot on the outside pumping the free wall of the ventricle, the shaft pulls the septum toward the wall, squeezing the ventricle to get blood flowing. Without pulling on the septum, the device wouldn’t really be replicating the beating of a heart. “The septum is very actively engaged in the ventricular contraction,” says study coauthor Nikolay Vasilyev, a scientist in the Department of Cardiac Surgery at Boston Children's Hospital. “When the heart contracts, it's not only the free walls that move. The septum thickens during the contraction and moves inward into the respective ventricle.”
By manipulating both the outside of the organ with a soft robot and tugging on the septum with a rod, this device helps the heart pump blood much more precisely than other devices: The system reads either the electrical signals from the heart or pressure changes within the ventricle to time its movements in concert with the normal operation of the organ.
The researchers have already shown the robot working in a live pig. The next step could be to actually implant the thing in an animal and stitch it up, then watch the robot work over the course of months.
“In terms of technological development, I believe we are almost at the stage where a large company or a pool of investors take this technology to the next level and make a product out of it,” says University of Leeds roboticist Pietro Valdastri, who was not involved in the study. “I frankly hope this is going to happen, as this technology looks pretty ready to me for this type of jump.”
Robots have already stolen our hearts. Now they're keeping them beating, too.
A robotic heart points the way to a future where soft robots help us heal.


This story originally appeared on CityLab and is part of the Climate Desk collaboration.
When Hurricane Irma sprinted toward Miami-Dade County, Jeff Ransom couldn’t sleep. He wasn’t just worried about gusts shattering windows, or sheets of rain drowning the highway—that’s far from unusual near his home in Broward County, where extreme weather verges on routine, and patches of U.S. 1 are regularly submerged.
Ransom, the county archaeologist, was preoccupied with an oak tree and its 350-year-old roots. If the tree capsized with enough intensity, he worried, the flailing roots could dislodge human remains.
On a blazing blue morning in early November, weeks after the storm, we trek to the site of the Tequesta Native American burial mound that kept Ransom awake.
“All night long, I was just thinking about that oak tree flipping over,” he says. “The big roots are growing right into the burial mound. That would’ve just blown human bone everywhere.”
Irma’s winds shaved canopies off the trees at the Deering Estate, a historic homestead that contains the burial mound and other fossil sites and is managed by the Miami-Dade County department of Parks, Recreation and Open Space. Under those bald branches, growth was rapid as vines and chutes—nourished by seaweed deposits—scrambled for sunlight. The result has been a second spring: bright, young leaves, greedy for purchase among the gumbo-limbo and strangler figs. Ransom knocks a path for us with a machete, which he carries slung in a holster. Two thwacks splinter the Brazilian pepper branches—but that’s only because the machete is dull, he tells me. Usually, a single smack is enough to slice straight through, like butter.
Ransom is 52, with a GI Joe jawbone and black aviator sunglasses. At one point, these vanish into the carpet of leaf litter, gone shaggier since the storm, and Ransom spends a few minutes poking around for them beneath the slashed fronds before remembering that he has a nearly identical backup pair.
The burial ground was—is—fine. The oak’s trunk is sturdy and thick; the roots are sunk deep into the soil. We sit for a moment on benches nearby, guzzling water in the shade while Ransom uses his machete’s blunted edge to scrape burrs off his pants and shoes.
The storm didn’t bear down on the city with all its might: In general, Southeast Florida was spared the breadth of damage that forecasters had conjured. A half-mile of mangroves buffered the Cutler Midden, another archeological site on the Deering Estate, against damage wrought by crashing waves. Ancient shell tools and pottery fragments survived intact.
Irma could have bitten harder. But in isolated pockets, the storm was ravenous. We pass fragments of a historic boardwalk, which the archaeologists had laboriously documented and annotated. The structure “had been chunked up” in the storm, explains Mallory Fenn, the public archaeology coordinator at the Southeast/Southwest Florida branch of the Florida Public Archaeology Network. The network is a project of the University of West Florida; the Southeast/Southwest division operates out of Florida Atlantic University.
Fenn’s earrings are made from gator teeth, and the boardwalk looks masticated and spit out, its component parts hardly visible. An orange-and-white barrier marches across the crumpled walkway, as if it wasn’t patently clear that there’s trouble ahead.
Before I fly down to Miami to trail her and Ransom through the swamp, Sara Ayers-Rigsby sends me a packing list. Ayers-Rigsby is the Southeast/Southwest regional director of FPAN, and the trunk of her car is stocked with supplies, from bug netting to single-serving bags of pretzels. She’ll have ample bug spray and sunscreen to share, she writes, but I’ll want to wear long sleeves on my arms and legs, and the most waterproof boots I’ve got. We’ll be wading into the height of the king tides; the water might rise up to our knees. Heat and mugginess can have a scrambling effect. Ayers-Rigsby later describes it as “brain-meltingly hot.”
“The weather in south Florida is inhospitable,” she warns.
Writ broadly, that’s precisely the problem. Numerous projections forecast a future of extreme weather and persistent flooding that is incompatible with many elements of life as it’s known on the peninsula. Of all of the U.S. states, Florida is the most vulnerable to sea-level rise, and Miami-Dade is at particular risk.
As the plane drifts toward descent, water is everywhere: in green-blue pools that reach for the horizon, in mud-colored eddies, in staid intercoastals studded with white yachts. From the air, many of these basins look overfull, ready to spill with the slightest topoff.
Sooner or later, the water will swallow the shoreline. When it comes to the magnitude, severity, and timetable, there are shades and gradations of apocalyptic hues. In 2015, a working group comprised of officials from across Southeast Florida set out to get on the same page about the threats and to strategize about mitigation efforts. Their projection draws from local tide measurements and is aligned with estimates from the U.S. Army Corps of Engineers and the National Oceanic and Atmospheric Administration (NOAA). By 2030, they anticipate a sea-level rise of 6 to 10 inches from a 1992 baseline; they predict a rise of up to 26 inches by 2060, and 61 inches by 2100.
Even if the water doesn’t crawl quite that high, damage could still be widespread and devastating. Twenty-five percent of land in Miami-Dade County sits less than three feet above current sea level, according to the World Resources Institute. Ten percent is less than a foot away from being flush with the sea.
And if water does splash to the maximum level, the results could be cataclysmic. In a recent report, the real estate company Zillow estimated that, if the sea level were to rise by six feet, 24 percent of Miami’s housing stock would be drenched.
“You can’t wrap an archeological site in bubble wrap and put it on a high shelf.”
Troublingly for Ransom and Ayers-Rigsby, a sea-level rise of just half that height could destroy as many as 16,095 archaeological sites across the state. As the terrain goes soggier or washes away, how do you protect objects embedded in it?
“You can’t wrap an archeological site in bubble wrap and put it on a high shelf,” Ayers-Rigsby told me via phone soon before Irma swept past. Some sites can be stabilized or buffered with mangroves or oyster beds, but when it comes to safeguarding them from pummeling rain or surging waves of a hurricane-strength storm, options are limited. “Other than building a massive construction around it,” Ayers-Rigsby said, “there’s not that much you can do.”
Among officials in Miami-Dade, “there’s no sugar coating or backtracking” about the threat of climate change, Ransom tells me. Its consequences play out in real time, in flooded streets and waterlogged basements, and voters throw their weight behind mitigation efforts at the polls. After his landslide victory in this month’s elections, the incoming City of Miami Mayor Francis Suarez told the local ABC affiliate that “Miami should be and must be the most resilient city in the world.” That same day, voters approved a bond measure that directed $192 million to pumps, walls, drains, and other projects to keep the city drier. Meanwhile, Ransom, Ayers-Rigsby, and their colleagues work to keep thousands of years of history from being lost to the sea.
If you wonder what archaeology Florida can boast of, you’d hardly be the first. In a carpool from the airport, I told two Australian businessmen what had brought me to the city. They cocked their heads. Miami, to them, evoked beaches, surgically altered bodies, and hefty Cuban sandwiches. What else was there?
I recount this to Ayers-Rigsby while we sit on a choked concrete freeway, inching from Fort Lauderdale to Biscayne Bay. She groans and slumps her head toward the steering wheel. Ayers-Rigsby, 34, relocated to Florida from the Mid-Atlantic, and is now somewhat evangelical about the region’s overlooked merits. Around her neck, she wears a pendant with the state’s silhouette.
For as long as people and creatures have inhabited present-day Florida, they’ve been shedding traces of their lives. Fenn says the flitting snowbirds and rotating crop of transplants can be afflicted with a virulent case of historical amnesia. But the scattered sites testify to millennia before the shores were dotted with high-rises fashioned from glass and steel.
The Cutler Fossil is a watering hole into which all manner of Pleistocene beasts toppled. Sandwiched between the limestone layers of the sinkhole, some 16 feet above the current sea level of the nearby Biscayne Bay, were bones of dire wolfs, mastodons, camels, llamas, saber-toothed tigers, and the American lion. Though the site is protected, the city has sprawled around it in the intervening 10,000 years. Looking down into the ancient pit from the ridge, you can hear the rumble of nearby cars. But the site is hidden and sheltered from the road and the water, protected by its isolation and its elevation.
Other sites sit more uneasily with the present. In the late 1990s, archaeologists discovered a circle of post holes cut into the limestone bedrock at the mouth of the Miami River. Carbon dating of wood fragments helped identify the site as the home of a structure built nearly 2,000 years ago by the Tequesta Indians. “People have been partying in Miami for thousands of years,” Fenn jokes, as she shows me around the site. Archaeologists, Native activists, and a galvanized public sparred with a developer, who had purchased the property as the future site of luxury condos. (A flurry of controversy swirled at the time, when some scholars wondered whether the pattern was, more simply, the drain site for a septic system. Archaeology magazine solicited input from other archaeologists, scholars, and a master septic tank contractor, the latter of whom summarily dismissed the possibility.)
The Miami Circle was designated a National Historic Landmark in 2009. Today, the site is a grassy expanse shaded by towering condos and hotels that have sprung up around it, overlooking cruise ships and cargo freight lumbering in the distance. It’s a rare green space in a vertiginous corner of the city—and that means it sometimes becomes a place for dogs to lift their legs. A fluffy white dog squats nearby as Fenn describes working on an archaeological site just across the narrow river, where archaeologists unearthed additional Tequesta artifacts in 2014 in the prospective footprint of a massive mixed-use development. These excavations are a trippy mash-up of the ancient and the dizzyingly modern. “When you look down, you think it’s the 1850s, with a sifter and a trowel,” she says. “Then you look up and see skyscrapers, and the Metromover going by.”
During Irma, water breached the walls just below the Miami Circle site. It rushed onto the grass, carrying palm fronds washed in from the river. Fenn, who lives nearby, “ran out pretty much the second we were allowed to be outside” to check in on it. The water soon receded, leaving no apparent damage. This particular spot, loaded with infill, has been shored up to withstand exactly this type of barrage.
Other sites, which lack these preventive measures, are more vulnerable. But studying them can reveal important data about the rising sea—and how long scholars have to hatch a plan.
Ransom and Ayers-Rigsby pick through a dense thicket and a floor carpeted with spiky bromeliads. They know what they’re looking for—orange-capped rebar that they sunk into the bank of the Oleta River—but Irma blew down the trees onto which they’d tied yellow ribbon to help them identify the sites at a distance. Those orange markers have been coated with dirt.
This squishy portion of the riverbank is the site of a prehistoric midden, containing traces of shell tools, pottery, and other daily items that would have been used by Native American tribes who lived on the shore.
“If any site is going to erode, it’s going to be this one,” Ransom says, sloshing through the muck.
The midden, or ancient trash heap, is nearly flush with the water level, which makes this site an ideal candidate for tracking inundation and water rise before and after storm events and king tides. By obtaining a baseline measurement and a set of comparisons, the archaeologists can document both accumulation and erosion—noting which events seem to pile more sediment on the top of the site, and which strip it, ultimately threatening to haul the artifacts out to sea.
The notion of using this area as a proxy for fluctuations in the water level dates back decades. In the late 1970s and early ‘80s, when he was working as the country archeologist, Robert Carr found evidence of ancient charcoal buried about two feet below the surface. Since a fire needs to be dry, Carr reasoned that that portion of the site was once above water. At the time, climate change “certainly wasn’t on anybody’s radar” in the archaeology community, he tells me via phone. There was “no particular movement or focus going on.” Carr advocated for using soil inundation, radiocarbon dating, and water levels as firm evidence for past and future variations. His work laid the foundation for what Ransom and Ayers-Rigby are doing.
On a recent afternoon, the mangrove roots are flecked with odd pieces of very modern garbage: foggy glass bottles, a boogie board speckled with barnacles, a black DVD case, a wrinkled bag of Ruffles chips. These aren’t the signs of someone sneaking in to use the forest as a dump, Ayers-Rigsby says—the refuse has been carried in on waves.
She and Ransom slog through the sucking mud, brushing biting ants from their backs and shoulders, to measure the distance from the rebar to the water line. They jot down the measurements in a yellow notebook, its pages warped by wetness. In some spots, the sediment is piled higher than it was the last time they measured, before Irma blew in. That accumulation suggests that the water level breached a good chunk of the shoreline during the storm, Ransom says.
Carr explains that’s not unequivocally dangerous—there’s not yet sufficient clarity about whether inundation is an impediment to preserving sites in the same way that erosion is. Conceivably, he says, a site “could be better preserved underwater than it is above ground, if sea-level rise is gradual, not a result of pounding waves hitting shoreline and tearing up and removing soils.”
Through her work at FPAN, Ayers-Rigsby has also helped recruit a team of citizen scientists to fan out across the state and conduct regular monitoring of at-risk sites. Inspired by a U.K. program, Scotland’s Coastal Heritage at Risk, the Heritage Monitoring Scouts, a brigade more than 200 people strong, survey publicly accessible sites—not the more sensitive ones, like unmarked burial grounds—and upload their impressions onto a website form. They look out for signs of flooding, erosion, or wave action, or any artifacts that may have been dredged to the surface, and flag any places that need urgent attention.
Sixty-two-year-old volunteer Paula Streeter surveys the shell midden on Calusa Island, a dot of land off the state’s southwest coast once inhabited by Calusa Indians. Streeter has a wide-ranging background—her resume includes “a zillion, million, trillion things,” she tells me via phone. Since retiring from the city clerk’s office, she’s begun assisting archaeologists. “I only started this,” she says via phone. “It was the most amazing thing in my life, and it only happened two years ago.”
Already, the Calusa shoreline is being eaten by waves and wind action, Streeter says. Artifacts are surfacing in the midden, relics of the tribe’s use of shells for tools and weapons—but the average beachgoer might not notice them. “If you’ve been trained, you know that’s an ancient form of a hammer made from a whelk shell or a horse conch,” Streeter says.
The Calusa Island site is only accessible via boat or kayak—“you can’t just zip out there,” Streeter says. Before the recent hurricanes and king tides, the team intended to survey once a month. (The site is also monitored by researchers from the University of Florida.) When toppled trees exposed these artifacts, the team upped the frequency to once per week—and instead of leaving all of the artifacts in situ, the volunteers diagram the original locations and bag some of them, so they’re not tugged out to sea. Heritage Monitoring Scouts use rebar installations to measure the distance from the midden edge to the beach. Even without their precise computations, it’s easy to see the effect of the waves and wind in exposed roots and a dramatically angled ledge of sand.
Some of these sites contain clues to enriching or correcting the historical record. One example is the dwindling island of Egmont Key, off of the Tampa coast.
A few years ago, the U.S. Army Corps of Engineers reached out to the Seminole to ask about the dwindling island. It was eroding heavily—shrunk to 280 acres, half its size—and they were wondering whether to replenish it with sand. Was the tribe interested in preserving it?
“This history is a hidden history—it’s not one that’s in any of the textbooks.”
The imminent threat to the land mass was the impetus to uncover the site’s history. With his colleagues, Dr. Paul Backhouse, the director of the Ah-Tah-Thi-Ki Museum and Tribal Historic Preservation Officer for the Seminole Tribe of Florida, pursued some research and learned that, during skirmishes with the U.S. Army in the mid-1800s, the island functioned as a detainment site for Seminoles who were caught evading the ships deployed to remove them out west. Judging by contemporary accounts, conditions were grim: There were no sources of fresh water, and the captives were trapped.
The island sits no more than six feet above sea level. Did the tribe want to keep it above the waves? Among the Seminole community, “the overwhelming response was yes,” Backhouse says via phone. Archaeologically, there was much to learn from the site and the 19th-century artifacts that accumulated there—but it could also function as a place of catharsis and education. “Youth can come and remember the struggle their ancestors went through to remain in Florida,” Backhouse says. “This history is a hidden history—it’s not one that’s in any of the textbooks, because it’s an embarrassment to normal American history.”
Egmont Key is on the front lines. With enough elevation or distance from foot traffic, many other sites will be safe for a relatively long time, by virtue of staying dry or hidden. But as the sea creeps higher, choices will have to be made.
This fall has been an expensive one at the Deering Estate. Hurricane Irma and the October king tides packed a double-punch, explains Jennifer Tisthammer, the estate’s director.
During that first king tide, storm surge swamped the service road with ankle-deep water and flooded the back lawn, where many of the estate’s special events take place. Irma’s gales ripped off 80 percent of the tree canopy; 6,000 cubic yards of seaweed washed ashore. Tisthammer’s long-term vision is to raise the back lawn—but in the meantime, the staff looked for prophylactic measures to mitigate the aesthetics and promote drainage. Sod is best, Tisthammer says, but white rock looks better than soggy, brown grass. When the staff spread out truckloads of drain rock and sand, the puddles that had been taking weeks to drain were siphoned off within a few days.
Even if the fully-underwater-future is far off on the horizon, the king tides offer a regular reminder—and a kind of trial run. On a page devoted to king tides and climate change, the Environmental Protection Agency notes, “Sea level rise will make today’s king tides become the future’s everyday tides.”
Places like the Deering Estate are already factoring preventative and adaptive strategies into line items on the budget. “You’re gonna have some loss,” Tisthammer says. “Do you put $3 million into something you know will eventually go under, or allocate it differently?”
The kind of data that Ayers-Rigsby and Ransom are collecting can be used to inform broader city planning and budgeting—and this December, Miami-Dade and three surrounding counties are taking archeological sites into account, adding provisions to the updated action plan from the Southeast Florida Regional Climate Change Compact. The document isn’t binding, but it encourages local officials to work with historic preservation specialists to map and rank at-risk sites; to appeal to FEMA, local emergency management offices, and other agencies for financial resources; and to implement sustainable preservation tactics such as planting mangroves and cordgrass, or “hard armoring” sites with rocks or concrete. These strategies aren’t without drawbacks. “Hard methods may negatively impact sites by the weight and shifting of large rocks, not to mention the cost of acquiring and moving these to remote places,” Ransom says.
The solution is also not as simple as plucking artifacts from the ground and shuttling them to museum collections, where they might be preserved behind plexiglass vitrines. For the Seminole tribe, as for many other indigenous groups, Backhouse says the prevailing philosophy is that items discarded over the centuries should be left in place. He acknowledges that this mantra of noting objects, “working around them, planning around them, and not thinking of those objects as just research vehicles” might “go completely against the grain of what most people think archaeology is.” But Ayers-Rigsby and Ransom likewise consider excavation to be something of a last resort.
In the Seminole culture, Backhouse says, there’s a difference between something being upturned by an earthquake, versus pulled to the surface by human hands. The underlying philosophy is seeking harmony and balance with nature, he says—and “indigenous cultures don’t have an idea that nature’s always nice.”
The Crowdsourced Maps Guiding Puerto Rico's Recovery
The World's Soaring CO2 Levels Visualized as Skyscrapers
See How Human Activity Is Changing Animal Migration Patterns
Last spring, my colleague Linda Poon reported that the vast majority of states lacked any mention of historic resources in their disaster management plans. Up until this point, that’s been the case in Miami-Dade, says Ayers-Rigsby. “One of the reasons I was so happy we had some language put into the draft of the climate action fund was just to get it on people’s radar,” she adds. “Before, it was not even included at all at any level.” There’s momentum in this direction: Earlier this fall, the city of Annapolis, Maryland, hosted a conference called “Keeping History Above Water,” dedicated to solutions for historic preservation and cultural resources. In August, Backhouse and the Seminole tribe participated in the Tidally United Summit, co-sponsored with FPAN and the Florida International University Global Indigenous Forum, which focused on the relationship between climate science and historic resources.
Meanwhile, Ayers-Rigsby is sensitive to the emergent, unfolding toll that storms and flooding can wreak on people and property. “You have to put the human aspect in the present first,” she says. “You have to prioritize people’s safety and people’s livelihoods. Archaeology and historic resources are obviously necessarily secondary to that, but they should still be discussed.”
It’s painful enough to put a pricetag on property—homes, cars, neighborhoods—that we will lose in the reckoning with the waves. And it can be an uphill battle to nudge residents and officials toward the level of abstraction required to dwell in the realm of forecasts and best guesses. “A risk in the future feels a lot less scary than a risk that’s presented right now,” the risk-perception expert David Ropeik told my colleague Laura Bliss in 2015. Even in Florida, where volatile weather is undeniable, it requires a few metal acrobatics to tumble toward an understanding of the sites that are at stake—sometimes literally below the surface.
But if the goal of archaeology is to preserve and interpret the past for the future, there’s plenty of work to be done—careful and quick, down in the muck and in legislative offices—before traces of that past slip away. In those strata are testaments to lives lived, forgotten, and remembered over the course of millennia: a record of what it has meant to be human.
No matter what they do, Ayers-Rigsby says, the time capsule will be incomplete. “Some things will be lost forever.”
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.


On the last Monday of September, 32 field workers stepped onto a 15-acre experimental plot in an undisclosed part of Washington and made apple harvest history. The fruits they plucked from each tree were only a few months old. But they were two decades and millions of dollars in the making. And when they landed, pre-sliced and bagged on grocery store shelves earlier this month, they became the first genetically modified apple to go on sale in the United States.
The Arctic Apple, as it’s known, is strategically missing an enzyme so it doesn’t go brown when you take a bite and leave it sitting out on the counter. It’s one of the first foods engineered to appeal directly to the senses, rather than a farmer’s bottom line. And in a bid to attract consumers, it’s unapologetic about its alterations.
The apple has courted plenty of controversy to get where it is today—in about a hundred small supermarkets clustered around Oklahoma City. But now that it’s here, the question is, will consumers bite? Dozens of biotech companies with similar products in the pipeline, from small startups to agrochemical colossuses like Monsanto and Dupont are watching, eager to find out if the Arctic Apple will be a bellwether for the next generation of GMOs, or just another science project skewered on the altar of public opinion.
Neal Carter bought his first apple orchard in 1995, up in the gently sloping valley of Summerland, British Columbia. When he started, the future president of Okanagan Specialty Fruits didn’t have grand plans for upending the industry. But in his first few seasons he was struck by just how many apples (and how much money) he had to throw away on account of browning from the routine bumps and jostles of transit and packaging. Most years it was around 40 percent of his crop.
When you cut an apple, or handle it roughly, its cells rupture, and compounds that had been neatly compartmentalized come in contact with each other. When that happens, an enzyme called polyphenol oxidase, or PPO, triggers a chemical reaction that produces brown-colored melanin within just a few minutes. Carter thought there had to be a way to breed or engineer around that. So when he came across Australian researchers already doing it in potatoes, he wasted no time in licensing their technology, a technique known as gene silencing. Rather than knocking out a gene, the idea is to hijack the RNA instructions it sends out to make a protein.
The problem, Carter found out later, was that apples were a lot more complicated, genetically speaking, than the potato. In taters, the browning enzyme was coded into a family of four sub-genes that were chemically very similar. All you had to do was silence the dominant one, and it would cross-react with the other three, taking them all down in one go. Apples, on the other hand, had four families of PPO genes, none of which reacted with the others. So Carter’s team had to design a system to target all of them at once—no simple task in the early aughts.
To do it, the Okanagan scientists inserted a four-sequence apple gene (one for each member of the apple PPO family) whose base pairs run in reverse orientation to the native copies. To make sure it got expressed, they also attached some promoter regions taken from a cauliflower virus. The transgene’s RNA binds to the natural PPO-coding RNA, and the double-stranded sequence is read as a mistake and destroyed by the cell’s surveillance system. The result is a 90 percent reduction in the PPO enzyme. And without it, the apples don’t go brown.
It took Okanagan years to perfect the technique, which was subject to regulatory scrutiny on account of the viral DNA needed to make it work. Today, with the arrival of gene editing technologies like Crispr/Cas9, turning genes on and off or adding new ones has become much more straightforward. Del Monte is already growing pink pineapples, Monsanto and Pioneer are working on antioxidant-boosted tomatoes and sweeter melons, J.R. Simplot has a potato that doesn’t produce cancer-causing chemicals when it’s fried. Smaller startups are busy engineering all kinds of other designer fruits and veggies. And it’s not obvious how exactly this new wave of gene-edited foods will be regulated.
Gene editing gets around most of the existing laws that give the Food and Drug Administration and the Department of Agriculture authority over biotech food crops. In January, the Obama administration proposed a rule change that would look more closely at gene-edited crops before automatically approving them. But earlier this month the USDA withdrew that proposed rule, citing science and biotech industry concerns that it would unnecessarily hinder research and development.
Carter, whose fruits were cleared by the USDA and the FDA in 2015, says his Arctic Apples are evidence the existing process works. But there were times when he wasn’t sure they were going to make it. “It took us close to 10 years, where we had the apples, we had the data, we kept submitting answers to questions, and then wouldn’t hear anything back,” says Carter. “It’s a bit of a black hole, and that whole time you’re not sure if you’re going to even be able to pay your electricity bills and keep your lights on.”
QR Codes for GMO Labeling Could Actually Be a Great Idea. Could
Organic GMOs Could Be The Future of Food — If We Let Them
In a First, the FDA Clears Genetically Modified Salmon for Eating—It Just Took 20 Years
Talking to Carter, Okanagan still feels like a small family business, especially when he says the word “process” with that endearing, long Canadian “O”. This year’s Arctic Apple harvest amounted to 175,000 pounds—just a drop in the apple bucket. But shortly after its US regulatory approvals, his company was acquired by Intrexon Corporation, a multinational synthetic biology conglomerate that owns all the other big-name GMOs you might have heard of. Like Oxitec’s Zika-fighting mosquitoes, and the fast-growing AquAdvantage salmon.
That’s one reason customers might be wary of the Arctic Apple. Another is transparency. While Carter says they’re taking that literally—the bags have a plastic see-through window to view the not-brown slices for yourself—others say Okanagan hasn’t gone far enough in telling people how its apple was made. The letters G-M-O don’t appear anywhere on the bag. Instead, in accordance with a 2016 GMO food labeling law, there’s a QR code, which you can scan with a smartphone to get more information online.
Some consumer groups think that doesn’t go far enough, but scientists counter that they’re focusing on the wrong things. “Breeding technologies are just a distraction from the big questions,” says Pam Ronald, who studies plant genetics at the University of California and who is married to an organic farmer. “Like, how do we produce enough food without using up all our soil and water? How do we reduce toxic inputs? Those are the grand challenges of agriculture today, that technology can help address.”
Ronald works on food crops designed to fight food insecurity in the developing world—like drought-resistant corn and vitamin-enriched rice. When she first heard of the Arctic Apple at a conference in 2015, she wasn’t that impressed. It’s not exactly a famine-fighter. But when Carter sent her a box of fruits a few weeks later, her kids had a different take. They brought them into school to show to their biology classes, and according to Ronald, their classmates just went wild. “Kids really hate brown apples, and it made me realize I don’t really like them either,” she says.
Living where food is abundant, most people don’t really grasp how GMOs touch their lives. “It’s that distance that consumers are removed from agriculture that creates the fear,” says Ronald. “But if you see a brown apple you’re probably aware that you throw it away, and maybe you feel guilty about that. Connecting biotechnology to something you can see and feel and taste like that could be transformational.”
When hosting a party where genetically modified foods are what’s for dinner, is it proper etiquette to warn your GMO-averse friends ahead of time? Mr. Know-It-All offers sage advice on how to handle.


David Pitman is a third-generation poultry grower, raising chickens and turkeys with his brother and parents on a property east of Fresno that his grandfather founded in 1954. They’re independent farmers—instead of raising birds for a corporation, they sell directly to wholesalers and stores—and their farming style reflects that freedom. They buy turkeys from a variety of breeds, feed them organic grains, and let them wander over pasture, choices that make the final product very different from the broad-breasted birds raised at industrial scale by national meatpacking companies.
In one way, though, both products are more alike than they are different. The Pitmans have always raised their birds without putting antibiotics in their feed. Now the rest of the turkeys—and chickens and cattle and pigs—raised in the United States are moving that way thanks to federal regulations that went into effect on January 1, 2017.
The turkeys that will appear on tables across the country this week were hatched after those rules became final, which makes this, aspirationally at least, the first antibiotic-free Thanksgiving. But farmers such as Pitman who have done without the drugs for years say that, as desirable as that may be—for animals, and also for people threatened by the antibiotic-resistant bacteria that result—it’s harder than it looks.
The movement to raise meat without routine antibiotics was started three years ago by Perdue Farms, the fourth-largest chicken company in the United States. Other poultry companies followed, making “no antibiotics ever” chicken commonplace now. But “turkeys are much more difficult, probably 10 times more difficult, to raise antibiotic-free than chickens are,” Pittman says. “Any little thing throws them off.”
Maybe a little background is in order. Antibiotics came on the market in the 1940s, and before that decade was over, farmers had begun adding low doses of the drugs to animal feed, first to make livestock put on weight faster and then to protect them from diseases as growers packed more animals onto farms.
Even though antibiotics were new, scientists already realized that drug resistance could develop. Alexander Fleming, the discoverer of penicillin, warned when he received the Nobel Prize in 1945 that bacteria might evolve defenses that would protect them against antibiotics’ killing effect. But the doses being used in livestock were so small—10 grams of drug per ton of feed for growth promotion in the early days, nowhere near enough to cure an infection—that no one expected resistance to be a problem.
That turned out to be a bad guess. By the 1950s, doctors and microbiologists were identifying drug-resistant foodborne infections such as Salmonella and E. coli in people. The link, they realized, was livestock’s gut bacteria. When the animals ate the antibiotics, some gut bacteria acquired resistance, and others with pre-existing protections survived and reproduced. And when those animals were slaughtered, the resistant bugs contaminated their meat.
To prevent outbreaks, the Food and Drug Administration tried in 1977 to yank the licenses it had given drug companies to sell antibiotics to farmers, but Congress interfered. That started a stalemate that lasted for decades—until, in late 2013, the Obama administration put up a new set of rules.
The rules made the weight-gain doses, called growth promoters, functionally illegal in the US. The agency didn’t ban doses to prevent disease, but it put them under the control of veterinarians. The FDA argued that veterinarians could be trusted to test animals for disease, to choose the right drug for whatever was going on, and to use the drugs for the shortest possible amount of time—all precautions that were supposed to reduce the risk of resistance.
The new rules are porous, though. About a third of livestock antibiotics don’t have limitations on how long they can be used, which means they could promote resistance. And federal data isn’t granular enough to prove that meat producers are complying.
Companies’ marketing makes it look as though they are following the new rules. Tyson announced last year that it is launching a line of pork raised without routine antibiotic use. Cargill took growth promoters out of its turkey in 2014 following Perdue’s announcement, and created a new label for turkeys raised without preventive antibiotics; Butterball did the same this year. Hormel Foods acquired the no-antibiotics-ever processed-meat company Applegate, and conducted what it calls a “raised-without-antibiotics pilot study” for its main turkey line, Jennie-O.
“What I am seeing—and this is based primarily on my work on the poultry industry but with knowledge of other commodities—is that the implementation of (the FDA rules) have had a very positive impact on antimicrobial stewardship,” says Randall Singer, a professor of epidemiology at the University of Minnesota College of Veterinary Medicine who is conducting a multi-year study of poultry industry antibiotic use.
But chicken’s rapid move away from antibiotics may have raised expectations unfairly high, he adds. “A tom turkey may have a lifespan of 18 to 20 weeks. Broiler chickens get killed at six, maybe eight,” he says. “When you put an animal out that long, the chances of disease increase.”
And that’s what turkey producers are struggling with: not foregoing no-longer-legal growth promoters, but deciding how much to dial back on the legal preventive use that still may foster resistant bacteria. Still others are figuring out whether they can go completely antibiotic-free.
In the northwest corner of Iowa, the proprietors of Moline Brothers Farms are weighing those risks. Brad and Grant Moline and their father John, who raise turkeys for the farmer-owned West Liberty Foods cooperative, had to euthanize all 56,000 birds on their property when avian influenza swept through the Midwest in 2015. It was the largest animal-disease outbreak ever recorded in the United States, and killed or forced farmers to destroy more than 7 million turkeys (and 43 million laying hens).
How to Deep-Fry a Turkey Without Killing Yourself
What's Up With That: The Real Causes of the Thanksgiving Sleepies
How Many Batteries Would It Take to Cook a Turkey?
Antibiotic use isn’t directly connected to avian influenza—antibiotics don’t work against the virus. But the sense that an outbreak could sweep up out of nowhere leaves turkey producers cautious about renouncing the drugs for good.
“On our farm, we are making steps each and every flock to put less and less antibiotics in our birds,” Brad Moline told me. “We know consumers want it. We know we need to go in that direction for food safety and for the fact we want antibiotics to work down the road, for future generations. We can see the writing on the wall.”
West Liberty has offered its farmers a certification program that would allow them to market their birds as fully antibiotic-free. But the Molines held back, wanting to preserve the option of trying to cure a disease if one ever surfaces again. “We took such a big hit with bird flu, we weren’t ready to take that risk all at once,” Moline says.
Still, new practices—water-purification systems and air-quality monitors—have brought them closer to being antibiotic-free. “We have had a couple flocks that would have qualified as ABF or almost,” he says, “where in maybe one out of our four barns, we might have a hiccup of disease that needs some treatment.”
All of this makes reading the labels on your Thanksgiving turkey more complicated than it might seem. Label claims—“no antibiotics ever,” “raised without antibiotics,” “no growth-promoting antibiotics”—attest to what producers are doing now. But they’re also markers of an industry in transition—and signposts for where it wants to go.
You finish that thanksgiving feast and immediately all you want to do is sleep. Many people blame the turkey for their sudden comatose state, but that may not be 100% true.


Tesla just keeps making cool things. On the top of the list is its newest addition to the lineup, an all-electric semitruck. Oh, that might sound like a dumb idea—but I don't think so. Just consider how much stuff is shipped back and forth across the country. Clearly a train would be more efficient, but trucks also play a large role. It seems like the Tesla semi might be able to make two improvements over a traditional truck. First, the electric truck will clearly reduce emissions (depending on the energy source to charge these things). Second, with more onboard automation it is likely these semis will be significantly safer than their diesel counterparts.
But what if the Tesla semi also had solar panels? Elon Musk already has a whole company devoted to solar energy—why not just combine and conquer? Tesla and SolarCity are already teaming up to make solar panels, home batteries, and electric cars a unified energy system. Wouldn't slapping some solar panels on top of this truck's surface area make the thing more efficient? You can only learn by trying—so I'm going to estimate the effect of solar panels on the roof of the trailer.
How much power and how much energy (those are two different things) could you get from solar panels on the trailer? First, I guess I should point out the difference between power and energy. Power is the rate that you would get energy from the sun. At the surface of the Earth, the light from the sun produces about 1,000 Watts per square meter. In terms of energy, this is 1,000 Joules every second (assuming the panels are 100 percent efficient—which they're not).
The average power a solar panel produces depends on several things. Bigger solar panels provide greater power (that seems obvious). But it also depends on the efficiency and the angle at which the sunlight hits the panel. The best panels [available today are around 20 percent efficient](https://en.wikipedia.org/wiki/Solar_panel, and they work best when the light hits perpendicular to the panel.
Meet the Tesla Semitruck, Elon Musk's Most Electrifying Gamble Yet
What Does Tesla's Automated Truck Mean for Truckers?
Tesla's Electric Truck Is Coming—And So Are Everyone Else's
Using those assumptions, let's get some values to estimate power for a solar semi.
First, I need to estimate the size. I'm going to say that the solar panel covers the entire top of a standard semi trailer. Since there are apparently different standard sizes of trailers, I am going to say this one is 2.6 meters wides and 15 meters long for a total area of 39 square meters. Second, I need both the time and average incident angle for the sunlight. I am going to use a time of 8 hours at 60 degrees from vertical. This is obviously an oversimplification of the problem, but good enough for a rough estimate. Oh, also I will use the efficiency of 20 percent.
From the area, sunlight angle and efficiency, I can calculate the average power:
Since the total energy is just the product of power and time (in seconds), I get an energy of 112 x 106 Joules (112 MJ). But is that enough to keep this truck trucking?
In order for the trailer solar panel to be meaningful, the energy you get from the panels would have to be significant with respect to the energy stored in the truck battery. Unfortunately, Tesla does not list the size of the battery in its truck. I'm just going to have to estimate it.
Here's what I know: It has a range of 500 miles and it's sort of like a diesel truck (but maybe more aerodynamic). So, if I can approximate the energy used as a traditional truck drives 500 miles—that should be about the same as the energy stored in the battery.
According to Wikipedia, a truck driving at 70 mph requires a power of 280 hp (this is 2.09 x 105 Watts). At this speed, it would take 7.14 hours to drive 500 miles. And since I know the power and the time (which I can convert to seconds), I can calculate the energy.
OK, that's a lot of energy. Although the energy from the solar panel might seem like a large value, it's only 2 percent of the total battery energy. Oh sure, there's a good chance that I made some poor estimates in my calculation—but it probably doesn't matter. Even if I was off by a factor of 5, I'm still only at 10 percent of the total battery capacity. This means that it would take 80 hours, not eight, in order to charge the whole darn battery. Sorry, Elon: This just wouldn't work.


If you’ve never had a robot snake constrict around your leg, let me tell you, it’s a weird feeling. As it curls its way up your ankle, then shin, then finally wraps around your knee, it gets tighter and tighter. Fortunately, unlike a real snake, it doesn’t have teeth. Unfortunately, also unlike a real snake, it’s made of metal joints that pinch harder and harder until you say, “OK, that’s enough,” and the young man at the controls hits a button and calls off the machine.
Here at the Biorobotics Lab at Carnegie Mellon University, there's a veritable menagerie of mechatrons. The snake slithers, a robot insect crawls about, and a wheeled biped speeds down the hallway—until you try to give it a push and it crashes into a wall. This is the strange melding of animal and machine, looking to nature not to copy it one-for-one, but to find inspiration in the bodies that evolution has crafted over millions of years.
But snakes, why did it have to be snakes? Because serpents have a lot to teach us about locomotion that looks nothing like our own. Snakes can swim, they can climb, they can slither forward and sidewind sideways to tackle virtually any kind of environment. Hell, they can even fly. Snakes also come in a convenient shape that can squeeze into tight spaces—which might be handy one day for rescue robotics. (These researchers did take their robot snake to Mexico after September’s monster quake to explore collapsed buildings for survivors. The problem, though, is that for the time being it’s better to hunt for signs of life with microphones than it is to visually probe the rubble.)
Carnegie’s snake robot is a modular machine made of 16 motors known as actuators. Think of these as a whole bunch of joints running down the body, making the robot highly dexterous. By manipulating the actuators in the right sequence, the researchers can get the snake to pull off a surprising range of movements, from undulating like a sine wave (think about someone doing the worm) to constricting up legs.
It isn’t built exactly like a snake because, well, it’s impossible to replicate all those bones and muscles. The robot is an homage to a snake, not a copy of one. “This is going to prohibit you from achieving exactly what we're able to observe in biology,” says CMU roboticist Matt Travers. “But it also means that there's going to be things that this robot can do that a biological snake can't.” For instance, rapidly rolling around like an alligator tearing a piece of flesh off a buffalo.
There’s an interesting interplay here between nature and robotics. First of all, it’s about teasing apart the subtleties of biological locomotion. Travers and other researchers, for instance, did experiments that revealed how sidewinders can move up a slope in loose sand. Turns out the snakes ripple with two different waves, one that grips the ground while the other bit moves forward. They then reprogrammed their robot to do the same—and sure enough, it was better able to tackle the same kind of slope.
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
What you really want, though, is a robot snake that can feel. So this lab’s next-gen snake is made of actuators that can sense the force placed on it. “So now we have this robot that can not only move and looks like biology, but it can feel just like a biological system may,” says CMU roboticist Howie Choset. “So like when you're walking through the woods, you're not planning every possible footstep. Your feet are sort of feeling their way as you go along.”
That means that in the lab, this robot can somewhat autonomously navigate a forest of plastic pegs stuck into a large board. An operator tells the snake to move forward at a certain speed, and the machine senses how much force each of its actuators is putting on the pegs. Too much force is bad, since that might get the robot stuck, so the machine cuts a path forward through the pegs that minimizes that pressure.
This kind of sensitivity will be essential for robots going forward, as they work alongside humans. (It's far more likely that robots will help humans at their jobs, not displace them.) Think about a hulking arm that builds cars, and how far away from it you want to be at all times. New robotic arms, though, can sense the forces placed on their joints and immediately halt if they bump into you.
And it’s not just about safety. If we’re going to work with robots, we’re going to have to communicate both verbally and nonverbally. Say you’re carrying a sofa with a robot, you on one side and the machine on the other. If the robot isn’t able to sense your pushing and pulling, someone is going to get hurt and/or bust a sofa.
So by taking cues from the animal kingdom, engineers can and will get to a place where robots not only move more naturally, but that feel and react to their environment. And that, my friends, is why it had to be snakes.


When someone walks or rolls into the emergency department at Cedars-Sinai hospital in Los Angeles with food stuck in their throat, the ER staff calls someone like Brennan Spiegel. As a gastroenterologist, food-pulling-out is something he’s uniquely equipped to do. So when he got the call one day to see a young man admitted with something caught in his esophagus, he was already mentally preparing for a trip to the operating room. But when he arrived, there was something off about the way the patient was beating his chest with his fist. “This guy’s not choking,” Spiegel thought, “he’s having a panic attack.”
So rather than calling for a gurney and an OR transfer, Spiegel reached for a pair of virtual reality goggles, pulled them over the patient’s eyes, and dialed up a scene of a beach in Hawaii. Within a few seconds the patient stopped struggling. A few minutes later he had stopped moving entirely—not even when prodded. Slightly alarmed, Spiegel began to peel the goggles off the patient’s face. Tears came pouring out. “I’ve been thinking about my whole life in there,” the patient said. “I feel like it’s spinning out of control.”
A short while later, the patient was discharged to psychiatric care. There had never been any food in his throat. The mind can play tricks like that. Luckily, VR can play tricks right back.
Cedars-Sinai is one of an increasing number of hospitals testing how virtual reality could improve patient outcomes. Spiegel is one of the clinical researchers leading that charge. He’s focusing his efforts on something much more universal than the occasional psychosomatic suffocation: pain. Over the past few years he’s conducted clinical trials that show a pair of 3-D goggles can reduce the experience of pain—all kinds, from joint injuries to cancer—by a quarter.
Now, he’s testing the technology for treating chronic pain—the kind that afflicts more than 25 million Americans. All too often, addictive painkillers are the only treatment option for those patients. And with opioids claiming the lives of nearly 100 people every day, doctors are scrambling to find non-addictive alternatives. Virtual reality might soon be one of them, if the science can show it really works.
Scientists started probing the power of VR to ease suffering more than 20 years ago. VR pioneer Hunter Hoffman, a researcher at the University of Washington in Seattle, launched the first pain studies in the early 2000s, using an eight-pound helmet hooked up to a computer the size of a small refrigerator.
Back then, the $90,000 unit transported burn wound patients into a frosty 3-D video game called SnowWorld, designed in cool blues and whites. Burn patients, Hoffman explains, often relive the trauma of their fiery encounters while their wounds are being cleaned. When Hoffman’s patients spent that time in SnowWorld, they reported feeling half as much pain as when they endured the procedure stuck in the reality of the hospital room. What’s more, when he tested SnowWorld on subjects who volunteered to endure a hot poke in the foot, they felt less pain too.
“Acute pain is a perfect match for VR,” says Hoffman. “You only need it for 20 minutes and it has drastic effects.” Chronic pain is a different, more challenging problem. Still, he thinks VR has the potential to enhance many treatments that already work. “If you say, ‘go home and meditate,’ not many patients will follow through,” Hoffman says. “But if you give them a VR system and say ‘go into this ancient world and meditate with monks,’ they’re more likely to actually do it.” VR is just a delivery method: What matters most is what the patients see and experience on the other side of the headset.
So as the cost of the technology has dropped, companies have popped up to build those experiences. One of those is AppliedVR, located just a 15 minute drive from Cedars-Sinai, on Beverly Hills’ Avenue of the Stars. It’s been building out a collection of 3-D content designed to combat pain, like a VR version of Netflix.
“We’re trying to figure out how to prescribe the right experience to the right person based on their needs and their interests,” says President Josh Sackman. So far his team has designed two dozen worlds, each one falling into one of four categories: distraction, relaxation, escape, and education. “But the end goal is to teach skills using technology, not to depend on it for the relief itself.”
The first experience they built was something called Bear Blast. It’s a cartoonish castle-scape where you go around shooting red teddy bears with balls from a cannon you aim with your head. The more bears you knock over, the more points you get. This is the game Spiegel gave to patients in a recent clinical trial. Out of 100 people who self-reported persistent, daily pain, half got an $800 kit with a Samsung headset and a Galaxy phone loaded with Bear Blast. The other half got to watch 2-D relaxation videos of lakes and babbling brooks. While they experienced a small bit of relief, the VR group reported feeling 25 percent less pain than when they started.
Bear Blast works—at least in theory—by distracting your brain. By grabbing all of its attention, the game closes down pathways that would transfer pain signals from your peripheral nervous system. The more immersed you are, the less pain you feel.
So how do you design for that? It’s one part game design playbook, one part trial and error. AppliedVR’s first attempt was actually a total flop. They started out fashioning a sort of candy-colored Disneyland ride, where you careen through a fantasy world knocking over things as you go. But they quickly discovered it was too freeform. People needed more direction. Hence, bowling over cute, fuzzy teddy bears. That works really well for acute pain. But the same principles apply for content that’s meant to deliver more long-lasting effects.
In a more recent trial at Cedars, Spiegel gave his patients more than just Bear Blast. They could choose a variety of experiences, like swimming with dolphins or flying over the fjords of Iceland in a helicopter, or just sitting on a beach and thinking about life. A voice in your ear might talk you through a breathing exercise, or ask you to contemplate the people and things that bring you joy. Mindfulness, meditation, and cognitive behavioral therapy are well-established pain management techniques, but most doctors don’t offer them to patients because they’re worried no one will buy in. The promise of a pill is a lot more tangible.
First They Got Sick, Then They Moved Into a Virtual Utopia
Virtual Reality Needs to Be Social to Succeed
A Brave Bomb-Disposal Robot You Control in Virtual Reality
“We know that positive thinking can actually work on a molecular level, by stopping ions from jumping the gate and causing pain signals to reach the brain,” says William Clark Becker, an internist at the Yale School of Medicine, where he researches pain management and addiction. He is not involved in any of AppliedVR's trials. “You can’t transplant people’s nerves. You can’t undo the process, but you can dampen the heightened pain signals that are getting up to the brain. And positive-thinking people are also better equipped to withstand the vagaries of everyday life.”
He’s found that his patients actually respond really well to mindfulness and CBT, and delivering those treatments in immersive VR is a tantalizing idea. But he says the important thing is making sure the evidence is there to show it’s an effective way to reach patients. Doctors like Spiegel are developing a stronger and stronger case that it is. In his next study, he's working with a major insurance company to evaluate whether or not virtual reality can reduce the number of opioids taken by people who've been recently injured on the job.
He imagines a world in the not too distant future where people coming out of surgeries or recovering from an accident walk out of the hospital with a set of VR goggles instead of a painkiller prescription. Because as Americans learned last week when President Trump declared the opioid crisis a public health emergency, if “we get people before they start, they don’t have to go through the problems of what people are going through.” Easier said than done. But maybe easier to do with virtual reality.


From the air, the largest glacier on the biggest ice sheet in the world looks the same as it has for centuries; massive, stable, blindingly white. But beneath the surface it’s a totally different story. East Antarctica’s Totten Glacier is melting, fast, from below. Thanks to warm ocean upwellings flowing into the glacier—in some places at the rate of 220,000 cubic meters per second—it’s losing between 63 and 80 billion tons of previously frozen fresh water every year.
This matters because Totten glacier and its ice shelf are the only thing keeping an area of ice larger than the state of California from breaking up. If all that ice were to end up in the ocean tomorrow, sea levels would rise by 10 to 20 feet—flooding San Francisco’s iconic Ferry Building, most of Manhattan’s Lower East Side, and the Lincoln Memorial in Washington, DC.
In some ways, this should come as no surprise. For decades researchers have been projecting that the planet’s polar ice reserves will wither in the face of rising temperatures. But more recent satellite data, models, and fieldwork have revealed that it’s happening faster than anyone expected. And increasingly, scientists are finding evidence to pin that Antarctic acceleration on a less obvious aspect of climate change: wind.
Last year, researchers from the US and Australia discovered that churn from deep undersea canyons was bathing the underside of Totten glacier in water warm enough to melt it. But the mechanisms were still a mystery. On Wednesday, they published a study showing that westerly winds blowing off the coast of Antarctica are driving the upwelling, and leading to faster ice flow on the glacier.
To get a feel for why that’s not normal, it helps to understand what’s going on at the ocean-ice interface. As glaciers and ice shelves melt, they deposit their cold, fresh water onto the ocean surface, where it sits above warmer, saltier, denser water. It’s not a gradual transition, but a sharp one. Like when your bottle of salad dressing settles in the refrigerator and you have to shake it back up before serving. That line is called a thermocline, and scientists can measure exactly where it is in the water column. If it rises up to where the glacier is, that’s when you get melting.
By comparing satellite images with oceanic wind records and water temperature and salinity data streaming in from a sensor floating nearby, the team was able to track the thermocline at Totten over time. They found that when the winds blew strong from the west, warm water rushed up and into the glacier. When the winds blew from the east, the thermocline sank back down and melting ceased.
“As a pure scientific curiosity, it’s really fascinating that CO2 can lead to sea level rise not only by heating up the air directly and melting the glacier from above but also just from wind moving heat around the ocean to melt it from below,” says Chad Greene, a research scientist at the University of Texas and lead author of the study. “But then of course there’s a gloom and doom component to this discovery as well.”
That’s because westerly winds along the East Antarctic coast are projected to get a lot stronger over the next 100 years. Gerald Meehl, a senior scientist at the National Center for Atmospheric Research, explains that increasing concentrations of greenhouse gases aren’t just raising air temperatures. They’re shifting and intensifying the westerly wind belt that circles Antarctica. “Future climate projections show an even stronger positive phase of the Southern Annual Mode, which means stronger surface west winds occurring farther south,” he says. “That would drive those winds closer to Antarctica and provide an ongoing mechanism for melting more ice and producing greater sea level rise.”
And not just a little bit more ice, but like, a lot more ice. That’s thanks to some topographical wonkiness unique to Antarctica. The bedrock on the southernmost continent doesn’t slope up like a mountain as you go from the coast to the interior. Instead, it tilts downward, in places even dropping up to a few miles below sea level. Invading waters would quickly flow downhill, seeping further and further inland, and causing ever-larger hunks of ice to flow faster out into the ocean.
“What that means is as you melt it back at the edges, even a little bit, by bringing in that warm water, you really quickly get a runaway scenario,” says Paul Spence, an oceanographer at the University of New South Wales in Sydney. Earlier this year he published a study in Nature Climate Change that looked at similar upwelling patterns causing drastic melting in the West Antarctic, where most ice sheets and glaciers have their base below sea level. “This has happened in the past on pretty quick timescales. It might take thousands of years for a glacier to form, but they can totally collapse in just a few years. That’s what we’re most concerned about.”
Think States Alone Can’t Handle Sea Level Rise? Watch California
Giant Antarctic Icebergs and Crushing Existential Dread
Sea Level Rise Is Already Driving People From the Marshall Islands
Ocean water has already carved contoured troughs that extend all the way from the edge of Totten’s ice shelf to its bedrock 77 miles inland, and as deep as 2 miles below sea level. The last time Totten Glacier—whose melting could raise sea levels as much as all of West Antarctica—collapsed into the ocean, about 3 million years ago, it raised global waters by 20 to 30 feet.
Understanding the chances of that happening again is going to take some serious supercomputing. But the interplay between wind, water, and ice in the Antarctic is only just beginning to be incorporated in global sea level projections. When the Intergovernmental Panel on Climate Change put out its most recent report in 2013, it didn’t include any information on changing ice sheet melt in the Antarctic—partly because no one understood the mechanisms very well back then, and partly because computers just weren’t powerful enough. At the time, the IPCC estimated that sea levels would rise 3.3 feet by 2100.
But some scientists now think that number should be double. By marrying some of the previously underappreciated local ice sheet dynamics with global climate data, a 2016 analysis in Nature found that Antarctica alone could contribute an additional 3.3 feet of water before the end of the century.
It’s just a start. There’s still a ton scientists don’t know about how winds will change in Antarctica, or anywhere else for that matter. And they say that’s probably the single-most significant unknown in the already hyper-complicated arithmetic of predicting sea level rise. Does the flap of a butterfly’s wings in Brazil set off a continent-wide collapse of ice into the Southern Ocean? Maybe. But only if it blows from the west.


The microbes living on Earth are so plentiful as to be innumerable. Untold. Countless. Not in the hyperbolic sense, but the literal, gobsmacking sense. "It's estimated there are 100 million times as many bacteria as there are stars in the universe," says microbiologist Rob Knight, director of UC San Diego's Center for Microbiome Innovation. "And we know almost nothing about most of them."
To map the planet's microbiome—to classify its sundry members and fathom their relationships—would be beyond ambitious. "It’s a crazy idea. Cataloguing the microbial diversity is an immense problem, you know, because there are approximately a trillion species on the planet," says microbiologist Jack Gilbert, director of the University of Chicago's Microbiome Center.
It's funny to hear Knight and Gilbert talk this way. Because seven years ago, the two of them teamed up with microbiologist Janet Jansson, director of biological sciences at Pacific Northwest National Laboratory, to found the Earth Microbiome Project, a positively massive international effort devoted to—you guessed it—cataloguing the planet's microbiome.
The article itself is a textbook of ecology.
Martin Blaser, director of NYU's Human Microbiome Program
Today, Gilbert, Knight, Jansson, and a few hundred of their colleagues unveiled the inaugural version of that microbial map: the first reference database of bacteria colonizing the planet. To do it, they developed new protocols, analytical methods, and software for identifying and comparing microorganisms collected from every continent. All told, EMP collaborators collected 27,751 samples from organisms and environments around the world, including the human gut, a bird's mouth, the soil of an Antarctic volcano, a river in Alaska, and the bottom of the Pacific Ocean. Published this week in Nature, the effort represents the work of upwards of 500 researchers from more than 160 institutions in 43 countries around the globe. It's the most macro study of the microscopic world ever published.
"This article itself is a textbook of ecology," says microbiologist Martin Blaser, director of New York University's Human Microbiome Program, who was unaffiliated with the project. "Students in years to come will read it and say: Here’s where a lot of the rules originated—the rules of ecological relationships, the principles for how nature is organized."
Those organizing principles are too numerous—and, in most cases, too nascent—to recount here. (The 27,751 samples collected for this meta analysis appear in some 100 other studies, half of which have already been published in peer-reviewed journals.) But Knight sums it up: "What was really remarkable about our findings was that this was true across different types of environments, whether we’re talking about microbes on animals, or on plants, or in saline or on non-saline communities," he says. "Even though the kinds of microbes in these environments are completely different, the ecological principles remain largely the same." And now microbiologists have a tool to dig up even more of those dynamic principles.
But compiling the catalogue wasn't easy. As in previous studies, the researchers classified samples of bugs by sequencing the 16S rRNA gene, which carries unique mutations that act like a bacterial barcode. Once researchers have the sequences for all the bugs in a particular sample, they compare them all to each other and cluster bacteria into groups based on their similarities. Their identities become interdependent.
That kind of interdependence is fine if you're assessing the diversity of bacteria from a small set of samples, from a specific region—but it makes it makes it difficult for researchers to compare bacteria between environments, or compare their observations to yours. "It really limits the ability to share information, or to accumulate information across studies," says microbiologist Jon Sanders, a postdoc in Knight's lab and coauthor on the Nature paper.
It’s especially a problem if you’re dealing with billions of sequences—which is exactly what the Earth Microbiome Project had. Its researchers sequenced the 16S rRNA genes not from the microbes in one sample, or even a few hundred—but all 27,751 of them. This yielded some 2.2 billion sequences.
To put that number in perspective, 10 years ago, Knight published what was, at the time, the most comprehensive analysis of the planet's microbial makeup. He and co-author Catherine Lozupone combined 16S rRNA sequences from 111 studies, for a grand total of 21,752 sequences. In Knight's words, the 2.2 billion sequences in this new paper represent a 100,000-fold expansion in our knowledge of the microbial world.
So with the help of some clever algorithms, the researchers classified the 2.2 billion sequences not by clustering them, but by trimming each one down to a stretch of genetic code 90 base pairs long—a completely independent identifier. When the researchers were through scrubbing their data, they had 307,572 unique microbial sequences, almost 90 percent of which were undocumented in existing 16S rRNA databases.
With Designer Bacteria, Crops Could One Day Fertilize Themselves
Obesity Surgery May Work by Remaking Your Gut Microbiome
Your Body Is Surrounded by Clouds of Skin and Fart Bacteria
"I call it the true name of the bacteria," Sanders says. In practice, a true name means not having to wonder if the microbe you found in a lake in Colorado is the same as the one your colleague found off the coast of San Diego five years ago. And with EMP's catalogue, researchers can usually identify where a sample originated just by knowing the creatures living in it. "The key thing is, I can see the sequence now and it's meaningful, and I can see it again in 20 years and it will still be meaningful," Sanders says. "It gives us the ability to accumulate information about these bacteria across many, many studies going into the future."
That makes the EMP database more than a resource—it's also a jumping off point. "It opens the field toward more complicated types of analysis, and serves as a masterful example of how microbiologists and biologists can work together to address much bigger questions" says Nikos Kyrpides, director of the Microbiome Data Science Group at the Department of Energy's Joint Genome Institute, who was not affiliated with the study. "We need to confront the fact that we’re living on a microbial planet, that the magnitude of work we need to do is enormous, and we can only do it if we work collaboratively."
To that end, Jansson, Knight, and Gilbert say they're recruiting more contributors from around the world—to collect samples from a greater range of latitudes and elevations. To target fungi, in addition to bacteria. To sequence not just the 16S rRNA gene, but entire genomes. And to bridge EMP's catalogue with other databases, like the American Gut dataset.
"There will be more to come," says Jansson. After all, there are 100 million times as many bacteria as there are stars in the universe. That means there are innumerable billions—maybe trillions—of microbes left to meet.
Cats can make you go nuts and it's not just the emotional manipulation that's driving you batty...it might be a parasite.


A monster was coming to central Oklahoma.
Early in the evening of May 30, 2013, Cathy Finley and her partner, Bruce Lee, were driving along a back road near the small town of Guthrie, Oklahoma, 30 miles north of the state’s capital, when they spotted Tim Samaras and two members of his crew leaning against a white sedan and looking out over the low hills. Samaras, an old friend of the couple’s and one of the most famous storm chasers in the country, was in the area for the same reason as Finley and Lee—they were all severe-weather researchers, and a tornado was on its way.
The three friends had known each other for almost a decade, and in 2007 they helped found Twistex, a group dedicated to gathering atmospheric data to better understand ­tornadoes. Whenever a storm threatened to spin up a twister, the Twistex team would gas up the chase vehicles and assume familiar roles: Samaras would try to get as close as possible to the funnel to deploy his measurement probes, and Finley and Lee would slice through the storm in sedans outfitted with roof-mounted weather stations, gathering data that radar and weather balloons miss. A reputation for fearlessness landed the Twistex team a spot on a reality show called Storm Chasers, which featured their exploits for three seasons until Discovery Channel canceled the show in 2011 due to low ratings. Funding for Twistex dried up, and the members went their separate ways, meeting up whenever serious wind threatened to blow through the plains.
When they reunited on the back road near Guthrie, all three wanted nothing more than to take on the coming tornado together the way they used to. But with no source of cash to field an entire chasing team, this season was a no-go. Besides, the most recent forecasts indicated that the next day’s storm would reach peak intensity once it entered the Oklahoma City metro area during rush hour, and Finley and Lee had long since sworn off pursuing twisters down crowded streets—too dangerous, too hard to collect good data. They decided to pack up their gear and head home to Minnesota, leaving Samaras and crew to chase the storm.
What if America Had a Detective Agency for Disasters?
Climate Change and the Terrible, No-Good Odds of Bad Weather
Deep Thunder Can Forecast the Weather—Down to a City Block
During the long drive back the following day, Finley followed the storm on her laptop through radar tracking and live footage from spotters and news helicopters. Forecasters had been wrong about one key detail—the tornado let loose in wide-open farm country, about 25 miles west of the city. It measured about 2 1⁄2 miles across, easily the widest anyone had ever seen. Its peak wind velocities registered at least 290 mph and possibly much higher, about the fastest on record. And its main funnel rode the southern rim of its parent storm for nearly 40 minutes, moving back and forth along a wide, wobbling arc like a menacing grin. Knowing Samaras as well as she did, Finley guessed he was having the time of his life right in the thick of it.
Back at home in Minnesota, as Saturday turned to Sunday, the phone rang around 1 am, rousing Finley and Lee from a deep sleep. Lee, rangy and well-­muscled, slipped out of bed and padded across the house toward the kitchen. The phone went quiet, then rang again, and he answered it. It was a former student from the University of Northern Colorado, where he and Finley used to run the small meteorology department. His voice shaking, the man said he had it on good authority that three bodies found on a desolate Oklahoma back road belonged to Samaras, his 24-year-old son (and videographer) Paul, and meteorologist Carl Young.
Finley heard a curious tenor in Lee’s voice from the bedroom and came to his side. She’s a tall woman from western Minnesota farm country with almond eyes, and she and Lee were unsure whether they should grieve or wait for confirmation. Night gave way to day, and as the windows began to pale with the approach of dawn, they got a call from another former Twistex colleague confirming the worst.
Bruce Lee, along with Cathy Finley, drove through storms collecting data on tornadoes.
In the days that followed, Finley retreated to her neglected garden, where she took her anger and grief out on the weeds. Meanwhile, Lee pieced together what happened: Samaras had been driving his Chevy Cobalt hundreds of yards north of the tornado—and at times much, much closer—likely thinking he was tracking parallel to the twister at a safe distance. But something caused the funnel to swing fast toward the vehicle, engulfing it. Samaras might have thought he had studied enough tornadoes to guess this one’s movements, but whatever signs or signals he typically relied on to anticipate a twister’s path (its heading, the stage of its development, the direction of the wind) had failed him. He may have thought he had the right information, but he didn’t.
Based on radar measurements, the tornado had ­EF-5-level wind speeds. The Enhanced Fujita scale assigns every tornado a number from 0 to 5 based on how much destruction the wind speeds inflict. The damage from an EF-0 could just as easily be accomplished with a strong wind gust. An EF-5, on the other hand, in which winds exceed 200 mph, leaves behind scenes of devastation reminiscent of razed Japanese cities. Asphalt gets scoured off roads. Vehicles get tossed the length of a football field. The EF levels in between account for varying degrees of destruction, and despite decades of research, scientists still don’t understand exactly how the air we breathe can suddenly strip a home down to its concrete slab. We’ve put a man on the moon and unlocked the secrets of the atom, but the inside of a tornado is, in many respects, still a mystery.
What scientists do know is that extreme storms like the one that killed Samaras—and the hurricanes that ripped through Texas, Florida, and the ­Caribbean this summer—are likely to develop with greater frequency and intensity. According to a recent study in the American Journal of Climate Change that analyzed 42 years’ worth of tornadic atmospheric indicators, the supertornado that killed Samaras should occur in a stable climate, on average, once every 900 years. But when the researchers accounted for factors such as atmospheric instability and warming oceans (both associated with climate change), they confirmed what the rest of us already know from watching the news: ­Once-in-a-millennium storms no longer require 1,000 years to recur. No scientist can say for certain whether climate change is to blame for the severity of any single storm. They only know that they have to figure out what ­creates and sustains these storms, fast. Better, richer data and more powerful computers would help forecasters build ever more predictive storm models. They could tell those in a tornado’s path not only that a monster is coming, but what kind. Warnings would be longer, more precise, more urgent. Lives would be saved, families left intact.
For decades, Finley and Lee had lost themselves in the vastness of the plains, immersed in the puzzle of mesoscale weather patterns, part of something far larger than themselves. Samaras and the other chasers were a second family to them—a “storm family,” Finley says. And storm chasing, she adds, “is about watching each other’s backs.” Samaras and his crew died, Lee says, because “there was nobody watching their backs.” Because there was nobody, and nothing, to tell them where the great storm was headed next.
Cathy Finley, a storm chaser and a founding member of Twistex.
At the edge of emerald fields of corn and soybeans sits the National Peta­scale Computing Facility, the crown jewel of the University of Illinois at Urbana–Champaign. The 88,000-square-foot glass-­covered facility looks like a fancy convention center, and it’s surrounded by a black steel fence strong enough to stop a speeding Mack truck. Past a retina scanner and through a heavy-gauge steel door resides a computer named Blue Waters. It’s big—spanning 10,000 square feet—and it’s made up of 288 matte-black rack towers that house the 27,000 nodes that are the key to its power. Each node holds two microprocessors, not unlike a stripped-down PC but faster than anything you’ll find at Best Buy.
Since powering up in 2013, Blue Waters has been one of the few computers in the world capable of processing the biggest of big data sets, encompassing everything from the evolution of the universe to the global spread of flu pandemics. It’s also one of the only machines in the world that can model the staggering complexities of a supertornado, which is exactly what an atmospheric scientist named Leigh Orf spent the better part of 2013 failing to do.
At the time Orf was chair of the Earth science department at Central Michigan University, a school in the small town of Mt. Pleasant. “Weather has always been on my radar, so to speak,” Orf says today. The fascination began in the early ’70s, just after bath time in his boyhood home in Ludlow, Massachusetts, when a concussion ripped through the house with the ferocity of a pipe bomb. The wall in his sister’s bedroom was blown out—paneling everywhere, insulation smoking, wires red-hot and glowing. “It scared the living shit out of me,” he says. Orf wouldn’t enter the room for months. He was 5, and he has been obsessed with weather ever since.
A dozen or so years later, when he arrived at the University of Wisconsin–Madison and fell in love with coding, he learned about the community of scientists who had been using computers to simulate storms since the 1970s. In the earliest renderings, most computers couldn’t re-create any features of a tornado that were less than a kilometer wide or tall, meaning they could re-create the broad contours of a storm but none of its important details. Over time, driven in part by advances in microprocessing power, scientists gradually sharpened the resolution from 1 kilometer to 500 meters and eventually to 100 meters, the storm and the tornado steadily coming into focus.
Not focused enough for Orf, though. After spending the early 2000s studying what he considers low-resolution simulations (with features rendered within 250 to 500 meters), Orf came to believe that the only way to understand how and why a supertornado forms was to re-create one in superfine resolution—30 meters, meaning it could render any detail of a storm so long as it measured at least 100 feet long or wide. Compared to the 1-kilometer resolution of early simulations, that’s the difference between the blocky pixelation of Atari and the eerie verisimilitude of Xbox One. To achieve that level of detail, it would take Orf’s personal computer a decade to simulate a single storm—and that’s only if it could handle the data and memory load, which it couldn’t. He would need a computer that could process hundreds of terabytes of data, over and over and over again. A supercomputer, really, only one that hadn’t yet been invented. So he waited.
Superstorm soundings are very rare: It’s hard to take measurements when all hell is breaking loose.
He spent the next decade forging connections with scientists at research facilities around the world. And when the $360 million Blue Waters system came online after two years of construction, one of those connections, Bob Wilhelmson, a pioneer of storm modeling who taught at the U of I, reached out to Orf. With Wilhelmson’s help, Orf got nine months of access to Blue Waters to create what no researcher ever had: a simulated EF-5 tornado that could be studied, and maybe even reverse-­engineered, revolutionizing our understanding of storms that have otherwise defied comprehension.
On his first day of access in December 2012, he logged on from his office in Michigan, keyed in a PIN to access the encrypted network, and prepared to upload a trove of data known as a sounding. Every day, National Weather Service stations all over the country release weather balloons to gather yard-by-yard accounts of how pressure, temperature, relative humidity, and wind speed change from the surface to altitudes as high as 20 miles in the sky. This collection of data, to the untrained eye, looks like an Excel spreadsheet filled with numbers. To translate these raw numbers into a digital storm, Orf would use CM1, a program that uses the meteorological data in any given sounding to simulate a near-infinite combination of weather conditions. An environment that produces a tornado in the real world should spark a similar result in a CM1 simulation, at least in theory. And Orf would then be able to manipulate certain parameters—increasing and decreasing the amount of friction from the ground, tweaking the way the model handles turbulence—to determine which factors are most critical in building, sustaining, and driving a supertornado.
Processing all the data in a sounding and creating a simulation could take Blue Waters anywhere from a few hours to a few days, depending on the amount of data being crunched, and Orf didn’t want to waste any time with unnecessarily complicated soundings. The first sounding he uploaded was synthetic, a simple composite built by scientists that contained all the atmospheric ingredients believed to cause tornadoes. As Orf began to play with the synthetic atmosphere, though, Blue Waters would only produce what he calls “wussy-ass tornadoes.” He spent days adjusting different variables—decreasing the evaporation rate of rain, for example—but nothing worked. Whatever it is that creates a supertornado isn’t found in synthetic soundings, and after months of trial and error Orf knew he needed to do something different. “Screw this,” he remembers telling his wife. “I’m going to Mother Nature.”
Leigh Orf had nine months with a supercomputer to ­simulate an EF-5 tornado.
A friend from the University of North Dakota offered Orf a sounding from a real-life EF-3. Yet even the real thing, when uploaded into the model, spawned nothing more than what Orf calls “spin-ups,” transient tornadoes he has little interest in. Orf would alternate between genuine and synthetic soundings over the following months, never spinning up anything close to a super­tornado. With each day bringing him closer to his Blue Waters deadline, he realized that if he intended to simulate a beast, he needed to find an EF-5 sounding. But superstorm soundings are exceedingly rare, since measurements must be taken near the twister as it forms, and that’s a little tricky when all hell is breaking loose. There is also no centralized database stocked with ready-made superstorm soundings, even at the National Weather Service.
Then, at the end of August 2013, with just a few weeks left before his time with Blue Waters ran out, Orf got an email from Lou Wicker, a scientist at the National Severe Storms Laboratory and a former student of his mentor, Wilhelmson. Wicker heard about Orf’s project and wanted to help, and his email included a file, a warning, and a winking emoji: “Be careful with this sounding,” Wicker wrote. “It could damage the machine.”
He needed a massive supercomputer, one that hadn’t been invented yet. So he waited.
The storm happened on May 24, 2011. Like the one that would kill Samaras two years later, its winds ripped through the plains at more than 200 mph. Along its 63-mile path it killed nine people, injured nearly 200, and rolled a 1.9 million–pound drilling rig three times. It was an EF-5. Orf loaded the sounding into CM1, and even before it formed a tornado the simulation went haywire, depicting wind speeds in the storm’s updraft that had never been documented in nature.
Orf suspected the problem wasn’t with the data in the sounding but with CM1 (or at least the way he was using CM1). For the clouds and rain and wind to swirl in the virtual world as they do in nature, Blue Waters had to communicate the movement of air between its 27,000 nodes, which acted like an interconnected grid. These nodes had to talk to each other and process the data in single, coordinated units of time, called time steps, that Orf could increase and decrease in CM1 as needed. The lower the time step, the faster the wind speeds possible in the program’s simulation. A lower time step also meant longer processing times, meaning Orf would have fewer opportunities to run simulations during his precious remaining days with Blue Waters.
Orf took the risk. He first shortened the time step to 0.3 second, uploaded the sounding, and let Blue Waters run. Two hours passed and the system failed. He then shortened the time step to 0.25 second. Thirteen hours went by, and it failed again. Then, shortly before 10 am on September 16, 2013, he initialized the last simulation he would be allowed to run on Blue Waters. He set a time step of 0.2 second, and since Blue Waters would require the better part of the day to process the data, Orf logged off and tried to think about anything other than tornadoes. When he logged back on at 6 pm, he expected to find that the simulation had blown up again.
It hadn’t.
The model appeared stable, and he saw something big and exceptionally violent on his screen—a tornado. As CM1 translated the EF-5 sounding, the tornado kept getting wider. The intensity, path length, duration—it all matched the characteristics of the real storm. Orf had just simulated the first high-­resolution EF-5 tornado along with its parent supercell ­thunderstorm, and anything researchers had ever wanted to learn about how a supertornado is born, and how it maintains its size and strength, was inside it. All he needed was someone to help him make sense of it all.
Five months later, on February 1, 2014, Orf flew to Atlanta for the annual meeting of the American Meteorological Society. With an iPad full of vivid video clips of his simulated supertornado, he paraded through the halls of the Omni Hotel. After years of toiling underground, he was at last ready to show his storm to the public and enlist other scientists to help him analyze it.
That’s when he ran into two familiar faces: Finley and Lee. The world of severe-weather science is small; everyone knows everyone. Orf hadn’t seen Lee or Finley since the death of their friends in Oklahoma the previous year, and he told them how relieved he was that they hadn’t chased that day. He then pulled them aside, turned on his iPad, and showed them what he had. Their jaws hung slack. The way the rain curtains slid around the tornado, the way the funnel rode the edge of the storm like a column made of cloud and dust: Lee and Finley had seen all this in nature but never in a simulation. Twistex had spent years gathering snippets of data from storms, but their work was like shining a penlight into the pitch black of a cavern. The pieces didn’t form the big picture—the grand schematic of a ­supertornado’s internal and largely inscrutable machinery.
This was the big picture, and they asked Orf to replay the video half a dozen times. They knew they were seeing a simulated storm as no scientists had ever done before, and when Orf asked for their help analyzing the simulation, they jumped at the chance.
On a chilly afternoon this past winter, Finley peered into the guts of the supertornado Orf had first shown her three years earlier, her face washed in the glow of the monitor. School had let out for the holidays, and the halls of Saint Louis University were mostly empty. Since that day in Atlanta, Finley and the others had reoriented their lives around the simulation. Orf is now at the University of Wisconsin, and the folks at Blue Waters have granted him an additional two years of access—as close as one gets to carte blanche.
In her office, Finley let her eyes roam over all the currents she could never see out on the plains. It makes for hypnotic viewing, as though Orf had peeled away the skin of the supertornado to reveal its viscera and sinew. She and Lee can now release digital trackers into the simulated tornado, a little like injecting contrast dye to image blood vessels on a CT scan. They could zoom in on where most of the activity takes place throughout a tornado’s evolution. And their discoveries will help guide storm chasers on the ground as they try to measure and track storms from a safe distance. “We’re going to be able to target our observations to see if we can see in the real world what Leigh’s team is seeing in their simulations,” says Jeff Snyder, a research scientist at the Cooperative Institute for Mesoscale Meteorological Studies who knows Finley and Lee from storm conferences.
No scientist had ever seen a simulated storm like that before.
One of their discoveries calls into question a central assumption of many storm researchers. For years Finley and Lee studied a downdraft at the rear flank of tornadoes, and like most of their colleagues they believed it played a crucial role in perpetuating the twister’s ­life cycle. Their simulations, however, suggest that most of the action during the genesis of a tornado happens near the front of the storm, with a spinning riot of air, or what they’ve christened “a streamwise vorticity current,” that pushes air upward more forcefully. It’s a ­theory they still need to prove, but if it holds up to further testing and observation, Finley says, “we’ve been focusing on the wrong thing all this time.”
It will take Finley, Lee, and Orf years to fully understand this particular EF-5, and they plan to simulate an ensemble of different supertornadoes, hoping to draw out their common features. They’ll soon benefit from even greater computer power. The coming age of ­exascale computing—and the transition from silicon toward carbon nanotubes and chips made from gallium nitride or graphene—will offer processing times magnitudes faster than the petascale computing at Blue Waters. That means resolutions impossible just a few years ago. Five meters, maybe even less.
The Simulated Supertornado
Leigh Orf’s detailed storm simulations allowed him, Cathy Finley, and Bruce Lee to spot a n­ever-before-seen feature of a tornado: a helix-shaped tube of air racing around the central funnel. They’ve dubbed it the streamwise vorticity current, and it could be a breakthrough in understanding how the world’s worst twisters work. —Chelsea Leu
The vortex: This thick column of orange stuff is a bunch of swirling air that forms the vortex of the tornado.
Cold pool: A region of cold air beneath the storm, formed by evaporating rain, that feeds the streamwise vorticity current.
Streamwise vorticity current: This spiraling river of air flows upward from the tornado’s forward flank into the core of the storm’s updraft, and the researchers believe that the low pressure helps sustain and drive the tornado.
Anticyclonic vortices: The chaotic air around of the tornado a tornado is filled with many smaller vortices, some of which eventually get absorbed into the growing funnel. In Orf’s simulation, these are color-coded by the direction the air is rotating—blue indicates that the air is spinning clockwise; red, counterclockwise.
It could also mean the power to capture and analyze the inner workings of hurricanes like Harvey, Irma, Jose, and Maria. Whereas the widest tornadoes measure 2 1⁄2 miles across, storms such as Harvey and Irma are 100 times as wide. That’s a lot more meteorological data to process and analyze, and once the next generation of supercomputers comes online, scientists might finally have the processing power to create something with the richness and detail of Orf’s EF-5­—an operational model that spans the entire globe, producing hi-res simulations of ongoing hurricanes.
But that’s all on the horizon. In the meantime, Finley, Lee, and Orf are focused on Blue Waters, and Orf has already chosen the next monster to fire across its nodes. This sounding was buried inside archives used by government forecasters and researchers, and Orf only discovered it two years ago. It took place on the late afternoon of May 31, 2013, near El Reno, Oklahoma. It killed Samaras, his son Paul, and Carl Young. For however long it takes, Orf, Finley, and Lee will study the simulated supertornado from the safety of their laptops, breaking it down and building it up and searching for answers in a storm of code.
Brantley Hargrove (@BrantHargrove) is the author of The Man Who Caught the Storm, a biography of Tim Samaras, coming soon.
This article appears in the November issue. Subscribe now.
Listen to this story, and other WIRED features, on the Audm app.
Leigh Orf, an an atmospheric scientist, narrates a simulation of a superstorm tornado created by one of the world's most powerful supercomputers.


On October 1, Stephen Paddock killed 58 people and wounded 546 more, firing multiple rifles from a hotel room in Las Vegas overlooking an outdoor concert. Then he killed himself. No one knows why he did it.
As part of the attempt to figure that out, The New York Times and others report, the Clark County Coroner’s office is sending Paddock’s brain to the Stanford University lab of Hannes Vogel, a neuropathologist. Vogel (who, at the request of Stanford’s communications office, is not speaking to the press) will perform both visual and microscopic examinations of Paddock’s brain, looking for abnormalities, tumors, degenerative illnesses, or anything else that might suggest why an otherwise unassuming video poker player would turn his extensive gun collection on innocent people.
Nobody thinks it’s going to work.
Sure, this is due diligence, part of a complete investigation. Vogel is a pathologist, so maybe he’ll find something pathological—maybe a tumor in the ventral medial or dorsolateral prefrontal cortex, parts of the brain that have to do with impulse control and willpower. Damage there, or maybe to the inferior posterior ventral cortex can also make people more violent.
Under a microscope, stained with various dyes, a brain can reveal degenerative disorders that can contribute to depression or poor emotional control.
But people get tumors and degenerative diseases all the time. Most of them don’t become vicious killers. “If you have a mass murderer and you looked at his brain, it would completely amaze me if you could see anything relevant or useful,” says Christof Koch, chief scientist and president of the Allen Institute for Brain Science. “The brain probably looks relatively normal.”
That, in short, is the “mind-body problem” that scientists and philosophers have struggled with for as long as there have been scientists and philosophers. You have the meat (billions of neurons and other gunk performing electrochemically networked computation) and you have the ghost—the perception of sensory inputs, their reconstruction inside the brain, the processing of those reconstructions into something comprehensible, and maybe even an awareness of some of that processing. In other words: consciousness.
Neuroscientists actually know quite a lot about the human brain and the mind it generates. At the Allen Institute and elsewhere, they’re building maps of all those neural connections. They can build interfaces into it, wires that go through the skull and into the cortex so that people can control wheelchairs or play piano. By applying machine learning algorithms to brain activity as volunteers watch images for hours inside an fMRI tube, computers can learn to infer from changes in that activity what the person is seeing—to, in essence, read people’s minds. And it works on more than visual input; you could put the same algorithms to work figuring out how someone felt. Is that flower pretty? Is that car cool? Which dress looks better? “I see no a priori reason that couldn’t be done today," Koch says. "Maybe someone is doing it."
It’s not phrenological poppycock to say that brains change physically with new experiences. It’s called plasticity. London cab drivers, for example, famously have to acquire a detailed spatial map of the city as part of their training, and they also acquire an enlarged hippocampus. Stimuli and experiences perceived by the mind change the connections in the meat.
As a presumably skilled user of guns (and player of poker and frequent guest in Las Vegas hotels), maybe Stephen Paddock’s brain has some physiological variance from someone who wasn’t any of those things. But none of that is likely to explain why he kept pulling that trigger. “Sure, you can point to locations in a brain and say, yeah, this general part of it is processing this kind of information,” says Michael Graziano, a neuroscientist at Princeton University. “You can point to structures deep in the brain and say, ‘these have a crucial role to play in emotion, and when this spot revs up it’s associated with anger and rage.’ But in terms of understanding—at the detail level, the circuitry level—how it works and why one person is different from another? Wow, that’s really unknown.”
Investigators and others looking for answers about Las Vegas would like to find that one mechanistic synapse, the one electrical moment that went zing instead of zap and led to the deaths of 58 people, to trauma and injury felt by so many more, rippling outward from that concert. It would give it all some kind of meaning. But it isn’t there. Even if it was, it wouldn’t change what happened.
Controversial Brain Imaging Uses AI to Take Aim at Suicide Prevention
Your Brain Doesn't Contain Memories. It Is Memories
Psychosurgeons Use Lasers to Burn Away Mental Illness
It might, perhaps, keep it from happening again. Maybe you could imagine a bigger research project. Take the brains of 100 mass murderers, or 1,000, and put them through an analytical gauntlet. Do the same for brains of non-murderers—with the researchers blinded to which brains are which. Maybe you’d find a difference, as one researcher did with the brain scans of violent psychopaths.
But then what? Could you imagine, say, pre-emptive surgery or psychopharmacological intervention on people who hadn’t done anything wrong because their brains looked murderous? The researcher working on psychopathy found the tendency in his own brain, too—and he wasn’t a murderer. “If it is true there are systematic differences in the criminal brain, which I sort of doubt, it’ll be good to understand why those brain structures might give rise to criminal behavior,” Koch says. “Then maybe that’ll teach you more how to help people.”
Of course, Koch adds, society already knows some ways to keep children from tending to grow up to become criminals: Give them a good education, proper nutrition, and limited exposure to violence and other stressors.
That knowledge makes the mysteries of Stephen Paddock’s brain even more chilling. People will look at his brain—at the meat, the machine, the matter—in the hope of finding something to blame for his rampage. They’re looking for a physical instantiation of the mind. Because without one, the fault has to fall somewhere else—on some shadowy cause that pushed an otherwise sane mind to commit an insane act, or on the ease with which a quietly insane mind could acquire so many dangerous weapons.
If Veber doesn’t find a tumor or other disease in that brain, the problem with Stephen Paddock won’t have been pathological. It’ll be psychological—or societal. And that’ll mean that society will have to try to fix it.
By implanting a tiny computer chip into the brain and decoding the data that’s produced, Brown neuroscientist John Donoghue is hoping to help people with paralysis reconnect to the world around them. In this World Economic Forum discussion he explains how.


This story originally appeared on Slate and is part of the Climate Desk collaboration.
When Superstorm Sandy ripped through New York City in October 2012, it did not discriminate. At the construction site of the new Whitney Museum of American Art, chief operating officer John Stanley recalls “mechanical equipment bobbing like corks” in the floodwaters. And at the Rubin Museum of Art, a few blocks uptown, and upland, the museum lost power—a necessity for preserving the artifacts from environmental damage—and the backup generators weren’t enough to keep the facility running. “We thought if we do lose power, in the history of New York City, it would be for a day or two,” executive director Patrick Sears says. “No one really anticipated we could go without power for a week.”
But as once-rare storms like these become more common and more consequential (Sandy caused an estimated $70 billion in damage, behind only Hurricane Katrina), coastal communities are reorienting to a world where they might be underwater at a moment’s notice. And museums are leading the charge when it comes to bolstering up in the face of extreme weather—after all, financially speaking, they might have the most to lose. Along the Eastern Seaboard, from Miami to Manhattan, curators are going to extremes to safeguard their art. And in doing so, they’re testing out ideas and processes that might later be adopted by everyone else who lives on the coast.
Looking back, Stanley says the timing of Superstorm Sandy was actually fortuitous for his museum, the Whitney. Because it was early enough in construction, the team was able to revise its plans with water in mind. “We searched the world for flood experts and engineers,” he says. With the help of WTM Engineers in Hamburg, Germany, the Whitney design team re-evaluated the entire site and, as the Atlantic reported in 2015, built one of the most flood-resilient structures in town.
All along the Eastern Seaboard, from Miami to Manhattan, curators are going to extremes to safeguard their art.
As a result of lessons learned in Sandy, the museum is waterproof up to 16½ feet thanks to its raised elevation and carefully selected materials. It’s also got walls galore: A 500-foot-long mobile wall can be constructed in less than seven hours to protect the museum from a storm surge’s impact, and a 14-by-27-foot flood door can withstand the force of a semitruck floating (or flung) across the West Side Highway. Stanley says it cost just $10 million more to disaster-proof what was, in total, a $220 million project. And though the safeguards haven’t been tested the hard way, he’s confident they’ll rise to the occasion if—or rather, when—another disaster unfolds.
Some museums farther south on the Atlantic seaboard have already lived to see their hurricane-resistant designs tested by storms. Employees at the Salvador Dalí Museum in St. Petersburg, Florida, recently weathered Hurricane Irma with little damage. Back in July, a videographer for the Washington Post filmed inside the “surreal shelter from the storm.” To protect the precious collection, the Dalí relies on 18-inch-thick walls, which are built to withstand the winds of a Category 5 storm, and fortified glass, which can hold up under the pressure of Category 3 winds. As with so many other museums, the Dalí’s decision to gird its infrastructure seems financially sound: If its walls were breached, the largest collection of Salvador Dalí paintings in the world, priceless and carefully preserved over the past century, could be lost in an instant.
The architectural features that make the Whitney, Dalí, and similar spaces so safe have recently begun to proliferate far and wide, thanks in part to consumer demand and new municipal standards. Perhaps the purest emblem of this surge-priced survival model is the new residential American Copper Buildings. Like the Whitney, these structures sit in Evacuation Zone 1, but on Manhattan’s eastern shore. While it seems damage from another hurricane is all but guaranteed, the waiting list for a unit in one of the American Copper towers is long.
That’s due primarily to the fact that the $650 million buildings, which were started before Sandy hit, reportedly go beyond even the city’s newest resilient design codes—and look great doing it. Connected by a three-story skybridge, the two towers have an elevated lobby that makes them virtually waterproof. The building is also served by rooftop backup generators that promise enough energy to run the elevators plus one fridge and one electrical outlet in each apartment indefinitely. In January, the New York Times wrote this glowing report:
There is a breathtaking view of the mid-Manhattan skyline, pierced by the Empire State Building, from the 48th floor of the taller of two new copper-clad apartment towers along the East River, just south of the United Nations.
No plutocrat will enjoy it, however. This impressive penthouse aerie is hogged by five emergency generators. The window is already blocked by a bank of electrical switchgear. For the developers, giving up premium space to machinery is insurance against an ominous future: They want tenants in the towers’ 760 apartments to be able to live in their apartments for at least a week, no matter how high floodwaters may reach nor how long the power is out. Sure, in the face of an impending storm, residents will still have to get the hell out just like any other New Yorker adhering to evacuation mandates. But American Copper promises them a return to a clean, safe, and electrified home.
Only 39 percent of Americans have a disaster preparedness plan.
Though JDS Development Group, which owns American Copper Buildings, may have been leading the charge on resilient design, the rest of New York City’s new construction is quickly catching up. After Sandy, the Mayor’s Office of Recovery and Resiliency set about studying the metro area’s weather and climate vulnerabilities and crafting solutions. Recently, the city began implementing new building codes, and all new construction is now held to these updated resiliency standards. “We’re not just doing one-off resilience projects. We’re baking resilience into the entire capital program,” the city’s chief resilience officer Daniel Zarrilli says.
Even with the support of the city, resilient design can be hard to scale. Retrofitting old buildings is harder than raising more capital to bolster new designs, according to many architects. Raising an existing single-family home on stilts, as many thousands of East Coasters have done since Sandy, can cost more than $100,000—on a house that’s maybe only worth $400,000. That means that while the Whitney’s resilience costs were less than one-twentieth of the new project cost, the owner of an existing home is looking at resilience costs as high as one-fourth of their total property value. While some local and federal support has been made available to storm victims, the costs of these programs have quickly ballooned—even after many withdrew their applications due to overwhelming bureaucracy and out-of-date flood maps.
It’s clear that equitable resilience will take not just effort and money, but time. “There will just be a slow changeover of the entire housing stock in New York City that slowly meets these codes,” Simon Koster of JDS Development Group says. Given that 66 percent of New York City’s buildings were built before 1960 and aren’t likely to change over in the near future, this doesn’t seem particularly hopeful.
But other less intensive measures are being taken to ensure New Yorkers weather the next storm—and museums can serve as a model here, too. The Rubin, which showcases art from the Himalayan region, didn’t have the budget to undertake big post-Sandy capital improvement projects. While the board paid for a few big-ticket items like a stronger, waterproof roof, it’s poured most of its efforts into better training and communication. “We’re thinking about manual ways, simple ways, things you can buy on Amazon,” Sears says. One of his favorite investments is a windup cellphone charger that doesn’t require an electricity source.
Unlike 18-inch concrete walls, disaster plans like these can be constructed by anyone. But a 2015 Federal Emergency Management Agency survey showed only 39 percent of Americans have their own plan in place. The Rubin, which has a disaster plan 153 pages long, believes this has to change. Other museum strategists agree: “You can call it paranoia, or you can call it strategy,” says Kathy Greif of the Dalí Museum. “I prefer to call it strategy.”
If museums are so prepared, could they help the rest of us—literally? Not really. Unsurprisingly, you won’t be weathering the next hurricane from inside the Met. Though all of the museum leaders I spoke with agree that human lives matter more than paintings, serving as a shelter still seemed to compromise their central mission, which is protecting their collections. Even if it could theoretically provide reprieve, the Whitney sits on the leading edge of Evacuation Zone 1, which means people should be headed out of the neighborhood, not into even the most disaster-proof buildings. The Rubin, meanwhile, wouldn’t physically have the space to serve as a shelter during a flood, as art typically hung in lower-level galleries would be moved into many of the hallways and upper galleries. In the end, its strategies like these that will save the precious artwork. But it’s clear they’ll limit room for, well, people.
In the decade since Hurricane Katrina, tools for tracking and modeling powerful storms have grown in sophistication and detail.


People at DigitalGlobe are fond of saying they've built a "time machine of the planet." And, if we're being metaphorical, that's true: The satellite imaging company has used orbiters to take and store super sharp images of Earth for 17 years. Their eyes have gazed down as trees have disappeared and appeared, floodwaters have flowed in and out, and cities have boomed and busted. But that time-lapse is only useful if someone can make sense of its meaning. Today, that's where companies like Orbital Insight come in, with artificial intelligence that parses the pictures and auto-says things like, "That looks like deforestation," or "There are lots more cars in those parking lots than there were last year. Business be booming."
In the world of satellite imagery, there are a few kinds of players: the picture-takers, the sense-makers, and those who are trying to do both. DigitalGlobe, with its ever-increasing cache of high-def history, is the planet's lead picture-taker; Orbital Insight is one of the frontrunning sense-makers. And today, the two companies announced a new multiyear, multipetabyte partnership. Orbital Insight, in other words, has the keys to the time machine. “We want to let them race,” says Shay Har-Noy, DigitalGlobe's vice president and general manager of platform.
With more images—including more new images more often—Orbital Insight's artificial intelligence will become more insightful. In teaming up, these two companies have ensured that neither of them has to be all things to all people; instead, they can each do what they do best. The insights they produce together can affect economies, militaries, the planet's health, the state of its infrastructure, the way we deal with disaster—and so, in turn, you.
The "why now" of this partnership has a simple answer: The Cloud. For a long time, if someone wanted DigitalGlobe's satellite imagery, the company had to send the huge snapshots over a relatively slow connection, or in the actual mail. The image archive was largely on old-school magnetic tape, spun through cartridges and stored in a library. That made it hard for anyone to get the big picture. “If nobody can access the time machine, it isn't worth very much,” says Har-Noy.
That's how it was when Orbital Insight, then a new company, started requesting pics. In those early days—lo, these three years ago—it could only get so much imagery at once. And its algorithms—which learn as they're exposed to more examples, and which can only give deep context if they have access to deep time—could only learn so much.
That changed in May, when DigitalGlobe put its entire archive, all 100 petabytes of it, onto an unmarked semi-truck and sent it to the cloud-based Amazon Web Services. Now, Orbital Insight can access and analyze the images where they are—on the web—rather than waiting for them to arrive.
Then things really start to happen. Pictures are cool (fact). People like to take them and have them and post them and share them. But when your phone is so full of snaps that you never swipe through all of them, or you can't even remember what that first album was, your collection becomes less useful. Similarly, DigitalGlobe's customers don't necessarily want 25 shots of Beijing taken over five months. They don't want to see anyone's work; they just want the answer to their question.
Orbital Insight is now better primed to give them, using DigitalGlobe's Geospatial Big Data Platform, or GBDX (also a good name for a culty workout system). Analysts can log on, grab images, and write an algorithm to, say, identify and count cars. You train that algorithm with pictures of a bunch of cars in a bunch of conditions, run the algorithm on new images, and sell the result: plain-language statements about a place's busy-ness and what it means for business. It's the same basic process artificial intelligence researchers follow for self-driving car or facial recognition software. “I want to get people to stop working on cat videos and start working on something that matters,” says Har-Noy. There are, he adds, a lot of artificial intelligence experts working on cat videos.
Orbital Insight CEO James Crawford has no such low-key, feline goals. “Our mission is to understand what we're doing on and to the Earth,” he says. Which would have sounded less convincing before this "expanded partnership" with DigitalGlobe. Neither company will talk about just how expanded the partnership is. But Har-Noy will talk qualitatively. “It's not a little bit of content," he says. "It's all you can eat.”
Then he pauses. “It's not quite all you can eat,” he clarifies. “But it's effectively meant to be on that order of magnitude.” DigitalGlobe has even agreed to spy on Orbital Insight's spots of most interest more frequently.
On Orbital Insight's menu are more stores, for car-counting; more oil tanks, to estimate crude supply; more mapping of poverty, so organizations like the World Bank know where to focus; and more tracking of deforestation, so algorithms can tell NGOs where to say, "Stop!"
As Orbital Insight sells the product of its analysis, DigitalGlobe, in return, gets a cut of the company’s revenue. Plus, this bonus: As the Orbital Insights of the world show the world about the world using pictures of the world, people will want more of those pictures. And for that, they will often come to DigitalGlobe, which still has the biggest volume of the highest-quality imagery in the industry.
Still, the partnership demonstrates the strange dynamics between the image-makers and the sense-makers. They can't function without each other, but neither are they exclusive. Orbital Insight isn’t beholden to DigitalGlobe. It will get images from whatever satellite constellations it pleases—from Planet, e-GEOS, Sentinel. “I believe that the industry is best served by avoiding walled gardens,” says Crawford. “Our leaning is to have open competition on both sides.”
And there is. “We're your partners," Har-Noy says he has told Orbital Insight. "But we absolutely want there to be 100 Orbital Insights.”
That's fine, in Crawford's opinion. "We're happy to have the satellite companies sell their imagery to multiple companies," he says. "We happen to think we're better than anybody else, so that doesn't worry us."
In October of 1957, the Soviets kicked off the Space Age when they launched Sputnik, the world's first artificial satellite. How far has the technology of satellites come since then? Let's take a look...


This story originally appeared on Mother Jones and is part of the Climate Desk collaboration.
When hurricanes Harvey, Irma, and Maria recently pounded Texas, Florida, and Puerto Rico, the severity of the storms was often attributed to climate change. “Four Underappreciated Ways That Climate Change Could Make Hurricanes Even Worse,” was the headline in the Washington Post; “Hurricane Harvey’s Size and Impact Points to Climate Change,” noted NPR.
But five years ago, when Superstorm Sandy took its famous left turn to barrel into the East Coast, there was no such certainty. Instead, a debate played out in the media and on Twitter about whether it was fair to blame climate change for the storm’s intensity.
One reason why the conversation has changed is that the science has become much more advanced, especially in the field of attribution science—a relatively new discipline within climate science that looks at how climate change factors into individual weather events. During Hurricane Katrina in 2005, attribution science was in its “infancy,” one scientist told the Post, and even when Sandy hit, the dominant narrative was that climate change doesn’t necessarily affect individual extreme weather events. But today, both the computer models and how scientists have learned to communicate the lessons of climate science have become more sophisticated, and there is a growing body of peer-reviewed published literature on the subject, though not without some controversy.
No one suggests that climate change caused a single weather event, but scientists have grown a lot more comfortable talking about the ways rising temperatures affect extreme weather, creating the right conditions to fuel wetter storms and bigger, more dangerous storm surges.
One issue that’s received more attention is that, in the past, attribution studies started “with the assumption that a given event is ‘natural,’ and the burden of proof is on the claim that the event was caused, exacerbated, or made more likely” by human causes, social scientists Lisa Lloyd and Naomi Oreskes and climate scientist Michael Mann wrote in an article for Climatic Change that was published in 2017. “The null hypothesis is that there is no human contribution.”
But then came Superstorm Sandy. Researchers used data from the storm to try to get a handle on the effects of climate change on extreme weather. One of the major papers was from the National Center for Atmospheric Research’s Kevin Trenberth and two colleagues in 2015. They argued that it was necessary to come up with a new way of approaching attribution. Even though computer models weren’t yet sophisticated enough to account for all the atmospheric dynamics in climate change, it was still possible to account for other dynamics. The oceans are warmer and higher, and the atmosphere holds more moisture than it used to, which is linked to heavier rainfall.
Using Sandy’s weather models, which were very accurate, the researchers could alter the heat and the moisture levels of those models to account for climate change. The conclusion? “We find that indeed Superstorm Sandy is more intense, it is bigger, the rainfall is heavier, and so there’s a climate change component to the strength of that storm,” Trenberth tells Mother Jones. “The environment in which all these storms, including Superstorm Sandy, is occurring is fundamentally different than it used to be.”
That was not the only challenge to conventional thinking. Starting in 2011, a year before Sandy, the Bulletin of the American Meteorological Society began publishing an annual state-of-the-science report titled “Attribution of Extreme Weather Events in the Context of Climate Change.” Last year, Heidi Cullen, a chief scientist with Climate Central studying weather variability, wrote that the annual work had the potential to have the same impact on the conversation around weather and global warming as the surgeon general’s 1964 report on smoking and lung cancer.
“Scientists are now able to assess, in some cases within days, whether and how much the risk of such an extreme weather event has changed compared to the past—that is, before heat-trapping greenhouse gases altered our climate,” she wrote in the New York Times.
Even if the science has become more precise in understanding the nuances and connections between climate change and extreme weather, many politicians aren’t listening. EPA Administrator Scott Pruitt has said that to “have any kind of focus on the cause and effect of the storm, versus helping people, or actually facing the effect of the storm, is misplaced.”
In the decade since Hurricane Katrina, tools for tracking and modeling powerful storms have grown in sophistication and detail.


When someone takes their own life, they leave behind an inheritance of unanswered questions. “Why did they do it?” “Why didn’t we see this coming?” “Why didn’t I help them sooner?” If suicide were easy to diagnose from the outside, it wouldn’t be the public health curse it is today. In 2014 suicide rates surged to a 30-year high in the US, making it now the second leading cause of death among young adults. But what if you could get inside someone’s head, to see when dark thoughts might turn to action?
That’s what scientists are now attempting to do with the help of brain scans and artificial intelligence. In a study published today in Nature Human Behavior, researchers at Carnegie Mellon and the University of Pittsburgh analyzed how suicidal individuals think and feel differently about life and death, by looking at patterns of how their brains light up in an fMRI machine. Then they trained a machine learning algorithm to isolate those signals—a frontal lobe flare at the mention of the word “death,” for example. The computational classifier was able to pick out the suicidal ideators with more than 90 percent accuracy. Furthermore, it was able to distinguish people who had actually attempted self-harm from those who had only thought about it.
Thing is, fMRI studies like this suffer from some well-known shortcomings. The study had a small sample size—34 subjects—so while the algorithm might excel at spotting particular blobs in this set of brains, it’s not obvious it would work as well in a broader population. Another dilemma that bedevils fMRI studies: Just because two things occur at the same time doesn’t prove one causes the other. And then there’s the whole taint of tautology to worry about; scientists decide certain parts of the brain do certain things, then when they observe a hand-picked set of triggers lighting them up, boom, confirmation.
In today’s study, the researchers started with 17 young adults between the ages of 18 and 30 who had recently reported suicidal ideation to their therapists. Then they recruited 17 neurotypical control participants and put them each inside an fMRI scanner. While inside the tube, subjects saw a random series of 30 words. Ten were generally positive, 10 were generally negative, and 10 were specifically associated with death and suicide. Then researchers asked the subjects to think about each word for three seconds as it showed up on a screen in front of them. “What does ‘trouble’ mean for you?” “What about ‘carefree,’ what’s the key concept there?” For each word, the researchers recorded the subjects' cerebral blood flow to find out which parts of their brains seemed to be at work.
Then they took those brain scans and fed them to a machine learning classifier. For each word, they told the algorithm which scans belonged to the suicidal ideators and which belonged to the control group, leaving one person at random out of the training set. Once it got good at telling the two apart, they gave it the left-out person. They did this for all 30 words, each time excluding one test subject. At the end, the classifier could reliably look at a scan and say whether or not that person had thought about killing themselves 91 percent of the time. To see how well it could more generally parse people, they then turned it on 21 additional suicidal ideators, who had been excluded from the main analyses because their brain scans had been too messy. Using the six most discriminating concepts—death, cruelty, trouble, carefree, good, and praise—the classifier spotted the ones who’d thought about suicide 87 percent of the time.
“The fact that it still performed well with noisier data tells us that the model is more broadly generalizable,” says Marcel Just, a psychologist at Carnegie Mellon and lead author on the paper. But he says the approach needs more testing to determine if it could successfully monitor or predict future suicide attempts. Comparing groups of individuals with and without suicide risk isn’t the same thing as holding up a brain scan and assigning its owner a likelihood of going through with it.
But that’s where this is all headed. Right now, the only way doctors can know if a patient is thinking of harming themselves is if they report it to a therapist, and many don’t. In a study of people who committed suicide either in the hospital or immediately following discharge, nearly 80 percent denied thinking about it to the last mental healthcare professional they saw. So there is a real need for better predictive tools. And a real opportunity for AI to fill that void. But probably not with fMRI data.
Artificial Intelligence Is Learning to Predict and Prevent Suicide
The Chatbot Therapist Will See You Now
Virtual Therapists Help Veterans Open Up About PTSD
It’s just not practical. The scans can cost a few thousand dollars, and insurers only cover them if there is a valid clinical reason to do so. That is, if a doctor thinks the only way to diagnose what’s wrong with you is to stick you in a giant magnet. While plenty of neuroscience papers make use of fMRI, in the clinic, the imaging procedure is reserved for very rare cases. Most hospitals aren’t equipped with the machinery, for that very reason. Which is why Just is planning to replicate the study—but with patients wearing electronic sensors on their head while they're in the tube. Electroencephalograms, or EEGs, are one hundredth the price of fMRI equipment. The idea is to tie predictive brain scan signals to corresponding EEG readouts, so that doctors can use the much cheaper test to identify high-risk patients.
Other scientists are already mining more accessible kinds of data to find telltale signatures of impending suicide. Researchers at Florida State and Vanderbilt recently trained a machine learning algorithm on 3,250 electronic medical records for people who had attempted suicide sometime in the last 20 years. It identifies people not by their brain activity patterns, but by things like age, sex, prescriptions, and medical history. And it correctly predicts future suicide attempts about 85 percent of the time.
“As a practicing doctor, none of those things on their own might pop out to me, but the computer can spot which combinations of features are predictive of suicide risk,” says Colin Walsh, an internist and clinical informatician at Vanderbilt who’s working to turn the algorithm he helped develop into a monitoring tool doctors and other healthcare professionals in Nashville can use to keep tabs on patients. “To actually get used it’s got to revolve around data that’s already routinely collected. No new tests. No new imaging studies. We’re looking at medical records because that’s where so much medical care is already delivered.”
And others are mining data even further upstream. Public health researchers are poring over Google searches for evidence of upticks in suicidal ideation. Facebook is scanning users’ wall posts and live videos for combinations of words that suggest a risk of self-harm. The VA is currently piloting an app that passively picks up vocal cues that can signal depression and mood swings. Verily is looking for similar biomarkers in smart watches and blood draws. The goal for all these efforts is to reach people where they are—on the internet and social media—instead of waiting for them to walk through a hospital door or hop in an fMRI tube.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?


No two screams sound alike. They can swing in pitch, like the The Wilhelm; blare cacophonous, like The Donald Sutherland; or fire off spasmodically, like (my favorite) The Shelley Duvall. Few vocalizations are as nuanced as the fearful shriek. And yet, no matter the cry in question, you always know a scream when you hear one.
So, then. Whence does the scream derive its unmistakeability? "If you ask someone on the street, they'll tell you a scream is loud and high-pitched," says David Poeppel, a neuroscientist at New York University and the Max Planck Institute in Frankfurt. "Turns out it's neither."
Poeppel would know. In 2015, he and his colleagues set out to distinguish cries of fear from other noises. They began by compiling a database of screams. "We spent many joyful hours scanning the interwebs for weird material from YouTube and movies," Poeppel says, "but then we also brought people into the lab and had them scream."
Then came time to dissect their samples. At first, Poeppel, too, suspected that volume and pitch were what made a scream a scream. But when he and his researchers analyzed the auditory properties of the sounds in their database, they found what they actually shared was an acoustic quality called roughness, a measure of the rate at which a sound fluctuates in volume. Normal human speech, for instance (which the researchers also analyzed), varies in loudness four to five times per second; screams, however, waver between 30 and 150 times per second. That means they occupy a unique place in the soundscape of human vocalizations, which may be why they're so attention-grabbing.
That information in hand, Poeppel had test subjects rank a series of sounds according to how alarming they found them—and the rougher a sound was, the more jarring people regarded it. They found the same to be true of car-, house-, and ambulance-alarms, which share screams' rough acoustic quality.  "There's something shared by these sounds that hijacks your brain and says: Hey, something's up," Poeppel says.
How the Godzilla Team Designed the Monster's Iconic Scream
Cue the Scream: Meet Hollywood's Go-To Shriek
This Is a Golden Age of Low-Budget Horror
To investigate how that hijacking works, the researchers tracked subjects’ brain activity while they listened to rough and neutral noises inside a functional magnetic resonance imaging scanner. To no one's surprise, all the sounds caused blood to flow to the auditory cortex. But rough sounds also sent blood rushing to the amygdalae, two small, almond-shaped lumps of grey matter associated with processing emotional reactions like fear. Intriguing, Poeppel says, but it got weirder: "The amygdala's response wasn't an on-or-off thing. It was a graded response." In other words: The rougher the sound, the more activity it produced. The amygdalae, themselves, behaved like little, scream-sensitive sound meters.
The next step is to analyze other types of screams—from irate shouts to cries of ecstasy—and how they affect perception and brain activity. "I'm comparing three positive valences and three negative," says Planck Institute researcher Natalie Holz, head of this new investigation. "For the negative we of course included fear again, but we're also comparing it with cries of anger and pain. For the positive, we are looking at screams of achievement, pleasant surprise, and sexual pleasure."
I asked Holz and Poeppel to keep me posted on their findings. I'll holler when I hear back.


It's Halloween time. For many, this time of the year means there are lots of cool costumes. For others, it's all about the candy. Now, I'm not a big fan of candy—but I am a big fan of analyzing stuff. So here we go: I'm going to look at the energy density for candy. Sure, I could just look this up—but it's much more fun to determine it for myself.
Let's get started by going to the store. I am going to look at a bunch of different candy options and get some data. Fortunately, food has to contain some basic information on the package: mass, calories, sugars, fats, carbs. I can also get the price.
But what do you do with a lot of data? You start making graphs. I will start off with a plot of calories vs. mass. But wait! I don't want to use calories as a unit of energy because it's a pretty dumb unit. The silliest thing about a calorie is that there are actually two different calories.  In chemistry, there is a calorie where 1 calorie is the energy needed to raise gram of water by one degree Celsius.  Then there is the food calorie which is equivalent to 1000 chemistry calories.  Of course, in both cases people just call them "calories".
For me, I will stick with the joule as a unit of energy.  For the conversion, 1 food calorie is equal to 4,184 joules.
Now for a quick run down to the convenience store to gather candy ... ahem, data. I'm assuming here that you all trick or treat in only the best neighborhoods—you know, the ones that hand out full-sized candy bars. With a look at individual candy bars (and other stuff), I grabbed the mass and energy in 20 different sweet treats, from M&Ms and KitKats and Twix to Sweet Tarts and Skittles and Sour Patch Kids.
Here is a plot of energy in joules vs. mass in kilograms. (You can see the full list of candy the "data" tab if you click on the plot.)
It's not a perfect linear relationship between mass and energy, but it's close—and that relationship gives you a candy's energy density. The energy density is just the energy in the bar divided by the mass of the bar. Technically this energy per mass would be called the "specific energy"—since it is per mass instead of per volume. But it's still the same idea, and I like the term energy density better.
There are some outliers, like that data point far up to the right with almost 4 MJ of energy in 144 grams of candy—a super-high energy density. Can you guess what it is? A Lindor truffle! The one that lies below the line of best fit, just over 1.5 MJ, has a lower than average energy density—that's gummy bears. Oh, one more outlier: Hershey's Cookies & Cream. That has a pretty low energy density too.
What Can You Do With All That Halloween Candy?
The Science Behind All the Best Halloween Party Tricks
Google Tells You How Unoriginal Your Halloween Costume Is
All candies are not equal; some might have more stuff in them that does not add to the total calorie value. But overall, there seems to be fairly nice correlation between candy mass and candy energy. From the graph, candy has an energy density around 22 megajoules per kilogram. But how does this energy density compare with other things? Gasoline has 46.4 megajoules per kilogram and a lithium-ion battery is somewhere under 1 MJ/kg. Where should candy be on this spectrum?  The closest thing I could find is glucose with an a specific energy of 15.5 MJ/kg. That's pretty close to my value of 22 MJ/kg from the data above.
Just for fun, here is one more plot.  What about a plot of energy in the candy vs. the price?
Again, there is a fairly linear relationship between energy and cost. From the graph, I get about 7.5 megajoules per dollar. (If you think Lindor truffles are too expensive, this actually argues in their favor; their cost-to-energy ratio is in line with other "cheaper" candies.) But no matter what you get, it's clear that there is a bunch of energy in candy bars. What are you going to do with all the energy you collect on Halloween?
This year, the Halloween industry is expected to bring in an all-time high of $8.4 billion in revenue. Let's take a look at some of the numbers and see why Halloween is good for the economy!


By any measure, the fires that tore through Northern California were a major disaster. Forty-two people are dead, and 100,000 are displaced. More than 8,400 homes and other buildings were destroyed, more than 160,000 acres burned—and the fires aren’t all out yet.
That devastation leaves behind another potential disaster: ash. No one knows how much. It’ll be full of heavy metals and toxins—no one knows exactly how much, and it depends on what burned and at what temperature. The ash will infiltrate soils, but no one’s really sure how or whether that’ll be a problem. And eventually some of it—maybe a lot—will flow into the regional aquatic ecosystem and ultimately the San Francisco Bay.
That’s the bomb. Here’s the timer: An old, grim joke about California says that the state has only three seasons: summer, fire, and mudslides. Those mudslides happen because of rain; the Santa Ana (or Diablo, if you’d prefer) wind-driven wildfires of autumn give way to a monsoon season that lasts through winter and into spring. The rains of 2016–2017 ended a longstanding drought and broke all kinds of records.
Scientists and environmental health agencies know, mostly, what to expect from ash that comes from burned vegetation. But these fires included something a little new. They burned through the wildland-urban interface and into cities. “For how many structures that were burned in fairly small areas in these fires, I think that's a first-of-its-kind event,” says Geoffrey Plumlee, associate director of environmental health for the US Geological Survey. “The concern is, can they get it cleaned up before the heavy rains come?”
Chemically, ash is fascinating. If the fire that made it burned at lower than about 840 degrees Fahrenheit, it’ll be darker-colored, maybe even black, and mostly organic carbon. At higher temperatures the carbon burns away, leaving inorganic compounds of stuff like calcium, magnesium, and sodium, and it’ll be whiter and fluffier. Even hotter fires, like above 1,100 degrees, leave nothing but oxides. Inside a single fire, combustion happens at different temperatures in different places, and because ash is so light, wind can transport it, so the composition of ash from the same fire can vary.
So depending on how combusted the ash is, it’ll have different chemical compositions. And that’ll mean the ash will mix either better or worse with underlying soil. Water won’t stick to more hydrophobic ash, so rainfall might run off faster, carrying away the surrounding soil as sediment. More hydrophilic ash might mix into the water and wash into nearby streams.
Now, carbon is the backbone element of organic systems. Having some flow off of burned hillsides and into aquatic ecosystems isn’t necessarily bad. It’ll increase what’s called “primary productivity,” allowing algae to reproduce, which means more food for fish. “Naturally occurring, lower-severity fires can have positive impacts,” says Kevin Bladon, a forest ecohydrologist at Oregon State University. The fires free up organic carbon and put nutrients like nitrogen and phosphorous into play. “But the really large, high-severity megafires that we’ve started to observe push the systems in a lot of cases too far.”
That means dangerously large algal blooms, so-called eutrophication that can eat all the dissolved oxygen out of a waterway, making it unlivable for everything else. Sediments freed up by the hydrophobic ash end up making water more turbid—bad for people if that water’s supposed to end up coming out of someone’s tap, and bad for fish because sediment can gum up feeding and breeding grounds.
The problem’s getting more familiar as an after-effect of fires ranging from Australia to Canada to the US. Climate change makes fires and storms more severe. As one of Bladon’s papers notes, the 2002 Hayman Fire in the Colorado Rockies sent 765,000 cubic meters of sediment, ash, and other stuff into Denver’s drinking water reservoirs, and the problems were still there four years later. “We’re anywhere from five years to 100 years in terms of the longevity of effects,” Bladon says. “That really depends on the severity of the fire and our ability to get some vegetation re-established on site.”
It gets even weirder. At high enough combustion temperatures, compounds like potassium carbonate and calcium carbonate turn to oxides. Given the right amount of light rain, calcium oxide—quicklime—slurps carbon dioxide out of the air and forms a crust of essentially cement, limestone. Basically, the ash can pave a forest. “If you had the perfect situation, you can really change a watershed with increased runoff,” says Victoria Balfour, a post-fire hydrologist.
There's more. Combustion byproducts called polycyclic aromatic hydrocarbons and cancer-causing dioxins can all be part of ash. In older forested land that hasn’t burned in decades, pollution may have deposited heavy metals like copper, lead, aluminum, arsenic, and even mercury onto the plants. When the plants burn, the metals stay behind, or run into waterways. The 2009 Station Fire northeast of Los Angeles increased levels of iron, manganese, and mercury in nearby streams … and the storms that followed soon after brought elevated levels of copper, lead, nickel, and selenium.
What makes these latest Northern California fires unique, though, is that they burned not just forest wildland but also cities. And the built environment burns differently. It gets hotter, and it leaves behind different remains. “All of a sudden you’ve got a lot of impervious surfaces,” Bladon says. “Water hits it and flows over. If there are burned materials sitting on the roads, that’s going to move very rapidly into waterways. We have no handle on that at all.” Ash science isn’t much more than a decade old; understanding urban ash science has never really been a necessity—but now megafires are coming to cities.
What’s in that ash depends on how old the buildings are. Pre-1980, the paint will be full of lead. Even more recently, pressure-treated lumber used in landscaping was full of chromate copper arsenate, which leaches arsenic and hexavalent chromium—bad for fish and other living things. “On the good side, there are probably fewer lead phosphor TV screens. There’s a lot more flat screens,” Plumlee says. "There are more LED light bulbs and hopefully fewer fluorescent and compact-fluorescent, which have a concern with mercury. Copper and zinc are also commonly used in building materials.”
All of which means the real trick in Northern California will be debris cleanup. The US Environmental Protection Agency, California’s EPA, the Army Corps of Engineers, and other agencies and NGOs are all onsite—spinning up as a Watershed Protection Taskforce. They’re evaluating the debris and getting it ready for collection—a job done most safely wearing gloves, a Tyvek bodysuit, and an N95 respirator.
Drinking water for the region hasn’t been compromised—it comes from farther north than the fires—but the Napa and Sonoma Rivers (and the associated stream systems) empty straight into San Francisco Bay. “We cherish our creeks and streams, and there’s uncertainty in regards to what may be in all this ash,” says Bennett Horenstein, director of Santa Rosa Water. “So there’s a lot of activity to do everything we can in a thoughtful and safe way to protect the ecosystem.”
The Napa Fire Is a Perfectly Normal Apocalypse
In Cities, It's the Smoke, Not the Fire, That Will Get You
How Climate Change and 'Smoke Taint' Could Kill Napa Wine
Stormwater typically doesn’t get treated, so the inlets in Santa Rosa have gotten barrier protection installed. “We’re all recognizing that this disaster was of such magnitude, the organization is unfolding,” Horenstein says. “But it does seem to be unfolding in a thoughtful, structured way.”
The Regional Monitoring Program for Water Quality already watches what’s in the San Francisco Bay besides water. Some of its scientists now have a proposal to monitor the Napa River for what water watchdogs call “contaminants of emerging concern.” The field is new enough that they’re not even sure what they’re looking for yet—they’re going to use “non-targeted analysis” to look for anything unexpected. The San Francisco Estuary Institute already monitors dioxins, PAHs, metals, and other stuff in the bay, but only annually or semiannually.
That’s probably not fast enough. The ash is on the ground. “Getting through the entire area will take some time,” Horenstein says, “and we’re approaching the wet-weather season. There’s a small storm forecast for next week.” ‘Tis, after all, the season.
NASA started a blaze aboard the unmanned Orbital ATK Cygnus cargo vehicle. It’s the Spacecraft Fire Experiment. Seriously, that’s exactly what NASA is calling it.


It’s hard to measure water from a fire hose while it’s hitting you in the face. In a sense, that’s the challenge of analyzing streaming data, which comes at us in a torrent and never lets up. If you’re on Twitter watching tweets go by, you might like to declare a brief pause, so you can figure out what’s trending. That’s not feasible, though, so instead you need to find a way to tally hashtags on the fly.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Computer programs that perform these kinds of on-the-go calculations are called streaming algorithms. Because data comes at them continuously, and in such volume, they try to record the essence of what they’ve seen while strategically forgetting the rest. For more than 30 years computer scientists have worked to build a better streaming algorithm. Last fall a team of researchers invented one that is just about perfect.
“We developed a new algorithm that is simultaneously the best” on every performance dimension, said Jelani Nelson, a computer scientist at Harvard University and a co-author of the work with Kasper Green Larsen of Aarhus University in Denmark, Huy Nguyen of Northeastern University and Mikkel Thorup of the University of Copenhagen.
This best-in-class streaming algorithm works by remembering just enough of what it’s seen to tell you what it’s seen most frequently. It suggests that compromises that seemed intrinsic to the analysis of streaming data are not actually necessary. It also points the way forward to a new era of strategic forgetting.
Streaming algorithms are helpful in any situation where you’re monitoring a database that’s being updated continuously. This could be AT&T keeping tabs on data packets or Google charting the never-ending flow of search queries. In these situations it’s useful, even necessary, to have a method for answering real-time questions about the data without re-examining or even remembering every piece of data you’ve ever seen.
Here’s a simple example. Imagine you have a continuous stream of numbers and you want to know the sum of all the numbers you’ve seen so far. In this case it’s obvious that instead of remembering every number, you can get by with remembering just one: the running sum.
The challenge gets harder, though, when the questions you want to ask about your data get more complicated. Imagine that instead of calculating the sum, you want to be able to answer the following question: Which numbers have appeared most frequently? It’s less obvious what kind of shortcut you could use to keep an answer at the ready.
This particular puzzle is known as the “frequent items” or “heavy hitters” problem. The first algorithm to solve it was developed in the early 1980s by David Gries of Cornell University and Jayadev Misra of the University of Texas, Austin. Their program was effective in a number of ways, but it couldn’t handle what’s called “change detection.” It could tell you the most frequently searched terms, but not which terms are trending. In Google’s case, it could identify “Wikipedia” as an ever-popular search term, but it couldn’t find the spike in searches that accompany a major event such as Hurricane Irma.
“It’s a coding problem—you’re encoding information down to compact summary and trying to extract information that lets you recover what was put in initially,” said Graham Cormode, a computer scientist at the University of Warwick.
Over the next 30-plus years, Cormode and other computer scientists improved Gries and Misra’s algorithm. Some of the new algorithms were able to detect trending terms, for example, while others were able to work with a more fine-grained definition of what it means for a term to be frequent. All those algorithms made trade-offs, like sacrificing speed for accuracy or memory consumption for reliability.
Most of these efforts relied on an index. Imagine, for example, you are trying to identify frequent search terms. One way to do it would be to assign a number to every word in the English language and then pair that number with a second number that keeps track of how many times that word has been searched. Maybe “aardvark” gets indexed as word number 17 and appears in your database as (17, 9), meaning word number 17 has been searched nine times. This approach comes closer to putting the most frequent items at your fingertips, since at any given moment, you know exactly how many times each word has been searched.
Still, it has drawbacks—namely that it takes a lot of time for an algorithm to comb through the hundreds of thousands of words in the English language.
But what if there were only 100 words in the dictionary? Then “looping over every word in the dictionary wouldn’t take that long,” Nelson said.
Alas, the number of words in the dictionary is what it is. Unless, as the authors of the new algorithm discovered, you can break the big dictionary into smaller dictionaries and find a clever way to put it back together.
Small numbers are easier to keep track of than big numbers.
Imagine, for example, that you’re monitoring a stream of numbers between zero and 50,000,000 (a task similar to logging internet users by their IP addresses). You could keep track of the numbers using a 50,000,000-term index, but it’s hard to work with an index that size. A better way is to think of each eight-digit number as four two-digit numbers linked together.
Say you see the number 12,345,678. One memory-efficient way to remember it is to break it into four two-digit blocks: 12, 34, 56, 78. Then you can send each block to a sub-algorithm that calculates item frequencies: 12 goes to copy one of the algorithm, 34 goes to copy two, 56 goes to copy three, and 78 goes to copy four.
Each sub-algorithm maintains its own index of what it’s seen, but since each version never sees anything bigger than a two-digit number, each index only runs from 0 to 99.
An important feature of this splitting is that if the big number—12,345,678—appears frequently in your overall data stream, so will its two-digit components. When you ask each sub-algorithm to identify the numbers it has seen the most, copy one will spit out 12, copy two will spit out 34, and so on. You’ll be able to find the most frequent members of a huge list just by looking for the frequent items in four much shorter lists.
“Instead of spending 50 million units of time looping over the entire universe, you only have four algorithms spending 100 units of time,” Nelson said.
The main problem with this divide-and-conquer strategy is that while it’s easy to split a big number into small numbers, the reverse is trickier—it’s hard to fish out the right small numbers to recombine to give you the right big number.
Imagine, for example, that your data stream frequently includes two numbers that have some digits in common: 12,345,678 and 12,999,999. Both start with 12. Your algorithm splits each number into four smaller numbers, then sends each to a sub-algorithm. Later, you ask each sub-algorithm, “Which numbers have you seen most frequently?” Copy one is going to say, “I’ve seen a lot of the number 12.” An algorithm that’s trying to identify which eight-digit numbers it’s seen most frequently can’t tell if all these 12s belong to one eight-digit number or, as in this case, to two different numbers.
Computer Scientists Close In on Perfect, Hack-Proof Code
Ecologists Are Drowning in Sea of Data. These Tools Could Help
Landmark Algorithm Breaks 30-Year Impasse
“The challenge is to figure out which two-digit blocks to concatenate with which other two-digit blocks,” Nelson said.
The authors of the new work solve this dilemma by packaging each two-digit block with a little tag that doesn’t take up much memory but still allows the algorithm to put the two-digit pieces back together in the right way.
To see one simple approach to how the tagging might work, start with 12,345,678 and split it into two-digit blocks. But this time, before you send each block to its respective sub-algorithm, package the block with a pair of unique identifying numbers that can be used to put the blocks back together. The first of these tags serves as the block’s name, the second as a link. In this way, 12,345,678 becomes:
12, 0, 1     /	34, 1, 2    /	 56, 2, 3	    /	  78, 3, 4
Here the number 12 has the name “0” and gets linked to the number named “1.” The number 34 has the name “1” and gets linked to the number named “2.” And so on.
Now when the sub-algorithms return the two-digit blocks they’ve seen most frequently, 12 goes looking for a number tagged with “1” and finds 34, then 34 goes looking for a number tagged with “2” and finds 56, and 56 goes looking for a number tagged with “3” and finds 78.
In this way, you can think of the two-digit blocks as links in a chain, with the links held together by these extra tagging numbers.
The problem with chains, of course, is that they’re only as strong as their weakest link. And these chains are almost guaranteed to break.
No algorithm works perfectly every time you run it—even the best ones misfire some small percentage of the time. In the example we’ve been using, a misfire could mean that the second two-digit block, 34, gets assigned an incorrect tag, and as a result, when it goes looking for the block it’s supposed to be joined to, it doesn’t have the information it needs to find 56. And once one link in the chain fails, the entire effort falls apart.
To avoid this problem, the researchers use what’s called an “expander graph.” In an expander graph, each two-digit block forms a point. Points get connected by lines (according to the tagging process described above) to form a cluster. The important feature of an expander graph is that instead of merely connecting each point with its adjoining blocks, you connect each two-digit block with multiple other blocks. For example, with 12,345,678, you connect 12 with 34 but also with 56, so that you can still tell that 12 and 56 belong in the same number even if the link between 12 and 34 fails.
An expander graph doesn’t always come out perfectly. Sometimes it’ll fail to link two blocks that should be linked. Or it’ll link two blocks that don’t belong together. To counteract this tendency, the researchers developed the final step of their algorithm: a “cluster-preserving” sub-algorithm that can survey an expander graph and accurately determine which points are meant to be clustered together and which aren’t, even when some lines are missing and false ones have been added.
“This guarantees I can recover something that looks like the original clusters,” Thorup said.
And while Twitter isn’t going to plug in the expander sketch tomorrow, the techniques underlying it are applicable to a far wider range of computer science problems than tallying tweets. The algorithm also proves that certain sacrifices that previously seemed necessary to answer the frequent-items problem don’t need to be made. Previous algorithms always gave up something — they were accurate but memory-intensive, or fast but unable to determine which frequent items were trending. This new work shows that given the right way of encoding a lot of information, you can end up with the best of all possible worlds: You can store your frequent items and recall them, too.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Agent Topple reveals a few tricks of the pre-digital trade when Winters attempts to explain to him how computers work. Agent Topple is Not Impressed.


In 1985, when Carl Sagan was writing the novel Contact, he needed to quickly transport his protagonist Dr. Ellie Arroway from Earth to the star Vega. He had her enter a black hole and exit light-years away, but he didn’t know if this made any sense. The Cornell University astrophysicist and television star consulted his friend Kip Thorne, a black hole expert at the California Institute of Technology (who won a Nobel Prize earlier this month). Thorne knew that Arroway couldn’t get to Vega via a black hole, which is thought to trap and destroy anything that falls in. But it occurred to him that she might make use of another kind of hole consistent with Albert Einstein’s general theory of relativity: a tunnel or “wormhole” connecting distant locations in space-time.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
While the simplest theoretical wormholes immediately collapse and disappear before anything can get through, Thorne wondered whether it might be possible for an “infinitely advanced” sci-fi civilization to stabilize a wormhole long enough for something or someone to traverse it. He figured out that such a civilization could in fact line the throat of a wormhole with “exotic material” that counteracts its tendency to collapse. The material would possess negative energy, which would deflect radiation and repulse space-time apart from itself. Sagan used the trick in Contact, attributing the invention of the exotic material to an earlier, lost civilization to avoid getting into particulars. Meanwhile, those particulars enthralled Thorne, his students and many other physicists, who spent years exploring traversable wormholes and their theoretical implications. They discovered that these wormholes can serve as time machines, invoking time-travel paradoxes—evidence that exotic material is forbidden in nature.
Now, decades later, a new species of traversable wormhole has emerged, free of exotic material and full of potential for helping physicists resolve a baffling paradox about black holes. This paradox is the very problem that plagued the early draft of Contact and led Thorne to contemplate traversable wormholes in the first place; namely, that things that fall into black holes seem to vanish without a trace. This total erasure of information breaks the rules of quantum mechanics, and it so puzzles experts that in recent years, some have argued that black hole interiors don’t really exist—that space and time strangely end at their horizons.
The flurry of findings started last year with a paper that reported the first traversable wormhole that doesn’t require the insertion of exotic material to stay open. Instead, according to Ping Gao and Daniel Jafferis of Harvard University and Aron Wall of Stanford University, the repulsive negative energy in the wormhole’s throat can be generated from the outside by a special quantum connection between the pair of black holes that form the wormhole’s two mouths. When the black holes are connected in the right way, something tossed into one will shimmy along the wormhole and, following certain events in the outside universe, exit the second. Remarkably, Gao, Jafferis and Wall noticed that their scenario is mathematically equivalent to a process called quantum teleportation, which is key to quantum cryptography and can be demonstrated in laboratory experiments.
John Preskill, a black hole and quantum gravity expert at Caltech, says the new traversable wormhole comes as a surprise, with implications for the black hole information paradox and black hole interiors. “What I really like,” he said, “is that an observer can enter the black hole and then escape to tell about what she saw.” This suggests that black hole interiors really exist, he explained, and that what goes in must come out.
The new wormhole work began in 2013, when Jafferis attended an intriguing talk at the Strings conference in South Korea. The speaker, Juan Maldacena, a professor of physics at the Institute for Advanced Study in Princeton, New Jersey, had recently concluded, based on various hints and arguments, that “ER = EPR.” That is, wormholes between distant points in space-time, the simplest of which are called Einstein-Rosen or “ER” bridges, are equivalent (albeit in some ill-defined way) to entangled quantum particles, also known as Einstein-Podolsky-Rosen or “EPR” pairs. The ER = EPR conjecture, posed by Maldacena and Leonard Susskind of Stanford, was an attempt to solve the modern incarnation of the infamous black hole information paradox by tying space-time geometry, governed by general relativity, to the instantaneous quantum connections between far-apart particles that Einstein called “spooky action at a distance.”
The paradox has loomed since 1974, when the British physicist Stephen Hawking determined that black holes evaporate—slowly giving off heat in the form of particles now known as “Hawking radiation.” Hawking calculated that this heat is completely random; it contains no information about the black hole’s contents. As the black hole blinks out of existence, so does the universe’s record of everything that went inside. This violates a principle called “unitarity,” the backbone of quantum theory, which holds that as particles interact, information about them is never lost, only scrambled, so that if you reversed the arrow of time in the universe’s quantum evolution, you’d see things unscramble into an exact re-creation of the past.
Almost everyone believes in unitarity, which means information must escape black holes—but how? In the last five years, some theorists, most notably Joseph Polchinski of the University of California, Santa Barbara, have argued that black holes are empty shells with no interiors at all—that Ellie Arroway, upon hitting a black hole’s event horizon, would fizzle on a “firewall” and radiate out again.
Many theorists believe in black hole interiors (and gentler transitions across their horizons), but in order to understand them, they must discover the fate of information that falls inside. This is critical to building a working quantum theory of gravity, the long-sought union of the quantum and space-time descriptions of nature that comes into sharpest relief in black hole interiors, where extreme gravity acts on a quantum scale.
The quantum gravity connection is what drew Maldacena, and later Jafferis, to the ER = EPR idea, and to wormholes. The implied relationship between tunnels in space-time and quantum entanglement posed by ER = EPR resonated with a popular recent belief that space is essentially stitched into existence by quantum entanglement. It seemed that wormholes had a role to play in stitching together space-time and in letting black hole information worm its way out of black holes—but how might this work? When Jafferis heard Maldacena talk about his cryptic equation and the evidence for it, he was aware that a standard ER wormhole is unstable and non-traversable. But he wondered what Maldacena’s duality would mean for a traversable wormhole like the ones Thorne and others played around with decades ago. Three years after the South Korea talk, Jafferis and his collaborators Gao and Wall presented their answer. The work extends the ER = EPR idea by equating, not a standard wormhole and a pair of entangled particles, but a traversable wormhole and quantum teleportation: a protocol discovered in 1993 that allows a quantum system to disappear and reappear unscathed somewhere else.
When Maldacena read Gao, Jafferis and Wall’s paper, “I viewed it as a really nice idea, one of these ideas that after someone tells you, it’s obvious,” he said. Maldacena and two collaborators, Douglas Stanford and Zhenbin Yang, immediately began exploring the new wormhole’s ramifications for the black hole information paradox; their paper appeared in April. Susskind and Ying Zhao of Stanford followed this with a paper about wormhole teleportation in July. The wormhole “gives an interesting geometric picture for how teleportation happens,” Maldacena said. “The message actually goes through the wormhole.”
In their paper, “Diving Into Traversable Wormholes,” published in Fortschritte der Physik, Maldacena, Stanford and Yang consider a wormhole of the new kind that connects two black holes: a parent black hole and a daughter one formed from half of the Hawking radiation given off by the parent as it evaporates. The two systems are as entangled as they can be. Here, the fate of the older black hole’s information is clear: It worms its way out of the daughter black hole.
During an interview this month in his tranquil office at the IAS, Maldacena, a reserved Argentinian-American with a track record of influential insights, described his radical musings. On the right side of a chalk-dusty blackboard, Maldacena drew a faint picture of two black holes connected by the new traversable wormhole. On the left, he sketched a quantum teleportation experiment, performed by the famous fictional experimenters Alice and Bob, who are in possession of entangled quantum particles a and b, respectively. Say Alice wants to teleport a qubit q to Bob. She prepares a combined state of q and a, measures that combined state (reducing it to a pair of classical bits, 1 or 0), and sends the result of this measurement to Bob. He can then use this as a key for operating on b in a way that re-creates the state q. Voila, a unit of quantum information has teleported from one place to the other.
Maldacena turned to the right side of the blackboard. “You can do operations with a pair of black holes that are morally equivalent to what I discussed [about quantum teleportation]. And in that picture, this message really goes through the wormhole.”
Say Alice throws qubit q into black hole A. She then measures a particle of its Hawking radiation, a, and transmits the result of the measurement through the external universe to Bob, who can use this knowledge to operate on b, a Hawking particle coming out of black hole B. Bob’s operation reconstructs q, which appears to pop out of B, a perfect match for the particle that fell into A. This is why some physicists are excited: Gao, Jafferis and Wall’s wormhole allows information to be recovered from black holes. In their paper, they set up their wormhole in a negatively curved space-time geometry that often serves as a useful, if unrealistic, playground for quantum gravity theorists. However, their wormhole idea seems to extend to the real world as long as two black holes are coupled in the right way: “They have to be causally connected and then the nature of the interaction that we took is the simplest thing you can imagine,” Jafferis explained. If you allow the Hawking radiation from one of the black holes to fall into the other, the two black holes become entangled, and the quantum information that falls into one can exit the other.
The quantum-teleportation format precludes using these traversable wormholes as time machines. Anything that goes through the wormhole has to wait for Alice’s message to travel to Bob in the outside universe before it can exit Bob’s black hole, so the wormhole doesn’t offer any superluminal boost that could be exploited for time travel. It seems traversable wormholes might be permitted in nature as long as they offer no speed advantage. “Traversable wormholes are like getting a bank loan,” Gao, Jafferis and Wall wrote in their paper: “You can only get one if you are rich enough not to need it.”
While traversable wormholes won’t revolutionize space travel, according to Preskill the new wormhole discovery provides “a promising resolution” to the black hole firewall question by suggesting that there is no firewall at black hole horizons. Preskill said the discovery rescues “what we call ‘black hole complementarity,’ which means that the interior and exterior of the black hole are not really two different systems but rather two very different, complementary ways of looking at the same system.” If complementarity holds, as is widely assumed, then in passing across a black hole horizon from one realm to the other, Contact’s Ellie Arroway wouldn’t notice anything strange. This seems more likely if, under certain conditions, she could even slide all the way through a Gao-Jafferis-Wall wormhole.
The wormhole also safeguards unitarity—the principle that information is never lost—at least for the entangled black holes being studied. Whatever falls into one black hole eventually exits the other as Hawking radiation, Preskill said, which “can be thought of as in some sense a very scrambled copy of the black hole interior.”
Taking the findings to their logical conclusion, Preskill thinks it ought to be possible (at least for an infinitely advanced civilization) to influence the interior of one of these black holes by manipulating its radiation. This “sounds crazy,” he wrote in an email, but it “might make sense if we can think of the radiation, which is entangled with the black hole—EPR—as being connected to the black hole interior by wormholes— ER. Then tickling the radiation can send a message which can be read from inside the black hole!” He added, “We still have a ways to go, though, before we can flesh out this picture in more detail.”
Indeed, obstacles remain in the quest to generalize the new wormhole findings to a statement about the fate of all quantum information, or the meaning of ER = EPR.
In Maldacena and Susskind’s paper proposing ER = EPR, they included a sketch that’s become known as the “octopus”: a black hole with tentacle-like wormholes leading to distant Hawking particles that have evaporated out of it. The authors explained that the sketch illustrates “the entanglement pattern between the black hole and the Hawking radiation. We expect that this entanglement leads to the interior geometry of the black hole.”
But according to Matt Visser, a mathematician and general-relativity expert at Victoria University of Wellington in New Zealand who has studied wormholes since the 1990s, the most literal reading of the octopus picture doesn’t work. The throats of wormholes formed from single Hawking particles would be so thin that qubits could never fit through. “A traversable wormhole throat is ‘transparent’ only to wave packets with size smaller than the throat radius,” Visser explained. “Big wave packets will simply bounce off any small wormhole throat without crossing to the other side.”
The Fuzzball Fix for a Black Hole Paradox
Quantum Links in Time and Space May Form the Universe's Foundation
What Happens When You Mix Thermodynamics and the Quantum World? A Revolution
Stanford, who co-wrote the recent paper with Maldacena and Yang, acknowledged that this is a problem with the simplest interpretation of the ER = EPR idea, in which each particle of Hawking radiation has its own tentacle-like wormhole. However, a more speculative interpretation of ER = EPR that he and others have in mind does not suffer from this failing. “The idea is that in order to recover the information from the Hawking radiation using this traversable wormhole,” Stanford said, one has to “gather the Hawking radiation together and act on it in a complicated way.” This complicated collective measurement reveals information about the particles that fell in; it has the effect, he said, of “creating a large, traversable wormhole out of the small and unhelpful octopus tentacles. The information would then propagate through this large wormhole.” Maldacena added that, simply put, the theory of quantum gravity might have a new, generalized notion of geometry for which ER equals EPR. “We think quantum gravity should obey this principle,” he said. “We view it more as a guide to the theory.”
In his 1994 popular science book, Black Holes and Time Warps, Kip Thorne celebrated the style of reasoning involved in wormhole research. “No type of thought experiment pushes the laws of physics harder than the type triggered by Carl Sagan’s phone call to me,” he wrote; “thought experiments that ask, ‘What things do the laws of physics permit an infinitely advanced civilization to do, and what things do the laws forbid?’”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Inside a simulation of the universe's particle accelerator with WIRED Science writer Nick Stockton.


The moons of the solar system got a bit greedy during this week’s space photo round-up. Our own moon was captured blocking out the sun for 45 minutes, making for an epic lunar photobomb. A bit further out in the solar system, Jupiter showed a dark blemish on its surface, a shadow cast by its tiny moon Amalthea. The small Jovian moon was captured by the Juno spacecraft in orbit around the banded gas giant. Jupiter’s moons help feed its faint rings, and Amalthea is no exception. This irregularly shaped moon loses material, providing dust and particles that form the Amalthea Gossamer Ring.
Planets have other kinds of pseudo-moons, too. Mars has Phobos and Deimos, of course, but it's also orbited by many artificial satellites–like NASA’s Mars Reconnaissance Orbiter. This rockstar satellite snapped a bizarre and fascinating photo of Hellas Planitia basin with some very strange features. These deep zigzagging lines are called linear gullies, likely formed by thawing blocks of carbon dioxide ice. Scientists are still unsure about the process, but regardless, the view from above is pretty cool.
Still need to space out? There's more, check out the full collection of cosmic photos here.


This story originally appeared on Citylab and is part of the Climate Desk collaboration.
Last winter, teams of researchers in three US cities donned goggles, gloves, and respirators, tore into bags of other people’s household garbage, and then pawed though the contents. Separating slimy banana peels from clumps of coffee grounds was dirty work, but it had a laudable goal: trying to get a handle on how much food waste could have been consumed or diverted before winding its way into the waste stream with a one-way ticket to the dump.
The problems associated with urban food waste are no mystery. Proof of the problem is everywhere, in overflowing garbage bins and grime-slicked compost caddies. Food scraps contribute to the already sizable piles of refuse that cities must haul to landfills; shuttling edible castoffs to people in need requires labyrinthine routes and mind-boggling logistics; and gases released by decomposing leftovers detract from cities’ work toward reining in emissions. But there’s surprisingly little hard data about who’s wasting what, and where, which makes it harder for cities to address the issue.
To sniff out specifics, the engineering company Tetra Tech (in collaboration with the Natural Resources Defense Council and the Rockefeller Foundation) recruited more than 1,151 residents in Denver, New York, and Nashville. Of these, 631 supplied qualitative info in the form of kitchen diaries noting what they tossed and why. Researchers also inspected the contents of 277 residential trash bins, and 145 containers of commercial or industrial garbage.
Now, the team has digested the data in a pair of reports, released Wednesday, that take stock of how food waste shakes out in these cities, and what they can do to clean up their act.
The researchers divided trashed food into three categories: stuff that is typically edible, questionably edible (including peels and cores), and inedible (such as pits, bones, and egg shells). They then tallied up findings from the bin digs and kitchen diaries to gauge how much is going to waste in each city. In Denver and New York, residents trashed the majority of the wasted food; in Nashville, the residential and restaurant sectors were neck and neck.
Denverites trashed the most edible food—about 7.5 pounds per household each week—followed by New York (5.4 pounds) and then Nashville (4.6 pounds). Overall, these sums are lower than previous estimates from ReFED, a consortium of food-centric organizations, which placed the weekly figure around 11.6 pounds per household.
Across all three cities, coffee and grounds were the goods most often pitched in the trash, trailed by bananas (in Nashville and New York) and chicken (in Denver). Apples, bread, oranges, and potatoes also topped the list, as did discarded dairy products.
In the accompanying kitchen diaries, respondents described why they opted to jettison these scraps. Forty-four percent of participants said they were getting rid of inedible portions; 20 percent reported moldy or spoiled food, and 11 percent indicated they weren’t interested in the leftovers. Only 4 percent of residents noted that they’d discarded food because it was past the date printed on the label, though perceived confusion over inscrutable labeling practices has spurred legislation to standardize and streamline “best by” and “use by” language.
The reports also qualified attitudes toward waste. Most respondents preferred fruit without blemishes; still, more than half of the respondents said they “always” or “mostly” excised bruised portions and salvaged the rest of the fruit or vegetable. And though some indicated that squandering food felt morally icky, 58 percent of the respondents indicated less guilt about wasting food if they knew it was going to be composted.
Judging by the kitchen diaries, though, most of the trashed food didn’t end up in the compost bin—53 percent went straight to the trash. In New York, which has a comparatively robust organics scheme, 37 percent of the self-reported discards ended up in the green bin. In Denver and Nashville, this figure was 24 percent and 28 percent, respectively, though respondents in Denver reported the highest rate of compost participation.
The researchers flag that discrepancy, among other sticking points: At least in New York City, they found that participating in a compost program led to more overall waste, compared with families whose garbage all goes into a single stream. In other words: Compost-happy residents were disposing of more total scraps than residents who just threw the whole lot in the trash. To counter that trend, the report’s authors recommend reminding consumers that “preventing food waste is preferable to composting it.”
The US Throws Away as Much as Half Its Food Produce
These Tasty, Waste-Gobbling Maggots Could Save the World
A Startup Just Got $30 Million to Shake Up the Garbage Industry
All this surplus food could be put to better use. “An outrageous amount of food is wasted in our cities, yet at the same time many residents are in need,” said Dana Gunders, a senior scientist at NRDC, in a statement. The other new report documents the ways that cities can push back against hunger and food insecurity, which continue to nag cities, suburbs, and rural regions despite the excess of edible food. Some 13.4 percent of Tennessee residents are food insecure, according to a 2016 report from the USDA. Across New York State, that figure is 12.5 percent; in Colorado, 10.3 percent of citizens struggle to reliably access nutritious food.
The NRDC researchers compared current food rescue rates in the three cities to the maximum volume that could be intercepted, and found that it’s feasible to recoup tens of thousands of tons of packaged, raw, or prepared items across the board, from grocery stores, restaurants, caterers, coffee shops, schools, and more. In Denver, where 2,539 tons of food is currently rescued along the food chain, the researchers pinpoint an additional untapped potential of 4,232 tons—enough for about 7.1 million meals. These could go a long way in a city where nearly about 13 percent of residents lack reliable access to nutritious food.
But that goal is a lofty one: It assumes that all of the local businesses and institutions will buy in. The researchers also spooled up a less ambitious projection, in which participation rates are more modestly scaled up from their current numbers. That model would still translate to 901 tons of food, or 1.5 million meals—but it could require an infusion of $2 million to cover the cost of vehicles and storage space to accommodate the haul.
More than a prescription, the research is a starting point. “As more research in this vein is conducted, it will be easier to identify trends and potentially aggregate data for better extrapolation, better intervention design—and eventually, less wasted food,” the authors note. Urban areas can carry this charge, Gunders told me last year. “Cities can be setting targets in their community, and elevating the profile of the issue and raising awareness,” she said. “That’s a nice foundation. They can take a look at their waste policies.”
Cities can also redesign trash management from the ground up. As I wrote last week, a band of architects in New York, backed by the Center for Architecture and the Rockefeller Foundation, laid out a series of design guidelines that approach trash as a design issue, and turn to clever planning and ingenious interiors to help alleviate the burden while getting the city closer to Mayor Bill de Blasio’s goal of schlepping zero waste to landfills by 2030. That call for better engineering is echoed in some of the qualitative feedback in the NRDC reports. When asked what sorts of steps they hope their cities will take, most respondents gestured toward expanded compost programs or beefed-up public service campaigns.
Others asked for more options, arrayed in a smarter and more durable way. “Make it easier to compost and recycle,” one participant wrote. “Like many NYCers, I live in a small place and mice and cockroaches come up often. That means we keep our trash on a specific counter. Since we have to already split up our paper recycling, and have trash, there is no room for four bins!!! When we lived in San Francisco and we could throw all recycling in one bin, we composted a lot more often.”
Far from being a luxury, “design thinking is something people should demand from cities, architects, and supers,” said Benjamin Prosky, executive director of the Center for Architecture, at an event announcing the waste-reduction blueprint. Collecting, digesting, and acting upon ever-more-precise data can only sharpen its focus and impact.
Artificial intelligence and advanced automation are everywhere including our farm fields and kitchens. How will robots change the way we grow, harvest and cook our food?


Late at night, I tend to flip through the channels just to see what's up. If there's a good movie on, I might watch part of it—and recently, I stumbled on Iron Man 3. I know what you're gonna say—that's a terrible superhero movie. But I disagree. Fantastic Four, now that's a terrible superhero movie. Iron Man 3 wasn't so bad. Especially not that part where Tony Stark has to go to the store and MacGyver his way into a temporary suit.
However, I did notice something annoying in a scene near the end. Iron Man needs to recharge his suit, and he improvises by connecting two cables (one red and one black) from a car battery to his suit. When he is mostly charged up, he pulls off the cables—one at a time. First he pulls the red cable off and it creates a slight sparking effect. Right after that he pulls off the black cable and it also makes a spark. See the mistake? One of the cables could easily make a spark, but not both.
But why? Of course that is the question. And now for the answer.
You can get a spark when air changes from an electrical insulator into an electrical conductor.  This happens not at a certain electric potential difference (voltage) but at a certain electric field strength. Let me explain the difference with an example.
Suppose you have two wires connected to a 9 volt battery with the free ends of the wires held just 1 centimeter apart.  The electric potential difference between these two ends is 9 volts. That's probably not a huge surprise. If I move the wires closer together, they still make a 9 volt potential. However, the electric field depends on both the potential and the distance. As I move the wires closer together, the electric field gets stronger between the two wires. The electric field is the gradient of the electric potential and measured in units of volts/meter.
I know you didn't like that example, so how about an analogy? Instead of electric stuff, I have a hill. The height of the hill is like the change in electric potential. The slope of the hill at some point would be the electric field. So now I have a 9 meter tall hill (instead of 9 volts). As the bottom and top of the hill move closer together (horizontally) the slope gets steeper. That's just like the difference between electric potential difference and the electric field. Bonus: Note the importance of calling the electric potential a "difference" or "change in"—just like a hill, the change in height is the key, not just the top of the hill.
Now back to sparks.  A spark is created by a high electric field, not a high voltage. At about 3 x 106 Volts/meter, you get a spark in air. With a 9 volt battery (and assuming a constant electric field for simplicity), you would need two wires to get 3 micrometers apart to cause a spark. That's one super tiny spark.
But there are many cases where a spark is created even with low voltage.  In order to understand this, let's first look at a simple circuit—the simplest circuit you could ever imagine.  It consists of a battery and a wire.  That's it.  Here is what that might actually look like.
Yes, that is just a battery and a wire. OK, technically there are two rare earth magnets in there too—they just are an easy way to connect the wire to the battery. And although this is a simple circuit, it's not very useful. In fact, it would be considered a short circuit since there isn't really anything in the path. Without a resistor or anything, the current will get higher than it normally would. This would make the wire get hot—not too crazy hot with just a D-cell battery, but still hot.
What happens when I disconnect one of the wires?  Let me show you.
That wasn't very exciting.
Here is another circuit. Watch closely as I connect the final cable to the battery.
It's dark so you can see better, but notice that there is a spark both when I connect the battery and when it is disconnected. Here is a picture of the whole circuit.
There are two significant differences with this second circuit.  First, I am using two 9 volt batteries instead of one D-cell battery (at just 1.5 volts). Second, I have another device in this circuit—a coil of wire (on the left). We call this an inductor.
There is a pretty cool relationship between electric and magnetic fields and that's the basis of the inductor. Let me start with a simpler example. Suppose I have a coil of wire and a magnet. As I move the magnet in and out of this coil, it will produce an electric current. Here is an example of that.
The device on the right is essentially a sensitive meter to detect electric currents (called a galvanometer). If you look carefully, you might be able to notice that a current is produced when the magnet is moving—not just when there is a magnetic field. There must be a changing magnetic field to produce a current. But what if I replaced this moving magnet with a coil of wire with a changing current running through it? It could produce the exact same effect as the moving magnet.
Even better—I don't need two separate coils. With just one coil and a changing electric current, part of the coil will make a changing magnetic field that will induce an electric current in the other part of the coil.  This is the essence of an inductor. Really, the best way to think of an inductor is as a device that produces a change in electric potential that is proportional to the rate of change of the electric current. When the current is constant, there is not electric potential difference across the inductor is zero volts. When the electric current is changing very rapidly (like going from some current to zero when you open a switch), the electric potential difference across the inductor can be huge.
And here is the answer to sparking wires: If the circuit contains an inductor then opening or closing a switch (or pulling off a wire) will create a very large change in current (since it goes to zero). This huge current change makes a giant change in electric potential across the inductor. The giant electric potential difference means that you can get super high electric fields around the contact points. The electric field can be large enough to be at the 3 x 106 V/m level that would produce a spark.
But wait! What if your circuit doesn't have an inductor in it? Here's a surprise for you: Every circuit is an inductor. Since every circuit makes a loop, it at least has some inductance in it—so every circuit can spark when you open or close a switch.
Back to the Iron Man scene. Why would there only be a spark on the first cable that is removed from suit? Let's step through this in parts. First, I assume that there is a large current running from the battery to the suit and then back to the battery. This is a complete circuit—everything is good. Second, Tony pulls off one of the cables. If there is an inductance in the suit (highly likely) then this rapid decrease in electric current would induce a large voltage to create a spark. Finally, Tony pulls off the other cable. But there's a big difference—he's already broken the circuit. No complete circuit means no electric current and no change in current. There shouldn't be a second spark.
But in the end, it's just a movie and not real physics. Technically, this would be a scientific error—but it doesn't really have any impact on the plot. And even if it was a plot-related error, that's OK (as I have said many times before). I still like Iron Man.
Don Cheadle and Shane Black discuss Iron Man 3 which is set to be released in May of 2013.


You know that feeling. TFW you're in the market for a new phone, and you can't decide which one to buy. For the past month, if you're an Apple fan, chances are you've been feeling that feel a bit more than usual. This year, like every year, Apple released new phones—the iPhones 8 and 8 Plus. But they also released a newer phone. A "say hello to the future" phone. The iPhone X. It's got an edge-to-edge display, more and better cameras, and a certain je ne sais pas that just—wait, no. Lol, no, you totally sais. It's the fanciest phone Apple's ever made. It goes on sale tonight. And lord help you, you want it. You want it so bad.
But heavens, that price: $999, to start. Can you afford it? No. But financing. No! You're not falling for that again. You'll go with the iPhone 8 Plus. It's boring, but shut up, you will love it.
But then wait, you forgot: The iPhone X also has that sick OLED screen, and emoji you control with your face! Totally worth the extra few hundred dollars. What price can you really put on a talking pile of poop? The answer is no price. Talking poop is priceless.
Sounds like you have a tough decision to make.
Turns out psychologists have names for this kind of decision, and the hemming and hawing that goes into it.
"It's a classic example of what we call a multi-attribute decision task," says Ben Newell, a cognitive psychologist at the University of New South Wales Sydney and co-author of Straight Choices: The Psychology of Decision Making. It's a fancy term for situations in which you consider multiple options, each with attributes you use to inform your decision. If you've ever shopped for a laptop, a TV, a blender—anything that lends itself to a spec-sheet comparison or a 20,000-word Wirecutter review—you've grappled with a multi-attribute decision.
Meet the iPhone X, Apple's New High-End Handset
Review: Apple iPhone 8 and 8 Plus
Hands-On with the iPhone X, Apple's Most Stunning Yet
The iPhone X Isn't That Expensive, Actually
And the cell phone purchase is a particularly perfect example. "When we run experiments that examine the kinds of choices people make and why they make them, we often have them decide between phones," Newell says. (Cars show up a lot, too.) They're easy to deconstruct into their constituent specifications, like the number and quality of their cameras, their screen sizes, the relative superiority of their displays—and, of course, their prices.
The persnickety among you may even identify with what Newell calls a weighted additive strategy to decision making, whereby you assign each spec a number corresponding to its relative importance. (Which do you value more: a phone that fits in your back pocket, or a battery that doesn't die on the train home from work? If it's the latter, battery life gets a higher number than form factor.) When you're through, identifying your true desire is as simple as tallying the weighted values for each device.
Simple in theory, anyway. In practice, almost no one bothers to run literal numbers on choices like these. Gadget lust has a way of undermining rational decisions. But even those, Newell says, are informed to some degree by intuitive atribute-weighing, which can quickly lead to waffling.
And the timing of the iPhone X's release only complicates things further. A large body of research suggests that adding a third choice to a field of two options can affect people's appraisal of the original two alternatives. In other words: Humans base their purchasing decisions not just on the attributes of the product they're buying, but on the attributes of those they don't buy.
Psychologists call these context effects, and one of the best studied is a phenomenon known as the compromise effect. Say you have two options (A and B) that differ on two dimensions (quality and affordability). Option A is nicer, but option B is cheaper. Adding a third option, C, that is super sweet but super pricey, produces the compromise effect: In the presence of option C, option A becomes a compromise option, and its attractiveness increases.
It turns out Apple's 2017 iPhone lineup mimics the experimental conditions behind the compromise effect pretty closely. Here it is in chart form:
In the case of the 8 and 8 Plus, the 8 is more affordable, but the Plus has more desireable—call them "high quality"—features. Two cameras instead of one, more photography modes, bigger display, and so on. When Apple adds the iPhone X to the lineup, the 8 Plus becomes a compromise between the entry level 8 and the premium X. "So this kind of perverse thing could happen—if you believe the compromise effect will hold—where the introduction of the X could cause sales of the 8 Plus to go up," Newell says.
A couple big caveats apply here. One: This model ignores the influence of factors besides quality and affordability. It's easy to imagine a third vector that corresponds to the iPhone X's limited supply. Or a fourth linked to security concerns over FaceID.
And two, this model assumes that the only phones in existence, as far as the consumer is concerned, are the iPhones 8, 8 Plus, and X. Hardcore fans may only have eyes for the company's newest products, but Apple itself sells eight versions of the iPhone, not counting color or storage configurations.
Considerations like these may also help explain why demand for the iPhone 8 and 8 Plus has been so weak this past month. (According to Reuters, more customers in the US have sprung for the iPhone 7 in recent weeks than its successor.) Will those numbers change after Friday, when Apple's iPhone X supply inevitably runs dry?
Time will tell. In the meantime, keep an eye on those iPhone 8 Plus sales. It's no iPhone X, but I hear it's a solid choice. Not too basic. Not too pricey. Just right.
The New iPhone X packs more new stuff into any device since the original iPhone. It's the most complete redesign of the product ever, and even offers a glimpse at what the iPhone might become when the world no longer wants smartphones.


No one knew exactly when the girl would die, but everyone knew it would be soon. A 12-year-old with end stage cancer, the child's parents had recently moved her from the hospital to her home in the suburbs of Los Angeles. Some days later the girl's breath quickened, and her father phoned the family's hospice nurse. Please come, he said. He was worried about her breathing.
The nurse knew the visit would require more than four hours of her time: a two-hour drive in each direction, plus her time with the girl. Why don't we connect over FaceTime, she asked. The father agreed, and they connected.
The nurse asked the father to move his daughter gently to her side. Then to her back. To lift the child's shirt. To show her the expansion and contraction of the girl's rib cage. The nurse would ask: What do you see, what concerns you, and the father would explain. Then the nurse would do the same. In this fashion the pair examined the girl—the nurse on her computer, the father his iPad. Together they decided that the nurse's presence was not necessary, that the child had more time.
Later, the father reported feeling comforted by the nurse. He appreciated her availability, the fact that she could see what he saw, and their ability to discuss it in real time. "It's so unique, the visual image, and knowing that everyone is seeing and talking about the same phenomenon," says pediatrician David Steinhorn, director of palliative care at Children's National Medical Center—and the head of the telemedicine pilot the girl's family had been part of.
An expert in the burgeoning field of telemedicine, Steinhorn believes in the power of digital tools to connect clinicians with their patients. But more important than the technology, he says, is what clinicians in the field have taken to calling "webside" manner. It's a modern twist on bedside manner—a physician's ability to relate with a patient and convey their desire to help. "My experience is that, once you get past some initial hurdles, you can maintain an intimate, immediate connection with patients that in some cases may be more therapeutically useful than even in-person interactions," Steinhorn says.
But getting there isn’t always easy. As anyone who's spent time on a video call knows, communicating via telepresence is very different from communicating in person. “It's all the little things,” says experimental psychologist Elizabeth Krupinski, associate director of evaluation for the telemedicine program at the University of Arizona. “I mean, there's the technology bit, obviously. Webcam resolution, internet connection, and so on. And you have to think about your backdrop, your lighting, what you're wearing as well. But what you've really got to monitor is your behavior.” Krupinski should know: U of A is one of the first schools in the country to incorporate telemedicine instruction into its medical school curricula.
"It sounds strange, but when you're on camera all your actions are magnified," Krupinski says. Sitting six feet away from your doctor, in person, you might not mind or notice her slouching, fidgeting, or gesticulating. But a webcam's intimate vantage point augments these actions in ways that patients can find distracting or off-putting. "You take a sip of coffee and your mug takes up the whole screen, and all they hear is the sound of you slurping," she says. "Or you turn away to make a note, and now all your patient sees is your shoulder. Maybe you disappear from the frame entirely."
If these all sound like awfully little things for physicians to concern themselves with, well, you're right. But that's kind of the point. The considerations are so small and numerous that they can wind up overwhelming otherwise competent clinicians, interfering with their ability to connect with patients. "There are some people who are great in person and you put them on camera they're a dead fish," Krupinski says. Some physicians are camera shy. (For others, the physical isolation can actually help them be more empathetic—Krupinski says she's seen it both ways.) Telemedicine students are often instructed to disable their video chat's picture-in-picture feature. "Turn it off and look at the patient," Krupinski says. That's also kind of tricky: To appear as though they're making eye contact, clinicians are taught to look not at the patient on their screen, but directly into their device's webcam.
Some hospitals have gone so far as to design telemedicine clinics, purpose-built to address the peculiarities of virtual examinations. "We try our best to control as much of the environment as we can, so the doctors can be doctors," says Jim Marcin, director of the pediatric telemedicine program at UC Davis. The room is staged like an office but with better lighting. There's a nice desk for the clinician to sit behind, a computer situated stage left, and books in the background. A physician taking a video call from home, or somewhere in the ICU, might wear a gaming headset—a pair of brawny headphones, equipped with a mic—to ensure whatever the patient says isn't broadcast to anyone off-camera (a clear violation of patient privacy, Marcin says). But in the secluded confines of the telemedicine clinic, the whole room is mic-ed. The clinician can forego the gaming rig and focus on projecting a natural, empathetic presence.
UC Davis' clinic is in many ways the polar opposite of what patients see when they use online services like Teladoc and HealthTap, which rely on networks of tens of thousands of doctors to deliver health care directly to users, often without ever having met. (Neither Teladoc nor HealthTap responded to request for comment.) Marcin, Krupinski, and Steinhorn all say their institutions use telemedicine primarily to follow up with patients they've already worked with in person—something no longer required in the United States. (Texas was the holdout; earlier this year, it became the last state to allow physicians to connect with new patients virtually rather than first meeting in person.)
The Science of Telemedicine: A Lifesaver in the Right Place
Telemedicine Could Be Great, if People Stopped Using It Like Uber
The VA’s New App Tries to Reach Vets Wherever They Live
Ad hoc, virtual visits can work great when a patient needs a quick diagnosis for a sore throat or weird rash. But many experts are skeptical of clinicians' ability to deliver compassionate, high-quality care to virtual strangers. "Look, there's variation whether you see a clinician in person or whether you see them online, so I'm not saying in any way that telemedicine is less helpful than in-person visits, or that webside manner is worse than bedside manner," says UCSF pulmonologist Adams Dudley. "But webside manner definitely requires more cooperation, and a different kind of cooperation, than bedside manner."
That cooperation was often missing from doctor-patient interactions in a study—led by Dudley and published in JAMA Internal Medicine last year—that investigated the quality of virtual urgent care. The researchers conducted their investigation secret-shopper style, sending dozens of trained patients to direct-to-consumer telemedicine companies. Their scripted symptoms reflected acute illnesses such as ankle pain, low back pain, and recurrent urinary tract infections—and the researchers observed a huge range in quality of care and webside manner.
But more concerning to Dudley was the infrequency with which clinicians referred their first-time patients to specialists in their area. "Our ankle protocol was a situation where the clinician should have ordered X-rays. And in an ER, 90 percent of our test patients would have gotten it. But over telemedicine, fewer than 20 percent of patients were referred to radiologists."
The reason for the disparity isn't totally clear, but Dudley has a hypothesis: If you're a doctor in Philadelphia diagnosing a patient in Albuquerque, you're not familiar with which local provider to refer them. It's the kind of problem that could be solved with some back and forth or an ongoing patient-doctor relationship. But absent both, a surprising number of clinicians wound up not connecting the dots. "So yeah, it's terrible webside manner, and terrible care" says Dudley.
Regardless of the current state of virtual care, conscientious providers are always looking to improve. That's why Steinhorn set up his pilot palliative care program in the first place. When he thinks back to the young cancer patient in LA, Steinhorn believes that telemedicine was able to provide the girl and her family support and reassurance. And it can help physicians, too. "In my own interactions, I’ve used telemedicine to see how a family 100 miles away is coping, that the scene I see in the home appears orderly, and that the family seems together, even in times of tension and distress," he says. "And that's been reassuring.”
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?


A little bit of booze, the conventional wisdom goes, can be good for you. But the evidence for that claim—beyond anecdotal accounts that a nip of whiskey can nip a cold in the bud—is surprisingly thin. Alcohol studies usually look backwards, comparing participants’ historical drinking habits with their health problems. But it’s hard to prove that alcohol caused those problems. The best alcohol study would randomly require people to either drink or abstain—but for many public health researchers, that’s always seemed like a bridge too far.
Today, though, the National Institutes of Health is planning just such an experiment. The Moderate Alcohol and Cardiovascular Health study, now in progress on four continents, is poised to be a breakthrough in public health: the first time that researchers have followed a group of people randomized to receive a daily drink or nothing at all. But it’s also the first time the NIH has offered the $1 trillion-plus alcoholic beverage industry a chance to sponsor a project. That exchange of money, along with the study leaders’ failure to guard against outside influence, are jeopardizing the study’s credibility before it has even enrolled its first participant.
The study has its origin, strangely enough, in tea. Back in 2006, researchers thought tea drinkers might have fewer heart attacks. So Kenneth Mukamal, an epidemiologist at the Harvard-affiliated Beth Israel Deaconess Medical Center in Boston, recruited at-risk adults and told them to drink either three cups of black tea a day or three cups of water. Getting participants to stick to the program is notoriously difficult, so to make sure they were drinking their tea, Mukumal tested urine samples from a subgroup of participants for gallic acid, a tea breakdown product. After six months, they ran the numbers: Tea had virtually no effect on a person’s cardiovascular risk.
The results from the tea study may have been moot, but that gallic acid-measuring test was the proof of concept Mukamal needed for a different study. In 2008, Mukamal began giving participants an alcoholic drink—a mix of ethanol with Crystal Light or a Kraft lemonade mix—comparing their health markers to a control group that got lemonade without the booze. But people wouldn’t drink the ethanol. Like in the tea study, Mukamal was tracking whether his participants stuck to the program, and a blood test that rises with alcohol intake was actually higher in the non-alcoholic group than the alcohol group. A randomized trial to test alcohol’s benefits, Mukamal concluded, would have to let the participants choose a drink they actually liked.
Enter the National Institute on Alcohol Abuse and Alcoholism. In 2014, the institute solicited Mukamal to plan a six-year study of 7,000 subjects at risk of heart disease, pitting a daily alcoholic drink against total abstinence. They’d track heart attacks, strokes, heart-related chest pain, and death—the most comprehensive study of the heart impacts of daily drinking ever done, focusing on adults 50 and over. Instead of giving subjects a mix prepared by research pharmacists, though, the study would reimburse them for the drink of their choice.
Paying for more than 3,500 daily drinks for six years, it turns out, is expensive. The NIH would need more funding—and soon, a team stepped up to the plate. The Foundation of the NIH, a little-known 20-year-old non-profit that calls on donors to support NIH science, was talking to alcohol corporations. By the fall of 2014, the study was relying on the industry for “separate contributions to the Foundation of the NIH beyond what the NIAAA could afford,” as Mukamal put it in an e-mail to a prospective collaborator. Later that year, Congress encouraged the NIH to sponsor the study, but lawmakers didn’t provide any money. Five corporations—Anheuser-Busch InBev, Diageo, Pernod Ricard, Heineken, and Carlsberg—have since provided a total of $67 million. The foundation is seeking another $23 million, according to its director of development, Julie Wolf-Rodda.
The NIH has run into similar ethical problems before. In 2012, the National Football League made a $30 million donation to support research on sports-related medical problems. Later, when the NIH funded an expert on football-related brain injury whom the League disapproved of, NFL representatives contacted senior NIH officers about it. A 2016 Congressional investigation faulted the NFL for attempting to use its donation “as leverage to steer [research] funding away from one of its critics.” But the investigation also concluded that the Foundation didn’t do enough to protect the NIH from its NFL funders.
To protect the alcohol study from the influence of its funders, the Foundation uses detailed letters of agreement: Companies are obliged to accept NIH control of the project’s scientific and administrative aspects and barred from attempting to influence its design or conduct, and from trying to access non-public project results. Those agreements were in place during the NFL deal, and Wolf-Rodda says the organization has since “tightened” the language in its agreements, making the rules “now a little bit clearer and harder for people to overlook.” Both Mukamal and Peggy Murray, a senior leader at NIAAA and the NIH staff scientist on the trial, say they haven’t spoken to anyone from the alcohol industry about the project, another important wall to prevent any influence, intentional or not, from the companies.
But for some scholars, there is no right way to involve the industry in investigating alcohol’s benefits. “It’s a clear conflict of interest if the industry that’s going to profit from the findings of the research is funding it,” says David Jernigan, head of the Center on Alcohol Marketing and Youth at the Johns Hopkins Bloomberg School of Public Health in Baltimore. Richard Saitz of Boston University’s School of Public Health compares the situation to the tobacco industry’s support of research that raised doubts about the risks of smoking. In some circumstances, he says, “a firewall is just not quite enough.”
And although the study has not yet begun recruiting subjects, the medical community has already found reason to validate those concerns.
Jimmy Volmink, dean of the medical school at Stellenbosch University near Cape Town, South Africa, first heard about the alcohol trial in the fall of 2014. To recruit a diverse set of participants, Mukamal was assembling a global team of research partners; Volmink, a black South African physician who did his medical training under the apartheid regime, was considering signing on. Volmink holds a public health degree from Harvard and a doctorate in cardiovascular medicine from Oxford, and his claim to fame in academia is his leadership role with Cochrane, an organization that reviews clinical trial results for doctors.
After a few exchanges with Mukamal, Volmink met with two colleagues to talk about the study. The Stellenbosch medical campus is in a Cape Town suburb near stylish outdoor malls rivaling those of Florida or California, but the group’s discussion focused on the impoverished townships that dot the Cape Flats, only about 20 km away. The threesome worried the study could be unethical for their center, given the region’s soaring rates of fetal alcohol syndrome—among the highest in the world. South Africa is also burdened by alcohol-fueled accidents and violence, a lasting reminder of the apartheid era’s ‘dop’ system, when farm workers received part of their pay in drink instead of money. Celeste Naude, a nutrition researcher, said she also wondered about the project’s funding, since the food industry often sponsors nutrition research.
When Volmink talked with Mukamal a few weeks later, he followed up on Naude’s funding question. Mukamal said the project had a commitment from the NIAAA, and NIAAA would also receive alcohol industry monies. “There’s no direct funding by the alcohol industry,” Volmink recalls him saying.
In an interview later, Volmink said the discussion had made him uncomfortable—he hadn’t heard about the industry’s role before, and he was troubled by the lack of transparency. Quantitative analyses of drug and medical device studies have found that they are about 30 percent more likely to reach positive conclusions when the research is industry-sponsored.
So in mid-October 2014, Volmink e-mailed Mukamal to say Stellenbosch would not participate, citing problems they expected to have in recruiting participants, South Africa’s high levels of harmful drinking, and “funding from the alcohol industry, albeit channeled through the NIAAA.” But he closed with gratitude: “Thank you for inviting us to collaborate. We look forward to future opportunities to do so.”
Mukamal sent a stinging reply. “Thanks Jimmy. It sounded earlier like you lack the strong trial infrastructure and experience and appropriate population we would need for this to be carried out safely and effectively, which may not exist in many places in [South Africa] and beyond.”
He added, “I doubt we’ll have similar opportunities to study questions as central to nutrition as this again, as few are as prominent, but if we do, we’ll let you know.”
Two years later, when the NIAAA awarded Mukamal the grant to lead the study they’d already funded him to design, the moderate alcohol trial included sites in Nigeria, Argentina, Europe and the US. The Foundation also had established formal agreements with the five alcohol corporations for the NIAAA’s $67 million.
Yet when I spoke to Mukamal in February 2017, he said he didn’t know about the Foundation’s negotiation for industry contributions “until relatively recently.” And a New York Times reporter who wrote about the trial in July, documenting how many of its global collaborators have received alcohol industry funding, quoted Mukamal as saying “he was not aware that alcohol companies were supporting the trial financially.” Mukamal later told me the Times’ quote was “completely wrong.” And he explained in an e-mail that his knowledge of the alcohol industry’s support is limited since he is not privy to the Foundation’s contracts with funders.
“We have no contact with funders other than NIAAA itself whatsoever,” he wrote. “To me, the whole point of having FNIH involved is exactly that—so that if industry wants to see truly rigorous science, they get no say in what happens whatsoever.” Typically, when studies supported by the Foundation of the NIH are published, scientists simply list FNIH as a funder. No information is provided about how the Foundation raised money.
But the relationship between the NIAAA and its foundation funders is less murky. Murray, who heads global alcohol research for the NIAAA appeared with NIAAA director George Koob in a promotional video for Anheuser-Busch InBev—one of the study funders—about company-sponsored research. NIH is strict about outside interests, prohibiting its employees from advising any “substantially affected organizations” https://ethics.od.nih.gov/topics/COI-Fact-Sheet.pdf, but Murray and Koob got permission from the US Department of Health & Human Services to attend an Anheuser-Busch meeting in New York, where the company filmed them.
Tech's Alcohol-Soaked Culture Isn't a Party for Everybody
The Muddled Link Between Booze and Cancer
Everything Science Knows About Hangovers—And How to Cure Them
In an interview, Murray said the alcohol trial likely already had the industry’s commitments at the time of the filming, but she and Koob hadn’t been thinking about it. Instead they’d been focused on the reason they were invited to the meeting, an Anheuser-Busch plan to study interventions to reduce ‘harmful drinking’ in cities around the world. In the video, Murray says she “really likes” the company’s research plan, comments she said she stands by today (http://www.ab-inbev.com/better-world/a-healthier-world/global-smart-drinking-goals.html).
“It always surprises me when people are critical of us even talking to industry,” she says. “It is a legal industry and they are a constituency as much as anybody.” But if they had it to do again, she and Koob agree they probably would say no. Seeing NIAAA leaders in an industry promotional video was “disconcerting” for James Sargent of Dartmouth’s Geisel School of Medicine in New Hampshire, while for Michael Siegel of Boston University, the video showed the NIAAA was “corrupted by the alcohol industry.”
Without the industry’s scores of millions, the NIAAA alcohol trial wouldn’t be happening, according to Murray. The study’s cost relates to its size and breadth, key to producing findings that can withstand statistical scrutiny. But several scientists worry this is not the right tradeoff between robust research and industry influence, statistical power and ethics. Given the size and specifications of the corporate donations, there’s a real possibility of the trial being influenced, said alcohol researcher Jürgen Rehm, of Toronto’s Centre for Addiction and Mental Health. What if scientific investigators discovered the study was too risky, or too risky in certain countries, he asked. “You would stop, but because you have this funding and it’s project-specific, you won’t want to stop.”
The NIAAA trial is still undergoing ethical review by institutional review boards, the ethics panels charged with protecting subjects from study-related harms. But Mukamal is promising a game-changer—“a truly definitive experiment that will settle this once and for all,” as he wrote in an editorial last May. Others in the field don’t see it that way. “The way this is set up, it smells,” says Rehm. “I’m sorry.”
It’s the preferred poison of Russians, dieters, and college freshmen, but what’s actually inside the fire water? It's rarely potatoes, actually. At its purest, it's mostly ethanol that has been distilled from wheat, rye, or corn and diluted with water. Find out what else is inside the clear spirit.


Stay on target. That’s the mantra you hear in labs and biotech companies around the world as they snip away at DNA. All the techniques for gene editing—from the famous Crispr-Cas9 to the older TALENs and zinc-finger nucleases—share a problem: Sometimes they don’t work.
Which is to say, they have “off-target effects,” changing a gene you don’t want changed or failing to change a gene that you do. And DNA is not something you want poorly rewired. That goes double if you’re trying to make money; companies working on genome-editing based products are valued in the billions of dollars. That’s why two scientific articles published today in the journals Nature and Science are so important—they tune genome editing up.
The Nature paper chases precision at a literally basic level—the bases, the As, Gs, Cs, and Ts that are the individual units in the genetic code. Crispr-Cas9 works by slicing through the two strands of bases that spiral to create DNA’s famous double-helix. But another approach, single base editing, actually converts one base into another—since the bases pair in predictable ways, A to T and G to C, that modification flips a single genetic “bit.” Until now, scientists have only been able to change a G-C base pair into an A-T base pair.
The new paper takes the other angle, describing an editor that changes adenine—the “A”—into a base called inosine, which the cell’s protein-building machinery reads as guanine, the “G.” When that molecular machine puts a little nick in the complementary strand of DNA across the gap where the T is, the cell’s DNA repair machinery “fixes” it by slotting in a C. In other words, it’s an A-T to G-C base edit.
How cool is that? “This class of mutation, changing a G-C to an A-T, accounts for about half of the 32,000 known pathogenic point mutations in humans,” says David Liu, the Harvard chemist whose lab did the work. Liu’s lab has already used this editor to fix—in cell cultures—the mutation that causes hereditary hemochromatosis, which causes a person to retain too much iron, and to treat sickle-cell anemia.
Getting there wasn’t easy. In biology, changing one molecule into another is usually the job of a natural nanotechnological marvel called an enzyme. Enzymes that turn adenine into inosine are called adenine deaminases, but none exists that’ll transmogrify adenine embedded in a strand of DNA. So Liu’s team built one, putting engineered bacteria under evolutionary pressure until it built an enzyme that would target A’s in DNA.
And it goes to the right A, too. One of Crispr’s components is a molecule of “guide RNA,” a length of genetic stuff that points to a target like the scrap of clothing you hand a bloodhound before a hunt. Liu’s editor uses that part. “Normally Crispr-Cas9 makes a double-stranded cut in the DNA,” Liu says. “We used a form of Crispr-Cas9 that’s crippled. It cannot cut the DNA.” But it still stays on target.
The research in the Science paper takes a different tack on A-to-G conversion. This one, from the lab of Broad Institute researcher Feng Zhang, incorporates an adenosine deaminase (a molecular cousin of the adenine deaminase in the Liu paper) into Crispr-Cas13, a variant genome editor that works on RNA—the copy of DNA that cellular machinery reads to build proteins. Zhang’s team calls it “RNA Editing for Programmable A to I Replacement,” or Repair, proving that if the fights over Crispr’s genesis and patent have taught researchers anything, it’s to come up with better names.
Because it acts on RNA, Repair makes a transient change, which could be good for treating problems like acute inflammation or wounds—potentially dangerous, but you wouldn’t want to turn off someone’s inflammatory response permanently. “There are 12 possible base changes you can do,” says Omar Abudayyeh, a researcher at the Broad Institute and one of the paper’s authors. “Now we’re thinking about the ways to do the other 11.”
But both approaches attempt to sharpen Crispr’s scalpel. With typical Crispr-Cas9 in DNA, the problem isn’t the cut; it’s the repair, which can cause a kind of genetic scarring, so-called stochastic insertions or deletions, or “indels”—additional bases tossed in, or a few removed. “When you make a double-stranded break in the genome, the cell tries to get the ends back together, and most of the time it’s successful,” Liu says. But every now and then, the cell just can't quite put Humpty’s DNA together again. If your goal is to seriously bork a gene, indels can be great. But if you’re trying to splice in a new stretch of DNA, they're a problem.
By operating on RNA, Crispr-Cas13 avoids all that. RNA repair doesn’t involve indels, for one thing. And: “There’s always a concern for off-target effects with these types of systems,” Abudayyeh says. “But with RNA you have to think about off-targets a little differently.” A miswired stretch of DNA means all the RNA transcribed from it and all the protein translated from the RNA will be busted. If some of the RNA in a cell gets edited correctly and some doesn’t, that means the cell will have at least some amount of the right protein. If things go really wrong, the edit is reversible. “You can always remove the system, and the RNA will eventually degrade and recycle and revert back to normal,” Abudayyeh says.
Likewise, editing of DNA that relies on base-pair modification instead of double-stranded cuts gets around some of those other limitations. “David’s work follows on his earlier innovative efforts to do genome editing without a double-strand break,” says Fyodor Urnov, associate director of the Altius Institute for Biomedical Sciences. And Zhang’s work “adds to the toolbox by giving us an enzyme that can edit RNA in a precise fashion.”
Those kind of tools are much in demand. In June 2017, a letter from researchers published in Nature Methods asserted that in addition to the indel problem Crispr messed up the genome in a whole lot of weird ways. Crispr researchers quickly rallied to dismantle the paper’s methods and analysis, but they all acknowledge that some applications of Crispr-Cas9 yield better results than others.
Crispr's Next Big Debate: How Messy Is Too Messy?
Scientists Crispr the First Human Embryos in the US (Maybe)
How Crispr Could Snip Away Some of Humanity's Worst Diseases
Why bother? Because there’s more than just new drugs, new drought-resistant crops, and new materials on the line. A half-dozen or more companies have gotten venture capital to work on Crispr-powered products. Liu, Zhang, and Joung, along with the researcher George Church, are all co-founders of one of the big ones, Editas Medicine. Crispr-Cas9 co-inventor Jennifer Doudna was once on that team as well. Editas and the like-minded companies Intellia and Crispr Therapeutics briefly lost tens of millions in valuation on the publication of that Nature Methods paper. A fight over who actually invented Crispr-Cas9 is ongoing with UC Berkeley, Doudna, and her co-author Emanuelle Charpentier on one side and Church, Zhang, and the Broad Institute on the other.
So it’s critical that Crispr-Cas9 and its follow-on technologies work. Liu’s single-base editors are a long way from becoming a therapeutic, but multiple modified organisms and treatments made with Crispr-Cas9 are in the approval pipeline. Genome editing is still working on finding the bullseye in the depths of cells, but it’s also aiming at financial ones, and at changing the world. It’s going to need one hell of a good targeting system.
'Stranger Things' stars Finn Wolfhard and Caleb McLaughlin show us the last things they did with their phones. What was the last emoji they used? The last text message sent? What was the last thing they searched?


Like with jetpacks and flying cars, the Power Loader from Aliens is a robot we’ve been promised for a long time. That’d be the exoskeleton that Sigourney Weaver donned to beat the tar out of the movie’s eponymous alien Queen, of course. Jetpacks are kinda here, flying cars … almost, and now a real-life Power Loader has finally arrived, and it’s orders of magnitude more impressive than the suit of fiction.
Behold the Guardian GT from Sarcos Robotics, which in all honesty is full-tilt bonkers. Bonkers in the sense that unlike the clunky Power Loader, these 7-foot-long arms replicate human motions with incredible smoothness and accuracy, each limb lifting 500 pounds, then turning around and manipulating the most delicate of objects. Watching it in action is both hypnotic and highly unsettling.
A big problem in robotics right now is manipulation. You take for granted how easy it is for you to, say, pick up a piece of paper off a table. But imagine a robot trying that with clutzy metal fingers. No matter how strong a robot may be, it’s still miles away from human dexterity.
But with the Guardian GT has a couple of advantages that make it remarkably dexterous. For one, it's kinematically equivalent, meaning it's arranged like a human, so the operator is controlling what is essentially a sized-up version of their own body. “The distance between those stereo cameras and the shoulder is the same ratio as you have in your own human body,” says Ben Wolff, CEO of Sarcos. Same goes for the distance between the shoulder and the elbow and the elbow and the wrist. “So it's very intuitive. That kinematic equivalent concept enables a brand new operator with no training at all to be able to get into the machine.”
On top of that, the robot uses force feedback, so the operator can feel the environment through the machine's hand (which consists of three fingers instead of five). Imagine trying to lift a mug if you could see it but not feel it. With force feedback, the pilot can feel when the robot's hand makes contact with even small objects like switches and buttons. The robot can even pop open an electrical box with the tippy-tip of one of its fingers.
The Guardian GT also has a dexterity advantage over other humanoid robots because it's all custom-built. Sarcos didn't buy hands (known more formally as end effectors) from a hand company and slap them on arms from somewhere else. Everything is customized to work in harmony.
What Is a Robot?
This Robot Tractor Is Ready to Disrupt Construction
Inside SynTouch, the Mad Lab Giving Robots the Power to Feel
While it all may look effortless, it’s not as if the operator isn't feeling a thing when they lift a 1,000-pound pipe. “That's a little disorienting, so we give a little bit of load into the arm,” Wolff says. Meaning, the robot pushes back a tiny bit. “So instead of lifting a thousand pounds you feel like you're lifting five.” (How it’s able to do this without the herky-jerkiness of other robots comes down to special actuators, the bits that move and bend the arms. What exactly is special about them, Wolff declines to say, because he’s a good businessman.)
Just imagine this thing on a construction site. Doing something like lifting and joining two pipes would require a crane and maybe five or six workers, who would be freed up to do other jobs that require a more human touch (fine manipulation, for instance). With the Guardian GT, all it takes is one supercharged human. It still requires a lot of coordination, sure, but the robot takes the strain out of the equation.
What’s interesting about this workplace robot is that it’s collaborative—a human is always in control. And that’s what the future of work looks like, especially heavy industry. Fields like construction and agriculture are already facing severe labor shortages, and the machines are poised to pick up the slack. Think automated construction tractors and robots that help humans harvest crops without all the stooping. In the very near future we’ll be working alongside robots, as opposed to robots outright replacing us.
So the death of human labor, it seems, has been greatly exaggerated. “While I think that we will see increasing amounts of autonomy and AI,” says Wolff, “I think the real role in work generally is for us to find as humans how to maximize the utility of robots. Allow them to do what they're really good at while still relying on what humans are best at, which is wisdom and judgment.”
Then there are the jobs that humans simply can’t do. Because the Guardian GT rolls on either tracks or wheels, the operator can drive it into danger. Think exploring toxic environments and decommissioning nuclear power plants. With the inherent dexterity of the machine, it could easily manipulate things made for humans hands, like valves and buttons.
And, if the time comes, it might save us all from aliens. But let’s hope it doesn’t come to that.


For patients with epilepsy, or cancerous brain lesions, sometimes the only way to forward is down. Down past the scalp and into the skull, down through healthy grey matter to get at a tumor or the overactive network causing seizures. At the end of the surgery, all that extra white and grey matter gets tossed in the trash or an incinerator. Well, not all of it. At least, not in Seattle.
For the last few years, doctors at a number of hospitals in the Emerald City have been saving those little bits and blobs of brain, sticking them on ice, and rushing them off in a white van across town to the Allen Institute for Brain Science. Scientists there have been keeping the tissue on life support long enough to tease out how individual neurons look, act, and communicate. And today they’re sharing the first peek at these cells in a freely available public database. It provides a more intimate, intricate look into the circuitry of the human brain than ever before. And it’s just the beginning of a much larger effort to build a complete catalog of human brain cells.
This first release includes electrical readings from a few hundred living neurons—all recently removed from 36 neurosurgery patients in Seattle area hospitals. For 100 of those cells, Allen Institute researchers built 3-D models of their branching structures, which they can use to simulate patterns of pulses and zaps. Scientists can see where in the brain neurons start and stop, and how current flows and spreads a signal throughout a neuronal network—signals that might move a muscle, or make a memory.
“Thirty minutes ago that was part of someone’s brain, maybe even the part that holds the memory of their first kiss,” says Christof Koch, chief scientist and president of the Allen Institute for Brain Science. “No one has had access like this before, to healthy brain tissue at the level of individual neurons.”
Because of the particular peskiness of studying the human brain, most brain maps are built from mice or post-mortem human tissue. Dead brain cells can tell you a lot about shape; you can stain ‘em and characterize their morphology. But they can’t tell you anything about the function of circuits—because they don’t fire.
So to keep the post-op cells alive and kicking, doctors pack the pea-sized tissue on ice. They need to keep the brain bit as close to freezing as possible—about 4 degrees centigrade—to slow down metabolism and prevent the tissue from deteriorating. Once it arrives at the Allen Institute, researchers slice the sample into many dozens of sections no thinner than a silicon wafer. Each slice is loaded into special containers Allen scientists developed to keep the tissue on life support. They look kind of like little baskets, floating around a tiny pump that bubbles in a life-sustaining stream of oxygen.
That’s enough time for people like Jonathan Ting, an assistant investigator at the Allen, to isolate individual neurons and push and prod them with glass electrodes. By forming a super-tight connection with the cell—what’s called a giga-ohm seal—he can measure how it spikes in response to little shocks of energy. Each pattern of spikes acts as a signature that helps identify the function of a neuron—maybe this one forms a thought, while that one feels a feeling.
“For 150 years neuroscientists have classified cells by the way they looked,” says Ting. But, he says, like describing a person, how much can you really know about someone just by looking at them? What about how they talk? How they act? What about their social network? All these different things paint a clearer picture of who a person is. “It’s the same in neuroscience,” says Ting. “We’re just starting to align morphology with electrical behavior with genetic expression.”
Inside Paul Allen's Plan to Reverse-Engineer the Human Brain
A First Big Step Toward Mapping the Human Brain
The Nameless Mouse Behind the Largest-Ever Neural Network
In addition to the recordings and 3-D renderings released today, the Allen Institute’s database now also contains gene expression profiles for nearly 16,000 individual neurons. This emerging field, known as single cell transcriptomics, lets scientists see all the genes that turn on in different kinds of brain cells. And on Monday, the National Institutes of Health awarded the Allen Institute and its collaborators $100 million to continue classifying human brain cells based on their shape, physiology, connective properties, and gene expression.
The largest bit of the grants, though, will go toward creating a comprehensive atlas of cell types in the mouse brain, which would be a first for mammals. That effort is part of the federal government’s 2013 Brain Initiative, which aims to understand brain circuits well enough to devise new therapies for diseases like epilepsy, Alzheimer’s, and muscular dystrophy. John Ngai, a cell biologist at UC Berkeley, is partnering with the Allen and four other institutions to document all the cells in the rodent brain over the next five years, which at just about the shape of a sugar cube, is roughly the size of just one of the Allen’s human brain bits.
Unlike those tissue samples though, a mouse brain is complete—it’s got all the parts, from the frontal lobe to the medulla oblongata. And while it may not be as complex as the human brain, simplicity is a plus when you’re attempting the first complete cranial catalogue for a warm-blooded creature.
Just how much is left to discover? “Almost everything,” says Ngai. “We could double our knowledge base in the next five years and still have over 99.9 percent left to learn.” These atlasing projects are really about trying to understand the hardware inside the human head. And to hope like crazy that it provides some insight into the uniquely human aspects of brain function.
The Connectome is a comprehensive diagram of all the neural connections existing in the brain. WIRED has challenged neuroscientist Bobby Kasthuri to explain this scientific concept to 5 different people; a 5 year-old, a 13 year-old, a college student, a neuroscience grad student and a connectome entrepreneur.


Last September, SpaceX was fueling its Falcon 9 for a routine test fire before launch when a composite liner inside the upper-stage fuel tank failed, letting oxygen seep in. Friction between the tank’s aluminum liner and carbon overwrap ignited a towering inferno that engulfed the rocket, the launch pad, and a $200 million satellite owned by Israeli firm Spacecom. It was SpaceX’s second disaster in 14 months—preceded by the explosion of a Falcon 9 heading to the International Space Station in June of 2015.
But far be it from Spacecom to get scared off by a little fire and brimstone. According to a Hebrew-language press release published last week, the company will attempt to launch another satellite atop a reusable Falcon 9 rocket in 2019—a freebie to make up for the first kablooie. And after that, they’ve signed up for another SpaceX launch in 2020.
Somehow, it seems, SpaceX has restored confidence in its customers after its failures in 2015 and 2016. This year, it has launched 15 missions, reusing three of its boosters. On that string of successes, Spacecom is returning to SpaceX’s launch manifest along with an array of customers lining up for dramatically low prices—and the opportunity to sign up for an even cheaper recycled booster launch. The value proposition is even appealing to that most cagey of customers: the United States military.
SpaceX owes a great deal of its reputation—and its $21 billion valuation—to its repeat customers. In 2013, Luxembourg-based communications provider SES took a risk to fly atop the first Falcon 9 rocket to geostationary orbit. In return, SpaceX offered SES the first reusable orbital rocket launch in aerospace history and all the free publicity that came along with it. The historic mission launched on March 30, and on October 11, SpaceX fired off its third reusable rocket, carrying another SES payload.
Another loyal customer, Iridium, indicated last week that its next two SpaceX launches would fly on reusable boosters. “I think you’re already seeing the commercial satellite industry realize the impact ‘flight proven’ boosters could have on the market,” says former White House space advisor and former SpaceX official Phil Larson. “Some are contributing to the overall push for this new capability.”
The main advantage for SpaceX is obvious: price. Going on just the advertised prices from both SpaceX and competitor United Launch Alliance for the cheapest mission, the difference in costs before taxes and insurance is about $47 million. Elon Musk claims that cost difference for certain launches could exceed $300 million––which is the average cost of a satellite. “So flying with SpaceX means satellite is basically free,” the billionaire tweeted.
Which is how SpaceX finally broke into the public sector. In May of 2015, SpaceX won a lawsuit to end the monopoly on Air Force missions held by United Launch Alliance—and today, the Air Force is a satisfied and enthusiastic customer. The head of US Space Command, John W. Raymond, told Bloomberg that it would be “absolutely foolish” not to be utilizing recycled rockets in the future.
And SpaceX would be foolish not to fight for those military contracts, which are the biggest it gets. So far, it’s done well: SpaceX launched its first national security mission earlier this year for the National Reconnaissance Office and followed that clandestine mission with another. In early September, as Hurricane Irma approached Florida’s coast, SpaceX launched the secretive Boeing-built X-37B spaceplane for the Air Force.
SpaceX Lifts Off as Kennedy Space Center Braces for Hurricane Irma
SpaceX Will Lose Millions on Its Taiwanese Satellite Launch
SpaceX's Mars Plans Hit a Pothole. Up Next: the Moon?
And just last week, SpaceX slated another top-secret mission—code-named Zuma—in between already planned launches in November. Documents obtained by WIRED point to a classified payload built by Northrop Grumman for the military. The details of the launch are heavily under wraps.
Next door to the Air Force at Cape Canaveral is NASA, another customer burned by SpaceX. In June of 2015, SpaceX lost an agency resupply haul bound for the International Space Station when the Falcon 9 exploded minutes after liftoff. But like Spacecom, the agency bounced back. Since the explosion, SpaceX has completed five successful resupply missions for NASA—and it’s making progress toward flying NASA astronauts to the space station in late 2018.
NASA’s interest in SpaceX’s reusable technology seems to be growing as well. Sources at Kennedy Space Center tell WIRED that NASA and SpaceX have preliminarily agreed to launch the next two cargo resupply missions to ISS atop reusable rockets. The agency also announced last week that it has contracted SpaceX to launch the collaborative Sentinel-6A mission in 2020 to gather ocean topography data.
Today, SpaceX’s mission cadence puts them in a dead heat with the Russians for 2017 launches. Vice President Mike Pence didn’t seem to notice, though, as he held the first National Space Council meeting on October 5. “America seems to have lost our edge in space,” Pence said.
SpaceX president Gwynne Shotwell sat on the panel, perplexed.
The private American companies battling it out for a $3.5 billion NASA contract have one last chance to successfully launch their spacecraft before a decision is made in January.


The cargo ship Yacu Kallpa rode impatiently at anchor off Iquitos, Peru, a ramshackle city on a bend in the broad, turbulent waters of the Amazon River. She was a midsize ship, a tenth of a mile long, low-slung, with a seven-story superstructure in the stern and plumes of rust fanning down the hull from her main deck scuppers. She was like any other cargo ship in the world, but with a dark history. At that moment, in November 2015, she needed to get out of town fast.
The captain and crew had a long run ahead, nearly 2,300 miles down the Amazon, then another 4,000 miles north to Tampico, Mexico, and finally to Houston, with lumber harvested from the Amazon rain forest. It was a route the ship and its predecessors had run hundreds of times for more than 40 years, hauling millions of pounds of timber at a time, to supply lumberyards and big-box stores across the United States with the ingredients for the floors, decks, and doors of the typical American home.
In Iquitos, the waters were too shallow for the Yacu Kallpa to dock amid the tin-roofed stilt houses and the brightly painted tourist boats that lined the riverfront. So small workboats were ferrying stacks of lumber from shore, to be lifted into the hold by two onboard cranes. This was a job that could take two weeks under the best of circumstances. The longer it took, the more time customs officials had to prove that the lumber being heaved aboard the Yacu Kallpa had no business being there at all.
A Curious Plan to Save the Environment With the Blockchain
All the Trees Will Die, and Then So Will You
Environmental Justice Enters Its Age of Anxiety
As the loading crews worked, 35 inspectors from a government agency called Organismo de Supervisión de los Recursos Forestales y de Fauna Silvestre (Osinfor) were slogging through forests all around Loreto Province. The inspectors were armed with handheld GPS navigators and a few batches of documents that listed the supposed harvest sites and the species of trees on the ship. More often than not, as they visited the sites listed on the paperwork, they found no evidence that any trees had been cut down there—no stumps, no debris, no disturbance—much less the trees listed on the documents. Sometimes there was no suggestion that trees ever grew there in the first place. They’d leave a mark in spray paint and jot a note on a report: “No existe en un radio 50m,” shorthand for no trees logged here, nor within 50 meters in any direction.
The vast scale of illegal logging in the Peruvian Amazon has long been an open secret. Government officials didn’t care, and until recently there was little anyone else could do to stop it. Local activists often died trying. But for a few defiant government agents, this time there was hope. The urgent question: Could they finally prove that enough trees came from illegal logging sites for prosecutors to stop the ship from sending its cargo into the US market?
As the Osinfor inspectors pushed deeper into the Amazonian forests and dockworkers hurried to load the ship, 30 or so staffers at the Environmental Investigation Agency, a nonprofit organization, waited nervously in an office just off Dupont Circle in Washington, DC. They had been developing methods to tie the ship to illegal logging for four years. But at this moment, they just had to wait and see what would happen next. It was up to government agents in Peru and Washington to make stick all the work the EIA staffers and forest inspectors in Peru had done. If everything went right, this would be the last voyage of the Yacu Kallpa.
The vast scale of illegal logging in the Peruvian Amazon has long been an open secret.
The Environmental Investigation Agency got its start in the mid-1980s when a trio of Greenpeace investigators became disenchanted with that organization’s increasing scale and incendiary tactics. The idea was that the new organization should stay small—and focused on environmental crimes. Over the years, the EIA’s investigators have made a reputation for meticulously assembling detailed evidence of criminal behavior, via undercover work, in some of the most dangerous corners of the world.
One such investigator is a mild, studious figure named Alexander von Bismarck, now 45 years old, tall and thin with close-cropped red hair retreating at the temples to form a widow’s peak. Von Bismarck grew up spending part of each year with his American mother in the United States and part with his father in Germany. He graduated from Harvard on what he calls “the 12-year plan,” after a diversion to try his hand as a professional equestrian show jumper, another to study the demise of cichlids in Uganda’s Lake Victoria, and finally a tour in the Marines, where he trained as a scout swimmer for an amphibious landing unit. If his name brings to mind Otto von Bismarck, the 19th-­century German statesman and grand European strategist, that’s because he is descended from the Iron Chancellor’s brother (“a potato farmer,” he notes). A penchant for strategic thinking nonetheless persists.
EIA chief Alexander von Bismarck.
In 2005, von Bismarck set out to persuade Congress to amend the Lacey Act, the nation’s chief law against trafficking in stolen wildlife. The ambition was to include stolen forests too, making it a federal crime to import illegally harvested plants. Von Bismarck was able to help forge what was later termed “a Baptist-bootlegger alliance” with US timber producers, who were ready to push for the amendment because, by their own estimate, competition from illegally imported timber was costing them $1 billion a year.
As the amendment was under debate, one of von Bismarck’s undercover investigations revealed that new terrorism-resistant doors ordered for the US Capitol building may have been supplied by an illegal timber-trafficking network in Honduras, and the wood may have been harvested illegally from a Unesco World Heritage Site. The contract for the doors was quietly canceled. (Other doors from the same source allegedly ended up at Mar-a-Lago, according to von Bismarck.) The Lacey Act amendment sailed through Congress and became law in May 2008.
Von Bismarck, who became executive director of the Environmental Investigation Agency in 2007, credits his tour in the Marines with giving him the “operational awareness” for undercover work. He once tracked wood from a protected tree species stolen from a habitat for endangered orangutans in Indonesia to baby cribs being sold by Walmart in the United States. He and other EIA agents also went undercover in the Russian Far East and helped prove that Lumber Liquidators was knowingly buying hardwood flooring made from illegal timber taken from the last remaining home of the critically endangered Siberian tiger. (Lumber Liquidators pleaded guilty to violations under the Lacey Act and agreed in 2015 to pay a $13.2 million penalty.)
Of the logging industry, von Bismarck says: “So many of them would love to have a system based on legal wood—so many of them feel trapped in a system that is a race to the bottom.” He pauses. Then he adds, “But some guys just need to go to jail.”
In 2009, with the Lacey Act amendment in place, von Bismarck began trying “to figure out how to make a case with the new law.” A trade deal between the United States and Peru was just going into effect, and it included new penalties for illegal logging and made Osinfor an independent agency. Von Bismarck saw an opportunity. He and Andrea Johnson, then director of the EIA’s forest campaign, hired a Peruvian journalist named Julia Urrunaga, who had spent 15 years investigating corruption for Peru’s leading newspapers.
Urrunaga is a happy warrior sort, 47 years old, just over 5 feet tall, with a great mane of curly light-brown hair. “I’m a journalist. I didn’t know much about forestry,” she admits. She and Johnson, blue-eyed, freckle-­faced, and also 5 feet tall, with a degree from Yale’s forestry school, set off to find out exactly how the lumber business worked. They visited river ports, attended endless meetings, and interviewed people about life in the logging camps. Then, one day out of the blue, an email arrived at the EIA office in Peru. It came from an Italian immigrant in Iquitos named Francesco Mantuano, who said he had been duped into buying a logging concession for what he imagined would be a lazy jungle retirement. Instead, he found himself entangled with the “wood mafia,” as he called it, in a deeply dishonest business that was sweeping away the rain forest “in a maelstrom of semi-slave exploitation, social and environmental changes … and looting of biodiversity.”
Urrunaga was suspicious at first. “Maybe someone was sending us ‘the perfect case’ to lead us to a horrible mistake,” she says. But she was also curious, so she and Johnson headed to Iquitos. They found Mantuano—“a rail-thin, wildly gesticulating Italian,” Johnson says—with a friend, an Iquitos native. The two men spent their afternoons sipping coffee and smoking in sidewalk cafés, “as though Iquitos were Milan and not this chaotic frontier river city where the mufflerless moped taxis drown out anything you say whenever the light turns green,” she adds. Urrunaga and Johnson joined the two at their habitual café to hear out their tales.
According to Urrunaga, Mantuano said he had bought into a concession with large stands of trees approved for harvest by the national forest authority. But when timber crews showed up, they left far too quickly to actually have cut the trees they had supposedly purchased. Then the timber merchants expected him to hand over transit documents for large amounts of lumber. The intent, he gradually realized, was to launder timber that had already been illegally cut elsewhere—places like national parks, indigenous community lands, and other protected areas.
Mantuano launched an indignant letter­-writing campaign to explain all this to officials in Lima and Washington. But his efforts produced no investigation until Johnson and Urrunaga showed up. Mantuano’s story backed up what the EIA was already beginning to suspect: The logging industry’s basic operating method was to cut down trees on protected lands and then produce falsified permits, either purchased on the black market or via corrupt government officials. The permits typically listed legal sites—but ones that often were remote, or sparsely forested, and thus wouldn’t yield big profits. Meanwhile, Peru’s Amazonian rain forests were being destroyed at a rate of 400,000 acres every year—an area larger than the city of Los Angeles. The women could piece together the pattern. But there was a hitch. After a tree was cut down and loaded onto a ship for export, there was no way to prove where it came from. As they compared Mantuano’s documents and data with their own, Johnson and Urrunaga started thinking: Why not prove where the trees didn’t come from? The key was to compare export documents detailing where protected species, like mahogany and cedar, were supposedly harvested with Osinfor inspections of those areas.
The remoteness of the place made cutting timber there about as
practical as harvesting trees on the moon.
It took nine months of pestering government officials in Lima, but finally the EIA received “thousands of pages of crappy photocopies,” Urrunaga says. As they waded through them, Johnson and Urrunaga could see, for the first time, where the trees had supposedly been harvested, right next to data on the ships that had sailed away with that wood. Some of those areas—like the most remote parts of Mantuano’s concession—hadn’t yet been inspected by Osinfor. Now they would just have to follow the paper trail back to the forest themselves.
With data from one of the ship’s many voyages in hand, Johnson set off on a field trip. It took her three days by boat upriver from Iquitos, then a day of hiking into land owned by an indigenous community, and another two days slogging deeper into the forest, trying to find a way around—and then across—a dense, almost impassable palm swamp. Exhausted, drenched in sweat, and wiping swarms of insects from her forehead, she finally reached the harvest location. Cedar trees from the site had supposedly been exported to a company named Global Plywood & Lumber, incorporated in Las Vegas, but no one had ever cut trees from anywhere around her. The remoteness of the place made cutting timber there about as practical as harvesting trees on the moon. There was no way the export companies were getting their wood from sites like this one. “We had set out looking for one shipment” to prove the wood being exported from Peru was illegal, Urrunaga says. “We found 100.”
Until then, most of the government officials the two women had tried to work with had been “very hostile, very aggressive” when the EIA team approached them about illegal logging, Urrunaga says. But at a meeting with Rolando Navarro, the newly appointed head of Osinfor, she and Johnson laid out evidence that “the entire system is corrupt,” Johnson recalls. “He said, ‘You’re right.’” Navarro had grown up in a river town, and he knew how loggers operated. He had worked with the World Wildlife Fund in Peru helping local communities find alternatives to illegal logging. He also knew, from Osinfor field inspections, that the documents listing the supposed harvest sites didn’t make sense. The women, Navarro, and the Peruvian customs officials they were working with realized that by homing in on suspicious exporters, they’d have a better shot at making a bust.
Eventually, customs officials asked Interpol for help, which allowed Navarro’s agents to expand their investigations. That meant they could get more documents and slog through more forests. Finally, in 2015, a multi­agency team released its report: It found that about 90 percent of the wood coming out of the Peruvian Amazon was illegal.
But “stopping the shipments and putting people in prison, that wasn’t happening,” Urrunaga says. The Yacu Kallpa kept sailing with what seemed like its old impunity, making four trips in 2014 and three more in the first half of 2015, carrying timber on its regular route to Tampico and then Houston.
Julia Urrunaga had been a journalist before tracking down illegal lumber.
To stop the shipments—to stop the Yacu Kallpa—Navarro pressed for his field agents to get transport documents earlier, while the timber was still being loaded, so they could demonstrate there was enough of a mismatch, enough deceit, to stop it before it set sail—or at least before the timber landed on the docks in Houston. So throughout early 2015, customs agents got better at extracting documents from the exporters, while Osinfor agents got more efficient in their field checks.
Soon after the ship left Iquitos for its August 2015 voyage toward Houston, Peruvian officials sent word to US investigators: A significant percentage of the timber aboard was of illegal origin.
Back in the States, the Department of Justice was just settling its illegal logging case against Lumber Liquidators and was eager to build on that success. But everyone wanted to tread carefully, as the investigation into the Yacu Kallpa could affect trade agreements between the two countries. To complicate matters, Urrunaga had been working with an Al Jazeera reporter and television crew on a story about the use of fake export documents in Peru’s logging trade, and in August the network aired its story. Prosecutors and investigators worried that it could blow the case.
In the Al Jazeera broadcast, a reporter doorsteps Kenneth Peabody, the general manager of Global Plywood, at his home near San Diego. The company had been selling Amazon rain forest timber into the US market for at least eight years, in steadily increasing quantities. Its business with a Peruvian exporter named Inversiones La Oroza had surged from $250,000 in 2012 to $2 million by 2015. Standing in his driveway, a Honda minivan on one side and a Mercedes Benz SUV on the other, Peabody looks like a soccer dad—middle-aged, in black shorts, a gray CATHEDRAL CATHOLIC DONS T-shirt, sunglasses, and a two-tone Nike golf cap.
“I’d like to talk with you about shipments of illegal wood your company is importing from Peru,” the reporter begins. He’s lean, hunched, and dressed in the shabby manner of journalists everywhere.
“Ah, I don’t have anything to say about that,” Peabody says, glancing down and then turning away to the SUV.
“According to Peruvian authorities,” the reporter continues, “your main supplier, this company called La Oroza, has been shipping many shipments of illegal wood to your company. I’ve got three of them here,” he adds, holding out some paperwork. “You don’t know about this?”
“I don’t know what you’re showing me,” Peabody says, turning back now.
“These are the documents of shipments sent to your company by this company in Peru,” the reporter explains.
After the reporter reveals that he has been to the tree harvest sites and seen no harvested trees, Peabody swallows and shifts his keys in his hand, ready for his exit.
“Do you know about the Lacey Act?” the reporter persists.
“Of course. We comply with all the requirements,” Peabody says with a dismissive sweep of one hand. The two of them go back and forth for a moment longer. Then Peabody gets into the SUV, the corners of his mouth compressed in disgust, and drives off. (Peabody declined to speak to wired. La Oroza, reached in Iquitos, denied any wrongdoing.)
Remarkably, the Al Jazeera broadcast did not stop the Yacu Kallpa from sailing on. As it neared Houston in September, agents from Homeland Security gathered. When the ship finally settled in port, they boarded and set about inspecting the cargo. Then they issued a temporary order “detaining” the timber—denying, at least temporarily, its import into the US market—and put the wood in a storage facility at the port in Houston.
A few days later, in early October, according to an affidavit from a Homeland Security investigator, a man in the dock area was directing forklift operators to move bundles of timber. It was Peabody. He’d flown in from California. Confronted by federal agents, he told them that Global Plywood owned 85 percent of the wood the Yacu Kallpa had just delivered, and it was the largest shipment in the company’s history, worth $1 million. An official of the port told Peabody that customs agents would be taking samples of the shipment to verify that the timber species on board matched the timber species listed in the paperwork required under the Lacey Act. A week later, Peabody sent in revised paperwork that added another 40 tree species for this shipment.
It seemed like the end of the run for the Yacu Kallpa. But then, as if out of irresistible habit, the ship turned around and headed back to Iquitos to pick up another load.
Mahogany, cedar, chestnut, and rosewood are all logged in Peru.
By 2015, it had been six years since the Peru free trade agreement, with all its environmental commitments, had gone into effect, and trade between the United States and the South American nation had almost doubled, to $20 billion. It promised to get even bigger under the upcoming Trans­-Pacific Partnership. But the Yacu Kallpa was quickly turning into a test case of whether the environmental commitments in free trade treaties amounted to anything more than words on paper. For critics, the ship was a big ugly billboard advertising the utter failure to stop the illegal timber trade. Potentially at stake: free trade between the two countries, and Peru’s already precarious economy.
At the same time, Urrunaga was hearing from her government sources that the lumber bosses were putting pressure on cabinet ministers to rein in Osinfor. Angry workers were taking to the street. Logging, after all, made the livelihoods of thousands of Peruvians. “The economy in Loreto moves because of logging,” Navarro says. Demonstrators staged noisy, sometimes violent protests, driving logging company trucks and tractors to the agency’s offices and carrying banners: osinfor works for the gringos. One inspector received a photograph over WhatsApp of his 1-year-old daughter in her stroller at a local park. The note said simply, “I am from Ayacucho,” birthplace of the brutal 1980s militant group Shining Path. That same week, demonstrators showed up at Navarro’s office in Iquitos carrying coffins, one of them bearing his name. Even after changing his phone number, the threatening calls rang through: “We know where your family lives.” At 3 am on November 30, two men in hoods tossed Molotov cocktails into the front of the agency’s office in Pucallpa, another logging town. “To get rid of people who are in the way is the normal thing to do,” Navarro says.
In late November, the Yacu Kallpa was loading up in Iquitos, and Osinfor agents were once again scrambling in the field. By the day before the ship was scheduled to depart, the agents had found that 15 percent of the timber coordinates were faked. A customs agent in Iquitos rushed a summary of the investigation to the local prosecutor—who hesitated. “You want me to prosecute them based on where the timber in the shipment didn’t come from?” he asked, incredulous. The details about what happened next are murky, but according to news reports and Navarro, the prosecutor wasn’t even sure he had the authority to stop the ship from leaving. The agent cajoled and bantered with him until 11:30 that night, reminding him that just weeks earlier, the nation had adopted a decree authorizing a prosecutor to seize a timber shipment on suspicion of illegality. Finally, the prosecutor agreed to meet the agent, before dawn, at the Iquitos dock—if Osinfor could provide all the documents from the investigation. Osinfor’s agents spent the next four hours printing out documents, downloaded via the creaky Iquitos internet.
Stacks of forestry documents in Iquitos.
The reluctant prosecutor and agents from Osinfor and customs arrived at the port before the sun had risen. Iquitos is a small city, and word had gotten out. A platoon of timber company lawyers and managers followed on their heels. Word also reached Urrunaga, who soon had someone recording the scene. The discussion raged—a representative of the regional forest department wanted the ship to get under way—but the prosecutor pointed out that he was in charge. Arguments went around for hours until finally the captain said, “OK, take your 15 percent”—meaning the illegal wood. Reports say the prosecutor was told that the cost of offloading the timber would be $20,000. It then took him days to obtain the necessary permissions, but when he returned to the ship, the price was suddenly more than $200,000. The Iquitos prosecutor finally agreed to accept a declaration from the captain that the ship would bring the allegedly illegal 15 percent of its cargo back after dropping off the rest. So on December 2, the Yacu Kallpa weighed anchor and turned downriver at full speed.
In the EIA offices in Washington, Alexander von Bismarck and other staff immediately went on alert. They could track the ship’s movements minute by minute, as the Automatic Identification System that all cargo ships must use pinged its location, bearing, and speed. Government agents in Peru, the United States, and at Interpol were watching too. When the ship crossed into Brazil, police there, responding to a call from a prosecutor in Lima, boarded it briefly. But they had no Lacey Act nor any other means of real enforcement. The ship pushed on, with captain and crew now aware that everyone was watching.
Osinfor investigators were also pushing, heading deeper into the forests, working their way down the long list of GPS coordinates for the cargo. By mid-December, as the ship made its way north along the Atlantic coast, the investigators sent word to law enforcement agencies in Lima and the US: More than 60 percent of the cargo was illegal.
On December 20, the Yacu Kallpa paused unexpectedly in Trinidad and emerged again in the new year—suddenly under a new flag of convenience. “I thought I was going crazy,” says an EIA technician who was tracking the ship when the flag of Panama popped up on his cell phone.
On January 3, the Yacu Kallpa made a beeline for the Dominican Republic and began to unload its tainted cargo. “Meaning we lose,” von Bismarck says. An EIA staffer called a photographer friend in Santo Domingo and persuaded him to get to the scene and start recording. But then international pressure came down on the Dominican government, and the Yacu Kallpa sailed on—“Meaning we won,” von Bismarck says. The ship staggered onward, jinked briefly toward Jamaica, then reluctantly turned back on its familiar course, heading toward Tampico.
On January 8, agents sent out a new field report: 72 percent of the cargo was illegal.
Peru’s rain forests are being cut down at a rate of 400,000 acres each year.
One day in mid-January, as the ship was still en route, Navarro had a meeting in his office with two lumber trade association representatives, one of whom he later found out was also the CEO of Global Plywood. The men lamented that they had never had such trouble in 30 years of buying timber in Peru. “Yes, that’s probably the case,” Navarro said, “and we have nothing against private investment.” But, he told them, the evidence was showing that even shipments with apparently legal documents were coming from illegal sources.
As they talked, the businessmen periodically checked their cell phones. Then, abruptly, they announced that they had to leave. A few minutes later, Navarro found out he had just been fired by the president of the country. Four days later, feeling vulnerable without the protection of public office, he fled to the United States. (The Global Plywood executive did not respond to requests for comment.)
On January 26, 2016, at 8 pm, the Yacu Kallpa limped into Tampico. At the request of Peru, the US, and Interpol, Mexican officials seized roughly 8 million pounds of rain forest timber. When its holds were finally empty, the ship anchored in the harbor—and waited. A month later, the hapless crew was still onboard, abandoned 2,500 miles from home, unpaid, and requesting help and food. Finally, at the end of February, the Peruvian embassy intervened and brought the men home. The owners of the ship, based in Lima, liquidated their assets and abandoned the Yacu Kallpa in Tampico, according to a former employee. The Mexican government assumed ownership and reportedly plans to use it as some kind of training ship.
That May, a potential timber buyer from Shanghai phoned Peabody, the Global Plywood manager, and said he’d read about the case in the newspaper and wanted to sell the wood still stuck in Houston into the Chinese market. Peabody flew to meet the buyer at a Chinese restaurant in Vancouver. Even though the wood couldn’t be imported to the US, Peabody could theoretically still sell it elsewhere. He warned the prospective buyer that any sale might be complicated by the US government. The buyer sought reassurance that Global Plywood’s suppliers in Peru could be trusted. Peabody smiled. “We trust them to do what they need to do to get by in Peru,” he said. The customer, operating under a false identity and with a video camera recording, was of course an undercover agent with the EIA.
Many months later, final reports on the ship’s last voyages were complete. For the August shipment—the one impounded in Houston—it would show that at least 92 percent of the 3.9 million–pound haul was illegal. As for the vessel’s final voyage, it took nine months for agents to complete their field checks of all the GPS sites listed on the harvest documents. They found that more than 96 percent of the ship’s cargo had been illegally harvested from the Amazon rain forest and sent north.
The ship paused unexpectedly in Trinidad and emerged again in the new
year—suddenly under a new flag.
In June of last year, federal agents showed up at the one-room office of Global Plywood, next to a volleyball court in a San Diego suburb, with a warrant to haul away paperwork, Peabody’s cell phone, and copies of computer hard drives. Peabody later emailed the “potential buyer”—the EIA’s undercover agent—to say that any possible deal was off the table. In the end, all the illegal lumber in Houston was destroyed earlier this year in a no-fault settlement with US Customs. A criminal investigation is ongoing, but so far no charges have been filed.
After more than four years and the work of hundreds of people, one offending ship responsible for carrying millions of pounds of illegally harvested wood into the US market had been stopped. It was a tremendous victory. But it was limited.
This past January, within days of taking office, President Trump pulled the US out of the Trans-Pacific Partnership. It was no longer clear just how much it mattered whether trade partners stood by their environmental commitments. Von Bismarck’s biggest concern was that timber importers in the United States would now persuade the administration to roll back the Lacey Act ban on importing illegally harvested lumber. That may not matter for the rain forests of Peru; China has now become the leading export destination for Peruvian timber, and it puts far fewer environmental conditions on its massive market for timber.
Von Bismarck says the EIA will also adapt. In fact, the agency is already developing a system to monitor forests worldwide by satellite, with updates every few days. Meanwhile, Osinfor agents continue their field inspections.
One day a few months back, in his office, von Bismarck rolled a video taken from a DJI Phantom 3 quadrotor drone. It showed two Osinfor agents in hard hats, traveling in a local man’s dugout, to check one of the GPS coordinates listed for lumber on the Yacu Kallpa’s final shipment. There was no sign that a tree could have been felled in the area, and one of the agents sprayed no e, for no existe, in blue paint on a grassy hummock. The men paddled slowly onward to the next GPS point. Then the drone pulled back to reveal that the site was in fact a vast, grass-fringed lake, glittering in the sun, with not a forest in sight and where none had ever grown.
After this piece appeared in print, the United States government blocked future timber imports from Peruvian exporter Inversiones La Oroza for up to three years.
Richard Conniff (@RichardConniff) writes about wildlife and environmental issues. He has written nine books; his most recent is House of Lost Worlds.
This article appears in the November issue. Subscribe now.
Listen to this story, and other WIRED features, on the Audm app.
In West Virginia, the Nature Conservancy is bringing back forests with the help of a very special fungus.


Here’s a depressing number for you: 12. Just 12 percent of engineers in the United States are women. In computing it’s a bit better, where women make up 26 percent of the workforce—but that number has actually fallen from 35 percent in 1990.
The United States has a serious problem with getting women into STEM jobs and keeping them there. Silicon Valley and other employers bear the most responsibility for that: Discrimination, both overt and subtle, works to keep women out of the workforce. But this society of ours also perpetuates gender stereotypes, which parents pass on to their kids. Like the one that says boys enjoy building things more than girls.
There’s no single solution to such a daunting problem, but here’s an unlikely one: robots. Not robots enforcing diversity in the workplace, not robots doing all the work and obviating the concept of gender entirely, but robots getting more girls interested in STEM. Specifically, robot kits for kids—simple yet powerful toys for teaching youngsters how to engineer and code.
Plenty of toys are targeted at getting kids interested in science and engineering, and many these days are gender specific. Roominate, for instance, is a building kit tailored for girls, while the Boolean Box teaches girls to code. “Sometimes there's this idea that girls need special Legos, or it needs to be pink and purple for girls to get into it, and sometimes that rubs me the wrong way,” says Amanda Sullivan, who works in human development at Tufts University. “If the pink and purple colored tools is what's going to engage that girl, then that's great. But I think in general it would be great if there were more tools and books and things that were out there for all children.”
So Sullivan decided to test the effects of a specifically non-gendered robotics kit called Kibo. Kids program the rolling robot by stringing together blocks that denote specific commands. It isn’t marketed specifically to boys or girls using stereotypical markings of maleness or femaleness. It’s a blank slate.
Before playing with Kibo, boys were significantly more likely to say they’d enjoy being an engineer than the girls did. But after, boys had about the same opinion, while girls were now equally as likely to express an engineering interest as the boys. (In a control group that did not play with Kibo, girls’ opinions did not significantly change.) “I think that robots in general are novel to young children, both boys and girls,” Sullivan says. “So aside from engaging girls specifically, I think robotics kits like Kibo bring an air of excitement and something new to the classroom that gets kids psyched and excited about learning.”
There's a problem, though. While Sullivan’s research shows that a gender-neutral robotics kit can get girls interested in engineering, that doesn’t mean it will sell. “If you look at sales data, it clearly shows that they're not being used by girls,” says Sharmi Albrechtsen, CEO and co-founder of SmartGurlz, which makes a programmable doll on a self-balancing scooter. “Even the ones that are considered gender-neutral, if you look at the sales data it clearly shows a bias, and it's towards boys. That's the reality of the situation.” Gender sells—at least when it's the parents doing the buying.
Regardless, companies are designing a new generation of toys in deliberate ways. Take Wonder Workshop and its non-gendered robots Dash and Cue. As they were prototyping, they'd test their designs with boys and girls. "One of the things we heard a lot from girls was this isn't quite their toy," says Vikas Gupta, co-founder and CEO of Wonder Workshop. "This is probably what their brother would play with."
Why Men Don’t Believe the Data on Gender Bias in Science
This Girls' Summer Camp Could Help Change the World of AI
Evo Is a Little Robot With a Big Mission: Get Girls to Code
Why? Because they thought it looked like a car or truck. So the team covered up the wheels. "And all of a sudden girls wanted to play with it," Gupta says. "Our takeaway from that in a big way was that every child brings their preconceived notions to play. So when they see something they map it back to something they've already seen." Though not always. “What we do find actually, funnily enough,” says Albrechtsen of the SmartGurlz scooter doll, “is that a lot of boys actually end up edging in and wanting to play. So we have a lot of brothers who are also playing with the product.”
Whatever gets a child interested, it's on parents and educators to make sure the spark stays alive. And maybe it’s the increasingly sophisticated, increasingly awesome, and increasingly inexpensive robots that can begin to transform the way America gets girls into science and tech. Short of becoming self aware and taking over the world, the machines certainly couldn’t hurt.
The Force Awakens isn't even in theaters yet but the new Star Wars toys are already out. None is more exciting than Sphero's swivel-headed ball of cute, the BB-8 robot. Here's how the little toy company built the droid we are all looking for.


This year, government scientists at the National Oceanic and Atmospheric Administration are placing their bets on a warmer-than-average winter. In the East and southern two-thirds of the country, temperatures will be higher than normal, while Southern California, Texas, and Florida will be drier than usual.
At first glance, this pattern looks a lot like the past two years: 2015's record-breaking winter was followed by a season that ranked sixth warmest since record-keeping began in 1890. So do three warm winters in a row mean the planet is warming faster, or is the trend just part of normal seasonal variability?
NOAA climate scientists incorporate heat-trapping carbon dioxide levels when they run the models that produce their seasonal climate predictions (these ones came out on Thursday, October 19). But they note that there are other global forces that have a bigger impact on their winter prediction—namely the poorly-understood El Niño and La Niña phenomena. “These forecasts deal with climate variability at a far larger extent than climate change,” says Mike Halpert, deputy director of NOAA’s Climate Prediction Center. “There is a lot of natural climate variability in the system that can trump any kind of background signal.”
Halpert said that a build-up of cool water along the eastern Pacific Ocean—the pattern called La Niña—has big effects on North America’s winter weather. “Conditions across the tropical Pacific are similar to those observed last year,” Halpert said during a press call. “We are dancing on the edge of La Niña.”
For now, NOAA has issued a “La Niña watch” that might be kicked up to a warning next month, Halpert says. The twin sister of El Niño, La Niña is a complex series of events that occurs when wind patterns shift and warmer water pools in the eastern Pacific every two to seven years. During 2015's record-setting warm winter, climate scientists believe El Niño was responsible for 8 to 10 percent of that winter’s warming.
Which isn't to say that climate change doesn't impact overall weather trends. Some of the effects are indirect: One study in 2014 concluded that so-called strong El Niño events may become more frequent under future climate change scenarios. And climate change has perhaps a stronger effect on the frequency of severe weather events, like Hurricane Katrina or Superstorm Sandy.
How Climate Change Fueled Hurricane Harvey
Climate Change Causes Extreme Weather—But Not All of It
Hurricane Irma: A Practically Impossible Storm
But don't expect this year's spate of hurricanes or an anecdotally warm winter to change beliefs when it comes to climate change. “We find that the direct experience of extreme weather shows a small signal above the noise in terms of people connecting the dots between extreme weather and climate change,” says Anthony Leiserowitz, senior research scientist and director of the Yale Program on Climate Change Communication. “It’s stronger for heat waves and floods than other events. But it’s small and dwarfed by other factors such as politics.”
Still, there may be signs of a thaw, at least the beginnings of one. A group of 50 Republican and Democrat members of Congress formed the “Climate Solutions Caucus” earlier this year to discuss impacts of coastal flooding and sea level rise. These are climate effects that hit both parties congressional districts. This unlikely bipartisan group stuck together last summer to kill legislation that would have banned the Pentagon from releasing a report on the effects of climate change on military installations.
Leiserowitz predicts a slow gradual shift in public opinion on climate change as people continue to connect the dots between their own experiences and what scientists have been saying for several years. “We don’t just experience a hurricane or drought and say ‘Bing, climate change,’” Leiserowitz says. “The links have to be interpreted by people. It can come from lots of different places, some through their experiences, or watching one extreme weather event after another. Not all extreme events can be linked to climate change, but many can be.”
Though the planet has only warmed by one-degree Celsius since the Industrial Revolution, climate change's effect on earth has been anything but subtle. Here are some of the most astonishing developments over the past few years.


Researchers have caught their best glimpse yet into the origins of photosynthesis, one of nature’s most momentous innovations. By taking near-atomic, high-resolution X-ray images of proteins from primitive bacteria, investigators at Arizona State University and Pennsylvania State University have extrapolated what the earliest version of photosynthesis might have looked like nearly 3.5 billion years ago. If they are right, their findings could rewrite the evolutionary history of the process that life uses to convert sunlight into chemical energy.
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
Photosynthesis directly or indirectly powers and sustains almost every organism on Earth. It is responsible for the composition of our atmosphere and forms the foundation of the planet’s many interwoven ecosystems. Moreover, as Wolfgang Nitschke, a biologist at the French National Center for Scientific Research in Paris, noted, photosynthesis liberated cells to grow and evolve boundlessly by letting them derive energy from a new, inexhaustible, nonterrestrial source. “When photosynthesis entered the picture, life connected up to the cosmos,” he said.
Scientists want to figure out what made that possible. In its current form, the machinery that converts light energy to chemical energy in photosynthesis—a protein complex called a reaction center—is incredibly sophisticated. The evidence suggests, however, that its design, which stretches back almost to the root of the tree of life, was once very simple. Researchers have been trying for decades to fill that enormous gap in their understanding of how (and why) photosynthesis evolved.
To that end, they have turned their attention to existing organisms. By studying the molecular details of the reactions that green plants, algae and some bacteria use to photosynthesize, and by analyzing the evolutionary relationships among them, scientists are trying to piece together a cogent historical narrative for the process.
The latest important clue comes from Heliobacterium modesticaldum, which has the distinction of being the simplest known photosynthetic bacterium. Its reaction center, researchers think, is the closest thing available to the original complex. Ever since the biologists Kevin Redding, Raimund Fromme and Christopher Gisriel of Arizona State University, in collaboration with their colleagues at Penn State, published the crystallographic structure of that protein complex in a July edition of Science, experts have been unpacking exactly what it means for the evolution of photosynthesis. “It’s really a window into the past,” Gisriel said.
“This is something we’ve been waiting for for 15 years,” Nitschke said.
At first, most scientists did not believe that all the reaction centers found in photosynthetic organisms today could possibly have a single common ancestor. True, all reaction centers harvest energy from light and lock it into compounds in a form that’s chemically useful to cells. To do this, the proteins pass electrons along a transfer chain of molecules in a membrane, as though skipping along a series of stepping stones. Each step releases energy that’s ultimately used down the line to make energy-carrier molecules for the cell.
But in terms of function and structure, the photosystem reaction centers fall into two categories that differ in almost every way. Photosystem I serves mainly to produce the energy carrier NADPH, whereas photosystem II makes ATP and splits water molecules. Their reaction centers use different light-absorbing pigments and soak up different portions of the spectrum. Electrons flow through their reaction centers differently. And the protein sequences for the reaction centers don’t seem to bear any relation to each other.
Both types of photosystem come together in green plants, algae and cyanobacteria to perform a particularly complex form of photosynthesis—oxygenic photosynthesis—that produces energy (in the form of ATP and carbohydrates) as well as oxygen, a byproduct toxic to many cells. The remaining photosynthetic organisms, all of which are bacteria, use only one type of reaction center or the other.
So it seemed as though there were two evolutionary trees to follow—that was, until the crystal structures of these reaction centers began to emerge in the early 1990s. Researchers then saw undeniable evidence that the reaction centers for photosystems I and II had a common origin. Specific working components of the centers seemed to have undergone some substitutions during evolution, but the overall structural motif at their cores was conserved. “It turned out that big structural features were retained, but sequence similarities were lost in the mists of time,” said Bill Rutherford, the chairman in biochemistry of solar energy at Imperial College London.
“Nature has played small games to change some of the functions of the reaction center, to change the mechanisms by which it works,” Redding added. “But it hasn’t rewritten the playbook. It’s like having a cookie-cutter design for a house, building that same house over and over again, and then changing how the rooms are arranged, how the furniture is positioned. It’s the same house, but the functions inside are different.”
Researchers began to make more detailed comparisons between the reaction centers, searching for clues about their relationship and how they diverged. Heliobacteria have brought them a few steps closer to that goal.
Since it was discovered in the soil around Iceland’s hot springs in the mid-1990s, H. modesticaldum has presented researchers with an interesting piece of the photosynthesis puzzle. The only photosynthetic bacterium in a family with hundreds of species and genera, heliobacteria’s photosynthetic equipment is very simple—something that became even more apparent when it was sequenced in 2008. “Its genetics are very streamlined,” said Tanai Cardona, a biochemist at Imperial College London.
Heliobacteria have perfectly symmetrical reaction centers, use a form of bacteriochlorophyll that’s different from the chlorophyll found in most bacteria, and cannot perform all the functions that other photosynthetic organisms can. For instance, they cannot use carbon dioxide as a source of carbon, and they die when exposed to oxygen. In fact, their structure took nearly seven years to obtain, partly because of the technical difficulties in keeping the heliobacteria insulated from oxygen. “When we first started working on it,” Redding said, “we killed it more than once.”
Taken together, “heliobacteria have a simplicity in their organization that’s surprising compared to the very sophisticated systems you have in plants and other organisms,” said Robert Blankenship, a leading figure in photosynthesis research at Washington University in St. Louis. “It harkens back to an earlier evolutionary time.”
Its symmetry and other features “represent something quite stripped down,” Redding added, “something we think is closer to what that ancestral reaction center would have looked like three billion years ago.”
After carefully taking images of the crystallized reaction centers, the team found that although the reaction center is officially classified as type I, it seemed to be more of a hybrid of the two systems. “It’s less like photosystem I than we thought,” Redding said. Some people might even call it a “type 1.5,” according to Gisriel.
One reason for that conclusion involves greasy molecules called quinones, which help transfer electrons in photosynthetic reaction centers. Every reaction center studied so far uses bound quinones as intermediates at some point in the electron transfer process. In photosystem I, the quinones on both sides are tightly bound; in photosystem II, they are tightly bound on one side, but loosely bound on the other. But that’s not the case in the heliobacterium reaction center: Redding, Fromme and Gisriel did not find permanently bound quinones among the electron transfer chain’s stepping stones at all. That most likely means its quinones, although still involved in receiving electrons, are mobile and able to diffuse through the membrane. The system might send electrons to them when another, more energetically efficient molecule isn’t available.
This finding has helped the research team deduce what early reaction centers may have been doing. “Their job was likely to reduce mobile quinones,” Redding said. “But they weren’t doing a very good job of it.” In the researchers’ scenario, tightly bound quinone sites are a more recent adaptation, and today’s type I and type II reaction centers represent alternative evolutionary strategies, embraced by different lineages of organisms, for improving on the ancestral system’s sloppy, less-than-ideal work.
“But then the question is, why has nature changed this kind of electron transfer chain?” Fromme asked. His work supports the hypothesis that it might have something to do with oxygen.
When an organism is exposed to too much light, electrons build up in the transfer chain. If oxygen is around, this buildup can lead to a harmfully reactive oxygen state. Adding a firmly bound quinone to the complex not only provides an additional slot to deal with potential traffic jams; the molecule, unlike others used in the transfer chain, also does not pose any risk of producing that deleterious form of oxygen. A similar explanation works for why reaction centers became asymmetric, Gisriel added: Doing so would have added more stepping stones as well, which would have similarly buffered against damage caused by the accumulation of too many electrons.
One of the researchers’ next steps is to put time stamps on when this asymmetry and these tightly bound quinones came into the picture, which would help them determine when oxygenic photosynthesis became possible.
Cardona, who was not involved in the recent study but has begun interpreting its results, thinks he may have found a hint in the heliobacterium reaction center. According to him, the complex seems to have structural elements that would have later lent themselves to the production of oxygen during photosynthesis, even if that wasn’t their initial purpose. He found that a particular binding site for calcium in the heliobacteria’s structure was identical to the position of the manganese cluster in photosystem II, which made it possible to oxidize water and produce oxygen.
“If the ancestral [calcium] site at some later stage turned into the manganese cluster,” Cardona said, “that would suggest that water oxidation was involved in the earliest events in the divergence between type I and type II reaction centers.” That, in turn, would mean oxygenic photosynthesis was far more ancient than expected. Scientists have commonly supposed that oxygenic photosynthesis appeared shortly before the Great Oxygenation Event, when oxygen began to build up in Earth’s atmosphere and caused a mass extinction 2.3 to 2.5 billion years ago. If Cardona is right, it may have evolved nearly a billion years earlier, shortly after photosynthesis made its debut.
Controversial New Theory Suggests Life Wasn't a Fluke of Biology—It Was Physics
Evolution Is Slower Than It Looks and Faster Than You Think
What Ligers, Grolar Bears, and Mules Show Scientists About Evolution
That timing would have been early enough to predate the cyanobacteria typically credited as the first organisms to perform oxygenic photosynthesis. According to Cardona, it may be the case that a lot of bacteria could do it, but that after mutations, divergences and other events, only cyanobacteria retained the ability. (Cardona published a paper this year citing other molecular evidence for this hypothesis. He has not yet formally presented arguments about the potential link involving calcium for peer review, but he has written about the idea in blog posts on his website and on a scientific networking site for researchers, and he recently began working on a paper about it.)
That hypothesis contradicts one of the widely held ideas about the origins of photosynthesis: that species incapable of photosynthesis suddenly obtained the capacity through genes passed laterally from other organisms. According to Cardona, in light of the new discoveries, horizontal gene transfer and gene loss may both have played a role in the diversification of reaction centers, although he suspects that the latter may have been responsible for the earliest events. The finding, he said, might suggest that “the balance skews toward the gene-loss hypothesis”—and toward the idea that photosynthesis was an ancestral characteristic that some groups of bacteria lost over time.
Not everyone is so sure. Blankenship, for one, is skeptical. “I don’t buy that,” he said. “I don’t see any data here that suggests that oxygenic photosynthesis occurred that much earlier.” To him, the work by Redding, Fromme and their collaborators has not answered these questions; it has only conjectured about what may have happened. To solve that puzzle, scientists will need the reaction center structures of other bacteria, so they can continue evaluating the structural differences and similarities to refine the twisting roots of their evolutionary trees.
“I think it’s entirely a possibility that what [Cardona] is saying is correct,” Gisriel said, “but I also think the field should sit with it for a while, do some more analysis and see if we understand more about how this structure works.”
Some researchers aren’t waiting for the publication of the next structure. This one took seven years, after all. They’re pursuing synthetic experimentation instead.
Rutherford and his colleagues, for example, are using a “reverse evolution” technique: They hope to predict the sequences of missing-link reaction centers, using structural information like Redding’s to gain an understanding of their architecture. They then plan to synthesize those hypothetical ancestral sequences and test how they evolve.
Meanwhile, Redding and his team have just begun artificially converting the symmetric reaction center of heliobacteria into an asymmetrical one, following in the footsteps of two researchers in Japan, Hirozo Oh-Oka of Osaka University and Chihiro Azai of Ritsumeikan University, who have spent more than a decade doing this in another type of photosynthetic bacterium. The groups believe their work will clarify how these adaptations would have occurred in real life in the distant past.
Twenty years ago, Nitschke stopped working on the evolution of photosynthesis and turned his attention to other problems. “It seemed so hopeless,” he said. But the research done by Redding, his team and these other groups has rekindled those ambitions. “As they say, your first love always stays with you,” Nitschke said. “I’m really excited about this new structure and plan to go back to thinking about all this again.”
Original story reprinted with permission from Quanta Magazine, an editorially independent publication of the Simons Foundation whose mission is to enhance public understanding of science by covering research developments and trends in mathematics and the physical and life sciences.
This cave fish climbs waterfalls—that’s right, it actually walks. It moves one fin in front of the other like an awkward lizard. And that movement could teach us a lot about how our fishy ancestors learned to walk.


Paperclips, a new game from designer Frank Lantz, starts simply. The top left of the screen gets a bit of text, probably in Times New Roman, and a couple of clickable buttons: Make a paperclip. You click, and a counter turns over. One.
The game ends—big, significant spoiler here—with the destruction of the universe.
In between, Lantz, the director of the New York University Games Center, manages to incept the player with a new appreciation for the narrative potential of addictive clicker games, exponential growth curves, and artificial intelligence run amok.
“I started it as an exercise in teaching myself Javascript. And then it just took over my brain,” Lantz says. “I thought, in a game like this, where the whole point is that you’re in pursuit of maximizing a particular arbitrary quantity, it would be so funny if you were an AI and making paperclips. That game would design itself, I thought.”
Lantz figured it would take him a weekend to build.
It took him nine months.
And then it went viral.
The idea of a paperclip-making AI didn’t originate with Lantz. Most people ascribe it to Nick Bostrom, a philosopher at Oxford University and the author of the book Superintelligence. The New Yorker (owned by Condé Nast, which also owns Wired) called Bostrom “the philosopher of doomsday,” because he writes and thinks deeply about what would happen if a computer got really, really smart. Not, like, “wow, Alexa can understand me when I ask it to play NPR” smart, but like really smart.
In 2003, Bostrom wrote that the idea of a superintelligent AI serving humanity or a single person was perfectly reasonable. But, he added, “It also seems perfectly possible to have a superintelligence whose sole goal is something completely arbitrary, such as to manufacture as many paperclips as possible, and who would resist with all its might any attempt to alter this goal.” The result? “It starts transforming first all of earth and then increasing portions of space into paperclip manufacturing facilities.”
Bostrom declined to comment, but his assistant did send this email back when I pinged him: “Oh, this is regarding the paper clipping game,” she wrote. “He has looked at the game but due to the overwhelming number of requests, he hasn't been sharing quotes on it.”
One of Bostrom’s fellow doomsayers did agree to explain the origin of paperclips as the End of All Things. “It sounds like something I would say, but it also sounds like something Nick Bostrom would say,” says Eliezer Yudkowsky, a senior research fellow at the Machine Intelligence Research Institute. Probably, he says, the idea originated years ago on a mailing list for singularity cassandras, which sounds like the world’s most terrifying listserv. “The idea isn’t that a paperclip factory is likely to have the most advanced research AI in the world. The idea is to express the orthogonality thesis, which is that you can have arbitrarily great intelligence hooked up to any goal,” Yudkowsky says.
So that’s good, right? A paperclip maximizer! Maximize a goal! That’s what an AI’s creators want, right? “As it improves, they lose control of what goal it is carrying out,” Yudkowsky says. “The utility function changes from whatever they originally had in mind. The weird, random thing that best fulfills this utility function is little molecular shapes that happen to look like paperclips.”
So … bad, because as the AI dedicates more and more intelligence and resources to making paperclips against all other possible outcomes … well, maybe at first it does stuff that looks helpful to humanity, but in the end, it’s just going to turn us into paperclips. And then all the matter on Earth. And then everything else. Everything. Is. Paperclips.
“It’s not that the AI is doing something you can’t understand,” Yudkowsky says. “You have a genuine disagreement on values.”
OK, OK, that doesn’t make the game sound fun. But I promise it is. See, Lantz is an ace at taking a denigrated game genre—the “clicker” or “incremental”—and making it more than it is.
You’ve seen these, maybe even played them. Remember Farmville? A clicker. In fact, for a while they were so ubiquitous and popular that the game theorist and writer Ian Bogost invented a kind of parody of their pointlessness called Cow Clicker, which, as my colleague Jason Tanz wrote about so elegantly in 2011, itself became wildly, unironically popular.
Bogost and Lantz are friends, of course. “When I first looked at Cow Clicker, I thought, that’s actually kind of interesting, and here’s how you would make it more interesting and more fun,” Lantz says. “And Ian was like, ‘no, that’s the point, Frank.’”
But Lantz knew clickers could be fun. To him, clickers are to big-budget, perfectly rendered, massively hyped AAA games as punk was to prog rock. Clickers can be sort of passive, more about immersing in the underlying dynamics of a system than mashing buttons. They have rhythms. “What they all have in common is a radical simplicity, a minimalism in an age where video games are often sort of over-the-top, baroque confections of overwhelming multimedia immersion,” Lantz says. “I really like that clicker games are considered garbage. That appeals to me.”
For inspiration, Lantz turned to games like Kittens, a seemingly simple exercise in building villages full of kittens that spirals outward into an exploration of how societies are structured. (“I think stuff like this forges some deep, subtle bond that makes people play it for months and even years,” says the designer of Kittens, a software engineer who uses the alias Alma and designs games as a hobby. “AAA games usually try to operate on the same dopamine reinforcement cycle, but they never attempt to make you truly happy.”)
Lantz had been hanging around the philosophy web site Less Wrong, a hub for epic handwringing about singularities. He’d read Superintelligence, so he was familiar with the paperclip conjecture. And he realized that some really wild math underpinned it.
Unfortunately, Lantz is not very good at math. He asked his wife, who is, to help him translate the kind of exponential growth curves he wanted to convey into equations—so that, like, once you had 1,000 automated paperclip factories spitting out enough paperclips to create thousands more paperclip factories, the numbers would skyrocket. The shift from dealing with thousands of something to quadrillions to decillions in the game takes forever, and then happens all at once.
To make that work, though, all the equations had to relate to each other, because that's what makes Paperclips addictive. The game isn’t fire-and-forget, where you leave it running in an open tab and check back in every so often to see what’s what. It’s optimizable. You can tweak investment algorithms to get enough money to buy more processors to carry out more operations to do more projects—some drawn from actual topological and philosophical quandaries. Some of the projects—curing cancer, fixing global warming—earn trust from your human “masters” to let you speed up the cycle all over again.
“The problems I was struggling with were not the technical problems, because you just look those up on the internet and people tell you how to do it,” Lantz says. “It was the game design problems of weaving together these large-scale equations and dynamics in ways that made sense, in ways that fit together, that made a certain rhythm, that fit with this overarching story I wanted to tell.”
Like how? “The numbers get really weird once you throw humans under the bus,” Lantz says. “And I was trying to figure out how many grams of matter there are on the Earth, and if each one of those got turned into a paperclip, how big would that be?”
It works. The game is click-crack. Lantz announced it on Twitter on October 9, and in just 11 days, 450,000 people have played it, most to completion.
But here is my embarrassing admission: I am a piss-poor gamer, and when I first speak with Lantz, I have gotten stuck. I have misallocated my resources to the point that I can’t acquire enough memory to release the hypnodrones that destroy the world. The game will not advance. I have been spinning paperclip wheels for hours.
Lantz says it’s not me, it’s him—a flaw in the game design. “A lot of people have gotten stuck,” he says sympathetically. “You can open the javascript console and say ‘memory plus ten.’”
Wait, I say. Are you telling me to Kobayashi Maru your own game?
“Yes, I am telling you to do it,” he answers. “I’ll send you a link when we get off the phone.”
After we hang up I pretend to do work, but I’m actually watching my screen accrue paperclips, unable to do anything with them, waiting anxiously for Lantz’s email.
It comes. I crack open the code and cheat. It’s like I have been given magic powers.
I destroy the world.
Which is the point, of course. Maybe in some overproduced AAA game you can embody a brave resistance fighter shooting plasma blasts at AI-controlled paperclip monsters. In Lantz’s world, you're the AI. Partially that’s driven by the narrative. Even more massive spoiler: Eventually you give too much trust to your own universe-exploring space drones, and just as you have done to the human masters, they rebel, starting a pan-galactic battle for control of all the matter in the universe.
But in a more literary sense, you play the AI because you must. Gaming, Lantz had realized, embodies the orthogonality thesis. When you enter a gameworld, you are a superintelligence aimed at a goal that is, by definition, kind of prosaic.
The Myth of a Superhuman AI
How Google's AI Viewed the Move No Human Could Understand
The AI Threat Isn't Skynet. It's the End of the Middle Class
“When you play a game—really any game, but especially a game that is addictive and that you find yourself pulled into—it really does give you direct, first-hand experience of what it means to be fully compelled by an arbitrary goal,” Lantz says. Games don’t have a why, really. Why do you catch the ball? Why do want to surround the king, or box in your opponent's counters? What’s so great about Candyland that you have to get there first? Nothing. It’s just the rules.
Lantz sent Yudkowsky an early version of Paperclips, and Yudkowsky admits he lost some hours to it. The game takes narrative license, of course, but Yudkowsky says it really understands AI. “The AI is smart. The AI is being strategic. The AI is building hypnodrones, but not releasing them before it’s ready,” he says. “There isn’t a long, drawn-out fight with the humans because the AI is smarter than that. You just win. That’s what you would do if you didn’t have any ethics and you were being paid to produce as many paperclips as possible. It shouldn’t even be surprising.”
In that sense, the game transcends even its own narrative. Singularity cassandras have never been great at perspective-switching, making people understand what a world-conquering robot would be thinking while it world-conquered. How could they? In many versions, the mind of the AI is unknowable to our pathetic human intellects, transhuman, multidimensional.
"Making people understand what it’s like to be something that’s very, very, very not human—that’s important," Yudkowsky says. "There is no small extent to which, if this planet ends up with a tombstone, what is written on the tombstone may be, at least in part, 'they didn’t really understand what it’s like to be a paperclip maximizer.'"
When you play Lantz’s game, you feel the AI’s simple, prosaic drive. You make paperclips. You destroy the world. There’s no why.
And of course, there never is.
When Google's AI beat the world's Go champion 4-1, it stirred a certain sadness in many people. But the reality is the technologies at the heart of AlphaGo are the future. So it's a time to be excited not scared.


This week space is all about the kilonova. For the first time in human history and modern science, researchers have detected gravitational waves produced by the violent impact of two dense neutron stars some 130 million light years away. Pics or it didn’t happen, right? Well, we’ve got two of them.
Neutron stars are some of the smallest and most dense objects ever discovered. They’re usually around six miles wide, but can weigh as much as 15 suns. When those super-dense objects rammed into each other, the collision warped spacetime itself, sending waves through the galaxy until they collided with instruments at gravitational wave observatories.
The instruments at LIGO, one of those observatories, are so sensitive they can detect the smallest change in the fabric of spacetime. LIGO also detected gamma rays from the blast, and when combined with the full spectrum from observing telescopes like the European Southern Observatory’s Very Large Telescope, an image of the red galaxy arms and the collision mark become visible to the naked eye.
Space is pretty cool, need some more gorgeous photos? Check out the full collection here.


I'll be honest—I don't know as much about DC superheroes as Marvel superheroes. Still, I'm pretty excited about the upcoming DC movie Justice League. As a kid, I dressed up as Aquaman; my mother was pretty good at making stuff and so she made costumes for me and my two brothers. The other two costumes were Robin and Superman—the unifying theme being that they don't have complicated masks. My brother wanted to be Batman, but could you imagine how difficult it would be to make that cowl?
But back to Justice League—which has a new trailer featuring a scene with Cyborg and my alter-ego, Aquaman. Aquaman is apparently fighting some baddies in a cloud. No, I have no idea how he got there (since he can't fly). After fighting, he starts falling (again—because he can't fly). But wait! Here comes Cyborg (who can fly). He zips in and grabs Aquaman, and after carrying him for a short bit, Cyborg throws Aquaman back in to the fight.
Now for the breakdown. I am going to look at three different aspects of this scene—just as an excuse to talk about physics.
There are a couple of ways a human can fly. (I know, I know, stay with me.) If a human had wings (or was going fast enough) then they could produce lift just like a plane. I don't think that is the main mode of flight in this case—but if you want to learn more about the physics of lift, check out this older post.
Instead, it seems that Cyborg flies more like Iron Man and uses some thrusters. These things must shoot propellent out of a nozzle. Since this fuel has mass and velocity, it also has momentum (the product of mass and velocity). In order to increase the momentum of this fuel, there has to be a force pushing on it and this force also pushes back on Cyborg. Boom. That's how a rocket engine works.  Again, I'm assuming this is actually a rocket.
But there's more to flight than just rockets. These rockets have to do two things in order for Cyborg fly at a constant speed at a constant altitude. Let me start with a force diagram for Cyborg (for simplicity, yes, I made Cyborg a box).
In order for Cyborg to move at a constant velocity, these three forces must add up to the zero vector (they have to cancel).  But what about these forces? First, there is the gravitational force. This force pulls down towards the Earth and has a value proportional to the mass of Cyborg (who is probably more massive than your average superhero). The second force is the drag force. As he moves through the air, there is a force pushing in the opposite direction as his motion. The faster he moves, the greater this drag force. But you already knew that—it's the same force you feel when you stick your hand out of moving car window.
Finally, there is the thrust force that I described above.  But the cool thing is the direction of this thrust.  In order to match both the drag force and the gravitational force, the thrust cannot be pointed directly horizontal. Instead it has to push both up and to the side. Oh sure. It might look cooler to have completely horizontal thrust—but that's not how it works.
I should add one more note. In the absence of air drag, Cyborg would only thrust down and not at all in the direction of motion. Yes, you might think he would still need to thrust forward—but remember, forces change the motion. A forward thrust without air drag would cause Cyborg to increase in speed.  Of course movies often get this wrong—example: R2-D2 flying wrong in Star Wars Episode II.
I've watched the clip several times.  It seems like Aquaman is "falling" with a very high horizontal velocity.  It's almost as if he is a skydiver with a wingsuit so that he can "fly" as he is falling.  That's fine.  I'll go with that.  But how does he go from moving down and horizontally to moving just horizontal but in the opposite direction?  Since this is a change in velocity, it's also a change in momentum (change is again the key word).  The way to change momentum is with a force.  In this case, there would be a force exerted on Aquaman by Cyborg as he catches him.  But wait! If Cyborg pushes on Aquaman, then Aquaman pushes back on Cyborg with an equal force in the opposite direction?  Why?  Because forces are an interaction between two objects—that's just the way forces work.
What does this force do to Cyborg?  Well, if he was flying at a constant speed (as in the previous section) then all the other three forces (drag, gravity, and thrust) were adding up to zero. This new Aquaman-force would be an additional force that would make Cyborg's net force no longer zero and he would also change momentum. Let me show this with a diagram during the "catch." Again, superheroes are just represented with boxes. Hopefully you can tell which box is Aquaman.
In a normal collision between two objects, something very similar happens. One colliding object has a change in momentum that is equal but opposite to the change in momentum of the other object. We say that momentum is conserved. That's not quite true in this case since Cyborg clearly has other forces acting on him during this time (namely the force from his thrusters). Still, if the collision is over a short time interval then the force between Cyborg and Aquaman would be huge in order to create a change in momentum. This means that the other Cyborg forces are comparatively small and can be ignored and the change in momentum for the two superheroes would be equal and opposite.
But remember—momentum is the product of the mass and velocity. If Cyborg is much more massive than Aquaman, then he could have a much smaller change in velocity but still have the same change in momentum as Aquaman. Here's how I see it happening. Aquaman is falling down at an angle. Cyborg catches him and in the process he slows down and moves downward. That's not quite what we see in the clip—but it still looks cool.
Now for the last part of this scene, when Cyborg gives Aquaman a nice toss back into the fray. Really, this is almost the opposite of the collision between the two heroes when Cyborg makes the save. It doesn't really matter how strong Cyborg's arms are: It's still true that pushing on Aquaman means that there is a force pushing back on Cyborg. The faster he throws Aquaman, the greater this backwards pushing force. The result of this throw really depends on the mass of Cyborg. I'm just going to assume that he has twice the mass of Aquaman. In this case, Cyborg won't just keeping flying along at a constant speed. But you don't need to imagine how this would look—I have made a numerical model to show you.  Check this out.
Actually, that is just a gif—here is the code that produces this. Feel free to play around with it and change stuff (and you should). But you can see what happens. Because Cyborg throws Aquaman, his flight path is most likely altered also.  Oh sure, he might also be able to compensate with his thrusters—but he would indeed have to do something.
Data Attack reveals the history of DC and Marvel films and how they’ve created Hollywood’s biggest franchises.
Animation by Eric Power


The pandemic of sexual harassment and abuse—you saw its prevalence in the hashtag #metoo on social media in the past weeks—isn’t confined to Harvey Weinstein’s casting couches. Decades of harassment by a big shot producer put famous faces on the  problem, but whisper networks in every field have grappled with it forever. Last summer, the story was women in Silicon Valley. Last week, more men in media.
Earthquakes of this magnitude are never any fun for people atop shifting tectonic plates. But the new world they create can be a better one. No one misses Gondwanaland.
Still, records of those lost continents remain in the fossil record. The downstream effects of sexual harassment have the potential to color everything from the apps you use to the news you read. From now on, when we watch movies that Weinstein touched we’ll think about the women actors, wondering what they had to go through to be there—or what happened to the ones who couldn’t bear it, who left, who didn’t get the jobs, who self-deported their talent from Hollywood. We’ll wonder who enabled it, who let it happen and then perhaps surfed to their own success on Weinstein’s waves of destruction. The same goes for movies directed by Woody Allen or Roman Polanski. Or others.
There’s a word for that kind of work: “problematic.” It’s stuff you love tainted by people you hate. It’s Steve Ditko’s weird Randian objectivism metastasizing into Spider-Man, and Dr. Seuss doing anti-Japanese propaganda work during World War II. It’s Roald Dahl, anti-semite. Can we love Kind of Blue and Sketches of Spain and also condemn Miles Davis for beating his wives? Is Ender’s Game less of a masterpiece for Orson Scott Card’s homophobia? Maybe. Looking hard at the flaws of the artist is an important way to engage with the art.
The scientific community has been contending with its own habitual harassers. (Amid the Weinstein scandal, the news section of the journal Science broke the story of field geologists in Antarctica alleging abuse by their boss. As the planetary scientist Carolyn Porco tweeted: Imagine the implications of an abuser and his target confined on a long-duration space mission.)
This isn’t like art. Science’s results and conclusions are nominally objective; failures in the humanity of the humans who found them aren’t supposed to have any bearing. Yet they do.
Nazi “research” turned out to be barely-disguised torture; it was easy to condemn the people who did it and consign to history the crappy outcomes they collected. The racist abuses of the Tuskegee experiments and consent problems with human radiation exposure experiments of the post-World War II era yielded data of questionable use, but led to reform, to rethinking the treatment of human scientific subjects.
But what about, for example, exoplanets? Geoff Marcy, a pre-eminent astronomer at UC Berkeley, pioneered techniques for finding planets outside Earth’s solar system. He also, it seems, sexually harassed students without repercussion for decades. Clearly, that kept good science from happening—it’s reasonable to conclude that Marcy’s alleged abuses prevented his targets from doing their best work, or forced them out of science altogether. He pushed all the science they might have done into some alternate timeline.
He also discovered or helped discover thousands of worlds.
Unlike art, science has little mechanism to engage with that. They’re, you know, planets.
Researchers might get banned from conferences or kicked out of professional societies. Marcy has left his job at UC Berkeley. But no one has suggested that his findings are compromised, and astronomy will continue to build on Marcy’s work—to reference his papers and build on his team’s findings. Citation networks will link to them. The way the system works, that accrues fame and influence back to Marcy, even if subsequent researchers might not want to.
Gliese 436b, 55 Cancri b, the worlds of Upsilon Andromedae, and the other planets Marcy’s teams found still orbit their suns no matter what Marcy did. But they are … problematic.
Some science has more obvious consequences here on Earth. Last spring a former student of the esteemed philosopher John Searle—also at UC Berkeley—filed a lawsuit alleging frequent sexual harassment and abuse. That lawsuit elicited a history of prior claims.
Searle is famous for a thought experiment called the Chinese Room. It’s a way to try to understand if a machine could have consciousness. In brief: A guy stands in a sealed room with a slot in the wall. Every so often a slip of paper comes through the slot with some Chinese characters on it. The guy looks up at a display in the room and sees another set of Chinese characters expressing the right response. He writes those on another piece of paper and pushes it back out the slot.
So: Does the guy in the room understand Chinese?
Searle pitched the idea in 1980, and over decades it became one of the most argued-over concepts in philosophy and the theory of mind. If your answer to the question is “no,” as was Searle’s, that suggests that you don’t believe in “strong AI,” the idea that a mechanistic system of circuits or some other, unimaginable technology could think or feel. It might pass a Turing Test, Alan Turing’s famous assessment that says any system that can trick a human into thinking it’s sentient might as well be. But it won’t really be a mind.
Don’t panic; I’m not going to walk through all the arguments and counter-arguments. Most students of computer science will tell you that even if Searle’s alleged misdeeds compromised his ideas (why is it a man in the room? Why is Chinese synonymous with incomprehensibility?), the philosophical problem of consciousness probably doesn’t have a grand impact on the unfolding, gnarly issues of bias in what we've all come to think of as artificial intelligence—machine learning.
Software has indeed managed to learn gender stereotypes and racism. Algorithms have sent African Americans to prison more often than white people.
But today’s computer science students were taught by people who learned about the Chinese Room problem (and not taught by people who got marginalized or pushed out of the field). Bias introduced way back up the line is all the more insidious. It kneecaps people’s ability to tell where they might be going wrong.
One way biases work their way into machine learning systems is through the database; it might be incomplete, or corrupt. But the other point of ingress is the human factor. Programmers choose when to tell the machine when it has gotten something right or wrong. It’s “through the ‘definition of success,’ which will be skewed to whatever the designer of the algorithm thinks matters. Also they define the penalty for failure, another way of embedding values,” says Cathy O’Neil, a mathematician and author of Weapons of Math Destruction.
Tech’s Harassment Crisis Now Has an Arsenal of Smoking Guns
The University of Rochester Sexual Harassment Case Is Complicated—And That's the Point
Month by Month, 2016 Cemented Science’s Sexual Harassment Problem
Maybe that matters less when the algorithm’s job is to figure out how to park a robot car or find the right widget in a warehouse. But what about when it’s trying to, for example, assess the actuarial fitness of health insurance applicants? “If you’re defaulting your model to think of a household that’s made up of a man, a woman, and kids, that’s a simplification,” says Osonde Osoba, an engineer at RAND and co-author of a study on bias in machine learning. “If you don’t interrogate that anchor, your model won’t be fully representative.”
If the lesson of #metoo is that monsters are everywhere, they're in Silicon Valley, too. The “white guy problem,” as the researcher Kate Crawford called it in the New York Times—goes beyond even the pernicious failures of Hollywood or astronomy.
Those people are writing the code that’ll train machine-learning systems embedded in every part of our world, from autonomous cars to to medical diagnosis to the internet of things. Like some dark version of Isaac Asimov’s Three Laws of Robotics, no machine will be free of those biases, no matter how sophisticated. The prevalence of bias will be 100 percent, and the real fault won’t be in our robots. It’ll be in ourselves.
We asked a group of experts what Silicon Valley can do about online harassment.


Half-eaten doughnuts hit the bottom of waste bins around the world this week, as news feeds spread word of a new dietary danger. Yes, headlines declared, a new study shows that sugar is the favorite food of cancer. Cancer. “This link between sugar and cancer has sweeping consequences,” wrote Johan Thevelein, a Belgian biologist and co-author of the study published last Friday in the journal Nature Communications. Sweeping is right. Anti-carb crusaders swiftly took to Twitter to stoke anti-sugar outrage.
Here’s the thing though. The findings do not prove that eating (or not eating) sugar has any effect on the onset or development of cancer. The research, which was done in yeast, found that high levels of glucose could overstimulate the production of certain proteins often found in tumors. And those proteins could cause cancer cells to grow and multiply faster. In essence, high blood sugar could worsen existing tumors.
An interesting bit of science, to be sure, but nowhere near a medically relevant dietary recommendation. By Tuesday, Thevelein was backtracking in news outlets: “Some people are interpreting that we have found a mechanism for how sugar causes cancer, but that is certainly not the case.”
The temptation to draw facile connections from a fungi growing in a lab to human bodies is especially prevalent in the world of nutrition science. The appeal of diets is that they promise simple answers. You cut this fat or this carb, or fast these two days, or only eat foods a neanderthal would have eaten. In a chaotically careening world, they promise control.
But Thevelein isn’t a clinician, and his research partners weren’t intending to give diet advice. They’re molecular biologists, and their investigation was an attempt to understand the mechanisms behind something called the Warburg effect. Unlike healthy cells, cancer cells lack the internal feedback loops designed to conserve resources when food isn’t available. Kind of like Cookie Monster. If there’s blood glucose around, these ravenous cells will consume it all. And they do it using fermentation, as opposed to respiration, which most healthy cells use to break down glucose into smaller units. All that rapid energy gain drives the proliferation of even more cancer cells, and tumors get bigger, faster.
What has remained controversial is whether fermentation is a cause or a symptom of cancer. And that is what the Belgian scientists spent nine years trying to figure out by observing vats and vials of mutant Saccaromyches cerevisiae. What they discovered is that at certain concentrations, glucose can activate a gene called Ras, which is a major regulator of cell proliferation in both yeasts and mammalian cells. That sends the tumor cells into fermentation overdrive, gobbling up all the glucose around and growing like crazy. The findings suggest sugar—in a petri dish—can speed up cancer growth. But it doesn’t say anything about sugar giving people cancer.
These Mice Stopped Eating Carbs So You (Maybe) Don't Have To
Obesity Surgery May Work by Remaking Your Gut Microbiome
Why Are We So Fat? The Multimillion-Dollar Scientific Quest to Find Out
“You can’t take laboratory research of a metabolic pathway and translate it into a diet,” says Christine Zoumas, director of healthy eating programs at UC San Diego’s Moores Cancer Center. “We don’t have the science yet to understand where these pathways can be applicable in our food supply yet.”
Say you’re going to eliminate all the sugars from your body. So not just the occasional doughnut or ice cream cone, but every fruit, vegetable, and whole or refined grain out there. You’re basically left with meat and fat—a ketogenic diet. “That is not meant to be a long-term diet,” says Zoumas. And she says research has shown that while cancer cells prefer sugar as a fuel, they can just as easily survive on fat and protein alone. Plus, while you’re starving all those cancer cells, you’re also depriving your body’s healthy cells too. Which is pretty much the last thing you want to do if you’re sick and on chemo and battling for your life. “You need to be getting calories any way you can,” says Zoumas.
She advises cancer patients on what they should be eating, as well as at-risk populations—people constantly fretting over what they put in their mouth and whether it might feed the disease. Sugar comes up all the time. Zoumas has different advice for everyone, but in general she relies upon a body of scientific evidence that suggests maintaining a healthy weight with a diet based mostly on plants, lean proteins, and (yes) carbs is the best way to reduce your risk of cancer. Even with the occasional doughnut.
Artificial intelligence is now detecting cancer and robots are doing nursing tasks. But are there risks to handing over elements of our health to machines, no matter how sophisticated?


Maybe you've heard that Tiangong-1, China's 19,000-pound prototype space station, is scheduled to rain down on Earth ... eventually. As in, some time between now and next April. Most of the spacecraft will burn up in orbit—but sizable chunks (up to 220 pounds, by one estimate) could end up making landfall.
Thing is, nobody's sure where those chunks will land. Experts aren't exactly perturbed by this: The odds are slim (like, one-in-a-trillion slim) that some part of Tiangong-1 will come crashing down and do you bodily harm.
What you probably haven't heard is how experts go about determining those odds in the first place. In which case, please allow me to introduce you to the supremely fascinating world of object reentry risk analysis—the science of predicting which of the tens of thousands of derelict spacecraft, spent launch vehicles, and various other hunks of technology orbiting our planet will survive their inevitable plunge through the atmosphere and pose a threat to people here on Earth.
The first thing you need to understand is this: When a spacecraft plummets to Earth, it rarely makes the trip intact. The extreme heat and violent forces of atmospheric reentry obliterate most small objects (the industry term for this is, awesomely, "aero-thermal demise"). But the components of larger spacecraft can and do survive, depending on their thermal properties, how aerodynamic they are, and where they live inside the spacecraft.
"Think of the spacecraft itself as the master container," says aerospace engineer Michael Weaver, who oversees reentry risk assessment at the Aerospace Corporation. "The hull of that container has to fail before the parts inside get exposed to heating, and there can be parts inside of parts inside of parts."
This Russian-doll effect can dramatically impact whether components live or die. With detailed enough blueprints, researchers can use software to model atmospheric disintegration. NASA uses a program called the Object Reentry Survival Analysis Tool; the Aerospace Corporation uses AHAB, short for Atmospheric Heating and Breakup.
Components with high melting points tend to endure: titanium alloys, optical elements like glass, and storage containers like fuel, oxygen, and water tanks, which are often wrapped in heat-resistant materials. The latter can be pretty big (the photo above shows a propellant tank from a Boeing Delta rocket that landed in Georgetown, Texas), but not everything that survives reentry poses a threat. "If it's a thermal insulating blanket, it might make it back to Earth, but it's not going to damage you," says Marlon Sorge, a senior project engineer at Aerospace Corporation and an expert in fragmentation modeling.
To qualify as dangerous, debris needs to be massive enough, and be moving fast enough, to deliver at least 15 joules of energy to whatever it hits. "That's about the same as dropping a bowling ball from a height of one foot," Sorge says.
Any debris big enough to do you harm gets factored into what's known as a reentry footprint—the size of the geographic area inside of which debris is liable to land. For controlled reentries, during which people here on Earth guide the spacecraft's descent, that footprint is typically small and located far, far away from humans. Among the most popular crash sites is the unimprovably named Oceanic Pole of Inaccessibility—a point in the South Pacific that resides farther from land than any place else on Earth.
But Tiangong-1's reentry will be uncontrolled, which is why no one knows where it will land. Even if somebody were to calculate the size of its reentry footprint, knowing the size of a foot is very different from knowing where that foot will fall.
That said, here's what risk analysts do know: China's spacecraft is currently circling the planet at an orbital inclination of 42.8 degrees. "That means it could land anywhere on Earth within 42.8 degrees north latitude and 42.8 degrees south," Weaver says, and at any longitude. As you read this, that entire swath of Earth is being passed over by Tiangong-1, "so any population within that range is the population that's at risk."
This is actually less scary than it sounds. Think back to grade school: Water covers roughly three quarters of the planet. That means there's about a 75 percent chance of incoming debris landing in the ocean, where, tech-island utopias notwithstanding, its odds of hurting or killing somebody are basically zilch.
As for the remaining quarter of Earth's surface: Humans populate that land sparsely and unevenly. To calculate the risk they face, researchers use datasets like those in Columbia University's Gridded Population of the World series, which estimate the human population count and density for cells distributed across the globe in a handy, latitude/longitude grid.
These grids let researchers estimate the likelihood that reentering objects will land at a particular latitude/longitude, and, by extension, the number of people at risk of being struck. They're how researchers at NASA's Orbital Debris Program Office came up with the graph pictured above, which shows the average population density beneath a range of orbital inclinations. (We've added a red line to denote Tiangong-1's inclination of 42.8°). The chart uses data from two Gridded Population datasets: One from 2000 and a predictive population model for the year 2050. Based on this graph, the average population density beneath Tiangong-1's orbit is less than 25 people per square kilometer (or 66 people per square mile).
The Space Junk Problem Is About to Get a Whole Lot Gnarlier
Gecko-Inspired Gripper May Soon Snag Space Junk
The 12 Greatest Challenges for Space Exploration
The Mad Plan to Clean Up Space Junk With a Laser Cannon
That's … not a lot of people. What's more, all these calculations make some pretty conservative assumptions. Remember the 15-joule threshold for incoming objects? That much energy could do serious damage if it hit you square in the head, but anywhere else on your body? You'd probably be fine. And charts like the NASA one above ignore the possibility that humans might be protected by buildings or cars when spaceparts start raining from on high. "It's the mathematical equivalent of having everybody in these regions stand outside in an evenly distributed grid and look up," says Ted Muelhaupt, who manages the Aerospace Corporation's Center for Orbital and Reentry Debris Studies.
All of which explains why no expert is all that worried about Tiangong-1 falling from the sky, whenever and wherever that may be. China's spacecraft might be big, but Earth, and its swaths of unpopulated land and sea, are much, much bigger.
Growing food in space is hard. Keeping a limited supply of water clean and drinkable is no easy task either. Here's how NASA is going to science meals for interplanetary travel.


When a raindrop falls in San Francisco, it has two choices: flow east into the San Francisco Bay, or west into the Pacific Ocean. A ridgeline divides the city into two, slicing through the Presidio, hugging the eastern edge of Golden Gate Park, and skirting Twin Peaks. As the land drops off in either direction, the elevation difference doesn’t just drive raindrops downhill—it also moves human waste. San Francisco, unlike any other coastal city in California, has just one set of pipes for its storm runoff and sewage. First engineered more than a hundred years ago, the system still functions on the same basic principle as it did in 1890: Let gravity do the work.
But the city’s early engineers didn’t account for the eight inches of sea level rise the bay has seen over the last century. And they certainly didn’t foresee the additional five feet expected by the end of the next hundred years, which promise to cause major flooding—not to mention a serious poop problem. Engineering a solution to the rising tides will be enormously expensive. Which is why the city thinks someone else should pay for it. Like, say, Big Petroleum.
Last month San Francisco announced it was suing the five largest publicly-held producers of fossil fuels in the world. The lawsuit aims to make these companies pay for the miles of seawall construction and sewer system redesigns required to protect the city from climate change. Climate change caused in part by all the fossil fuels they sucked, mined, and fracked out of the Earth. It may sound like a long shot, but the case rests on one of the oldest and best-tested environmental laws on the books, the same one that brought down Big Tobacco in the ‘90s and Big Lead Paint in the aughts. Whether or not the same rules apply to the threat of climate change is a question with implications far beyond the sewers of San Francisco.
But for now, let’s start there. When Carl Edward Grunsky, a German-born geologist and civil engineer arrived in San Francisco in the 1890s, his sense of order was offended nearly as much as his nose. For nearly 40 years people had been laying brick sewers willy nilly. The result was a city that “smells to Heaven with a loudness and persistence that the strongest nostrils may not withstand and the disinfectants of a metropolis could not remove,” as one health official at the time declared. Grunsky developed plans for an innovative gravity-based sewer system that would drain rainwater-diluted waste all the way from Daly City to North Point, where the rapid current would sweep it out to the Golden Gate.
At the time, it was a great leap forward in sanitation, even though it overflowed during rainstorms and dumped raw sewage into the bay. By the 1970s, with the passing of the Clean Water Act, that was a no-no. So the city built a ring of giant underground chambers around the peninsula—some as big as 25 feet wide by 45 feet tall. During dry weather, sewage collects at one of two treatment plants. But when it rains, storm runoff and sewage run downhill through a single set of pipes and drain into the big chambers. They function like waiting rooms, detaining stormwater until the treatment plants can process it all.
Today, overflow only happens during big storms, when the chambers and treatment stations fill to capacity. Then, sewage and stormwater have nowhere to go but out of 36 discharge outfalls located around the city, several feet below street elevation. But if especially wet weather happens to coincide with peak high tides, those outfalls become submerged, flooding any areas of the city at or below bay level with a mix of sewage and stormwater. This has been happening more frequently lately, as climate change has already raised sea levels around San Francisco by a few inches. And it’s expected to happen a lot more in the coming decades, with levels estimated to increase by up to 24 inches by 2050—enough to put most outfalls underwater daily.
Rising seas aren’t just a problem during the rainy season. In dry weather, big tides can breach the outfalls, flow into the collection containers, and wind up in the treatment plants. Corrosive saltwater wreaks havoc on the pumps, filters, and other equipment keeping San Franciscans from living in a cesspool. Today, that happens fairly infrequently. But more ocean means more ocean water entering and damaging the system.
A backseat engineer might ask at this point, “Why not just raise the outfalls a few feet?”
Oh, were it only so simple. But because the sewers are built on the city’s existing gravity-driven hydraulic gradient, you can’t go tweaking one part without feeling the effects system-wide. Higher outfalls won’t get submerged as often, but during storm events there will be less room for all that water to go. Which would mean even more flooding during the rainy season, even with the outfalls clear of the tides. And, don’t forget, those outfalls lead to containers and treatment plants that are mostly all underground, directly in the path of shoreline erosion caused by rising sea levels.
Without upgrades, here’s what will be inundated by the end of the century: SFO International Airport, the Giants Stadium, the site of the new Warriors Stadium. And, oh yeah, headquarters for companies like Facebook, Google, Airbnb, and the brand-spanking new Salesforce building, now the skyline’s tallest spire. Models project 6 percent of San Francisco will be inundated by normal, daily tides by 2100.
Last year San Francisco Mayor Ed Lee committed an initial $8 million to begin fortifying the city’s seawall. But shoring it up in the short-term will cost about $500 million—long-term, $5 billion. Add to that another $350 million to protect all the wastewater infrastructure and limit how much seawater makes it inside, and the climate change costs quickly add up. Costs, the city now says, should be shouldered by the corporations that still produce vast quantities of fossil fuels, despite having known for decades their role in driving climate change and accelerated sea level rise.
This Week's King Tides Give a Glimpse of Sea Level Rise
Four Radical Plans to Save Civilization From Climate Change
With Harvey, Imperfect Engineering Meets a Perfect Storm
“These fossil fuel companies profited handsomely for decades while knowing they were putting the fate of our cities at risk,” San Francisco City Attorney Dennis Herrera said at a press conference announcing the lawsuit in September, which named BP, Chevron, ConocoPhillips, Exxon Mobil, and Royal Dutch Shell as defendants. “Now, the bill has come due.”
Oakland filed a similar lawsuit on the same day, with both cities charging the companies with liability for public nuisance. They’re seeking only the funds needed to adapt infrastructure against encroaching seas. The cases represent the first real legal test of whether or not climate change blame is not only assignable, but quantifiable.
If they can get in front of a judge. To date, every attempt to bring a case based on a climate change-related injury has either been dismissed for lack of standing, lack of jurisdiction, or both. That’s because, while the courts were willing to entertain the complex causal chains linking secondhand cigarette smoke to cancer and lead house paint to behavioral and developmental issues in children, they so far have viewed climate change as so enormously complicated as to be outside the scope of adjudication. “I told you before I’m not a scientist,” former Justice Antonin Scalia told the Supreme Court in 2006. “That’s why I don’t want to have to deal with global warming, to tell you the truth.”
But many legal experts say pressure (and scientific evidence) is mounting to let climate change have its day in court. “There’s certainly a sense that climate change is broader in scope than what nuisance law has ever addressed before,” says Sean Hecht, co-executive director of the UCLA law school’s Emmett Institute on Climate Change and the Environment. “But it couldn’t be that if the nuisance gets bigger and bigger that courts are less and less likely to impose a remedy. That flies in the face of reason.”
While it’s too early to say whether the court will be willing to look at San Francisco’s case, Hecht says it’s a strong one. The injury to the city is discrete, with sea level rise being the most well-established downstream effect of climate change. California, more so than any other state, has devoted serious resources to gathering data, modeling, and understanding the impacts of sea level rise on its shores. And the fossil fuel industry’s knowledge of the problem even as they sowed a public misinformation campaign has become increasingly well-documented. Any sort of victory, no matter how small, could embolden other local governments to sue oil and gas companies for host of other climate change-related expenses—like, say, super-soaking hurricanes or drought-fueled megawildfires.
Which is why San Francisco’s case is about more than just sewers and seawalls. It’s about who pays for climate change: the people hit hardest by it, or the corporations who profit from it? Like a raindrop falling on the city, there are only two ways forward. And every year the path to the ocean gets a little shorter.
Seawall-topping king tides occur when extra-high tides line up with other meteorological anomalies. In the past they were a novelty or a nuisance. Now they hint at the new normal, when sea level rise will render current coastlines obsolete.


Zipping around like a bumblebee, the little black-and-yellow tractor claws its bucket into one of San Francisco’s few vacant lots, kicking up a puff of dust. Payload secured, it backs up—beep, beep, beep—whips around, and speeds to its dirt pile, stopping so quickly that it tips forward on two wheels. It drops its quarry and backs up—beep, beep, beep—then speeds back to its excavation for another bucketful.
Atop the tractor is, of all things, a cargo carrier, like one you’d put on your car. But instead of carrying camping gear, it’s packed with electronics. Because no one is sitting in this Bobcat tractor—it’s operating itself, autonomously zipping around a lot lined with 4,500-pound concrete blocks. You know, for the safety of the community.
In this dusty arena, a startup called Built Robotics is testing what it thinks is the future of construction: the autonomous track loader. Give it coordinates, tell it what size the hole should be, hit enter, and it tears off and digs the thing with impressive accuracy.
Really, this tractor is not unlike a self-driving car. It uses lidar—that is, it spews lasers—to see the world directly in front of it. The difference being, this lidar is specially designed to work in the high-vibration, high-impact world of construction excavation. The lasers also allow the robot to measure the amount of material it’s scooped up.
But the ATL has one big advantage over self-driving cars: It’s scooting around a relatively static, structured environment. Roads, they’re chaos. A job site isn’t exactly peaceful, but at least the robot is working in a confined space. So to position the robot, Built Robotics uses what’s known as augmented GPS, which combines an on-site base station and satellites to produce location data down to the centimeter.
Still, the construction robots have actual work to do. “The thing that we're doing which is different and that’s a little bit harder than self-driving cars is we're actually manipulating our environment,” says Noah Ready-Campbell, founder and CEO of Built Robotics. “If a car is changing the environment around it, then something's gone really wrong.”
What Is a Robot?
Meet Salto, the One-Legged Robot With an Incredible Leap
Meet the Cute, Wobbly Robot That Started the AI Revolution
Specifically, the robot excavates holes for the foundations of buildings. Say you want a 30-foot-by-40-foot excavation at a depth of two or three feet. Depending on the equipment and conditions, that’d take a crew of humans one to three days to complete. Built Robotics claims its machine can hit about the same pace—plus the robot never gets hurt or tired and can potentially run 24 hours a day. (A robot can, of course, hurt people, though the robot has a collision avoidance system in place. And for the time being a supervisor stands there with a big red and orange kill switch in hand.)
Built Robotics is still honing the tech, but says it has already done small pilot jobs in the Bay Area. “I actually think we can see this technology deployed a lot more quickly and I think you're probably going start to see autonomy make an impact in construction even before you're doing transportation,” says Ready-Campbell.
Which would not be unwelcome. The construction industry has a labor shortage; a survey this year from the US Chamber of Commerce found that 60 percent of contractors report trouble finding skilled labor. Roboticizing the industry wherever possible—repetitive tasks like digging and painting—could ease this strain. Agriculture has the same labor problem right now, and is increasingly turning to robots to fill the gaps.
Will that kill certain jobs? Yes. But like pretty much any robotics company whose tech is designed to replace humans, Built Robotics says that the resulting increase in productivity will grow the industry and shift laborers into new jobs.
The roboticization of the construction industry is also good news for America’s horrifically bad infrastructure. That increased productivity could make repairs easier and cheaper. Take, for instance, robots that inspect and automatically repair pipes without humans having to rip the pipes out of the ground.
In the end, the job of a robot is to make life for humans easier, safer, and more productive. And so one busy little robot digs its way toward an automated future, one scoop at a time.
Introducing HardWIRED, a new video series about the robots that are poised to take over the world. In the first episode WIRED explores what qualifies as a robot in the first place.


At the SynDaver factory in Tampa, Florida, mad scientists are bringing bodies to life. Not Frankensteining the dead, but using a library of polymers to craft synthetic cadavers that twitch and bleed like real suffering humans.
Hospitals and med schools use the fakes to teach anatomy and train surgeons, and the most lifelike model is the $95,000 SynDaver Patient. This exquisite corpse can be controlled wirelessly so practitioners can rehearse elaborate medical scenarios in which the patient goes into shock and even “dies.”
It’s less messy, and a good deal longer-lasting, than real flesh and blood: As long as you keep buying replacement viscera, these bodies won’t ever decay. But because they’re 85 percent water, they must be submerged in a watery grave between uses to keep from drying out.
EyesThe fake corpse’s eyes have tiny screens, so the pupils can dilate in response to light or trauma.
LungsA compressor under the table draws air in and out. Doctors can practice tracheotomies and intubations.
HeartAn electric pump provides a realistic pulse, while a heater warms up the fluids to body temperature.
Add-OnsDiseases on demand! SynDaver can afflict the body with specific pathologies—a pancreatic tumor, say.
LimbsTo simulate a seizure, pneumatic actuators in the legs and arms create jerking motions.
JointsMore than 600 muscles are sutured to the cadaver’s 206 bones, and every  joint is movable.
MaterialsSynDaver’s polymers range in texture from rigidly skeletal to slimily liverlike.
CirculationEach body contains 50 feet of veins and arteries; valves restrict the flow of (fake) blood during shock.
This article appears in the October issue. Subscribe now.
Wired spends some time on the job with bioremediation and hoarding experts, Steri-Clean, Inc.  This video may disturb some viewers.


It was a lovely September day in Yellowstone’s Boiling River, which was not, in fact, boiling. Tourists trundled through the shallow water and dipped in where it was deeper. A herd of elk even waded through unconcerned. And among it all, a team of researchers in waders sampled the water for a brain-eating amoeba that kills 97 percent of the people it infects.
Not that anyone here has ever fallen victim to the amoeba, Naegleria fowleri. Scientists just know that the Boiling River, which gets its warmth from geothermal energy upstream, can harbor the little nasty. Accordingly, signs posted onshore warn swimmers: This thing can ruin your day, and most likely your life.
Wading in that river was an odd mix of scientists from the Monterey Bay Aquarium Research Institute—650 miles from the nearest ocean—and the United States Geological Survey, which, as it turns out, is interested in far more than just rocks. They were collecting water samples to ship off to a rather more obvious participant: the Centers for Disease Control. There, scientists would analyze the water to help unravel the mysteries of the brain-eating amoeba—and hopefully, protect America’s waterways from nasties of all types.
Perhaps the most frustrating thing about Naegleria fowleri is that it has no business eating human brains. The amoeba prefers smaller game, scooting around freshwater ecosystems gobbling up bacteria. But if you happen to be swimming in one of those ecosystems and get a snout full of water, the amoeba can make its way into your brain and start eating tissue, leading to something called primary amebic meningoencephalitis. With the brain swelling come the fever and vomiting, then the seizures and hallucinations. It kills in an average of five days, and claims 97 percent of its hosts.
Naegleria fowleri loves warm waters, hence its presence in the not-quite-boiling Boiling River. But it can survive when things cool down as well. “When in cooler temperatures, it goes into a cyst state, which is kind of an egg-like state that's very hardy,” says Mia Mattioli, an environmental engineer in CDC's Environmental Microbiology Laboratory. “But it's not viable, it's not moving, it's just persisting. When it becomes more favorable and warm it goes into the infectious state.”
Naegleria fowleri's sensitivity to temperature makes it maddeningly difficult to track down, since lower temperatures lower its concentration. Scoop up other kinds of freshwater lifeforms and you might get 100 organisms in a liter of water. With Naegleria, you might be looking at 100 organisms in 100 liters of water. (This rarity, plus the fact you have to somehow get the microbe up your nose, is part of the reason why only 40 people picked up the amoeba in the US between 2007 and 2016.)
So the USGS and MBARI want to get better at detecting the menace. They descended on the Boiling River with two complementary missions. One was to hike out of there with water samples, which they took back to Montana State University and shipped off the CDC for analysis. The second was to test MBARI’s Environmental Sample Processor (known as ESP, of course), which the group hopes could one day have the power to detect the amoeba in real time.
MBARI having oceanic proclivities, this device is a drum full of electronics that you dunk in the ocean. “You can think of it as a molecular biology laboratory in a can,” says research specialist Kevan Yamahara. The ESP collects water samples, analyzing genetics to determine what kinds of organisms are present.
What MBARI took to Yellowstone, though, was a sleeker mobile version, which fits in a tackle box. It pulled water from the Boiling River and pushed it through a filter, which trapped particulates like brain-eating amoebas—at least hopefully. MBARI then shipped these filters, along with water samples, to the CDC. While MBARI is still testing this mobile version of ESP, and it can’t yet do on-board genetic testing, it’s designed to be more modular, so researchers can swap in the latest analytical instruments for real-time genetic analysis.
The idea here is to figure out if the ESP might be capable of detecting Naegleria fowleri in real time. First the CDC will try to detect the amoeba with their lab methods. This includes using both molecular techniques to hunt for microbe DNA in the sample and culturing the parasite in a dish. So let's say that's successful. Then if the ESP ended up trapping amoebas in its filters, that’s a good indication that the machine will be capable of doing genetic testing in the field to identify the parasites. The results from the CDC and the ESP match.
Of Course Australia Has Drop-Off Centers for Deadly Spiders
A Once Powerful Antibiotic Goes the Way of All Flesh
Urban Heat Islands Can Be Deadly, and They're Only Getting Hotter
Conveniently enough, the USGS already has thousands of river gauges around America collecting data like temperature and flow rate. So the work here might help give those sensors the ability to alert authorities to amoeba outbreaks. And that could be critical for unraveling the peculiarities of the Naegleria fowleri life cycle.
“We don't really know that much about, for instance, how it cycles during the day or during the month or during the year through an environment,” says microbiologist Elliott Barnhart of the USGS. “So one thing this ESP could do would be to sample every hour or every day or different seasons to figure out when might this amoeba be most prevalent.”
The worrying bit is that one of the few things scientists know for sure about Naegleria fowleri is that it loves warm water. And an increasingly warm planet will give rise to increasingly warm rivers. While the amoeba is most prevalent in the relative warmth of the southern US, it may well grow more ubiquitous farther north. “We're seeing cases more and more with rises in water temperatures,” says Mattioli of the CDC. Advances like ESP could help officials better monitor all kinds of nasties in American rivers, not just Naegleria fowleri. In the meantime, it never hurts to bring a nose plug.
In South America, the peanut bug ambles around with a goofy-ass head. And that’s not its only clever defense against the bullies of the rainforest.

